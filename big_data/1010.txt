A Recommendation Algorithm Using Multi-Level Association Rules Choonho Kim, Juntae Kim Department of Computer Engineering, Dongguk University, Seoul, Korea chkim, jkim}@dongguk.edu Abstract Recommendation systems predict user  s preference to suggest items Collaborative filtering is the most popular method in implementing a recommendation system. The collaborative filtering method computes similarities between users based on each user  s known preference and recommends the items preferred by similar users Although the collaborative filtering method generally shows good performance it suffers from two major problems data sparseness and scalability In this paper we present a model-based recommendation algorithm 
that uses multi-level association rules to alleviate those problems In this algorithm we build a model for preference prediction by using association rule mining Multi-level association rules are used to compute preferences for items The experimental results show that applying multi-level association rules is effective and performance of the algorithm is improved compared with the collaborative filtering method in terms of the recall and the computation time 1. Introduction Recommendation systems predict a user's preference and suggest items by analyzing the past preference information of users The data used for recommendation can be stored as a preference matrix that represents each user's preference for each item The preference of a user 
can be obtained by either explicit or implicit rating The explicit rating means that a user provides his or her preference of an item on a numerical scale. Implicit rating means that user's actions such as purchase of an item or the click on a web page is interpreted as preferences Items may be a simple set in one level or may be organized into a hierarchical category structure with multi-levels like classification of goods in a department store The collaborative filtering is widely used as a recommendation algorithm It first finds users who have similar preference with the target user and predicts preference of the target user based on preferences of the 
similar users GroupLens 9 and Ringo  were the first recommendation systems using CF algorithms GroupLens recommended Usenet news articles based on user  s ratings and used Pearson correlation as similarity measure Ringo recommended music by using weighted average of ratings of restricted number of similar users Also there have been many researches for performance analysis and improvement of recommendation systems Breese 3 compared CF with clustering model and Bayesian model Helocker 6 tested various similarity measures including 
Pearson correlation Spearman correlation vector similarity and entropy He also applied various weighting schemes such as similarity weighting significance weighting and variance weighting To reduce computing time dimensionality reduction methods using SVD\(singular value decomposition were proposed by Billsus 2 and Sarwar  Combination of collaborative filtering method with content-based or knowledge-based approach were suggested by Balabanovic 1  Burke 4  and Nguyen 8  Lin 
7 and Sarwar  applied association rule mining technique to collaborative recommender systems The collaborative filtering is a memory-based algorithm Although it generally shows good performance when sufficient explicit preference information is given it suffers from two major problems  the data sparseness and the scalability When the number of known preferences is very small  in other words the user-item preference matrix is very sparse  performance of recommendation can be very poor since it becomes 
difficult to find similar users This is a critical problem because in practical applications such as recommending items in a web shopping mall the number of known preferences for example purchasing an item is very small compared to the total number of items Also due to the similarity computation between users computation time for recommendation increases as the number of users grows This scalability problem may also be a critical problem for practical applications with millions of users In this paper we present a model-based recommendation algorithm that uses multi-level association rules to alleviate those problems In this 
algorithm we build a model for preference prediction by using association rule mining Multi-level association rules are used to compute preference for items that are not covered by the single-level association rules due to the data sparseness We compared performance of the suggested algorithm with that of the collaborative filtering algorithm and single-level association rule algorithm in terms of the accuracy and the computation time Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WI03 0-7695-1932-6/03 $17.00  2003 IEEE 


2 Recommendation using multi-level association rules 2.1 Association rule mining Association rule mining is to search for interesting relationships between items by finding items frequently appeared together in the transaction database If item B appeared frequently when item A appeared then an association rule is denoted as A  B if A then B The support and confidence are two measures of rule interestingness that reflect usefulness and certainty of a rule respectively 5  Support as usefulness of a rule describes the proportion of transactions that contain both item A and B and confidence as validity of a rule describes the proportion of transactions containing item B among the transactions containing item A The association rules that satisfy user specified minimum support threshold  minSup  and minimum confidence threshold minCon  are called strong association rules In our approach each user  s preference is represented as a boolean vector  1 means that the user prefers the item Then we regard each user  s preference vector as a transaction, and search for association rules among items 2.2 Recommendation using association rules Recommendation using association rules is to predict preference for item k when the user preferred item i and j  by adding confidence of the association rules that have k in the result part and i or j in the condition part Sarwar  used the rule with the maximum confidence but we used the sum of confidence of all rules in order to give more weight to the item that is associated with more rules Recommendation using association rules describes as follows Let P be the preference matrix of n users on m items In this matrix p ij is 1 if the user i has preference for the item j  and 0 otherwise Let A be an association matrix containing confidence of association rules of m items to each other The matrix A is computed from P  In this matrix a ij is confidence of association rule i  j Then the recommendation vector r for the target user can be computed from the association matrix A and the preference vector u of the target user as equation \(1 The top-N items are recommended to the target user based on the values in r  A u r     1  2.3 Recommendation using multi-level association rules In recommendation using association rules if the amount of available preference information is small then the number of strong association rules would also be small and consequently the association matrix becomes very sparse Actually in case the purchase data is used as preference of users the number of applicable association rules is very small because each user usually purchases only small number of items In this case it is impossible to predict preference for most of items and performance of recommendation becomes very poor To overcome this problem we present a recommendation method using additional information  the association rules between higher-level categories  if items are organized into a hierarchical category structure We first find the category c i to which the preferred item belongs If there is a category association rule of the form c i  c j  then we give certain amount of preference to all items that belong to category c j  Recommendation using multi-level association rules describes as follows Let C k be a category relation matrix that represents the inclusion relation between lower level\(k-1 category and higher level\(k category level 0 category means the item In this matrix C k  i  j s1if category i in level k-1 belongs to category j in level k and 0 otherwise Using this matrix, the preference matrix P k and the preference vector u k of the target user for level k category is computed as follows k k C C C P P       2 1 0  2  k k C C C u u       2 1 0  3  In equation 2 elements of P k for k  1 are not binary If a user has preference to multiple items in a category it can be greater than 1 The association matrix A k for level k category is computed from P k  It consists of confidence of association rules between categories Note that all diagonal elements of A k should be 1 to give preference to other items in the same category the user preferred In other words the confidence of association rule c i  c i is always 1 for level k  1 From pre-computed association matrices and category relation matrices we can compute the recommendation vector r as follows  1  1 1 1 0 0                 k k k k 0      T 1 T k T 1 C C A u C A u A u r  4  In equation \(4  k is the weight to preference computing by level k association rules and the sum of weights is 1 The weights for the best recommendation are determined empirically Each predicted preference of all level categories is normalized to 0.5 through sigm oid function respectively Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WI03 0-7695-1932-6/03 $17.00  2003 IEEE 


3 Experiment and Results 3.1 Dataset We used two separate datasets for the experiments The MovieLens  is a set of ratings to movies The users rated each movie with 0 to 5 integer values We selected the users who rated at least 20 movies which become a set of about 100,000 ratings from 943 users on 1682 movies MovieLens dataset are organized into two levels  there are 18 categories genres as higher level of items movies A movie belongs to one or more genres We assumed that the explicit ratings for items are not generally given so converted the original ratings into preference of binary form To reflect user  s rating scale each rating was converted to 1 if the rating is greater than the user  s average rating, and 0 otherwise The KDD Cup 2000  domain contains clickstream and purchase data from Gazelle.com a legwear and legcare web retailer We selected a set of 2,295 purchase data from 793 users on 253 items KDD dataset consists of three category levels  there are 39 items as category level 1 item and 2 items as category level 2 item Each level  s item belongs to one of higher level items 3.2. Experimental method and metric We divided dataset into 80 of training set and 20 of test set and averaged the results of 5 trials with different test set 5-fold cross validation To measure accuracy of recommendation we hided one preference data for each user in test set and recommended top-N items with highest predicted preference based on remaining preference information Accuracy is computed as the ratio of correct prediction If the hided item is actually recommended  contained within the top-N recommendation list it  s called hit  We used the Recall the overall number of hits over the total number of preferred items in test set the number of  1  s in matrix P  as the accuracy measure The first experiment is the performance comparison between two different methods of applying association rules  using a rule with the maximum confidence and using the sum of confidences when more than one association rules are available The second experiment is the performance comparison among simple frequency method\(SF collaborative filtering method\(CF singlelevel association rule method\(AR and multi-level association rule method\(MAR The simple frequency method is the basic recommendation method that recommends the most popular top-N items to all users by counting overall preferred frequencies for each item from the preference matrix In CF the cosine similarity was used as a similarity measure and the number of similar users was set to 20 which was the best among 10 20 30 40 and 50 In AR we used all available association rules because applying higher minSup and minCon to select strong rules make the number of applicable association rules decreased rapidly and causes poor performance due to sparseness of dataset In MAR we used up to level 1 category information for MovieLens dataset and up to level 2 category for KDD dataset MAR showed better performance as minSup and minCon thresholds for high-level categories are set to higher values We set both thresholds to 80 for two datasets respectively in this experiment In all of experiments we used N  10 20 30 40 and 50 as the number of items to be recommended and measured Recall for each N respectively All experiments were performed on Pentium-4 running at 1.8GHz 1Gbytes of memory and Windows 2000 operating systems 3.3 Results 3.3.1 Applying methods of association rules We compared the two different methods of applying association rules AR-MAX is the method of applying the maximum confidence rule and AR is the method of applying the sum of confidence of all available rules The results show that AR outperforms AR-MAX by 20 for MovieLens dataset and by 7 for KDD dataset in all cases of top-N This result means that the item associated with more rules is more appropriate to recommend 3.3.2 Performance of various algorithms Figure 1 and 2 show the results of four different recommendation algorithms for two datasets For the MovieLens dataset when we computed the recommendation vector in MAR we used  0  0.9 and  1  0.1 as the weight for each level MAR showed relatively good performance for small values of  1  From empirical results We understood that higher level category information genre in the MovieLens domain is less closely related to lower level movie In Figure 1 the result shows that MAR achieved 2.5 1.2 1.3 1.8 and 2.0 higher accuracy for each value of top-N compared with AR This result shows that the application of multi-level association rules is effective MAR showed somewhat less performance compared with CF when N is less than 20 but showed higher accuracy by 1.5 5.4 and 8.6 when N is more than 30 Performance of CF becomes relatively poor as N increases because the CF method can  t predict preference of many items due to data sparseness Figure 2 shows the result of the KDD dataset We used  0  0.6  1 0.3,and  2  0.1 as the weight for each level MAR outperformed AR by average 90 for all top-N and CF up to 43 when N is more than 30 In the Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WI03 0-7695-1932-6/03 $17.00  2003 IEEE 


CF method performance was almost the same when N is more than 30 It also revealed the sparseness problem of CF Especially AR shows less performance than SF for all top-N The KDD data set was so sparse that the number of association rules was very small  0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Top10 Top20 Top30 Top40 Top50 Recall SF CF AR MAR Figure 1. Performances of four recommendation algorithms for the MovieLens dataset 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Top10 Top20 Top30 Top40 Top50 Recall SF CF AR MAR Figure 2. Performances of four recommendation algorithms for the KDD dataset 3.3.3 Recommendation time The computation times for recommendation for 10000 users were 0.501 sec for CF and 0.011 sec for MAR in case of MovieLens dataset and 0.072 sec for CF and 0.00037 sec for MAR in case of KDD dataset The experimental results with 20000 and 30000 users also show that the model-based method MAR took almost constant time but time of memory-based method CF increases as the number of users grows for two dataset respectively 4 Conclusion In this paper we present a model-based recommendation algorithm that uses multi-level association rules to alleviate the sparseness and scalability problems in memory-based recommendation In this algorithm we build a model for preference prediction by using association rule mining Multi-level association rules are used to compute preferences for items that are not covered by the association rules between items due to the data sparseness Through the experiments using MovieLens and KDD dataset it is shown that applying multi-level association rules increases recommendation accuracy compared with applying single-level association rules only The suggested algorithm also shows better performance compared with the basic collaborative filtering as the number of recommendation increases in a sparse environment 5. References 1 M  Balabanovi and Y Shoham   Fab Content-Based Collaborative Recommendation   Communication of the ACM vol. 40\(3 2 D. Billsus and M.J P azzani  Learning Collaborative Information Filters   In Proceedings of the Fifteenth International Conference on Machine Learning 1998 3 J.S. Breese, D. Heckerm an, and C. Kadie  Empirical Analysis of Predictive Algorithms for Collaborative Filtering   In Proceedings of the Fourteenth Annual Conference on Uncertainty in Artificial Intelligence  1998 4 R. Burke   Integrating Knowledge-Based and Collaborative filtering Recommender System   Proceedings of the Conference on Artificial Intelligence for Electric Commerce  1999 5 J. Han and M. Kam ber Data Mining: Concepts and Techniques Morgan Kaurmann Publishers, 2000 6 J.L. Herlocker J.A Konstan, A  Borchers, and J. Riedl  An Algorithmic Framework for Performing Collaborative Filtering   In Proceedings of the Conference on Research and Development in Information Retrieval 1999 7 W. Y. Lin, S. A  A lvarez, and C. Ruiz  Collaborative recommendation via adaptive association rule mining   Int Workshop Web Mining for E-Commerce 2000 8 H. Nguy en and P  Haddaw y   DIVA Applying Decision Theory to Collaborative Filtering   Proceedings of the Conference on Artificial Intelligence for Electric Commerce  1999 9 P Resnick, N. Iacovou, M. Suchak, P  Bergstrom and J Riedl  GroupLens An Open Architecture for Collaborative Filtering of NetNews   In Proceedings of CSCW 1994  B.M. Sarw ar G  Kary pis, J.A Konstan, and J.T. Riedl  Analysis of Recommendation Algorithms for ECommerce   ACM Conference on Electronic Commerce  2000  B.M. Sarw ar G  Kary pis, J.A Konstan, and J.T. Riedl  Application of Dimensionality Reduction in Recommender System-A Case Study   In WebKDD 00-Web-mining for ECommerce Workshop 2000  U. Shardanand and P  Maes  Social Information filtering Algorithms for automating  word of mouth   In Proceedings of ACM CHI  95 Conference on Human Factors in Computing Systems 1995  http://w w w cs.um n.ed u/Research/G roupLens/index.htm l  http://w w w ecn.purdue.edu/KDDCUP  Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WI03 0-7695-1932-6/03 $17.00  2003 IEEE 


      Before encoding, we need firstly find out the span of each parameter \(variable in genetic algorithm  Apparently, xmin and xmax are respectively the minimum and maximum in the dataset contributing to the attribute Generally, a larger span of parameter will cause a longer chromosome, hence an exponential doubling of computing cost. Thus we can shorten the span to some extent according to restriction among parameters themselves. For function F, the parameters should meet the inequation a  lt;b-d&lt;c  lt;b&lt;a&lt;b+d&lt;c                    \(4 Evaluating the approximately value of parameters in fittest chromosome, we set the span of each parameters as xmax - xmin xmin?a  xmin+xmax 7xmin+xmax 3xmin+5xmax Consequently the size of gene- segment \(represents an encoded parameter shortened span. Finally, we encode each parameter and generate the initial population 3.2.2.  Fitness Function A fitness function is used to quantify the optimality of a solution \(that is, a chromosome In anomaly detection, the fittest membership function should be capable to minimize the similarity of rule-set S profile of normal state profile of anomalous state Let P={x1,x2  xn} be the parameter-set of all membership functions, xi \(1?i?n of parameters. Thus we can define the fitness function as f\(P P,S1,S2 generated by fuzzy data mining using parameter-set P respectively with system in normal state and anomalous state In addition, because the parameter-set P should meet the inequation \(4 meet the inequation are infeasible solutions and should be punished. Consequently the final fitness function is defined as the following 1 2   1 similarity P S S f P 0 if   P meet \(4 otherwise     5 a  c  b d a c a  c  b d a c 1011010 001101  001101   Figure 2. Encoding parameters of function F 1989 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  3.2.3.  Genetic Operator The commonly used operators in genetic algorithm are selection, crossover and mutation There are several well-defined organism selection methods, and roulette wheel selection and tournament selection are popular ones. Here we use tournament selection method and select the best chromosome to replace the least one. The selection rate can be set to be between 0.1 and 0.2. Finally, the best chromosome is carried over to the 


and 0.2. Finally, the best chromosome is carried over to the next generation unaltered according to the elite selection strategy Following the selection, the crossover operation is performed upon the selected chromosomes. Let v be the size of the population and Pc \(typically between 0.6 and 1.0 be the crossover rate, and the number of chromosomes undergone crossover is v  Pc. A random starting point is selected, along with a random length, a section of each parent is then removed and swapped with the other parent In anomaly detection, the chromosome size is always large because of many quantitative attributes involved in database. Here we perform two-point or multi-point crossover between two selected individuals The next step is to mutate the newly created offspring Typical genetic algorithms have a fixed, very small mutation rate \(Pm of chromosome, therefore  the number of bits undergone crossover is . The new chromosome is randomly mutated by randomly altering bits in the parents   chromosome data structure mv p L    3.2.4.  Termination Criteria An important part of the genetic algorithm is the termination criteria i.e. when to stop reproducing the individuals. Because the fitness of the ideal individual is uncertain in anomaly detection, we can define termination criteria in two ways I individual is higher than the predefined expectation value II number of generations 4. Experiments on Anomaly Detection of Network Traffic Applying the approach described above, we have developed a fuzzy data mining and genetic optimization system \(FGS experimental network to validate the feasibility of the approach 4.1.  Database of Network Traffic In the experiment, four characteristic attributes related to network flow are selected to analyze the network traffic with system in normal and anomalous states, including Ptcp\(the proportion between number of TCP packets and the number of total packets the proportion between number of UDP packets and the number of total packets Avg.packet/sec\(the average number of packets in one second the average number of bits in one second calculate the value of the attributes for ten times in both normal and anomalous states \(intruding one host computer in the LAN by DoS attack network traffic involves twenty transactions as shown in table 1 Table 1.  Database of network traffic Ptcp Pudp Avg.pkt/sec Avg.Mb/sec State 94.5 0.6 169.368 0.527 normal 94.5 0.7 171.532 0.548 normal           90.2 0.4 143.566 0.416 normal 96.2 1.0 181.477 0.523 normal 95.2 0.3 169.542 0.530 anomalous 95.9 0.4 171.837 0.531 anomalous 


95.9 0.4 171.837 0.531 anomalous           96.2 0.9 171.476 0.523 anomalous 93.8 0.4 183.937 0.596 anomalous  Each attribute is divided into three fuzzy sets characterized by the membership function F shown is figure 3 2 2 0 2 2  1 2 2 1      x a x a a a x c a S x a c c x a c c x c c a x c Z x a c S x a c S x b d b x b x a c Z x b b d x b    lt    lt  gt    lt     Figure 3. Membership function F 4.2.  Parameters Optimization The database of network traffic consists of four quantitative attributes characterized by four membership 1990 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  functions respectively, and each function is controlled by six parameters. Thus the parameter-set that is to be encoded 


six parameters. Thus the parameter-set that is to be encoded into a chromosome involves 24 parameters. Considering the span of each attribute, we specify the size of chromosome section as shown in table 2 For example, each parameter in membership function characterizing the attribute Ptcp will be encoded into a five-bit string \(chromosome section  0  and  1   Table 2. Size of chromosome section Attribute Section size \(bit Ptcp 5 Pudp 3 Avg.packet/sec 5 Avg.Mbit/sec 3  The whole size of an individual is L=5  6+3  6+5  6+3  6=96 \(bit With the method described in 3.2.1, we encode the 24 parameters separately and link them into an individual. Set the size of population equal to 5, we generate 5 individuals randomly as the initial population Choose an individual in population and decode corresponding chromosome sections into real numbers which are the parameters of four membership functions Using the parameters in mining fuzzy association rules[10 from the database of network traffic \(setting the minimum support degree minsupport=0.25 and the minimum confidence degree minconfidence=0.6 association rule-set of normal state S1 and that of anomalous state S2. Calculating the similarity of S1 and S2 we subsequently get the fitness of the current individual with the fitness function \(5 In the same way, we calculate the fitness of other individuals and the result is shown in table 3 Table 3. Individuals of initial population and their fitness Individual Similarity Fitness 00001  01100 0.712 0.288 00100  10110 0.848 0.152 10000  01101 0.741 0.259 00010  01110 0.587 0.413 00101  11110 0.652 0.348  The evolution operations include I give up the one with the lowest fitness II the other four individuals with random starting points and random length of swapping section III random bit in the four individuals and perform complementary operation Repeatedly, calculate fitness of the new individuals and reproduces next generation population. Finally, the genetic algorithm terminates after 1000 generations and the fittest individual is generated The highest fitness and corresponding similarity of each generation population in the evolution process is shown in figure 4  Figure 4.  Fitness and similarity of the best individual in generations As we can see in figure 4, the curve of fitness denotes an incremental trend in the whole process, and the best individual \(fitness=0.789 minimizes the similarity \(equals to 0.211 and anomalous rule-set Repeat the experiment with the system in other different anomalous states, and the experimental results are shown in figure 5  Figure 5. Similarity curve with system in different states 


Figure 5. Similarity curve with system in different states  The experimental results denote that we can find an ideal parameter-set by genetic algorithm. Using the 1991 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  parameter-set in anomaly detection, we can differentiate normal state of the protected system from anomalous state to the largest extent. Obviously, the membership functions generated by the genetic algorithm is more efficient than those defined with experts  domain knowledge 5.  Conclusions and Future Work The goal of applying genetic algorithm in anomaly detection is to search ideal membership functions. The main advantage is that we can define membership function randomly without experts  domain knowledge. For an IDS genetic algorithm is applied only in system design. The optimized membership functions will be used directly in real-time detection, therefore the performance of IDS will not be influenced although genetic optimization is a time-consuming process Nevertheless, membership functions still need to be tuned constantly because that the normal state of protected system changes with time. We can adapt the normal profile characterized by rule-sets membership functions ultimately, and how to build an adaptive anomaly detection system by genetic algorithm or other intelligent methods will be investigated in future research References 1] Stefan Axelsson. Intrusion detection systems: Asurvey and taxonomy. Technical Report No. 99-15, Dept. of Computer Engineering, Chalmers University of Technology, Sweden, March 2000 2] Debar H., Dacier M., Wepspi A. A Revised Taxonomy for Intrusion Detection Systems. Technical Report Computer Science/ Mathematics. 1999 3] Wenke Lee et. al. Mining audit data to build intrusion detection models. Proc. Int. Conf. Knowledge Discovery and Data Mining \(KDD'98 1998 4] Wenke Lee et. al. Real Time Data Mining based Intrusion Detection. Proceedings of DISCEX II, 2001 5] Susan M. Bridges, Rayford B, Vaughn. Intrusion Detection Via Fuzzy Data Mining. Proc. of 12th Annual Canadian Information Technology Security Symposium, June 19-23, Ottawa, Canada, pages 109-122, 2000 6] Kuok, C., A. Fu, and M. Wong. Mining fuzzy association rules in databases. SIGMOD Record 17\(1 41-6. 1998 7] WengdongWang. Genetic Algorithm Optimization of Membership Functions for Mining Fuzzy Association Rules. International Joint Conference on Information 8] Agrawal R., R.Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th international conference on very large databases 1994 9] Guosizong, Chengang. Method of Soft Computing in Information Science. Northeastern University Press China. 2001 10] 10 Zhutianqing, Wangxianpei, Xiongping. Mining Fuzzy Association Rules and Response in IDS Computer Engineering and Applications. Vol.40\(15 pages 148-150. 2004  1992 pre></body></html 





 0 100 200 300 400 500 600 1 21416181 Time \(epoch counts 0.6 0.9 1.0 a MinMaxLoad WC 0 1000 2000 3000 4000 5000 6000 7000 8000 1 21416181 Time \(epoch counts 0.6 b SS2 Figure 4 Space needed at node R to store answer synopsis S A  non-distributed single-stream case In contrast when SS2 is used to set the precision gradient Figure 4b the space requirement is almost an order of magnitude greater This difference in synopsis size occurs because in SS2 frequency counts are only pruned from synopses at leaf nodes so counts for all items that are locally frequent in one or more local streams reach the root node No pruning power is reserved for the root node and therefore no count in S A is ever discarded irrespective of the  value The same situation occurs if Algorithm 2b is used instead of Algorithm 2a This result underscores the importance of setting  1  in order to limit the size of S A  as discussed in Section 2.1 3.4 Communication Load Figure 5 shows our communication measurements under each of our two metrics for each of our three data sets under each of the ve strategies for setting the precision gradient listed in Table 3 First of all as expected the overhead of SS1 is excessive under both metrics Second by inspecting Figure 5a we see that strategy MinRootLoad does indeed incur the least load on the root node R in all cases as predicted by our analysis of Section 2.1.2 Under this metric MinRootLoad outperforms both simple strategies SS1 and SS2 by a factor of 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 247k 10k 296k 132k 20k counts transmitted \(per epoch a Load on root node R 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 43k    21k 52k   19k 23k   7k counts transmitted \(per epoch b Maximum load on any link Figure 5 Communication measurements k denotes thousands ve or more in all cases measured However MinRootLoad performs poorly in terms of maximum load on any link as shown in Figure 5b because no early elimination of counts for infrequent items is performed and consequently synopses sent from the grand-children of the root node to the children of the root node tend to be quite large As expected MinMaxLoad NWC performs best under that metric on all data sets For the AUCTION data set even though SS2 outperforms MinMaxLoad WC to be expected because of the high  value our hybrid strategy MinMaxLoad NWC is superior to SS2 by a factor of over two For the I NTERNET 2 and BBOARD data sets the improvement over SS2 is more than a factor of three On the negative side total communication not shown in graphs is somewhat higher under MinMaxLoad WC than under SS2 increase of between 7.5 and 49.5 depending on the data set 4 Related Work Most prior work on identifying frequent items in data streams 6 8 9 19 only considers the single-stream case While we are not aware of any work on maintaining frequency counts for frequent items in a distributed stream setting work by Babcock and Olston does adProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


dress a related problem In the problem is to monitor continuously changing numerical values which could represent frequency counts in a distributed setting The objective is to maintain a list of the top k aggregated values where each aggregated value represents the sum of a set of individual values each of which is stored on a different node The work of assumes a single-le v e l communication topology and does not consider how to manage synopsis precision in hierarchical communication structures using in-network aggregation which is the main focus of this paper The work most closely related to ours is the recent work of Greenwald and Khanna which addresses the problem of computing approximate quantiles in a general communication topology Their technique can be used to nd frequencies of frequent items to within a congurable error tolerance The work in focuses on providing an asymptotic bound on the maximum load on any link our result adheres to the same asymptotic bound It does not however address how best to congure a precision gradient in order to minimize load which is the particular focus of our work 5 Summary In this paper we studied the problem of nding frequent items in the union of multiple distributed streams The central issue is how best to manage the degree of approximation performed as partial synopses from multiple nodes are combined We characterized this process for hierarchical communication topologies in terms of a precision gradient followed by synopses as they are passed from leaves to the root and combined incrementally We studied the problem of nding the optimal precision gradient under two alternative and incompatible optimization objectives 1 minimizing load on the central node to which answers are delivered and 2 minimizing worst-case load on any communication link We then introduced a heuristic designed to perform well for the second objective in practice when data does not conform to worst-case input characteristics Our experimental results on three real-world data sets showed that our methods of setting the precision gradient are greatly superior to na  ve strategies under both metrics on all data sets studied Acknowledgments We thank Arvind Arasu and Dawn Song for their valuable input and assistance References  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB  1994  I Akamai T echnologies Akamai http://www akamai.com   A Ak ella A Bharambe M Reiter  and S Seshan Detecting DDoS attacks on ISP networks In PODS Workshop on Management and Processing of Data Streams  2003  A Arasu and G S Manku Approximate quantiles and frequency counts over sliding windows In PODS  2004  B Babcock and C Olston Distrib uted top-k monitoring In SIGMOD  2003  M Charikar  K  Chen and M F arach-Colton Finding frequent items in data streams In International Colloquium on Automata Languages and Programming  2002  E Cohen and M Strauss Maintaining time-decaying stream aggregates In PODS  2003  G Cormode and S Muthukrishnan What s hot and whats not Tracking frequent items dynamically In PODS  2003  E D Demaine A Lopez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space In European Symposium on Algorithms  2003  DynaServ er R UBis and R UBBos http://www.cs rice.edu/CS/Systems/DynaServer   eBay Inc eBay  http://www.ebay.com   C Estan and G V a r ghese Ne w directions in traf c measurement and accounting In SIGCOMM  2002  M F ang N Shi v akumar  H  Garcia-Molina R Motwani and J Ulmann Computing iceberg queries efciently In VLDB  1998  P  B Gibbons and Y  Matias Ne w sampling-based summary statistics for improving approximate query answers In SIGMOD  1998  P  B Gibbons and S T irthapura Estimating simple functions on the union of data streams In Symposium on Parallel Algorithms and Architectures  2001  M Greenw ald and S Khanna Po wer conserving computation of order-statistics over sensor networks In PODS  2004  J Han J Pei G Dong and K W ang Ef cient computation of iceberg queries with complex measures In SIGMOD  2001  Internet2 Internet2 Abilene Netw ork http abilene.internet2.edu   R M Karp S Shenk er  and C H P apadimitriou A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  2003  H.-A Kim and B Karp Autograph T o w ard automated distributed worm signature detection In Proceedings of the 13th Usenix Security Symposium  2004  A Manjhi V  Shkapen yuk K Dhamdhere and C Olston Finding recently frequent items in distributed data streams Technical report 2004 http://www cs.cmu.edu/manjhi/freqItems.pdf   G S Manku and R Motw ani Approximate frequenc y counts over data streams In VLDB  2002  Open Source De v elopment Netw ork Inc Slashdot http://slashdot.org  Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


