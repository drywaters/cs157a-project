Protecting Sensitive Knowledge By Data Sanitization 001 Stanley R M Oliveira 1  2 Osmar R Za\250 031ane 2 1 Embrapa Inform\264 atica Agropecu\264 aria 2 Department f Computing Science 13083-886 Campinas SP Brasil University of Alberta Edmonton Canada oliveira@cs.ualberta.ca zaiane@cs.ualberta.ca Abstract In this paper we address the problem of protecting some sensitive knowledge in transactional databases The challenge is on protecting actionable knowledge for strategic decisions but at the same time not losing the great bene\336t 
of association rule mining To accomplish that we introduce a new ef\336cient one-scan algorithm that meets privacy protection and accuracy in ssociation rule mining without putting at risk the effectiveness of the data mining per se 1 Introduction Despite its bene\036t in marketing modern business medical analysis and many other applications association rule mining can also pose a threat to privacy and information security if not done or used properly There are a number of realistic scenarios in which privacy and security issues in association mining arise We describe one challenging 
scenario as follows Two or more companies have a very large dataset of records of their customers\222 buying activities These companies decide to cooperatively conduct association rule mining on their datasets for their mutual bene\036t since this collaboration brings them an advantage over r ompetitors However some of these companies may not want to share some strategic patterns hidden within their own data also called restrictive association rules with the other parties They would like to transform their data in such a way that these restrictive associations rules cannot be discovered Is 
it possible for these companies to bene\036t from such collaboration by sharing their data while still preserving ome restrictive association rules In this paper we address the problem of transforming a database into a new one that conceals some strategic patterns restrictive association rules while preserving the 001 S Oliveira was partially supported by CNPq Brazil and O.R Za\250 031ane by NSERC Canada We would like to thank Y Saygin and E Dasseni for providing us with the code of their respective algorithms general patterns and trends from the original database The 
procedure of transforming an origi nal database into a sanitized one is called data sanitization The sanitization process acts on the data to remove or hide a group of restrictive association rules that contain sensitive knowledge On the one hand this approach slightly modi\036es some data but this is perfectly acceptable in some real applications 2 6 4 On the other hand an appropriate balance between a need for privacy and knowledge discovery must be guaranteed Our contribution in this paper is two-fold First we introduce an ef\036cient one-scan algorithm called Sliding Window Al 
gorithm SWA This algorithm requires only one pass over a transactional database regardless of the database size and the number of restrictive association rules that must be protected This represents a signi\036cant improvement over the previous algorithms presented in the literature 2 6 3  which require various scans depending on the number of association rules to be hidden Second we compare our proposed algorithm with the similar counterparts in the literature Our experiments demonst rate that our algorithm is effective scalable and achieves signi\036cant improvement over the other approaches presented in the literature We 
also introduce e notion of disclosure threshold for every single pattern to restrict In other words rather than having one unique threshold 001 for the whole sanitization process we can have a different threshold 001 i for each pattern i to restrict This provides a greater 037exibility allowing an administrator to put different weights for different rules This paper is organized as follows Related work is reviewed in Section 2 In Section 3 we describe our heuristic to improve the balance between privacy and knowledge discovery In Section 4 we present the experimental results 
Section 5 presents our conclusions and a discussion 2 Related Work The idea behind data san itization was introduced in Atallah et al considered the problem of modifying a given database so that the support of a given set of sensitive rules decreases below the minimum support value The authors Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


focused on the theoretical approach and showed that the optimal sanitization is an NP-hard problem In 2 the authors investigated con\036dentiality issues of a broad category of association rules and proposed some algorithms to preserve privacy of such rules above a given privacy threshold In the same direction Saygin et al 6 i nt roduced s o me al gorithms to obscure a given set of sensitive rules by replacing known values with unknowns while minimizing the side effects on non-sensitive rules Like the algorithms in 2 thes e a lgorithms are C P U-intens i v e and require v a rious scans depending on the number of association rules to be hidden Oliveira and Za\250 031ane 3 i ntroduced a frame w o rk for protecting restrictive patterns composed of sanitizing algorithms that require only two scans over the database The 036rst scan is required to build an index inverted 036le for speeding up the sanitization process while the second scan is used to sanitize the original database The work presented here differs from the related work in some aspects as follows First we study the effectiveness of SWA and the counterpart algorithms by quantifying how much information is preserved after sanitizing a database So our focus s not y on hiding restrictive association rules but also on maximizing the discovery of rules after sanitizing a database Second in terms of balancing between privacy and disclosure our approach is very 037exible since one can adjust a disclos ure threshold for every single association rule to be restricted nother advantage is t SWA is not a memory-based algorithm and therefore can deal with very large databases This represents a signi\036cant improvement over the previous algorithms 2 6 3 3 Heuristic Approach Before introducing our heuristic for data sanitization we present some preliminary concepts The explicit de\036nitions can be found in 5 es tricted a s s o ciation rules are rules that need to be hidden In other words applying an algorithm such as ri should not lead to the discovery of such rules We note such rules as R R  230 R R are e non restricted rules such as 230 R R 002 R R  R the set of all association rules in a transactional database D  A group of restrictive association rules is mined from a database D based on a special group of transactions referred to sensitive transactions Sensitive transactions are transactions that contain items involved in any restricted association rule For each restrictive association rule we have to identify a candidate item that should be removed from its sensitive transactions We refer to this item as the victim item In many cases a group of restrictive rules share one or more items In this case the selected victim item is one shared item The rationale behind this selection is that by removing the victim item from a sensitive transactions that contains a group of rules such rules would be hidden in one step Our heuristic approach has essentially 036ve steps s ollows hese steps are applied to every group of K transactions window size read from th e original database D  Step1 For each transaction read from a database D we identify if it is sensitive If not the transaction is copied directly to the sanitized database D 001 Otherwise it must be sanitized Step2 We select the victim item the one in the restrictive association rules related to the current sensitive transaction with the highest frequency Otherwise the victim item is selected randomly Step3 Given the disclosure threshold 001  we compute the number of transactions to be sanitized Step4 We sort in ascending order of size the sensitive transactions computed in the previous step for each restrictive rule Thus we start marking the shortest transactions to be sanitized since shortest transactions have less combinations of association rules This will minimize the impact on the sanitized database Step5 Every restrictive rule has now a list of sensitive transaction IDs with their respective selected victim item Every time we remove a victim item from a sensitive transaction we perform a look ahead procedure to verify if that transaction has been selected as a sensitive transaction for her restrictive rules If so and the victim item we just rem oved from the current transaction is also part of this other restrictive rule we remove that transaction from the list of transaction IDs marked in the other rules In doing so the transaction will be sanitized and then copied to the sanitized database D 001  This look ahead procedure is done only when the disclosure threshold is 0 This is because the look ahead improves the misses cost but could signi\036cantly degrade the hiding failure When 001 0  there is no hiding failure i.e all restrictive rules are hidden and thus there is no degradation possible but an improvement in the misses cost The intuition behind the Sliding Window Algorithm SWA is that SWA scans a group of K transactions at a time Then SWA sanitizes the set of sensitive transactions denoted by R R  considering a disclosure threshold 001 For each restrictive association rule there is a disclosure threshold assigned to it We refer to the set of mappings of a restrictive association rule into its corresponding disclosure threshold as the set of ining permissions denoted by M P  in which each mining permission mp is characterized by an ordered pair de\036ned as mp  rr i 001 i  where 003 i rr i 004 R R and 001 i 004 0  1  The sketch of SWA and the proof of its runtime complexity are given in  Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


4 Experimental Results We compare SWA with respect to the following benchmarks 1 the result of Apriori algorithm without transformation 2 the results of similar algorithms in the literature We compare the effectiveness and scalability of SWA with a similar one proposed in t o hi de rul e s b y reducing support called Algo2a The lgorithm GIH designed by Saygin et al 6 i s s i m i l a r t o A l go2a The bas i c di f ference is that in Algo2a some items are removed from sensitive transactions while in GIH a mark 223?\224 unknowns is placed instead of item deletions We also compare SWA with the Item Grouping Algorithm IGA our best algorithm so far published and presented in 3 We performed two series of experiments the 036rst to measure the effectiveness of SWA IGA and Algo2a and the second to measure the ef\036ciency and scalability of these algorithms All the experiments were conducted on a PC AMD Athlon 1900/1600 SPEC CFP2000 588 with 1.2 GB of RAM running a Linux operating system To measure the effectiveness of the algorithms we used a dataset generated by the IBM synthetic data generator to generate a dataset containing 500 different items with 100K transactions in which the average s ize per transaction is 40 items The effectiveness is measured in terms of the number of restrictive association rules effectively hidden as well as the proportion of legitimate rules accidentally hidden due to the sanitization We selected a set of ten restrictive association rules from the dataset ranging from two to 036ve items in length with support ranging from 20 to 42 and con\036dence ranging from 80 to 99 in the database With our ten original restrictive association rules 94701 rules became restricted in the database since any association rule that contains restrictive rules should also be restricted 4.1 Measuring effectiveness We measure the effectiveness Algo2a taking into account the performance measures introduced in 3 W e s u mmarize such performance measures as follows 1 Hiding Failure HF measures the amount of restrictive association rules that are disclosed after sanitization 2 Misses Cost MC measures the amount of legitimate association rules that are hidden by accident after sanitization 3 Artifactual Patterns AP measure the arti\036cial association rules created by the addition of noise in the data and 4 Dif\(D D\325 difference between the original an d sanitized databases i.e information loss We evaluated the effect of window size with respect to the difference of the original database D and the sanitized one D 001  Todoso,wevaried K from 500 to 10000 transactions with the disclosure threshold 001  15  Figure 1A shows that up to 3000 transactions the difference between the original and the sanitized database improves slightly After 3000 transactions the difference remains the same Similarly Figure 1B shows that after 3000 transactions the values of misses cost MC and hiding failure HF tend to be constant his shows that on our example database a window size representing 3 of the size of the database suf\036ces to stabilize the misses cost and hiding failure The distribution of the data may affect these values However we have observed that the larger the window size the better the results The reason is that when the heuristic is applied to a large number of transactions the impact in the database is minimized Consequently the value of misses cost and the difference between D and D 001 improve slightly  2.8 2.85 2.9 2.95 3 0 2000 4000 6000 8000 10000 Difference between D and D\222 in Window Size 0.5 1 1.5 2 2.5 3 3.5 4 0 2000 4000 6000 8000 10000 Hiding Failure Window Size   13 13.2 13.4 13.6 13.8 14 14.2 14.4 0 2000 4000 6000 8000 10000 Misses Cost A B Figure 1 Effect of window size on A dif\(D,D\222 B MC and HF To measure the misses cost we set the the disclosure threshold 001 to 0 This means no restrictive rule is allowed to be mined from the sanitized database In this situation 18.30 of the legitimate association rules in the case of SWA 20.08 in the case of IGA and 24.76 in the case of Algo2a are accidentally hidden We intentionally selected restrictive association rules with high support in the reported experiments to accentuate the differential between the sizes of the original database and the sanitized database and thus to better illustrate the impact of the sanitization on the mining process SWA and IGA are the ones that impact the least on the database In this particular case 3.55 of the database is lost in the case of SWA and IGA and 5.24 in the case of Algo2a Figure 2 shows the effect of the disclosure threshold 001 on the hiding failure and e misses cost for SWA and IGA considering the minimum support threshold 002 5 Since Algo2a doesn\222t allow the input of a disclosure threshold it is not compared in this 036gure with our algorithms As can be observed when 001 is 0 no restrictive association e is disclosed for both algorithms However 20.08 of the legitimate association rules in the case of IGA and 18.30 in the case of SWA are accidentally hidden What can also be observed is that the impact of SWA on the database is smaller and the misses cost of SWA is slightly better than that of IGA Moreover he hiding failure for SWA is slightly better than that for IGA in all the cases except at 001  50  Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 0 20 40 60 80 100 0 20 40 60 80 100 Hiding Failure Disclosure Threshold    SWA  IGA  0 5 10 15 20 25 0 20 40 60 80 100 Misses Cost Disclosure Threshold    SWA  IGA  Figure 2 Effect of 001 on HF and MC 4.2 CPU Time for the Sanitization Process We tested the scalability of our sanitization algorithms vis a-vis the size of the database as well as the number of rules to hide Our comparison s tudy also includes the algorithm Algo2a We varied the size of the original database D from 20K transactions to 100K transactions while 036xing the disclosure threshold 001 0 and e support threshold 002 5  and keeping the set of restrictive rules constant 10 original patterns We set the window size for SWA with K  20000 Figure 3A s that IGA and SWA increase CPU time linearly with the size of the database while the CPU time in Algo2a grows fast This is due the fact that Algo2a requires various scans ove r the original database while IGA requires two and SWA requires only one Although IGA requires 2 scans it is faster than SWA The main reason is that IGA clusters restrictive association rules in groups of rules sharing the same itemsets Then by removing the victim item from the sensitive transactions related to the rules in the group all sensitive rules in the group would be hidden in one step As can be observed SWA increases CPU linearly even though its complexity in main memory is not linear 0 20 40 60 80 100 120 0 20 40 60 80 100 CPU Time Database Size  in thousands  SWA  IGA   Algo2a  0 10 20 30 40 50 60 2 3 4 5 6 7 8 9 10 CPU Time Set of Restrictive Rules SWA  IGA   Algo2a  A B Figure 3 Results of CPU time We also varied the number of restrictive rules to hide from approximately 6000 to 29500 while 036xing the size of the database to 100K transactions and 036xing the support and disclosure thresholds to 001  0 Figure 3B shows that our algorithms scale well with the number of rules to hide We varied the size of the original set of restricted rules from 2 to 10 This makes the set of all restricted rules range from approximately 6097 to 29558 This scalability is mainly due to the inverted 036les we use in our approaches for indexing the sensitive transaction IDs per restrictive rules There is no need to scan the database again whenever we want to access a transaction for sanitization purposes The inverted 036le s direct access with pointers to the relevant transactions The CPU e for Algo2a is more expensive due the number of scans over the database 5 Conclusions In this paper we have introduced an ef\036cient algorithm that improves the balance between protection of sensitive knowledge and pattern discovery called Sliding Window Algorithm SWA This algorithm is useful for sanitizing large transactional databases based on a disclosure threshold or a set of thresholds controlled by a database owner The experimental results revealed that SWA is effective and can achieve signi\036cant i mprovement over the other approaches presented in the literature SWA slightly alters the data while enabling 037exibility for someone to tune it A strong point of SWA is that it does not introduce lse drops to the data In addition SWA has the lowest misses cost among the known sanitizing algorithms It is important to note that our sanitization method is robust in the sense that there is no de-sanitization possible Moreover there is no encryption involved There is no possible way to reproduce the original database from the sanitized one References  M Atallah E  Bertino A E l magarmid M Ibrahim and V Verykios Disclosure Limitation of Sensitive Rules In Proc of IEEE Knowledge and Data Engineering Workshop  pages 45\22652 Chicago Illinois November 1999  E  Dasseni V  S  V erykios A K E lmagarmid and E  Bertino Hiding Association Rules by Using Con\036dence and Support In Proc of the 4th Information Hiding Workshop  pages 369\226 383 Pittsburg PA April 2001 3 S  R M O l i v e i r a a n d O  R  Z a 250 031ane Privacy Preserving Frequent Itemset Mining In Proc of the IEEE ICDM Workshop on Privacy Security and Data Mining  pages 43\22654 Maebashi City Japan December 2002 4 S R M  O l i v e i r a a n d O  R  Z a 250 031ane Algorithms for Balancing Privacy and nowledge Discovery in Association Rule Mining In Proc of the 7th International Database Engineerg and Applications Symposium 32503  Hong Kong China July 2003 5 S  R  M  O li v e ira an d O  R  Z a 250 031ane An Ef\036cient One-Scan Sanitization For Improving The Balance Between Privacy And Knowledge Discovery Technical report TR03-15 Computer Science Department University of Alberta Canada June 2003 6 Y  S aygi n V  S  V e r yki os and C  C l i f t on U s i n g U nkno w n s to Prevent Discovery of Association Rules SIGMOD Record  30\(4\:45\22654 December 2001 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 The algorithm returns to Step 2   Step 5  The miner sends the results to the participants  Although this algorithm is somewhat similar to the VDC algorithm in [23 it i s d i f f er en t in t w o r es p ect s    First, the calculator doesn\222t know which itemset is being checked, and the  exact support value is not disclosed. Furthermore, only one level of encryption is used. \(only transaction\226ids are encrypted and not the itemsets  The privacy analysis  The participants and the "miner" learn nothing but the results as in  secure computation The "calculator" learns the support of the itemsets but without the possibility to know which itemset is being tested No probing attack is possible, since the miner does not hold any part of the database However, since the results are known to each participant, there is a probability of exposure in case the local support is exactly the support threshold as discussed in detail in  T h e r e   t h e e a pp r o xi m a t i o n method was suggested and it can be used here as well Note that the problem of inferring knowledge from the results of data mining is independent of the algorithm used and exists also in all secure computation based methods  The computation and communication cost  In VDC  two sided algorithm, for every itemset frequency computation it was  necessary to perform secure scalar product computation which takes for every tested set at least three times sending the N values between two sides. In our algorithm in the worst case every party sends once N values to the calculator, which is much less communication overhead. In the n-party case we have the same communication as in VDC but our algorithm is simpler, since we use one level of encryption only Comparing to GR we save all the preprocessing stage of sending the database with faked transactions   3.2 Second algorithm  for horizontally partitioned DB   The model  We use the same model but for horizontally partitioned database and  with the addition of one more "calculator". The modified architecture is depicted in Fig. 2. We also add the assumption  that the database is binary. The goal and privacy definition stay as in the first algorithm We are going to use the following lemma which was already proven in [16  Lemma : If the database is horizontally partitioned and if an itemset has support > p% globally, it must have support > p% in at least one of the individual parties databases  3.2.1  The algorithm\( first version without using  the lemma   The first version is not realistic since it requires exponential space, but its useful for explaining the concept  Step 1 The "miner" sends to every participant two instructions  to create two databases each containing for each itemset the frequency of its occurrence Then each   participants   add to   each such frequency a random noise sent by the miner one  database  with  positive noise,  and  the other with negative noise  to send the first database to "calculator1 and  to  send  the  second    database   to calculator2 After execution of this step "calculator1" and calculator2" both have the whole information about the number of appearances of each itemset but with noise  Step 2 The "miner" performs apriori  algorithm  against the calculators. When the miner needs to compute support for some itemset, he sends the request to both calculators and some random number to one of them The first calculator, who gets the random number adds to it the value which appears in his database about this itemset and sends the result to the second calculator. The second  adds his value and sends the result back to the miner. The miner divides the result by  \(2 * \(size of the database\nd gets the real support Privacy analysis  The parties \226 learn nothing but the results The calculators \226 learn nothing; because of the random number they don\222t know the real support The miner \226 learns global support of the itemsets but nothing on the local databases  Communication overhead  Much less than in the first algorithm since the parties communicates only once with the calculators, although in this one time they send a large database Let us see a small example 


 Suppose we have three parties :d1, d2, d3 ; the "miner and two calculators. Let us take a look on some itemset ABC, see Figure 2 The Miner sends the random noises: 3,2,1 to the three parties respectively   Figure 2:  The second algorithm  Next, all the parties add the noise to the number of appearances of every set and send the results to both calculators, in this case for ABC: the first party sends 2+3 = 5 and 2-3 = -1, the second 2+2 = 4 and 2-2=0 the third 2+1=3 and 2-1=1 Next \(see Figure 3\ng the Apriori algorithm to compute support of some itemset, the Miner selects some random number R, sends R to first calculator which adds his support about the itemset and sends R1 to the  second calculator, which  computes R2 and sends it to the Miner. The Miner computes R2 \226 R\\(2*3\ and this is the support of itemset ABC    Figure 3: Second algorithm \226 computing the support  3.2.2 Second version \(using the lemma  Basically the same algorithm, except for the following  changes 1 Initially, the miner chooses an encryption key to encrypt each item-set and send it to all participants 2 The participants, before sending their two databases to the calculators encrypt the corresponding itemsets 3 the participants compute the frequent itemsets locally and send results to the miner The Miner does the Apriori only for itemsets of which one of the local parties has found frequent! \(i.e. using the Lemma For those   itemsets, it sends the calculators their encrypted for to enable them to retrieve their respective frequency  Privacy analysis  Same as the first algorithm  except  that the Miner now knows frequency of itemset at local sites  The purpose of the encryption is to prevent the calculators from knowing that a particular itemset is frequent at some local database  To prevent the Miner from knowing the local site frequency, we can add an extra Mediator site. The mediator will not know the itemset which is currently being tested, but will get the frequency counts from the various sites and just send the Miner a flag whether there is a site in which the frequency of the itemset is above the threshold. This increases the privacy but introduces additional communication overhead The computation and communication overhead are less than in the previous  algorithm because of the use of the Lemma, but obviously, this algorithm also requires exponential space for each of the local databases, therefore we need the third  version   3.2.3 Third version \(using Apriori and the lemma  This algorithm is a mix of the first algorithm \(3.1 and the last one The Miner performs the Apriori algorithm, and at each step asks the participants to send the two frequency results with the noise to the calculators. Then the Miner asks for the support from the calculators as before. This algorithm has the same security properties as the previous two, however, no encryption is necessary, since as in the first algorithm, the Claculators don\222t know which itemset is being computed The  communication overhead of this version  is higher than in the first two versions and is similar to the overhead in the first algorithm. The main advantage of-course is that the local parties don\222t need to create and send large \(exponential\  databases  3.3 Third algorithm General DB   The mode l The same as in the first model, one miner, one calculator and N participants. Also, all the definitions stay as in the first algorithm,  except the for goal  The goal  To perform any data mining algorithm without compromising privacy  The algorithm  The mine r   Calculato r 1 Calculato r 2 A B C  1 | 1  1 1 2 | 1  1 1 3 |0  0  1 A B C  4 | 1  1 1 5 | 1  1 1 6 |0  0  1 A B C  7 | 1  1 1 8 | 1  1 1 9 | 0  0 1 The mine r  Calculato r 1 Calculato r 2 


 Step 1 The "miner" tells to the participants to add N versions of  specific pseudorandom  noise to their data and to send it to the "calculator". That it, the calculator gets from each participant its database N times.\(with a different noise  Step 2 The miner sends to the calculator some "seed" to create the pseudo-random noise. This seed will create a noise which is identical to one of the N noises, but the calculator doesn\222t know which one it is  Step 3 For every created noise the calculator subtract the noise from all N databases and perform the mining and sends the results back to the miner  Step 4 The miner picks up the right results  Privacy analysis  The parties \226 learn nothing but the results The miner \226 learns global data mining results but nothing on the local databases The calculator \226 The Calculator has one real database out of the N databases, so he has the probability of 1/N to learn everything! Obviously we can reduce this probability by adding some more artificial partcipants but each such participant increases the computation overhead  Computation and Communication overhead  The computation overhead is much higher, it is essentially multiplied by N. The communication overhead is N*overhead of Algorithm 2, but this is a one-shot overhead, and the communication overhead between the Miner and the Calculator is greatly reduced \(one message only  4. Conclusions   We discussed the problem of privacy-preserving data mining in distributed databases. We suggested a new architecture \(paradigm\ which is based on using two separate entities: a Miner and a Calculator, both not having parts of the database. We presented three algorithms which are based on this paradigm, one for Vertically partitioned data, one for Horizontally partitioned data, and one for any data mining method In contrast to the previously known methods of Perturbation or Secure computation, our algorithms produce accurate results \(better than perturbation\, but usually with much less computation and communication overhead than secure computation In the future we plan to investigate these algorithms and evaluate their performance on real-life databases   Acknowledgment   Thanks to the reviewers for their useful comments    References  1 R  A g r a w a l a nd R   Sr i k a n t  P r i v ac y pr es e r v i ng da t a  mining. In Proc. of the ACM SIGMOD\22200, pages 439\226450 Dallas, Texas, USA, May 2000  2 R  A g r a w a l and R  Sr i k a n t  Fa s t A l g o r i t h m s f o r M i ni ng  Association Rule  Proceedings . of the 20th Int'l Conf. on VLDB}, Santiago, Chile, 1994  3 P C h an A n E x t e ns i b l e M e t a L ear n i ng A ppr o a c h  f o r  Scalable and Accurate Inductive Learning. PhD thesis Department of Computer Science, Columbia University, New York, NY, 1996. \(Technical Report CUCS044-96  4  W  L  C h e u n g  V  N g  A  W  C  F u a n d Y  F u  Efficient mining of association rules in distributed databases IEEE Transactions on Knowledge and Data Engineering 8\(6\:911-922, Dec. 1996  5  I   D in u r a n d K  N is s im R e v e a l in g in f o rm a tio n w h i le  preserving privacy. In Proc. of PODS\22203, pages 202 \226 210 June 2003  6 W  D u and M  J  A t a l l a h Sec u r e m u l t i par t y co m put at i o n  problems and their applications: A review and open problems. In Proceedings of the 2001 New Security Paradigms Workshop, Cloudcroft, New Mexico, Sept. 11-13 2001  7 C  D w o r k a nd K  N i s s i m  P r i v ac y pr es er v i ng dat a m i ni ng  on vertically partitioned databases. In Proc. of CRYPTO\22204 August 2004  8  E v fi mi e v s k i  J  Ge h r k e   a n d R  S r i k a n t L i mit in g  privacy breaches in privacy preserving data mining. In Proc of PODS\22203, pages 211\226222, San Diego, California, USA June 9-12 2003  9 A  E v fi mi e v s k i R  S r i k a n t  R  A g r a w a l  a n d  J  G e h r k e  Privacy preserving mining of association rules. In Proc. of ACM SIGKDD\22202, pages 2 17\226228, Canada, July 2002   B  G i l bu r d A  Sc hus t e r  and R  W o l f f  kt t p a new  privacy model for large-scale distributed environments. In Proc. of ACM SIGKDD\22204, pages 563\226568, 2004   O  G o l dr e i c h S  M i c a l i  and A  W i g der s o n H o w t o play any mental game \226 a completeness theorem for 


 protocols with honest majority. In 19th ACM Symposium on the Theory of Computing, pages 218-229, 1987   E  G u d e s  B  R o z e nber g  C o l l abo r a t i v e P r i v acy  Preserving Frequent Item Set Mining in Vertically Partitioned Databases. Proceedings of DBSec 2003: 91-104   E  G ude s  B  R o z e nbe r g  A n al y s i s o f T w o  A ppr o a c h es  for Association Rules Mining in Vertically Partitioned Databases , Proceedings of ICDM Workshop on Privacy and Security Aspects of Data Mining, Brighton, UK, November 2004   Z  H u ang  W  D u  an d B  C h en D e r i v i ng pr i v a t e  information from randomized data. In Proc. of ACM SIGMOD\22205, 2005   M  K a nt a r c i o g l u and C  C l i f t o n Pr i v ac y pr es e r v i ng  distributed mining of association rules on horizontally partitioned data. In The ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery DMKD'02\, June  2002  16 M  K a n t a r c i o g l u  C  C lif to n  P r iv a c y P re s e rv in g  Distributed Mining of Association Rules on Horizontally Partitioned Data. IEEE Trans. Knowl. Data Eng. 16\(9 1026-1037,  2004   H  K a r g upt a S  D a t t a  Q  W a ng  an d K   Si v a kum ar   On the privacy preserving properties of random data perturbation techniques. In Proc. of ICDM\22203, page 99 Washington, DC, USA, 2003. IEEE Computer Society   Y  L i nd el l  and B  P i nk as  P r i v acy pr es e r v i ng da t a  mining. In Proc. of CRYPTO\22200, pages 36\22654. SpringerVerlag, 2000  19 A  P r od r om i d i s  P  C h an  a n d S St ol f o M e t a l e a r n i n g in distributed data mining  systems: Issues and approaches chapter 3. AAAI/MIT Press, 2000   S J  R i z v i and J  R  H a r i t s a  P r i v ac y pr es e r v i ng  association rule mining. In  Proceedings of 28 th International Conference on Very Large Data Bases. VLDB, Aug. 20-23 2002    B.Rozenberg , MSc thesis, Ben-Gurion University 2003  22 J  V a id y a C  C lif to n  S e c u re  s e t i n t e r s e c tio n c a r d i n a li ty  with application to association rule mining. Journal of Computer Security 13\(4\: 593-622 \(2005  23 J  V a id y a C  C lif to n  P r iv a c y P r e s e r v in g A s s o c ia tio n  Rule Mining in Vertically Partitioned Data. In Proceedings of SIGKDD 2002, Edmonton, Alberta, Canada, 2002  24 A  C  Y a o H o w  t o ge n e r a t e  a n d e x ch an ge s e cr e t s   In Proceedings of the 27th IEEE Symposium on Foundations of Computer Science pages 162-167,  1986           


Number of Processors Time \(seconds 0 20000 40000 60000 80000 1 2 4 8 Figure 9 Average execution time per node to nd frequent 3-itemsets 2-itemsets processed by each node in our four different system conﬁgurations Note that the number of candidate 2itemsets for the 1-node case is approximately the same as the average number of candidate 2-itemsets for the 2-node case This result is consistent with the observed total and average execution times for the 1-node and 2-node cases There is signiﬁcant reduction in the average number of candidate 2-itemsets processed for the 4-node and 8-node cases over the 1-node and 2-node cases This result represents the nonuniform distribution of itemsets over the local databases as well as the effective reduction of the candidate itemsets by the Inverted Hashing and Pruning technique Number of Candidate 2-itemsets 0 100000 200000 300000 400000 MIHP 2-node PM IHP 4-node PM IHP 8-node PM IHP Figure 10 Average number of candidate 2itemsets per node Figure 11 shows the average number of candidate 3itemsets processed by each node We included the number of candidate 3-itemsets processed in Apriori to demonstrate the usefulness of the Inverted Hashing and Pruning The number of candidate 2-itemsets for Apriori was about 82 million which is why we did not show that in Figure 10 We can observe the same pattern of reduction in the candidates 3-itemsets as in the candidate 2-itemsets This reduction in the average number of candidate itemsets processed by each processing node may be the most clear explanation for the high increasing rate of the speedup observed as the number of processing nodes increases Number of Candidate 3-itemsets 0 200000 400000 600000 800000 Apriori MIHP 2-node PM IHP 4-node PM IHP 8-node P M IHP Figure 11 Average number of candidate 3itemsets per node We also ran a test with a larger database 8 weeks of the Wall Street Journal published from January 2 1991 through February 22 1991 February 23rd was a Wall Street Journal holiday There were 6,170 documents and 64,191 unique words of which 31,948 were frequent words at the minimum support level of 0.03 i.e 2 out of 6,170 documents The 1-node system required 845,702 seconds to nd 1,554,442 frequent 2-itemsets whereas the 8-node system required 33,183 seconds This performance represents a superliner speedup of 25.5 of the 8-node system over the 1-node system Thus we can conclude that the performance of PMIHP is quite scalable when the database is large and the minimum support level is low which is the case of high workload The 1-node case generated 16,174,357 candidate 2itemsets whereas the 8-node case generated 2,459,629 candidate 2-itemsets per node on the average The total number of candidate 2-itemsets counted by the 8 nodes were 19,677,031 which means that only 21.7 of the candidate 2-itemsets were counted at more than one processing node This implies that the distribution of words across the 8-week sample of the Wall Street Journal is quite skewed In the Count Distribution algorithm all the nodes count the same set of candidate itemsets in each pass over the database regardless of the distribution of items over the local databases On the other hand in our PMIHP algorithm not all candidate itemsets are counted at more than one node when the distribution of items over the local databases is not uniform Obviously the more skewed the data distribution the better the performance of PMIHP Cheung et al 4 propos ed s e v e ral approaches to partition the databas e 0-7695-2132-0/04/$17.00 \(C Proceedings of the 18th International Parallel and Distributed Processing Symposium \(IPDPSê04 


to achieve a high degree of skewness Text documents arranged in a chronological order do appear to have a high degree of skewness and beneﬁt the PMIHP algorithm 4 Conclusions The proposed Parallel Multipass with Inverted Hashing and Pruning PMIHP algorithm is a parallel version of our Multipass with Inverted Hashing and Pruning MIHP algorithm and it is effective for mining frequent itemsets in large text databases The Multipass approach reduces the required memory space at each processor by partitioning the frequent items and pro cessing each partition separately Thus the number of candidate itemsets to be processed is limited at each instance The Inverted Hashing and Pruning is used to prune the local and global candidate itemsets at each processing node and it also allows each processing node to determine the other peer processing nodes to poll in order to collect the local support counts of each global candidate itemset PMIHP distributes the workload to multiple processing nodes to reduce the total mining time without incurring much parallelization overhead The average number of candidate itemsets to be counted at each processing node is much smaller than the case of sequential mining while the time for the synchronization between processing nodes to exchange the count information for the global candidate itemsets is very small compared to the total execution time PMIHP is able to exploit the natural skewed distribution of words in text databases and demonstrates a superlinear speedup as the number of processing nodes increases It has a much better performance than well-known parallel Count Distribution algorithm 2 becaus e the a v erage number of candidate itemsets to be counted at each processing node is much smaller especially when the minimum support level is low Overall the performance of PMIHP is quite scalable even when the size of the text database is large and the minimum support level is low which is the case of high workload References  R  A gra w al and R  S r i kant   Fast Al gori t h ms for M i n i n g A ssociation Rules Proc of the 20th VLDB Conf  1994 pp 487–499  R Agra w a l and J C S hafer  Paral l e l M i n i n g o f A ssoci at i o n Rules IEEE Trans on Knowledge and Data Engineering  Vol 8 No 6 1996 pp 962–969 3 M  S  C hen J Han and P  S  Y u   Dat a Mi ni ng An Overview from a Database Perspective IEEE Trans on Knowledge and Data Engineering  Vol 8 No 6 1996 pp 866–883 4 D  W  C heung S  D L ee and Y  Xi ao  E f f ect of Dat a S k e w ness and Workload Balance in Parallel Data Mining IEEE Trans on Knowledge and Data Engineering  Vol 14 No 3 2002 pp 498–514 5 S  M  C hung and J Y ang  A Par al l e l D i s t r i b ut i v e J oi n A l gorithm for Cube-Connected Multiprocessors IEEE Trans on Parallel and Distributed Systems  Vol 7 No 2 1996 pp 127–137  R  F e l dman and H Hi rsh F i ndi ng Associ at i ons i n Col l ections of Text Machine Learning and Data Mining Methods and Applications  R Michalski I Bratko and M Kubat editors John Wiley and Sons 1998 pp 223–240  R F e l dman I Dagen and H  H i rsh Mi ni ng T e xt Usi n g Keyword Distributions Journal of Intelligent Information Systems  Vol 10 No 3 1998 pp 281–300  C  Fox L e x i cal Anal ysi s and S t opl i s t s   Inforamtion Retrieval Data Structures and Algorithms W.FrakesandR Baeza-Yates editors Prentice Hall 1992 pp 102–130 9 M  G or don and S  Dumai s Usi ng L a t e nt S e mant i c I nde xi ng for Literature Based Discovery Journal of the Amer Soc of Info Science  Vol 49 No 8 1998 pp 674–685  J Han J P e i  and Y  Y i n Mi n i n g F r equent Pat t e r n s w i t hout Candidate Generation Proc of ACM SIGMOD Intêl Conf on Management of Data  2000 pp 1–12  J D Holt and S  M Chung Multipass Algorithms for Mining Association Rules in Text Databases Knowledge and Information Systems  Vol 3 No 2 Springer-Verlag 2001 pp 168–183  J D Hol t and S  M C hung Mi ni ng Associ at i o n R ul es Using Inverted Hashing and Pruning Information Processing Letters  Vol 83 No 4 Elsevier Science 2002 pp 211–220  J D Hol t and S  M C hung Mi ni ng associ at i o n R ul es i n Text Databases Using Multipass with Inverted Hashing and Pruning Proc of the 14th IEEE Intêl Conf on Tools with Artiìcial Intelligence  2002 pp 49–56  J S  Park M S  C hen and P  S  Y u   Usi n g a Hash-B a sed Method with Transaction Trimming for Mining Association Rules IEEE Trans on Knowledge and Data Engineering  Vol 9 No 5 1997 pp 813–825  G  S a l t on Automatic Text Processing The Transformation Analysis and Retrieval of Information by Computer  Addison-Wesley Publishing 1988  E  M V oorhees and D K Harmon edi t o rs The Fifth Text Retrieval Conference  National Institute of Standards and Technology 1997  O R  Z a i a ne and M L  Ant o i ne C l assi f y i n g T e x t D ocuments by Associating Terms with Text Categories Proc of the 13th Australian Database Conf  2002 0-7695-2132-0/04/$17.00 \(C Proceedings of the 18th International Parallel and Distributed Processing Symposium \(IPDPSê04 


  11 could be improved by a simple modification of the feed by adding a small tuning vane to th e feed. Therefore, it can be stated that some improvement can be expected by modification of the feeds, and adaptation of the test antenna in such a way that surrounding Ku-band element are closed   Figure 28 Reflection coefficient of Ku-band stacked patch antenna element in dual-frequency antenna stack  Figure 29 shows the influence of the L-band slots on the return loss of the Ku-band antenna element. To this end, the four connectors of the L-band elements were alternately open and terminated by means of 50 loads. The deviations were measured with respect to the set-up where all connectors were terminated Apparently, the deviations are acceptable  Figure 29 Influence of L-band termination on return loss of Ku-band antenna element, with and without termination Figure 30 and Figure 31 show the isolation between the Lband and Ku-band elements in L-band and Ku-band respectively. To this end the S21-parameters have been measured. These figures reveal that the mutual coupling between the L-band and Ku-band elements is sufficiently small  Figure 30 Measured isolation between L-band and Ku-band antennas in L-band frequencies  Figure 31 Measured isolation between L-band and Ku-band antennas in Ku-band frequencies From these measurements it can be concluded that opportunities should be considered to improve the design of the dual-frequency antenna \(by optimizing the feeds of the Ku-band elements antenna and the measurement set-up \(closure of surrounding Ku-band ports and use of appropriate connectors for the open Ku-band ports 7  M ODIFIED DUAL FREQUENCY ANTENNA  In order to benefit the str ong points of the two separate designs as discussed in section 4, an alternative antenna is proposed that exploits the properties of a \221best of both worlds\222 solution employing ideas from both designs. The modified antenna possesses an aperture fed L-band patch of a similar form to first design, but situated towards the bottom of the stack. Ku band el ements are located within the L-band perforations and para sitic patches are situated above a foam spacer \(see Figure 32 and Figure 33\A measurement campaign is underway to assess the behaviour of this modified test antenna 


  12  Figure 32 Bottom view of dual frequency antenna tile with perforated L-band patc h in lower layer with Kuband patches  Figure 33 Layer stack with perforated L-band patch in lower layer with Ku-band patches  8  B EAM FORMING N ETWORK  A major keystone for the su ccess of phased array antenna onboard aircraft is the capability of steering the main beam in the direction of the geosta tionary satellites. This requires the inclusion of a broadband beam forming network. Beam steering can be realized by adding RF-phase shifters and LNA\222s to the antenna elements of the array. However traditional phase shifters in ge neral have a narrow band, and hence do not yield the re quired broadband capability Alternative technologies for broadband beam forming are switched beam networks \(using Butler matrices innovative designs for RF-compone nts such as phase shifter LNA components in \(M\IC technology, or beam forming by using opti cal ring resonators  The German SME IMST is involved in several projects for development of electronica lly steerable phased array antennas for satellite communication. In the NATALIA project \(New Automotive Track ing Antenna for Low-cost Innovative Applications\ ESA, IMST is investigating the possibility of realizing a compact costeffective solution for a recei ve-only full electronically steerable antenna for cars in Ku-band. This antenna is a planar array composed of approximately 150 patches circularly polarised by using a 90\260 hybrid, and arranged in a hexagonal fashion. Each patc h is equipped with a MMIC corechip containing a phase sh ifting unit, LNA and digital steering logic  In the Netherlands, a consortiu m \(consisting of University of Twente, Lionix BV, National Aerospace Laboratory NLR and Cyner Substrates developing in the national FlySmart project technology for a broadband optical beam forming network. For the steering of the beam of the conformal phased array a squi nt-free, continuously tunable mechanism is proposed that is based on a fully integrated optical beam forming network \(OBFN optical ring resonators \(ORRs as tunable delay elements. A narrowband continuously tunabl e optical TTD device is realized as a recirculating wa veguide coupled to a straight waveguide. This straight wave guide can behave as a bandpass filter with a periodic, bell-shaped tunable group delay response. The maximum group delay occurs at a tunable resonance frequency. A larger delay-bandwidth product can be achieved by cascading multiple ORR sections. A complete OBFN can be obtaine d by grouping several delays and combining elements in one optical circuit. Such an OBFN can be realized on a si ngle-chip. Electrical/Optical E/O O E by means of filter based single-sideband modulation suppressing the carrier lanced coherent optical detection. Further details of the optical beamforming network have been presented in Re The proof-ofconcept has been shown by manufacturing a chip for an 8x1 OBFN. Essential components of the OBFN are the optical modulators, which are used to modulate the light in the ORR system 9  C ONCLUSIONS  For enhanced communicati on on board aircraft, novel antenna systems with broa dband satellite-based capabilities are required. So far, existi ng L-band satellite based systems for communications are used primarily for passenger application \(APC\i nistrative communications AAC and now data are tending to evolve towards broadband dig ital applications \(Voice over IP\any studies are going on worldwide to employ Kuband TV geostationary sate llites for communication with mobile terminals on aircraft The inbound traffic is about 5 times higher than the outbound The inbound traffic requires the availability of a broadband Ku-band antenna in receive mode only. The outbound traffic services can be supplied by the Inmarsat SBB link, whic h requires the installation of an L-band transmit antenna. In order to avoid both the installation of L-band antenna and Ku-band antenna, the concept of a hybrid dual frequency antenna operating L 


  13 band and Ku-band with low aerodynamic profile has been investigated in this paper. Keyaspects of this research are 200  Design and testing dual-fre quency antenna elements operating in both L-band and Ku-band 200  Conformal aspects of Ku-band phased array antennas 200  Beam forming algorithms for planar and conformal phased array antennas Two designs for dual-frequency antenna tiles consisting of 8x8 Ku-band antenna elements and one L-band element The designs have been analysed by means of computer simulations. Both designs show promising performance both in L-band and Ku-band. The design with slotted Lband antenna has a resonant fre quency in receive mode with a bandwidth of about 1 GHz. The Ku-band antenna is a stacked patch configuration where a parasitic element is placed above a lower patch separated by dedicated space filler. The manufactured protot ype antennas indicate that the bandwidth is sufficiently large In order to be able to communicate with geostationary satellites also at high latitudes e.g. during inter-continental flights\stem should have sufficient performance at low elevation angles. The antenna Ku-band system is required to have a small beamwidth \(to discriminate between the satellite signals\gain 30 dB angles. The effects of these requirements on the size and positioning of the antenna on the aircraft fuselage have been investigated. These requirements can be best satisfi ed by installing two planar phased array antennas on both side s of the fuselage with at least 1600 Ku-band elements. Each element has two feed lines, one for each polarization Every feed line has to be connected to the beam formi ng network. This means that the connections cannot be routed to one of the four sides of the antenna. Instead the concept of vertical feed lines \(by means of vias in a sufficiently thick substrate recommended. These vertical f eed lines connect the L-band and Ku-band antenna elements in the upper layer with feed networks in multiple lower laye rs. This vertical feed line system was not available so far due to manufacturing problems The performances of one dua l-frequency antenna design have been investigated by manufacturing two test antennas without vertical feed line syst em. The first antenna contains only a multilayer structure with L-band slots and 8x8 Kuband stacked patches. The performances of the L-band slots and Ku-band stacked patches c ould be measured separately It was concluded that opportun ities should be considered to improve the design of the dual-frequency antenna \(by optimizing the feeds of the Ku-b and elements the dual frequency test-antenna and the measurement set-up More important, however, is the realization of a mechanically stable vertical feed line system, so that the properties of L-band and Ku-band elements can be measured adequately The second test antenna contains only a multilayer structure with 8x8 Ku-band stacked patches and a feed network with 8 combiners, where each comb iner coherently sums 8 antenna elements. In combination with a prototype 8x1 OBFN, a Ku-band phased arra y antenna is obtained of which the beam can be steered in one direction. This second test antenna is used to analyze the broadband properties of the 8x8 Ku-band antenna array and 8x1 OBFN. The measured performances of this antenna are presented in Ref   A CKNOWLEDGMENT  This work was part of the EU 6 th Framework project ANASTASIA., and the FlySmart project, supported by the Dutch Ministry of Economic A ffairs, SenterNovem project numbers ISO53030 The FlySmart project is part of the Eureka PIDEA  project SMART Cyner Substrates is acknowle dged for technical assistance during the fabrication of the prototype antennas 


  14 R EFERENCES  1  P. Jorna, H. Schippers, J. Verpoorte, \223Beam Synthesis for Conformal Array Antennas with Efficient Tapering\224 Proceedings of 5 th European Workshop on Conformal Antennas, Bristol, September 11-12, 2007 2  The Radio Regulations, editi on of 2004, contain the complete texts of the Radio Regulations as adopted by the World Radio-communication Conference \(Geneva WRC-95 tly revised and adopted by the World Radio-communication Conference WRC-97\RadioWRC2000\and the World Radio-communication Conference WRC-03 Resolutions, Recommendations and ITU-R Recommendations incorporat ed by reference 3  RECOMMENDATION ITU-R M.1643, Technical and operational requirements for ai rcraft earth stations of aeronautical mobile-satellite service including those using fixed satellite service network transponders in the band 14-14.5 GHz \(Earth-to-space 4  ETSI EN 302 186 v1.1.1 \(2004-01 Stations and Systems \(SES\onised European Norms for satellite mobile Aircraft Earth Stations AESs\the 11 12/14 GHz frequency bands covering essential requirement s under article 3.2 of the R&TTE directive 5  EUROCAE ED-14E; Environmental Conditions and Test procedures for Airbor ne Equipment, March 2005 6  F. Croq and D. M. Pozar, \223Millimeter wave design of wide-band aperture-coupled stacked microstrip antennas,\224 IEEE Trans. Antennas Propagation, vol. 39 pp. 1770\2261776, Dec. 1991 7  S. D. Targonski, R. B. Waterhouse, D. M. Pozar Design of wide-band aperture stacked patch microstrip antennas ", IEEE Transactions on Antennas and Propagation, vol. 46, no. 9, Sep. 1998, pp. 1245-1251 8  R. B. Waterhouse, "Design of probe-fed stacked patches", IEEE Transactions on Antennas and Propagation, vol. 47, no. 12, Dec. 1999, pp. 1780-1784 9  D.M. Pozar, S. D. Targonski, \223A shared aperture dualband dual-polarised microstrip array\224, IEEE Transactions on Antennas and Propagation,Vol. 49 no. 2,Feb. 2001, pp. 150-157 10  http://www.ansoft.com 11  J-F. Z\374rcher, F.E. Gardiol, \223Broadband patch antennas\224 Artech House, \(1995\N 0-89006-777-5 12  H. Schippers, J. Verpoorte, P. Jorna, A. Hulzinga, A Meijerink, C. G. H. Roeloffzen, L. Zhuang, D. A. I Marpaung, W. van Etten, R. G. Heideman, A. Leinse, A Borreman, M. Hoekman M. Wintels, \223Broadband Conformal Phased array with Optical Beamforming for Airborne Satellite Communication\224, Proc. of the IEEE Aerospace Conference, March 2008, Big Sky, Montana US 13  H. Schippers, J. Verpoorte, P. Jorna, A. Hulzinga, L Zhuang, A. Meijerink, C. G. H. Roeloffzen, D. A. I Marpaung, W. van Etten, R. G. Heideman, A. Leinse M. Wintels, \223Broadband Op tical Beam Forming for Airborne Phased Array An tenna\224, Proc. of the IEEE Aerospace Conference, March 2009, Big Sky, Montana US 


  15  B IOGRAPHIES  Harmen Schippers is senior scientist at the National Aerospace Laboratory NLR. He received his Ph. D. degree in applied mathematics from the University of Technology Delft in 1982. Since 1981 he has been employed at the National Aerospace laboratory NLR. He has research experience in computational methods for aero-eleastics, aeroacoustic and electromagnetic problems. His current research activities are development of technology for integration of smart antennas in aircraft structures, and development of computational tools for installed antenna analysis on aircraft and spacecraft  Jaco Verpoorte has more than 10 years research experience on antennas and propagation Electromagnetic compatibility \(EMC and radar and satellite navigation He is head of the EMC-laboratory of NLR. He is project manager on several projects concerning EMCanalysis and development of advanced airborne antennas    Adriaan Hulzinga received his BEng degree in electronics from the hogeschool Windesheim in Zwolle Since 1996 he has been employed at the National Aerospace laboratory \(NLR as a senior application engineer. He is involved in projects concerning antennas and Electromagnetic compatibility \(EMC  Pieter Jorna received the M.Sc degree in applied mathematics from the University of Twente in 1999 From 1999 to 2005 he was with the Laboratory of Electromagnetic Research at the University of Technology Delft. In 2005 he received the Ph.D. degree for his research on numerical computation of electromagnetic fields in strongly inhomogeneous media Since 2005 he is with the National Aerospace Laboratory NLR\ in the Netherlands as R&D engineer   Andrew Thain is a research engineer in the field of electromagnetic modelling of antennas. He specialises in the use of surface integral methods for the calculation of coupling and radiation patterns and works closely with Airbus on the topic of antenna positioning. He has experience in the field of electromagnetic modelling  Gilles Peres is head of the Electromagnetics group of EADS-IW He has a wide experience in computational EM modelling particularly the use of FDTD, integral and asymptotic techniques for antenna structure interactions. He has contributed with Airbus experts to the certification campaign of the A340/500 and A340/600. Dr Peres holds a PhD thesis from University of Toulouse \(1998\ on impulsive Electromagnetic Propagation effects through plasma   Hans van Gemeren has a BEng degree in electronics. From the beginning of Cyner substrates he is involved in development and production of prototyping and nonconventional Printed Circuit boards Working mainly for design and research centers Cyner got involved in many high tech projects and from this developed a great expertise in the use of different \(RF materials. In the FlySmart project Hans and his colleagues are able to do what they like most: In close cooperation with designers, creatively working on substrate solutions 


  16  


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


