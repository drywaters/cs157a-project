Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 PARALLEL MINING ASSOCIATION RULES WITH BIT STRING ARRAY IN LARGE DATABASE XIANG-PING MENG I JIN QIAN  XIN QI 1 Department of Electrical Engineering, Changchun Institute of Technology, Changchun 130012 Jilin China 2 Department of Information Engineering, Northeast China Institute of Electric Power Engineering,Jilin 1'320 
12 China Jilin Education and Research Center, Changchun 130012 Jilin China E-MAIL: mxpt@public.cc.jl.cn hydqqi283@163.com Abstract Association rule mining is one of the important data mining tasks However the previously proposed methods still encounter some problems such as complex data structure candidate set generation and so on To improve efficiency association rules can be mined in parallel In this paper we use a simpler data structure called bit string array and propose a new approach to apply parallel projection 
and compress technique in parallel mining association rules It conducts various operations on bit string array according to the frequency of frequent items For frequent item with less frequency we conduct set operation on them for frequent item with more frequency we adopt compress technique and conduct bit AND operator on them Moreover it will reduce the communication cost and also response time This method can be scaled up to very large databases 
by parallel projection and compress technique Keywords Array Association Rule Mining Parallel Mining Bit String 1 Introduction Data mining an important step in this process of knowledge discovery consists of methods that discover interesting, non-trivial and useful patterns hidden in the data Association rule mining is one of the important problems of data mining Developing efficient frequent itemsets mining techniques has been an important research direction in data mining Various algorithms are proposed 
for frequent itemsets generation Apriori 11 and DHP[2 are the most two important ones The key idea of Apriori algorithm lies in the downward-closed property of support It means if an itemset has minimum support then all its subsets also have minimum support so any subset of frequent of itemset must also be frequent The candidate itemsets having k items can be generated by joining frequent itemsets having k-1 items and deleting those that 
contain any subset that is not frequent DHP algorithm is an extension of Apriori using a hashing technique. They may need to generate a huge number of candidate itemsets and repeatedly scan the database and check a large set of candidates by pattern matching Recently pattern-growth methods, such as FP-growthC41 TreeProjection[3 and FPL[6 have been proposed A pattern-growth method uses the Apriori property However instead of generating candidates itemsets it recursively partitions the databases into 
sub-databases according to the frequent patterns found and searches for local frequent patterns to assemble longer global ones Since their data structures are complex when the database is large they are sometimes unrealistic to construct a main memory-based FP-tree and so on The huge size of the available database and their high-dimensionality make large-scale data mining applications computationally very demanding to an extent that high-performance parallel computing is fast becoming an essential component of the solution 
Moreover the quality of the data mining results often depends directly on the amount of computing resources available In fact data mining applications are poised to become the dominant consumers of supercomputing in the near future There is a necessity to develop effective parallel algorithms for various data mining techniques Some parallel association rule mining algorithms are proposed 7 Count Distribution and Data Distribution 0-7803-7865-2/03/$17.00 02003 IEEE 1 a3 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 c are two of them. However the communication cost will be increased obviously when the number of processors is increased Also the communication cost will be increased when the processors are in the wide-area network instead of local-area work In this paper we use a simpler and more straightforward data structure bit string array and propose a new approach to apply parallel projection and compress technique in parallel mining association rules It conducts various operations on bit strings according to the frequency of frequent items. For frequent item with less frequency we conduct set operation on them for frequent item with more frequency we adopt compress technique and conduct bit AND operator on them Moreover it will reduce the communication cost and also the response time. This method can be scaled up to very large databases by parallel projection and compress technique The rest of the paper is organized as follows In Section 2 we review some basic concepts and describe the construction of bit string array In Section 3 details the algorithm for parallel mining association rules based on the array and set Discussion is given in Section 4 Finally we give the conclusion in Section 5 2 Bit string array construction In this section we mainly construct bit string and array Before going into the details let\222s describe the problem as follows 2.1 Problem Description Let I={i  i be a set of items Let TDB be a set of transactions, where each transaction T is a set of items such that T C I Associated with each transaction is a unique identifier called its TID We say that a transaction T contains X a set of some items in I if X C T An association rule is an implication of the form X=Y where XCI YCI and XnY=0  The rule X 3 Y holds in the transaction set TDB with confidence c if c of transactions in TDB that contain X also contains Y The rule X  Y has support s in the transaction set TDB if s of transactions in TDB contains XU Y The problem of frequent mining is to find the complete set of frequent itemsets in a given transaction database with respect to a given support threshold 2.2 Algorithm for bit string array construction In this section we describe the procedures to construct bit string array. Our general idea is illustrated in the following example Example 1 Let the transaction database TDB be the first two columns of Table 1 and the minimum support threshold be 2 \(i.e min sup  2 Table 1. A transaction database a c d e IT4 I a c, d h I a c, d 1 1 The first scan of the database is the same as Apriori which derives the set of frequent items 1-itemsets and their support counts \(frequencies\The set of frequent items is sorted in the order of descending frequency Therefore we conclude ordered frequent items satisfied the minimum threshold are: d a c e g Meanwhile we store natural number 1,2,3,4,5 into the frequent items of count respectively, store 1 into other infrequent count array For convenience of later discussions, the frequent items in each transaction are listed in this ordering in the rightmost column of Table 1 2 For each transaction T in TDB do the following 1\Select and sort the frequent items according to the order in the array 2 Traverse the array and compare the items in the array with the items in T A bit string is formed from left to right to indicate the existence and absence of frequent items in T as follows: If there is a corresponding item in T set the bit to l;othenvise set it to 0 When all the frequent items in T are examined a transaction will turn into a bit string and be stored in its corresponding array We employ parallel projection for database projection. Parallel projection is implemented as follows Scan the database to be projected once For each transaction T in the database, for each frequent item I in T project T to the I-projected database based on the transaction projection rule, specified in the definition of projected database We use arrays to store these results This process is illustrated in Table 2 Table 2 The complete construction of array from table 1 184 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi'an 2-5 November 2003 A[4 A[5 A[2 I 11,11,11 A[3 I 101,111,011 101 1,111 1,1101 101 11,1101 1 smaller which we conduct more easily 3 Parallel mining frequent patterns with bit string array From the structure of bit sting array we conclude All the transactions contained the same frequent item belong to the same partition and all the bit strings of the transactions have the same length 2 Since the bit string array is built in descending order of item frequencies, transactions containing less frequent items will have longer bit strings But the number of these transactions must be small since the rightmost items contained in these transactions are less frequent items with smaller support count 3 Transactions containing more frequent items will have shorter bit strings Although the number of these transactions must be large they will not consume much memory because of the short lengths of their bit strings If there are a huge number of frequent items while each transaction has only several items transactions containing less frequent items will have longer bit strings Therefore we adopt set operation to handle these cases Let the number of frequent items be N the average size of each transaction be ITJ if JTJ  N we use sets to store the transactions containing the frequent item with less frequency instead of converting them into bit strings Since the items in the frequent item sets are ordered in the support-descending order more frequent items will have shorter bit strings Therefore their corresponding arrays will have a number of same bit strings For example in A[2 there have three bit strings but they are identical So we can adopt a compress technique to store these bit strings Let the length of frequent pattern containing frequent item i be I we will generate only 2 not 2  various frequent patterns containing item i because we require the end of frequent patterns containing item i Suppose the frequent item in bit strings has support s that is, its corresponding array has s bit strings If 2  s we can employ two one-dimension arrays one containing only 2 bit strings to store s bit strings another to store the counts of the same bit strings Thus the size of the array will be the following properties \(in FPL[6 1 1-1 I 1-1 I After constructing bit string array, the remaining of the mining can be performed on the array only without referencing any information in the original database Then we are able to discover frequent patterns by performing simple operations on them For this purpose we devise a parallel mining algorithm PMAR which can generate frequent patterns in a very straightforward way A simple example is also given to illustrate the complete mining process Bit counting for each bit position count the number of I-bits For frequent item with smallest frequency g A[5 have two bit strings We conduct bit counting on all bit positions other than the least significant bit LSB position Ignore the bit positions whose bit counts are below the minimum support threshold Figure 1 shows the details for the example Figure 1 The result of bit counting on The items whose bit counts are equal to the count of item g are d e We use a set P for storing these items Generate patterns by enumeration of all the combinations of P and combine together with item g The final pattern generated g:2\\(dg:2 eg:2 deg:2 The search for frequent patterns associated with item g completes Then find patterns associated with item e Figure 2 shows the details for A[4 Figure 2 The result of bit counting on A141 185 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 186 The item whose bit counts are equal to the count of item e is d We still use a set P for storing this item and also find the items a c whose bit counts are not equal to the count of item e but are high the minimum support threshold We employ a set Q for storing these items We must check whether the combinations of these items are frequent. The above method can be recursively used for mining them The complete set of the frequent pattems in A[4 consists of the following three portions 1.The set of frequent patterns generated from P by enumeration of all the combinations of the items combined with item e together de:3 2 The set of frequent patterns generated from Q combined with item e together ae:2 ce:2 3 The set of frequent patterns combining P and Q formed by taken the cross-product of the frequent patterns generated from P and Q denoted as fieq-pattern-set\(P x freq-pattern-set\(Q that is each frequent item set is the union of one frequent item from P and one from Q and its support is the minimum one between the supports of the two itemsets Thus the complete set of frequent patterns generated by combining the results of P and Q will be fieq-pattern-set\(Q x freq-pattern-set\(P with the support being the support of the itemset in Q which is always no more than the support of the itemset from P The final pattern combined with item e generated dae:2\\(dce:2 Similarly, the mining goes on. It is easy to see that the above mining process finds the complete set of frequent pattems without duplication As the database size becomes larger and larger, in order to improve efficiency association rules can be mined in parallel. From the algorithm and its reasoning one can see that the construction process is a divide-and-conquer process which can use parallel processing method 1 For frequent items with less frequency we use sets to store the transactions containing the frequent item with less frequency instead of converting them into bit strings Then, we conduct set operations on the sets The framework of the mining process is similar to that of the above First employ count arrays to check each frequent item in each set and find their support counts; next, conduct set operations on the sets to find whether the combinations of frequent items are frequent; finally generate all frequent patterns 2 For frequent item with more frequency adopt compress technique and conduct bit AND operator on them For example, suppose the size of frequent pattern containing frequent item a in large database is 1000, let the length 3 then we need a one-dimension array to store 4 bit strings:001,011,101,111 another to store 4 counts 200,100,400,300 We do not need large array to store more bit strings. Thus we do not need recursively call the above method Note that our partition employ parallel projection for database projection each partition is mined respectively. The idea of Algorithm PMAR is that the main processor sends the bit string array to every processor All processors can perform functions to calculate the global count or global frequent itemsets and send the result to the main processor Any two processors do not communicate with each other This will reduce the communication cost and also the response time It\222s particularly useful in mining association rules over wide-area networks The general idea of algorithm PMAR is shown in the above example. We give a formal presentation of PMAR The main processor performs functions as follow Begin F=find-frequent-1 itemsets\(D L Sort\(F\Sort F in support descending order for\(j=l j<N;j do begin N the size of database D for each transaction T do begin if exist item i in L then else bit[i]=O Bitstringlj U bit[i end C=C U Bitstringlj end Send the bit string array to other processor Receive the result End Other processors performs functions as follow  Generate Frequent Patterns Begin Receive the bit string array Bit Counting If the count of item i in array are equal to the count of the least frequency item IJi 


Proceedings of the Second International Conference on Machine Learning and Cybernetics xi\224 2-5 November 2003 P=PU i LL L1  P If the count of item i in LL more than min-sup Q=Qu i frequent 1 Check whether the combinations of Q are  output the complete set of frequent patterns  Generate pattems from freq-pattern-set\(P with Generate pattems from freq-pattem-set\(Q with Generate pattems from freq-pattem-set\(P x item lfi item lfi freq-pattem-set\(Q with item lfi Send the result to the main processor End 4 Discussions Comparing with other frequent pattern mining methods the efficiency of BSA-mine comes from the following aspects First algorithm PMAR does not generate candidate itemsets test them or store any frequent patterns in memory It adopts a frequent-pattern growth method Therefore it avoids to costly handle a huge number of candidate sets Once a frequent pattern is generated it is output to disk directly In contrast the candidate- generation-and-test method has to save and use the frequent pattems found in the k-th round to generate candidates for the k+l\round Second PMAR uses a simpler and more straightforward data structure bit string and array It avoids the pointer operation and the complex data structure constructing and requires only a limit space Unlike other frequent pattern growth methods, such as FP-tree it does not need to physically construct memory structures of projected database It fully utilizes the information well stored in the array Moreover for frequent item with less frequency we conduct set operation on them for frequent item with more frequency we adopt compress technique and conduct bit AND operator on them Third since any two processors do not communicate with each other this will reduce the communication cost and also the response time It\222s particularly useful in mining association rules over wide-area networks 5 Conclusions In this paper we used a simple data structure bit string array for store information about frequent patterns and developed a new mining algorithm PMAR We also adopted compress technique to reduce the size of the original database Since algorithm PMAR represents a new highly efficient and scalable mining method which may have strong impact in the future development of efficient and scalable data mining methods it can be applied to other applications like the mining of Web, Text and so on References 1111 c21 31 41 151 1161 71 R Agrawal and R Srikant, Fast algorithms for mining association rules In VLDB\22294 pp:487-499,1994 J S Park M S Chen and Philip S Yu An effective hash-based Algorithm for Mining Association Rules In proceedings of the International Conference on Very Large Databases 1995 pp:487-499,1995 R Agarwal C Aggarwal and V V. V Prasad A tree projection algorithm for generation of frequent itemsets In J of Parallel and Distributed Computing Special Issue on High Performance Data Mining 2000 J Han J Pei and Y Yin Mining frequent patterns without candidate generation In SIGMOD\222OO H Mannila H Toivonen and A I Verkamo Efficient algorithms for discovering association rules In KDD\22294 pp 181-192, 1994 Fan-Chen Tseng Ching-Chi Hsu Generating Frequent Patterns with the Frequent Pattem List In R Agrawal and J C Shafer Parallel Mining of\222 Association Rules IEEE Transactions on Knowledge and Data Engineering 8\(6 pp:962-969, 1996 pp:1-12,2000 PAKDD 2001 ,LNAI 2035,~~:376-386,2001 187 


2\d all the frequent closed itemsets in a  i n c e    class a cond   and   nclass a cond   cond a in Case 1. Because nclass nclass b       itemsets of   a are not mined 3\d all the frequent closed itemsets in b  i n c e    pure b cond      npure b cond   cond b Case-3 As no itemset has the same support as b  b 3 is added into b  Because of nclass nclass ace        c and e are removed from P. Since sup d  min_sup cond bd ith P  needs to be mined. Clearly cond bd Case-3 as   class bd cond   and   class bd cond    As no itemset has the same support as bd  bd 2 is added into b  The mining then finishes 4\ Find all the frequent closed itemsets in   ecau s e of    class cond   and    nclass cond   cond   Case-3. The task of mining cond  further decomposed into mining of cond c ith P d,e cond d ith P e  and cond e ith P  cond  c Case-3 as   class c cond   and   nclass c cond    Because e appears in every transaction of cond c d sup ce  ce 4 is added into Z   d does not appear in every transaction of nclass c cond    even though ab does so, hence mining  cond cde ith P  is still needed. With sup cde  cde 3 is added into   cond d Case-3 due to   class d cond   and   nclass d cond    Since sup d  d 4 is added into   cond de ith    is in Case-1 as   class de cond   and   nclass de cond   Due to nclass nclass c        cde 3 is not added to   even though sup cde  min_sup cond e Case-1 as   class d cond   and   nclass d cond    Because nclass nclass c        ce 4 is not added to   The mining of    finishes  It is worth of pointing out     pmine  D I,min_sup k  can be directly applied to parallel mining. In this case k is determined by the number of processors in a parallel computing system. Each processor is loaded a conditional database of a class, and the mining of each class can be done in parallel. When a processor finishes its job much earlier than others, load balancing is required 5. Experimental Results  In this section, we compare the performance of our method PMINE with AFOPT. All experiments are performed on a 500MHz Pentium III with 192MB of memory, running RedHat Linux 6.0. For performance comparison we used the original source for AFOPT obtained from its authors  We choose benchmark datasets downloaded from FIMI’03 workshop web site. Table 3 shows the datasets characteristics Dataset Size #Trans MaxTL AvgTL T10I4D100K 3.93M 100000 30 10.10 T40I10D100K 15.12M 100000 78 39.61 Connect-4 9.11M 67557 43 43.00 Pumsb 16.30M 49046 74 74.00 Table 3 Dataset characteristics  Figures 3-6 show the running time for different datasets with different minimum support threshold. In the experiment, we combine our technique with FP-Tree data structure. Generally, it is faster than AFOPT.  In addition the new technique can be easily used in the existing frequent closed itemsets mining algorithms to improve their efficiency. The most important is its capability of parallel mining with workload balance  abcde bd cde abce cde D cde ce cond\(ab P={c,d,e bcde bce cond\(a P={c,d,e acde ace d cond\(b P={c,d,e abcde bd abce cde cde cond  P={c,d,e Z ab abce:2 P Z a  P Z b b:3 P={d Z  P={c,d,e abde abe de de cond\(c P={d,e Z ce:4 P={d ace cond\(bd P Z b b:3, bd:2 P abce b ce ce cond\(d P={e abcd cd abc cd cond\(e P ab cond\(cde P Z ce:4,d:4 P={e Z ce:4,d:4 P  Z ce:4,d:4,cde:3 P abc c c cond\(de P Z ce:4,d:4,cde:3 P non-class part class part Note A conditional database is divided into two parts, shown as the right figure. The part above the dashed line is non-class segment. The part under the line is class segment conditional database Z={abce:2, bd:2, b:3,cde:3,ce:4,d:4 Figure 2  Mining frequent closed itemsets using pmine  


6  Conclusions  In this paper, we investigate a new frequent-closeditemset mining method that can greatly enhance the efficiency and scalability of the current association rule algorithms. The main contributions of this work are: 1\he method completely eliminates the computationally expensive closure-checking procedure in the frequent closed itemset mining and thus is particularly useful for mining very large datasets; 2\ the method is applicable to not only sequential mining but parallel mining   Dataset: T10I4D100K 0 2 4 6 4 321 M inim um Suppor t     PM INE AFOPT  Figure 2 Running time for dataset T10I4D100K  Dat aset  T40I 10D100 K 0 5 10 15 20 2 5 21.51 M inim um Suppor t     PM INE AFO PT  Figure 3 Running time for dataset T40I10D100K Dataset: Connect-4 0 2 4 6 8 10 95 90 85 80 M inim um Suppor t     PM INE AFO PT  Figure 4 Running time for dataset Connect-4  Dataset: Pum sb 0 5 10 15 95 90 85 80 M inim um Suppor t     PM INE AFO PT  Figure 5 Running time for dataset Pumsb 7. References 1  N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal Discovering frequent closed itemsets for association rules In ICDT’99, Jan. 1999 2  R. Agrawal, T. Imielinski, and A. Swami.  Mining association rules between sets of items in large databases SIGMOD'93, 207-216, Washington, D.C 3  J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proc. 2000 ACMSIGMOD Int. Conf. Management of Data \(SIGMOD’00 Dallas, TX, May 2000 4  R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In Proc. 1994 Int. Conf. Very large Data Bases \(VLDB’94\es 487-499, Santiago, Chile September 1994 5  H. Mannila, H. Toivonen, and A. I. Verkamo Efficient algorithms for discovering association rules KDD'94, 181-192, Seattle, WA, July 1994 6  A. Savasere, E. Omiecinski, and S. Navathe. An efficient algorithm for mining association rules in large databases. VLDB'95, 432-443, Zurich, Switzerland 7  H. Toivonen.  Sampling large databases for association rules.  VLDB'96, 134-145, Bombay, India Sept. 1996 8  M.J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li New algorithms for fast discovery of association rules KDD’97. August 1997 9  S. Brin, R. Motwani, J. D. Ullman, and S. Tsur Dynamic itemset counting and implication rules for market basket analysis. SIGMOD'97, Tucson, Arizona, May 1997 10  D.W. Cheung, J. Han, V. Ng, and C.Y. Wong Maintenance of discovered association rules in large databases: An incremental updating technique. ICDE'96 New Orleans,  LA 


11  J.S. Park, M.S. Chen, and P.S. Yu. An effective hash-based algorithm for mining association rules SIGMOD'95, San Jose, CA, May 1995 12  J. Wang, J. Pei, and J. Han. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD 2003 13  C. Liu, H. Lu, J. X. Yu, W. Wang, X. Xiao. AFOPT An Efficient Implementation of Pattern Growth Approach In SDM 2003 14  M. J. Zaki and C. Hsiao. CHARM: An efficient algorithm for closed itemset mining. In SDM, 2002 15  T. Uno, T Asai, Y. Uchida, and H. Arimura. LCM: An Efficient Algorithm for Enumerating Frequent Closed Item Sets. In SDM 2003 16  C. Hughes, T. Hughes. Parallel and Distributed Programming Using C++. Addison Wesley Professional Aug 25, 2003 17  J. Adamo. Data Mining for Association Rules and Sequential Patterns --Sequential and Parallel Algorithms Springer  2000   


Figure 3 Example of a patent abstract with its generated multi-index. The multi-index that has been generated for the above patent abstract corresponds to the \223 Final indexation 224 field. The terms of the generated multi-index are prefixed by the name of the viewpoint to which they are associated: \223adv.\224 for the Advantages viewpoint, \223titre.\224 for the Title viewpoint, \223use.\224 for the Use viewpoint, \223soc.\224 for the Patentees viewpoint Patentees Title Use es GlobMin WEBSOM  WEBSOM  Number of indexed documents \(NID 1000 1000 745 624 1000 1000 Number of rough indexes generated \(NRI 73 605 252 231 1395 1395 Number of final indexes \(NFI 32 589 234 207 1075 1075 Numbers of map nodes with members \(/100 28 55 57 61 89 238 Table 1 Summary of the results of patent indexation and map building. Note that the NRI \(resp. NFI\ \223global viewpoint\224 are less than the sum of the NRIs \(resp. NFIs\ specific viewpoints \(i.e. 1089\ecause there are similar indexes occurring in different viewpoints Patentees Title Use es MSOM GlobMin WEBSOM  WEBSOM R 0,94 0,89 0,78 0,77 0,85 0,87 0,84 P 0,92 0,40 0,63 0,60 0,64 0,48 0,65 F 0,93 0,55 0,70 0,67 0,71 0,61 0,68 Table 2 Summary of the results of Quality, Recall and Precision evaluation. The nearer the different values are from 1, the better are the clustering results. The F value provides a synthesis of the results of R and P Proceedings of the Fourth International Conference on Coordinated Multiple Views in Explor atory Visualization \(CMV\22206 ISBN-10: 0-7695-2605-5/06 $20.00 \251 2006  IEEE 


Figure 4 Example of a generated map. Partial view of a topographic map of 10 x 10 nodes. The map is initially organized as a square 2D grid of nodes. The viewpoint chosen for the showed map is the Advantages viewpoint. The names of the clusters illustrate the topics \(considering the chosen viewpoint\ been highlighted by the learning. After the learning, the no des related to the same topics have been grouped into coherent areas thanks to the topographic properties of the map. The number of nodes of each area can then be considered as a good indicator of the topic weight in the database. Topics or areas near one to another represent related notions. For example, the \223 extending oil live 224 area shares some of its borders with the \223 black sludge control 224 area on the map. The proximity of these two areas illustrates the fact that oil duration strongly depends \of maintaining a low level of sludge in it. The surrounding circles represent the centers of gravity of the areas  1 2 3 Patentees Title Advantages Use 3 2 1 Figure 5 Example of exploitation of the inter-map communication mechanism. The analyst decision to activate the area corresponding to the TONEN CORP. company on the Patentees map and to propagate the activity to the thematic maps associated to the Use  Advantages and Title viewpoints corresponds to a "viewpoints crossing query" whose explicit formulation might look like: "I want to know which are the specific areas of competence \(concerning oil use, oil composition and expected advantages\". The MultiSOM application let him interactively find that TONEN CORP. company is a specialist of the lubrication of the automatic transmissions [arrow n\2602 on th r  this kind of lubrication sulfur-containing organo-molybdenum compound [arrow n advantages are to provide oil with a friction coefficient that is stable on a wide range of temperature [arro In this ca se, an inverted propagation fr om the target topics should be also used to verify that these topics only belong to TONEN CORP. areas of competence. The whiter is the color of a node representing a map cluster \(topic\ing activity Proceedings of the Fourth International Conference on Coordinated Multiple Views in Explor atory Visualization \(CMV\22206 ISBN-10: 0-7695-2605-5/06 $20.00 \251 2006  IEEE 


 Profile of topic Extending oil life Figure 6 Results of a WEBSOM-like global mapping of 10x10 nodes GlobMin The left part of the figure represents the WEBSOM-like mapping \(i.e. without viewpoint management\repre sents the description \(i.e. profile\ \223 extending oil life 224 WEBSOM global topic. Even if a strong relationship between \223 extending oil life 224 and \223 black sludge control 224 topics has been highlighted by the MultiSOM viewpoint-oriented clustering \(see map of figure 3 relationship has been lost by the WEBSOM-like clustering due to the noise of the global clustering \(this relationship do not ap pear neither in the above map, nor in the \223 extending oil life 224 topic profile Figure 7 Comparison between a 11x11 \223Use viewpoint\224 thematic map and a 16x16 \223Use viewpoint\224 thematic map through map extracts The 11x11 map extract is presented at the left, the 16x16 map extract is presented at the right. On the figure, the focus is gi ven 223 machine oil 224 topic. The comparison highlights, as an example, that the logical surrounding of this topic is more precisely defined in the 16x16 map \(optimal quality\n in the 11x11 map \(lower quality\n the 11x11 map, the topic \223 machine oil 224 has been derived in a more fuzzy scope topic named \223 machine and vehicles 224 Proceedings of the Fourth International Conference on Coordinated Multiple Views in Explor atory Visualization \(CMV\22206 ISBN-10: 0-7695-2605-5/06 $20.00 \251 2006  IEEE 


ANNEX: example of interactive dynamic multi-viewpoint analysis Dynamic analysis takes place main ly by using the inter-map communication mechan ism which makes it possible to bring to successful conclusion sets of topics deductions between differen t viewpoints chosen like investigation under-fields. This a nalyze is based on the generation of an initia l activity corresponding to the premises of the deduction to check. According to stages of analyze, this activity can itself be generated several manners by the analyst on one or more source map. If the activity generation is d irectly operated by analyst on a map, it corresponds then to broad a set of topics question. If the activity generation is operated indirectly by projection query on a map or by activation of documents group stored befo rehand in a collector of documents, it corresponds then to more targeted question, which can intervene in one second stage of the analyze The analyst interest is to hi ghlight the specific areas of competence of the Exxon company. On the simulation of analysis we develop on figure 7, we will consider two different viewpoints Patentees which will represent the source of the analysis and the Title viewpoint which will represent its destination The analyst starts the process of deduction by generating an initial activity on the main Exxon topic i.e. Exxon area gravity center\ of the Patentees viewpoint map. To obtain a broad set of potential deductions, he selects the Possibilistic mode of deduction 17  The activity generated by the inter-map  communication mechanism on the Title viewpoint map is focused in two different zone s of this map, corresponding to two potential results In the first active zone \(1\, the analyst makes use two different naming strateg ies to facilitate its interpretation namely a naming strategy based on the profile of the topics more generic\ and a naming strategy based on the profile of the best members \(i.e. patents\of the topics more specific\. These operations enable him to highlight that the Exxon company is specialized in a correlative way on topics: \215 marine diesel engine 216 215 surfactant system 216 and \215 basic calcium compound 216 The expert checks the correlation between these topics by consulting the patents associated to the topic 215 surfactant system 216 \(2\. The title of the patents already confirm him the problematic de tected by the application A thorough examination of th e contents of the documents will show him than the pur pose of use of surfactant containing calcium in add ition with the normal formula of oils is to protect the combustion chambers of the marine diesel engines against corrosion due to the absorption of air charged out of salt during their operation. The problem of protection of the marine engines against corrosion is sufficiently important to represent a field of investigation for an oil manufacturer like Exxon The construction of a query containing the single descriptor \215 surfactant system 216 \(3\ on the Title viewpoint will allow the analyst 1 To validate the correlation between 215 surfactant system 216 and \215 marine diesel engine 216 topics which will be interpreted by the fact that 215 surfactant system 216 is only associated with 215 marine diesel engine 216 2 To check the inverse deduction 215 surfactant system 000\306 Exxon 216 which will insure him that Exxon is the only company whose interest in the conception of \215 surfactant system 216 The result of the projection of the query on the Title viewpoint map \(4\ shows that the generated activity is peculiar to the logical topic area \215 marine diesel engine 216, which confirms th e first assumption Simultaneously with projection the documents that are relevant for the query are presented in a Collector \(5 The global activation of these documents allows analyst to initiate a new de duction.  Then, the result of this latter can be examined on the Patentees map. Like only the main Exxon topic has been activated \(6\, the second assumption of the an alyst is confirmed The second active zone \(7\ generated by the initial process of deduction will allow the analyst to observe that the second major field of activity of Exxon is the 215 biodegradable 216 oils. It will be able to also note that these oils are more specifically us ed for the lubrication of the two-stroke engines \(\215 two cycle engine 216\ that generally reject much unburned oil Probabilistic mode of deduction will allow him to check if the inverse deduction, namely, that Exxon is the only company to be worked on biodegradable oils, can be validated \(8\. This process will lead the analyst to conclude that 215 biodegradable 216 oil manufacturing is shared between Exxon and Mobil companies \(9\, which are the most important oil manufacturers A complementary use of negative activity setting on the \215 two cycle engine 216 topic \(10\ will show more precisely to the analyst that that Mobil company mainly focus on manufacturing of biodegradable oils for \215two stroke engines\216 and, in a complementary way, that Mobil company only focus on manufacturing of biodegradable oils for \215four stroke engines\216 \(11 The simulation of analysis presented here above shows clearly how the analyst can make use of the MultiSOM functionalities in order to highlight all the privileged activity fields of the Exxon company starting from a patents database related to engineering of oils Main functionality is inter-map communication. Multiple naming strategies, generation of queries and collection intermediate results that have been implemented complementary to inter-map communication also play an important role in the analysis process Proceedings of the Fourth International Conference on Coordinated Multiple Views in Explor atory Visualization \(CMV\22206 ISBN-10: 0-7695-2605-5/06 $20.00 \251 2006  IEEE 


     Activated area 1    Inverse validation 10 Categorical rejection Categorical choice Activity resulting from the inverse validation Result  28,6 and 17 11 Focalization Inverse validation Activity resulting from the inverse validation 9 Result 28,6% and 23 5 6 3 Legend  Viewpoint 215Patentees\216 Viewpoint 215Title\216  Projection resulting from the inverse validation Activated area 2 7 1 8 Analysis of deduction 2  4 Figure 8 Diagram of the analysis simulation Result 100 Proceedings of the Fourth International Conference on Coordinated Multiple Views in Explor atory Visualization \(CMV\22206 ISBN-10: 0-7695-2605-5/06 $20.00 \251 2006  IEEE 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


