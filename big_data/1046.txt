Fast Algorithm for Mining Multilevel Association Rules N.Rajkumar M.R.Karthik S.N.Sivanandam Research scholar Department of Electrical Head Depilrtment of Computer Dept of Computer Science Engg and Electronics Engg Science Engg PSG College of Technology PSG College of Technology Coimbatore  641004 Coimbatore  641004 Coimbatore  641004 India India India nrk29@rediffmail.com KarthikvZk@yahou.co.uk PSG College of Technology Ahsrruct-In this paper we present two algoiithms AprioriNewMulti and AprioriNewSingle for Data Mining multilevel and single level association rules in large database respectively The database consists of following fields Transaction ID and items purchased 
iii the transaction The algorithms introduce a new concept called multi minimum support i.e minimum support will vary for different length of the itemset Unlike other algorithms AprioriNewMulti does not depend on number of levels in concept hierarchy i.e it doesn\222t scan the database for each level of abstraction for finding association rules Scale up experiments show that 222both of these algorithms have scale linear with the number of customer transaction Keywords Data Mining Association rules Minimum support 1 INTRODUCTION Datu Mining refers to extracting or 223Mining\224 knowledge from large amounts of data 
Today\222s Industrial scenario is having manifold of data which is data rich and information poor The information and knowledge gained can be used for applications ranging from business management production control and market analysis to engineering design and science exploration Data Mining can be viewed as a result of natural evolution of information technology L2l Associution ride mining finds interesting association among a large set of data items With massive amounts of data continuously being collected and stored Many industries are becoming interested in mining association rules from their databases The discovery of 
interesting association relationships among huge amounts of business transaction records can help in many busine ision making process such as catalogue design cross eting and loss leader analysis[2 A typical example of association rule mining is market based analysis This process analyzes customer buying habits by finding association between the different items that customers place in their 223shopping baskets\224 The discovery of such associations can help retailers develop marketing strategics by gaining insight into which items of frequently 0-7803.765 I-X/O3/$17.00 0 2003 IEEE purchased together by customers For instance if customers buying milk how likely are 
they to also buy bread on the same trip to super market Such information can lead to increased sales by helping retailers do selective marketing and plan their self space For example placing milk and bread with in close proximity may fufrher encourage the sales of these items together within visit to the store 2 PROBLEM STA?EMENT We are given a database of customer transactions Each transaction consists of following fields Transaction ID and items purchased in that transaction We don\222t consider quantities of items brought in a transaction The 
items are represented by an item ID An itemset is a non erirpty set of items in which items\222are arranged in ascending der with respect to its ID In order to explore the frequent k-itemseL we use the prior knowledge of the k-I itemsets k-2 itemset up to I itemset To improve the prior knowledge an important property called the Apriori Property presented below is used to reduce the search space In order to use the Apriori Property all nonempty subsets of a frequent itemset must also be frequent 41 this property is based 
on the following observation By definition if an itemset I does not satisfy the minimum support threshold minimum support then I is not frequent i.e P I minimum support If an item A is added to the itemset I then the resultink itemset i.e I U A cannot occur more frequently than I Therefore I U A is not frequent either That is P\(I U A minimum support This property belongs to a special category of properties called anti-monotone in the 
sense that if a set cannot pass a test ali of its supersetS%iH:fail the same test as well It is called anti-moootone bec*e the property is monotonic in the context of failing a-test Multi Minimum Support the concept is used to find valuable sequence in higher length of itemsets In the existing algorithms the minimum support is constant for all length of sequence this will not be suitable for most of the transactional data because the higher length itemsets will have very low support when compared to I-itemsets 


Web Technology and Dafa Mining 689 T2000 T3000 T4000 T5000 3 RELATED WORK The problem of discovering what items are brought together in a transaction over basket data was introduced This problem of finding what items are bought together is concerned with finding intra transaction patterns whereas the problem of finding sequential patterns is concerned with inter-transaction patterns A pattern in the first problem consists of an unordered set of items whereas pattern in laner case is an ordered list set of items 111 Discovering rules from data has been a topic of active research in AI in IO the rule discovery programs have been categorized into those that find quantitative rules and those find qualitative laws The purpose of quantitative rule discovery programs is to automate the discovery of numeric laws of the type commonly found In scientific data such as boyle's law PV=C The problem is sated as follows[l I1:Given m variables x,,xz,x 3....xm and k groups of observational data di,dz,d3  d where each d: is set of m values one for each variable find a formula f\(xI,x2,x3  x that best fits the data and symbolically reveals the relationship among variables because too many formulas might fit the given data the domain knowledge is generally used to provide the bias towards the formulas that are appropriate for the domain Examples of some well known systems in this category include ABACUS IZ Bacon 181 and COPER 9  IlJ3 I5 12,14,16 11.15 I6 Ii,12 15,16 Litemet Any itemset having support satisfying the minimum threshold value is termed as litemset Level Pass Threshold In multilevel association &le mining a value that determines whether itemsets of sub levels are to be processed or not This usually set between minimum supports of the k-level and k-1 for k-level i.e the itemsets of k-1 are processed only if its support satisfies the level pass threshold of k-level Through out this paper we set.the level pass threshold of k-level equal to the minimum support of the k-1 5 ALGORITHM APRIORINEWMULTI We solve the problem of finding all multilevel interesting patterns in three phases Mapping Phase I-itemset Phase Main Phase The following section describes the above phases in detail TID I Items in the transaction mnnn I 1.4 16 1 Business database reflect the uncontrolled real 5.1 Mapping Phase world where many different causes overlap and many  patterns are likely to co.exist 91  Rules in such data are likely to have uncertainty The qualitative rule discovery programs are targeted at such business and they Only used for mining multilevel association rules Here each item's concept hierarchy path is mapped into its address table 5.1.1 Each item has unique   generally use little or no domain knowledge There has been  level  0 level  1 considerable work in discovering classification Given examples that belong to one of the pre-specified classes discover rules for classifying them Classic work in this area includes SI 4 TERMINOLOGY Supporr Consrrainrs These constraints concern the number of transactions that support a rule The support for a rule is defined to be the fraction of transactions that satisfy the Support should not be confused with confidence While confidence is a measure of the rule's strength support corresponds to statistical significance Besides statistical significance another motivation for support constraints comes from the Fact that we are usually interested only in rules with support above some minimum threshold for business reasons If the support is not large enough it means that the rule is not worth consideration or that it is simply jess preferred \(may be considered later Q 5.1.1 Shows item in concepr tree union of items in the consequent and antecedent of the rule   Table 5.1.1 Mapping ofaddress to irems 


TENCON 2003  690 Level  Iremset Table 5.1 Minimum support of each item as a function of level and length of itemset 5.2 Idemset Phase Here we scan the database and find the frequency of all 1 itemset and stored in a concept hierarchy tree created physically\phase is pruned to get the frequent 1 itemsets This will be used as a reference for the rest of the process Fig 5.2.1 Gives support of each and every item in the database Fig 5.l.l is prefixed by's 5.3 Main Phose This phase is split into 4 more modules, repeated for every transaction 5.3.1 Primaryprune module The transaction is split into I-itemsets each of which is tested for the minimum support If it fails it is pruned from the transaction; else it retains its position in the transaction Now a new transaction is formed consisting of only 1 Lilemsets 5.3.2 Split Module   The prunqd transaction is mapped to contiguous integers The number of possible k-subsets for the mapped transaction is derived from the database of subsets The database of sub sets is created preliminarily to save time during mining These possible k-subsets with the primed transaction are given to the next module Fig 5.3.2 Possible subsets of a 4-itemset in a tree 5.3.3 econdary prune Module The possible k-subsets of the pruned transaction are tested for the existence of non litemsets of length  k If a k subset is not a litemset then all superset of this subset is removed from the tree In this example 1:3 is not a litemset so all its superset are removed from the tree Fig 5.3.3 Pruned tree offis 5.3.2 5.3.4 Tree construction module The itemsets from the secondary prune step are stored in concept tree The address of the each item in an item set is used to construct the tree This enables to find thesupport of the itemsets at each level The support of a node is the sum of the support of its child node i.e support of k-level node  sum of the support of its k-1 child nodes Though non-litemsets do not form nodes their support is added to parent node Fig 5.3.4 shows 2-Litemsets arranged in concept tree with its support at each level support is prefxed by s 


Web Technology and Data Mining 691 Itemset Level 2 has no litemset Fig 5.3.5 shows 3-Liremsers arranged in concept tree with irs support at each level support is prefixed by's 6 ALGORITHM APRIORINEWSINGLE 123 Minimum suppori corresponding level but AprioriNewMultUSingle will scan the database as if there is only one level If a database contains IO levels and large itemset.of length IO then it will scan thedatabase lOO\(lO*lO times But AprioriNewMultilSingle will scan the database only 10 times 8 GENERATION OF SYNTHETIC DATA To evaluate the performance of the algorithms over a large range of data characteristics we generate synthetic customer transactions The synthetic data takes the following arameters Average number items per Transaction Maximum length of the frequent itemset Number of Levels in Concept Hierarchy Fig 7.0.1 Terminology used in synthetic data generation 9 EXPERIMENTAL RESULTS To assess the performance our algorithms on a large itemset generations by using an IBM PC powered by Pentium 111 750 MHZ 64MB RAM 5400 RPM HDD We compare our algorithm with existing Apriori algorithm performances are shown in the following graphs The default values are D=100000 N=1000 L10 Mz0.5 F=7 T20 543 2000 1000 500 0 50000 100000 150000 200000 4 Aprion             Fig 9.1 Varinrion ofexecurion time with D I I 5000 10000 15000 20000 0 c ApriociNewMulti e Apriori t+No.of items Fig 9.2 Variation of execution time with N 


TENCON 2003 692 0 5 222 10  15 20 25 j i i i Fig 9.3 Viriarion ofexecution.rime with L        222 4000 2000 _  0 0.1 0.2 0.3 0.4 0.5 0.6 1 M Mlnlnmum Support t ApiofiewMuhi   a-Apriori       pie 9.4 Variation ofexecution rime with M 10 PERFORMANCE EVALUATION It is seen from the above results that AprioriNewMulti/Single performed better than Apriori due to following reasons Time wasted in candidate generation is saved in AprioriNewMultilSingle If number of 2-Litemset is very large then generating candidate sets is time consuming Among these candidate sets only few will be useful AprioriNewMulti scans the database much less often than Apriori Performance of AprioriNewMultiISingle will increases with increase in the number of levels in a database 11 CONCLUSION We extend the scope of the study of mining from single level to multiple level association rules from a large transaction databases Mining multiple-level association rules may lead to progressive mining of refinedJywledge from the data and have interesting applications for knowledge discovery in transaction-based as well as other business or engineering databases With the help of multi minimum support concept finding interesting frequent itemset in highemength of itemsets made easy AprioriNewMulti scans the database less this made it to perform well over other existing algorithms in duration of the process REFERENCES R.Agrawal and Ramakrishnan Srikant 223Mining Sequenrial Parrerns\224 IBM Almaden Research Center 650 Harry Road San Jose CA 95 120 Jiawei Han Micheline Kamber 223Data Mining Conceprs and Techniques\224 Harcourt India Private Limited ISBN:81-7867-023-2 KAgrawal and Ramakrishnan Srikant 223Mining Association Rules\224 IBM Almaden Research Center 650 Harry Road San Jose CA 95120 Rhonda delmater Monte Hancock 221I Dura Mining Explnines\222 Butterworth-Heimann Reprinted 2002 ISBN 1.55558-231-1 151 long So0 Park Ming-Syan Chen and Philip S.Yu 223An Eflective Hash Based Algorirkm for Mining Association Rules\224 1BM Thomas J.Watson Research Center New York 10598 6 M.Kokan 223Discovering Functional Formulas rhrough Changing Representarion Base\224 Proceedings of the fifth National Conference on Artificial Intelligence 1986,455-459 171 P-Langley H.simon G.BradshawAnd JZytkow Scientific Discovery Compurarional Explorarions of Crearive process The MIT Press Cam bridge Mass 1987 181 J.Ross Quinlan 223lnducrion of Decision Trees\224 Machine Learning I 1986.81-106 191 G.Piatesky-Shapiro Discovery Analysis and Presenrurion of Srrong Rules In[10],229-248 


100 0.64290.76595 spend on bread s pe nd on bu tte r 0.765950.6429 0 20 40 60 80 100 0 20 40 60 80 100 0.63907 0.36093 0.20718 0.79282 spend on bread s pe nd on bu tte r Proceedings of the Fourth IEEE International Conference on Data Mining ICDM04 0-7695-2142-8/04 $ 20.00 IEEE Notice that we have chosen to measure sparseness by a linear activation penalty \(i.e. minimum the 1-norm of the column of H minimization can be found by the following update rules   6 ik kl kl il iki i ik klk w h h v w w h  7 j kl ljl h w w v h w h  


To make the solution unique, we further require that the 1normal of the column vector in matrix W is one. In addition, matrix H needs to be adjusted accordingly 8 9 It is proved that the objective function is non-increasing under the above iterative updating rules, and the convergence of the iteration is guaranteed \(in Appendix 3.3 Principal SNMF When the dataset V is decomposed with W and H each column value of H represents the corresponding projection on the basis space W . As a whole, the sum of every row vector of H represents the importance of corresponding base. Therefore, we define a support measurement after normalizing every column of H 10 Definition. For every rule \(column vector define a support measurement  11 j ij support w h h Consequently, we can measure the importance of each rule for the entire dataset by their support values. The more value of support implies the more importance of such rule for the whole dataset In order to select the principal k rules as Ratio Rules firstly, we rank the whole rules in descending by the support value. And then, retain the first k principal rules as Ratio Rules because they are more important than others. About the selection of k value, a simple method is taken such as 1 1  min \(12  k ii Mk ii support w threshold support w       


From above \(12 according that the sum of k support values of rules cover threshold \(i.e.90 4. Experiments Synthetic dataset We have applied both the PSNMF and the PCA to a dataset that consists of two clusters, which contains 25 Gaussian distribution points on x-y plain \(generated with mu=[3;5], sigma=[1,1.2;1.2,2 Fig2 Generated with mu=[3;5], sigma=[2,1.6;1.6,2 0 10 20 30 0 10 20 300 5 10 15 20 25 Y X Z Fig 2.Dataset with two clusters Table 1. Ratio Rules based on PSNMF and PCA PSNMF 1RR 2RR 3RR X Y Z   a b Table 1.\(a values. After ranking such rules, Ratio Rules are obtained since 1 2  1 2 0 : 0.493 : 0.507 \(0.6650 0.696 : 0.304 : 0 \(0.2885 rule X Y Z rule X Y Z   For example, 2/3 transactions \(the cluster with distribution on y-z plain 2rule . Therefore, the corresponding support value \(0.665 of 1rule does not contradict with intuition. Otherwise Table 1.\(b to explain the negative association obviously 


Real Dataset: NBA \( 459 11   This dataset comes from basketball statistics obtained from the 97-98 season, including minutes played Point per Game, Assist per Game, etc. The reason why we select this dataset is that it can give a intuitive meaning of such latent associations. Table 2 presents the first three Ratio Rules \( 1RR , 2RR , 3RR general knowledge of basketball, we conjecture the 1RR represent the agility of a player, which gives the ratio of Assists per Game and Steals, is 0.206:0.220 1:1? . It means that the average player who possess one time of assist per game will be also steal the ball one time, and so does 2 \(0.117 : 0.263 1: 2.25 cannot give such information behind the dataset PCA 1RR 2RR X Y Z Proceedings of the Fourth IEEE International Conference on Data Mining ICDM04 0-7695-2142-8/04 $ 20.00 IEEE Table 2. Ratio Rules by PSNMF from NBA field 1RR 2RR 3RR Games 0.450 Minute 0.013 Points Per Game 0.010 Rebound Per Game 0.117 Assists per Game 0.206 Steals 0.220 Fouls 0.263 3Points 5. Conclusion In this work, we proposed Principal Sparse NonNegative Matrix Factorization \(PSNMF sparse non-negative components in matrix factorization. It aims to learn latent components which are called Ratio Rules. Experimental results illustrate that our Ratio Rules are more suited for representing associations between items than that by PCA References 1] Agrawal, R., Imielinski, T. and Swami, A.N., Mining association rules between sets of items in large databases In the Proc. of the ACM SIGMOD, \(1993 2] Aumann, Y. and Lindell, Y., A statistical theory for quantitative association rules. In the Proc. KDD, \(1999 3] Han, J. and Fu, Y., Discovery of Multiple-Level Association Rules from Large Databases. In Proc. of the 


VLDB, \(1995 4] Korn, F., Labrinidis, A., Kotidis, Y. and Faloutsos, C Ratio rules: A new paradigm for fast, quantifiable data mining. In the Proc. of the VLDB, \(1998 5] Lee, D.D. and Seung, H.S., Algorithms for nonnegative matrix factorization. In Proc. of the Advances in Neural Information Processing Systems 13, \(2001 6] Olshausen, B.A. and Field, D.J. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature 381\(1996 7] Srikant, r. and Agrawal, R., Mining quantitative association rules in large relational tables. In Proc. of the ACM SIGMOD \(1996 Appendix To prove the convergence of the leaning algorithm 6 7  objective function     multiplicative update rule corresponds to setting ,at each iteration ,the new state vector to the values that minimize the auxiliary function 1  13  Then the objective function Z is updated using \(13 1 1     Updating H : with W fixed, H is updated by minimizing  constructed for        ik kj ik kj ij ij ij ik kj i j i j k ik kj ik kjk k ij ij ij i j i j i j w h w h G H H v v v w h w h w h y v h      


   Since it is easy to verify 1  j ij j i j l h=  , therefore it is not difficult to testify     function, the following holds for all i ,j and 1ijkk     log log  thus log log log ik kj ik kj ik kj ijk ijkk k ijk ik kjk ik kj ik kj ik kj ik kjk k ik kj ik kjk k w h w h w h where w h w h w h w h w h w h w h           Thus  To minimize 1  14  for all kl    


0ik kji l i ki i kl ik kj klk w hG H H v b h w h h   Solving for H , this gives     ik kl kl il iki i ik klk w h h v w w h  which is the desired updated H Updating W : with H fixed, W is updated by minimizing         ik ik ik ik kj kj ij ij ij ik kj i j i j k kj kjk k ij ij ij i j i j i j w h w h G W W v v v w h w h w h y v h         It is easily to prove    likewise. we can get 


  lj kl kl kj ljj j kl ljk h w w v h w h  This completes the proof Proceedings of the Fourth IEEE International Conference on Data Mining ICDM04 0-7695-2142-8/04 $ 20.00 IEEE 


available, CPU utilization drops quickly due to the frequent swapping. Compared with them, MSPS and GSP process the customer sequences one by one, hence only a small memory space is needed to buffer the customer sequences being processed. MSPS can also handle the situation that LDBk or C DB k can not be totally loaded into memory by using the signatures as explained in Section 4. Therefore MSPS does not require the memory space as much as GSP SPADE and SPAM Many real-life customer market-basket databases have tens of thousands of items and millions of customers, so we evaluated the scalability of the mining algorithms in these two aspects. First, we started with a very small database D1K-C10-T5-S10-I2.5 and changed the number of items from 500 to 10,000. The user-speci?ed minsup was 0.5 To run MSPS on such a small database with only 1000 customers, we selected the whole database as the sample and keep the user-speci?ed minsup unchanged to mine it Since MSPS does not apply the sampling on such a small database, supersequence frequency based pruning is not performed in mining. Thus, in this case, SPADE and SPAM performed better than MSPS and GSP as long as their memory requirement is satis?ed As the number of items is increased, SPAM shows its scalability problem. Theoretically, the memory space required to store the whole database into bitmaps in SPAM is D ? C ? N/8 bytes. For the id-lists in SPADE, it is about D ?C ? T ? 4 bytes. But we found these values are usually far less from their peak memory space requirement during the mining, because the amount of intermediate data in both algorithms is quite huge Compared with SPAM, SPADE divides search space into small pieces so that only the id-lists being processed need to be loaded into memory. Another advantage of SPADE is that the id-lists become shorter and shorter with the progress in mining, whereas the length of the bitmaps does not change in SPAM. These two differences make SPADE much more space-ef?cient than SPAM Second, we investigated how they perform on C10-T5S10-I2.5-N10K when the user-speci?ed minsup is 0.18 We ?xed the number of items as 10,000 and increased the number of customers from 400,000 to 2,000,000. SPAM cannot perform the mining due to the memory problem For SPADE, we partitioned the test database into multiple chunks for better performance when its size was increased Otherwise, the counting of CDB2 for a large database could be extremely time-consuming. We made each chunk contain 400,000 customers so that it is only about 100 Mbytes which is one tenth of our main memory size. Figure 3 shows that the scalability of MSPS and GSP are quite linear. As the database size is increased, MSPS performs much better than the others When database is relatively small with only 400,000 customers, SPADE performed the best, about 20% faster than MSPS. But SPADE cannot maintain a reasonable scalability as the database becomes larger, and MSPS starts outperforming SPADE. When the database size is increased from 1600K customers to 2000K customers, there is a sharp performance drop in SPADE, such that it is even slower than GSP. In that case, MSPS is faster than SPADE by a factor of about 8. As discussed before, counting CDB2 is a performance bottleneck for SPADE, because the transformation of a large database from the vertical format to the horizontal format takes too much time. When the database is very large, the transformation also requires a large amount of memory and frequent swapping, hence the performance drops drastically. Partitioning the database can relieve this problem to some extent but does not solve it completely. In addition, for the database with a large number of items and customers, SPADE needs more time to intersect more and 


customers, SPADE needs more time to intersect more and longer id-lists Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 400 800 1200 1600 2000 Number of Customers \('000s Ex ecu tion Ti me se c GSP SPADE MSPS Figure 3. Scalability: Number of Customers on C10-T5-S10-I2.5-N10K, minsup=0.18 Finally, we mined a large database D2000K-C10-T5S10-I2.5-N10K, which takes about 500 Mbytes, for various minsups. This database is partitioned into 5 chunks for SPADE, and the results are shown in Figure 4 Based on our tests, we found SPADE performs best for small size databases. For medium size databases, MSPS performs better for relatively big minsups while SPADE is faster for small minsups. When database is large SPADE  s performance drops drastically and MSPS outperforms SPADE very much. If the user-speci?ed minsup is big and there are very few long patterns, GSP may perform as well as, or even better than, others due to its simplicity and effective subsequence infrequency based pruning 100 1000 10000 100000 0.33 0.3 0.25 0.2 0.18 Minimum Support Ex ec uti on Ti me se c GSP SPADE MSPS Figure 4. Performance on a Large Database D2000K-C10-T5-S10-I2.5-N10K 6 Conclusions In this paper, we proposed a new algorithm MSPS for mining maximal frequent sequences using sampling. MSPS combined the subsequence infrequency based pruning and the supersequence frequency based pruning together to reduce the search space. In MSPS, a sampling technique is used to identify potential long frequent patterns early. When the user-speci?ed minsup is small, we proposed how to adjust it to a little bigger value for mining the sample to avoid many overestimates. This method makes the sampling technique more ef?cient in practice for sequence mining. Both the supersequence frequency based pruning and the customer sequence trimming used in MSPS improve the candidate counting process on the new pre?x tree structure developed. Our extensive experiments proved that MSPS is a practical and ef?cient algorithm. Its excellent scalability 


makes it a very good candidate for mining customer marketbasket databases which usually have tens of thousands of items and millions of customer sequences References 1] R. Agrawal and R. Srikant  Fast Algorithms for Mining Association Rules  Proc. of the 20th VLDB Conf., 1994, pp 487  499 2] R. Agrawal and R. Srikant  Mining Sequential Patterns  Proc. of Int  l Conf. on Data Engineering, 1995, pp. 3  14 3] J. Ayres, J. Gehrke, T. Yiu, and J. Flannick  Sequential Pattern Mining Using a Bitmap Representation  Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining 2002, pp. 429  435 4] B. Chen, P. Haas, and P. Scheuermann  A New TwoPhase Sampling Based Algorithm for Discovering Association Rules  Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining, 2002, pp. 462  468 5] S. M. Chung and C. Luo  Ef?cient Mining of Maximal Frequent Itemsets from Databases on a Cluster of Workstations  to appear in IEEE Transactions on Parallel and Distributed Systems 6] F. Masseglia, F. Cathala, and P. Poncelet  The PSP Approach for Mining Sequential Patterns  Proc. of European Symp. on Principle of Data Mining and Knowledge Discovery, 1998, pp. 176  184 7] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U Dayal, and M. C. Hsu  Pre?xSpan: Mining Sequential Patterns Ef?ciently by Pre?x-Projected Pattern Growth  Proc of Int  l. Conf. on Data Engineering, 2001, pp. 215  224 8] R. Srikant and R. Agrawal  Mining Sequential Patterns Generalizations and Performance Improvements  Proc. of the 5th Int  l Conf. on Extending Database Technology, 1996 pp. 3  17 9] H. Toivonen  Sampling Large Databases for Association Rules  Proc. of the 22nd VLDB Conf., 1996, pp. 134  145 10] M. J. Zaki, S. Parthasarathy, W. Li, and M. Ogihara  Evaluation of Sampling for Data Mining of Association Rules  Proc. of the 7th Int  l Workshop on Research Issues in Data Engineering, 1997 11] M. J. Zaki  SPADE: An Ef?cient Algorithm for Mining Frequent Sequences  Machine Learning, 42\(1  60 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


