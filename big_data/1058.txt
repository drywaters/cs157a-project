Probabilistic Data Association Methods in Visual Tracking of Groups G Gennari G D Hager Information Engineering Computer Science University of Padova Johns Hopkins University Padova Italy 35131 Baltimore MD 21218 Abstract Data association is a fundamental problem when tracking large numbers of moving targets Most commonly employed methods of data association such as the JPDA estimator are combinatorial and therefore do not scale well to large numbers of targets How 
ever in many cases large numbers of targets form natural groups which can be eciently tracked We describe a method for dening groups based on the position and velocity of targets This denition introduces a natural set of merging and splitting rules that are embedded into a Kalman ltering framework for tracking multiple groups In cases where groups of dierent velocities cross a general methodology for matching measurements to groups is introduced This algorithm is based 
on a modied version of the PDA estimator It is well suited to handle a high number of measurements and extends naturally to additional grouping constraints such as color or shape 1 Introduction Consider the problem of tracking crowds of clumped people walking in a station or ocks of birds ying together or colonies of ants At one level of granularity each of these phenomena consists of individual motions However taken together the dominant charac 
teristic is a gestalt group motion Furthermore in many cases it is the group motion that is of interest rather than the motions of each individual Typically the problem of tracking large numbers of targets has been considered using data association techniques such as the JPDAF and its variants 1 Extensions of these ideas to computer vision problems can be found in 6 In g e ner a l  d a t a a sso cia tio n tec hniques attack the problem of distinguishing among targets so that individual tracks can be established but 
do not address the notion of abstraction to a larger group Indeed one of the major limitations of these methods are the combinatorics that result when scaling to large numbers of targets In situations like these it is often dicult to distinguish the individual objects of interest because they are quite similar to the others and may be partially or totally occluded Indeed in such cases it often makes sense to track the overall ensemble of objects as a group using a model of the group dynamics In this paper a general class of partitioning func 
tions is introduced which allows us to formally dene a group Each group is completely described by its state which takes into account information about spatial location cardinality and velocity We introduce rules for splitting and merging groups based on state spaces In general this leads to situations in which groups sharing visual measurements are not merged together because they are moving with dierent velocities To deal with such situations we propose a data association methodology that is a PDA estimator properly modied to 
embed the notion of a group One of the unique aspects of our approach is that we rst associate each single measurement to a group on the basis of the state predictions and then we further partition the resulting subsets of measurements into connected components In traditional approaches 3 4 5 t h e o v e rall set of measurements is usually rst partitioned into connected components without reference to existing targets and only then the resulting sets of measurements 
are matched to targets 2 Description of a group In what follows we rst present a formal denition of a group Then we focus on the group tracking problem introducing an appropriate state model Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition \(CVPR04 1063-6919/04 $20.00  2004 IEEE 


2.1 Denition of group As we mentioned in the introduction our goal is to dierentiate between collections of clumped objects that can be treated jointly as a group and subsets of objects that can be reliably dierentiated from one another We will formally dene groups in terms of a partitioning of the set of available measurements and will say a collection of objects forms a group when it cannot be segmented in ner partitions To this end consider a set A   a 1   a N  and a grouping or partitioning function G  The partitions obtained applying G to the set A are indicated through the following notation G  A   a i  i  I 1    a i  i  I M  where I i form a partition of  1   N  Thecardinality of the partition M will be denoted G  A   in the sequel Note that each element of the partition is an invariant set of the grouping function G thatisfora given I j above G   a i  i  I j   a i  i  I j We say an arbitrary set A is a group with respect to a grouping function G if it is an invariant set of G  i.e G  A   A  or G  A   1and A is maximal Equivalently we say that A satises the group constraint with respect to G  The invariance of partition elements under G is the analog of saying that a group cannot be segmented into smaller groups For instance a process that extracts connected components of a graph is a grouping function and each connected component is an invariant set or a group 2.2 State model Consider now a set G of N objects moving in the scene with measured positions 1 C y   y 1   y N   and suppose that the set C y satises the group constraint with respect to an appropriate grouping function we defer the discussion of the measurement grouping function until section 3.3 In this case the N objects of G are treated as a unique object which can be described completely through a state vector that contains the spatial location the velocity and the cardinality of the overall ensemble G   v  N  1 In general additional features such as the color can be considered as well as position for the sake of simplicity we just use position where  and v are the position and velocity of the centroid of the group respectively In this model the position of the i th object in the group is expressed as follows y i    s i 2.1 where s i N 0   In other words  is a covariance matrix which denes the spatial extension of the group the positions C y are expected to be in the following search region SR    y   y    T    1  y      2.2 with a probability higher that a threshold which depends on the choice of   Finally N is the number of points forming the group Summing the equations 2.1 over i and introducing a dynamical model for the position and velocity of the group centroid we obtain the following model  x  k 1 Fx  k  n x  k  y c  k  Hx  k  n y  k  2.3 where x T   v  T and y c is the measured position of the centroid Note that n y  k  includes both the uncertainty in the position of a group member and observation noise In order to propagate x over time a standard Kalman lter is applied to the model 2.3 In addition the group covariance  and the group cardinality N are estimated at time k as   k   s    k  1  1   s   cov  C y   N  k   N   N  k  1  1   N   C y  2.4 where cov  C y nd  C y  indicate the sample covariance and the cardinality of the set C y of measurements generated by the group while  s and  N are appropriately chosen constants positive and smaller than one One role of the Kalman lter is to produce an estimate for the velocity of the group as well as its covariance matrix By analogy with SR   we indicate with SR v the search region dened by the predicted velocity and its corresponding error covariance matrix At this point we have dened a group and given a method for computing groups we have dened a state updating mechanism We must now incorporate mechanisms for managing groups subject to a given partitioning method and we must dene a data association scheme for cases where the association of a measurement to a group is ambiguous 3 Merging and splitting groups In many situations groups interact forming larger groups or disperse forming smaller groups We address the formation of groups by creating connected Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition \(CVPR04 1063-6919/04 $20.00  2004 IEEE 


components of measurements that are similar in both position and velocity When these measurements belong to other groups the overall ensemble of groups is treated as a single group by replacing the collection of individual states by a unique merged state A group splits when it no longer satises the group constraint under an appropriate partitioning function In section 3.3 we introduce the specic group constraint adopted in our algorithm 3.1 State space grouping Two groups are usually considered in spatial proximity when their SR  search regions overlap and have similar velocities when their SR v search regions overlap We dene the relation R x among groups through the following expression G i R x G j   SR  i  SR  j    SR v i  SR v j    Thus R x induces an undirected graph among groups Let G x be a partitioning function that computes the connected components of this graph That is given existing groups C G   G 1   G N   G x computes a partition G x  C G   G i  i  I 1    G i  i  I M  3.2 Merging operator If any of the I i above has cardinality greater than 1 we consider the group constraint to be violated and merge the associated groups Let  x i   i N i ethe state of the i th group G i  The merging operator applied to the collection of groups  G i  i  I j yields a new merged group G m,j                  x m,j   i  I j  N i,j  x i  m,j   i  I j  N i,j    i   i   m,j     i   m,j  T 012 N m,j   i  I j N i 3.1 where  N i,j  N i N m,j  3.3 Group constraint The measurements of a given group must satisfy a specic group constraint Generally the group constraint is tested as estimation is performed however there are two cases where we must group measurements with no prior state information First a given group can in general be associated to several sets of measurements more details in section 4 The group constraint will be tested for each possible conguration of measurements and when none of them satises the group constraint the group is split into new groups initialized starting from the measurements in its search region Second measurements that are unclaimed by any group must themselves be grouped The extreme case is the rst frame of video when no group yet exists In this section we describe the initialization process based on a group constraint for a given set of measurements The group constraint is dened in such a way that it is guaranteed that the new groups cannot be merged back into a single group We proceed in two stages First since we have no prior state information we begin by clustering based on spatial proximity essentially an analog to connected components In particular a relation R 0 in the measurement space is dened as follows y i R 0 y j  SR y i  0  SR y j  0    where SR y 0 indicates a region centered in y with an appropriate radius As before given a collection of measurements C y   y 1   y N   each measurement y i becomes a node in a graph whose edges are determined by R 0  C y can then be partitioned into connected components G 0  C y   y i  i  I 1  0    y i  i  I M 0  To each set of grouped measurements  y i  i  I j 0 is associated a group G j 0 whose initial state is obtained by computing the cardinality the sample mean and covariance of  y i  i  I j 0  The initial velocity is assumed to be null The nal result is a collection of groups C G 0   G 1  0   G M 0   To these groups the state grouping operator G x   s applied to partition them into new groups G x  C G 0   G j 0  j  I 1    G j 0  j  I L  Let G y denote the composition of these two grouping processes That is G y is a grouping function which operates on sets of measurements and produces groups as follows G y  C y   y i  i  I j 0  j  I 1    y i  i  I j 0  j  I L  A given set of measurements C y satises the group constraint if and only if G y  C y   1  4 Data association Data association techniques are crucial to deal with the ambiguity in the origin of measurements shared Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition \(CVPR04 1063-6919/04 $20.00  2004 IEEE 


among several groups This is the case in our approach since groups sharing measurements pixel coordinates are not merged together when they are moving with dierent velocities We propose a PDAF-like methodology to deal with the problem of assigning large sets of measurements to each group In order to keep the computational load acceptable only an appropriate subset of all the associations is considered The group constraint is properly embedded in the data association process 4.1 Feasible associations In what follows we do not consider false measurements and missed detections focusing instead on the problem of associating measurements shared among several groups To this end consider a group G and let Y G represent the set of measurements which are in the corresponding search region 2.2 Let Y G,ns and Y G,s denote the subset of measurements which G does not share with other groups and the remaining shared measurements respectively The shared measurements Y G,s are sorted according to their Mahalanobis distance from the centroid of G in ascending order Y j G,s indicates the rst j sorted measurements We now form a set of associations where the j th association  j  associates the set Y G j  Y G,ns  Y j G,s to the group G  The remaining measurements in Y G are assumed to be generated by the other groups sharing measurements with G  4.2 Probability of associations Each association  j is assigned a probability mass which takes into account both the dynamic information given by the model 2.3 and the group constraint We also assume prior knowledge on the range of variation over time of the cardinality N of a generic group The mass which takes into account the dynamic information is chosen proportional to the gaussian density of the measured position of the centroid m d   j  k d  p    Y G j    y c P y c  where  y c and P y c are the predicted position of the centroid and the corresponding error covariance given by the Kalman estimator and   Y G j  is the centroid of the measurements Y G j  The mass on the number of measurements is assumed being of gaussian form as well m N   j  k N  e  1 2    Y G j   N  n  2 where  N is the estimated number of measurements  Y G j  is the cardinality of Y G j and  N is chosen based on the expected variability of the number of measurements in a group Finally in order to have state estimates which actually represent groups the updating process should consider only congurations of measurements satisfying the group constraint which we take into account through the following mass m g   j  k g  p g  Y G j  where p g is given by p g  Y G j   1if G y  Y G j   1 0otherwise 4.1 The overall mass for a given association  j is chosen proportional to the product of the single factors introduced above m tot   j   m d   j   m N   j   m g   j  4.2 which takes into account dynamics cardinality and the group constraint Note that congurations of measurements that do not satisfy the group constraint are given a null mass In particular if all associations for a group are given a null mass for a certain number of consecutive frames the group is eliminated and the measurements are regrouped and initalized as described in section 3.3 4.2.1 Computational eciency Verifying the group constraint for each set Y G j involves the computation of G 0  Y G j  which clusters many measurements into connected components G 0  Y G j anbeeciently computed as follows The set Y G j can be written in the following way Y G j  Y G j  1  y j  where y j is the j th sorted measurement Assume G 0  Y G j  1  has been computed G 0  Y G j  1   Y G 1  j  1   Y G M  j  1  Lets indicate with I R the indices of the connected components Y G i  j  1 which contain at least one element related to y j  I R   i  y 012 Y G i  j  1 y R 0 y j  and with I R the remaining indices I R   1   M  I R Theset Y G j can be partitioned as follows G 0  Y G j   Y G i  j  1  i  I R   y j   i  I R Y G i  j  1  The computation of the centroid of Y G j is eciently performed as well starting from the centroid of Y G j  1  Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition \(CVPR04 1063-6919/04 $20.00  2004 IEEE 


Given the predictions at time k   group G   Y G are the measurements in the search region 2.2 of G  Y G,s  Y G and Y G,ns  Y G are the shared and unshared measurements respectively  The measurements Y G,s are sorted in ascending order according to their Mahalanobis distance from the centroid of G yielding the set  y 1   y N s   Y G 0  Y G,ns  j 0    Y G,s   Y G j  Y G j  1  y j  is associated to G  The conditional state estimate  x k j is computed sec 4.3  m tot   j  is computed through 4.2 and in particular p g  Y G j  is computed through 4.1  If m tot   j 0  j the group G splits into new groups obtained through the initialization process sec 3.3 applied to the set Y G  else the overall state estimate for group G is computed  x k   j  x k j  m tot   j  We have now a collection of groups C G along with their state estimates The merging process is applied G x  C G   G i  i  I 1    G i  i  I M   j 1   M  i  I j G i  G m,j eq 3.1  For each new merged group G m,j the predictions for time k  1 are computed Figure 1 Summary of the algorithm 4.3 State estimation Given an association   the conditional state estimate of a group G is obtained updating the Kalman lter using the measured position of the centroid   Y G  The cardinality  Y G  and the sample covariance cov  Y G  are used in 2.4 The overall estimate is obtained computing the weighted sum of the single conditional estimates with weights being the masses of the corresponding associations as is done in the PDAF The overall data association and estimation process is summarized in gure 1 5 Experiments We tested our algorithm in video sequences of groups of people merging and crossing in an outdoor scene The pixel coordinates of these groups are obtained at each frame through background subtraction techniques Experiments demonstrate that taking into ac   1 2 3 4 A 1 2 3 B 1 2 C 1 D Figure 2 Groups of people merging into a single group count velocity improves in general the tracking performances in situations in which groups cross each other In section 5.2.1 it is shown how the data association methodology proposed in section 4 contributes in obtaining good tracking results 5.1 Groups merging into a single group In the rst video sequence groups of people are approaching and merging together Four frames of the sequence are reported in gure 2 Ellipsoids and arrows represent search regions and velocities of groups respectively In frame A three groups are coming from the lower right corner and approaching group 1 coming from the opposite side Groups 2 and 3 in frame A which are closely spaced and moving with similar velocities are merged together into group 2 of frame B In the same way groups 2 and 3 of frame B merge into group 2 of frame C People of groups 2 and 3 in frame C are changing direction in order to move together as a single group The estimated velocities take few seconds to change accordingly At that point the groups are merged together as shown in frame D 5.2 Groups crossing each other In the second video sequence groups of people are moving along opposite directions and crossing each other We report four frames of the sequence in gure 3 In frame A group 1 is moving toward the upper right corner while groups 2 and 3 are coming from the opposite direction In frame B groups 2 and 3 are crossing group 1 The algorithm fails in tracking group 3 which Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition \(CVPR04 1063-6919/04 $20.00  2004 IEEE 


  3 2 1 A 3 2 1 B 1 2 C 1 2 3 D Figure 3 Groups of people moving along opposite directions and crossing each other is eliminated while it is passing through group 1 see frame C On the other hand the algorithm succeeds in tracking group 2 Taking into account the velocity is in general advantageous in dealing with such challenging situations In fact although group 1 and group 2 are partially overlapped the dierence in the corresponding velocities prevents them from being merged together and when the crossing phase is over groups 1 and 2 are still tracked reliably see frame C In frame D group 3 is tracked again when splits apart from group 1 1 2 A 1 2 B Figure 4 Groups of people moving along opposite directions and crossing each other No proper data association is performed resulting in inferior tracking performances 5.2.1 Data association Data association techniques are crucial in obtaining good tracking performances We processed the video sequence of groups crossing each other eliminating the data association step i.e assigning all the validated measurements Y G to the group G  A couple of key frames are reported in gure 4 In frame A groups 1 and 2 start crossing During this interaction all the measurements generated by group 2 and being in the search region of group 1 are assigned to group 1 and viceversa This results in group 1 engulng all objects of group 2 The same happens to group 2 see frame B 6 Conclusions and future work In the present work we introduced a formal denition of group and a general merging process dened in the phase space The data association process dynamically segments the measurements at pixel level on the basis of the predictions and it is properly modied in order to embed the additional group constraint The algorithm has been tested on real data yielding good tracking results in challenging situations such those of groups crossing each other The approach is promising and it can be further improved Integration of additional features such as pixels color and online identication procedures will be considered References 1 Y  B a r Sha l o m a nd X R Li  Multitarget-Multisensor Tracking Principles and Techniques  YBS Publishing 1995 2 S S B l a c k m a n  Multiple Target Tracking with Radar Applications  Artech House Norwood MA 1986 3 S te phe n J  M c K e nna   T r a c ki ng g r o ups o f p e o p l e   Computer vision and Image Understanding  Vol 80 42-56 2000  Pro c of 1st I n t  W ork sh op s o n P erforman ce Ev alu at ion of Tracking Systems PETS 2000  Pro c of 2n d I nt  W ork sh op s o n P erforman ce Ev alu at ion of Tracking Systems PETS 2001  C R asm u ssen  G.D  H ager Prob ab ilist ic D a t a A sso ciation Methods for Tracking Complex Visual Objects IEEE Trans on Pattern Analysis and Machine Intelligence  Vol 23 No.6 June 2001 pp 560-576 7 T  K i r uba ra J a n Y B a r Sha l o m K R P a tti pa ti   M u l t i assignment for Tracking a Large Number of Overlapping Objects IEEE Trans on Aerospace and Electronic Systems  Vol.37 No.1 Jan 2001 Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition \(CVPR04 1063-6919/04 $20.00  2004 IEEE 


For each un-instantiated variable Xij of ci  Do For each link t = H\(t  Xij =d\(t, Xij interprets ci  Do Compute the score sijk of t according to Eq.\(1 2 If sijkts  Put \(t,sijk Find in each Sij an element \(tij, sij H\(ti1 ti2  H\(tih and the sum of scores 6j sij is maximal Replace the missing value of Xi1, Xi2  Xih in ci by Xi1=d\(ti1,Xi1 ti2,Xi2  Xih =d\(tih,Xih Return D  the updated D Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE Incomplete case di Incomplete case di Does t H\(t  Xj=zjk exist in T Does t H\(t  Xj=zjk exist in T For each missing datum Xj=? in di For each missing datum Xj=? in di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Compute score\(t according to Eq.\(1 2 Compute score\(t according to Eq.\(1 2 Does H\(t consistently interpret di Does H\(t consistently interpret di Apply the value to complete the missing data Apply the value to complete the missing data Figure 4. The proposed completing procedure Table  1. \(a b transactional form of the dataset Case X1 X2 X3 X4  TID Items c1 2 1 ? ?  t1 X1=2,X2=1 c2 2 1 2 1  t2 X1=2,X2=1, X3=2, X4=1 c3 2 1 2 1  t3 X1=2,X2=1, X3=2, X4=1 c4 1 2 1 ?  t4 X1=1,X2=2, X3=1 c5 1 2 1 2  t5 X1=1,X2=2, X3=1, X4=2 c6 ? 1 2 1  t6 X2=1, X3=2, X4=1 c7 1 ? 1 ?  t7 X1=1, X3=1 c8 1 2 1 1  t8 X1=1,X2=2, X3=1, X4=1 c9 1 ? 1 ?  t9 X1=1, X3=1 c10 1 2 1 2  t10 X1=1,X2=2, X3=1, X4=2 Table  2. Data associations of D ID Data association supp conf r1 {X3=1  X1=1} 0.60 1.00 r2 {X2=2, X3=1  X1=1} 0.40 1.00 r3 {X2=2  X1=1} 0.40 1.00 r4 {X2=1  X1=2} 0.30 0.75 r5 {X3=2, X4=1  X2=1} 0.30 1.00 r6 {X3=2  X2=1} 0.30 1.00 r7 {X1=2  X2=1} 0.30 1.00 r8 {X4=1  X2=1} 0.30 0.75 r9 {X1=1  X3=1} 0.60 1.00 


r9 {X1=1  X3=1} 0.60 1.00 r10 {X2=2  X3=1} 0.40 1.00 r11 {X1=2,X2=2  X3=1} 0.40 1.00 r12 {X2=1, X4=1  X3=2} 0.30 1.00 r13 {X2=1  X3=2} 0.30 0.75 r14 {X4=1  X3=2} 0.30 0.75 r15 {X2=1, X3=2  X4=1} 0.30 1.00 r16 {X3=2  X4=1} 0.30 1.00 r17 {X2=1  X4=1} 0.30 0.75 5. Experiments In order to test the effectiveness of our method, several experiments are performed and the results are compared with RBE. The datasets, DA and DB, are generated syntactically. In DA, data are randomly generated and the variables in DA have no particular dependency relations. In DB, three variables, X1, X2 and X3, are defined and each variable has three different instantiations, xij, i=1,2,3 and j=1,2,3. Cases in DB are generated \(1 X1=x11  X2=x22}, {X2=x21  X3=x32}, and X3=x33  X1=x13}, and \(2 associations do not hold. The datasets, Monk, TAE and Solaris are from [4]. Table 3 describes the basic information of these datasets. In these datasets, we randomly remove some data from completed cases and test if our method can successfully recover the missing ones. For nm missing data, the accuracy of recovery D is defined as 1- nw/nm if nw data are incorrectly guessed From the experimental results shown in Table 4, our method is more accurate than RBE is Table 3. Description of the test datasets Dataset ID cases variables niv missing ratio source DB -1 15 DB -2 30 3 3 20 DA -1 15 DA-2 100 6 2 20 Syntactic Monk -1 10 Monk -2 415 7 3 20 TAE -1 20 TAE -2 151 5 3 30 Solaris -1 5 Solaris -2 1066 13 4 10 UCI [4 niv: # of instances in each variables \(in average Table 4. Experimental results Our method RBEDataset ID \(s, c, nda s, c, nda DB -1 \(25,65, 6 15,50,8 DB -2 \(25,65, 6 15,50,8 DA -1 \(20,20,350 30,40,179 DA-2 \(20,20,350 30,40,179 Monk -1 \(15,15,115 10,35,181 Monk -2 \(15,15,115 10,35,181 TAE -1 \(10,30,73 5,60,115 TAE -2 \(10,30,73 5,60,115 Solaris-1 \(40,40,2926 60,60,988 Solaris-2 \(40,40,2926 60,60,988 s, c, nda data associations and nda  is the number of data associations obtained 6. Discussions &amp; Conclusion We presented in this paper a new method for completing missing data using data associations. The basic concept is that association rules describe the dependency relationships among data entries in a Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE 


0-7695-2291-2/04 $ 20.00 IEEE dataset and missing data should hold the similar relationships. One of the advantages of using this approach for completing missing data is that data associations can be easily and reasonably obtained Most reasonable datasets include reasonable association rules. Clearly, data associations truly describe the cross-relation among data instantiations more accurately. The score function that we presented in Eq.\(1 2 There may exist other measures, such as [16], that reveal more information for the applicability of data associations. The score function can be further improved. Unfortunately, the accuracy of the proposed method depends on the number of data associations When the threshold of minsupp and minconf are low there will be a large number of data associations being generated, resulting in inefficiency of the completing procedure. The future work includes developing a better informative score function and reducing the number of data associations and determining suitable values of minsupp and minconf. Currently, in this paper, only discrete data are discussed and the data existing in datasets are assumed to be correct and noise-free. We are extending the proposed method to handle non-discrete, noisy, and incorrect datasets 7. References 1] Agrawal, R., and Srikant, R. \(1994  Fast algorithm for mining association rules  in the Proceedings of the International Conference on VLDBases, pp. 487-499 2] Agrawal, R., Imielinksi, T., and Swami, A. \(1993  Dataset mining: a performance perspective  IEEE TKDE vol. 5, no. 6, pp. 914-925 3] Agrawal, R., Srikant, R., and Vu, Q. \(1997  Mining association rules with item constraints  in the Proceedings of the Third International Conference on KDD, Newport Beach, California, pp. 67-73 4] Black, C., Keogh, E., and Merz, C.J. \(1999 repository of machine learning databases, URL http://www.ics.uci.edu/~mlearn/MLRepository.html 5] Buntine, W.L. \(1994  Operations for Learning with Graphical Models  Journal of AI, vol. 2, pp. 159-225 6] Chen, M.S., Han, J., and Yu, P.S. \(1996  Data mining an overview from a database perspective  IEEE TKDE, vol 8, no. 6, pp.866-883 7] Coenen, F., Goulbourne, G., and Leng, P. \(2004  Tree structures for mining association rules  Data Mining and Knowledge Discovery, vol. 8, no. 1, pp.25-51 8] Dempster, A. P., Laird, N.M., and Rubin, D.B. \(1977  Maximum likelihood from incomplete data via the EM algorithm  Journal of the Royal Statistical Society, vol. 39 no. 1, pp.1-38 9] Frick, J.R., and Grabka, M.M. \(2003  Missing Income Information in Panel Data: Incidence, Imputation and its Impact on the Income distribution  in the Proceedings of the Workshop on Item-Non-response and Data Quality in Large Social Surveys, October 9-11 10] Giudici, P., and Castelo, R. \(2003  Improving Markov Chain Monte Carlo model search for data mining   Machine Learning, vol. 50, no. 1-2, pp.127-158 11] Hern  ndez, M.A., and Stolfo, S.J. \(1998  Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem  DMKD, vol. 2, no. 1, pp.9-37 12] Intelligent CAM Systems Laboratory  Methodologies for dealing with the Missing Data   http://www.eng.uc.edu/icams/resources/missing_data.htm 13] Kryszkiewicz, M., \(2000  Probabilistic Approach to Association Rules in Incomplete Databases  Web-Age Information Management, pp. 133-138 14] Kryszkiewicz, M., and Rybinski, H. \(1999 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





