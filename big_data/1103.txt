Mining Frequent Itemsets from Secondary Memory G 250 osta Grahne and Jianfei Zhu Concordia University Montreal Canada  grahne j  zhu  cs.concordia.ca Abstract Mining frequent itemsets is at the core of mining association rules and is by now quite well understood algorithmically for main memory databases In this paper e investigate approaches to mining frequent itemsets when the database or the data structures used in the mining are too large to 336t in main memory Experimental results show that our techniques reduce the required disk accesses by orders 
of magnitude and enable truly scalable data mining 1 Introduction Mining frequent itemsets is a fundamental problem for mining association rules 2 3 It also plays an impor tant role in many other data mining tasks such as sequential patterns episodes multi-dimensional patterns and so on 4 9 In addition frequent itemsets are one of the key abstractions in data mining The description of the problem is as follows Let I   i 1 i 2 i n   be a set of items 
 Items will sometimes also be denoted by a b c    An I transaction 001 is a subset of I An I transactional database D is a 036nite bag of I transactions The support of an itemset S 001 I is the proportion of transactions in D that contain S  The task of mining frequent itemsets is to 036nd all S such that the support of S is greater than some n 
minimum support 002  where 002 either is a fraction in 0  1  or an absolute count Most of the algorithms such as Apriori DepthProject and dEclat 12 w ork well when the main memory is big enough to 036t the whole database or/and the data structures candidate sets FP-trees etc When a database is very large or when the minimum support is very low either the data structures used by the algorithms may not be accommodated in main memory r the algorithms spend too much time on multiple passes r the database In the t IEEE ICDM Workshop on Frequent Itemset Mining Implementations FIMI 32503 
 man y well kno wn algorithms were implemented and independently tested The results show that 223 none of the algorithms is able to gracefully scale-up to very large datasets with millions of transactions\224 At the same time very large databases do exist in real life In a medium sized business or in a company big as Walmart it\222s very easy to collect a few gigabytes of data Terabytes of raw data are ubiquitously being recorded in commerce science and government The question of how to handle these databases is still one of the most dif\036cult problems in data mining In this paper we consider the problem of mining frequent itemsets from 
very large databases We adopt a divide-and-conquer approach First we give three algorithms the general divide-and-conquer algorithm then an algorithm using basic projection division and an algorithm using aggressive projection We also analyze the disk I/O\222s required by these algorithms In a detailed divide-and-conquer algorithm called Diskmine  we use the highly ef\036cient FPgrowth method  to mine frequent itemsets from an FP-tree for the main memory part of data mining We describe several l techniques useful in mining frequent itemsets from disks such as the FP-array technique and the item-grouping technique We also present experimental results that demonstrate 
the fact that our Diskmine algorithm can outperform previous algorithms by orders of magnitude and scales up to terabytes of data 2 Mining from disk How should one go about when mining frequent itemsets from very large databases residing in a secondary memory storage such as disks Here 223very large\224 means that the data structures constructed from the database for mining frequent itemsets can not 036t in the available main memory One approach is sampling  Unfortunately  the results of sampling are probabilistic some critical frequent itemsets could be missing Besides the sampling there are basically two strategies for mining frequent itemsets the 
datastructures approach and the divide-and-conquer approach The datastructures approach consists of reading the database ffer by ffer and generate datastructures i.e candidate sets or FP-trees Since the datastructure do not 036t into main memory additional disk I/O\222s are required The number of passes and disk I/O\222s required by the approach depend on the algorithm and its datastructures r examples if the algorithm is Apriori using a hash-tree Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


for candidate itemsets disk based hash-trees have to be used If the algorithm is FP-growth method as suggested in FP-trees ha v e to be written to the disk Then the number of disk I/O\222s for the trees depends on the size of the trees on disk Note that the size of the trees could be the same as or even bigger than the size of the database The basic strategy for the divide-and-conquer approach is shown in the procedure diskmine  In the approach D denotes the size of the data structures used by the mining algorithm and M is the size of available main memory Function mainmine is called if candidate frequent itemsets not necessary all can be mined without writing the data structures used by a mining algorithm to disks In diskmine  a very large database is decomposed into a number of smaller databases If a 223small\224 database is still too large i.e the data structures are still too big to 036t in main memory the decomposition is recursively continued until the data structures 036t in main memory After all small databases are processed all candidate frequent itemsets are combined in some y obviously depending on the y the decomposition s done to get all frequent itemsets for the original database Procedure diskmine  D M  if D 002 M then return mainmine D  else decompose D into D 1  D k  return combine diskmine D 1 M     diskmine D k M   The ef\036ciency f diskmine depends on the method used for mining frequent itemsets in main memory and on the number of disk I/O\222s needed in the decomposition and combination phases Sometimes the disk I/O is the main factor Since the decomposition step involves I/O ideally the number f recursive calls should be kept small The faster we can obtain small decomposed databases the fewer recursive call we will need On the other hand if a decomposition cuts down the size of the projected databases drastically the trade-off might be that the combination step becomes more complicated and might involve heavy disk I/O In the following we discuss two decomposition strategies namely decomposition by partition and decomposition by projection Partitioning  is an approach in which a lar ge database is decomposed into cells of small non-overlapping databases The cell-size is chosen so that all frequent itemsets in a cell can be mined without having to store any data structures in secondary memory ever since a cell only contains partial frequency information of the original database all frequent itemsets from the cell are local to that cell of the partition and could only be candidate frequent itemsets for the whole database Thus the candidate frequent itemsets mined from a cell have to be veri\036ed later to 036lter out false hits Consequently those candidate sets have to be written to disk in order to leave space for processing the next cell of the partition After generating candidate frequent itemsets from all cells another database scan is needed to 036lter out all infrequent itemsets The partition approach therefore needs only two passes over the database t writing and reading candidate frequent itemsets will involve a signi\036cant number of disk I/O\222s depending on the size of the set of candidate frequent itemsets We can conclude that the partition approach to decomposition keeps the recursive levels down to one t the penalty is that the combination phase becomes  To get an easier combination phase we adopt another decomposition strategy which e call projection  This approach projects the original database on several databases each determined by one or more frequent item\(s One advantage of this approach is that any frequent itemset mined from a projected database is a frequent itemset in the original database To get all frequent itemsets we only need to take the union of the frequent itemsets discovered in the small projected databases The biggest problem of the projection approach is that the total size of the projected databases could be too large and there could be too many disk I/O\222s for the projected databases Thus there is a tradeoff between the easier combination phase and possible too many disk I/O\222s To analyze the recurrence and required disk I/O\222s f the general divide-and-conquer algorithm when the decomposition strategy is projection let us suppose that The original database size is D bytes The data structure is an FP-tree The FP-tree constructed from original database D is T  and its size is  T  bytes f a conditional FP-tree T 001 is constructed from an FPtree T  then  T 001 002 c 267 T   for some constant c 1  The main memory mining method is the FP-growth method T w o database scans are needed for constructing an FP-tree from a database The block size is B bytes The main memory available is M bytes In the 036rst line of the algorithm diskmine if T can not 036t in memory then projected databases will be generated We assumed that the size of the FP-tree for a projected database is c 267 T  If c 267 T 002 M  function mainmine can be called for the projected database otherwise the decomposition goes on At pass m  the size of the FP-tree constructed from a projected database is c m 267 T   Thus the number of passes needed by the divide-and-conquer projection algorithm is 1 003 log c M/T 004  Based on our experience and the analysis in we can say that for all practical purposes the number of passes will be at most two r example Let D  100 gigabytes T 10 gigabytes M 1 gigabytes and c  10  Then the number of passes is 1 003 log 0  1 2 30  10 327 2 30  004  2 In 036ve passes we can handle databases up to 100 Terabytes Namely e get 1 003 log 0  1 2 30  10 327 2 40  004 5 Assume that there are two passes and that the sum of the sizes of all projected databases is D 001  After the 036rst Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


database scan for 036nding all frequent single items the second database scan attempts to construct an FP-tree from the database If the main memory is not big enough the scan will be aborted We assume on average half of D is read at this stage which means 1  2 267 D/B disk I/O\222s The third scan is for decomposition Totally there are 5  2 327 D/B disk I/O\222s The projected databases have to be written to the disks 036rst then later each scanned twice for building the FP-tree This step needs 3 327 D 001 B disk I/O\222s Thus the total disk number of disk I/O\222s for the general divideand-conquer projection algorithm is at least 5  2 267 D/B 3 267 D 001 B 1 Obviously the smaller D 001  the better the performance One of the simplest projection strategies is to project the database on each frequent item which we call basic projection  First we need some formal de\036nitions De\336nition 1 Let I be a set of items By I 002 we will denote strings over I  such that each symbol occurs at most once in the string If 003  004 are strings then 003.\004 denotes the concatenation of the string 003 with the string 004  Let D be an I database Then freqstring  D  is the string over I  such that each frequent item in D occurs in it exactly once and the items are in decreasing order of frequency in D  As an example consider the  a b c d e  database D   a c d    b c d e    a b    a c   If the minimum support is 50 then freqstring  D  acbd  De\336nition 2 Let D be an I database and let freqstring  D  i 1 i 2 267\267\267 i k For j 005 1 k  we de\036ne D i j   001 006 i 1 i j   i j 005 001 001 005D  Let 003 005 I 002  We de\036ne D 001 inductively D 002  D  and let freqstring  D 001  i 1 i 2 267\267\267 i k  Then for j 005 1 k   D 001.i j   001 006 i 1 i j   i j 005 001 001 005D 001   Obviously D 001.i j is an  i 1 i j  database The decomposition of D 001 into D 001.i 1  D 001.i k is called the basic projection  To illustrate the basic projection let\222s consider the above example starting from the least frequent item in the freqstring  e obtain D d   a c d    b c d   D b   c b    a b   D c   a c    c    a c   and D a   a    a    a   De\336nition 3 Let 003 005 I 002  i j 005 I  and let D 001.i j be an I database Then freqsets  002 D 001.i j  denotes the subsets of I that contain i j and are frequent in D 001.i j when the minimum support is 002  Usually we shall abstract 002 away and write just freqsets  D 001.i j   Lemma 1 Let D 001 be an I database and freqstring  D 001  i 1 i 2 267\267\267 i k  Then freqsets  D 001  001 j 003 1 k  freqsets  D 001.i j  In the previous example for D d  freqsets  D d   d    c d   Note though  c  is also frequent in D d  it s not listed since it does not contain d  It will be listed in freqsets  D c  Similarly freqsets  D b   b   freqsets  D c   c    a c  and freqsets  D a   a   We also can notice that D d and D c are not that much smaller than the original database The upside is though that the set of all frequent itemsets in D now simply is the union of freqsets  D d  freqsets  D b  freqsets  D c  and freqsets  D d  This means that the combination phase is a simple union The following procedure basicdiskmine s a divideand-conquer algorithm that uses basic projection A transaction 001 in D 001 will be partly inserted into D 001.i j if and only if 001 contains i j  The parallel projection algorithm introduced in is an algorithm of this kind Procedure basicdiskmine  D 001 M  if D 001 002 M then return mainmine D 001  else let freqstring  D 001  i 1 i 2 267\267\267 i n  return basicdiskmine  D 001.i 1 M  007  007 basicdiskmine  D 001.i n M   Let\222s analyze the disk I/O\222s f the algorithm basicdiskmine  s before we assume that there are two passes that the data structure is an FP-tree and that the main memory mining method is FP-growth Ifin D 002  each transaction contains on the average n frequent items each transaction will be written to n projected databases Thus the total length of the associated transactions in the projected databases is n  n 212 1 267\267\267 1  n  n 1  2  the total size of all projected databases is  n 1  2 267 D b n 2 267 D  Still there are two full database scans and a incomplete database scan for D 002  s explained for formula 1 The number of total disk I/O\222s is 5  2 267 D/B  The projected databases have to be written to the disks 036rst then later scanned twice each for building an FP-tree This step needs at least 3 267 n 2 327 D/B  Thus the total disk I/O\222s for the divide-and-conquer algorithm with basic projection is 5  2 267 D/B  n 267 3  2 267 D/B 2 The recurrence structure of basicdiskmine is shown in Figure 1 The reader should ignore nodes in the shaded area at this point they represent processing in main memory 001 001 001\002 001 001\003 001 001\004 001 001\005 006 001\004 006 001\005 006 001\002 006 002\001\003 006 002\001\004 006 002\001\005 006 002\003\001\004 006 002\003\001\005 006 002\004\001\005 006 002\003\004\001\005 006 003\001\004 006 003\004\001\005 006 003\001\005  006 004\001\005 006 001\003 Figure 1 Recurrence of Basic Projection Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


In a typical application n  the average number of frequent items could be hundreds or thousands It therefore makes sense to devise a smarter projection strategy Before we go further e introduce some de\036nitions and a lemma De\336nition 4 Let D 001 be an I database and let freqstring  D 001  004 1 004 2  267\267\267 004 k  where each 004 j is a string in I 002  We call 004 1 004 2  267\267\267 004 k a grouping of freqstring  D 001   Let 004 j  i j 1 i j m  for j 005 1 k   We now de\036ne D 001.\003 j   001 006 i 1 1 i j m  001 005D 001 001 006 i j 1 i j m  t  n  In D 001.\003 j  items in 004 j are called master items  items in 004 1 004 j 212 1 are called slave items  r the previous example freqstring  D 001  acbd  004 1  ac  004 2  bd s the grouping ac.bd of acbd  Now D bd   a c d    b c d    a b  and D ac   a c    c    a    a c   De\336nition 5 Let  003 004 013 I 002  and let D 001.\003 be an I database Then freqsets  D 001.\003  denotes the subsets of I that contain at least one item in 004 and are frequent in D 001.\003  Lemma 2 Let 003 005 I 002  D 001 be an I database and freqstring  D 001  004 1 004 2 267\267\267 004 k  Then freqsets  D 001  001 j 003 1 k  freqsets  D 001.\003 j  By following the above example we can get freqsets  D bd   d    b    c d   and freqsets  D ac   c    a    a c   Based on Lemma 2 we can obtain a more aggressive divide-and-conquer algorithm for mining from disks The following shows the algorithm ressivediskmine  Here freqstring  D 001  is decomposed into several substrings 004 j  each of which could have more than one item Each substring corresponds to a projected database A transaction 001 in D 001 will be partly inserted into D 001.\003 j if and only if 001 contains at least one item a in 004 j  Since there will be fewer projected databases there will be fewer disk I/O\222s Compared with the algorithm basicdiskmine  we can expect that a large amount of disk I/O will be saved y the algorithm ressivediskmine  Procedure ressivediskmine  D 001 M  if D 001 002 M then return mainmine D 001  else let freqstring  D 001  004 1 004 2 267\267\267 004 k  return ressivediskmine  D 001.\003 1 M  007  007 ressivediskmine  D 001.\003 k M   Let\222s analyze the recurrence and disk I/O\222s f the aggressive divide-and-conquer algorithm The number of passes needed is still 1 003 log c M/T 004\b 2  since grouping items does not change the size of an FP-tree for a projected database r for disk I/O suppose in D 002  each transaction contains on average n frequent items and that we can group them into k groups of equal size Then the n items will be written to the projected databases with total length n/k 2 267 n/k    k 267 n/k  k 1  2 267 n  Total size of all projected databases is  k 1  2 267 D b k 2 267 D  The total disk I/O\222s for the aggressive divide-and-conquer algorithm is then 5  2 267 D/B  k 267 3  2 267 D/B 3 The recurrence structure of algorithm aggressivediskmine is shown in Figure 2 Compared to Figure 1 we can see that the part of the tree that corresponds to decomposition the nonshaded part is much smaller in Figure 2 Although the example is very small it exhibits the general structure of the two trees 001 001 002\003\004 001 002\005\006 001 002\004 001 002\005 001 002\006 001 002\003 001 003\002\004 001 003\002\005 001 003\002\006 001 003\004\002\005 001 003\004\002\006 001 003\005\002\006 001 003\004\005\002\006 001 004\002\005 001 004\005\002\006 001 004\002\006  001 005\002\006 001 002\003\004 001 002\005\006 Figure 2 Recurrence of Aggressive Projection If k f n  we can expect the aggressive divide-and-conquer algorithm will signi\036cantly outperform the basic one 3 Algorithm Diskmine The algorithm Diskmine is shown below In the algorithm D 001 is the original database or a projected database and M is the maximal size of main memory that can be used by Diskmine  Procedure Diskmine  D 001 M  scan D 001 and compute freqstring  D 001  call trialmainmine  D 001 M  if trialmainmine  D 001 M  aborted then compute a grouping 004 1 004 2 267\267\267 004 k of freqstring  D 001  Decompose D 001 into D 001.\003 1  D 001.\003 k for j=1 to k do begin if 004 j is a singleton then Diskmine  D 001.\003 j M  else mainmine  D 001.\003 j  end else return freqsets  D 001  Diskmine uses the FP-tree as data structure and FPgrowth  as main memory mining algorithm Since the FP-tree encodes all frequency information of the database we can shift into main memory mining as soon as the FPtree 036ts into main memory Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


Since an FP-tree usually is a igni\036cant compression of the database our Diskmine algorithm begins optimistically by calling trialmainmine  which starts scanning the database and constructing the FP-tree If the tree can be successfully completed and stored in main memory e have reached the bottom l f the recursion and can obtain the frequent itemsets of the database by running FPgrowth on the FP-tree in main memory Procedure trialmainmine  D 001 M  start scanning D 001 and building the FP-tree T 001 in main memory if  T 001  exceeds M then return the incomplete T 001 else call FPgrowth  T 001  and return freqsets  D 001   If at any time during trialmainmine we run out of main memory e abort and return the partially constructed FP-tree and a pointer to where we stopped scanning the database We then resume processing Diskmine  D 001 M  by computing a grouping 004 1 004 k of freqstring  D 001   and then decomposing D 001 into D 001.\003 1  D 001.\003 k  We recursively process each decomposed database D 001.\003 j  During the 036rst l of the recursion some groups 004 j will consist of a single item only f 004 j is a singleton we call Diskmine  otherwise we call mainmine directly since we put several items in a group only when we estimate that the corresponding FP-tree will 036t into main memory In computing the grouping 004 1 004 k we assume that transactions in a very large database are evenly distributed i.e if the size of the FP-tree is n for p  of the database then the size f the FP-tree for whole database is n/p 267 100  Most of the time this gives n overestimation since an FPtree increases fast only at the beginning stage when items are encountered for the 036rst time and inserted into the tree In the later stages the changes to the FP-tree will be mostly counter updates Procedure mainmine  D 001.\003  build a modi\036ed FP-tree T 001.\003 for D 001.\003 for each i in 004 do begin construct the FP-tree T 001.i for D 001.i from T 001.\003 call FPgrowth  T 001.i  and return freqsets  D 001.i   end In basicdiskmine  since there is only one master item in each projected database for D 002  no master item at all an FP-tree can be constructed without considering the master item In procedure mainmine  since D 001.\003 is for multiple master items the FP-tree constructed from D 001.\003 has to contain those master items r the item order is a problem for the FP-tree because we only want to mine all frequent itemsets that contain master items To solve this problem we simply use the item order in the partial FP-tree returned by the aborted trialmainmine  D 001   This is what we mean by a 223modi\036ed FP-tree\224 on the 036rst line in the algorithm mainmine  The entire recurrence structure of Diskmine can be seen in Figure 2 Compared to the basic projection in Figure 1 we see that since the aggressive projection uses main memory more effective and that the decomposition phase is shorter resulting in fewer disk I/O\222s In Figure 2 the shaded area shows the recursive structure of FP-growth Comparing with the shaded area in Figure 1 which shows the recursive structure of the FPgrowth method we can see that the main difference is the extra shaded l in Figure 2 This l is for the FP-trees of groups r each group since the total size of all FPtrees for its master items may be greater than the size of main memory a 223modi\036ed FP-tree\224 is constructed This FP-tree will 036t in main memory From the FP-tree smaller FP-trees can be constructed one by one as shown in both 036gures As an example in Figure 1 basicdiskmine enters the main memory phase for instance for the conditional database D 002.a  Then FP-growth 036rst constructs the FP-tree T 002.a from D 002.a in Figure 2 T 002.a is constructed from T 002.ab  The tree rooted at T 002.a shows the recursive structure of FPgrowth assuming for simplicity that the relative frequency remains the same in all conditional pattern bases Theorem 1 Diskmine  D  returns freqsets  D   Applying the FP-array Technique In Diskmine  the Frequent Pairs Array FP-array technique developed for FPgrowth is also applied t o s a v e one tree tra v ersal for each recursive call Furthermore when projected databases are generated the FP-array technique can save a great number of disk I/O\222s Recall that in trialmainmine  if n FP-tree can not be accommodated in main memory the construction stops Suppose now e decided to stop scanning the database Then later after generating all projected databases two database scans are required to construct an FP-tree from a projected database To save one scan in Diskmine we calculate an FP-array for each FP-tree When constructing the FP-tree from D 001  f it is found that the tree can not 036t in main memory the construction of the FP-tree T 001 stops t the scan of the database D 001 continues so that we 036nish 036lling the cells of the array A 001  Later only one database scan is needed to construct an FP-tree from a projected database because of the existence of the array A 001  Grouping items In Diskmine  the fourth line computes a grouping 004 1 004 2 267\267\267 004 k of freqstring  D 001   For each 004 a new projected database D 001.\003 will be computed from D 001  then written to disk and read from disk later Therefore the more groups the more disk I/O\222s In other words there should be as many items in each 004 as possible To group items two questions have to be answered 1 If 004 currently only has one item i j  after projection is the main memory big enough for accommodating T 001.i j constructed from D 001.i j and running the FPgrowth method on T 001.i j  2 If more items are put in 004  after projection is the main memory big enough for accommodating T 001.\003 constructed from D 001.\003 and running FPgrowth on T 001.\003 only for items in 004  Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


To answer the questions algorithm Diskmine collects statistics on the partial FP-tree T 001 and the rest of database D 001  r the 036rst question for each item i j  by counting the number of nodes in the FP-tree T 001.i j constructed from the partial FP-tree T 001  we can use the number to estimate the size of FP-tree T 001.i j constructed from D 001  We write the number as 265  j  T 001  for each i j  and suppose the number of transactions in D 001 is t  D 001  and the number of transactions used for constructing the partial FP-tree T 001 is t  T 001   y the assumption that the transactions in D 001 are evenly distributed and that the partial T 001 represents the whole FP-tree for D 001  the estimated size of FP-tree T 001.i j is 265  j  T 001  267 t  D 001  t  T 001   Before answering the second question we introduce the cut point from which the 036rst group can be easily found Finding the cut point Notice that in FPgrowth  when mining frequent itemsets for i k  all frequency information about i k 1 i n is useless Thus though a complete FPtree T 001 constructed from D 001 could not 036t in main memory we can 036nd many k 222s such that the trimmed FP-tree containing only nodes for items i k i 1 will 036t into main memory All frequent itemsets for i k i 1 can be then mined from one trimmed tree We call the biggest of such k 222s the cut point  t this point main memory is big enough for storing the FP-tree containing only i k i 1  and there is also enough main memory for running FPgrowth on the tree Obviously if the cut point k can be found items i k i 1 can be grouped together Only one projected database is needed for i k i 1  There are two ways to estimate the cut point One y is to get cut point from the value of t  D 001  and t  T 001   Figure 3 illustrates the intuition behind the cut point In the 036gure l  t  T 001   and m  t  D 001   Since the partial FP-tree for t  T 001  of t  D 001  transactions can be accommodate in main memory we can expect that the FP-tree containing i k i 1  where k  r n 267 t  T 001  t  D 001  016  also will 036t in main memory  001 1 001 2 i 1 i 2 i k i n 001 l 001 m  Figure 3 Cut Point The above method works well for many databases especially for those databases whose corresponding FP-trees have plenty of sharing of pre\036xes for items from i 1 to the cut point r if the FP-tree constructed from a database does not share pre\036xes that much the estimation could fail since now the FP-tree for items from i 1 to the cut point could be too big Thus we have to consider another method Let 005  j  T 001  be the size of the FP-tree after the partial FP-tree T 001 is trimmed and only contains items i 1 i j  Based on 005  j  T 001  the number of nodes in the complete FP-tree for item i j can be estimated as 005  j  T 001  267 t  D 001  t  T 001   ow suppose 005  T 001  is the number of nodes in T 001  036nding the cut point becomes 036nding the biggest k such that 005  k  T 001  267 t  D 001  t  T 001  002 005  T 001   and 005  k 1  T 001  267 t  D 001  t  T 001  005  T 001   Sometimes the above estimation only guarantees that the main memory is big enough for the FP-tree which contains all items between i 1 and the cut point while it does not guarantee that the descendant trees from that FP-tree can 036t in main memory This is because the estimation does not consider the size of descendant trees correctly Actually from 265  j  T 001  we can get a more accurate estimation of the size of the biggest descendant tree To 036nd the cut point we need to 036nd the biggest k  such that  005  k  T 001  265  j  T 001  267 t  D 001  t  T 001  002 005  T 001  and  005  k 1  T 001  265  m  T 001  005  T 001   where j 002 k  265  j  T 001  max j 003 1 k  265  j  T 001   and m 002 k 1  265  m  T 001  max m 003 1 k 1  265  m  T 001   Grouping the rest of the items Now e answer the second question how to put more items into a group Here we still need 265  j  T 001   Starting with 265  cutpoint 1  T 001   we test if 265  cutpoint 1  T 001  267 t  D 001  t  T 001  005  T 001   If not we put next item cutpoint+2 into the group and test if  265  cutpoint 1  T 001  265  cutpoint 2  T 001  267 t  D 001  t  T 001  005  T 001   We repeatedly put next item in freqstring  D  into the group until we reach an item i j  such that j 002 m  cutpoint 1 265  m  T 001  267 t  D 001  t  T 001  005  T 001   Then starting from i j  e put items into next group until all items 036nd its group Why can we put items i j i k together into group 004  This is because even if we construct T 001.i j T 001.i k from the projected databases D 001.i j  D 001.i k and put all of them into main memory the main memory is big enough according to the grouping condition At this stage T 001.i j T 001.i k all can be constructed by scanning D 001 once Then we mine frequent itemsets from the FP-trees However we can do better Obviously T 001.i j T 001.i k overlap a lot and the total size of the trees is de\036nitely greater than the size of T 001.\003  t also means that we can put more items into each 004  only if the size of T 001.\003 is estimated to 036t in main memory To estimate the size of T 001.\003  part of T 001 has to be traversed by following the links for the master items in T 001  The disk I/O\325s Let\222s re-count the disk I/O\222s used in Diskmine  The 036rst scan is still for obtaining all frequent items in D 002  and it needs D/B disk I/O\222s In the second scan we construct a partial FP-tree T 002  then continue scanning the rest database for statistics The second scan is a full scan which needs another D/B disk I/O\222s Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


Suppose then that k projected databases have to be computed According to Section 2 the total size of the projected databases is approximately k 2 267 D  For computing the projected databases the frequency information in T 002 is reused so only part of D 002 is read We assume on average half of D 002 is read at this stage which means 1  2 267 D/B disk I/O\222s By using of the FP-array technique writing and later reading k projected databases now only take 2 267 k 2 267 D/B  k 267 D/B disk I/O\222s Suppose all frequent itemsets can be mined from the projected databases without going to the third level Then the total disk I/O\222s is 5  2 267 D/B  k 267 D/B 4 Compared with formula 3 Diskmine s at least k 2 267 D/B disk I/O\222s thanks to the various techniques used in the algorithm 4 Performance Study In this section we present the results from a performance comparison of Diskmine with the Parallel Projection algorithm in and the Partitioning algorithm in The scalability of Diskmine is also analyzed and the accurateness of our memory size estimations are validated As mentioned in Section 2 the Parallel Projection algorithm is a basic divide-and-conquer algorithm since for each item a projected database is created r performance comparison we implemented Parallel Projection algorithm by using FP-growth as main memory method as introduced in The P artitioning algorithm is also a divide-and-conquer algorithm We implemented the partitioning algorithm by using the Apriori implementation 1  We chose this implementation since it s well written and easy to adapt for our purposes We ran the three algorithms n both synthetic datasets and real datasets Some synthetic datasets have millions of transactions and the size of the datasets ranges from several megabytes to several hundreds gigabytes Due to lack of space only the results for some synthetic datasets and a real dataset are shown here All experiments were performed on a 2.0Ghz Pentium 4 with 256 MB of memory under Windows XP For Diskmine and the Parallel Projection algorithm the size of the main memory is n s n input r the Partitioning algorithm since it only has two database scans and each mainmemory-sized partition and all data structures for Apriori are stored into main memory the size of main memory is not controlled and only the running time is recorded We 036rst compared the performance of three algorithms on synthetic dataset Dataset T100I20D100K s generated from the benchmark application of IBM research center 2  The dataset has 100,000 transactions and 1000 items and occupies about 40 megabytes of memory The average transaction length is 100 and the average pattern length is  1 www.cs.helsinki.fi/u/goethals/software 2 www.almaden.ibm.com/software/quest/Resources 20 The dataset is very sparse and FP-tree constructed from the dataset is bushy For Apriori a large number of candidate frequent itemsets will be generated from the dataset T100I20D100K 10 100 1000 10000 123456789 Minimum Support Ti me  s  10 100 1000 10000 Basic Disk I/O Aggr. Disk I/O a Time for Disk I/O\222s T100I20D100K 10 100 1000 1098765432 Minimum Support Ti me  s  10 100 1000 Aggr. CPU Basic CPU b CPU time Figure 4 Experiments on synthetic dataset When running the algorithms the main memory size was given as 128 megabytes Figure 4 shows the experimental results In the 036gure 223Basic\224 represents the Parallel Projection algorithm and 224 represents the Diskmine algorithm Since the Partitioning algorithm is the slowest in the group its total running time is always an order of magnitude greater than the Basic algorithm and the Aggressive algorithm we didn\222t separate its CPU time and the time for disk I/O\222s Consequently the lines for Partitioning algorithm are not shown in the 036gures From Figure 4 a as expected we can see that the disk I/O time of the Aggressive algorithm is orders of magnitude smaller than that of the Basic algorithm On the other hand in Figure 4 b we can see that the Basic algorithm r is not slower than the Aggressive algorithm if we only compare their CPU time In where we were concerned with main memory mining we found that if a dataset is sparse the boosted FPgrowth method has a much better performance than the original FP-growth  The reason here the CPU time of the Aggressive algorithm is not always less than that of Basic algorithm is that the Aggressive algorithm has to spend CPU time on calculating statistics r from Figure 4 we also can see that the CPU overhead used by the Aggressive algorithm now become insigni\036cant compared to the savings in disk I/O We then ran the algorithms n a real dataset Kosarak  which is used as a test dataset in The dataset is about 40 megabytes Since it is a dense dataset and its FP-tree is fairly small we set the main memory size as 16 megabytes for the experiments Results are shown in Figure 5 Kosarak 10 100 1000 123456789 Minimum Support Ti me  s  10 100 1000 Basic Disk I/O Aggr. Disk I/O a Time for Disk I/O\222s Kosarak 1 10 100 0.5 0.45 0.4 0.35 0.3 0.25 2 0.15 0.1 Minimum Support Ti me  s  1 10 100 Aggr. CPU Basic CPU b CPU time Figure 5 Experiments on real dataset Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


In Figure 5 for the same reason as  results for the Partitioning algorithm is not shown It is still the slowest comparing the total running time This is because it generates too many candidate frequent itemsets from the dense dataset Together with the data structures the candidate sets use up main memory and virtual memory s used In Figure 5 a the time used for disk I/O\222s of the Aggressive algorithm is still remarkably less than the time used for disk I/O\222s of the Basic Algorithm We can again notice that the CPU time of the Basic Algorithm is less than that of the Aggressive algorithm This is because Kosarak is a dense dataset so the FP-array technique does not help a lot In addition calculating the statistics takes an amount of time To test the effectiveness of the techniques for grouping items we run Diskmine on T100I20D100K and see how close the estimation of the FP-tree size for each group is to its real size We still set the main memory size as 128 megabytes the minimum support is 2 When generating the projected databases items were grouped into 7 groups the total number of frequent items is 826 As we can see from Figure 6 a in all groups the estimated size is always slightly larger than the real size Compared with the Basic Algorithm which constructs an FP-tree for each item from its projected database the Aggressive algorithm almost fully uses the main memory for each group to construct an FP-tree                 Estimation size vs. Real size 0 20 40 60 80 100 120 140 160 1234567 Group Memo ry M eg ab yt es  Estimated size  Real size a   Scalability 0 100 200 300 400 500 600 700 200 400 600 800 1000 1200 1400 1600 1800 2000 NO. of Transactions \(k Time s  CPU Disk I/O b Figure 6 Estimation Accuracy and Scalability of Diskmine As a ivide-and-conquer algorithm one f the most important properties of Diskmine is its good scalability We ran Diskmine on a set of synthetic datasets In all datasets the item number s set as 10000 items the average transaction length as 100 and the average pattern length as 20 The number of the transactions in the datasets ried from 200,000 to 2,000,000 Datasets size ranges from 100 megabytes to 1 gigabyte Minimum support was set as 1.5 and the available main memory was 128 megabytes Figure 6 b shows the results In the 036gure the CPU and the disk I/O time is always kept in a small range of acceptable values Even for the datasets with 2 million transactions the total running time is less than 1000 seconds Extrapolating from these 036gures using formula 4 we can conclude that a dataset the size of the Library of Congress collection 25 Terabytes could be mined in around 18 hours with current technology 5 Conclusions We have investigated several divide-and-conquer algorithms for mining frequent itemset from secondary memory We also analyzed the recurrences and disk I/O\222s of all algorithms We then gave a detailed divide-and-conquer algorithm which almost fully uses the limited main memory and saves a numerous number of disk I/O\222s We introduced many l techniques used in our algorithm Our experimental results show that our algorithm successfully reduces the number of disk access sometimes by orders of magnitude and that our algorithm scales up to terabytes of data The experiments also validate that the estimation techniques used in our algorithm are accurate Future extensions of this work will include mining maximal and closed frequent itemsets as well as exploring disk layout for various datastructures for instance for candidate sets since there are some situations where Apriori indeed outperforms the FP-tree based methods References  R C Agarw al C C Aggarw al and V  V  V  Prasad Depth 036rst generation of long patterns In KDD\32500  pages 108\226 118 2000  R Agra w al T  Imielinski and A N Sw ami Mining association rules between sets of items in large databases In ACM SIGMOD\32593  pages 207\226216 Washington D.C 1993  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB\32594  pages 487\226499 1994  R Agra w a l and R Srikant Mining sequential patterns In ICDE\32595  pages 3\22614 1995  B Goethals and M J Zaki Adv ances in frequent itemset mining implementations Introduction to 036mi03 In Prodeeding of the 1st IEEE ICDM Workshop on Frequent Itemset Mining Implementations FIMI\32503  ov 2003  G Grahne and J Zhu Ef 036ciently using pre\036x-trees in mining frequent itemsets In 1st IEEE ICDM Workshop on Frequent Itemset Mining Implementations FIMI\32503 Nov 2003  J Han J Pei Y  Y in and R Mao Mining frequent patterns without candidate generation A frequent-pattern tree approach Data Mining and Knowledge Discovery  8:53\226 87 2004  M Kamber  J  Han and J Chiang Metarule-guided mining of multi-dimensional association rules using data cubes In Knowledge Discovery and Data Mining  pages 207\226210 1997  H Mannila H T o i v onen and A  I  V erkamo Disco v ery of frequent episodes in event sequences Data Mining and Knowledge Discovery  1\(3 1997  A Sa v asere E Omiecinski and S B Na v athe An ef 036cient algorithm for mining association rules in large databases In VLDB\32595  pages 432\226444 1995  H T o i v onen Sampling lar ge databases for association rules In VLDB\32596  pages 134\226145 Sep 1996  M Zaki and K Gouda F ast v ertical mining using dif fsets In ACM SIGKDD\32503  Washington DC Aug 2003 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


are computed from the transactions without taking into account the other attributes When multiple mdis are obtained, one of them is focused and the transactions in the mdi is retained. Next, the identiProceedings of the The 2005 Symposium on Applications and the Internet Workshops \(SAINT-W  05 0-7695-2263-7/05 $20.00  2005 IEEE Figure 2. Trie data structure Figure 3. Time complexity cal process is applied to   and this recursively continues in depth ?rst search \(DFS   is computed, the process continues again from  until the mdi of every              c o n v e r g e s   T h e  m d i s  a l w a y s  c o n v e r g e to these of the mdr  because the denseness is a MINT measure. After the convergence, the search is backtracked to the next mdr  The computation of mdis in each step requires       t i m e  a t  m o s t   I n  t h e  w o r s t  c a s e   o n l y  o n e t r a n s a c t i o n  i s  d r o p p e d  i n  e a c h  s t e p   a n d    s t e p s  r e q u i r e d until the mdis converge. Thus         H o w e v e r   t h i s  d o e s not likely occur. Practically, only a portion of the transact i o n s  a r e  r e t a i n e d  i n  e a c h  s t e p   L e t          b e  a n  e x pected rate of transactions retained in each step the required steps for convergence. The process to search an mdr stops at the latest when the number of retained transa c t i o n s     b e c o m e s  l e s s  t h a n   By solving the equation       w i t h   is        A c cordingly, the expected time complexity of this most expensive process is         4. Performance Evaluation The performance of QARMINT has been evaluated through both arti?cial data and real bench mark data Sets of arti?cial data have been generated under various conditions. The characteristics of the computation time is simlilar to the conventional Basket Analysis except for       T h e  t i m e  m o d e r a t e l y  i n c r e a s e s  w h e n s of all attributes are increased. This is because wider permissible ranges increases the number of mdrs. Figure 3 shows the dependency of the computation time on the n u m b e r  o f  t r a n s a c t i o n      T h e  c u r v e  a l m o s t  f o l l o w s  t h e  r e lation         The real bench mark data  Labor relations Database  in UCI Machine Learning Repository [3] was analyzed by QARMINT. It contains 57 instances, 8 numeric attributes and 8 categorical attributes and many missing values. We ignored the attributes of missing values in each instance and transformed the data into transactions. Though the size 


and transformed the data into transactions. Though the size of this data is quite small, we found many interesting QARs associated with the labor conditions under      and      w h i c h  i s  1 0   o f  t h e  m a x i m u m  a n d  m i n i mum values of each  in the data. The following two are examples                                                                                                                  These rules indicate that the workers having longer durat i o n  c o n t r a c t s  a n d  e v a l u a t i n g  t h e i r  l a b o r  c o n d i t i o n  a s   admit longer working times and less wage increase. These evaluations indicate the suf?cient tractability and the practical applicability of QARMINT 5. Conclusion The mathematical characterization and the extension of the Basket Analysis presented in this paper are expected to provide variety of new approaches of data mining. Their potential has demonstrated by a novel approach called QARMINT for complete mining of generic QARs within a low time complexity. We are implementing QARMINT in a more ef?cient algorithm and evaluating its performance in near future Acknowledgement This research has conducted under the support of JSPS Kiban Kenkyuu \(B 2 References 1] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. Proc. of 20th Int. Conf. on Very Large Data Bases VLDB  499, 1994 2] R. Srikant and R. Agrawal. Mining quantitative association rules in large relational tables. Proc. of 1996 ACM SIGMOD Int. Conf. on Management of Data, pages 1  12, 1996 3] U. C. I. \(UCI http://www.ics.uci.edu/ mlearn/MLRepository.html, 2004 4] J. Wijsen and R. Meersman. On the complexity of mining quantitative association rules. Data Mining and Knowledge Discovery, 2\(3  281, September 1998 Proceedings of the The 2005 Symposium on Applications and the Internet Workshops \(SAINT-W  05 0-7695-2263-7/05 $20.00  2005 IEEE 


0-7695-2263-7/05 $20.00  2005 IEEE pre></body></html 


n M L N n t n t n t n t L M L t L t L tt L t kkkk kkkk kkk kkkk kkkkkkkkP VK VK VK VK PP       kkP t 31 where L  s the error covariance associated with the state estimate t i    kkLX  tt kkk P1  00 0  0                     s s sss s s sss s s sssss N n t n t n 


n t n N n t n t n t n N n t n t n t n t n t n c t L kkkkkk kkkkk kP VKVK VKVK  32 4. Simulations One has run simulations comparing the sequential implementations of MSJPDA algorithm and the new algorithm here. A typical multisensor multitarget tracking environment is assumed in the simulations. According to article [1,3], One known that the performance of sequential MSJPDA is better than the performance of parallel MSJPDA. Therefore, the performance of parallel MSJPDA algorithm will not be compared here There are three sensors, which are fixed in three platforms. Regarding the 2nd sensor as fusion centre situation of the other sensors are: =?-500m?-500m 0m??N =?-500m? 500m?0m??The distance error of each sensor is: =300m, =200m, =100m?The bear error of each sensor is 0.03rad, =0.02rad, =0.01rad?The of sample is T=1s?The nonparametric model of clutter is used in the simulations, and expected number of false measurement is m=1.8 1 sN 3 s 1r 2 2r 3 3r 1 Simulations have been run for racking two targets. The true initialization state of the targets is X1?[-29500m,400m/s,34500m,-400m/s X ?[-26250m,296m/s,34500m,-400m/s]'? 2 The two targets will cross above 31seconds later. To evaluate tracking performance, 50 Monte Carlo runs were performed for three case of the target detection probability Pd=0.97 ? Pd=0.76 ? Pd=0.58. In every run, the total simulation time is 140 steps 


simulation time is 140 steps            Figure 1  RMS position error in case of Pd=0.97          Figure 2  RMS velocity error in case of Pd=0.97       Figure 3  RMS position error in case of Pd=0.76 567 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005         Figure 4  RMS velocity error in case of Pd=0.76         Figure 5  RMS position error in case of Pd=0.58          Figure 6  RMS velocity error in case of Pd=0.58  Table 1 The emanative times comparison for sequential MSJPDA and SD-CMSJPDA algorithm  Pd N A  0.97 0. 76 0.58 Sequential MSJPDA 2 11 17 SD-CMSJPDA 0 3 5 Pd denotes detection probability, N denotes emanative 


Pd denotes detection probability, N denotes emanative times, A denotes the kind of algorithm Table 1 shows the summation of emanative times for sequential MSJPDA and SD-CMSJPDA algorithm in 50 Monte Carlo simulations. From table 1 , it is shown that the stability of SD-CMSJPDA is better than that of sequential MSJPDA as the detection probability varied Figure 1,2 show the RMS errors for position and velocity in case of Pd=0. 97, respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.76 respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.58, respectively. From the figures we can see that the average RMS position error is lower for the SD-CMS JPDA algorithm. We also see that the state estimation precision of sequential MSJPDA get worse as the detection probability decreases The reasons for these simulation results lies:1 state estimation precision will get worse when the detection probability decrease;2 algorithm is to process measurement from each sensor using single sensor JPDA algorithm sequentially. Therefore the estimation error from each sensor will be accumulated Moreover, the sequential MSJPDA algorithm can  t improve the joint detection probability of the multisensor system The estimation error of the SD- CMSJPDA  algorithm will not be accumulated for it processes the measurement from each sensor directly in the mean time .What  s more the new method can greatly improve the joint detection probability of the multisensor system. Therefore, the tracking performance of SD-CMSJPDA algorithm is better than that of sequential MSJPDA. Algorithm All of the simulations are run in the personal computer with a 2.0G CPU and a 256M memory. The average cost time per step is 0.0251 in the sequential implementations of MSJPDA algorithm. And the average cost time per step is 0.0282 in the sequential implementations of MSJPDA algorithm. According to the results we can see that there is few difference in real time between the new method and the sequential   MSJPDA when there is not so many sensors and targets 568 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  5. Conclusion In order to solve the problem of multisensor multi target tracking, a new centralized multisensor  joint probabilistic data association  algorithm is proposed in this paper. The simulation results shows that the tracking performance of the new algorithm is better than that of the sequential MSJPDA algorithm The computational complexity of the new method will increase as the number of sensors and targets grow Therefore, how to improve the real time of SD- CMSJPDA algorithm will be pay attention References 1] He You, Wang Guohong, Lu Dajin, Peng Yingning Multisensor Information Fusion With Application[M Publishion House of Electronics Industry. 2000, Beijing.  [11] B..Zhou and N.K.Bose Multitarget  Tracking in Clutter:Faste Algorithms for Data Association .IEEE Transaction on Aerospace and Electronic Systems 1993,29\(2 2] Bar-shalom,Y\(Ed Applications and Advances,2: Norwood,MA Artech  House, 1992 3] L.Y. Pao, C.W.Frei. A Comparison of Parallel and Sequential Implementation of a Multisensor Multitarget Tracking Algorithm. Proc. 1995 American Control Conf. Seattie, Washington,June 1995 1683~1687 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





