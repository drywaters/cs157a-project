A Multi-objective GeneticFuzzy Mining Algorithm   Chun-Hao Chen 1 Tzung-Pei Hong 2,3 Vincent S. Tseng 1 and Lien-Chin Chen 1   1 Department of Computer Science and Information Engineering National Cheng-Kung University, Tainan, 701, Taiwan  2 Department of Computer Science and Information Engineering National University of Kaohsiung Kaohsiung, 811, Taiwan 3 Department of Computer Science and Engineering National Sun Yat-sen University, Kaohsiung, 804, Taiwan chchen, tsengsm, cljimmy}@idb.c sie.ncku.edu.tw, tphong@nuk.edu.tw    Abstract  In this paper, we propose a multi-objective genetic 
fuzzy mining algorithm for extracting both membership functions and associati on rules from quantitative transactions. Two objective functions are used to find the Pareto front. The first one is the suitability of membership functions. It consists of two factors coverage factor and overlap factor, to avoid two bad types of membership functions. The second one is the total number of large 1-itemsets from a given set of minimum support values. The two criteria have a trade-off relationship. Experimental results also show the effectiveness of the proposed approach in finding 
the Pareto-front membership functions  1. Introduction  Data mining is commonly used in attempts to induce association rules from transaction data. An association rule is an expression X Y where X is a set of items and Y is a single item r eal application s dif f e rent  items, however, may have different criteria to judge their importance and quantitative data may exist. Fuzzy data mining approaches can be divided into two types 
respectively for solving Single-minimum-Support Fuzzy-Mining  SSFM 2, 8, 13 d Multipleminimum-Support Fuzzy-Mining  MSFM   problems In fuzzy data mining, the membership functions have a critical influence on the final mining results Besides, several criteria may be considered in a real application. The multi-objective evolutionary algorithms, that are used to find a set of solutions with trade-offs among different criteria, are thus very 
suitable for solving such a task [4, 5 Kay a  et al thus proposed an approach that integrated the multiobjective genetic algorithm into clustering for fuzzy mining T h e n u m b er of larg e ite m s ets a n d th e  spent execution time were considered as two objective functions to derive appropriate membership functions for mining fuzzy association rules. Besides, Kaya also proposed an approach based on multi-objective genetic algorithms for mining opt imized fuzzy association 
rules e def i n e d t h ree ob j ect i v es n a m e l y  strongness, interestingness and comprehensibility, to derive appropriate membership functions for mining optimized fuzzy association rules In the past, we proposed a genetic-fuzzy datamining algorithm for extracting both association rules and membership functions from quantitative transactions [7 itn ess fu n c tio n  w a s ev a l u a ted b y  the number of large 1-itemsets over the suitability of membership functions. The suitability measure was 
used to reduce the occurrence of bad types of membership functions. The two criteria, the number of large 1-itemsets and the suitability of membership functions, also have a trade-off relationship. In this paper, we thus propose a multi-objective genetic-fuzzy mining approach to find the Pareto solutions based on the two objective functions for deriving membership functions for the SSFM problem. Experimental results also show the effectiveness of the proposed algorithm 
 2. GA-based multi-obj ective optimization problems  In traditional optimization problems, some certain goals are to be achieved and are usually transformed into fitness functions for maximization or minimization. Unfortunately, it is not easy to find the best fitness function for a problem in advance. Besides 


several criteria may be consider ed in a real application such that multi-objective optimization problems become more and more important. Formally, a multiobjective optimization problem can be defined as follows Min/Max y g x  g 1  x  2  x  m  x  subject to x  x 1  x 2  x n    X and y  y 1  y 2  y m    Y  where x is the decision vector y is the objective vector X represents the decision space, and Y represents the objective space. In the past, several GA-based approaches were proposed to get the solutions. For example, Schaffer first proposed the Vector Evaluated Genetic Algorithm \(VEGA\ solve the multiobjective optimization problem s eca et al pointed out that VEGA had two problems T h e f i rs t  one was that two non-dominated individuals were sampled at different rates. The second one was that the population would tend to split into different species They thus proposed a modified approach called MultiObjective Genetic Algorithm \(MOGA\ by using the extended rank-based fitness assignment o r s o lv i n g the above two problems. They also defined three relationships among chromosomes, namely inferiority superiority and non-inferiority. The MOGA strategy was thus proposed to find the set of non-inferiority solutions, also called Pareto optimal solutions or Pareto front. Figure 1 explains the three relationships and the Pareto optimal solutions  Pareto-Optimal Solutions Objective function 1 Objective function 2 c 1 c 2 c 3 c 4 c 5 c 6 c 7 c 8 c 9 c 10 good bad bad good Pareto-Optimal Solutions Objective function 1 Objective function 2 c 1 c 2 c 3 c 4 c 5 c 6 c 7 c 8 c 9 c 10 good bad bad good  Figure 1. An example for the Pareto optimal solutions In Figure 1, there are ten chromosomes and two objectives. The two objective values of a chromosome are represented by a data point in the figure. Take chromosomes C 1 and C 2 as an example. The chromosome C 2 is said to be inferiority to C 1 since both the objective values of C 2 is worse than those of C 1 In this case, we also say that C 2 is dominated by C 1  On the contrary, chromosome C 1 is said to be superiority to C 2 We also say that C 1 dominates C 2  Besides, the chromosome C 1 is said to be noninferiority to C 7 or vice versa. In this case, we also say that both C 1 and C 7 are non-dominated points. The goal of MOGA is thus to find the non-dominated points also called Pareto optimal solutions. In this example the chromosomes C 1  C 7  C 8  C 9 and C 10 are nondominated points  3. The multi-objective genetic-fuzzy mining approach  3.1 Chromosome representation  It is important to encode membership functions as string representation for GAs to be applied. Several possible encoding approaches have been described in 3, 15 In th is p a p e r, th e set o f  m e m b ersh ip f u n c tio n s  for an item is encoded as shown in Figure 2 c j1 w i1 Membership value 1 R j1 Quantity R jk R jl c jk w ik c jl w il R j2 c j2 w i2   c j1 c j1 w i1 w i1 Membership value 1 R j1 Quantity R jk R jl c jk c jk w ik w ik c jl c jl w il w il R j2 c j2 c j2 w i2 w i2   Figure 2. The membership functions for an item I j  In Figure 2, each membership function is assumed to be isosceles-triangle and represented by a pair c  w  with c indicating the center abscissa and w representing half the span R jk denotes the membership function of the k th linguistic term of item  I j All pairs of c  w s for a certain item are concat enated to represent its membership functions. Since both c and w are numeric values, a chromosome is thus encoded as a fixedlength real-number string rather than a bit string  3.2 Initial population  A genetic algorithm requires a population of feasible solutions to be initialized and updated during the evolution process. As mentioned above, each individual within the population is a set of isoscelestriangular membership functions. Each membership function corresponds to a linguistic term in a certain item. The initial set of chromosomes is randomly generated with some constraints for forming feasible membership functions  3.3 The two objective functions  In our previous work, we have also proposed a genetic-fuzzy approach to learn an appropriate set of membership functions for mining problem th at  paper, the fitness values were evaluated by the numbers of large 1-itemsets over the suitability of membership functions. The two factors \(numbers of 


large 1-itemsets and suitability of membership functions\sually show a trade-off relationship. In this paper, we thus consider the mining of membership functions and fuzzy association rules as a multiobjective optimization problem, in which the above two factors are used as two objectives functions. A MOGA-based mining algorithm is thus proposed to find the Pareto optimal solutions. The first objective function Obj 1 for a chromosome C q is defined as follows     1 q q C y suitabilit C Obj   where suitability  C q resents the shape suitability of the membership functions with C q  Suitability  C q s defined as   _   _  1 qj qj m j C factor coverage C factor overlap    where m is the number of items Overlap_factor  C qj  represents the overlap factor of the membership functions for an item I j  in the chromosome C q and is defined as     i k ji jk ji jk qj w w R R overlap C factor overlap  1  1    min    max   _  where overlap\(R jk R ji  is the overlap length of R jk and R ji  Coverage_factor  C qj esents the coverage ratio of a set of membership functions for an item I j  in the chromosome C q and is defined as   max     1   _ 1 j jl j qj I R R range C factor coverage   where range  R j1  R j2  R jl is the coverage range of the membership functions l is the number of membership functions for I j and max\(I j  is the maximum quantity of I j in the transactions. The suitability factor is used to reduce the occurrence of the two bad kinds of membership functions shown in Figure 3, where the first one is too redundant and the second one is too separate 589 Low Middle High Quantity 0  a  52025 Low Middle High Quantity 0  b  589 Low Middle High Quantity 0  a  52025 Low Middle High Quantity 0  b  Figure 3. Two bad membership functions The second objective function is the total number of large 1-itemsets in a given set of minimum support values ms 1  ms 2  ms h It is formally defined as follows        1 1 2    h g ms q q q g L C totalNumL1 C Obj  where   1 g ms q L  is the number of large 1-itemsets obtained when the minimum support value is ms g  Using the number of large 1-itemsets can achieve a trade-off between execution time and rule interestingness. Usually, a larger number of 1-itemsets will result in a larger number of all itemsets with a higher probability, which will thus usually imply more interesting association rules. In this paper, the proposed approach uses the above two objective functions to find appropriate Pareto solutions for the SSFM problems  3.4 Fitness assignment  The fitness assignment is similar to that used in MOGA B a s i call y it con t ain s t h ree s t eps in cl u d ing ranking chromosomes, assigning fitness, and averaging fitness values of the individuals with the same rank For example, the previous ten chromosomes in Figure 1 are ranked according to their two objective values with the results shown in Figure 4 c 3 3 2 c 6 c 4 c 2 2  Objective function 1 Objective function 2 c 1 c 5 c 7 c 8 c 9 c 10 good bad bad good 1 1 1 1 1 2 3 c 3 3 2 c 6 c 4 c 2 2 Objective function 1 Objective function 2 c 1 c 5 c 7 c 8 c 9 c 10 good bad bad good 1 1 1 1 1 2 3  Figure 4. The ranking results of the ten chromosomes In Figure 4, a chromosome with a lower ranking value represents it has a better quality. Besides, the chromosomes with their ranking values at 1 are also called non-dominated solutions. The fitness value of a chromosome is then assigned according to its rank value. For a chromosome C q with its ranking value at 1 its fitness value is assigned by the following formula f  C q  DomonatedBy  C q   P 1 where DominatedBy  C q e number of chromosomes dominated by the chromosome C q and P  is the population size. For a chromosome with its ranking value larger than 1, its fitness value is assigned by the following formula    q p p C dominates C and P C p q C f C f   1    where f  C p the fitness value of the chromosome C p  which dominates chromosome C q The constant value 1 is used here to ensure the fitness value of a dominated chromosome is larger than a non-dominated chromosome. Therefore, a chromosome with a smaller 


fitness value is considered better. For instance, the chromosome C 1 in Figure 4 dominates three chromosomes. Its fitness value is thus 3/11, which is 0.27. In the same way, the fitness values of the chromosomes C 7  C 8  C 9 and C 10 are 0.36, 0.36, 0.27 and 0.18, respectively. The chromosome C 2 is dominated by C 1 and C 7 Its fitness value is thus calculated as 1+0.27+0.36, which is 1.63. The results of other chromosomes are shown in Figure 5 c 3 c 6 c 4 c 2 1.63  Obj 1 Obj 2 c 1 c 5 c 7 c 8 c 9 c 10 good bad bad good 0.27 0.36 0.36 0.27 0.18 1.99 1.81 3.62 6.24 c 3 c 6 c 4 c 2 1.63 Obj 1 Obj 2 c 1 c 5 c 7 c 8 c 9 c 10 good bad bad good 0.27 0.36 0.36 0.27 0.18 1.99 1.81 3.62 6.24  Figure 5. The results of assign fitness of the ten chromosomes There are five non-dominated chromosomes in Figure 5 and their fitness values may be different Since they are all non-dominated, they are assumed to have equal importance to be reproduced in the selection procedure. Therefore, instead of the original fitness values, the average fitness value of the nondominated chromosomes is calculated and assigned to each of them. In this example, the average fitness value of the non-dominated chromosomes is 0.288 0.27+0.36+0.36+ 0.27+0.18\ / 5\ The fitness values for the chromosomes with the same ranks are also calculated in the above way  3.5 Genetic operators  Genetic operators are important to the success of specific GA applications. Two genetic operators, the  max-min-arithmetical \(MMA\ossover proposed in  d th e one-point  mutation are used in the proposed approach. The max-min-arithmetical \(MMA crossover operator will generate four candidate chromosomes from them. The best two chromosomes of the four candidates are then chosen as the offspring The one-point mutation operator will create a new fuzzy membership function by adding a random value  between w jk to w jk    to the center or to the spread of an existing linguistic term, say R jk Assume that c  and w represent the center and the spread of R jk The center or the spread of the newly derived membership function will be changed to c   or w  by the mutation operation. Mutation at the center of a fuzzy membership function may however disrupt the order of the resulting fuzzy membership functions. These fuzzy membership functions then need rearrangement according to their center values. Besides, the selection strategy used in the proposed approach can be the elitist or the roulette-wheel strategy  4. The proposed mining algorithm  According to the above description, the proposed multi-objective genetic-fuzzy algorithm for mining both membership functions and fuzzy association rules is described below The Multi-Objective Genetic-Fuzzy Mining Algorithm  INPUT: A body of n quantitative transactions, a set of m items, each with a number of linguistic terms, a population size P a crossover rate P c  a mutation rate P m a set of h minimum support values, and a confidence threshold   OUTPUT: A set of non-dominated solutions \(sets of membership functions\with their fuzzy association rules STEP 1: Randomly generate a population of P  individuals, with each one being a set of membership functions for all the m items encode each set of membership functions into a string representation according to the schema stated in Section 3, and initialize the non-dominated set NDS as empty STEP 2: For each chromosome C q calculate its two objective values, the suitability  sutiability  C q d the total number of large 1-itemsets in the given set of minimum support values totalNumL1  C q the following substeps SUBSTEP 2.1: For each transaction datum D i  i 1 to n and for each item I j  j 1 to m transfer the quantitative value v j i into a fuzzy set f j i  represented as    jl i jl j i j j i j R f R f R f   2   2 1   1   using the corresponding membership functions represented by the chromosome, where R jk is the k th fuzzy region \(term\ of item I j  f jk i  is v j i s fuzzy membership value in region R jk  and l  I j is the number of linguistic terms for I j  SUBSTEP 2.2: For each item region R jk calculate its scalar cardinality on the transactions as follows   n i i jk jk f count 1    


SUBSTEP 2.3: Calculate the suitability value suitability  C q using the formula defined in Section 3; set it as the first objective value of C q  SUBSTEP 2.4: For each R jk 1   j   m 1   k   I j  and for each minimum support value ms g 1   g    h check whether count jk  is larger than or equal to the minimum support value ms g If R jk  satisfies the above condition, set   1 g ms q L    1 g ms q L 1, where   1 g ms q L  is the number of large 1-itemsets obtained by using the set of membership functions in chromosome C q and the minimum support value ms g let   h g ms q q g L C totalNumL1 1 1     as the second objective value of C q  STEP 3: Rank the chromosomes according to the two objectives suitability  C q d totalNumL1  C q  by the following substeps SUBSTEP 3.1: Set the variable r for representing the current rank, which is initially at 0 SUBSTEP 3.2: Find the non-dominated chromosomes among the un-ranked ones in the population, set r  r 1, and set the ranking values of the non-dominated chromosomes as r  SUBSTEP 3.3: If there still exist un-ranked chromosomes in the population, go to SUBSTEP 3.2; otherwise, do the next step STEP 4: The fitness value of each chromosome is then calculated based on the ranking value by the following substeps SUBSTEP 4.1: Calculate the fitness values of the chromosomes with their ranking values equal to one as follows f  C q  DomonatedBy  C q   P 1 where DominatedBy  C q s the number of chromosomes dominated by the chromosome C q and P is the population size SUBSTEP 4.2: Calculate the fitness values of the chromosomes with their ranking values larger than one as follows    q p p C dominates C and P C p q C f C f   1    where f  C p s the fitness value of chromosome C p which dominates chromosome C q and the constant value 1 is used to ensure the fitness values of dominated chromosomes are larger than those of non-dominated ones STEP 5: Calculate the average fitness values of the chromosomes with the same ranking values such that each of them can be selected equally by the selection strategy STEP 6: Copy the chromosomes with their ranking values equal to one into the non-dominated set NDS and remove the chromosomes which are dominated by other chromosomes in NDS  STEP 7: Execute the crossover operation on the population STEP 8: Execute the mutation operation on the population STEP 9: Calculate the fitness values of the new chromosomes by STEPs 2 to 8 STEP 10: Use the selection operation to choose appropriate individuals from the set of NDS to form the next generation. Here, the selection strategy can be elitist or roulette wheel. If the size of NDS called NDSSize is less than population size PSize all the chromosomes in NDS are copied into next population and the number PSize  NDSSize chromosomes are selected from the difference set of the offspring chromosomes and the current NDS  STEP 11: If the termination criterion is not satisfied go to Step 6; otherwise, do the next step STEP 12: Execute the truncation operator is used on the non-dominated set NDS to find the best k  solutions. Since there may be more than one chromosome kept in NDS the goal of this step is to keep the k representative solutions at the Pareto front. Note that this step is optional STEP 13: Mine fuzzy association rules from the given database and based on the derived chromosomes in NDS or the k representative chromosomes if STEP 12 is applied\, where each chromosome represents a set of membership functions. The fuzzy mining algorithm proposed in i s t h en  adopted to achieve this purpose for each set of membership functions STEP 14: Output the non-dominated set NDS and their corresponding fuzzy association rules  5. Experimental results  In this section, experiments made to show the performance of the proposed approach are described They were implemented in Java on a personal computer with Intel Pentium IV 3.20 GHz and 512 MB RAM. 64 items and 10000 transactions were used in the experiments. The dataset followed the exponential distribution. The initial population size P is set at 50 the crossover rate p c is set at 0.8, and the mutation rate p m is set at 0.001. The parameter d of the crossover operator is set at 0.35 according to Herrera et al s paper an d t h e s e t of  m i n i m u m su pport v a l u es i s  3%, 4%, Ö, 13%}. The experiments were first made for demonstrating the evolution of the Pareto fronts by 


the proposed approach. The evolution of the Pareto fronts along with different generations by the proposed approach is shown in Figure 6             Figure 6. The evolution of the Pareto fronts From Figure 6, it can be observed that the solutions were distributed on the Pareto fronts in different generations. Besides, the final solutions \(after 500 generations\were better than those in other generations. Since the data distribution followed the exponential distribution, the solutions on the Pareto fronts were a little narrow. The proposed approach is thus effective in finding an appropriate set of solutions  6. Conclusion and future works  In this paper, we have proposed a multi-objective genetic-fuzzy mining algorithm for extracting membership functions from quantitative transactions for the SSFM problem. Two objective functions namely suitability and totalNumL1 have been used to find the Pareto-front solutions. Experimental results also show that the proposed approach is effective in finding the Pareto-front solutions. In the future, we will continuously enhance the multi-objective genetic-fuzzy approach for more complex problems, such as for solving the MSFM problem  7. Acknowledgment This research was supported by the National Science Council of the Republic of China under contract NSC 96-2213-E-390-003  8. References   R  A g ra w a l an d R S r i k an t   F ast al go ri t h m f o r m i n i n g  association rules The International Conference on Very Large Databases pp. 487-499, 1994  C  C C h an and W  H A u  M i n i n g f u zzy asso ci at i o n  rules The Conference on Information and Knowledge Management Las Vegas, pp. 209-215, 1997 3 O Co r d  n F. Herrera, an d  P. Villar, ìGenerating the knowledge base of a fuzzy rule-based system by the genetic learning of the data base IEEE Transactions on Fuzzy Systems Vol. 9, No. 4, pp. 667ñ674, 2001 4 C A  C o e llo D  A  Va n Ve l dhu iz e n a n d G  B  L a m ont Evolutionary Algorithms for Solving Multi-objective Problems Kluwer Academic Publishers, 2002 5 K Deb   Multi-objective Optimization Using Evolutionary Algorithms John Wiley & Sons, 2001  C  M  F o n s eca an d  P  J F l em i n g  G en et i c al go ri t h m s  f o r  multiobjective optimization: Formulation, discussion and generalization The International Confidence on Genetic Algorithms pp. 416-423, 1993  T  P  Hon g  C H C h en  Y L   W u an d Y C L e e  A  GA based fuzzy mining approach to achieve a trade-off between number of rules and suitability of membership functions Soft Computing   Vol. 10, No. 11, pp. 1091-1101. 2006 8 T  P  H ong C  S  K u o a n d S  C  C h i   M ini n g a s s o c i a tio n rules from quantitative data Intelligent Data Analysis Vol 3, No. 5, pp. 363-376, 1999 9 T P  H ong C  S. K uo a n d S C  C h i T r a de o f f be t w ee n time complexity and number of rules for fuzzy mining from quantitative data International Journal of Uncertainty Fuzziness and Knowledge-based Systems Vol. 9, No. 5, pp 587-604, 2001 1 F  Herrera  M  L o zan o and J L   V e rd ega y  F u zzy connectives based crossover operators to model genetic algorithms population diversity Fuzzy Sets and Systems Vol. 92, No. 1, pp. 21ñ30, 1997 11 M  Ka y a a nd R A l ha jj I nte g ra ting m u tlti-obje c tiv e  genetic algorithms into clustering for fuzzy association rules mining The IEEE International Conference on Data Mining pp. 431-434, 2004 12 M  Ka y a  M ulti-o b je c tiv e g e ne tic a l g o rithm ba se d approaches for mining optimized fuzzy association rules Soft computing Vol. 10, pp. 578-586, 2006 13 C  K u ok A  Fu a nd M. W ong  M ining f u z z y as s o c i a tion rules in databases SIGMOD Record Vol. 27, No. 1, pp. 4146, 1998 14 Y  C  L e e  T  P  H o n g a n d  W  Y  L i n   M i n i n g f u z z y  association rules with multiple minimum supports using maximum constraints Lecture Notes in Computer Science  Vol. 3214, pp. 1283-1290, 2004 1 H R o ub o s and M   S e t n es Co m p act an d t r an sp aren t  fuzzy models and classifiers through iterative complexity reduction IEEE Transactions on Fuzzy Systems Vol. 9, No 4, pp. 516-524, 2001 16 J. D  Sc ha f f e r M ultiple  o b je c tiv e optim iz a tion w ith vector evaluated genetic algorithms The International Conference on Genetic Algorithms pp. 93-100, 1985  


0 10 20 30 40 50 60 3210.75 minimum support execution time \(minutes GMFI GMAR BASIC Cumulate  Fig. 10 Mining time in DENSE databases  7.  Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log2n\ where n is the total number of maximal itemsets. For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results. Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces, especially in dense databases  7.    Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log 2 n\ where n is the total number of maximal itemsets For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces especially in dense databases 8. References  1 g ra w a l, T  Im ieli n s k i an d A  S w a m i M in i n g  association rules between sets of items in large databases,î Proc. ACM International Conference on Management of Data \(1993\, pp. 207-216 2 g ra w a l an d R. Srik an t F a s t alg o rit h m s f o r  mining association rules,î Proc. 20th International Conference on Very Large Data Bases \(1994\ pp.487499  J i aW e i Han  J i an  P e i an d YiW en Yi n   Mi n i n g frequent patterns without candidate generation,î Proc ACM International Conference on Management of Data 2000\p. 1-12 4 J  S   Pa r k  M  S  C h en  an d P S  Y u     A n ef f e c t iv e hash-based algorithm for mining association rules,î Proc ACM International Conference on Management of Data 1995\p.175-186 5 A  Sa va se r e  E  O m i e c i ns ki  a n d S N a va t h e    A n  efficient algorithm for mining association rules in large databases,î  Proc. 21st International Conference on Very Large Data Bases \(1995\ pp.432-443 6 Y i n-F u H u a ng a n d  Chi e h M i ng W u   M i n i n g  generalized association rules using pruning techniques Proc. IEEE International Conference on Data Mining 2002\p.227-234 7 a k i a n d C.J  Hsia o   E ff icie n t al g o rith m s f o r  mining closed itemsets and their lattice structure, î  IEEE Transactions on Knowledge and Data Engineering, vol 17, no. 4, April \(2005\p. 462-478   Bu rdick  M. C a li m l i m  an d J. Geh r k e  M AF I A a maximal frequent itemset algorithm for transactional databases,î Proc. 17th International Conference on Data Engineering, \(2001\p.443-452  a k i S  P a rth a s a rat h y   M. Ogih ara, a n d W. Li   New algorithms for fast discovery of association rules Proc. 3rd ACM International Conference on Knowledge Discovery in Databases and Data Mining, \(1997\pp 283-286 10 M  J  Za ki a n d K  G o ud a  Fa s t ve r t i c a l  mi ni ng usi n g  diffsets,î Proc. 9th ACM International Conference on Knowledge Discovery and Data Mining, Aug. \(2003   Mam o u l i s D  W. C h e u ng an d W. L i a n    Similarity search in sets and categorical data using the signature tree,î Proc. 19th International Conference on Data Engineering, \(2003 12 R. Srik a n t a n d R Ag ra w a l   M i n i n g g e n e ralized  association rules,î Proc. 21st International Conference on Very Large Data Bases, \(1995\.407-419    
571 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


