SUMMARY 001ciently Summarizing Transactions for Clustering 001 Jianyong Wang and George Karypis Department of Computer Science Digital Technology Center  Army HPC Research Center University of Minnesota Minneapolis MN 55455  jianyong karypis  cs.umn.edu Abstract Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining In recent years several studies have also extended its application to the transaction or document classi\223cation and clustering However most of 
the frequent-itemset based clustering algorithms need to 223rst mine a large intermediate set of frequent itemsets in order to identify a subset of the most promising ones that can be used for clustering In this paper we study how to directly 223nd a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to cluster the categorical data By exploring some properties of the subset of itemsets that we are interested in we proposed several search space pruning methods and designed an e\001cient algo 
rithm called SUMMARY Our empirical results have shown that SUMMARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size and surprisingly as a pure frequent itemset mining algorithm it is very e\002ective in clustering the categorical data and summarizing the dense transaction databases 1 Introduction Frequent itemset mining was initially proposed and has been studied extensively in the context of associa\001 This work was supported in part by NSF CCR-9972519 
EIA-9986042 ACI-9982274 ACI-0133464 and ACI-0312828 the Digital Technology Center at the University of Minnesota and by the Army High Performance Computing Research Center AHPCRC under the auspices of the Department of the Army Army Research Laboratory ARL under Cooperative Agreement number DAAD19-01-2-0014 The content of which does not necessarily re\224ect the position or the policy of the government and no o\001cial endorsement should be inferred Access to research and computing facilities was provided by the Digital Technology Center and the Minnesota Super-computing Institute 
tion rule mining 2 3 24 29 15 9 18 35  I n r ecen t years some studies have also demonstrated the usefulness of frequent itemset mining in serving as a condensed representation of the input data in order for answering various types of queries 22 8  a nd the t r a nsactional data or document classi\223cation 5 20 19 and clustering 32 7 11 34 33  Most frequent-itemset based clustering algorithms need to 223rst mine a large intermediate set of frequent itemsets in many cases it is the complete set of frequent temsets on which some further post-processing 
can be performed in order to generate the 223nal result set hich can be used for clustering purposes In this paper we consider directly mining a 223nal subset of frequent itemsets which can be used as a concise summary of the original database and to cluster the categorical data To serve these purposes we require the 223nal set of frequent itemsets have the following properties 1 it maximally covers the original database given a minimum support 2 each 223nal frequent itemset can be used as a description for a group of transactions and the transactions with the same description can 
be grouped into a cluster with approximately maximal intra-cluster similarity To achieve this goal our solution to this problem formulation is that for each transaction we 223nd one of the longest frequent itemsets that it contains and use this longest frequent itemset as the corresponding transaction\220s description The set of so mined frequent itemsets is called a summary set  One signi\223cant advantage of directly mining the 223nal subset of frequent itemsets is that it provides the possibility of designing a more e\001cient algorithm We proved that each itemset in the 
summary set must be closed thus some search space pruning methods proposed for frequent closed itemset mining can be borrowed to accelerate the summary set mining In addition based on some properties of the summary set  we proposed several novel pruning methods which greatly improve the algorit hm e\001ciency By incorporating these pruning methods with a traditional freProceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


quent itemset mining framework we designed an e\001cient summary set mining algorithm SUMMARY Our thorough empirical tests show that SUMMARY runs very fast even when the minimum support is extremely low and scales very well w.r.t the database size and its result set is very e\002ective in clustering the categorical data and summarizing the dense transaction databases The rest of this paper is organized as follows Section 2 and Section 3 introduce the problem de\223nition and some related work res pectively Section 4 describes the algorithm in detail Section 5 presents the empirical results Section 6 shows an application of the algorithm in clustering categorical data and the paper ends with some discussions and conclusion in Section 7 2 Problem De\223nition A transaction database TDB is a set of transactions where each transaction denoted as a tuple 002 tid X 003  contains a set of items i.e X  and is associated with a unique transaction identi\223er tid Let I   i 1 i 2 i n  be the complete set of distinct items appearing in TDB An itemset Y is a non-empty subset of I and is called an l itemset if it contains l items An itemset  x 1 x l  is also denoted by x 1 267\267\267 x l Atransaction 002 tid X 003 is said to contain itemset Y if Y 004 X The number of transactions in TDB containing itemset Y is called the absolute support of itemset Y  denoted by sup  Y  In addition we use  TDB  and  Y  to denote the number of transactions in database TDB and the number of items in itemset Y  respectively Given a minimum support threshold min sup an itemset Y is frequent if sup  Y  005 min sup Amongthe longest frequent temsets supported by transaction T i  we choose any one of them and denote it by SI T i  SI T i is called the summary itemset of T i 1  The set of the summary itemsets w.r.t the transactions in TDB i.e 006  TDB  i 1  SI T i   is called a summary set w.r.t database TDB  Note that the summary set of a database may not be unique this is because a transaction may support more than one summary itemset  Given a transaction database TDB and a minimum support threshold min sup  the problem of this study is to 223nd any one of the summary sets w.r.t TDB  Example 2.1 The 223rst two columns in Table 1 show the transaction database TDB in our running example Let min sup 2 we sort the list of frequent items in support ascending order and get the sorted item list which is called f list Inthisexample f list  a 3 b 4 1 Transaction T i may support no frequent itemset in this case SI T i is empty and T i can be treated as an outlier Tid Set of items Ordered frequent item list 01 a c e g a c e 02 b d e b d e 03 d f i d f 04 e f h e f 05 a b c d e f a b c d e f 06 b c d b c d 07 a c f a c f 08 e f e f 09 b d b d Table 1 A transaction database TDB c 4 d 5 e 5 f 5   The list of frequent items in each transaction are so rted according to f list and shown in the third column of Table 1 It is easy to 223gure out that  ace 2 acf 2 bcd 2 bd 4 bde 2 df 2 ef 3  is one summary set w.r.t TDB  001 3 Related Research Since the introduction of the association rule mining 2  n u m erou s f requ en t i t e m s et m i n i n g al gorithms have been proposed In essence SUMMARY is a projection-based frequent temset mining gorithm 18 1 a n d ad op t s t h e n at u r al m a t r i x st ru ct u r e instead of the FP-tree to rep resent the conditional database 26 12 It g r o w s a cur r e n t pr e\223x i temset b y physically building and scanning its projected matrix In 15 a n a l g o r ithm w a s p r o p o sed t o m ine a ll mo st speci\223c sentences however both the problem and the algorithm in this study are di\002erent from those in 15  In Section 4 we prove that each summary itemset must be closed thus some pruning methods previously proposed in the closed or maximal itemset mining algorithms 6 25 27 10 35 30 23 21 c an b e u s ed to enhance the e\001ciency of SUMMARY Like several itemset mining algorithms with length-decreasing support constraint 28 31 SUMMAR Y a d o p ts so me pr uning methods to prune the unpromising transactions and pre\223xes However because the problem formulations are di\002erent the pruning methods in SUMMARY are di\002erent from the s studies One important application of the SUMMARY algorithm is to concisely summarize the transactions and cluster the categorical data There are many algorithms designed for clustering categorical data typical examples include ROCK 14 and C A CTUS  13  Recently several frequent-itemset based clustering algorithms have also been proposed to cluster categorical or numerical data 7 11 34  T h e se m e t h o d s 223 rst m i n e a n intermediate set of frequent itemsets and some postprocessing are needed in order to get the clustering solution SUMMARY mines the 223nal subset of frequent Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


itemsets which can be directly used to group the transactions to form clusters and enables us to design more e\002ective pruning methods to enhance the performance Contributions The contributions of this paper can be summarized as follows 1 We proposed a new problem formulation of mining the summary set of frequent itemsets with the application of summarizing transactions and clustering categorical data 2 By exploring the properties of the summary set  we have proposed several pruning methods to effectively reduce the search space and enhance the e\001ciency of the SU MMARY algorithm 3 Thorough performance study has been performed and shown that SUMMARY has high e\001ciency and good scalability and can be used to cluster categorical data with high accuracy 4 SUMMARY An E\001cient Algorithm to Summarize the Transactions In this section we 223rst brie\224y introduce a traditional framework for enumerating t he set of frequent itemsets which forms the basis of the SUMMARY algorithm Then we discuss how to design some pruning methods to speed up the mining of the summary set  Finally we present the integrated SUMMARY algorithm and discuss how to revise SUMMARY to mine all the summary itemsets for each transaction 4.1 Frequent Itemset Enumeration Like some other projection-based frequent itemset mining algorithms SUMMARY employs the dividenquer and depth-\223rst search strategies 18 30  which are applied according to the f list order In Example 2.1 SUMMARY 223rst mines all the frequent itemsets containing item a  then mines all frequent itemsets containing b but no a   and 223nally mines frequent itemsets containing only f  In mining itemsets containing a  SUMMARY treats a as the current pre\223x and builds its conditional database denoted by TDB  a  002 01 ec 003  002 05 efc 003  002 07 fc 003 where the local infrequent items b  d and g have been pruned and the frequent items in each projected transaction are sorted in support ascending order By recursively applying the divide-and-conquer and depth-\223rst search methods to TDB  a  SUMMARY can 223nd the set of frequent itemsets containing a  Note instead of using the FP-tree structure SUMMARY adopts the natural matrix structure to store the physically projected database 12  T his i s b eca use t he ma tr ix str u ctur e allows us to easily maintain the tid sinordertodetermine which set of transactions the pre\223x itemset covers In addition in the above enumeration process SUMMARY always maintains the current longest frequent itemset for each transaction T i that was discovered 223rst so far In the following we call it the current Longest Covering Frequent itemset w.r.t T i denoted by LCF T i  4.2 Search Space Pruning The above frequent itemset enumeration method can be simply revised to mine the summary set Upon getting a frequent itemset we check if it is longer than the current longest covering frequent itemset w.r.t any transaction that this items et covers If so this newly mined itemset becomes the cu rrent longest covering frequent itemset for the corresponding transactions Notice that this na\250 232ve method is no more e\001cient than the traditional all frequent itemset mining algorithm However the above algorithm for 223nding the summary set can be improved in two ways First as we will prove later in this section any summary itemset must be closed and thus the pruning methods proposed for closed itemset mining can be used Second since during the mining process we maintain the length of the current longest covering itemset for each transaction we can employ additional branch-and-bound techniques to further prune the overall search space De\223nition 4.1 Closed itemset An itemset X is a closed itemset if there exists no proper superset X 001 007 X such that sup  X 001  sup  X  001 Lemma 4.1 Closure of a summary itemset Any summary itemset w.r.t a transaction T i  SI T i must be a closed itemset Proof  We will prove it by contradiction Assume SI T i is not closed which means there must exist an itemset Y  such that SI T i b Y and sup  SI T i  sup  Y  Thus Y is also supported by transaction T i and is frequent However  Y    SI T i  contradicts with the fact that SI T i is the summary itemset of transaction T i  001 Lemma 4.1 suggests that any pruning method proposed for closed itemset mining can be used to enhance the performance of the summary set mining In SUMMARY only one such technique item merging 30  i s adopted that works as follows For a pre\223x itemset P  the complete et of its local frequent items that have the same support as P are merged with P to form a new pre\223x and these items are removed from the list of the local frequent items of the new pre\223x It is easy to Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


see that such a scheme does not a\002ect the correctness of the algorithm 30  Example 4.1 Assume the current pre\223x is a 3 whose local frequent item list is e 2 f 2 c 3  amongwhich c 3 can be merged with a 3 to form a new pre\223x ac 3 with local frequent item list e 2 f 2   001 Besides the above pruning method we developed two new pruning methods called conditional transaction and conditional database pruning that given the set of the currently maintained longest covering frequent itemsets w.r.t TDB  they remove some conditional transactions and databases that are guaranteed not to contribute to and generate any summary itemsets  Speci\223cally let P be the pre\223x itemset that is currently under consideration sup  P  its support and TDB  P  002 T P 1 X P 1 003  002 T P 2 X P 2 003  267\267\267  002 T P sup  P  X P sup  P  003 its al database Note that some or all of the transactions X P i 1 t i t sup  P  can be empty De\223nition 4.2 Invalid conditional transaction A conditional transaction T P i in TDB  P where 1 t i t sup  P  is an invalid conditional transaction if it falls into e of the following two cases 1  X P i t   LCF T P i 212 P   2  X P i     LCF T P i 212 P   but  n j 013 1 sup  P  T P j  X P j     LCF T P i 212 P     min sup  Otherwise T P i is called a valid conditional transaction 001 The 223rst condition states that a conditional transaction is invalid if its size is no greater than the difference between its current longest covering frequent itemset and the length of the pre\223x itemset whereas the second condition states that the number of conditional transactions which can be used to derive itemsets longer than LCF T P i by extending pre\223x P is smaller than the minimum support Lemma 4.2 Unpromising summary itemset generation If T P i is an invalid conditional transaction there will be no frequent itemset derived by extending pre\223x P that T P i supports and is longer than LCF T P i  Proof  Follows directly from De\223nition 4.2 i If a transaction T P i is invalid because of the 223rst condition it will not contain su\001cient items in its conditional transaction to identify a longer covering itemset ii If a transaction T P i is invalid because of the second condition the conditional database will not contain a su\001ciently large number of long conditional transactions to obtain an itemset that is longer than LCF T P i and frequent 001 Note it is possible for an invalid conditional transaction to be used to mine summary itemsets for other valid conditional transactions w.r.t pre\223x P thus,we cannot simply prune any invalid conditional transaction Instead we can safely prune some invalid conditional transactions according to the following Lemma Lemma 4.3 Conditional transaction pruning An invalid conditional transaction T P i  can be safely pruned if it satis\223es  X P i t min 002 j T P j is d   LCF T P j 212 P  1 Proof  Consider an invalid conditional transaction T P i that satis\223es Equation 1 Then in order for a frequent itemset supported by the conditional transaction T P i and pre\223x P to replace the current longest covering frequent itemset of a valid conditional transaction T P j  T P i needs to contain more than  X P i  items in its conditional transaction As a result T P i can never contribute to the support of such an itemset and can be safely pruned from the conditional database 001 Lemma 4.3 can be used to prune from the conditional database some unpromising transactions satisfying Equation 1 even when there exist some valid conditional transactions However in many cases there may exist no valid conditional transactions in this case the whole conditional database can be safely pruned Lemma 4.4 Conditional database pruning Given the current pre\223x itemset P and its projected conditional database TDB  P  if each of its conditional transactions T P i isinvalid TDB  P can be safely pruned Proof  According to Lemma 4.2 for any invalid conditional transaction T P i  we cannot generate any frequent itemsets longer than LCF T P i by growing pre\223x P  This means that if each conditional transaction is invalid we can no longer change the current status of the set of the currently maintained longest covering frequent itemsets w.r.t pre\223x P  006 sup  P  i 1  LCF T P i  by extending P thus TDB  P can be safely pruned 001 Example 4.2 Assume the pre\223x is c 4 i.e P  c  From Table 1 we get that TDB  c  002 01 e 003  002 05 def 003  002 06 d 003  002 07 f 003 and LCF 01  ace 2 LCF 05  ace 2 LCF 06  bcd 2 and LCF 07  acf 2 Conditional transactions 002 01 e 003  002 06 d 003 and 002 07 f 003 fall into case 1 of De\223nition 4.2 while 002 05 def 003 falls into case 2 of De\223nition 4.2 thus all the conditional transactions in TDB  c are invalid According to Lemma 4.4 conditional database TDB  c can be pruned 001 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


ALGORITHM 1  SUMMARY  TDB min sup  INPUT 1 TDB  a transaction database and\(2 min sup  aminimum support threshold  OUTPUT 1 SI  the summary set  01 for all t i 003 TDB 02 SI t i 004\005  03 call summary  005 TDB SUBROUTINE 1  summary  pi cdb  INPUT 1 pi  apre\336xitemset and\(2 cdb  the conditional database w.r.t pre\336x pi  04 I 004 336nd frequent items\(cdb,min sup 05 S 004 item merging\(I pi 004 pi 006 S I 004 I-S 06 if\(pi 007  005  07 for all t i 003 cdb 08 if  SI t i    pi   09 SI t i 004 pi 10 if\(I 007  005  11 if\(conditional database pruning\(I,pi,cdb 12 return 13 cdb 004 conditional transaction pruning\(I,pi,cdb 14 for all i 003 Ido 15 pi 001 004 pi 006 i   16 cdb 001 004 build cond database\(pi 001 cdb 17 call summary pi 001 cdb 001  4.3 The m By pushing deeply the search space pruning methods of Section 4.2 into the frequent itemset mining framework described in Section 4.1 we can mine the summary set as described in the SUMMARY algorithm shown in Algorithm 1 It 223rst initializes the summary itemset to empty for each transaction lines 01-02 and calls the Subroutine 1 i.e summary f TDB ominethe summary set line 03 Subroutine summary pi  cdb  223nds the set of local frequent items by scanning conditional database cdb once line 04 and applies the search space pruning methods such as the item merging line 05 conditional database pruning lines 11-12 and conditional transaction pruning line 13 updates the summary set information for conditional database cdb w.r.t pre\223x itemset pi lines 06-09 and grows the current pre\223x builds the new conditional database and recursively calls itself under the projection-based frequent itemset mining framework lines 14-17 Discussion  A transaction may be covered by multiple summary itemsets In this paper we mainly focus on the SUMMARY algorithm which for each transaction only inserts into the summary set the summary itemset that was discovered 223rst However it is rather straightforward to revise SUMMARY to 223nd all the summary itemsets supported by each transaction Speci\223cally if we change the 217 t 220to\217  220 in case 1 of De\223nition 4.2 all the 217  220to\217 005 220 in case 2 of De\223nition 4.2 the 217 t 220to 217  220 in Equation 1 of Lemma 4.3 and the 217  220to\217 t 220in line 08 of Algorithm 1 the revised SUMMARY algorithm will 223nd all the summary itemsets We denote the so-derived algorithm by SUMMARY-all 5 Experimental Results We have implemented both the SUMMARY and SUMMARY-all algorithms and performed a thorough experimental study to evaluate the e\002ectiveness of the pruning methods their algorithmic e\001ciency and their overall scalability All the experiments except the e\001ciency test were performed on a 2.4GHz Intel PC with 1GB memory and Windows XP installed In our experiments we used some databases which were popularly used in evaluating various frequent itemset mining algorithms 35 30 16  su c h as connect  chess  pumsb  mushroom and gazelle  and some categorical databases obtained from the UCI Machine Learning repository such as SPECT  Letter Recognition andsoon  1 10 2 4 8 16 32 Runtime in seconds Absolute support threshold SUMMARY-all with no pruning SUMMARY with no pruning   SUMMARY-all with pruning   SUMMARY with pruning   a Database  mushroom  5 10 15 20 25 30 200 300 400 500 600 700 800 900 1000 Runtime in seconds Base size \(in K tuples SUMMARY-all: min_sup=0.2 SUMMARY: min_sup=0.2   SUMMARY-all: min_sup=1   SUMMARY: min_sup=1   b Scalability T10I4Dx  Figure 1 Effectiveness of the pruning methods and the scalability test E\001ectiveness of the Pruning Methods We 223rst evaluated the e\002ectiveness of the pruning methods by comparing SUMMARY and SUMMARY-all themselves with or without the conditional database and transaction pruning methods Figure 1a shows that the algorithms with pruning can be over an order of itude faster than the corresponding algorithms without pruning for database mushroom  This illustrates that the pruning methods newly proposed in this paper are very e\002ective in reducing search space ability We also tested the algorithm scalability using the IBM synthetic database series T10I4Dx by setting the average transaction length at 10 and changing the number of transactions from 200K to 1000K We ran both SUMMARY and SUMMARY-all at two di\002erent minimum relative supports of 0.2 and 1 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


Figure 1b shows that these two algorithms scale very well against the database size E\002ciency To mine the summary set ana\250 232ve method is to 223rst mine the complete set of frequent closed itemsets from which the summary set can be further identi\223ed Our comparison with FPclose 17  o ne o f the most recently developed e\001cient closed itemset mining algorithms 16  sh o w s t h a t s u c h a sol u t i on i s n o t p ractical when the minimum support is low As we will discuss n Section 6 such ow minimum support ues are bene\223cial for clustering applications The e\001ciency comparison was performed on a 1.8GHz Linux machine with 1GB memory by varying the solute support threshold and turning o\002 the output of FPclose The experiments for all the databases we used show consistent results Due to limited space we only report the results for databases connect and gazelle   100 1000 128 256 512 1024 2048 4096 Runtime in seconds Absolute support threshold FPclose SUMMARY-all   SUMMARY   a Database  connect  1 10 2 4 8 16 32 Runtime in seconds Absolute support threshold FPclose SUMMARY-all   SUMMARY   b Database  gazelle  Figure 2 Efficiency test for connect and gazelle Figure 2 shows the runtime for databases connect and gazelle  It shows that both SUMMARY and SUMMARY-all scale very well w.r.t the support threshold and for connect database they even run faster at low support value of 128 than at high support value of 512 This is because these two algorithms usually mine longer itemsets at lower support which makes the pruning methods more e\002ective in removing some short transactions and conditional databases Because the FP-tree structure adopted by FPclose is very e\002ective in c ondensing dense databases at high support FPclose is faster than SUMMARY and SUMMARY-all for dense databases like connect  but once we continue to lower the support it can be orders of magnitude slower While for sparse databases like gazelle  FPclose can be several times slower In addition the above results also show that SUMMARY always runs a little faster than SUMMARY-all this is because SUMMARY-all mines more summary itemsets than SUMMARY For example at absolute minimum support threshold of 32 on average SUMMARYall 223nds 11.1 summary itemsets for each transaction of database mushroom  and 223nds 1.3 summary itemsets for each transaction of database gazelle  6 Application Summary Set based Clustering One important application of the SUMMARY algorithm is to cluster the categorical data by treating each summary itemset as a cluster description and grouping the s with the same cluster description into a cluster In SUMMARY we adopt a pre\223x tree structure to facilitate this task which has been used extensively in performing di\002erent data mining tasks 18 30  For each transaction T i  if its summary itemset SI T i is not empty we sort the items in SI T i in lexicographic order and insert it into the pre\223x tree The tree node corresponding to the ast item of the sorted summary itemset represents a cluster to which the transaction T i belongs root a d c e:2 d:1 e:1 f:1 b e f:2 c d:1 f:1 cid 01 02 03 04 05 06 07 tid list 01 05 02 03 04 08 06 07 09 Figure 3 Clustering based on summary set Example 6.1 The summary itemsets for the transactions in our running example are SI 01  ace  SI 02  bde  SI 03  df  SI 04  ef  SI 05  ace  SI 06  bcd  SI 07  acf  SI 08  ef and SI 09  bd  f we insert these summary itemsets into the pre\223x tree in sequence we can get seven lusters with cluster descriptions ace  bde  df  ef  bcd  acf and bd  as shown in Figure 3 From Figure 3 we see that transactions 01 and 05 are grouped into cluster 01 transactions 04 and 08 are grouped into cluster 04 while each of the other sactions forms a separate cluster of their own Note that a non-leaf node summary itemset in the pre\223x tree represents a non-maximal frequent itemset in the sense that one of its proper supersets must be frequent For example summary itemset bd is non-maximal because summary itemset bde is a proper superset of bd Inthiscase,we have an alternative clustering option merge the nonleaf node clusters with their corresponding leaf node Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


clusters to form larger clusters In Figure 3 we can merge cluster 07 with cluster 02 to form a cluster 001 Clustering Evaluation We have used several categorical databases to evaluate the clustering quality of the SUMMARY algorithm including mushroom  SPECT  Letter Recognition and Congressional Voting  which all contain class labels and are available at http  www.ics.uci.edu  b mlearn  Wedidnotusethe class labels in mining the summary set and clustering instead we only used them to evaluate the clustering accuracy which is de\223ned by the number of correctly clustered instan ces i.e the instances with dominant class labels in the computed clusters as a percentage of the database size SUMMARY runs very fast and can achieve very good clustering accuracy for these databases especially when the minimum support is low Due to limited space we only show results for mushroom and Congressional Voting databases which have been widely used in the previous studies 14 32 34  The mushroom database contains some physical characteristics of various mushrooms It has 8124 instances and two classes poisonous and edible Table 2 shows the clustering results for this database including the minimum support used in the tests the number of clusters found by SUMMARY the number of misclustered instances  clustering accuracy  compression ratio and runtime in seconds for both the summary set discovery and clustering The compression ratio is de\223ned as the total number of items in the database divided by the total number of items in the summary set Wecan see that SUMMARY has a ng accuracy higher than 97 and a runtime less than 0.85 seconds for a wide range of support thresholds At support of 25 it can even achieve a 100 accuracy The MineClus algorithm is one of the most recently developed clustering algorithm for this type of databases 34  I t s rep o rt ed clustering solution for this database 223nds 20 clusters with an accuracy 96.41 and in the meantime declares 0.59 of the instances as outliers which means it misclusters about 290 instances and treats about another 48 instances as outliers Compared to this algorithm SUMMARY is very competitive in considering both of its high e\001ciency and clustering accuracy In addition the high compression ratios demonstrate that the summary set can be used as a concise summary of the original database Note in each case of Table 2 the summary set covers each instance of the original database which means there is no outlier in our solution The Congressional Voting database contains the 1984 United States Congressional Voting Records and has two class labels Republican and Democrat In our sup clu miscl accur com rat time 1400 30 32 99.6 660 0.38s 1200 35 32 99.6 549 0.42s 1000 37 32 99.6 509 0.44s 800 63 208 97.4 268 0.48s 400 128 8 99.9 120 0.66s 200 140 6 99.93 97 0.77s 100 197 32 99.6 62 0.81s 50 298 1 99.99 37 0.79s 25 438 0 100 23 0.75s Table 2 Clustering mushroom database cid Rep Demo cid Rep Demo 1 2 244 4 1 3 2 155 16 5 2 1 3 5 0 6 1 1 Table 3 Clustering Congressional Voting database experiments we removed four outlier instances whose most attribute values are missing and used the left 431 instances Table 3 shows the clustering solution of SUMMARY at a minimum support of 245 at which point the clusters produced by SUMMARY covers the entire database while a minimum support higher than 245 will make SUMMARY miss some instances and SUMMARY only uses 0.001 seconds to 223nd the six clusters with an accuracy higher than 95 and a compression ratio higher than 1164 Even we simply merge the four small clusters with the two large clusters in order to get exact two clusters the accuracy is still higher than 93 in the worst case e.g clusters 3 and 5 are merged into cluster 1 and clusters 4 and 6 are merged into cluster 2 d is much better than the reported accuracy 86.67 of the MineClus algorithm 34  7 Discussions and Conclusion In this paper we proposed to mine the summary set that can maximally cover the input database Each summary itemset can be treated as a distinct cluster description and the transactions with the same description can be grouped together to form a cluster Because the summary itemset of a cluster is one of the longest frequent itemsets that is common among the corresponding transactions of the same cluster it can approximately maximize the intra-cluster similarity while di\002erent clusters are dissimilar with each other because they support distinct summary itemsets  In addition we require each summary itemset be frequent in order to make sure it is statistically signi\223cant Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


Directly mining the summary set also enabled us to design an e\001cient algorithm SUMMARY By exploring some properties of the summary set  we developed two novel pruning methods which signi\223cantly reduce the search space Our perfo rmance study showed that SUMMARY runs very fast even when the minimum support is extremely low and the summary set is very e\002ective in clustering categorical data In addition we also evaluated SUMMARY-all a variant of SUMMARY which mines all the summary itemsets for each transaction In future we plan to explore how to choose the one among the summary itemsets supported by a transaction which can reduce the number of clusters while achieving a high clustering accuracy References  R  A garw a l C A ggarw a l V  Prasad  A T r ee Pro j ection Algorithm for Generation of Frequent Item Sets Journal of Parallel and Distributed Computing 61\(3 2001  R  A gra w al T  I mielin sk i A  S w ami Mining Association Rules between Sets of Items in Large Databases  SIGMOD\22293  R  A gra w al R  S rik a n t  Fast Algorithms for Mining Association Rules  VLDB\22294  M  A n t on ie O  Z aian e Text Document Categorization by Term Association  ICDM\22202  R J B a y ard o  Brute-force Mining of High-con\223dence Classi\223cation rules  KDD\22297  R J B a y ard o  E\001ciently Mining Long Patterns from Databases  SIGMOD\22298  F  B eil M Est er X  X u  Frequent Term-Based Text Clustering  KDD\22202  J  Bou licau t  A  By k o wsk i  C  R igot t i  Free-Sets A Condensed Representation of Boolean Data for the Approximation of Frequency s  Journal of Data Mining and Knowledge Discovery 7\(1 2003  S  B rin  R  Mot w an i J.D  U llman  S  T su r Dynamic Itemset Counting and Implication Rules for Market Basket Data  SIGMOD\22297  D  Bu rd ic k  M Calimlim J  G eh rk e MAFIA A Maximal Frequent Itemset Algorithm for Transactional Databases  ICDE\22201  B F u n g  K  W an g M Est er Hierachical Document Clustering Using Frequent Itemsets  SDM\22203  K  Gad e  J  W an g G K a ry p i s E\001cient Closed Pattern Mining in the Presence of Tough Block Constraints  KDD\22204  V  Gan t i J Geh r k e  R  R amak rish n a n  CACTUS Clustering Categorical Data Using Summaries  KDD\22299  S  Gu h a  R  R ast ogi K  S h i m  ROCK A Robut Clustering Algorithm for Categorical Attributes  ICDE\22299 15 D G uno pul o s  H  M a nni l a  S  S a l uj a  Discovering All Most Speci\223c Sentences by Randomized Algorithms  ICDT\22297  B Go et h a ls M  Z ak i An Introduction to FIMI\22003 Workshop on Frequent Itemset Mining Implementations  ICDM-FIMI\22203  G Grah n e  J  Z h u  E\001ciently Using Pre\223x-trees in Mining Frequent Itemsets  ICDM-FIMI\22203 18 J  Ha n J  P e i  Y Yi n Mining Frequent Patterns without Candidate Generation  SIGMOD\22200  W  L i  J  H an  J  P ei CMAR Accurate and E\001cient Classi\223cation based on multiple class-association rules  ICDM\22201  B L i u  W  H s u  Y  Ma Integrating Classi\223cation and ation Rule Mining  KDD\22298 21 G  L i u  H  L u W L o u J X  Y u  On Computing Storing and Querying Frequent Patterns  KDD\22203 22 H M a nni l a  H  T o i v o ne n Multiple Uses of Frequent Sets and Condensed Representations  KDD\22296  F P a n  G Con g  A K H  T u n g  J  Y an g M Z a k i  CARPENTER Finding Closed Patterns in Long Biological Datasets  KDD\22203 24 J  P a rk M  C he n P  S Y u  An E\002ective Hash Based Algorithm for Mining Association Rules  SIGMOD\22295  N  P a sq u i er Y  Bast i d e  R  T aou il L  L a k h al Discovering Frequent Closed Itemsets for Association Rules  ICDT\22299  J P e i J H a n  H  L u  S  N ish i o S  T a n g  D  Y an g H-Mine Hyper-structure Mining of Frequent Patterns in Large Databases  ICDM\22201  J P e i J H a n  R  Mao CLOSET An E\001cient Algorithm for Mining Frequent Closed Itemsets  DMKD\22200  M S e n o  G  K ary p is LPMiner An Algorithm for Finding Frequent Itemsets Using Length-Decreasing Support Constraint  ICDM\22201  H  T o iv on en  Sampling e Databases for Association Rules  VLDB\22296 30 J  W a n g J H a n J P e i  CLOSET Searching for the Best Strategies for Mining Frequent Closed Itemsets  KDD\22203  J W a n g  G  K ary p is BAMBOO Accelerating Closed Itemset Mining by Deeply Pushing the LengthDecreasing Support Constraint  SDM\22204 32 K W a ng  C  X u B  Li u Clustering Transactions using Large Items  CIKM\22299  H  X i on g M S t ein b ac h  P  T a n  V  K u m a r HICAP Hierarchial Clustering with Pattern Preservation  SDM\22204 34 M  Yi u N M a m o ul i s  Frequent-Pattern based Iterative Projected Clustering  ICDM\22203  M Z a k i  C  H siao CHARM An E\001cient Algorithm for Closed Itemset Mining  SDM\22202 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


proposed system has achieved good performance with high satisfaction and applicability REFERENCES M. Eirinaki and M.  Vazirgiannis  Web mining for web personalization   ACM Transaction!, on Internet Technology, Vol. 3, No. I ,  2003, pp. 1 27 1. Konstan, B. Miller, D. Mall i  J. Herlocker, L. Gordon, and 1. Riedl  GroupLens: applying collaborative filtering to usenet news   Communications of the ACM, 40\(3 T.W. Yan and H. Garcia-Molina  The SIFT information dissemination system  ACM Transactions on Database Systems, Vol. 24, No. 4, 19W T. loachims, D. Freitag, and T. Mitchel  Webwatcher: a tour guide for the World Wide Web  Proc. of the 5th International Joint Conference on AI. Japan. 1997, pp. 770-775 C. Shahabi, F. Banaei-Kashani, Y. Chen, and 0. McLeod  Yoda: an accurate and scalable web-based recommendation system  Roc. of the 6th International Conference on Cooperative Information Systems CoopIS 2001 B. Mobasher, H. Dai, T. Luo and M. Nakagawa  Effective personalization based on association rule discovery from web usage data  hoc. of the 3rd ACM Workshop on Web Information and Data Management \(WIDMOI W. Lin, S.  A.AIvarez, and C. Ruiz  ollaborative recommendation via adaptive association rule mining  Proc. of the Web Mining for E Commerce Workshop \(WebKDD2000 B. Mobwher  A web personalization engine baed on user transaction clustering  Prcc. of the 9th Workshop on Information Technologies and Systems \(WlTS  99 D.S. Phatak. and R. Mulvaney  Clustering for personalized mobile web usage  Roc. ofthe IEEE FUZZ  OZ, Hawaii. May 2002. pp. 705-710 pp.529-565 IO] R Agnwal, and R. Srikant  Mining Sequential Patterns  Prw. of the 1 lth International Conference on Data Engineering, Taiwan, 1995 I  11 B.Y. Zhou, S.C. Hui, and A.C.M. Fong  CS-mine: an efficient WAP tree mining for web acces  patlems  the 6th Asia Pacific Web Conference \(APWEBW 532 I21 R. Srikant, and R. Agrawal  Mining sequential patterns: generalizations and performance improvements  hac. of the 5th lntematianal Conference on Extending Darabase Technology \(EDBT France, 1996, pp. 3-17 I131 R. Cooky, B.  Mobasher, and J. Srivasrava  Data preparation for mining World Wide Web browsing patterns  Journal of Knowledge and Information Systems, Vol. 1, No. 1. 1999 1141 D. Knuth  The art of computer programming  Vol. 2, 2nd edition Addison-Wesley, October 1998 398 pre></body></html 


in four unknowns: n, ?np, \(?2?j |HT 2?j |HL The Neyman-Pearson decision rule is said to maximize the probability of detecting the track-lost regime, PDET for a given n, subject to the constraint that PFA is less than or equal to some user-de?ned probability. [6]. However, it is convenient in this context to instead ?x both PDET and PFA at some desired values, and use \(41 43 the corresponding minimum \(integer n that must be used in order to meet these goals. Typically there is a range of values of ?np that will achieve them Appropriate values of n and ?np can be determined for each pe element ?j using \(41 43 determinations for each element can then be made using 38 39 for each pe element; if a single overall track-loss metric is desired, Wishart distributed versions of \(33 35 also be used in the development of the Neyman-Pearson decision rule. However, because of the approximate nature of the distribution in \(35 additional covariance information is useful in the matrix likelihood test [5]. Further, a Wishart implementation has the disadvantage of being more computationally intensive than the implementation outlined above D. Application to the PDAF In order to implement the Neyman-Pearson rule, \(15 be rewritten x  k|k  k|k ? 1 k k 44 where, for the PDAF e?\(k  mk i=1 i zi\(k 1? ?0  k|k ? 1   45 This  effective  measurement innovation is then the single lter prediction error for purposes of calculating the sample variances s2?j . The j-th diagonal of the innovations covariance for the tracking regime, ST?sirf , and track-lost regime SL?sirf , can then be used for \(?2?j |HT 2?j |HL respectively Some method of handling the case of no gated measurements must be implemented. As suggested earlier, it is at least mathematically consistent in this case to de?ne the measurement innovations as zero. However, when there is a signi?cant probability of zero measurements gating the assumption that the prediction errors have a Gaussian distribution will be a poor one; in practice, this probability is likely to be signi?cant both when PD &lt; 1 and when in the track-lost regime. So the goals for PDET and PFA will not generally be met unless the innovations from timesteps when no measurements gate are excluded, and it is necessary to use the previous nC timesteps to ?nd n nonzero innovations with which to compute s2?j . The average number of timesteps n  T used when the ?lter is operating in the tracking regime is approximately n/\(PDPG average number of timesteps n  L used to when the ?lter is in the track-lost regime is problem speci?c E. Extensions to Other Data Association Methods This strategy is suitable for other data association methods. For the NN ?lter, the KF measurement innovations are the prediction errors. For MAP and maximum likelihood ML determined by similarly deriving a relationship between the estimated state, predicted state, and Kalman gain IV. EXAMPLE: PDAF TRACK REGIME TEST In this section, an example system using the PDAF is constructed. The SIRF approximation steady-state innovations covariances for the tracking and track-lost regimes are calculated and compared to simulation results. An example 


calculated and compared to simulation results. An example Neyman-Pearson decision rule is determined. Theoretical and simulation results for the probabilities of detection of the track-lost regime, PDET, and of false alarm while in the tracking regime, PFA, are given. An estimate of the number of timesteps required to detect track-loss is also provided 4320 A. Dynamics The kinematic model system \(with timestep x\(k + 1  1 0 1  x\(k  2/2   w\(k 46 y\(k  1 0  x\(k 47 is the standard zero-order hold discrete approximation to a continuous double-integrator system. For ? = 0.1 x\(k + 1  1 0.1 0 1  x\(k  0.005 0.1  w\(k 48 B. Kalman Filter System For Q = Q = 1000, R = R = 0.1 P  kf  0.3000 1.9998 1.9998 19.9965  Skf = 0.4000 are the steady-state covariances C. Clutter and Gating Though the example system is dependent on Q,R and ?, it can be described in the tracking regime using just three independent parameters [12]: the probability of detecting the truth measurement, PD, the normalized target acceleration, \(NTA sity, \(NCD the target is maneuvering, and NCD is a measure of how dif?cult it is to localize the target from the measurements For the example system, NTA = 1. Choosing PD = 1 0.02 results in NCD = 0.002, and an average track-lifetime of approximately 500 timesteps using the PDAF [8], [12 this is a tracking problem of  moderate  dif?culty for the example dynamics Using a four standard deviation gate \(? = 16 PG = 0.99994. For the example system, nz = 1, cnz = 2 and Vk = 8 |S\(k D. Experimental Tracking and Track-lost Regimes The following experimental setup allows for  controlled  track-loss and is used to verify operation of the trackloss detector. Until timestep 1000, the ?lter is tracking in the sense that PD = 1 \(the truth measurement is always 


available to the gating test 2000, PD = 0 \(the truth measurement is never available to the gating test sense that the truth measurement is never gated. PFA can then be calculated using measurement innovations from the rst 1000 timesteps, and PDET from using measurement innovations from the second 1000 timesteps, with roughly 1000 values of s2?j being tested by the decision rule in each regime. Note that PDET is thus calculated using both data points from the transient of  controlled  track-loss as well as from the  steady-state  operation of the ?lter in the tracklost regime E. PDAF SIRF Tracking Approximation For the example system with tracking regime assumptions: PG ? PD = 1, q1 ? PD = 1 [3], and \(25 T\(?Vk cnz 2  mk=1 exp\(??Vk Vk mk ? 1   nz nz/2  I2\(mk 49 Figure 1 shows the resulting function for ?T\(?Vk 0 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Vk   T  L Fig. 1. ?T and ?L as functions of ?Vk for example system ating \(4 6 7 11 24 using \(49 T\(?Vk  T = 0.9602, ?V  k 0.1047, and P  T?sirf  0.3286 2.1122 2.1122 20.5492  ST?sirf = 0.4286 Simulating and then averaging P\(k|k? 1 k example PDAF tracking regime over 1000 timesteps gives P  T?sim  0.3210 2.0831 2.0831 20.3698  ST?sim = 0.4210 Thus ST?sirf has only 1.7% error when compared to the more time-consuming simulation results F. PDAF SIRF Track-lost Approximation Figure 1 shows the function for ?L\(?Vk system with track-lost regime assumptions. Iterating \(4 6 


7 11 30 31  L = 0.3264 V  k = 0.4594, and P  L?sirf  8.1431 15.8765 15.8765 56.2960  SL?sirf = 8.2431 Simulating and then averaging P\(k|k? 1 k example PDAF track-lost regime over 1000 timesteps gives P  L?sim  9.1253 17.3318 17.3318 54.3980  SL?sim = 9.2253 SL?sirf has 10.8% error when compared to the more time-consuming simulation results. In general, it has been observed that the approximation for the tracking regime is more accurate than for the track-lost regime when compared to the simulation results 4321 G. Neyman-Pearson Decision Rule PDET ? 0.99 and PFA ? 0.01 are reasonable targets for the decision rule. The number of effective innovations n necessary to meet these goals can be calculated by incrementing n until, for some value\(s and PFA ? 0.01 \(see Figure 2 the slope of the curve is ?np 0 0.005 0.01 0.015 0.02 0.975 0.98 0.985 0.99 0.995 1 target PD &amp; PFA PFA P D n=6 n=7 n=8 Fig. 2. Neyman-Pearson Threshold Curves As can be seen from the ?gure, the goals require n ? 7 timesteps. A good value for ?np \(one that exceeds the goals for both PDET and PFA 41 43 the means of 25 trials of the example experimental setup and the decision rule \(38 39 ST?sirf = 0.4286, and SL?sirf = 8.2431, the experimental results were PDET = 0.9997 and PFA = 0.0132. Though the actual process of track-loss is dif?cult to quantify, the experimental results n  T = 7.0 and n  L = 24.9 do provide some insight into the number of timesteps necessary for track-loss detection, as the average number of timesteps necessary, n  Loss, should be bounded by n  T ? n  Loss ? n  L V. SIMULATION RESULTS Table 1 provides a comparison of experimental values for PDET, PFA, n  T, and n  F across large ranges of NTA and NCD for PDAF simulation data when PD = 1. For each NTA-NCD combination, n and ?np have been chosen such that, theoretically, PDET ? 0.99 and PFA ? 0.01, with n as small as possible. The mean track-lifetime of all NTA-NCD combinations is approximately 100 timesteps where NCD is labeled  High  1000 timesteps where NCD is labeled  Medium  and 10,000 timesteps where NCD is labeled  Low  12]. The experimental values of PDET, and PFA were calculated using the means of 25 trials of the example experimental setup; trials were selected from realizations where track-loss did not occur until forced at timestep 1001 


where track-loss did not occur until forced at timestep 1001 Having the tracking and track-lost variances of the innovations spaced well apart is the condition for small n As Table 1 shows, this condition is met less often for low values of NTA In the track-lost regime, the assumption that the validated prediction errors are Gaussian distributed is usually quite conservative since the sample variances are generally larger than for a Gaussian distribution. This contributes to generally exceeding the PDET goals. Conversely, the sample variances in the tracking regime, while more Gaussian are still somewhat larger than would be expected from a Gaussian distribution, meaning that the PFA goals may not always be met. However, because n is restricted to integer values, the theoretical values of both PDET and PFA often exceed the desired values signi?cantly for low values of n Unlike the tracking regime, the track-lost regime cannot be described solely in terms of PD , NTA, and NCD. SL is non-linearly dependent on ?, so n, PDET, and PFA vary with ? as well. This can be seen by comparing the Medium NTA results in Table 1 with those in Table 2, where identical values of PD , NTA, and NCD constructed from different values of Q, R, and ? yield different results for n In general, lowering PD increases the tracking regime innovations variance ST?sirf , reducing the separation from SL?sirf and thus having the tendency to raise the required n to meet the goals for PDET and PFA. This can be seen by comparing the Low NTA data in Table 1 \(PD = 1 Table 3 \(PD = 0.9 Over the parameter space explored, the test \(decision rule VI. CONCLUSION AND ONGOING WORK A strategy has been laid out for creating a two-class decision rule to determine the regime of operation for the PDAF in the absence of truth data. Scalar information reduction factors can be used in an iterative scheme to predict the steady-state innovations covariance for both the tracking and track-lost regimes, which results in lower computational burden when compared to Monte Carlo simulation. Then a distribution can be assumed for the sample variances of the prediction errors. Together, these pieces of information constitute a model around which a Neyman-Pearson decision rule can be constructed, where the con?dences in both the probability of track-loss detection and of false alarms are explicitly chosen. Good performance of the test as a trackloss detector was demonstrated for an example system over a large range of tracking dif?culties Ongoing work includes modeling the effective innovations \(45 theoretical distributions for the prediction errors used in the decision rule \(38 desirable to more accurately model the sample distribution of the innovations variance, and it has been shown that the prediction errors of many data association algorithms can be well approximated by a Gaussian mixture [13 REFERENCES 1] Y. Bar-Shalom and T. Fortmann, Tracking and Data Association, Academic Press Inc., 1988 2] T. Fortmann, Y. Bar-Shalom, and Y. Scheffe  Sonar Tracking of Multiple Targets Using Joint Probabilistic Data Association  IEEE J. of Oceanic Engineering, July 1983 4322 TABLE I SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.152e3 1.153e3 1.073e4 5.963e3 11 11.0 336 0.591 0.9900 0.0081 1.0000 0.0099 5e-5 1e-4 0.1 Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





