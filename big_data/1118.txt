Eager Learning in Two-Stages for Precise and Complete Web Personalization Olfa Nasraoui Dept. of Computer Science and Engineering Speed Scientific School, University of Louisville Louisville, KY  40292 Mrudula Pavuluri Dept. of Electrical and Computer Engineering The University of Memphis Memphis, TN  38152-3180 Abstract  We present a systematic approach to automatic web recommender systems based on Web usage mining in a first stage to learn user profiles, and a second data mining phase that is devoted to learning several accurate models for predicting user requests for each profile. Our approach differs from existing methods because it includes two separate learning phases 
one to learn the user profiles, and another to learn a recommendation model. Most previous approac hes do not include adaptive learning in a separate seco nd phase, and instead base the recommendations on simple assumptions such as nearest profile recommendations, or deployment of pre-discovered association rules Index Terms  personalization, recomme nder systems collaborative filtering, web usage mining, neural networks I I NTRODUCTION Personalization aims to customize the interactions on a website depending on the user’s explicit and/or implicit interests. The move from traditional physical stores of products or information such as grocery stores or libraries virtual stores of products or information \(such as e-commerce sites and 
digital libraries  has practically eliminated physical constraints traditionally limiting the number and variety of products in a typical inventory. At the same time, the move from the physical to the virtual space has flattened the traditional three dimensional layout of products for which access is further facilitated thanks to the sales representativ e or librarian who know their products and their customers As a result, the users can be overloaded by the huge number of options, most of which they may never even get to know. Hence, in both the e-commerce sector and digital libraries, Web personalization has become more of a necessity than an option. One of the most successful examples of personalization comes in the form of recommender systems  Automatic Web personalization can analyze the data to compute 
recommendations in different ways, including 1 Rule-based filtering In this approach, used frequently to customize products on e-commerce sites such as Dell on Line 000  the user answers several questions, until receiving a customized result such as a list of products. This approach is mostly based on heavy planning and manual concoc tions of a judicious set of questions, possible answer combinati ons, and customizations by an expert. It suffers from a lack in intelligence no automatic learning static  2 Content-based or Item-based filtering This system recommends items deemed to be similar to the items that the 
user liked in the past. Item similarity is typically based on domain specific item attributes such as author and subject for book items, artist and genre for music items worked well for Amazon 000  including brand new items in the recommendation process, since there is no need for any previ ous implicit or explicit user rating or purchase data to make recommendations. However, this model is unable to include any element of surprise, meaning that it cannot capture interest in items that are different and yet linked by some conceptual association 3 Collaborative filtering or user-based filtering Based on the assumption that users with similar past behaviors \(rating browsing, or purchase history\ila 
r interests, this system recommends items that are liked by other users with similar  s on a historic record of all user interests such as can be inferred from their ratings of the items on a website \(products or web pages explicit explicit ratings, previous purchases, customer satisfaction questionnaires\or implicit browsing activity on a website\Computing recommendations can be based on lazy or eager learning to model the user interests. In lazy learning all previous user activities are simp ly stored, until recommendation time, when a new user is compared against all previous users to identify those who are similar, and in turn generate recommended items that are part of these similar users’ interests For example 
K-Nearest-Neighbors  KNN  provide recommendations from th e previous history of the K most similar users. Lazy models are fast in training/learning, but they take up huge amounts of memory to store all user activities and can be slow at recommendation time because of all the required comparisons. On the other hand eager learning relies on data mining techniques to l earn a summarized model of user interests \(a decision tree, clus ters/profiles, etc requires only a small fraction of the memory needed in lazy approaches. While eager learning can be slow, and is thus performed 
offline using a learned model at recommendation time is generally much faster than lazy approaches.  Several approaches to automatically generate Web recommendations based on previously mining user’s Web navigation patterns or    Probabilistic models  frequent itemsets session clusters or user profiles can form a user model obtained using data mining Examples of previous profile modeling methods include Naives Bayes   and session clustering 3, 23 In order to take into account 0-7803-9158-6/05/$20.00 © 2005 IEEE The 2005 IEEE International Conference on Fuzzy Systems 1026 


the inherent uncertainties in all the phases of Web personalization presented a recommendation strategy based on Web usage mining based on clustering sessions and fuzzy approximate reasoning to recommend relevant URLs. Among the most popular methods the ones based on collaborative filtering and the ones based on fixed support association rule discovery may be the most difficult and expensive to use. This is because, for the case of high-dimensional and extremely sparse Web data, it is difficult to set suitable support and confid ence thresholds to yield reliable and complete web usage patterns Similarly collaborative models may struggle with sparse data, and do not scale well to the num  In this paper, we investigate several single-step and two-step recommender systems. The Context Sensitive Approaches based on single-step Recommender systems \(CSA-1-step-Rec simply predict the URLs that are part of the nearest estimated profile as recommendations The nearest profile prediction model simply bases its recommendations on the closest profile The Context Ultra-Sensitive Approaches based on two-step Recommender systems \(CUSA-2-step-Rec first maps a user session to one of the pre-discovered profiles, and then uses one of several profilespecific URL-predictor neural networks \(such as Multilayer Perceptron or Hopfield Autoassociativ e memory networks the second step to provide the final recommendations. Based on this classification, a different recommendation model is designed for each profile separately. Each neural network is trained to complete the missing URLs of several complete ground-truth sessions from a given profile, given as input several incomplete subsessions This learning is anal ogous to completing some missing parts of a puzzle. The tw o-step recommendation method not only handles overlap in user interests, but also can mend the effects of some types of misclassifications in the first nearest profile assignment step, and even mend the effect of a coarse profile dichotomy due to the profile discovery stage The rest of the paper is organized as follows. In Section 2 we summarize our profile discovery using Web usage mining In Section 3, we present the singl e-step profile prediction based recommendation process, and the two-step recommender system based on a committee of profile-specific URL-predictor neural networks In Section 4, we present our experimental results using real web usage data, and fina lly, in Section 5, we wrap with our conclusions II P ROFILE D ISCOVERY BASED ON W EB U SAGE M INING Our approach is based on first extracting user profiles or ratings using a method, such as Web usage mining. In this case the profile discovery can be executed offline by mining user access log files using the following steps 1 Preprocess log file to extract user sessions  2 Categorize sessions by clustering  3 Summarize the session categories in terms of user profiles  After automatically grouping sessions into different clusters, we summarize the session categories in terms of user profile vectors  p i  The k th component/weight of this vector  p ik  captures the relevance of URL k in the i th profile, as estimated by the conditional probability that URL k is accessed in a session belonging to the i th cluster III D ESCRIPTION OF THE S INGLE S TEP AND T WO S TEP R ECOMMENDATION S TRATEGY O PTIONS Let U url 1 url 2 url N U be a set of N U urls on a given web site visited in web user sessions s j  j 1 N s as defined in \(1 Let P  p 1  p 2  p N P be the set of N P Web user profiles computed by the profile di scovery engine. Each profile consists of a set of URLs associated with their relevance weights in that profile. The problem of recommendation can be stated as follows. Given a current Web user session vector s j s j 1 s j 2  s jN U   predict the set of URLs that are most relevant according to the user’s interest, and recommend them to the user usually as a set of Hypertext links dynamically appended to the contents of the Web document returned in response to the most recent Web query. It may be useful to associate the k th recommended URL with a corresponding URL relevance score  r j k Hence it is practical to denote the recommendations for current Web user session s j by a vector r j r j 1 r j 2 r jN U  I n this study, we limit the scores to be binary A Context Sensitive Approach Based on Single-Step Profile Prediction Recommender System CSA-1-step-Rec  A.1 Single-Step Nearest-Profile Prediction Based Recommender System The most rudimentary approach to profile based Web recommendation is to simply de termine the most similar profile to the current session, and to recommend the URLs in this profile, together with their URL relevance weights as URL recommendation scores. The simila rity score between an input session s and the i th profile p i can be computed using the cosine similarity as follows 000 000 000 000 000 000 000 U U U N k k N k ik N k k ik ine si s p s p S 1 1 1 cos 1 If a hierarchical Web site structure should be taken into account then a modification of the cosine sim that can take into account the Website structure can be used to yield the followi ng input membership 000 000 000 000 000 000 000 000 000 000 000 000 000 000¦\000 000 000 000 \000 ine si N k k N k ik N l N k k u il web si S s p s k l S p S U U UU cos 1 1 11     max 2 where S u is a URL to URL similarity matrix that is computed based on the amount of overlap bet ween the paths leading from the root of the website \(main page and is given by 000\013\000\014 000\013\000\014 000 000 000 000 000 000 000 000 000\020 000 000 1  max  1 max  1 min    j i j i u p p p p j i S 3 We refer to the special similarity in \(2 Web Session Similarity  A.2 Single-Step DecisionTree Based Profile Prediction Recommender System The nearest profile prediction model makes the critical assumption that sessions in different profiles are linearly separated. While this may be applicable for certain web mining methods, it may not be true for others. In order to be able to reliably map new unseen sessions to a set of mined profiles without such assumptions about the profiles or how they separate the sessions, we can resort to classification methods that The 2005 IEEE International Conference on Fuzzy Systems 1027 


are not based on distance or similarity computations. In this paper, we explore both decision trees and neural networks for this task. Once trained, using the decision tree or neural network model to classify a new session is fast, and constitutes the single step of the recommendation process, since the classified profile is the recommendation set. The decision tree profile prediction model is very similar to the nearest profile prediction model. An input binary vector is presented as  and a profile/class is predicted as the output. Each URL in the input vector is considered as an at tribute In learning first the entire training data set is presented An attribute value is tested at each decision node with two possible outcomes of the test, a branch and a sub-tree.  The class node indicates the class to be predicted A.3 Single-Step Neural Network Based Profile Prediction Recommender System In the neural based approach of profile prediction a feed-forward multilayer perceptron is used and is trained with Back-Propagation. The inputs \(session class or profile\odel remain the same as the ones described above. The neural network replaces the nearest profile classification Hence the input la yer of the network consists of as many input nodes as the number of valid URLs \(i.e N U nodes an output layer having one output node for each profile \(i.e N p nodes N U N p 2 nodes. The index of the output node with highest ac tivation indicates the final class/profile B Context Ultra-Sensi tive Approach Based on Two-Step Recommender System with A Committee Of Profile-Specific URL-Predictor Neural Networks \(CUSA-2-step-Rec The single-step Profile predict ion recommendation procedure is intuitively appealing and simple. In particular, its implementation and deployment in a live setting is very efficient essentially amounting to a look-up table However, it has several flaws i the degree of similarity between the current session and the nearest profile that is iden tified may not be taken into account ii the above procedure does not take into account sessions that are similar to more than a single profile iii it cannot handle sessions which are different from all known profiles, and iv the set of recommendations derive directly from the contents of a singl e \(assigned assigned to this profile, without any further distin ction between the specific access patterns. For this reason, we propose a twostep approach that in additi on to exploiting the profile information, is able to recommend more highly personalized recommendations that depend not only on the assigned profile  user-to-user collaboration filtering but also explicitly, on the input session itself item-to-item collaboration filtering B.1 Description of the Mu lti-Layer Perceptron URL-Predictor Neural Network A Multilayer predict the recommendation URLs The architecture of this network, shown in Figure 1, is different from the network used in the profile prediction scenario of Section A.3 This is because the number of output nodes is now equal to the number of input nodes. The neural network is trained to complete the missing URLs of several complete ground-t ruth sessions, given as input several incomplete subsessions. Thi s learning is analogous to completing some missing parts of a puzzle Each training input consists of a user sub-session ss a ground-truth complete session S while training by example teaches the network output nodes to conform to the remainder of this session  S-ss   This means that there is one output node per URL. Hence the architecture of the network can become extremely complex Training such a network may prove to be unrealistic on large websites that may consist of thousands of URLs. To overcome this problem, a separate network is learned for each profile independently with an architecture of its own. The number of input and output nodes depends only on the number of significant URLs in that profile and possibly those related to its URLs by URL-level or con ceptual/semantic similarity e.g using Eq. \(3\ber of hidden nodes is set to the average of number of input and output nodes. Figure 1 shows the architecture of each URL-predictor neural network. There will be a committee of N p specialized networks of similar kind used in developing this URL recomme ndation prediction model, as illustrated in Figure 2. Each of these networks is completely specialized to forming the recommendations for a single profile hence offering a local, more refined model Fig. 1. Architecture of a Profile-Specific URL-Predictor Multi-Layer Perceptron Neural Network used in CUSA-2-step-Rec B.2 Learning the Pro file-Specific URL-Predictor Neural Network Models The URL-Predictor network for each profile is learnt independently with a separate set of training data. Learning each network involves presenting a sub-session consisting of some of the URLs visited by the user belonging to that profile as input and adjusting the network weights by back propagation to recommend URLs that are not part of the sub-session given as input but which are a part of th e ground truth complete session as output of the network. For each ground truth complete session we find all the sub-sessions for window sizes 1-10, and use them to generate independent tr aining and testing sets Cosine similarity is used to map each sub-session to the closest profile and the URL-Predictor ne twork specialized for that profile is invoked to obtain the recommendations. A URL is considered to be recommended if its activation value exceeds a 0.5 at the corresponding output node of the invoked network The 2005 IEEE International Conference on Fuzzy Systems 1028 


Fig. 2. Context Ultra-Sensitive Approach based on Two-Step Recommendation Process CUSA-2-step-Rec using a Committee of Profile-Specific URL-Predictor Neural Networks \(Any URL-Predictor model can be substituted for the MultiLayer Perceptron, e.g. a Hopfield network C Recommendations Based On Autoassociative Memory Hopfield Networks Hopfield networks are a speci al kind of recurrent neural networks that can be used as associative memo A Hopfield network can retrieve a complete pattern stored through the training process from an imperfect or noisy version of it. In some sense, a recommender system performs a similar operation when it recommends certain URLs from an incomplete session Given N url fully connected \(via symmetric weights w ij between each two units i and j ultaneously as input and as output, and assuming th at the activation values x i  are bipolar \(+1/-1\al weights to memorize N p patterns can be determined by Hebbian learning as follows for all 000 000 000 p N p p j p i ij x x w 1 j i 000z 0, otherwise 4 During testing/recall when a new noisy pattern x new is presented as input, we set the activation at node i at iteration 0 to be x i 0  x new-i then the units are adjusted by iteratively computing, at each iteration t 000 000 000\016 000 url j N j t ij t i x w x 1 1 5 until the network converges to a stable state. However, the desired behavior of recall in a Hopfield network is expected to hold only if all the possible complete session prototypes can be stored in the Hopfield netw ork’s connection weights, and if these complete sessions do not interact or cross-talk  excessively. Severe deteriorati on starts occurring when the number of patterns exceeds a certain fraction of the number of nodes N p 0.138 N url  6 hence limiting a Hopfield recommender system to sites with a large number of URLs and yet very little variety in the user access patterns. This limitation is paradoxical in the context of large websites or transactional database systems. Our preliminary simulations with both a single global Hopfield network as well as several profi le-specific Hopfield networks have resulted in low recall qu alities since the network seemed to be able to memorize only very few stable states. However several profile-specific Hopfield networks perform better than one global network, but onl y for some of the profiles IV E XPERIMENTAL R ESULTS A Mining User profiles from Anonymous Web Usage Data 1703 web sessions accessing 343 URLs extracted from log files of a university Web server, were used to generate training and testing sets. For each complete session considered as the groundtruth all possible sub-sessions of different sizes are generated The test dataset forms an independent 20% of the sub-sessions Hierarchical Unsupervised Niche Clustering \(H-UNC partitioned the web sessions into 20 clusters, each characterized by one of 20 profile vectors that were validated for consistency B Comparative Simulation Results for CUSA-2-step-Rec CUSA-2-step-Rec, and K-NN Collaborative Filtering We used the following parameters in training the multilayer perceptron URL-Predictor neural networks Maximum number of epochs = 2000, Learning Rate = 0.7 \(for Input to Hidden layer and 0.07 for Hidden to Out put layer\omentum factor of 0.5.  The Collaborativ  is based on using K Nearest Neighbors \(K-NN\ top-N recommendations for different valu es of K and N. First the closest K complete sessions from the entire history of accesses are found. Then the URLs present in these top K sessions are sorted in decreasing order of their frequency and the top N URLs are treated as the recommendation set We show only the best results obtained for K-NN at K=50 neighbors and N=10 URLs Figures 3 and 4, depicting the 20-profile averaged precision and coverage measures, show that the two-step profile-specific URLpredictor multilayer perceptron neural network recommender system CUSA-2-step-Rec wins in terms of both precision and coverage, particularly above input sub-session size 2 It may at first appear unusual that a recommendation strategy scores highly on both precision and coverage, and that an increase in precision did not seem to com promise coverage in any way However by looking at the details of the design of the profilespecific URL-predictor neural network, we explain this relentless increase in precision by the fact that the neural network output is trained to predi ct only the URLs that the user has not seen before, i.e S-ss where S is the complete session and ss is the sub-session \(URLs visited by the user the sub-session size increases, more URLs are presented to the output of the neural network, making the prediction task easier since fewer URLs need to be predicted compared to smaller input sub-sessions. Similarly, coverage increases, since with more input URLs the neural network is able to predict more of the missing URLs to complete the puzzle. However, this does not happen at the expense of precisi on. On the contrary, giving more hints about the user in the form of more of the visited URLs makes the prediction task easier, and hence will only result in more accurate predictions We notice that the single-step recommender systems CSA-1step-Rec do not have this nice feature, i.e., precision and coverage will generally have opposing trends. The performance of k-NN fares competitively with all the single-step recommender strategies but only for longer session sizes. This is not surprising, considering that k-NN can yield very accurate predictions because it too is based on local context-sensitive The 2005 IEEE International Conference on Fuzzy Systems 1029 


profile Number of Nodes URLs Number of Sessions Average Cosine similarity Hopfiled Max Length Median Length models. However, k-NN is notorious for its excessive computational and memory costs at recommendation time in contrast to all the other investigated techniques While lazy in the learning phase, involving not hing more than storing the previously seen cases k-NN takes its toll during the recommendation phase when it needs to compare a new session with all past cases to produce recommendations Table 1 summarizes the characteristics of the session lengths for each profile The median session length for most profiles is larger than 5 and for six of the profiles \(0, 3, 4, 5, 11, and 15 is greater than 9 For these profiles, half of the sessions have length greater than or equal to 9. Moreover, because we generate a large number of subsession combinations from each session for testing, we end up with a reasonably large number of test sessions \(in the hundreds between session size 2 and 8. We notice from Fig. 3 and 4 that at longer session lengths above 5\mendations with CUSA-2-stepRec NN far exceeds that of k-NN. This can be explained by the fact that while the performance of k-NN eventually saturates and even starts decreasing beyond a certain session length, that of the CUSA-2-step-Rec NN approach can only improve, since each specialized network is essentially train ed to complete the missing pieces \(URLs\plete session, when given as input only some of the pieces. Hence it is only natural in this context that when more pieces are shown, a specialized neural network is better able to predict the missing pieces. The degradation of precision that results from higher coverage in kNN approaches is avoided because the neural networks in CUSA-2-step-Rec are trained to be precise while excessive coverage is controlled thanks to the specialization of each NN to only one of the profiles. Finally, we note that if all input subsession lengths are taken into account, then it is clear that a combination of several different recommender strategies, each applied only within its optimal range of sub-session length will outperform each one of the recommender strategies acting on its own In fact, in this case, even the very simple CSA-1-step-Rec strategy based on nearest profile identification outperforms all other strategies for very short input sessions \(< 2 URLs crucial to the retention and guidance of users who may be in their very initial browsing stages Finally, in Table 1 – column 4, we show the performance averaged over all session lengths of the CUSA-2-step-Rec approach when specialized Hopfield networks are used for each profile instead of the multilayer perceptron neural networks. It is important to note that, while testing both types of neural networks was performed in a similar fashion training them was a different matter. The Hopfiel d networks in our context are analogous to auto-associative memory banks. Hence they were trained to memorize each complete session, and not to complete missing parts of a complete sessions from a large number of incomplete subsessions as in the multilayer perceptron neural networks We notice that while some profiles can be handled using the Hopfield networks, the performance for many profiles is poor even sinking to complete failu re for profiles 10, 17, 18, and 19 We attribute this failure to the excessive amount of cross-talk between the patterns to be me morized by the Hopfield networks for these profiles compared to the low number of nodes/URLs especially in light of the cons traint in \(6\ple, as shown in Table 1 the Hopfield network for profile 18 had to memorize a large number of patterns N p  65 training sessions in contrast with only N url 5 nodes. We have also trained a single global Hopfield network for all pr ofiles to predict the URLs of incomplete sessions. Note that in this case, the constraint in 6 is severely violated with N p 1703 training patterns and N url  343 nodes Not surprisingly the average similarity between the memorized and retrieved sessions obtained in this case, was nil  TABLE 1: SESSION PROPERTIES AND AVERAGE COSINE SIMILARITY BETWEEN COMPLETE SESSION AND SESSION RETRIEVED FROM AN INCOMPLETE INPUT USING SEVERAL SPECIALIZED HOPFIELD NETWORKS \(ONE PER PROFILE A SINGLE GLOBAL HOPFIELD NETWORK WAS USED FOR ALL PROFILES WAS NIL  0 189 106 57 40 10 1 194 104 4 40 6 2 171 177 6 132 7 3 101 61 18 40 10 4 134 58 47 40 9 5 153 50 13 132 10 6 104 116 43 24 5 7 64 51 62 23 7 8 139 134 29 36 4 9 73 41 31 25 3 10 134 95 0 19 4 11 98 185 68 36 9 12 170 74 26 132 5 13 136 38 27 132 5 14 163 33 28 31 6 15 86 51 54 37 9 16 105 77 3 132 2 17 23 68 0 6 1 18 5 65 0 3 1 19 24 120 0 10 2 Precision 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 01234567891 subsession size Precision K-NN\(K=50,N=10 1-step Nearest Profile-Cosine 1-step Nearest Profile1-step Decision Tree 1-step Neural Network 2-step Profile-specific NN 0 Fig. 3. Precision Values for all recommendation strategies  CSA-1-step-Rec  CUSA-2-step-Rec and K-NN  V C ONCLUSIONS We have investigated several single-step and two-step recommender systems. The single-step recommender systems CSA-1-step-Rec simply predict the URLs that are part of the nearest estimated profile as recommendations. The nearest profile prediction model simp ly bases its recommendations on the closest profile based on a similarity measure, hence favoring linearly separable profile classes. In or der to be able to reliably map new unseen sessions to a set of mined profiles without such assumptions about the profile separation, we can resort to more powerful classification methods We explored both decision The 2005 IEEE International Conference on Fuzzy Systems 1030 


trees and neural networks for this task. Once trained, using the decision tree or neural network m odel to classify a new session constitutes the single step of the recommendation process, since the classified profile is the recommendation set. The two-step recommender system CUSA-2-step-Rec first maps a user session to one of the pre-discovered profiles and then uses one of several profile-specific URL-predi ctor neural networks in the second step to provide the final recommendations. A  different recommendation model specializes to each profile Coverage 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 01234567891 subsession size Coverage K-NN\(K=50,N=10 1-step Nearest Profile-Cosine 1-step Nearest Profile-WebSession 1-step Decision Tree 1-step Neural Network 2-step Profile-specific NN 0 Fig. 4. Coverage Values for all recommendation strategies CSA-1-step-Rec  CUSA-2-step-Rec and K-NN  The Hopfield auto-associative memory network is an alternative to the multilayer percep tron, and is trained to memorize a complete session Our experiments confirmed that Hopfield networks can only form a reliable memory bank under severe constraints governing the relationship between the number of patterns to be memorized and the number of units in the network, and that unfortunately, these constraints are easily iolated in typical real web usage environments Nevertheless several profile-specialized Hopfield networks in a CUSA-2-stepRec framework performed better than a single network. The idea of using a separate network specialized to each profile provides an even higher level of context awareness in personalization than the level already offered through collaborative filtering This modular design could be ext ended by replacing the URLPredictor neural network modul es by different learning paradigms that are faster to train, while not compromising the accuracy of predictions ACKNOWLEDGEMENTS This work is supported by a National Science Foundation REFERENCES  M. Perkowitz and O. Etzioni atically Sep.2002  M Pazzani and D. Billsus, Learning and revising User Profiles: The identification of Interesting Web Sites, Machine Learning, Arlington 27, pp. 313-331, 1997  rtin-Bautista, and M.A Vila Textual Information Retrieval with User Profiles Using Fuzzy Clustering and Inferencing, in Intelligent Exploration of the Web Szczepaniak, P.S Segovia, J., Kacprzyk, J., and Zadeh, L.A. \(eds.\Physica-Verlag Heidelberg, Germany, 2002  B. Mobasher, H. Dai, T Luo, and M. Nakagawa Effective personalizaton based on association rule discovery from Web usage data, ACM Workshop on Web information and data management Atlanta, GA, Nov. 2001  J H. Holland Adaptation in natural and artificial systems MIT Press, 1975  L. Zadeh \(1965 sets. Inf. Control 8, 338-353  J. Borges and M. Levene Patterns, in Web U  Lecture Notes in ive Foundation  Macmillan, New York, 1994  J R Quinlan Inductio  M achine Learning V ol v CAREER Award IIS-0133948 to Olfa Nasraoui Adaptive web sites: Autom learning for user access pattern. Proc. 6th int. WWW conference, 1997  R. Cooley B. Mobasher and J Srivastava Web Mining Information and Pattern discovery on the World Wide Web, Proc. IEEE Intl. Conf. Tools with AI, Newport Beach, CA, pp. 558-567, 1997  O. Nasraoui and R. Krishnapuram and A Joshi Mining Web Access Logs Using a Relational Clustering Algorithm Based on a Robust Estimator, 8 th International World Wide Web Conference Toronto, pp. 40-41, 1999  O. Nasraoui, R. Krishnapuram, H. Frigui, and A. Joshi. Extracting Web User Profiles Using Relational Competitive Fuzzy Clustering International Journal on Artificial Intelligence Tools, Vol. 9, No. 4, pp 509-526, 2000  O Nasraoui and R K rishnapuram A Novel Approach to Unsupervised Robust Clustering using Genetic Niching, Proc. of the 9 th IEEE International Conf. on Fuzzy Systems, San Antonio, TX May 2000, pp. 170-175  O Nasraoui and R. Krishnapuram. A New Evolutionary Approach to Web Usage and Context Sensitive Associations Mining, International Journal on Computational Intelligence and Applications Special Issue on Internet Intelligent Systems, Vol 2 No 3 pp 339-348  G. J. Klir and B. Yuan Fuzzy Sets and Fuzzy Logic Prentice Hall 1995, ISBN 0-13-101171-5  R. Agrawal and R. Srikant \(1994 mining association rules, Proceedings of the 20th VLDB Conference, Santiago Chile, pp. 487-499  G. Linden, B. Smith, and J. York Amazon.com Recommendations Iem-to-item collaborative filtering, IEEE Internet Computing Vo 7 No. 1, pp. 76-80, Jan. 2003  J. Breese, H. Heckerm an, and C. Kadie Em pirical Analy sis o f Predictive Algorithms for Collaborative Filtering Proc 14 th Conf Uncertainty in Artificial Intelligence, pp. 43-52, 1998  J.B. Schafer, J. Konstan, and J. Reidel, Recom m e nder Sy stem s in E-Commerce, Proc. ACM Conf. E-commerce, pp. 158-166, 1999   J Srivastava R Cooley  M Deshpande, and P-N Tan, Web usage mining Discovery and applications of usage patterns from web data SIGKDD Explorations, Vol. 1, No. 2, Jan 2000, pp. 1-12  O Zaiane, M. Xin, and J. Han, Discovering web access patterns and trends by applying OLAP and data mining technology on web logs in "Advances in Digital Libraries", 1998, Santa Barbara, CA, pp. 19-29  M. Spiliopoulou and L. C. Faulstich WUM A Web utilization Miner, in Proceedings of EDBT workshop WebDB98, Valencia, Spain 1999 Data Mining of User Navigation sage Analysis and User Profiling Computer Science", H. A. Abbass, R A Sarker, and C.S. Newton Eds Springer-Verlag , pp. 92-111,1999  S. Hay kin Neural Networks A Comprehens n of Decision Trees 1, pp. 81--106, 1986  Nasraoui O., Petenes C., “Combining Web Usage Mining and Fuzzy Inference for Website Personalization,” In Proc of WebKDD 2003 Workshop on Web mining as a Premise to Effective and Intelligent W eb Applications, Washington DC, August 2003 The 2005 IEEE International Conference on Fuzzy Systems 1031 


 CONNECT  Average size of closed 155 5 10 5 316 10 82 15 19 4897 20 1197 30 427 89 9017 I 1 3.767 2 5.840 3 7.138 4 8.364 5 8.998 6 7 11.179 1 3.418 2 4.621 3 6.358 4 7.332 5 8.218 11 1 2.5 2 4.041 3 5.052 4 6.209 5 7.333 6 8.5 2.045 2 5.322 4 6 7 10.272 8 11.341 1 1.952 3.691 3 5.174 4 6.512 5 7.751 6 8.911 7 10.006 8 1 1.032 1 1.952 2 3.596 3 5.017 4 6.323 5 7.539 6 8.671 7 9.732 8 10.71 1 2 3.543 4.61 5.612 6.573 7.525 8.453 9.319 10: 10 1.21 2.401 3.537 4.564 5.546 6.518 7.472 8.369 9.181 1 1.25 2.366 3.406 4.414 5.394 7.185 8 Table 6 Experimental results rules This additional knowledge highlighting connected rules to a user select rule allows an improvement of machine interaction To do so we constructed a set of fuzzy meta rules extracted from the discovered fuzzy closed sets The visualization prototype is currently under imple mentation and in the near we plan also to include rec ommended visualization tasks history and extract and to lead extensive experimental results to assess their sat isfaction vs the user graphical interface Acknowledgments The authors would like to thank Mrs Audrey for its helpful efforts in the Figure 3 Displaying derived association rule of visualization prototype and the anonymous review ers for their helpful comments This work was partially fi nanced by the region Nord Pas de Calais References P Adriaans and D Zantinge Data mining Addison Wesley 1997 2 R. Agrawal and R Fast algorithms for mining asso ciation rules In Proceedings of the 20th International Con ference on Very Large Databases pages June 1994 3 M and B Monjardet Ordre et classijication Hachette Tome 1970 4 Y N. Pasquier R Taouil L Lakhal and G Stumme. Mining minimal non redundant association rules using frequent closed itemsets In Proceedings of the International Conference Lecture Notes in Computer Sciences, Springer verlag pages 972 986 july 2000 5 R Belohlavek. Fuzzy Galois connections Math Logic 1999 6 S BenYahia and A Jaoua. Discovering knowledge from fuzzy concept lattice In mining and Computational Intelligence Studies in Fuzziness and Computing Vol 68 chapter 7 pages Verlag Heidelberg March 2001 7 S BenYahia and E M Nguifo. Approches dextraction de dassociation la correspondance de galois ISI 2004 8 S BenYahia and E M Nguifo. Revisiting generic bases of association rules In Proceedings of 6th International Proceedings of the 16th International Conference on Tools with Artificial Intelligence ICTAI 2004 20.00 2004 C OM P U T E R SOCIETY 


Conference on Data Warehousing and Knowledge Discov ery Da 2004 Springer Verlag to appeal Spain 2004 9 S and Discovering Association Rules using the Formal Concept Analysis Extraction des Apprenrissage special issue of Intl Conference of es francophones et Gestion des France January 2002 Brin R Motawni and C Silverstein. Beyond market baskets  Generalizing association rules to correlation In Proceedings of the SIGMOD Tucson Arizona USA pages 265 276 May 1997  S Brin, R Motwani and J Ullman. Dynamic count ing and implication rules In Proceedings ACM SIGMOD conference on Management of Data Arizona USA pages 255 264 1997 P Buono M F Costabile, and F A. Lisi Supporting data analysis through visualizations In Proceedings of Workshop on Visual Data Mining Freiburg Germany pages 67 78 September 2001 Recherche de dassociations par une approche Revue spe cial issue of Intl Conference of francophones et Gestion des Clermont Ferrand France Cepaduks Editions 572, 19 23 January 2004 F and R Demolombe. How to recognize inter esting topics to provide cooperative answering Systems 1989 V Frantz and J. Shapiro Algorithms for automatic con struction of query formulations in boolean form Journal of the American for Information Science 1991 B. Ganter and R Formal Concept Analysis Verlag Heidelberg, 1999 J Guigues and V Duquenne Familles minimales dimplications informatives dun tableau de binaires Sciences  J. Han and Y Fu Discovery of multiple level association rules from large databases In Proceedings of the VLDB Con ference pages 1995  A. Jaoua S Elloumi S and F con nection in fuzzy binary relations: applications for discover ing association rules and decision making Methodos Pub lisher, 2002 M. Kryszkiewicz Concise representations of association In D J Hand N Adams, and R editors Proceedings of Detection and Discovery, ESF Ex ploratory Workshop, London volume 2447 of Lec ture Notes in Computer Science pages 92 109 Springer September 2002 M. Luxenburger Implication dans un contexte Sciences 29 1 0 Couturier E M Nguifo, and B 1986 H H Toinoven and I Verkamo Efficient algo rithms for discovering association rules In Worshop on Knowledge Discovery in Databases pages 18 1 1 92, July 1994 R Meo G Psaila and S Ceri A new sql like operator for mining association rules In Proceedings of the VLDB Con ference pages 122 133 1996 A Motro Extending the relational Databases Model to sup port Goal queries. In L Lerschberg, editor Expert Database Systems pages The Publish ing Company, 1987 R T Ng V S Lakshmanan, J han and A Pang. Exploratory mining and pruning optimizations of constrained association rules In Proceedings of the Conference pages 24, 1998 S Pollandt Fuzzy Begriffe Begriffsanalyse scharfer Springer verlag, 1997 B. Shneiderman The eyes have it: A task by data type tax onomy for information visualization In I C S Press, editor Proceedings IEEE Symposium on Visual Languages del Colorado 336 343, 1996 R. Srikant Q and R. Agrawal Mining association rules with item constraints In Proc. of the 3rd Intl Conference on Knowledge Discovery in Databases and Data Mining port Beach, California pages 67 73 August 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence ICTAI 2004 20.00 2004 IEEE SOCIETY 


 1   1   l  1 i 2  i  Hence the true frequency count of any item occurring on some input stream must be C  l  1 j  x 1  tc  j    j    where  is a small quantity 3  The number of items present in each input stream is thus n  C 4  Since synopses for d l  1  x input streams are transmitted through a node at level x  the load on the most heavily loaded link\(s is L  x  d l  2  x  n C  Clearly the maximum value of L  x  is achieved when   0  The expression for L  x  can be simpli“ed to L  x  1  l  1 j  x 1  j  d x  j 1  Now our expression for the worst-case load on any link can be reduced to W  T    max x 0  1 l  2  L  x   We desire to minimize W  T   subject to the constraints  2   l  1  0 and  l  1 j 2  j   1 It is easy to show that this minimum is achieved when L 0  L 1    L  l  2  Solving for  2   l  1  we obtain  i   1  d  1  l  2   d  1 d  2  i  l  2 and  l  1   1  d  l  2   d  1 d  Translating to error tolerances we set  i   1   l  1  i    d  1 d  l  2   d  1 d for all 2  i  l  1  This setting of  2  l  1 minimizes worst-case communication load on any link We term this strategy MinMaxLoad WC Under this strategy the maximum possible load on any link is L wc   l  2   d  1 d d   1 counts per epoch Lastly we note that MinMaxLoad WC remains the optimal precision gradient even if nodes of the same level can have different  values Informally since with worst-case inputs all incoming streams have identical characteristics maximum link load cannot be improved by using non-uniform  values for nodes at a given level we omit a formal proof for brevity 2.1.4 Good Precision Gradients for Non-Worst-Case Inputs Real data is unlikely to exhibit worst-case characteristics Consequently strategies that are optimal in the worst case may not always perform well in practice In terms of minimizing the maximum communication load on any link the worst-case inputs are ones in which the set of items occurring on each input stream are disjoint When this situation arises a gradual precision gradient is best to use as shown in Section 2.1.3 Using a gradual precision gradient some of the pruning of frequency 3 Recall that we allow the frequency of an item to be a real number 4 More precisely each stream contains  n C  items of weight 1 each and one item of weight  n C  n C   Note that each input stream contains at most one item with weight less than 1 as stipulated earlier counts is delayed until a better estimate of the overall distribution is available closer to the root thereby enabling more effective pruning In the opposite extreme when all input streams contain identical distributions of item occurrences there is no bene“t to delaying pruning and performing maximal pruning at the leaf nodes as in strategy SS2 is most effective at minimizing communication In fact it is easy to show that SS2 is the optimal strategy for minimizing the maximum load on any link when all input streams are comprised of identical distributions we omit a formal proof Note however that SS2 still incurs a high space requirement on the root node R since it sets  1    We posit that most real-world data falls somewhere between these two extremes To determine where exactly a data set lies with regard to the two extremes we estimate the commonality between input streams S 1 S m by inspecting an epoch worth of data from each stream We compute a commonality parameter   0  1 as   1 m   m i 1 G i L i  where G i and L i are de“ned over stream S i as follows The quantity G i is de“ned as the number of distinct items occurring in S i that occur at least   S i  times in S i and also at least   S  times in S  S 1  S 2  S m  where  S  denotes the number of item occurrences in S during the epoch of measurement The quantity L i is de“ned as the number of distinct items occurring in S i that occur at least   S i  times in S i  Hence commonality parameter  measures the fraction of items frequent enough in one input stream to be included in a leaf-level synopsis by strategy SS 2 that are also at least as frequent globally in the union of all input streams A natural hybrid strategy is to use a linear combination of MinMaxLoad WC and SS2 weighted by   The strategy is as follows set  i 1       1   l  1  i    d  1 d  l  2   d  1 d        for 2  i   l  2  and  l  1 1       1  d  l  2   d  1 d         We term this hybrid strategy MinMaxLoad NWC for non-worstcase Commonality parameter  1 implies that locally frequent items are also globally frequent and SS2 modi“ed to use  1   is a good choice Conversely  0 indicates that MinMaxLoad WC is a good choice For 0  1  a weighted mixture of the two strategies is best 2.1.5 Summary The precision gradient strategies we have introduced are summarized in Table 3 Sample precision gradients are illustrated in Figure 3 3 Experimental Evaluation In this section we evaluate the performance of our newly-proposed strategies for setting the precision graProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


Table 3 Summary of precision gradient settings studied  Strategy Description Section Introduced Simple Strategy 1 SS1 Transmits raw data to root node R 1.3 Simple Strategy 2 SS2 Reduces data maximally at leaves 1.3 MinRootLoad Minimizes total load on root in all cases 2.1.2 MinMaxLoad WC Minimizes worst-case maximum load on any link 2.1.3 MinMaxLoad NWC Achieves low load on heaviest-loaded link under non-worst-case inputs 2.1.4 0 0.0002 0.0004 0.0006 0.0008 0.001 43210 Tree level \(i SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC Error tolerance  i input            leaf root Figure 3 Precision gradients for  0  001   0  5  dient using the two na  ve strategies suggested in Section 1 as baselines We begin in Section 3.1 by describing the real-world data and simulated distributed monitoring environment we used Then in Section 3.2 we analyze the data using our model of Section 2.1.4 to derive appropriate parameters for our MinMaxLoad NWC strategy that is geared toward performing in the presence of non-worst-case data We report our measurements of space utilization on node R in Section 3.3 and provide measurements of communication load in Section 3.4 3.1 Data Sets As described in Section 1 our motivating applications include detecting DDoS attacks and monitoring hot spots in large-scale distributed systems For the rst type of application we used traf“c logs from Internet2 and sought to identify hosts recei ving lar ge numbers of packets recently For the second type we sought to identify frequently-issued SQL queries in two dynamic Web application benchmarks con“gured to execute in a distributed fashion The I NTERNET 2 traf c traces were obtained by collecting anonymized net”ow data from nine core routers of the Abilene network Data were collected for one full day of router operation and were broken into 288 ve-minute epochs To simulate a larger number of nodes we divided the data from each router in a random fashion We simulated an environment with 216 network nodes which also serve as monitor nodes For the web applications we used Java Servlet versions of two publicly available dynamic Web application benchmarks RUBiS and R UBBoS 10 R U BiS is modeled after eBay an online auction site and RUBBoS is modeled after slashdot an online bulletin-board so we refer them as AUCTION and BBOARD  respectively We used the suggested con“guration parameters for each application and ran each benchmark for 40 hours on a single node.We then partitioned the database requests into 216 groups in a roundrobin fashion honoring user session boundaries We simulated a distributed execution of each benchmark with 216 nodes each executing one group of database requests and also serving as a monitor node For all data sets we simulated an environment with 216 monitoring nodes  m  216  and a communication hierarchy of fanout six  d 6  Consequently our simulated communication hierarchy consisted of four levels including the root node  l 4  We set s 0  01   0  1  s  and  1 0  9    Our simulated monitor nodes used lossy counting in batch mode whereby frequency estimates were reduced only at the end of each epoch in all cases less than 64KB of buffer space was used to create synopses over local streams The epoch duration T was set to 5 minutes for the I NTERNET 2 data set and 15 minutes for the other two data sets 3.2 Data Characteristics Using samples of each of our three data sets we estimated the commonality parameter  for each data set Recall that we use  to parameterize our strategy MinMaxLoad NWC presented in Section 2.1.4 We obtained  values of 0.675 0.839 and 0.571 for the I NTER NET 2 AUCTION and BBOARD data sets respectively Hence the AUCTION data set exhibited the most commonality among all three data sets Results presented in Section 3.4 show that AUCTION indeed has the most commonality 3.3 Space Requirement on Root Node Figure 4 plots space utilization at the root node R as a function of time in units of epochs using Algorithm 2a to generate the synopsis for different values of the decay parameter   using two different strategies for the precision gradient The plots shown are for the I N TERNET 2 data set The y-axis of each graph plots the current number of counts stored in the     synopsis S A maintained by the root node R  Figure 4a plots synopsis size under our MinMaxLoad WC strategy under three different values of   0.6 0.9 and 1 As predicted by our analysis in when  1 the size of S A remains roughly constant after reaching steady-state whereas when  1 synopsis size increases logarithmically with time similar results were obtained for the Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 0 100 200 300 400 500 600 1 21416181 Time \(epoch counts 0.6 0.9 1.0 a MinMaxLoad WC 0 1000 2000 3000 4000 5000 6000 7000 8000 1 21416181 Time \(epoch counts 0.6 b SS2 Figure 4 Space needed at node R to store answer synopsis S A  non-distributed single-stream case In contrast when SS2 is used to set the precision gradient Figure 4b the space requirement is almost an order of magnitude greater This difference in synopsis size occurs because in SS2 frequency counts are only pruned from synopses at leaf nodes so counts for all items that are locally frequent in one or more local streams reach the root node No pruning power is reserved for the root node and therefore no count in S A is ever discarded irrespective of the  value The same situation occurs if Algorithm 2b is used instead of Algorithm 2a This result underscores the importance of setting  1  in order to limit the size of S A  as discussed in Section 2.1 3.4 Communication Load Figure 5 shows our communication measurements under each of our two metrics for each of our three data sets under each of the ve strategies for setting the precision gradient listed in Table 3 First of all as expected the overhead of SS1 is excessive under both metrics Second by inspecting Figure 5a we see that strategy MinRootLoad does indeed incur the least load on the root node R in all cases as predicted by our analysis of Section 2.1.2 Under this metric MinRootLoad outperforms both simple strategies SS1 and SS2 by a factor of 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 247k 10k 296k 132k 20k counts transmitted \(per epoch a Load on root node R 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 43k    21k 52k   19k 23k   7k counts transmitted \(per epoch b Maximum load on any link Figure 5 Communication measurements k denotes thousands ve or more in all cases measured However MinRootLoad performs poorly in terms of maximum load on any link as shown in Figure 5b because no early elimination of counts for infrequent items is performed and consequently synopses sent from the grand-children of the root node to the children of the root node tend to be quite large As expected MinMaxLoad NWC performs best under that metric on all data sets For the AUCTION data set even though SS2 outperforms MinMaxLoad WC to be expected because of the high  value our hybrid strategy MinMaxLoad NWC is superior to SS2 by a factor of over two For the I NTERNET 2 and BBOARD data sets the improvement over SS2 is more than a factor of three On the negative side total communication not shown in graphs is somewhat higher under MinMaxLoad WC than under SS2 increase of between 7.5 and 49.5 depending on the data set 4 Related Work Most prior work on identifying frequent items in data streams 6 8 9 19 only considers the single-stream case While we are not aware of any work on maintaining frequency counts for frequent items in a distributed stream setting work by Babcock and Olston does adProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


dress a related problem In the problem is to monitor continuously changing numerical values which could represent frequency counts in a distributed setting The objective is to maintain a list of the top k aggregated values where each aggregated value represents the sum of a set of individual values each of which is stored on a different node The work of assumes a single-le v e l communication topology and does not consider how to manage synopsis precision in hierarchical communication structures using in-network aggregation which is the main focus of this paper The work most closely related to ours is the recent work of Greenwald and Khanna which addresses the problem of computing approximate quantiles in a general communication topology Their technique can be used to nd frequencies of frequent items to within a con“gurable error tolerance The work in focuses on providing an asymptotic bound on the maximum load on any link our result adheres to the same asymptotic bound It does not however address how best to con“gure a precision gradient in order to minimize load which is the particular focus of our work 5 Summary In this paper we studied the problem of nding frequent items in the union of multiple distributed streams The central issue is how best to manage the degree of approximation performed as partial synopses from multiple nodes are combined We characterized this process for hierarchical communication topologies in terms of a precision gradient followed by synopses as they are passed from leaves to the root and combined incrementally We studied the problem of nding the optimal precision gradient under two alternative and incompatible optimization objectives 1 minimizing load on the central node to which answers are delivered and 2 minimizing worst-case load on any communication link We then introduced a heuristic designed to perform well for the second objective in practice when data does not conform to worst-case input characteristics Our experimental results on three real-world data sets showed that our methods of setting the precision gradient are greatly superior to na  ve strategies under both metrics on all data sets studied Acknowledgments We thank Arvind Arasu and Dawn Song for their valuable input and assistance References  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB  1994  I Akamai T echnologies Akamai http://www akamai.com   A Ak ella A Bharambe M Reiter  and S Seshan Detecting DDoS attacks on ISP networks In PODS Workshop on Management and Processing of Data Streams  2003  A Arasu and G S Manku Approximate quantiles and frequency counts over sliding windows In PODS  2004  B Babcock and C Olston Distrib uted top-k monitoring In SIGMOD  2003  M Charikar  K  Chen and M F arach-Colton Finding frequent items in data streams In International Colloquium on Automata Languages and Programming  2002  E Cohen and M Strauss Maintaining time-decaying stream aggregates In PODS  2003  G Cormode and S Muthukrishnan What s hot and whats not Tracking frequent items dynamically In PODS  2003  E D Demaine A Lopez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space In European Symposium on Algorithms  2003  DynaServ er R UBis and R UBBos http://www.cs rice.edu/CS/Systems/DynaServer   eBay Inc eBay  http://www.ebay.com   C Estan and G V a r ghese Ne w directions in traf c measurement and accounting In SIGCOMM  2002  M F ang N Shi v akumar  H  Garcia-Molina R Motwani and J Ulmann Computing iceberg queries ef“ciently In VLDB  1998  P  B Gibbons and Y  Matias Ne w sampling-based summary statistics for improving approximate query answers In SIGMOD  1998  P  B Gibbons and S T irthapura Estimating simple functions on the union of data streams In Symposium on Parallel Algorithms and Architectures  2001  M Greenw ald and S Khanna Po wer conserving computation of order-statistics over sensor networks In PODS  2004  J Han J Pei G Dong and K W ang Ef cient computation of iceberg queries with complex measures In SIGMOD  2001  Internet2 Internet2 Abilene Netw ork http abilene.internet2.edu   R M Karp S Shenk er  and C H P apadimitriou A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  2003  H.-A Kim and B Karp Autograph T o w ard automated distributed worm signature detection In Proceedings of the 13th Usenix Security Symposium  2004  A Manjhi V  Shkapen yuk K Dhamdhere and C Olston Finding recently frequent items in distributed data streams Technical report 2004 http://www cs.cmu.edu/˜manjhi/freqItems.pdf   G S Manku and R Motw ani Approximate frequenc y counts over data streams In VLDB  2002  Open Source De v elopment Netw ork Inc Slashdot http://slashdot.org  Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


