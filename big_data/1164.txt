Effective Data Mining by Integrating Genetic Algorithm into the Data Preprocessing Phase Janaki Gopalan 1 Erkan Korkmaz 2 Reda Alhajj 1  3 Ken Barker 1 1 Dept of Computer Science 2 Dept of Computer Engineering 3 Dept of Computer Science University of Calgary Yeditepe University Global University Calgary Alberta Canada Kadikoy Istanbul Turkey Beirut Lebanon Abstract Dividing a data set into a training set and a test set is a fundamental component in the pre-processing phase of data mining DM Effectively the choice of the training set is an important factor in deriving good classi\223cation rules Traditional approach for association rules mining divides the dataset into training set and test set based on statistical methods In this paper we highlight the weaknesses of the existing approach and hence propose a new methodology that employs genetic algorithm GA in the process In our approach the original dataset is divided into sample and validation sets Then GA is used to 223nd an appropriate split of the sample set into training and test sets We demonstrate through experiments that using the obtained training set as the input to an association rules mining algorithm generates high accuracy classi\223cation rules The rules are tested on the validation set for accuracy The results are very satisfactory they demonstrate the applicability and effectiveness of our approach Keywords pre-processing data ing classi\223cation association genetic algorithms clustering data-splitting 1 Introduction Data mining is generally de\223ned as the process of extracting previously unknown knowledge from a given database Classi\223cation rules mining and association rules mining are two important data mining techniques The former aims to discover a set of rules in the database to form an accurate classi\223er and the latter extracts in the database all rules that satisfy some minimum support and minimum con\223dence constraints 8 The main tar get o f the as s o ciation rules mining process is to obtain the best set of rules We focus on a special subset of association rules whose consequent right-hand-side is restricted to the classi\223cation class attribute This subset is often referred to as class association rules CARs Associative classi\223cation has high classi\223cation accuracy and strong 224exibility at handling unstructured data Al s o  e xponent i a l t i m e c ompl e x i t y of constructing an optimal classi\223er is a strong factor to resort to heuristics based solutions to save time Classi\223cation algorithms like Neural Network SOM etc have proven to be very accurate in classifying datasets 13 14  The i nherent probl ems wi t h t h es e c l a s si\223ers is that the rules are implicit  and therefore hard to decipher It is easier to derive classi\223cation rules using rule mining because the rules are explicit  In our research our aim is to bene\223t from the advantages of both approaches We also want to exploit the power of good classi\223cation algorithms in deriving good CARs using rule mining To produce accurate classi\223cation rules using the ARM technique in the mining stage of the DM process selecting an appropriate training set in the pre-processing stage is an important factor The traditional methods for data-splitting split the dataset into a training set and a test set to represent the unknown sample population and the unknown validation sets of the real-world T hese methods try to simulate the sample population using different sample training and test sets based on different statistical techniques The derived training s is used as e input to the ARM technique to derive the classi\223cation rules and the test set\(s is used to test the accuracy of the rules before validating the rules using a validation set The union of the derived classi\223cation rules using different splits with good classi\223cation accuracy forms the resulting rul e set There are two 224aws in using this approach 1 In any DM process the role of the test set is to detect over-training from the trained model Unfortunately the traditional data-splitting methods do not ensure that the trained classi\223cation model using the RM technique built from their training sets 223ts their test sets with least noise 2 It is still unclear how well these rules will perform on an 215unknown\216 validation set 1 Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


As opposed to the traditional methods where the training set is used to simulate the population in this paper the entire dataset should represent the population as closely as possible Different samples can be simulated to retain the generality of the resulting DM model by splitting the dataset into 1 sample set and 2 validation set Then each obtained sample set should be divided into training and st sets ef\223ciently so that i the training set represents the true relationships in the sample set as closely as possible without over-\223tting and ii the test set detects overtraining from the trained model as closely as possible As a result the problem of deriving the best training and test sets from a iven sample set is addressed in this paper The method proposed is based on searching for the split which maximizes the classi\223cation accuracy of the trained model when tested with the splits and at the same time minimizes the noise percentage between the splits The search process is carried out by GA which is well known as effective optimization technique We 223rst split the original dataset into sample set and validation set using statistical strategies 2 3 4  W e t h e n a p ply our GA-based method to the sample set to appropriately split it into training and test sets Then the training set is used as the input to an association rules mining model to derive CARs The accuracy of the CARs tested on the unknown validation set is satisfactory We tested and validated our approach on breast cancer dataset The reported results are promising in demonstrating the applicability and effectiveness of the proposed method The rest of the paper is organized as follows Section 2 describes the traditional methods for the data-splitting process Section 3 discusses how GA can be used for this problem In Section 4 we give the experimental results and compare the results of random sampling with the results from e GA method Section 5 is summary and conclusions 2 Related Work In this section we discuss three currently available methods to address the data-splitting process namely sampling cross validation and bootstrap Simple random sampling refers to e method of selecting members or instances from a dataset population such that every possible instance from the sample has an equal chance of being selected to represent the sample population For classi\223cation models simple random sampling does not guarantee that every class from the dataset is properly represented in both training and test sets 2 Strati\223ed random sampling 2 refers t o a s ampl i n g m et hod i n whi c h the dataset is divided to subgroups called stratas so that each instance in the dataset belongs to only one strata The objective is to form a strata so that the instances of interest from the dataset in each strata are similar For example each strata could contain the instances representing the same class A general way to mitigate any bias arising from the particular sample chosen for training or testing is to repeat thewholeprocess i.e  train and test several iterations with different random samples using any of the sampling methods 16 The o v e ral l error rat e i n t h e performance of t h e DM model is the average of the error rates on different iterations An alternative data-splitting method that draws the training and test sets from the original dataset is the cross validation method 2 3 Th e a d v an tag e o f t h i s m eth o d is th at th e greatest possible amount of data is used for training in each iteration which increases the chance of generating an accurate DM model The disadvantage is that it is impossible to guarantee that each class is properly represented in the test set so it is unsuitable for building classi\223cation models In all these methods samples were drawn without replacement i.e  an instance once selected could not be selected again Sampling with replacement is a statistical method which can use the same instance twice Bootstrap is an estimation method that uses sampling with replacement to form the training set Given a dataset D containing K instances generate the training set D 002 by randomly selecting K 002 instances from D with replacement The instances not in the training set forms the test set 1 2 3  Th e 0  632 bootstrap is e most popular estimation technique The whole bootstrap method is repeated several times with different replacement samples for better results Current methods for the data-splitting task are general statistical methods for data-splitting These methods are applicable to a wide class of problems However these methods are not robust and are based on random processes Hence the splits derived from these methods may not be the best splits for a special class of problems such as the ones considered in this paper In fact the choice of TRS is an important factor in deriving good CARs using the ARM technique Therefore these methods must be modi\223ed to obtain a split of the dataset in a manner which guarantees that the splits cover the classi\223cation relationships without over-\223tting or developing a more suitable approach that can effectively handle the splitting process 3 Data Splitting Using Genetic Algorithms GA\220s were oduced by Holland in 1962 5 ori gi nal l y intended as a general model of adaptive processes but subsequently widely opted as optimizers 6  B asically  G A can be used for solving problems for which it is possible to construct an objective function also known as 223tness function to estimate how a given representative solution 223ts the considered environment problem Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


In general the main motivation for using GA\220s in any DM process is that they perform a global search and cope better with interaction than e greedy rule induction algorithms often used in DM In data pre-processing GAs have been used for handling one of the key problems namely the attribute selection problem 7 Using the attribute selection problem as the basic idea we have explored the possibility of using GA for the instances selection problem in the area of data pre-processing We discuss how GA can be used for one of the key problems in data preparation namely to divide a data set into training and test sets Here it is also important to emphasize that to the best of our knowledge this is the 223rst time a problem of this nature is being explored using GA The goal is to 223nd the best split of the sample set into training and test set such that 1 the CARM model i.e classi\223cation model built from the ARM technique using the sample set 223ts the training and test sets with least noise 2 Classi\223cation accuracy of the derived CARs using such a aining set on an unknown validation set is high A binary encoded GA is used to search for the best training and test sets that can be obtained Hence each chromosome in the population is a possible split of the dataset The length of the chromosome is e number of stances in the data set For example if there are 10 instances in the dataset then 2151001100111\216 is a possible chromosome where the genes 2151\220s\216 and 2150\220s\216 represent instances in the training and test sets respectively The goal is to split the sample set so that the classi\223cation relationships learned from the sample set 223ts the training and test sets with minimum noise Therefore the 223rst task is to 223nd the classi\223cation algorithm that performs the best on the entire sample set So prior to running the GA the classi\223cation algorithms namely Support Vector Method Decision Trees Neural Network Naive Bayes Nearest Neighbor Method and Kernel Density are run on the existing sample set The algorithm that gives the highest accuracy on the correctly classi\223ed instances veri\223ed against the sample set is chosen Although the neural network is the best classi\223er in most cases the time exity of a neural network is very igh So using this classi\223er for the data-split problem would increase the time complexity of the entire task signi\223cantly Hence this classi\223er is eliminated from future discussions of the data-splitting problem Table 1 Confusion Matrix for the Classi\223cation Model Actual Class Predicted Class a b a T a F a b F b T b The predictive accuracy of the classi\223cation model is obtained by computing the con\223dence factor CF of the model This con\223dence factor denoted CFSample is computed with a confusion matrix 16 as illu strated in T a b l e 1  The con\223dence factor of the training model has a direct impact on how well the relationships are learned from TRS The higher this factor the better is the learning The most accurate classi\223cation algorithm that maximizes CFSample is chosen Therefore the aim is to derive a TRS which represents the complete classi\223cation relationships in the sample set with a con\223dence factor of at least e without over-\223tting the test set as well Let a and b be two classes in the dataset The labels in each quadrant of the matrix have the following semantics 200 T a represents true positives of a  i.e the number of examples in the tested set satisfying e model and a  200 F a represents false positives of a  i.e the number of examples satisfying the model but not a  200 F b represents false positives of b  i.e the number of examples satisfying the model but not b  200 T b represents true positives of b  i.e the number of examples satisfying the model and b  CF is de\223ned in terms of the above notation as CF  T a  T b N 327 100 where N denotes the total number of instances in the dataset The 223tness function uses the classi\223cation model learned from the sample set to test whether the model 223ts the training and test sets evenly That is the classi\223cation model s tested on the training set to determine how well the relationships are learned in the training set Thus the con\223dence factor of the trained model is determined it is denoted CFTraining If CFTraining is greater than or equal to CFSample the performance on the test set is observed That is the classi\223cation model is tested on the test set to determine how well the relationships are learned in the test set Thus the con\223dence factor denoted CFTest of the test model is determined If CFTest is greater than or equal to CFSample then ErrorFit is calculated That is the difference of ErrorTraining de\223ned as the number of incorrectly predicted instances by the model on the training set and ErrorTest de\223ned as the number of incorrectly predicted instances by the model on the test set is found If ErrorFit is less than speci\223ed threshold 223xed as 5 chosen arbritrarily then the 223tness value of the chromosome is CFTraining Otherwise the chromosome is given a default 223tness value The intuitive idea to choose this default value is 1 the value has to be greater than zero to continue the GA runs in subsequent generations and 2 since the 223tness criteria is not satis\223ed by the respective chromosome an arbitrary value is chosen to punish the less 223t chromosome This value is intuitively chosen as a positive integer less than CFTraining There are multiple ways of choosing this value For example one way is to choose this value as CF T raining 2  Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


An alternative could be any value that is less than CFSample since CFSample is always less than CFTraining in this case The 223tness function is de\223ned as If CF T raining and CFTest 002 CFSample and ErrorF it 003 0  05 then Fitness unction  CF T raining F itnessF unction  CF T raining 2 Otherwise 1 There is a possibility that the GA will attempt to capture as many 1\220s as possible in a chromosome to maximize the global 223tness value in subsequent generations This could result in a solution where the size of the test set is signi\223cantly smaller than the size of the training set This could also affect the convergence of the GA To avoid this problem a size factor is added to e 223tness function When the size of the test set falls below 30 the 223tness value of the represented chromosome is decremented by 30 The value 30 is chosen because the most common split in the literature is performed with 70 of the instances in the training set and 30 instances in the test set All these aspects are precisely encoded and implemented into the GA and all the chromosomes potential solutions should be awarded or punished according to the criteria stated above during the process of evolution The outcome of several evolutions modeled by this GA generates the right split of the dataset into training and test sets The approach has been implemented as a standard GA written in C similar to Gref enstette\220s GENESIS program Baker\220s SUS selection algorithm 17 i s e mpl o yed 2-poi nt crossover is maintained at 60 and mutation is very low and selection is based on proportional 223tness The GA starts with a random population of 100 chromosomes that represent a random splitting of the dataset into training and test sets In the 223tness function a UNIX shell script is used to call the classi\223cation model using the sample set from the WEKA data ng tool 16 to test o n th e t r a in in g a n d test sets for over-\223tting 4 Results and Discussions The original dataset is divided into a sample set and a validation set using different statistical strategies 2 3 4  The GASPER algorithm is used to split the sample set into training and test sets The results are tested on the validation set As an alternative the traditional method namely the random g is also used to obtain the training and test sets from the sample set As compared to the other two traditional methods it is suitable for both small and large datasets In fact bootstrap is a form of random sampling with replacement and cross validation is a very rudimentary form of random sampling We tested our approach on 3 data sets All the tests have been conducted using a single Processor Intel\(R Xeon\(TM UNIX machine with CPU power of 2.80GHz and cache size of 512 KB The proposed approach has been tested using Breast Cancer Data Set a real data set obtained from Tom Baker Cancer Center Calgary Alberta Canada The original dataset consists of 221 records and 16 attributes Each record represents follow-up data for one breast cancer case Breast cancer 215recurred\216 in some of these patients after the initial occurrence Hence each patient is classi\223ed as 215recurrent\216 or 215non-recurrent\216 depending on his or her status With respect to classi\223cation of the dataset there are 2 classes 1 Recurrent Patients and 2 Non Recurrent Patients The original dataset is divided into a sample dataset and a validation set using the random sampling technique The sample dataset has 121 instances whereas the validation set has 100 instances The proposed GA based approach is applied to split the sample dataset into training and test sets The 223tness function uses SVM classi\223cation algorithm to train the classi\223cation model using TRS The thresholds in the 223tness function are CFSample  85 and Def aultV alue  70  Finally the GA parameters are PopulationSize  100  2 P ointCrossOverF raction 0  6  ReproductionF raction 0  1  M utationF raction  0  1 andthe SelectionM ethod is P roportional  Figure 1 Trend of the Best Fitness Values of the GA Method The Average number of generations it took to 223nd the best solution Standard n  002  and Best 223tness mean  265  of the solutions obtained in 30 experiments for the GA method are 78  0  4024 and 93  05  respectively Figure 1 shows the plot of the best 223tness values obtained at the end Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


of every experiment for 30 GA experiments The best solution results in 75 instances in the training set and 46 instances in the test set The training set is used as input to the association rules mining algorithm in the WEKA datamining tool 16 t o obt ai n t he pos s i bl e c l a s s i 223 cat i o n r ul es  The obtained rules have been checked and validated by experts at Tom Baker Cancer Center This increased the con\223dence in the accuracy and effectiveness of the approach Some of these rules are given next 1 t=1 002 sx1=mast  003 recurrence  1 2 cancer1=IDC 002 t=1 002 sx1=mast  003 recurrence  1 3 sx1=mast 002 rad=R  003 recurrence=-1 4 method=lump 002 rad=R 002 sx1=seg  003 recurrence=1 5 n=0 002 sx1=mast  003 recurrence=-1 6 method=lump 002 n=0 002 sx1=mast  003 recurrence=-1 Table 2 Performance Evaluation of CARs on the Validation Set for Dataset1 Correctly P edicted  in  75 Incorrectly P redicted  in  10 Unknown  in  15 Table 3 Results of GA and Random Sampling for Dataset 1 GA Random Sampling Con\223dence Factor of Training Model 94  4 92  4 Noise\(With the Test Model 0  01 0  05 Number Training Instances 75 61 Number Test Instances 46 60 The above classi\223cation rules can be interpreted as follows For example it can be inferred from rule 4 that if the method of diagnosis is lump that is the patient found this herself the treatment is segmentectomy  meaning that she had removal of the tumor only and not the adjacent breast tissue and the treatment of radiotherapy is consistent o he remaining breast tissue in an attempt to kill off any remaining cancer cells then the cancer is likely to recur  The performance evaluation of the derived CARs using TRS from the GA method from e rule g lgorithm on the validation set is shown in Table 2 The results of the GA method and random sampling are listed in Table 3 It is important to observe that the con\223dence factor of TRS obtained using the GA method is higher than e con\223dence factor of TRS derived using random sampling Further the training model from the GA method 223ts the test set with 1 noise whereas the raining model from he sampling method 223ts the test set a e GA Method b Random Sampling Method Figure 2 Variation of Fitness Value in a Single Run with 5 noise As a result it is necessary to emphasize that the GA based method is an intelligent and ef\223cient search technique as compared to a random search technique such as sampling for the pre-processing problem Learning in the GA search technique is the best because the solution increases monotonically from a default value to the best lution in a 223nite number of generations In random sampling as the name signi\223es the search process is andom so the solution starts with a default value and 224uctuates at different splits displaying an erratic behavior Running the classi\223cation algorithm is an off-line process So once TRS is derived using the GA method the invaluable trade off is the high accuracy of the CARs derived using the ARM technique with Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


this TRS as input In applications such as medical diagnosis this trade-off in the form of accurate results CARs from this method would prove to be invaluable 5 Conclusions In this paper we proposed a novel approach that employs GA for splitting a data set into training and test sets We focused speci\223cally on the appropriate split for deriving the best classi\223cation rule set from the output of an association rule-mining model The data-splitting problem for the ARM technique presented in this paper identi\223ed the following tasks the entire dataset should represent the population as closely as possible different samples are simulated by using the traditional methods to split the dataset into a sample set and b validation set and each such sample set should be divided into training and test sets ef\223ciently so that a the training set represents the true classi\223cation relationships in the sample dataset as much as possible without over\223tting and b the test set detects overtraining of the trained classi\223cation model as much as possible The advantages are 1 he primary problem of simulating the sample and the validation sets of the real-world is addressed and 2 one approach is identi\223ed to de\223ne the right datasplit for ef\223cient ARM namely to distribute the classi\223cation relationships between the splits evenly Usingexperimental validations it is v eri\223ed in this paper that the performance of the CARM model g TRS derived from such a split performs with good accuracy on the validation set References  D P y le 215 Data P reparation f or Data Mining\216 Mor gan Kaufmann 1999 2 R R Picard  an d K N Berk KN 215Data sp littin g 216  American Statistician 44 pp 140-147 1990  G Gong 215C ros s v a l i d at i on t h e j ackkni fe and t he bootstrap excess error estimation in forward regression logistic regression\216 J Am Statistical Association 81 393 pp 3648 1986  B  Efron and R  T i b s h i rani  A n I nt roduct i o n t o Bootstrap Chapman  Hall 1993  J  H Hol l a nd 215 A dapt at i o n i n N at ural and Art i 223 cial Systems Applications to Biology Control and Arti\223cial Intelligence\216 University of gan Press 1975  D.E Gol dber g 215Genet i c Al gori t h ms i n S earch Optimization and Machine Learning\216 Addison Wesley Longman Publishing Co Inc Boston MA 1989  A.A F rei t a s  215 A S u rv e y of Ev ol ut i onary Al gorithms for Data Mining and Knowledge Discovery\216 Advances in Evolutionary Computation Springer-Verlag 2001 8 B L i u W H s u a n d Y i m i n g M a 215 I n t e g r a t i n g C l a s si\223cation and Association Rule Mining\216 Proc of KDD pp 80-86 1998  R  J  B ayardo 215B rut e-F orce Mi ni ng of Hi ghCon\223dence Classi\223cation Rules\216 Proc of KDD pp 123-126 1997  J  Han 215C MAR  Accurate and Ef 223cient C las s i 223cation Based on Multiple Class-Association Rules\216 Proc of IEEE-ICDM pp 369-376 2001  R  Agra w a l  and R  S r i kant  215 F a s t al gori t h ms for mining association rules,\216 Proc of VLDB Santiago Chile September 1994  J  R  Qui n l a n 215C 4.5 program for machi ne l earning,\216 rgan Kaufmann 1992  J  B r omle y  and E S ackinger  215Neural netw ork and k-nearest neighbor classi\223ers,\216 Technical Report 11359-910819-16TM AT&T 1991  C  C o rt es  a nd V  V a pni k 215S upport V ect or Net works\216 Machine Learning vol 20 pp 273-297 1995  C  L B l ak e and C  J  Merz 215UC I R epos itory of machine learning databases\216 University of California Dept of Inf  Comp Sci Irvine CA 1998 16 I.H W itten an d E  F ran k  215Data Min i n g  Practical Machine Learning Tools and Techniques with Java Implementations\216 rgan Kaufmann October 1999  J  E B a k e r  215R educing B i as  Inef 223 c ienc y i n the Selection Algorithm,\216 Proc of the International Conference on Genetic Algorithms  p 1421 1987 Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


rule mining can be used to overcome these disadvantages  7. Conclusions  This paper, discusses the application of data mining techniques within Web documents to discover what users want. It introduces the concept of decision patterns in order to interpret decision rules in terms of association mining. It has proved that any decision pattern is a closed pattern. It also presents a new concept of rough association rules to improve of the quality of text mining. To compare with the traditional association mining, the rough association rules include more specific information and can be updated dynamically to produce more effective results The distinct advantage of rough association rules is that they can take away some uncertainties from discovered knowledge through updating supports and weight distributions of association rules. It also demonstrates that the proposed approach gains a better performance on both precision and recall, and it is a considerable alternative of association rule mining This research is significant since it takes one more step further to the development of data mining techniques for Web mining  References   M. L. Antonie and O R. Zaiane, Text document categorization by term association, 2 nd  IEEE International Conference on Data Mining Japan, 2002, 19-26  G Cha n g, M  J  Healey J   A. M  M c Hugh a n d J   T L Wang Mining the World Wide Web: An information search approach Kluwer Academic Publishers, 2001   M   E i rinaki and M. Vazirgiannis  W e b  mining for web personalization ACM Transactions on Internet Technology  2003 3\(1 1-27 4  D A  Evans et a l   CL ARIT  e x pe ri ment s i n  bat c h filtering: term selection and th reshold optimization in IR and SVM Filters TREC02 2002  U Fay y a d G. P i atetsky Shapiro, P  S m y t h a n d R Uthrusamy, eds Advances in knowledge discovery and data mining Menlo Park, California: AAAI Press/ The MIT Press, 1996  R. F e ldman and H. Hirsh Mining as sociation s in text in presence of background knowledge, 2 nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1996, 343-346  J  W  Guan D. A Bell D Y L i u T h e rough s et approach to association rules, 3 rd IEEE International Conference on Data Mining USA, 2003, 529-532 8 J  D Hol t a n d S  M. Chung, M u ltipas s algor ithms  for mining association rules in text databases Knowledge and Information Systems 2001 3 168-183  L i and B. Liu, Learning to classify texts using positive and unlabeled data IJCAI 2003, 587-592  i and N Zhong, W e b mining model and its applications on information gathering Knowledge-Based Systems 2004 17 207-217  N Zhong Ca pturing evolving patterns for ontology-based IEEE/WIC/ACM International Conference on Web Intelligence Beijing, China, 2004, 256-263  Z hong Interp retations of association rules by granular computing, 3 rd  IEEE International Conference on Data Mining USA, 2003, 593-596  i ning ontology fo r automatically  acquiring Web user information needs IEEE Transactions on Knowledge and Data Engineering 2006 18\(4 554-568   Li u Y  Da i X L i  W S. Lee, and P. S. Yu, Building text classifiers using positive and unlabeled examples, 3 rd  IEEE International Conference on Data Mining USA, 2003 179-186  J. Mo staf a  W   Lam  and M. Palakal A m u ltil e v el approach to intelligent information filtering: model, system and evaluation ACM Transactions on Information Systems  1997 15\(4 368-399   Pawlak In purs uit of patterns in data re as oning from data, the rough set way 3 rd International Conference on Rough Sets and Current Trends in Computing USA, 2002 1-9  Sebas tiani M a chin e  learning in autom a te d text categorization ACM Computing Surveys 2002 34\(1 1-47  z vetkov X Yan a nd J. Han, TSP: Mining top-K closed sequential patterns 3 rd IEEE International Conference on Data Mining USA, 2003, 347-354  S.T W u  Y  Li Y X u  B  P h am and P Ch en Automatic pattern taxonomy exatraction for Web mining IEEE/WIC/ACM International Conference on Web Intelligence Beijing, China, 2004, 242-248  H  Y u  J H an, and K  C h ang, P E B L  pos itive exam pl e  based learning for Web page classification using SVM KDD02 2002, 239-248  n Y. Fu M i ning M u ltiple-level Association Rules in Large Databases IEEE Transactions on Knowledge and Data Engineering 1999 11\(5 798-805  G I. Webb and S  Z hang, K-op timal rule discovery  Data Mining and Knowledge Discovery 2004 10 39-79  Y Yao Y Zhao a nd R.B. Maguire, Explanation oriented association mining using rough set theory 9 th  International Conference on Rough Sets, Fuzzy Sets, Data Mining and Granular Computing China, 2003, 165-172 


33 3 xyz3 Itemset SUPPORT xyz3Closed Itemset Equivalence Class abcd abce abcde abde acde abd abe ade 1 11 12 2 2 2 cb  a bdbc de bcd cde d e4 4 4 4 4 4 4 3 3 3 3 6 6 6 acd ace2 2 2 2 Cfreq CM a b Figure 3. Equivalence classes of frequency when intersected by a CAM \(a b  constraint Example 1 In Figure 3\(a itemsets lattice with the frequency equivalence classes and the border of the theory of CAM ? sum\(X.price have that I1 = {?bc, 6?} while on the other hand I2 = {?ac, 3?, ?bc, 6?, ?bd, 4?, ?cd, 4?, ?ce, 4? ?cde, 3 What has happened is that some equivalence classes have been cut by the CAM constraint. With interpretation I1 we mine closed frequent itemsets and then we remove those ones which do not satisfy the CAM constraint: this way we lose the whole information contained in those equivalence classes cut by the CAM constraint. On the other hand, according to interpretation I2, we mine the set of itemsets which satisfy the CAM constraint and then we compute the closure of such itemsets collection: thus, by de?nition, the itemsets bd and cd are solutions because they satisfy CAM and they have not a superset in the result set with the same support and satisfying the constraint Which one of the two di?erent interpretations is the most reasonable? It is straightforward to see that interpretation I1 is not a condensed representation since it loses a lot of information. In extreme cases it could output an empty solutions set even if there are many frequent itemsets which satisfy the given set of user-de?ned constraints. On the other hand, interpretation I2, which corresponds to the de?nition Cl\(FThD\(Cfreq[D,?] ? CAM representation of FThD\(Cfreq[D,?] ? CAM Observe that I2 is a superset of I1: it contains all 


Observe that I2 is a superset of I1: it contains all closed itemsets which are under the CAM border \(as I1 plus those itemsets which arise in equivalence classes which are cut by the CAM border \(such as for instance ce and cde in Figure 3\(a Proposition 3 Cl\(FThD\(Cfreq ? CAM FThD\(Cfreq CAM Let us move to the dual problem. In Figure 3\(b show the usual equivalence classes and how they are cut by CM ? sum\(X.prices are upward closed, we have no problems with classes which are cut: the maximal element of the equivalence class will be in the alive part of the class. In other words when we have a CM constraint, the two interpretations I1 and I2 correspond Proposition 4 Cl\(FThD\(Cfreq ? CM FThD\(Cfreq CM The unique problem that we have with this condensed representation, is that, when reconstructing FThD\(Cfreq[D,?] ? CM care of testing itemsets which are subsets of elements in Cl\(FThD\(Cfreq ? CM not to produce itemsets which are below the monotone border B+\(Th\(CM not need to access the transaction dataset D anymore Since we mine maximal itemsets of the equivalence classes it is impossible to avoid this problem, unless we store, together with our condensed representation the border B+\(Th\(CM closed itemset. This could be an alternative. However since closed itemsets provide a much more meaningful set of association rules, we consider a good tradeo? among performance, conciseness and meaningfulness the use of Cl\(FThD\(Cfreq?CM resentation Finally, if we use free sets instead of closed, we only shift the problem leading to a symmetric situation. Using free sets interpretations I1 and I2 coincide when dealing with anti-monotone constraints because minimal elements are not cut o? by the constraint \(e.g. de in Fig. 3\(a constraints \(e.g. no free solution itemsets in Fig. 3\(b Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 4. Algorithms In this Section we study algorithms for the computation of MP5. We ?rst discuss separately how monotone and anti-monotone constraints can be pushed in the computation, then we show how they can be exploited together by introducing the CCIMiner algorithm 4.1. Pushing Monotone Constraints Pushing CAM constraints deep into the frequent itemset mining algorithm \(attacking the problem FThD\(Cfreq[D,?] ? CAM  since they behave exactly as Cfreq . The case is di?erent for CM constraints, since they behave exactly the opposite of frequency. Indeed, CAM constraints can be used to e?ectively prune the search space to a small downward closed collection, while the upward closed collection of the search space satisfying the CM constraints cannot be exploited at the same time. This tradeo? holding on the search space of the computational problem FThD\(Cfreq[D,?] ? CM extensively studied [18, 9, 4], but all these studies have failed to ?nd the real synergy of these two opposite types of constraints, until the recent proposal of ExAnte [6]. In that work it has been shown that a real synergy of the two opposites exists and can be exploited by reasoning on both the itemset search space and the transactions input database 


set search space and the transactions input database together According to this approach each transaction can be analyzed to understand whether it can support any solution itemset, and if it is not the case, it can be pruned In this way we prune the dataset, and we get the fruitful side e?ect to lower the support of many useless itemsets, that in this way will be pruned because of the frequency constraint, strongly reducing the search space. Such approach is performed with two successive reductions  reduction \(based on monotonicity reduction \(based on anti-monotonicity to  reduction we can delete transactions which do not satisfy CM , in fact no subset of such transactions satis?es CM and therefore such transactions cannot support any solution itemsets. After such reduction, a singleton item may happen to become infrequent in the pruned dataset, an thus it can be deleted by the ?reductions. Of course, these two step can be repeated until a ?xed point is reached, i.e. no more pruning is possible. This simple yet very e?ective idea has been generalized in an Apriori-like breadth-?rst computation in ExAMiner [5], and in a FP-growth [10] based depth-?rst computation in FP-Bonsai [7 Since in general depth-?rst approaches are much more e?cient when mining closed itemsets, and since FP-Bonsai has proven to be more e?cient than ExAMiner, we decide here to use a FP-growth based depth?rst strategy for the mining problem MP5. Thus we combine Closet [16], which is the FP-growth based algorithm for mining closed frequent itemset, with FPBonsai, which is the FP-growth based algorithm for mining frequent itemset with CM constraints 4.2. Pushing Anti-monotone Constraints Anti-monotone constraints CAM can be easily pushed in a Closet computation by using them in the exact same way as the frequency constraint, exploiting the downward closure property of antimonotone constraints. During the computation, as soon as a closed itemset X s.t  CAM \(X ered, we can prune X and all its supersets by halting the depth ?rst visit. But whenever, such closed itemset X s.t  CAM \(X e.g. bcd in Figure 3\(a some itemsets Y ? X belonging to the same equivalence class and satisfying the constraint may exist \(e.g bd and cd in Figure 3\(a ery such X in a separate list, named Edge, and after the mining we can reconstruct such itemsets Y by means of a simple top-down process, named Backward-Mining, described in Algorithm 1 Algorithm 1 Backward-Mining Input: Edge, C, CAM , CM C is the set of frequent closed itemsets CAM is the antimonotone constraint CM is a monotone constraint \(if present Output: MP5 1: MP5 = C split Edge by cardinality 2: k = 0 3: for all c ? Edge s.t. CM \(c 4: E|c| = E|c| ? {c 5: if \(k &lt; |c 6: k=c generate and test subsets 7: for \(i = k; i &gt; 1; i 8: for all c ? E|i| s.t. CM \(c 9: for all \(i? 1 10: if  Y ?MP5 | s ? Y 11: if CAM \(s 12: MP5 = MP5 ? s 13: else 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





