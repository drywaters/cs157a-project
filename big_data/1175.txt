Mining Association Rules from Data with Missing Values by Database Partitioning and Merging Takahiko Shintani Central Research Laboratory Hitachi Ltd 1-280 Higashi-Koigakubo Kokubunji-shi Tokyo 185-8601 Japan t-shin@rd.hitachi.co.jp Abstract Often real world applications contain many missing values n mining association rules from real datasets treating missing values is an important problem In this paper we propose a pattern-growth based algorithm for mining association rules from data with missing values No data imputations are performed Each association rule is evaluated using all the a records with which attributes of it are not missing values Our algorithm partitions the database so that the data record with which the same attributes contain missing values is assigned to the same database partition and the algorithm mines association rules by combining these database partitions We propose methods of reducing processing workload estimating the upper bound of global support using local supports reutilizing part of the constructed tree structure and merging redundant database partitions Our performance study shows that our algorithm is e 002 cient and can always 223nd all association rules 1 Introduction Mining association rules within a large database is a representative problem in data mining Several e 002 ective algorithms such as candidate generation approaches 1 a nd pattern growth approaches 3 4 ha v e been prop os ed Ho wever these algorithms have not dealt with missing values and they cannot 223nd correct association rules from data with missing values The missing values are inevitable in real applications 200 Medical data The ki nd of input data di 002 er with each patient since the suspected disease or condition details of consultation and medical therapy di 002 er with each patient Thus many missing values appear in medical data Genotype dat a also contains many missing values The locus cannot be analyzed for various reasons such as the condition of the specimen material neighboring sequence of analyzing locus position and the condition of analyzer 200 Basket data of retail businesses Basket data would seem not to contain any missing values A basket data record consists of a combination of bought items Different basket data records contain di 002 erent combinations of items because di 002 erent customers bought different items We cannot consider an item that is not contained in a basket data record as a missing value It is simply an item that was not purchased by a customer owever there are cases when missing values have to be taken into consideration in basket data The types of items being sold changes at s times The items that are not sold cannot be interpreted as not having been purchased These items cannot be judged as having been purchased or not When we mine association rules between items A and B we have to use basket data records for the period when both items A and B are sold Additionally the items being sold differ with each shop The basket data records of the shop where item A is not sold are equal to a dataset where the attribute value of attribute A is a missing value The correct association rules cannot be mined when we apply traditional association rul e mining algorithms by disregarding missing values Therefore missing values are removed in preprocessing The most common preprocessing technique for treating missing values is data imputation 223lling in the missing values such as mean multiple imputation and expectation maximization EM 2 W hiche v e r method is selected the imput ed data are biased Another method is record deletion A record with missing values is simply deleted from the data base altogether The drawbacks of this method are that info rmation included in the deleted records is lost and the sample size becomes smaller The problem of mining association rules from data with missing values has been reported 6 5 7 The the idea that to cut a database into several valid databases for each rule a valid database must not have any missing values was introduced and the robust association rule mining RAR algorithm for mining association rules from valid database was proposed 6 The R A R algorith m can mine corr ect as s o ciation rules However its performance is not good because it is based on the Apriori algorithm  F urt he rmore t h e RAR algorithm is not always able to 223nd all the association Proceedings of the 5th IEEE/ACIS International Conferenc e on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineeri ng, Software Architecture and Reuse \(ICIS-COMSAR\22206 0-7695-2613-6/06 $20.00 \251 2006  IEEE 


rules which is a drawback of this algorithm 037AR algorithm for generating approximate rules 037AR rules by imputing missing values was proposed 5 A a met ho d t o c ompu t e the upper and lower bounds of rules under all possible assignments f issing values was proposed 7  T hes e t w o methods mine approximate associ ation rules by estimation In this paper we propose a pattern-growth based algorithm for mining association rules from data with missing values called 215Association rules from data with Missing values by Database Partitioning and Merging AMDPM In the same way as 6  each as s o ciation rule is e v aluated using all the data records with which attributes of it are not missing values It consists of evaluating a rule only with known values that is ignoring missing values The AMDPM algorithm divides the database into rtitions where the records in which the same attributes contain missing values are assigned to the same database partition and it counts the local support of itemsets for each database partition The association rules are derived by combining some database partitions The AMDPM algorithm e 002 ectively reduces processing workload by 1 estimating the upper bound of global support using local supports 2 constructing and reutilizing part of a tree structure and 3 merging redundant database partitions The experimental results show the e 002 ectiveness of this algorithm The rest of this paper is organized as follows In the next section we explain the problem of mining association rules from data with missing values In section 3 e propose our algorithm Performance evaluations are given in section 4 Section 5 concludes the paper 2 Data and association rule with missing values 2.1 Problem de\223nition First we introduce some basic concepts of association rules from data with missing values Let A   A 1  A 2  A m  be a set of all attributes A set of attributes is called an attribute pattern A pair comprising an attribute A i and an attribute value a j is called an item and denoted as  A i  a j  Let I be a set of all items A set of items is called an itemset Let D   r 1  r 2  r n  be a set of all records where each record has an associated unique identi\223er and a set of attribute values When an attribute value is NULL we call it a missing  For example e 1 presents a database with 223ve attributes A,B,C,D,E of which the attribute values are a0,a1,a2 for A b0,b1,b2 for B c0,c1,c2 for C d0,d1,d2 for D and e0,e1 for E Missing values appear in all of the records except ID 1 7 14 17 and 20 For example the record for ID 2 has two missing values at attributes A and E A record r is disabled for itemset X if r contains missing values for at least one attribute value of an item in X  These records are called disabled records We denote Dis  X  the set of disabled records for X  D X the subset of D containing X and  D  the number of records in D  The valid records of an itemset X  which can ble 1 Database ID A B C D E 1 a0 b0 c1 d1 e0 2 b2 c1 d1 3 b0 c1 d0 4 b0 c1 d2 e0 5 b0 c1 d0 6 a2 d1 7 a2 b1 c2 d1 e1 8 a0 c0 d1 9 a0 b0 c0 d1 10 b1 c2 d1 e0 ID A B C D E 11 a2 d0 12 b1 c0 d0 13 a0 c0 d1 14 a1 b0 c0 d0 e0 15 c0 d2 16 b0 c1 d0 17 a1 b1 c0 d2 e1 18 b0 c1 d0 19 b1 c2 d1 20 a1 b1 c2 d1 e0 be utilized to evaluate association rules derived from X is VR  X   D\212 Dis  X  For example disabled records for itemset  B:b1,C:c1,D:d1  are records ID 6 8 11 13 and 15 and valid records for itemset  A:a0,B:b0,C:c0,D:d0  are records ID 1 7 9 14 17 and 20 An itemset X has support s if s  of records in VR  X  contain X  here we denote s 100  Supp  X  The number of records containing X is called the support count of X  An association rule is an implication of the form X 002 Y with X  Y 003I  X 004 Y  002  A rule is evaluated by support Supp con\223dence Conf and representativity Repr The support of rule X 002 Y is Supp  XY     D XY  D|\212 Dis  XY    The con\223dence of the rule X 002 Y means that c  of records in VR  XY  that contain X also contain Y  which can be written as the ratio  D XY   D X 212 Dis  Y  004 D X   To avoid ng association rules from a small VR that are over-speci\223ed rules another evaluation value called representativity is de\223ned the representativity of itemset X is calculated by  VR  X   D  The problem of mining association rules from data with missing value D is to 223nd all the rules that satisfy a userspeci\223ed minimum support MinSupp minimum con\223dence MinConf and minimum representativity MinRepr This problem can be decomposed into two sub-problems Step 1 Find all itemsets that have support above the userspeci\223ed MinSupp and MinRepr Itemsets that have support above the speci\223ed inimum support are called frequent itemsets The items contained in a frequent itemset are called frequent items Step 2 For each itemset found in Step 1 derive all rules that have more than the user-speci\223ed minimum con\223dence as follows for itemset X and Y  Y 004 X  002  if  D XY   D X 212 Dis  Y  004 D X   MinCon f  then the association rule X 002 Y is derived 2.2 ntional algorithms Here we explain the RAR algorithm 6 fo r 223 n d i n g all frequent itemsets from data with missing values The main idea of the RAR algorithm is that each itemset holds a set of ID lists of disabled records Since the RAR algorithm is based on the Apriori algorithm it 223nds frequent itemsets for each pass At the 223rst database scan the RAR algorithm generates an ID list of disabled records for each item At Proceedings of the 5th IEEE/ACIS International Conferenc e on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineeri ng, Software Architecture and Reuse \(ICIS-COMSAR\22206 0-7695-2613-6/06 $20.00 \251 2006  IEEE 


each pass it generates a set of candidate itemsets from the frequent itemsets found at the previous pass An item which is not frequent at pass k is not considered after pass  k  1 The ID list of disabled records of itemset X is the union of the ID list of disabled records for each item in X TheRAR algorithm then scans all the records to obtain the count supports of the candidate itemsets and determines the frequent itemsets The RAR algorithm reduces the number of generated candidate itemsets by using the itemset\220s down-closed property if an itemset is frequent all its sub-itemsets have to be frequent However this property is not always consistent for data with missing values For example assume that the number of records in the entire database is 15 the minimum support is 0.35 the support count of itemset X is 5 with 1 disabled record and the support count of itemset XY is 4 with 5 disabled records In this case the supports of X and XY are 0.29 and 0.40 respectively X does not satisfy the minimum support but XY does Since the RAR algorithm does not 223nd itemsets that contain X  it cannot 223nd XY  Thus it is not always possible for the RAR algorithm to 223nd all association rules Although the RAR algorithm is based on the Apriori algorithm the method of holding an ID list of disabled records for each itemset can be applied to the pattern growth approaches such as the FP-growth algorithm 3 and the A F O P T as cend ing frequ enc y ord ered pre\223x-tree algorithm 4  T he pattern gro wth appr oaches 223nd frequent itemsets wit hout candidate generation The basic idea of the pattern growth approach is to grow an itemset from its pre\223x It constructs a conditional database for each frequent itemset X  then the support counting for the itemsets that have X as the pre\223x is performed only on X 220s conditional database The FP-growth algorithm 223rst compresses the database into an FP-tree but retains the itemset association information at the same time It then divides the FP-tree into a set of conditional databases each of which is associated with one frequent item and it mines each such database separately The AFOPT algorithm uses the ascending frequency ordered pre\223x-tree to represent the conditional databases and the tree is traversed top-down It will 223rst mining on the 223rst item\220s conditional database After the mining of 223rst item is 223nished all of its subtrees are merged with its siblings called the push-right step and the second subtree of the root becomes the 223rst subtree of the root Then the mining process of the st item is contained The AFOPT algorithm is more e 002 ective than the FP-growth algorithm 4  H o w e v er  t h e re is a d ra wb ack in th at th ere i s no guarantee that all frequent itemsets will be mined These conventional pattern growth approaches can mine all association rules by using the minimum support count on the minimum number of records as the pruning itemset threshold Since the minimum number of records that satis\223es MinRepr is  D 327 MinRepr  the inimum number of minimum support count is  D 327 MinRepe 327 MinSupp  This value is the minimum number of support count of itemsets that have the potential to derive association rules that satisfy minimum thresholds Thus with the conventional pattern growth approaches it is possible to mine all rules by using  D 327 MinRepe 327 MinSupp  as a threshold of the itemset pruning instead of MinSupp Since it is the same as reducing the threshold of itemset pruning the number of processing itemsets increases Moreover excessive support counting occurs for itemsets that cannot derive association rules Thus there is the possible disadvantage of a higher processing cost However it is possible to mine all association rules 3 Proposed algorithm In this section we describe our algorithm AMDPM Association rule mining from data with Missing values by Database Partition and Merger for mining association rules from data with missing values The AMDPM algorithm adopts a pattern growth approach and handles partitioned databases It consists of two processes database partitioning and association rule mining 3.1 Database partitioning process This process generates database partitions The database is divided into partitions so that each partition consists of records that have the same missing values in the same attributions The association rule mining process utilizes these database partitions When an attribute pattern is Z wedenote DP  Z  as the database partition that consists of records containing no missing value of attributes in Z  Here the support and the support count in a database partition are called local support and local support count respectively An itemset whose local support is greater than MinSupp is called a local frequent itemset Furthermore the support and the support count in an entire database are called global support and global support count respectively An itemset whose global support is greater than MinSupp is called a global frequent itemset The procedure of this process is as follows Step 1 Assign records to each database partition A record is read from the database and is assigned to the associated database partition At the same time the occurrence of each item and the number of records containing no missing values for each attribute are counted Step 2 Detect exclude-attributes If an attribute A i does not satisfy MinRepr we do not need to 223nd itemsets containing items in A i  When the number of records containing no missing values of A i is less than  D 327 MinRepr  A i is inserted into the exclude-attribute list Step 3 Detect exclude-items If n item I j does not satisfy MinSupp and MinRepr together we do not need to 223nd itemsets containing I j  When the global support count of I j is less than  D 327 MinSupp 327 MinRepr  I j is inserted into the exclude-item list An attribute A i is inserted in the exclude-attribute list when all items of A i are in the exclude-item list Step 4 Merge redundant database partitions All attributes in the exclude-attribute list are deleted from Proceedings of the 5th IEEE/ACIS International Conferenc e on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineeri ng, Software Architecture and Reuse \(ICIS-COMSAR\22206 0-7695-2613-6/06 $20.00 \251 2006  IEEE 


each database partition The database partitions that consist of identical attribute patterns are merged into one database partition and the local support counts of each item in the merged database partitions are updated Step 5 Decide the processing order of each attribute The processing order of each attribute in the association rule mining process is determined To reduce the amount of data in the main memory attributes are arranged in ascending order of the number of database partitions they are contained in The database partitioning process generates database partitions and deletes unnecessary attributes from them Since Repr of an attribute is calculable from the number of records containing no missing valu es it is possible to detect the attributes that cannot satisfy MinRepr Step 2 Moreover as described in Section 2.2 the item whose support count is less than the minimum support count in the minimum number of records cannot be contained in association rules that satisfy all minimum thresholds Therefore it is not necessary to take into consideration items whose support count is less than  D|\327 MinSupp 327 MinRepr Step3 These attributes are deleted from database partitions since they are unnecessary in the association rule mining process When an unnecessary attribute is deleted from the database partitions database partitions with an equal attribute pattern may occur By merging such database partitions into one database partition we can reduce the number of partitions to be processed Step 4 A database partition-based algorithm called 215Partition algorithm\216 was proposed 8 P a rtitio n a lg o r ith m d i v id es the database into database partitions so that each database partition can 223t in the main memory and it 223nds local frequent itemsets for each database partition After all the partitions have been processed all the local frequent itemsets become candidate itemsets for the entire database This lgorithm is one of the methods for handling very large databases It does not take missing values into account and cannot be applied to data with missing values On the contrary the AMDPM algorithm focuses on mining association rules from data with missing values and the method of dividing the database is di 002 erent Additionally the AMDPM algorithm does not count all the local frequent itemsets as will be described in detail in section 3.2 By estimating the upper bound of global support the AMDPM algorithm reduces the amount of counting records for some itemsets Example  Here we show an example using the database in Table 1 with MinSupp 0.25 MinConf 0.70 and MinRepr 50 First each record is assigned to the associated database partitions For example the record of ID 2 is assigned to DP\(B,C,D since this record has missing values at attribute A and E After all records are processed seven database partitions DP\(A,B,C,D,E   1,7,14,17   DP\(A,B,C,D   9   DP\(B,C,D,E   4,10   DP\(A,C,D   8,13   DP\(A,D   6,11   DP\(B,C,D   2,3,5,12,16,18,19   and DP\(C,D   15  are generated Here the number of records containing no missing values for each attribute and the support count of each item are counted Next the exclude-attributes and items are detected In this example attribute E is an exclude-attribute since Repr of attribute E is 0.35 Since the number of records in the entire database is 20 MinSupp is 0.25 and MinRepr is 0.50 items whose global count is less than 3 become exclude-items In this example item B:b2 is inserted to exclude-item list Then exclude-attributes are deleted from database partitions and redundant database partitions are merged When attribute E s deleted from DP\(B,C,D,E all the records in DP\(B,C,D,E are added to DP\(B,C,D After deleting all the exclude attributes the database partitions consist of 223ve database partitions 3.2 n rule mining process Next we describe the association rule mining process The association rules are mined by using the database partitions This process mines association rules with items of an attribute by rotation To determine the number of support counts the AMDPM algorithm uses the pre\223x-tree structure similar to AFOPT and t ra v ers es i t us i n g a t op-do wn strategy The root node of the pre\223x-tree is empty\(NULL Each node in the pre\223x-tree contains three types of information the item the support count array of the itemset corresponding to the path from the root to the node for each database partition and the pointers pointing to the node\220s children and parent The nodes in the pre\223x-tree are ordered based on the processing order of attributes Here we de\223ne semi-minimum support SemiMinSupp whose value is less or equal to MinSupp and an item whose local support is greater than SemiMinSupp is a local semi-frequent item The AMDPM algorithm calculates the global support of an itemset from the local support count of database partitions The global support GS of an itemset X is calculated by the following equation GS  X   002 m i  1 LC  X  DP  Z i  002 m i  1 RC  DP  Z i  1 Here database partitions that contain all attributes in X are DP  Z 1   DP  Z m  LC  X  DP  Z i  is the local support count of X on DP  Z i  and RC  DP  Z i  is the number of records in DP  Z i  GS  X  is calculable by the local support count of X and the number of records of database partitions containing all attributes of X  However GS  X  cannot be calculated when the local support counts of X are unknown in some database partitions In this case the upper bound of global support UGS can be calculated by the following equation UGS  X   002 n i  1 LC  X  DP  Z i   002 m j  n  1 min  LC  X 005  DP  Z j   002 m i  1 RC  DP  Z i  2 Here database partitions that contain all attributes in X are DP  Z 1   DP  Z m  database partitions in which the local support count of X is known are DP  Z 1   DP  Z n  database partitions in which the local support count of X Proceedings of the 5th IEEE/ACIS International Conferenc e on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineeri ng, Software Architecture and Reuse \(ICIS-COMSAR\22206 0-7695-2613-6/06 $20.00 \251 2006  IEEE 


is unknown are DP  Z  n  1  DP  Z m  and any subitemset of X is X 220 Since LC  X  DP  Z j  is less than or equal to any LC  X 005  DP  Z j  UGS  X  calculated by equation 2 is less than or equal to GS  X  If all of min  LC  X 005  DP  Z j    0 UGS  X  is equal to GS  X  We show the procedure for mining association rules containing items of attribute A i  Step 1 Construct a pre\223x-tree The database partitions containing attribute A i are read to construct the pre\223xtree for counting the local support counts of itemsets with A i  By reading records from each database partition the pre\223x-tree grows Assume that record r is read from database partition DP  Z  The local frequent items and local semi-frequent items are extracted from r  and exclude-items are removed and sorted by the processing order of attributes The remaining items r 005 are used for building the pre\223x-tree The pre\223xtree generation for i-th item I i in r 005 is as follows the current node has a child node of item I i  the current node moves to it and increments the support count of database partition DP  Z  When the current node does not have a child node of I i  a new node for the current node\220s child node of item I i is created the current node moves to the created node the support count of database partition DP  Z  is set to 1 Additionally the AMDPM algorithm counts the local support count of combinations of two items each containing one item of attribute A i  This information is utilized when calculating the upper bound of global support in the process of 223nding global frequent itemsets Step 2 Find global itemsets For each item I p of attribute A i  the subtree of rooted at I p is traversed and the local supports of all items in it are counted When an item I q satis\223es MinRepr and is a local frequent itemset at at least one database partition I q 220s GS or UGS is calculated If GS is calculated and satis\223es MinSupp itemset  I p  I q  is inserted into the global frequent itemset list If UGS is calculated and satis\223es MinSupp  I p  I q  may become a global frequent itemset  I p  I q  and all database partitions in which the local support of  I p  I q  is unknown are inserted into the additional counting itemset list Then the subtree of rooted at I p is traversed again to build a new pre\223x-tree that contains only the items satisfying SemiMinSupp at at least one database partition The above procedure is repeated on a new pre\223x-tree Step 3 Count additional counting itemsets By reading the records from database partitions in the additional counting itemset list nodes corresponding to additional counting itemsets are created or their support count arrays are updated For each node that is created or updated the local support counts of additional counting itemsets are counted by traversing the path from the updated node to the root node Just as in Step 2 the global support of each additional counting itemsets is calculated and itemsets whose global support is greater or equal to MinSupp are inserted into the global frequent itemset list Step 4 Reconstruct the pre\223x-tree None of the attribute A i nodes are required following this process since all the itemsets with A i have already been mined The A i nodes are deleted from the pre\223x-tree All of these are child nodes of the root node The subtree of a child node of a deleted node is merged with siblings Step 5 Count sub-itemsets By traversing the subtree of all the root node children the local support counts of all items in it are counted Here only the itemsets that are sub-itemsets of the global frequent itemsets mined in Step 3 and 4 Step 6 Derive association rules For each global frequent itemset association rules are derived in the same way as when using an ordinary association rule mining algorithm Step 7 Reutilization of constructed pre\223x-tree and merge redundant database partitions There are three types of methods for this process a Reutilization of partial pre\223x-pattern tree AMDPM-R This method reuse the constructed pre\223x-tree In subsequent processes this pre\223x-tree is reused b Merger of database partitions AMDPM-M The pre\223x-tree is deleted since this method does not reutilize the constructed pre\223x-tree This method deletes attribute A i from database partitions and merges the database partitions with the same attribute pattern c Hybrid of the two methods above AMDPMH For database partitions with a small number of records attribute A i is deleted and it is merged with a database partition with the same attribute pattern For database partitions with a large number of records the constructed pre\223xtree of these database partitions is reused In the AMDPM-H algorithm we de\223ne a small database partition as a database partition whose local minimum support count is 1 It is expected that both the e 002 ects of partial pre\223x-tree reutilization and database partition merger can be attained The AMDPM algorithm selects suitable items for every database partition and builds the pre\223x tree using them The kinds of items that constitute the pre\223x tree di 002 er with each database partition By avoiding construction of unnecessary nodes we can expect to reduce the pre\223x-tree construction cost Moreover the information for calculating correct supports of all itemsets may not be contained in the pre\223x-tree built in the stage of Step 1 In that case UBS of the itemsets is calculated Since an itemset whose upper bound of support satis\223es MinSupp may become a global frequent itemset the additional support counting process is required when there is insu 003 cient information However when the Proceedings of the 5th IEEE/ACIS International Conferenc e on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineeri ng, Software Architecture and Reuse \(ICIS-COMSAR\22206 0-7695-2613-6/06 $20.00 \251 2006  IEEE 


Figure 1 Pre\223x-tree for attribute B Figure 2 Conditional database for item B:b0 Figure 3 Reconstructed pre\223x-tree Figure 4 Fre\223x-tree for attribute A AMDPM-R Figure 5 Reutilizing pre\223x-tree AMDPM-H upper bound does not satisfy the minimum support the support counting of the itemset is unnecessary In this case excessive count processing is avoidable and we can expect to reduce the support counting cost Example  We show an example of the association rule mining process using the database in Table 1 Here SemiMinSupp is set to 0.20 In this example we show the process for mining association rules containing items of attribute B First the pre\223x-tree is constructed by reading records from database partitions containing attribute B that is DP\(A,B,C,D and DP\(B,C,D The pre\223xtree is constructed without item A:a2 B:b2 C:c1 D:d0 D:d2 for DP\(A,B,C,D B:b2 C:c0 D:d2 for DP\(B,C,D Item B:b2 is an exclude-item and the other items do not satisfy SemiMinSupp on each database partition Additionally all itemsets which are combination of the items containing an item of attribute B are counted After all database partiti ons containing attribute B are processed the pre\223x-tree in Figure 1 is constructed Next the support counting is performed by constructing conditional database for each item of attribute B For mining association rules containing item B:b0 the pre\223x-tree is traversed By traversing the subtree rooted at B:b0 GS or UGS of all items in it are calculated For item D:d1 the local support count in DP\(A,B,C,D and DP\(B,C,D are 2 and 0 In this case GS is calculated as 0.13   2  0 6  9  and itemset  B:b0,D:d1  is pruned For item C:c1 the local support count in DP\(B,C,D is 5 but that in DP\(A,B,C,D is unknown In this case UGS is calculated as 0.40   1  5 6  9  This calculated value satis\223es MinSupp itemset B:b0,C:c1 may become global frequent itemset  B:b0,C:c1  at database partition DP\(A,B,C,D is inserted to a dditional counting itemset list Then conditional database is constructed By picking up items which satisfy SemiMinSupp at at least one database partition from the subtree rooted at B:b0 new pre\223x-tree is constructed In this case a pre\223x-tree shown in Figure 2 is constructed Then by traversing subtree rooted at itemset B:b0,C:c1 the support count of itemset  B:b0,C:c1,D:d0  is counted By this way all itemsets containing item of attribute B are checked Nex t additional counting process is performed In order to count the local support count of itemset  B:b0,C:c1,D:d0  in DP\(A,B,C,D node of item C:c1 and D:d0 is inserted and  or updated and they are linked By traversing the linked nodes and its antecedent nodes all itemsets whose GS is recalculated are checked Since all global frequent itemsets containing an item of attribute B have mined all nodes of attribute B are deleted from the pre\223x-tree From the pre\223x-tree the nodes of item B:b0 and B:b1 are deleted the subtree of child nodes of them are merged and it is decomposed into subtrees for database partition DP\(A,B,C,D and DP\(B,C,D the subtrees in Figure 3 is constructed Then the support counting for sub-itemsets is performed and association rules are derived From the global frequent itemset B:b1,C:c2,D:d1 association rule  B:b1,C:c2 002 D:d1  Conf 1.00 Supp 0.27 Repr 0.75 is derived At this point the mining association rule containing attribute B was completed The reutilization of the pre\223x-tree and  or merger of database partitions are performed In AMDPM-R the constructed pre\223x-tree is reutilized For example the association rule mining process of attribute A construct the pre\223x-tree from the subtree of DP\(A,B,C,D and records in DP\(A,C,D and DP\(A,D Figure 4 On the contrary merger of database partitions is performed in the AMDPM-M algorithm By deleting attribute B from all database partitions and merging redundant database partitions into one database partitions three database partitions DP\(A,C,D   1,7,8,9,13,14,17,20   DP\(A,D   6,11  DP\(C,D   2,3,4,5,10,12,15,16,18,19  are genProceedings of the 5th IEEE/ACIS International Conferenc e on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineeri ng, Software Architecture and Reuse \(ICIS-COMSAR\22206 0-7695-2613-6/06 $20.00 \251 2006  IEEE 


Connect-4 T10I4D100K  0 10 20 30 40 50 60 10 20 30 40 50 60 70 Execution time \(sec of missing values AMDPM-R \(Unif AMDPM-M \(Unif   AMDPM-H \(Unif   AMDPM-R \(Exp   AMDPM-M \(Exp   AMDPM-H \(Exp   0 10 20 30 40 50 10 20 30 40 50 60 70 Execution time \(sec of missing values AMDPM-R \(Unif AMDPM-M \(Unif   AMDPM-H \(Unif   AMDPM-R \(Exp   AMDPM-M \(Exp   AMDPM-H \(Exp   a With varying the ratio of missing values Connect-4 T10I4D100K 0 10 20 30 40 50 60 30 35 40 45 50 55 60 Execution time \(sec Minimum support value AMDPM-R \(Unif AMDPM-M \(Unif   AMDPM-H \(Unif   AMDPM-R \(Exp   AMDPM-M \(Exp   AMDPM-H \(Exp   0 10 20 30 40 50 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 Execution time \(sec Minimum support value AMDPM-R \(Unif AMDPM-M \(Unif   AMDPM-H \(Unif   AMDPM-R \(Exp   AMDPM-M \(Exp   AMDPM-H \(Exp   b With varying MinSupp 0 0.2 0.4 0.6 0.8 1 1.2 20 25 30 35 40 45 50 Execution time \(relative ratio Semi-minimum support value Connect-4 \(Unif Connect-4 \(Exp   0 0.2 0.4 0.6 0.8 1 1.2 20 25 30 35 40 45 50 Execution time \(relative ratio Semi-minimum support value T10I4D100K \(Unif T10I4D100K \(Exp   c AMDPM-H with varying SemiMinSupp Connect-4 T10I4D100K 0 10 20 30 40 50 60 0 2000 4000 6000 8000 10000 Execution time \(sec of database partitions Rule gen \(Unif Rule gen \(Exp   DB partition \(Unif   DB partition \(Exp   0 10 20 30 40 50 60 0 10000 20000 30000 40000 50000 Execution time \(sec of database partitions Rule gen \(Unif Rule gen \(Exp   DB partition \(Unif   DB partition \(Exp   d AMDPM-H with varying  of database partitions Figure 6 Execution time erated Since the AMDPM-M algorithm does not reutilize pre\223xtree all ed e\223x-tree are deleted In the AMDPM-H algorithm both reconstruction of pre\223x-tree and merger of database partitions are performed In this example the AMDPM-R algorithm is employed for DP\(B,C,D Figure 5 and the AMDPMM algorithm is employed for DP\(A,B,C,D four database partitions DP\(A,C,D   1,7,8,9,13,14,17,20   DP\(A,D   6,11   DP\(B,C,D   2,3,4,5,10,12,16,18,19  DP\(C,D   15  are generated 4 Performance evaluations We implemented our algorithm on a Linux machine 2.4GHz Xeon processor 4GB RAM and 36GB local disk drive and conducted performance evaluations Table 2 contains several datasets used in this performance study Connect-4 is a dense dataset obtained from the UCI Machine Learning Repository http  www.ics.uci.edu  037mlearn  MLRepository.html T10I4D100K is a synthetic dataset generated by the procedure in 1 an d i s v ery s p a rse T ab le 2 lists so me statistical in fo rmatio n about the datasets The last column is the average number of items in the records We append missing values to these datasets as follows 1 A set of missing attribute patterns is generated by selecting attributes at random The number of missing attribute patterns is equal to the number of database partitions 2 For each record a missing attribute pattern is selected and is applied to it The attribute values included in the selected missing attribute pattern are changed into missing values By changing the ratio of missing values for each attribute the number of missing attribute patterns and the distribution of selecting missing attribute patterns we generate several types of datasets Here we denote the dataset where the missing attribute patterns are applied by selected uniform distribution as 215\(Unif\\216 and that of exponential distribution as 215\(Exp\\216 In all experiments we set MinConf at 70 and Repr at half of the ratio of the records not containing missing values Figure 6 shows the execution time The ratio of missing values is set to 25 in b c d MinSupp is set to 50 for Connect-4 and 0.05 for T10I4D100K in a c d SemiMinSupp is set to 50 on Connect 4 and 0.05 for T10I4D100K in a b d and the number of database partitions is set to 5000 for Connect4 and 10000 for T10I4D100K in a b c The performance Table 2 Data sets Data sets  of records  of attributes  of items AveRItems Connect-4 67,557 43 129 43 T10I4D100k 100,000 1,000 1,000 10 of the AMDPM algorithm decreases as the ratio of missing values increases When the database contains several missing values the support of each itemset increases and the number of processing itemsets increases Furthermore the performance of AMDPM is a 002 ected by SemiMinSupp setting When SemiMinSupp is low the number of nodes in the pre\223x-tree and the cost of traversing increase but the number of additional counting itemsets decreases When SemiMinSupp is high the cost of traversing decreases t the number of additional counting itemsets increases Since the e 002 ect of the SemiMinSupp setting is data dominant it is di 003 cult to de\223ne an optimal value Empirically we can attain good performance when setting SemiMinSupp at about 3 5 to 4 5 of MinSupp In the AMDPM algorithm the AMDPM-H algorithm attains the best performance Since AMDPM-H uses the method of reutilizing a partial pre\223x-tree and merging database partitions it bene\223ts from the e 002 ects of both methods AMDPM reduces the processing cost by reutilizing the constructed pre\223x-tree and  or merging redundant database partitions e e 002 ects depend on the number of records in the database partition It is presumed that the pre\223xtree construction cost of a database partition with many records is large and that of a database partition with only a few records is small The kinds of items in\224uence the pre\223x-tree construction cost When there are few records in a database partition the number of items that satisfy SemiMinSupp increases For example assume that the database partition contains 223ve records with MinSupp 0.1 All the items appearing in it have to be used to construct the pre\223x-tree In this case the number of nodes of the pre\223xtree increases and the support count cost also increases To improve performance it is necessary not only to avoid constructing a pre\223x-tree for a database partition ith many records but also to reduce the kinds of items in the pre\223x-tree Additionally the number of database partitions a 002 ects the performance of AMDPM The number of treating database partitions varies with the processing order of each attribute The execution time of AMDPM-H with the processing order of each attribute in ascending order of the number of database partitions they each are contained in is 39.4 sec on T10I4D100K Unif with MinSupp 0.05 SemiMinSupp 0.04 Proceedings of the 5th IEEE/ACIS International Conferenc e on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineeri ng, Software Architecture and Reuse \(ICIS-COMSAR\22206 0-7695-2613-6/06 $20.00 \251 2006  IEEE 


Table 3 Performance comparison Execution time sec  Ratio of mined rules   of Missing values 5 25 50 5 25 50 MinRepr 47.5 37.5 25 47.5 37.5 25 Connect-4 Unif with MinSupp 50 Connect-4 Exp with MinSupp 50 AMDPM-H 45.8  100 43.0  100 38.6  100 42.1  100 38.2  100 35.2  100 RAR 72.5  99.1 68.4  96.6 59.1  91.2 72.1  98.2 67.9  94.6 57.5  88.1 RAR  152.9  100 176.3  100 238.6  100 152.1  100 176.7  100 237.1  100 Extd AFOPT 20.8  99.1 20.1  96.6 18.4  91.2 21.2  98.2 19.9  94.6 18.5  88.1 Extd AFOPT  33.3  100 37.0  100 47.2  100 35.1  100 38.9  100 49.0  100 T10I4D100K Unif MinSupp 0.025 T10I4D100K Exp MinSupp 025 AMDPM-H 43.0  100 39.5  100 34.1  100 34.9  100 33.6  100 29.4  100 RAR 58.8  98.6 58.4  95.0 54.0  86.7 58.8  98.2 58.5  93.1 54.1  85.8 RAR  109.1  100 113.7  100 116.2  100 109.1  100 113.8  100 117.8  100 Extd AFOPT 17.1  98.6 16.8  95.0 16.3  86.7 17.3  98.2 16.9  93.1 16.3  85.8 Extd AFOPT  27.6  100 33.7  100 40.5  100 27.6  100 33.8  100 40.3  100 And the execution time in descending order and in random order are 46.2 sec and 42.1 sec respectively AMDPM-H holds down the number of treating database partitions by the processing order for each attribute and by merging redundant database partitions Table 3 lists the execution time and the ratio of mined rules with varying ratios of missing values The number of database partitions is set to 5000 and 10000 for Connect-4 and T10I4D100K respectively With the AMDPM-H algorithm SemiMinSupp is set to 50 and 0.05 for Connect-4 and T10I4D100K respectively These results show that the AMDPM-H ithm can always 223nd all rules The ratio of mined rules decreases as the ratio of missing values increases and it depends on the ratio of missing values for each attribute This tendency is high with sparse datasets Here the results of RAR and AFOPT are shown for comparison As described in section 2.2 AFOPT is extended to hold a disabled record ID list Extended AFOPT where  D 327 MinRepe 327 MinSupp  is used for the threshold of itemset pruning are denoted by AFOPT   Also in conventional pattern growth approach it is possible to mine all association rules by setting the threshold of itemset pruning to  D|\327 MinRepe 327 MinSupp  However the number of kinds of processing items increases and the execution time is longer This tendency is high when the ratio of missing values is high and MinRepr is low The execution time of conventional algorithms increases exponentially as MinSupp decreases When the ratio of missing values is low we can set a high MinRepr In this case there is only a slight increase in the incremental processing cost when taking missing values into account However few associatio n rules are mined if MinRepr is set to high at a high ratio of missing values In order to 223nd interesting association rules we have to set to low the MinRepr for dataset with a high ratio of missing values Consequently the performance degrades rapidly When the ratio of missing values is very low and MinRepr is high the execution time of AMDPMH becomes longer than that of conventional pattern growth approaches There is little point in considering missing values when the ratio of missing values is very low However AMDPM-H outperforms other algorithms when the ratio of missing values is high and MinRepr is low and it is bene\223cial to consider missing values 5 Conclusion Missing values are a natural phenomenon in real datasets If we mine association rules in complete disregard of missing values mistaken rules are derived Thus the problem of treating missing values is important In this paper we proposed the AMDPM algorithm for mining association rules from data with missing values and examined its e 002 ectiveness through performance evaluations The AMDPM algorithm does not perform data imputations and it utilizes all records not containing missing values for each association rule Our approach is to divide the database into database partitions to count the support of itemsets for each database partition and to mine association rules by combining some of the database partitions Unnecessary record counting is avoided by estimating global support from the local support counts of local frequent itemsets and the processing cost is reduced by reutilizing a constructed partial pre\223x-tree structure and merging redundant database partitions Experimental results showed the AMDPM algorithm can always 223nd all association rules satisfying minimum thresholds and it can attain good performance by reutilizing trees and merging redundant database partitions In our future work we plan to extend our algorithm to distributed environments References  R  A gra w al and R S r i kant  F ast al gori t h ms for m i n i n g a ssociation rules In Proceedings of 20th VLDB International Conference  pages 487\205499 September 1994 2 A  D empst er  N  L ai r d  and D R ubi n Maxi mum l i k el i hood from incomplete data via the em algorithm In Journal of the Royal Statistical Society Series B  volume 39 1977 3 J  H an J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In Proceedings of 2000 ACM SIGMOD International Conference on Management of Data  pages 1\205 12 June 2000  G  L i u  H  L u Y  Xu a nd J Y u  A scendi ng frequenc y o rdered pre\223x-tree E 003 cient mining of frequent patterns In Proceedings of Eighth International Conference on Database Systems for Advanced Applications  pages 65\20572 March 2003  J  N ayak and D C ook Approxi mat e associ at i o n r ul e m i n i ng In Proceedings of the Florida Arti\223cial Intelligence Research Symposium  pages 259\205263 May 2001 6 A  R ag el an d B  C remilleu x  T r eatmen t o f missin g v alu e s f o r association rules In Proceedings of the Second Paci\223c-Asia Conference on Knowledge Discovery and Data Mining  pages 258\205270 April 1998 7 A  R ag el an d B  C remilleu x  M v c a p rep r o c essin g m eth o d to deal with missing values In Knowledge-Based Systems  pages 285\205291 1999  A S a v asere E  Omiecinski and S  Na v a the An e 003 cient algorithm for mining association rules in large databases In Proceedings of 21th VLDB International Conference  pages 432\205444 September 1995 Proceedings of the 5th IEEE/ACIS International Conferenc e on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineeri ng, Software Architecture and Reuse \(ICIS-COMSAR\22206 0-7695-2613-6/06 $20.00 \251 2006  IEEE 


 then we can additionally bound the space by  Furthermore since the frontier contains all uncompleted  and n n n  This is for the worst case when all itemsets are frequent Clearly a closer bound is if we let 1 2 2 1 1 2 1 2 2 1 itemvectors of space Furthermore 2    n n          b  212  b   The cardinality of both these sets equal to the number of nodes along the path is 942  001 001 001 001 be the largest itemset GLIMIT uses at most 001  007  100  n 1 1 1 1  t\212 n is even the last node is t   t t  b 3 1 t\212 l l l l step 006 n 2 n   n n n minSup 1 1  6 Experiments even or odd b 002 1 and up in the tree we are the more this contributes Secondly we need to keep itemvectors in memory until we complete their respective nodes That is check all their children Fact 6 or if they can\325t have children Fact 5 Now the further we are up in the tree or any subtree for that matter without completing the node the longer the sequence of incomplete nodes is and hence the the more itemvectors we need to keep Considering both these factors leads to the situation in Figure 3 320 that is we are up to the top item and the topmost path from that item so that no node along the path is completed If we have corresponding to the single items children of the root There are a further inclusive to the last coloured node these are the uncompleted nodes 16  Therefore the total space required is just is so that we do not double count the itemvector for  is low and in practice with non-pathological support thresholds we use far fewer than itemvectors If we know that the longest frequent itemset has size 16 When directly into As an aside note we could perform the transpose operation in memory before mining while still remaining within the worst case space complexity However on average and for practical levels of objects Algorithm 1 describes 17 the additional types we use such as items respectively To apply GLIMIT we 336rst transpose the dataset as a preprocessing step 19  We compared GLIMIT to a publicly available implementation of FP-Growth and Apriori We used the algorithms from ARtool 20 as it is written in 17 The pseudo-code in our algorithms is java-like and we assume a garbage collector which simpli\336es it Indentation de\336nes blocks and we ignore type casts 18 http://\336mi.cs.helsinki.\336/data 19 This is cheap especially for sparse matrices 320 precisely what the datasets in question typically are Our data was transposed in 8 and 15 seconds respectively using a naive Java implementation and without using sparse techniques 20 http://www.cs.umb.edu laur/ARtool It was not used via the supplied GUI The underlying classes were invoked directly itemvectors itemvectors along the path from node  We have sketched the proof of Lemma 2 Let  and shows the initialisation and the main loop 320 which calls methods used by We evaluated our algorithm on two publicly available datasets from the FIMI repository 18 320 T10I4D100K and T40I10D100K These datasets have  where the be the number of frequent items Hence we need space linear in the number of frequent items The multiplicative constant  method whereby a list priority queue of states each containing a node that has yet to be completely expanded is maintained The general construct is to retrieve the 336rst state evaluate it for the search criteria expand it create some child nodes and add states corresponding to the child nodes to the frontier Using different criteria and frontier orderings leads to different search techniques Our frontier contains any nodes that have not yet been completed wrapped in is odd it is  5     n y  1  3  5 n 212 3 n 212 1  items the worst case itemvector usage is just the number of coloured nodes in Figure 3 There are  n i  Note that in the even case the next step to that shown will use the same memory the itemvector for node t\212 2 y  1  3  5 n 212 3 n 212 1 n  frontier is no longer needed once we create  and when 3 5 3 5 3 5 5 3 n n 1 1 as we compute it so both need never be in memory at the same time nodes we know from the above that its upper bound is 3 It also describes the 3 step Figure 3 Maximum number of itemvectors Two cases   n   n   n   n 267 267 267 267 n 000 870 minM easure n be the number of frequent items Let b State State    n n n y n n n l n n be the number of items and n l calculateF  check 212 212 212   212 212 212 212  by Fact 6 and we write i eg  this would require more memory The algorithm is a depth 336rst traversal through the Pre\336xTree Any search can be implemented either recursively or using the frontier transactions and a realistic skewed histogram of items They have  n b  


Main Loop to join with  as we are creating    can write result directly into  pref ixT ree.getRoot y 002 003 004     004            1                         002                  boolean localT op F localT op localT op null localT op localT op localT op localT op m State State State State newState       if 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 Data-types initialisation main loop and methods  initialise i i i i i i i i  267 267 267 002   state.y state.y M minM easure state.itemvectors.next f f minM easure m p item   Iterator itemvectors boolean top P air newP air List buf f er  state.buf f er.iterator 002 localF rontier.removeF irst be the itemset corresponding to    nd P ref ixN ode M y y M M m m I I I I I I I I I I I I I I       P air p   f rontier.getF irst state.node p.item m 002             I I i i i i i i i i I i K K  Iterator is over   make use of anti-monotonic pruning property if   else   if   void y  hatwecreateif y y  P ref ixN ode node Item item double m  so we don\325t add it again  We need and and and becomes we create if   Fact 5 is not complete we  will add has been completed See and  1 Dataset  have already applied 2 Completed  containing all is the itemvector for in Fact 1 We keep reusing them through  is the itemvector corresponding to and in Fact 1  for the s created to hold the children of to make use of Fact 3 provides the as empty Create initial state objects Reads input one row at a time and annotates the itemvector with the item it corresponds to could also apply   while  is the parent of the new and is set so that is true only for a node that is along the  see end of method  and hence delete   the top child of in this step Fact 6  we are dealing with itemsets of length  Fact 6 or 7 No longer need as this is the last child we can create under  and it is not a single item other than perhaps the topmost  else  need to use additional memory for the child  don\325t need to calculate we know  if   Found an interesting itemset 320 create  there is potential to expand  Let is the itemset represented by a child of would be  This method calculates to look up the to get their values  Then it returns  details depend on  Check whether the itemset to check whether subsets of by Fact 3 exist details omitted Iterators any after              is used to create the may       P air P air   m   m inputF ile state.newP air state.newP air state.newP air p.item p.item pref ixT ree.createChildU nder p.item sequenceM ap.put state.buf f er.size state.newP air p.item P ref ixT ree pref ixT ree pref ixT ree p.y calculateF p.y check p.y p.y calculateF p.m p.M map map map map M   K  Remove y 002 y     006 006 002 002 002 002 002 002 002 002 002      001 001 m F-itemsets topmost branch  006 006 006 001 Itemvector y P ref ixN ode node Itemvector y itemvectors f rontier Iterator itemvectors f rontier.add  null itemvectors f alse null new LinkedList step nextT op State state null null state f rontier Itemvector y null boolean nextT op state.node.isRoot state.top nextT op nextT op state.top state.top state.node P ref ixN ode newN ode newN ode.item newN ode.M  nextT op new LinkedList state state.node node.item  Item item new AnnotatedItemvetorIterator new State state.node newN ode.M newN ode.top double m state.node state.node newN ode.m new State newN ode y f rontier.addF ront newState newState double boolean newN ode top top top newN ode newN ode newN ode node node node node node 002 267 267 267 f rontier.isEmpty 1 1 if  of the pre\336x tree  Algorithm 1 check such as  s else to so that      required subsets of from so  212\005 Input step Output Initialisation could be interesting by exploiting the anti-monotonic property of except  itemvectors 1 0  in transpose format   helps us do this  with its root Initialise     Perform one expansion  is true iff we are processing the top sibling of subtree   if    in the next   initialise  if  for it   add to front of frontier ie in front of if it\325s still present so depth 336rst search Fact 2  by using s corresponding to the 001  P ref ixN ode inputF ile  212 newP air    001 g f minM easure SequenceM ap item buf f er y buf f er buf f er y g state.buf f er.add state.itemvectors.hasN ext state.y I m state.node.getDepth  state.y y y y M M  minM easure  p state.buf f er item p item m F F 002 item 002 item item P ref ixN ode node Item item        calculateF  and corresponds to F use 


The fact that it is also fast when applied to traditional FIM is secondary sum 0 100 000 82  b Runtime and frequent itemsets T40I10D100K longer than 30 minutes for 267  To represent itemvectors for traditional FIM we used bit-vectors 21 so that each bit is set if the corresponding transaction contains the item\(set Therefore  0 001  5 5 step    n minSup 1 1  f  creates the bit-vector 21 We used the Lemma 2 the 336gure clearly shows this is never reached in our experiments Our maximum was approximately 24 over the calls to m    Java like our implementation and it has been available for some time In this section we really only want to show that GLIMIT is quite fast and ef\336cient when compared to existing algorithms on the traditional FIM problem Our contribution is the itemvector framework that allows operations that previously could not be considered and a 337exible and new class of algorithm that uses this framework to ef\336ciently mine data cast into different and useful spaces F m  n n b Colt Figure 4 Results g  Figure 4\(a shows the runtime 22 of FP-Growth GLIMIT and Apriori 23 on T10I4D100K as well   267 267 is larger much of the time and space is wasted GLIMIT uses time and space as needed so it does not waste as many resources making it fast The downside is that the operations on bit-vectors in our experiments of length times the number of items would be so small that the runtime would be unfeasibly large anyhow Furthermore the space a Runtime and frequent itemsets T10I4D100K Inset shows detail for low support  http://dsd.lbl.gov/\367hoschek/colt BitVector implementation 22 Pentium 4 2.4GHz with 1GB RAM running WindowsXP Pro 23 Apriori was not run for extremely low support as it takes as the number of frequent items The analogous graph for T40I10D100K is shown in Figure 4\(b 320 we did not run Apriori as it is too slow These graphs clearly show that when the support threshold is below a small value about 0.29 and 1.2 for the respective datasets FP-Growth is superior to GLIMIT However above this threshold GLIMIT outperforms FP-Growth signi\336cantly Figure 5\(a shows this more explicitly by presenting the runtime ratios for T40I10D100K FP-Growth takes at worst 19 times as long as GLIMIT We think it is clear that GLIMIT is superior above the threshold Furthermore this threshold is very small and practical applications usually mine with much larger thresholds than this GLIMIT scales roughly linearly in the number of frequent itemsets Figure 5\(b demonstrates this experimentally by showing the average time to mine a single frequent itemset The value for GLIMIT is quite stable rising slowly toward the end as there we still need to check itemsets but very few of these turn out to be frequent FP-Growth on the other hand clearly does not scale linearly The reason behind these differences is that FP-Growth 336rst builds an FP-tree This effectively stores the entire Dataset minus infrequent single items in memory The FPtree is also highly cross-referenced so that searches are fast The downside is that this takes signi\336cant time and a lot of space This pays off extremely well when the support threshold is very low as the frequent itemsets can read from the tree very quickly However when  can be time consuming when compared to the search on the FP-tree which is why GLIMIT cannot keep up when is very small Figure 5\(c shows the maximum and average 24 number of itemvectors our algorithm uses as a percentage of the number of items At worst this can be interpreted as the percentage of the dataset in memory Although the worst case space is  1 and  minSup minSup minSup  By the time it gets close to  AN D 


Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 7 Conclusion and Future Work References frontier  pages 487\320499 Morgan Kaufmann 1994  W orkshop on frequent itemset mining implementations 2003 http://\336mi.cs.helsinki.\336/\336mi03  W orkshop on frequent itemset mining implementations 2004 http://\336mi.cs.helsinki.\336/\336mi04  J  Han J  Pei and Y  Y in Mining frequent patterns without candidate generation In Proceedings of 20th International Conference on Very Large Data Bases VLDB VLDB Journal Very Large Data Bases Data Mining and Knowledge Discovery An International Journal Lecture Notes in Computer Science  2004  J W ang and G Karypis Harmon y Ef 336ciently mining the best rules for classi\336cation In The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD\32504 Symposium on Principles of Database Systems 2 b Average time taken per frequent itemset shown on two scales T10I4D100K is increased and hence the number of frequent items decreases Figure 5\(c also shows that the maximum frontier size is very small Finally we reiterate that we can avoid using the pre\336x tree and sequence map so the only space required are the itemvectors and the minSup SIAM International Conference on Data Mining required drops quite quickly as ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2000 ACM SIGMOD Intl Conference on Management of Data Figure 5 Results  8\(3\3204 2000  F  P an G C ong A T ung J Y ang and M Zaki Carpenter Finding closed patterns in long biological datasets In  2121:236 2001  M Steinbach P N T an H Xiong and V  K umar  Generalizing the notion of support In a Runtime ratios T10I4D100K c Number of Itemvectors needed and maximum frontier size T10I4D100K  pages 1\32012 ACM Press May 2000  F  K orn A Labrinidis Y  K otidis and C F aloutsos Quanti\336able data mining using ratio rules  Morgan Kaufmann 2003  J Pei J Han and L Lakshmanan Pushing convertible constraints in frequent itemset mining We showed interesting consequences of viewing transaction data as itemvectors in transactionspace and developed a framework for operating on itemvectors This abstraction gives great 337exibility in the measures used and opens up the potential for useful transformations on the data Our future work will focus on 336nding useful geometric measures and transformations for itemset mining One problem is to 336nd a way to use SVD prior to mining for itemsets larger than  pages 205\320215 2005  We also presented GLIMIT a novel algorithm that uses our framework and signi\336cantly departs from existing algorithms GLIMIT mines itemsets in one pass without candidate generation in linear space and time linear in the number of interesting itemsets Experiments showed that it beats FP-Growth above small support thresholds Most importantly it allows the use of transformations on the data that were previously impossible  That is the space required is truly linear  D Achlioptas Database-friendly random projections In  2001  R Agra w al and R Srikant F ast algorithms for mining association rules In  8:227\320252 May 2004  J Pei J Han and R Mao CLOSET An ef 336cient algorithm for mining frequent closed itemsets In  pages 21\32030 2000  S Shekhar and Y  Huang Disco v ering spatial colocation patterns A summary of results 


mator from sensor 1 also shown 6. CONCLUSIONS This paper derives a Bayesian procedure for track association that can solve a large scale distributed tracking problem where many sensors track many targets. When noninformative prior of the target state is assumed, the single target test becomes a chi-square test and it can be extended to the multiple target case by solving a multidimensional assignment problem. With the noninformative prior assumption, the optimal track fusion algorithm can be a biased one where the regularized estimate has smaller mean square estimation error. A regularized track fusion algorithm was presented which modifies the optimal linear unbiased fusion rule by a less-than-unity scalar. Simulation results indicate the effectiveness of the proposed track association and fusion algorithm through a three-sensor two-target tracking scenario 7. REFERENCES 1] Y. Bar-Shalom and W. D. Blair \(editors Tracking: Applications and Advances, vol. III, Artech House, 2000 2] Y. Bar-Shalom and H. Chen  Multisensor Track-to-Track Association for Tracks with Dependent Errors  Proc. IEEE Conf. on Decision and Control, Atlantis, Bahamas, Dec. 2004 3] Y. Bar-Shalom and X. R. Li, Multitarget-Multisensor Tracking Principles and Techniques, YBS Publishing, 1995 4] Y. Bar-Shalom, X. R. Li and T. Kirubarajan, Estimation with Applications to Tracking and Navigation: Algorithms and Software for Information Extraction, Wiley, 2001 5] S. Blackman, and R. Popoli  Design and Analysis of Modern Tracking Systems  Artech House, 1999 10 15 20 25 30 35 40 45 50 55 60 2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 1 Sensor 1 Centralized Est Track Fusion 10 15 20 25 30 35 40 45 50 55 60 0 2 


2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 2 Sensor 1 Centralized Est Track Fusion Fig. 7. Comparison of the NEES for centralized IMM estimator \(configuration \(i estimators \(configuration \(ii sensor 1 also shown 6] H. Chen, T. Kirubarajan, and Y. Bar-Shalom  Performance Limits of Track-to-Track Fusion vs. Centralized Estimation: Theory and Application  IEEE Trans. Aerospace and Electronic Systems 39\(2  400, April 2003 7] H. Chen, K. R. Pattipati, T. Kirubarajan and Y. Bar-Shalom  Data Association with Possibly Unresolved Measurements Using Linear Programming  Proc. 5th ONR/GTRI Workshop on Target Tracking Newport, RI, June 2002 8] Y. Eldar, and A. V. Oppenheim  Covariance Shaping Least-Square Estimation  IEEE Trans. Signal Processing, 51\(3 pp. 686-697 9] Y. Eldar  Minimum Variance in Biased Estimation: Bounds and Asymptotically Optimal Estimators  IEEE Trans. Signal Processing, 52\(7 10] Y. Eldar, A. Ben-Tal, and A. Nemirovski  Linear Minimax Regret Estimation of Deterministic Parameters with Bounded Data Uncertainties  IEEE Trans. Signal Processing, 52\(8 Aug. 2004 11] S. Kay  Conditional Model Order Estimation  IEEE Transactions on Signal Processing, 49\(9 12] X. R. Li, Y. Zhu, J. Wang, and C. Han  Optimal Linear Estimation Fusion  Part I: Unified Fusion Rules  IEEE Trans. Information Theory, 49\(9  2208, Sept. 2003 13] X. R. Li  Optimal Linear Estimation Fusion  Part VII: Dynamic Systems  in Proc. 2003 Int. Conf. Information Fusion, Cairns, Australia, pp. 455-462, July 2003 14] X. D. Lin, Y. Bar-Shalom and T. Kirubarajan  Multisensor Bias Estimation Using Local Tracks without A Priori Association  Proc SPIE Conf. Signal and Data Processing of Small Targets \(Vol 


SPIE Conf. Signal and Data Processing of Small Targets \(Vol 5204 15] R. Popp, K. R. Pattipati, and Y. Bar-Shalom  An M-best Multidimensional Data Association Algorithm for Multisensor Multitarget Tracking  IEEE Trans. Aerospace and Electronic Systems, 37\(1 pp. 22-39, January 2001 pre></body></html 


20 0  50  100  150  200  250  300 Pe rc en ta ge o f a dd iti on al tr af fic Cache size 200 clients using CMIP 200 clients using UIR c Figure 6. The percentage of additional traf?c the cache at every clock tick. A similar scheme has been proposed in [13], which uses fv, a function of the access rate of the data item only, to evaluate the value of each data item i that becomes available to the client on the channel If there exists a data item j in the client  s cache such that fv\(i j replaced with i A prefetch scheme based on the cache locality, called UIR scheme, was proposed in [7]. It assumes that a client has a large chance to access the invalidated cache items in the near future. It proposes to prefetch these data items if it is possible to increase the cache hit ratio. In [6], Cao improves the UIR scheme by reducing some unnecessary prefetches based on the prefetch access ratio \(PAR scheme, the client records how many times a cached data item has been accessed and prefetched, respectively. It then calculates the PAR, which is the number of prefetches divided by the number of accesses, for each data item. If the PAR is less than one, it means that the data item has been accessed a number of times and hence the prefetching is useful. The clients can mark data items as non-prefetching when PAR &gt; b, where b is a system tuning factor. The scheme proposes to change the value of b dynamically according to power consumption. This can make the prefetch scheme adaptable, but no clear methodology as to how and when b should be changed. Yin et al. [19] proposed a power-aware prefetch scheme, called value-based adaptive prefetch \(VAP the number of prefetches based on the current energy level to prolong the system running time. The VAP scheme de?nes a value function which can optimize the prefetch cost to achieve better performance These existing schemes have ignored the following characteristics of a mobile environment: \(1 query some data items frequently, \(2 during a period of time are related to each other, \(3 miss is not a isolated events; a cache miss is often followed by a series of cache misses, \(4 eral requests in one uplink request consumes little additional bandwidth but reduces the number of future uplink requests. In this paper, we addressed these issues using a cache-miss-initiated prefetch scheme, which is based on association rule mining technique. Association rule mining is a widely used technique in ?nding the relationships among data items. The problem of ?nding association rules among items is clearly de?ned by Agrawal et al. in [5]. However in the mobile environment, one cannot apply the existing association rule mining algorithm [4] directly because it is too complex and expensive to use This makes our algorithm different from that of [4] in 


This makes our algorithm different from that of [4] in twofold. First, we are interested in rules with only one data item in the antecedent and several data items in the consequent. Our motivation is to prefetch several data items which are highly related to the cache-miss data item within Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE the cache-miss initiated uplink request. We want to generate rules where the antecedent is one data item, but the cache-missed data item and the consequent is a series of data items, which are highly related to the antecedent. If we have such rules, we can easily ?nd the data items which should also be piggybacked in the uplink request. Second in mobile environment, the client  s computation and power resources are limited. Thus, the rule-mining process should not be too complex and resource expensive. It should not take a long time to mine the rules. It should not have high computation overhead. However, most of the association rule mining algorithms [4, 5] have high computation requirements to generate such rules 5. Conclusions Client-side prefetching technique can be used to improve system performance in mobile environments. However, prefetching also consumes a large amount of system resources such as computation power and energy. Thus, it is very important to only prefetch the right data. In this paper, we proposed a cache-miss-initiated prefetch \(CMIP scheme to help the mobile clients prefetch the right data The CMIP scheme relies on two prefetch sets: the alwaysprefetch set and the miss-prefetch set. Novel association rule based algorithms were proposed to construct these prefetch sets. When a cache miss happens, instead of sending an uplink request to only ask for the cache-missed data item, the client requests several items, which are within the miss-prefetch set, to reduce future cache misses. Detailed experimental results veri?ed that the CMIP scheme can greatly improve the system performance in terms of increased cache hit ratio, reduced uplink requests and negligible additional traf?c References 1] S. Acharya, M. Franklin, and S. Zdonik. Prefetching From a Broadcast Disk. Proc. Int  l Conf. on Data Eng., pages 276  285, Feb. 1996 2] S. Acharya, M. Franklin, and S. Zdonik. Balancing Push and Pull for Data Broadcast. Proc. ACM SIGMOD, pages 183  194, May 1997 3] S. Acharya, R. Alonso, M. Franklin, and S. Zdonik. Broadcast disks: Data Management for Asymmetric Communication Environments. Proc. ACM SIGMOD, pages 199  210 May 1995 4] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In J. B. Bocca, M. Jarke, and C. Zaniolo editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB pages 487  499. Morgan Kaufmann, 12  15 1994 5] R. Agrawal, Tomasz Imielinski, and Arun Swami. Mining Association Rules Between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207  216, Washington, D.C May 1993 6] G. Cao. Proactive Power-Aware Cache Management for Mobile Computing Systems. IEEE Transactions on Computers, 51\(6  621, June 2002 7] G. Cao. A Scalable Low-Latency Cache Invalidation Strategy for Mobile Environments. IEEE Transactions on Knowledge and Data Engineering, 15\(5 ber/October 2003 \(A preliminary version appeared in ACM MobiCom  00 8] K. Chinen and S. Yamaguchi. An Interactive Prefetching Proxy Server for Improvement of WWW Latency. In Proc INET 97, June 1997 9] E. Cohen and H. Kaplan. Prefetching the means for docu 


9] E. Cohen and H. Kaplan. Prefetching the means for document transfer: A new approach for reducing web latency. In Proceedings of IEEE INFOCOM, pages 854  863, 2000 10] R. Cooley, B. Mobasher, and J. Srivastava. Data preparation for mining world wide web browsing patterns. Knowledge and Information Systems, 1\(1  32, 1999 11] C. R. Cunha, Azer Bestavros, and Mark E. Crovella. Characteristics of WWW Client Based Traces. Technical Report TR-95-010, Boston University, CS Dept, Boston, MA 02215, July 1995 12] D. Duchamp. Prefetching hyperlinks. In USENIX Symposium on Internet Technologies and Systems \(USITS  99 1999 13] V. Grassi. Prefetching Policies for Energy Saving and Latency Reduction in a Wireless Broadcast Data Delivery System. In ACM MSWIM 2000, Boston MA, 2000 14] S. Hameed and N. Vaidya. Ef?cient Algorithms for Scheduling Data Broadcast. ACM/Baltzer Wireless Networks \(WINET  193, May 1999 15] Q. Hu and D. Lee. Cache Algorithms based on Adaptive Invalidation Reports for Mobile Environments. Cluster Computing, pages 39  48, Feb. 1998 16] Z. Jiang and L. Kleinrock. An Adaptive Network Prefetch Scheme. IEEE Journal on Selected Areas in Communications, 16\(3  11, April 1998 17] V. Padmanabhan and J. Mogul. Using Predictive Prefetching to Improve World Wide Web Latency. Computer Communication Review, pages 22  36, July 1996 18] N. Vaidya and S. Hameed. Scheduling Data Broadcast in Asymmetric Communication Environments. ACM/Baltzer Wireless Networks \(WINET  182, May 1999 19] L. Yin, G. Cao, C. Das, and A. Ashraf. Power-Aware Prefetch in Mobile Environments. IEEE International Conference on Distributed Computing Systems \(ICDCS 2002 Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE pre></body></html 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





