Multi-Tier Granule Mining for Rep resentations of Multidimensional Association Rules   Yuefeng Li, Wanzhong Yang, Yue Xu School of Software Engineering and Data Communications Queensland University of Technolo gy, Brisbane, QLD 4001, Australia y2.li@qut.edu.au  w3.yang@student.qut.edu.au  yue.xu@qut.edu.au    Abstract  It is a big challenge to promise the quality of 
multidimensional association mining. The essential issue is how to represent meaningful multidimensional association rules efficiently. Currently we have not found satisfactory approaches for solving this challenge because of the complicated correlation between attributes. Multi-tier granule mining is an initiative for solving this challenging issue. It divides attributes into some tiers and then compresses the large multidimensional database into granules at each tier. It also builds association mappings to illustrate the correlation between tiers. In this way, the 
meaningful association rules can be justified according to these association mappings  1. Introduction  Multidimensional association mining discusses two or more data dimensions or predicates [3   Us ua l l y  multidimensional association mining is designed for searching frequent predicate sets and that can be classified into inter-dimension and hybrid-dimension association rule mining We can obtain a huge amount of association rules using the existing data mining techniques. However 
not all strong association rules are interesting to users 3 eral a p p ro ach es h av e b ee n con d ucted  in or d er to guarantee the quality of discovered knowledge: the concept of closed patterns [1 1 5 no nre du nda n t  rules [17 8 an d co ns t r ai n t-b ased ass o ciati o n ru les 1  5   1 0] [1 2] [1 6   These approaches have significant performance for decreasing the number of association rules for transaction databases. However, they are not very efficient for representation of associations in very large multidimensional databases because we have to 
transfer multidimensional rule mining into single dimensional mining when we use these approaches Different to these approaches, in this paper we present the concept of granule mining \(GM\ in multidimensional databases for directly representations of associations between attributes, where a granule is a group of objects \(transactions\ that have the same attributes\222 values Basically attributes are divided by users into two groups: condition attributes and decision attributes and decision tables can be used to represent the 
association between condition granules and decision granules [1  8  I n cases o f l ar g e n u m b er  o f  attributes, however, decision tables become inefficient Decision tables also cannot describe association rules with shorter premises \(we call such rules general rules in this paper\. To solve these drawbacks, in this paper we present multi-tier structures and association mappings to manage associations between attributes. It provides an alternative way to represent multidimensional association rules efficiently The remainder of the paper is structured as follows 
We begin by introducing the concept of closed patterns and decision tables for granule mining in Section 2. In section 3, we discuss the relationship between granule mining and data mining. In Section 4, we introduce the multi-tier structure. In Section 5, we formalize association mappings for GM.  In Section 6, we introduce the experiment results. The last section includes related work and conclusions  2. Basic Definitions  Formally, a transaction database can be described as an information table T 
 V T where T is a set of objects in which each record is a sequences of items, and V T    a 1  a 2 205 a n is a set of selected items \(or called attributes in decision tables\r all objects in T Each 


item can be a tuple, e.g name  cost  price s a product item  2.1 Closed Patterns  Definition 1 A set of items X is referred to as an itemset if X  V T Let X be a itemset, we use  X  to denote the covering set of X which includes all objects t such that X   t i.e  X   t  t  T  X   t   Given an itemset X its occurrence frequency is the number of objects that contain the itemset that is  X   and its support is  X   T An itemset X is called frequent pattern if its support  min _ sup a minimum support  Definition 2 Given a set of objects Y its itemset  which satisfies itemset  Y  a  a  V T  t  Y  a t   Given a frequent pattern X its closure  Closure  X  itemset   X   From the above definitions, we have the following theorem \(see [18 Theorem 1 Let X and Y be frequent patterns. We have 1  Closure  X   X for all frequent patterns X  2  X  Y  Closure  X   Closure  Y     Definition 3 An frequent pattern X is closed if and only if X  Closure  X    2.2. Decision Tables  Decision tables can be used for dealing with multiple dimensional databases in line with user constraints Formally, users may use some attributes of a database and they can divide these attributes into two groups condition attributes and decision attributes respectively. We call the tuple T  V T  C  D  decision table of T  V T  C D and C D V T  We usually assume that there is a function for every attribute a V T  such that a  T   V a where V a is the set of all values of a We call V a the domain of a  C or D etermines a binary relation I  C or I  D on T  such that t 1  t 2   I  C f and only if a  t 1  a  t 2 or every a C where a  t denotes the value of attribute a  for object t T It is easy to prove that I  C s an equivalence relation, and the family of all equivalence classes of I  C that is a partition determined by C is denoted by T  C  The classes in T  C or T  D are referred to Cgranules or D-granule e class which contains t is called C-granule induced by t and is denoted by C  t   Object  items t 1 a 1 a 2  t 2 a 3 a 4   a 6  t 3 a 3 a 4  a 5 a 6  t 4 a 3 a 4   a 5 a 6  t 5 a 1 a 2  a 6 a 7  t 6 a 1 a 2  a 6 a 7   Table 1 lists a part of an transition database, where V T   a 1  a 2 205 a 7  T  t 1  t 2 205 t 6 We also can represent Table 1 as a decision table.  Let a 1  a 2  a 3  a 4  and a 5  be the condition attributes and a 6 and a 7  be the decision attributes  Table 2 shows a decision table of Table 1, where T C D  g 1  g 2  g 3  g 4 and N g is the number of objects that are in the same granule  Table 2 A decision table Granule a 1 a 2 a 3 a 4 a 5 a 6 a 7 N g g 1 1 1 0 0 0 0 0 1 g 2  0 0 1 1 0 1 0 1 g 3  0 0 1 1 1 1 0 2 g 4  1 1 0 0 0 1 1 2  Every granule in the decision table can be mapped into a decision rule [11 wh ere we t r eat the pr esen ce and absence of items as the same position if we view the decision table as a multidimensional database Therefore, we can obtain 4 decision rules in Table 2 and the first one can be read as the following decision rule a 1 =1 a 2 = 1 a 3 = 0 a 4 = 0 a 5 = 0   a 6 = 0 a 7 = 0 or in short C  g 1   D  g 1 or C  t 1   D  t 1 where means \223 and 224  3. Data Mining and Granule Mining  Decision tables provide an efficient way to represent discovered knowledge. However, currently we can only obtain decision rules, a kind of very special association rules, in decision tables. To interpret what kinds of association rules in the decision tables, we present the concept of decision patterns Given a C granule cg= C  t its covering set   cg    t 222   t 222  T  t 222 t  I  C  cg be a C granule and dg  be a D granule we define  cg dg    cg    dg   For example, in Table 2 g 1  a 1 =1 a 2 = 1 a 3 0 a 4 = 0 a 5 = 0 a 6 = 0 a 7 = 0 C  g 1  D  g 1  cg 1  dg 1 therefore  g 1   cg 1  dg 1   cg 1   dg 1  


  d 1 d 5 d 6   d 1 d 1  Table 3 illustrates the covering sets of granules where \(a\des the covering sets of C-granules b includes the covering sets of D-granules and \(c includes the covering sets of C D-granules     Table 3 Covering sets of granules Granule a 1 a 2 a 3 a 4 a 5 covering set cg 1 1 1 0 0 0 t 1 t 5 t 6 cg 2  0 0 1 1 0 t 2 cg 3  0 0 1 1 1 t 3 t 4 a\ of C-granules   Granule a 6 a 7 covering set dg 1 0 0 t 1 dg 2 1 0 t 2 t 3 t 4 dg 3 1 1 t 5 t 6 b\Covering sets of D-granules   Granule a 1 a 2 a 3 a 4 a 5 a 6 a 7 covering set g 1 1 1 0 0 0 0 0 t 1 g 2  0 0 1 1 0 1 0 t 2 g 3  0 0 1 1 1 1 0 t 3 t 4 g 4  1 1 0 0 0 1 1 t 5 t 6 c\ of C D-granules   Definition 4 Let X be a frequent pattern. We call it a decision pattern if g   T C D  such that X  a i   C D   a i  g We call X the derived decision pattern of g   Theorem 2 Let T  V T  C  D a decision table We have 1  C  t     C D  t   for all t  T  2\The derived decision pattern of every granule g T C D is a closed pattern  Proof 1\ is obvious in accordance with the definition of closure   For \(2 X be the derived pattern of g that is X  a i  C D   a i  g 1}. From the definition of the granules, we know there is a object t 0    g  such that X   a i  C D  a i  t 0 1}, that is t 0   X   Given an item a   itemset   X  ding to Definition 2 we have a  t  for all t     X  that is a   t 0 and also a   X Therefore Closure  X  itemset   X    X  We also have X   Closure  X from Theorem 1, and hence we have X  Closure  X     4. Multi-Tier Structures  In cases of large number of attributes, the decision tables become inefficient. Also, we cannot discover general rules in decision tables, for example association rules with shorter premises. In addition some decision rules may be meaningless. In this section, we present a multi-tier structure to manage the correlation between attributes in order to overcome these disadvantages of decision tables. We also clarify the meaning of meaningless in this section Let T   C be the set of condition granules and  T   D  be the set of decision granules. To describe the association between condition granules and decision granules, we can further divide the condition attributes into some groups in accordance with user constraints We assume that C i  and C j  are two subsets of condition attributes; and they satisfy C i  C j  and C i   C j   C  Figure 1 illustrates the structure of the multi-tier granule mining, which describes the hierarchy of the possible associations between tiers, where T  C i   cg i 1  cg i 2 205 cg i  k  T   C j    cg j 1  cg j 2 205 cg j  m and T   D   dg 1  dg 2 205 dg s     Figure 1 The hierarchy of the multi-tiers  The decision rules in the structure of multiple tiers can be illustrated as follows cg i  x  cg j  y   dg z   conf   cg i  x  cg j  y   dg z    cg i  x  cg j  y   Different to decision tables, we can obtain some general association rules with shorter premises as follows cg i  x   dg z  conf   cg i  x    dg z    cg i  x   In Figure 1, we assume that   cg i 1  cg j 1   dg 1  3  cg i 1  cg j 1   dg 2  1   cg i 1  cg j 2   dg 1  4  cg i 1  cg j 2   dg 3  2   cg i 1  cg j 1  4  cg i 1  cg j 2  6  cg i 1  10 According the above assumption and the structure in Figure 1, we have the following decision rules cg i 1  cg j 1   dg 1   conf 276 = 0.75 cg i 1  cg j 1   dg 2   conf 274 = 0.25 cg i 1  cg j 2   dg 1   conf 4/6 = 0.67 cg i 1  cg j 2   dg 3   conf 2/6 = 0.33 c g i  1 c g i  2 c g i  k 205  c g j  1 c g j  2 205  c g j m d g 1 d g 2 d g 3  d g s 4 6 3 1 4 2 


We also can obtain a general association rule with the shorter premise cg i 1   dg 1   conf 7/10 = 0.70  Definition 5 A rule cg i  x  cg j  y   dg z is called meaningless if its confidence is less than or equals to the confidence of its general rule cg i  x    dg z     Based on Definition 5, rule cg i 1  cg j 2   dg 1   conf  4/6 = 0.67\ is a meaningless rule since its confidence 0.67\ is less than the confidence \(0.70\ of its general rule cg i 1   dg 1   conf 7/10 = 0.70 The rationale of the above definition is analogous to the definition of interesting association rules. If add extra evidence to a premise and obtain a weak conclusion, we can say the piece of evidence is meaningless  5. Association Mappings  In this section, we firstly formalize the basic association in a decision table, and then we develop methods to derive other associations between granules in different tiers based on this basic association The basic association between condition granules and decision granules can be described as an association mapping i,j,d such that       y j x i d j i cg cg  is a set of D-granule integer pairs  For example, using the granules in Figure 1, we have  1    3     2 1 1  1    dg dg cg cg j i d j i  Let N  T and dg z  dg  dg  f        y j x i d j i cg cg  we can obtain a decision rule cg i  x  cg j  y   dg z with the support and confidence  N f dg cg cg y j x i d j i z cg cg f dg z y j x i              sup  conf   cg i  x  cg j  y   dg z    cg i  x  cg j  y                     y j x i d j i y j x i d j i z cg cg f dg cg cg f dg f f  The association mapping i,j between T  C i and T  C j  can be derived from association mapping i,j,d where     x i j i cg is a set of C j granule integer pairs. The following is the equation that we can derive  1      1                    y j x i d j i cg cg f dg y j x i d j i y j x i j i f f cg cg f cg cg  For example, using the granules in Figure 1, we have  6    4     2  1  1   j j i j i cg cg cg  It is more complicated to derive the association i,d  between T  C i and T  D based on association i,j,d and i,j To simplify this process, we first review the composition operation that defined in [9   Let P 1 and P 2 be sets of D-granule integer pairs We call P 1   P 2 the composition of P 1  and P 2 which satisfies                               2 1 2 1 2 1 2 2 1 1 2 1 2 1 P  P f dg P gname P gname P gname P gname dg f dg P f dg P f dg f f dg P P  where gname  P i  dg  dg  f  P i  The operands of are interchangeable, therefore we can use  P 1  P 2  P 3 to be the short form of P 1   P 2   P 3 The result of the composition is still a set of D-granule integer pairs Let i,d be the association mapping between T  C i  and T  D we have the following equation for it    1                  x i j i y j y j x i d j i x i d i cg f cg cg cg cg  for all cg i  x  T  C i   Algorithm 5.1 Construction of Multi-Tiers  Input parameters   T  V T  C  D  C i  C j   Output Association mappings Method 1  Evaluate  i,j,d   T  C    for \(each g  T  C D  start from a decision table  if C  g   T  C  notice  g  C  g   D  g    i,j,d  C  g   i,j,d  C  g   D  g  N g  else  T  C    T  C  C  g   i,j,d  C  g    D  g  N g   Method 2 Evaluate  i,j  T  C i     for \(each cg  T  C   f 1= 0 notice  cg  C i  cg   C j  cg   for dg  f   i,j,d  cg  f 1   f 1 f  if C i  cg   T  C i   i,j  C i  cg   i,j  C i  cg   C j  cg  f 1 else  T  C i    T  C i   C i  cg   i,j  C i  cg    C j  cg  f 1   Method 3  Evaluate  i,d  for \(each cg i,x  T  C i   i,d  cg i,x    for cg j,y  f 1 i,j  cg i,x    i,d  cg i,x  i,d  cg i,x   i,j,d  cg i,x cg j,y   For example, using the information in Figure 1 we have i,d  cg i 1   dg 1 3  dg 2 1  dg 1 4  dg 3 2 


 dg 1 7 dg 2 1 dg 3 2 Algorithm 5.1 describes the main procedure of the construction of the multi-tier structure. It includes three methods for calculating the three kinds of association mappings The time complexity of Algorithm 5.1 is determined by Method 1 because T  C i   T  C In Method 1, checking C  g   T  C takes O T  C so the time complexity of the algorithm is O n  T  C ere n is the number of granules in the decision table, and the basic operation is the comparison between granules. Since T  C   n the time complexity of Algorithm 5.1 O n 2  After constructed the multi-tier structure, we can easily obtain decision rules and general rules by traversal of the multi-tier structure. Pruning meaningless decision rules can also be simply implemented by removing pairs from the corresponding association mapping. For example given a condition granule cg based on its general rule we might remove pairs in i,j,d  cg the conclusions of meaningless rules  6. Experiments  We simulate the data in a real multiple store environment where a fact table of sales can be described using one star schema including multiple dimensions: customer dimension, time dimension, data dimension, store dimension and production dimension In our current experiment, we use time and production dimensions only. A product includes name  cost and price attributes The fact table of sales in a financial year includes 26,590 transaction records and 5000 different products. We view each product as an item. We set one to an item for a transaction if it appears in the transaction; otherwise we set up zero to it We first select 300 most frequent products as items \(attributes a 1 a 2, \205 a 300}. We also choice 162 products C from the 300 products, which profits are more than 50 price > 1.5 cost s condition attributes; and select 35 products D from the 300 products, which profits are less than or equal to 20  1.2 cost > price as decision attributes After compressed the transaction records, we obtain a decision table which includes 2486 granules and hence we can generate 2486 decision rules The condition attributes are further divided into two tiers C i tier the products that profits are more than 90 price  1.9 cost  C j tier the products that  1.9 cost price 1.5 cost The products are now classified into three tiers high profit C i tier  medium profit C j tier and low  profit D-tier The association between high profit products and low profit produces can also be described as general rules Figure 2 illustrates the numbers of granules and attributes in the three tiers. To compare to decision table, the multi-tier structure is extremely impressive since only very small amounts of granules are useful for generating rules                 0 500 1000 1500 2000 2500 Decision table C-tier Ci-tier Cj-tier D-tier General rules Granules Attributes  Figure 2 Granules and attributes in multi-tiers  Figure 3 depicts the percentages \(57.9%\f meaningless decision rules that can be pruned  It is also shows the percentages \(68.9%\ of transactions that are covered by these meaningless decision rules  The Percentage of Corresponding Granule and Transaction Numbers 0 10 20 30 40 50 60 70 80 90 100 Granules Transactions Meaningful Meaningless  Figure 3 The percentage of meaningless decision rules  In summary, the results demonstrate that the multitier structure uses only a very small space to store meaningful multi-dimensional association rules. It is a very efficient and promising alternative of decision tables for representations of multidimensional association rules  7. Related Work  As mentioned in the introduction, several approaches have been conducted for the quality of discovered knowledge. We also noticed another interesting research, which discussed the similarity between patterns to discover the real useful patterns [1 For multidimensional association mining, Han et al in [3  4  su m m arized t h e p o s s i b le tec h n iqu es in  accordance with the corresponding treatments of 


quantitative attributes. Habitually, most current researchers on multidimensional association mining endeavor to use the existing efficient algorithms for single dimensional mining. Lee et al. presented an approach for multidimensional constraints [6 checked constraint during FP-tree constructions. The approach firstly grouped products at the same cost and price into an item, and view the product table as a set of transactions In this paper, we concentrate on inter-dimension association mining. Different to other ideas, we want to describe the associations in multidimensional databases based on some abstractions \(granules In the beginning, rough set theory looked like an adequate tool for this question, and can be used to describes the knowledge in information tables [2 7    Further, rough set based decision tables  p r e s e nt e d by Pawlak can be used to represented some sorts of association rules. Li and Zhong [8 also pr esen t ed a structure to disconnect the condition granules and decision granules in order to improve the efficiency of generating associate rules from decision tables Rough set theory is elegant, and has a clear logical semantics. However, it lacks the accurateness when we use it to deal with the associations between granules in multiple tiers  8. Conclusions  In this research, we present a multi-tier granule mining approach in order to provide a foundational framework for efficiently representations of multidimensional association rules. We demonstrate that it is a significant replacement of decision tables. We also prove that granules in decision tables are kinds of closed patterns. In addition, we present the definition of meaningless decision rules. The definition can also be justified in the multi-tier structure  Acknowledgments  This paper was partially supported by Grant DP0556455 from the Australian Research Council The authors also wish to thank Professor Ning Zhong at Maebashi Institute of Technology, Japan for his valuable comments  References   C  B u cila, J  G e hrk e  D  K i f e r  W  W h it e   223D ua l Miner  a Dual-Pruning algorithm for Itemsets with constraints,\224 ACMSIGKDD Alberta, Canada, 2002, 42-51  J  W  Guan, D. A Bell, D. Y. Liu, \223T h e  rough s et approach to association rules,\224 ICDM USA, 2003, 529-532  Han and M  Ka m b e r  223 Data Mining: Concepts   and Techniques\224 Morgan Kaufmann Publishers, 2000 4 J  H an, Y   Fu, \223M ining m u l tiple-level a s s o c i a t ion ru les in large databases,\224 IEEE Transaction on Knowledge and Data Engineering 1999 11\(5 798-805  e ung, L  V.S   La ks hmanan R T  Ng 223Exploiting succinct constraints using FP-trees,\224 ACMSIGKDD Explorations 2002, 40-49 6 A   J  T  L e e  W  L i n  C  Wang, \223Mining association rules with multi-dimensional constraints,\224 The Journal of Systems and Software 2006, 79-92  Y L i 223Extended random sets for knowledge dis c overy in information systems\224 9 th International Conference on Rough Sets, Fuzzy Sets, Data Mining and Granular Computing  China, 2003, 524-532  Y. L i and N Z hong 223Interpr etations of association rules by granular computing,\224 ICDM USA, 2003, 593-596  Li and N. Z hon g 223M in ing ontology for automatically acquiring Web user information needs,\224 IEEE Transactions on Knowledge and Data Engineering 2006 18\(4 554-568   Ng, L  V.S   Lakshm a n a n, J  Han A. Pang 223Exploratory mining and pruning optimizations of constrained associations rules,\224 ACM-SIGMOD Seattle Washington, USA, 1998, 13-24  uit of patterns in data reasoning from data, the rough set way,\224 3 rd International Conference on Rough Sets and Current Trends in Computing USA, 2002 1-9  J  Pei, J Han, L.V.S Lakshmanan, \223Mining frequent itemsets with conver tible constraints,\224 17th International Conference on Data Engineering Heidelberg, Germany 2001, 433-442  Tsu m oto and S. Hira no, \223Visualization of rule\222s similarity using multidimensional scaling\224 ICDM  Melbourne, Florida, USA, 2003, 339-346  z vetkov X. Yan a nd J. Han, \223TSP: Mining top-K closed sequential patterns,\224 ICDM Melbourne, Florida USA, 2003, 347-354  S.T W u  Y  Li Y X u  B  P h am and P Ch en 223Automatic pattern taxonomy exatraction for Web mining,\224 IEEE/WIC/ACM International Conference on Web Intelligence Beijing, China, 2004,  242-248  I W e bb and S  Zhang, \223K-op timal rule d i scovery 224  Data Mining and Knowledge Discovery 2004 10 39-79  Xu, and Y. L i  223M ining for us e f ul a s s ociation rules using the ATMS,\224 CIMCA2005, Vienna, Austria Nov. 2005 271-276  M   J  Z a ki, \223M ining non-re d undant as s o c i a t ion rules  224  Data Mining and Knowledge Discovery 2004 9 223 -248 


                                                                       Table 5. A Generalized Table GK with a new named attribute 3. A level two concept is a named \(or interpreted alence class of ?rst level concepts, in general, are the second innermost relation, and etc 4. A n level two concept is a named \(or interpreted e q u i v a l e n c e  c l a s s  o f      level concepts 5. These concept hierarchy groups the base concepts into a nested sequence of named partitions of based concepts. For each named partition \(that is, the partition and each equivalence class has a name is introduced into the given table I n  t h i s  e x a m p l e     a t t r i b u t e  v a l u e s  a r e  t h e  b a s e  c o n c e p t s   A named partition is de?ned: The equivalence class     i s  n a m e d     a n o t h e r  e q u i v a l e n c e  c l a s s        a n d  t h e p a r t i t i o n      T h i s  g e n e r a l i z a t i o n  i n t r o d u c e s  a  n e w  n a m e d a t t r i b u t e     c o l u m n   i n t o  t h e  g i v e n  t a b l e   T a b l e  5   4.2 AOG on GDM In traditional concept hierarchy, all partitions are named To be uniform, we will consider the unnamed case. We will take the following   C o n v e n t i o n   u n n a m e d  p a r t i t i o n  w i l l  b e  r e g a r d e d  a s canonically named, that is, the partition and equivalence classes themselves are their own names T o  i l l u s t r a t e  t h e  i d e a   T h e  n e w l y  n a m e d    will be  unnamed  in its GDM. Let us use GDM          of the partition           a t t r i b u t e  i n d u c e s a new partition on  as follows: From Table 2   s a granule             d e  n e s      a n d    d e  n e s           T h e  n e w  g r a n u l e  i s                            a n d            T h e s e  t w o  n e w granules de?ne a new partition Table 6 The GDM of new             


                      Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE                                                                                                Table 6. A Generalized Table GK with a uninterpreted attribute By the convention, a canonically named partition or equivalence class will be referred to as an un-interpreted attribute or attribute value. Following the same spirit, a TOG, TOB or GDM is called a un-interpreted table or data model 4.3 The Feature Completion on GDM Traditional AOG focuses on one attribute. There are no reasons to stop at considering one attribute here we will consider a concept hierarchy on a set                      o f  a t t r i b u t e s   A s  w e  h a v e  o b s e r v e d in Theorem 3.4., it is adequate to do AM in GDM is      I n  t h i s  c a s e     i s  a  s u b s e t  o f we will denote it bu  Definition 4.1. A generalization over in a GDM is a coarser partition of             where    


             i s  a  n o n e m p t y  s u b s e t  o f   If we let varies through all non-empty subsets of  we have all possible generalizations of in GDM. We w i l l  d e n o t e  t h e  s e t  o f  a l l  g e n e r a l i z a t i o n s  b y     Observe that the intersection of generalizations is still a generalization. For any given ?nite set of generalizations t h e r e  i s  t h e  s m a l l e s t  g e n e r a l i z a t i o n   S o     i s  c l o s e d under meet \(=the intersection the smallest g e n e r a l i z a t i o n    S o     i s  a  l a t t i c e   M o r e  i m p o r t a n t l y  t h e  m e e t  a n d  j o i n  a r e  t h e  m e e t  a n d  j o i n  i n  t h e  l a t t i c e      of partitions on  Let      b e  t h e  s m a l l e s t  s u b l a t t i c e  o f      t h a t  c o n t a i n s and      b e  t h e  s m a l l e s t  s u b l a t t i c e f      t h a t  c o n t a i n s  a l l  c o a r s e n i n g  o f  Now we have Theorem 4.2         Based on this observation, we de?ne Definition 4.3. GDM           i s  c a l l e d  U n i v e r s a l  M o d e l of      i n  t h e  s e n s e  i t  c o n t a i n s  a l l  i t s  g e n e r a l i z a t i o n s     i s  t h e  f e a t u r e  c o m p l e t i o n  o f   4.4 Intuitive Discussions on Features/attributes We often hear such an informal statement  a new feature  a t t r i b u t e     i s  s e l e c t e d   e x t r a c t e d   o r  c o n s t r u c t e d  f r o m  a s u b s e t                      o f  a t t r i b u t e s  i n  t h e  t a b l e     What does such a statement mean First we observe that feature and attribute have been used interchangeably. In the classical data model, an attribute or a feature is a representation of property, characteristic, and etc.; see e.g., [15]. A feature represents a human perception about the data; each perception is represented by a symbol, and has been called attribute and the set of attributes a schema. Based on our convention, they are words in TDP Section 3 L e t  u s  a s s u m e  a  n e w  f e a t u r e    h a s  b e e  s e l e c t e d   e x t r a c t e d   o r  c o n s t r u c t e d   L e t  u s  i n s e r t  i t  i n t o  t h e  t a b l e    T h e  n e w  t a b l e  i s  d e n o t e d  b y           T h e  i n f o r m a l  s t a t e m e n t  p r o b a b l y  m e a n s  i n  t h e  n e w  t a b l e     i s  a n  a t t r i b u t e   A s i t  i s  d e r i v e d  f r o m     i t  i s  f u n c t i o n a l l y  d e p e n d e d  o n   s extraction and construction are informal words, we can use the functional dependency as formal de?nition of feature selection, extraction and constructions. Formally we de?ne 


selection, extraction and constructions. Formally we de?ne Definition 4.4.1   i s  a  f e a t u r e  d e r i v e d   s e l e c t e d   e x t r a c t e d  a n d  c o n s t r u c t e d f e a t u r e   f r o m     i f    i s  f u n c t i o n a l  d e p e n d e n t  o n    i n  t h e n e w  t a b l e          In Theorem 4.2, we have shown that      i s  f e a t u r e completion of By the convention in Section 4.2    i s  u n i n t e r p r e t e d  f e a t u r e  c o m p l e t e i o n  o f   This theorem is rather anti-intuitive. Taking human  s view there are in?nitely many features. But the theorem says there are only ?nitely many features \(as      i s  a   n i t e set the ?nite-ness slip in? Our analysis says it comes in at the representation phase. We represent the universe by ?nite words. However, in phase two, suddenly these words are reduced to symbols. Thus the in?nite world now is encoded by a ?nite set of symbols. In particular, features can only be encoded in a ?nite distinct ways. A common confusing most likely comes from the confusing of data mining and  facts  mining 5 Generalized Associations in GDM As we have observed that it is adequate to conduct AM in the canonical model, such as GDM Main Theorem 5.1. Let          b e  t h e  u n i v e r s a l m o d e l   L e t  g  b e  a  g r a n u l e  i n  a  p a r t i t i o n          h t h a t         T h e n  g  i s  a n  u n i n t e r p r e t e d  g e n e r a l i z e d associations Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Let us de?ne an operation of binary number x and a set S We write S*x to be de?ned by The two equations            i f                       i f       r     Main Theorem 5.2                         be the smallest element in     L e t                b e  t h e  g r a n u l e s  i n     T h e n  t h e union                    is a granule that represents a un-interpreted generalized association rule, if its cardinality             where s is the threshold 


where s is the threshold Remark: The cardinal number of     i s  b o u n d e d  b y the Bell number [2] of       t h e  c a r d i n a l  n u m b e r  o f               The total number of derived attributes is bounded by Bell number  However the complexity of  minimal solutions  is bounded by the combination   We will report the calculation on real world data in future report soon 5.1 Find Generalized Association Rule by Linear Inequalities - an example We will illustrate the idea of the procedure of ?nding generalized association rules in Table 7 by linear inequality \(supp o r t         T h e  a s s o c i a t i o n  c a n  b e  e x p r e s s e d  a s  g r a n u l e s  1. Associations of length one  a   T E N                  b   S J                  c   L A          2. Associations of length two  a    T E N  S J     T E N    S J                  w h e r e   T E N  S J        


                        is in table format, that is equivalent to GDM f o r m a t   T E N    S J  3. No associations of length      Now let us examine the universal model in Table ??. The c o l u m n    i n  T a b l e     i s  t h e  s m a l l e s t  e l e m e n t  i n  t h e  c o m plete relation lattice      S o  e v e r y  e l e m e n t  o f     s a  c o a r s e n i n g  o f     I n  o t h e r  w o r d s   e v e r y  g r a n u l e  i n     i s  a  u n i o n  o f  s o m e  g r a n u l e s  f r o m  t h e  p a r t i t i o n     b y  t h e expression  a granule in     we mean a granule belonging to one of its partitions I n  t h i s  e x a m p l e   t h e  g r a n u l e s  i n   e    T W E N T Y    N Y           S J                     T W E N T Y    L A        T H I R T Y          be the cardinality of The following expression represents the cardinality of granules in      w h i c h  i s  a u n i o n  o f  s o m e  g r a n u l e s  f r o m  t h e  p a r t i t i o n     T W E N T Y  Y        N  J       T W E N T Y        


L A         T H I R T Y    L A           By taking the actual value of the cardinalities of the granules, we have                                                                 We will express the solutions in vector form              I t  i s  a n   integral convex set  in 4dimensional space The  boundary solutions  are 1   0   1   0   0    t h i s  s o l u t i o n  m e a n s         s cardinality b y  i t s e l f  a l r e a d y  m e e t s  t h e  t h r e s h o l d        2 \(0, 0, 1, 1 granules T W E N T Y    L A  a n d  T H I R T Y    to meet the threshold. In other words, we need a generalized concept that covers both the sub-tuple  T W E N T Y   L A    T W E N T Y    L A  a n d  T H I R T Y   L A    T H I R T Y    For this particular case, since L A     T W E N T Y   L A      T H I R T Y   L A   hence LA is the desirable generalized concept 3 \(1, 0, 0, 1 granules      T H I R T Y    L A  Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Table of Granules Table of Symbols                      


                                                       


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





