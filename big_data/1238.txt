Mining Ratio Rules Via Principal Sparse Non-Negative Matrix Factorization Chenyong Hu1,Benyu Zhang2,Shuicheng Yan3,Qiang Yang4,Jun Yan3,Zheng Chen2,Wei-Ying Ma2 1      3             Abstract Association rules are traditionally designed to capture statistical relationship among itemsets in a given database. To additionally capture the quantitative association knowledge, F.Korn et al recently proposed a 


paradigm named Ratio Rules [4] for quantifiable data mining. However, their approach is mainly based on Principle Component Analysis \(PCA cannot guarantee that the ratio coefficient is non-negative This may lead to serious problems in the rules application. In this paper, we propose a new method called Principal Sparse Non-Negative Matrix Factorization \(PSNMF itemsets in the form of Ratio Rules. In addition, we provide a support measurement to weigh the importance of each rule for the entire dataset 1. Introduction Association rules are one of the major representations in representing the knowledge discovered from large databases. The problem of association rule mining \(ARM in large transactional databases was introduced in [1, 3 Its basic idea is to discover important and interesting associations among the data items. The form of such association is as following 80 To find association rules, most prevalent approaches assume the transactions only carry Boolean information and ignore the valuable knowledge inherent in the quantities of the items. In fact, considering that the quantities of the items normally contain valuable information for us, it is necessary to provide a definition of quantitative association rules when the datasets contain quantitative attributes. Several efficient algorithms for mining quantitative association rules have been proposed in the past [2, 7]. A notable algorithm is the work [4 where they provided a stronger set of rules as Ratio Rules A rule under this framework is expressed in the following form   bread milk butter a b c a b c is arbitrary numerical values  This rule states that for each a amount spent on bread, a customer normally spends b amount on milk and c amount on butter Principal Component Analysis \(PCA discover the eigen-vectors of a dataset. Ratio Rules [4 can represent the quantitative associations between items as the principal eigen-vectors, where the values a, b and c in the example above correspond to the projections of the eigenvector. Because the element of eigen-vector can be either positive or negative, sometime the ratio coefficient of Ratio Rules may contain negative value, such as 1: -2 : -5Shoe Coat Hat 


Obviously, such rule loses the intuitive appeal of associations between items, because a customers spending should always be positive Our method amounts to a novel application of nonnegative matrix factorization \(NMF  cannot directly apply NMF for our purpose, because it is still difficult to explain that these latent components represent the latent association between items in a quantifiable dataset. We need to provide a bridge to bring NMF closer to association rules In this work, we propose a novel method called Principal Sparse Non-Negative Matrix Factorization PSNMF the non-negativity constraint in the standard NMF[5 The rest of the paper is organized as follows: Section 2 describes the problem and the intuition behind the Ratio Rules. Section 3 introduces our new algorithm \(PSNMF Section 4 presents the experimental results. Section 5 concludes the paper. The convergence of PSNMF learning procedure is provided in Appendix 2. Problem Definition The problem that we tackle is as follows. Given a N M  matrixV \(e.g., market basket databases ijv gives the amount spent by customers on the product The goal is to find all Ratio Rules of the form Proceedings of the Fourth IEEE International Conference on Data Mining ICDM04 0-7695-2142-8/04 $ 20.00 IEEE a b c Fig 1. A data matrix and latent associations discovered by PCA and PSNMF  The above form means that customers who buys the items will spend 1v , 2v  respectively on each itemset Fig1.\(a organized with N customers and M \( 2 Here we assume that the dataset is consisted with two clusters. Our goal is to capture the associations between items. We list two Ratio Rules discovered by PCA[4] in Fig.1 \(b 0.77 : 0.64bread butter Obviously, the negative association between items bread and butter it is obvious that the Ratio Rules deviate with the latent associations behind the distribution of these points In fact, from Fig 1.\(a associations are not mutually orthogonal, while the method by PCA imposes the orthogonality constraint on these ratio rules. Therefore, Ratio Rules based on PCA 


cannot truly reflect the latent associations among the items correctly. Compared to Fig.1 \(b c Ratio Rules captured by our proposed PSNMF Surprisingly, each rule could be treated as an association in the two clusters respectively 3. Principal Sparse Non-Negative Matrix Factorization \(PSNMF Given a M N  non-negative matrixV , denote a set of P M basis components by a M P  matrixW , where each transaction \(column vector linear combination of the basis components using the approximate factorization 1 where H is a P N  coefficients matrix 3.1 Non-negative Matrix Factorization\(NMF Because the entries of W and H calculated by PCA may contain negative values, NMF [5] is proposed as a procedure for matrix factorization which imposes nonnegative instead of orthogonal constraint, and NMF uses the I-divergence of V fromY , which is defined as   log 2 i j ij v D V Y v v y y  As the measurement of fitness for factorizing V into ijWH Y Y= , a NMF factorization is defined as  min 3 i D V WH s t W H w j The above optimization can be done by using multiplicative update rules [5 3.2 Sparse Non-negative Matrix Factorization SNMF Although NMF is successful in Matrix Factorization the NMF model does not impose the sparse constraints Therefore, it can hardly yield a factorization, which reveals local sparse features in the dataV . Related sparse coding is proposed in the work of [6] for matrix factorization Inspired by the original NMF and sparse coding, the aim of our work is to propose Sparse Non-negative Matrix Factorization \(SNMF negative constraint. Therefore, we put forward the following constrained divergence as objective function  1 


 1 2 3  log 4  ij ij ij ij j i j jij T j j j j pj v D V Y v v y l y l h h h h denotes the column of H    where [ ]ijWH Y Y= , and ? obtained by experience was assumed a positive constant. As the measurement of fitness for factorizing V into [ ]ijWH Y Y= , a SNMF factorization is defined as  1 min 5 0 0, 1 W H ij ij i D V WH s t i j W H and i w 0 20 40 60 80 100 0 20 40 60 80 100 spend on bread s pe nd on bu tte r 20 0 20 40 60 80 100 20 0 20 40 60 80 


100 0.64290.76595 spend on bread s pe nd on bu tte r 0.765950.6429 0 20 40 60 80 100 0 20 40 60 80 100 0.63907 0.36093 0.20718 0.79282 spend on bread s pe nd on bu tte r Proceedings of the Fourth IEEE International Conference on Data Mining ICDM04 0-7695-2142-8/04 $ 20.00 IEEE Notice that we have chosen to measure sparseness by a linear activation penalty \(i.e. minimum the 1-norm of the column of H minimization can be found by the following update rules   6 ik kl kl il iki i ik klk w h h v w w h  7 j kl ljl h w w v h w h  


To make the solution unique, we further require that the 1normal of the column vector in matrix W is one. In addition, matrix H needs to be adjusted accordingly 8 9 It is proved that the objective function is non-increasing under the above iterative updating rules, and the convergence of the iteration is guaranteed \(in Appendix 3.3 Principal SNMF When the dataset V is decomposed with W and H each column value of H represents the corresponding projection on the basis space W . As a whole, the sum of every row vector of H represents the importance of corresponding base. Therefore, we define a support measurement after normalizing every column of H 10 Definition. For every rule \(column vector define a support measurement  11 j ij support w h h Consequently, we can measure the importance of each rule for the entire dataset by their support values. The more value of support implies the more importance of such rule for the whole dataset In order to select the principal k rules as Ratio Rules firstly, we rank the whole rules in descending by the support value. And then, retain the first k principal rules as Ratio Rules because they are more important than others. About the selection of k value, a simple method is taken such as 1 1  min \(12  k ii Mk ii support w threshold support w       


From above \(12 according that the sum of k support values of rules cover threshold \(i.e.90 4. Experiments Synthetic dataset We have applied both the PSNMF and the PCA to a dataset that consists of two clusters, which contains 25 Gaussian distribution points on x-y plain \(generated with mu=[3;5], sigma=[1,1.2;1.2,2 Fig2 Generated with mu=[3;5], sigma=[2,1.6;1.6,2 0 10 20 30 0 10 20 300 5 10 15 20 25 Y X Z Fig 2.Dataset with two clusters Table 1. Ratio Rules based on PSNMF and PCA PSNMF 1RR 2RR 3RR X Y Z   a b Table 1.\(a values. After ranking such rules, Ratio Rules are obtained since 1 2  1 2 0 : 0.493 : 0.507 \(0.6650 0.696 : 0.304 : 0 \(0.2885 rule X Y Z rule X Y Z   For example, 2/3 transactions \(the cluster with distribution on y-z plain 2rule . Therefore, the corresponding support value \(0.665 of 1rule does not contradict with intuition. Otherwise Table 1.\(b to explain the negative association obviously 


Real Dataset: NBA \( 459 11   This dataset comes from basketball statistics obtained from the 97-98 season, including minutes played Point per Game, Assist per Game, etc. The reason why we select this dataset is that it can give a intuitive meaning of such latent associations. Table 2 presents the first three Ratio Rules \( 1RR , 2RR , 3RR general knowledge of basketball, we conjecture the 1RR represent the agility of a player, which gives the ratio of Assists per Game and Steals, is 0.206:0.220 1:1? . It means that the average player who possess one time of assist per game will be also steal the ball one time, and so does 2 \(0.117 : 0.263 1: 2.25 cannot give such information behind the dataset PCA 1RR 2RR X Y Z Proceedings of the Fourth IEEE International Conference on Data Mining ICDM04 0-7695-2142-8/04 $ 20.00 IEEE Table 2. Ratio Rules by PSNMF from NBA field 1RR 2RR 3RR Games 0.450 Minute 0.013 Points Per Game 0.010 Rebound Per Game 0.117 Assists per Game 0.206 Steals 0.220 Fouls 0.263 3Points 5. Conclusion In this work, we proposed Principal Sparse NonNegative Matrix Factorization \(PSNMF sparse non-negative components in matrix factorization. It aims to learn latent components which are called Ratio Rules. Experimental results illustrate that our Ratio Rules are more suited for representing associations between items than that by PCA References 1] Agrawal, R., Imielinski, T. and Swami, A.N., Mining association rules between sets of items in large databases In the Proc. of the ACM SIGMOD, \(1993 2] Aumann, Y. and Lindell, Y., A statistical theory for quantitative association rules. In the Proc. KDD, \(1999 3] Han, J. and Fu, Y., Discovery of Multiple-Level Association Rules from Large Databases. In Proc. of the 


VLDB, \(1995 4] Korn, F., Labrinidis, A., Kotidis, Y. and Faloutsos, C Ratio rules: A new paradigm for fast, quantifiable data mining. In the Proc. of the VLDB, \(1998 5] Lee, D.D. and Seung, H.S., Algorithms for nonnegative matrix factorization. In Proc. of the Advances in Neural Information Processing Systems 13, \(2001 6] Olshausen, B.A. and Field, D.J. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature 381\(1996 7] Srikant, r. and Agrawal, R., Mining quantitative association rules in large relational tables. In Proc. of the ACM SIGMOD \(1996 Appendix To prove the convergence of the leaning algorithm 6 7  objective function     multiplicative update rule corresponds to setting ,at each iteration ,the new state vector to the values that minimize the auxiliary function 1  13  Then the objective function Z is updated using \(13 1 1     Updating H : with W fixed, H is updated by minimizing  constructed for        ik kj ik kj ij ij ij ik kj i j i j k ik kj ik kjk k ij ij ij i j i j i j w h w h G H H v v v w h w h w h y v h      


   Since it is easy to verify 1  j ij j i j l h=  , therefore it is not difficult to testify     function, the following holds for all i ,j and 1ijkk     log log  thus log log log ik kj ik kj ik kj ijk ijkk k ijk ik kjk ik kj ik kj ik kj ik kjk k ik kj ik kjk k w h w h w h where w h w h w h w h w h w h w h           Thus  To minimize 1  14  for all kl    


0ik kji l i ki i kl ik kj klk w hG H H v b h w h h   Solving for H , this gives     ik kl kl il iki i ik klk w h h v w w h  which is the desired updated H Updating W : with H fixed, W is updated by minimizing         ik ik ik ik kj kj ij ij ij ik kj i j i j k kj kjk k ij ij ij i j i j i j w h w h G W W v v v w h w h w h y v h         It is easily to prove    likewise. we can get 


  lj kl kl kj ljj j kl ljk h w w v h w h  This completes the proof Proceedings of the Fourth IEEE International Conference on Data Mining ICDM04 0-7695-2142-8/04 $ 20.00 IEEE 


available, CPU utilization drops quickly due to the frequent swapping. Compared with them, MSPS and GSP process the customer sequences one by one, hence only a small memory space is needed to buffer the customer sequences being processed. MSPS can also handle the situation that LDBk or C DB k can not be totally loaded into memory by using the signatures as explained in Section 4. Therefore MSPS does not require the memory space as much as GSP SPADE and SPAM Many real-life customer market-basket databases have tens of thousands of items and millions of customers, so we evaluated the scalability of the mining algorithms in these two aspects. First, we started with a very small database D1K-C10-T5-S10-I2.5 and changed the number of items from 500 to 10,000. The user-speci?ed minsup was 0.5 To run MSPS on such a small database with only 1000 customers, we selected the whole database as the sample and keep the user-speci?ed minsup unchanged to mine it Since MSPS does not apply the sampling on such a small database, supersequence frequency based pruning is not performed in mining. Thus, in this case, SPADE and SPAM performed better than MSPS and GSP as long as their memory requirement is satis?ed As the number of items is increased, SPAM shows its scalability problem. Theoretically, the memory space required to store the whole database into bitmaps in SPAM is D ? C ? N/8 bytes. For the id-lists in SPADE, it is about D ?C ? T ? 4 bytes. But we found these values are usually far less from their peak memory space requirement during the mining, because the amount of intermediate data in both algorithms is quite huge Compared with SPAM, SPADE divides search space into small pieces so that only the id-lists being processed need to be loaded into memory. Another advantage of SPADE is that the id-lists become shorter and shorter with the progress in mining, whereas the length of the bitmaps does not change in SPAM. These two differences make SPADE much more space-ef?cient than SPAM Second, we investigated how they perform on C10-T5S10-I2.5-N10K when the user-speci?ed minsup is 0.18 We ?xed the number of items as 10,000 and increased the number of customers from 400,000 to 2,000,000. SPAM cannot perform the mining due to the memory problem For SPADE, we partitioned the test database into multiple chunks for better performance when its size was increased Otherwise, the counting of CDB2 for a large database could be extremely time-consuming. We made each chunk contain 400,000 customers so that it is only about 100 Mbytes which is one tenth of our main memory size. Figure 3 shows that the scalability of MSPS and GSP are quite linear. As the database size is increased, MSPS performs much better than the others When database is relatively small with only 400,000 customers, SPADE performed the best, about 20% faster than MSPS. But SPADE cannot maintain a reasonable scalability as the database becomes larger, and MSPS starts outperforming SPADE. When the database size is increased from 1600K customers to 2000K customers, there is a sharp performance drop in SPADE, such that it is even slower than GSP. In that case, MSPS is faster than SPADE by a factor of about 8. As discussed before, counting CDB2 is a performance bottleneck for SPADE, because the transformation of a large database from the vertical format to the horizontal format takes too much time. When the database is very large, the transformation also requires a large amount of memory and frequent swapping, hence the performance drops drastically. Partitioning the database can relieve this problem to some extent but does not solve it completely. In addition, for the database with a large number of items and customers, SPADE needs more time to intersect more and 


customers, SPADE needs more time to intersect more and longer id-lists Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 400 800 1200 1600 2000 Number of Customers \('000s Ex ecu tion Ti me se c GSP SPADE MSPS Figure 3. Scalability: Number of Customers on C10-T5-S10-I2.5-N10K, minsup=0.18 Finally, we mined a large database D2000K-C10-T5S10-I2.5-N10K, which takes about 500 Mbytes, for various minsups. This database is partitioned into 5 chunks for SPADE, and the results are shown in Figure 4 Based on our tests, we found SPADE performs best for small size databases. For medium size databases, MSPS performs better for relatively big minsups while SPADE is faster for small minsups. When database is large SPADE  s performance drops drastically and MSPS outperforms SPADE very much. If the user-speci?ed minsup is big and there are very few long patterns, GSP may perform as well as, or even better than, others due to its simplicity and effective subsequence infrequency based pruning 100 1000 10000 100000 0.33 0.3 0.25 0.2 0.18 Minimum Support Ex ec uti on Ti me se c GSP SPADE MSPS Figure 4. Performance on a Large Database D2000K-C10-T5-S10-I2.5-N10K 6 Conclusions In this paper, we proposed a new algorithm MSPS for mining maximal frequent sequences using sampling. MSPS combined the subsequence infrequency based pruning and the supersequence frequency based pruning together to reduce the search space. In MSPS, a sampling technique is used to identify potential long frequent patterns early. When the user-speci?ed minsup is small, we proposed how to adjust it to a little bigger value for mining the sample to avoid many overestimates. This method makes the sampling technique more ef?cient in practice for sequence mining. Both the supersequence frequency based pruning and the customer sequence trimming used in MSPS improve the candidate counting process on the new pre?x tree structure developed. Our extensive experiments proved that MSPS is a practical and ef?cient algorithm. Its excellent scalability 


makes it a very good candidate for mining customer marketbasket databases which usually have tens of thousands of items and millions of customer sequences References 1] R. Agrawal and R. Srikant  Fast Algorithms for Mining Association Rules  Proc. of the 20th VLDB Conf., 1994, pp 487  499 2] R. Agrawal and R. Srikant  Mining Sequential Patterns  Proc. of Int  l Conf. on Data Engineering, 1995, pp. 3  14 3] J. Ayres, J. Gehrke, T. Yiu, and J. Flannick  Sequential Pattern Mining Using a Bitmap Representation  Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining 2002, pp. 429  435 4] B. Chen, P. Haas, and P. Scheuermann  A New TwoPhase Sampling Based Algorithm for Discovering Association Rules  Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining, 2002, pp. 462  468 5] S. M. Chung and C. Luo  Ef?cient Mining of Maximal Frequent Itemsets from Databases on a Cluster of Workstations  to appear in IEEE Transactions on Parallel and Distributed Systems 6] F. Masseglia, F. Cathala, and P. Poncelet  The PSP Approach for Mining Sequential Patterns  Proc. of European Symp. on Principle of Data Mining and Knowledge Discovery, 1998, pp. 176  184 7] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U Dayal, and M. C. Hsu  Pre?xSpan: Mining Sequential Patterns Ef?ciently by Pre?x-Projected Pattern Growth  Proc of Int  l. Conf. on Data Engineering, 2001, pp. 215  224 8] R. Srikant and R. Agrawal  Mining Sequential Patterns Generalizations and Performance Improvements  Proc. of the 5th Int  l Conf. on Extending Database Technology, 1996 pp. 3  17 9] H. Toivonen  Sampling Large Databases for Association Rules  Proc. of the 22nd VLDB Conf., 1996, pp. 134  145 10] M. J. Zaki, S. Parthasarathy, W. Li, and M. Ogihara  Evaluation of Sampling for Data Mining of Association Rules  Proc. of the 7th Int  l Workshop on Research Issues in Data Engineering, 1997 11] M. J. Zaki  SPADE: An Ef?cient Algorithm for Mining Frequent Sequences  Machine Learning, 42\(1  60 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


