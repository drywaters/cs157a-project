Sliding-Window Filtering with Constr aints of Compactness and Recency in Incremental Database   Jiadong Ren College of Information Science and Engineering YanShan University Qinhuangdao 066004 P.R.China jdren@ysu.edu.cn   Haiyan Tian College of Information Science and Engineering YanShan University Qinhuangdao 066004 P.R.China haiyan7155012@sohu.com   Shiyong Lv College of Information Science and Engineering YanShan University Qinhuangdao 066004 P.R.China Shiyong_lv@yahoo.com.cn   Abstract   
In true-life the database is changed continually in many applications. Incremental mining technique has been developed to avoid rescanning database for knowledge discovery. Recent and compact constraints also are developed for fre quent patterns mining. We store the database with a time-vertical bitmap representation, therefore the supports of frequent pattern and recent pattern can be computed fast. Link and bitmap are adopted, so a mass of running time can be saved during incremental mining process. Besides to mine more efficiently in the incremental database two concepts of recency and compactness are introduced into sliding-window filtering \(denoted as 
SWF\. In essence, an incremental database is divided into several partitions, and a filtering threshold is employed in each partition to handle candidate itemsets generation under constraints of recency and compactness. By employing SWF with constraints of compactness and recency, user satisfactory CFRpatterns \(compactness, frequency and recency\ can be discovered. Experimental result shows that the running time can be reduced  Keyword sliding-window filtering, constraint incremental mining  1 
 Introduction  Sequential pattern was first proposed by R.Agrawal as an important KDD research branch  Main task of    This work is supported by the Natural Science Foundation of Hebei Province P.R.China, NO. F2008000888  sequential pattern mining is to discover the temporal patterns and the sequential patterns. Some research works which discover frequent sequential pattern have 
been done in incremental database. Algorithm FUP 2  was proposed to update the association rules when new transactions are added to the database by D.Cheung and J.Han. Algorithm FUP is based on the framework of Apriori and designed to discover the new frequent itemsets iteratively. Algorithm FUP 2   extended from Algorithm FUP was proposed by Cheung D W and Lee S D. The existing association rules are updated when transactions are added and deleted from the database 
FUP 2 is equivalent to FUP in the case of insertion however, a complementary algorithm of FUP in the case of deletion. FUP 2 avoids re-running the association rule mining algorithm on the whole updated database. Another FUP-based algorithm, call FUP 2 H, was also designed to utilize the hash technique for performance improvement. But a new frequent itemset L k is generated in the data subsets, k scans of the database are needed by FUP-based algorithms. So 
an algorithm based on sliding-window filtering denoted as SWF 4 for incremental mining of association rules was proposed by Chang-Hung Lee and Cheng-Ru. Essentially, by partitioning a transaction database into several partitions, algorithm SWF employs a filtering threshold in each partition to deal with the candidate ite msets generation.  SWF can reduce the amount of candidate itemsets efficiently Owing to generating the small number of candidate 
itemsets, the scan reduction technique 5 can be applied efficiently Various constraints of user-specific to improve mining efficiency were proposed. Sliding window maxgap, mingap and maxspan called universal constraints 6 were employed in mining sequence patterns. After time attribute was introduced into 
Fourth International Conference on Networked Computing and Advanced Information Management 978-0-7695-3322-3/08 $25.00 © 2008 IEEE DOI 10.1109/NCM.2008.78 665 


transactions, concepts of compactness and recency 7  were cited under CFR-postfixspan algorithm for sequential pattern mining by Yen-Liang Chen and YaHan Hu, but they only were adapted to the fixed database. When the new sequence was added or the old sequence was deleted, the database will be rescanned How to represent sequential pattern is a key problem in mining frequent patterns. Bitmap based binary format was employed by Josh-ua Ho. The most powerful feature of SPAM is bitmap data structure for counting sequence supports 8 Then semi-vertical bitmap representation was presented in SPADE which was a vertical database by  Sujeevan Aseervatham et al 9 The speed of counting supports was improved by using  semi-vertical bitmap representation Many candidate itemsets were generated by SWF algorithm, but they weren’t demanded by user. In this paper compact and recent constraints will be introduced into SWF in incremental mining. So the compact frequent recent itemsets can be obtained by user-specific. And the time-vertical bitmap which employs binary format will be developed for the database representing. The running time can be reduced for counting compact and compact recent supports  The rest of the paper is organized as follows: the section 2 discusses the construction of time-vertical bitmap. Section 3 describes algorithm CR-SWF by an example. Section 4 shows the experimental result Finally, a conclusion is given in Section 5  2. Construction of time-vertical bitmap  Let S=<\(b 1 t 1 b 2 t 2 b n t n be a sequence with time attribute. A data sequence is sorted first by time, then by alphabet. Let I I 1 t I1  I 2 t I2 I s  t Is ubsequence of S. Let m_span be the userspecified maximum span and t R be the user-specified minimum recent threshold P is a compact frequent pattern\(CF-pattern\f t Is t I1 m_span and P s support is no less than a predefined minimum frequent support threshold, denoted as F  P  is a compact recent pattern \(CR-pattern\f t Is t I1  m_span and P s recent support is no less than a predefined minimum recent support threshold, denoted as R If P is CF-pattern and CR-pattern, then P is CFR pattern \(compactness, frequency and recency  2.1. Construction  A time-vertical bitmap is defined as follows It consists of a header table and item tables The header table has two fields: p_partition and p_link: p_partition records the number of items partition and p_link is linked the item of sequence The item table has five fields: s_item, s_sid, s_bit s_time, s_link: s_item records the name of the item s_sid is the identifier of sequence. The length of bit is equivalent the sequence’s, the bit of item b i is set to one, the rest are set to zero. s_time records timestamps of item b i All items of the same s_sid are linked in s_link Based on the definition, construction algorithm of time-vertical bitmap is as follows Algorithm 1: Time-vertical bitmap construction Input: sequence database DB of partitions n Output: time-vertical bitmap TVB Method 1. Create the header table: the p_link is labeled as null 2. Scan the database, create the item table of the first item in sequence database and the item table is linked behind the header table 3. for \(i=1;i<=n;i 4. Call insert_function\(TVB,P i  5. end insert_function\(TVB,P i  1.P is the pointer of pointing first item in TVB 2.For each item b j P i  3.{While\(P!=null 4. {If item b j P.s_item then 5. {Item b j s detail information is recorded in item table of item b j  6.   P points the first item of TVB 7.      j 8.  Else if item b j P.s_item then 9.         P 10.     Else { create item table of item b j  11.         item b j is inserted before P.s_item 12.         P points the first item of TVB 13.           j++;}} end while 14.  create item table of item b j  15.  item b j is inserted behind all items 16.  P points the first item of TVB 17.     j 18. end All items of the database are linked in the timevertical bitmap by the dynamic link. The dynamic link is much more convenient than the  ordinal table in the case of insertion or deletion. The running time can be reduced for counting compact and compact recent supports. So the total running time can be reduced effectively  Table1 A sequence database  partition sid sequence P1 1 <\(a,1\,\(c,3\,\(d,6\,\(e,10\,\(f,10 
666 


b,15 2 <\(b,7\,\(d,9\,\(f,14 3 <\(a,10\,\(e,13\,\(d,17 P2 4 <\(a,5\,\(b,10\,\(f,10\,\(d,13\,\(e,1 5 5 <\(a,15\,\(b,17\,\(c,18\,\(f,20\,\(e 22 6 <\(b,11\\(f,19 P3 7 <\(a,14\,\(d,16\,\(e,17\,\(f,20 8 <\(a,10\,\(b,13\,\(d,14\,\(f,16 9 <\(a,7\,\(f,9\,\(d,13  The partition P2 of table 1 is shown in Figure1. The number of partition is 2. For item a s_item, s_sid s_bit, s_time are a 4 and 5, {10000} and {10000}, 5 and 15 respectively. So do the other items. All items of the sequence 4 are linked in s_link. Owing to being restricted by space, the part of time-vertical bitmap is given  p_partition p_link 2  s_item s_sid s_bit s_time s_link a 4 10000 5 5 10000 15   s_item s_sid s_bit s_time s_link b 4 01000 10 5 01000 17  6 10 11   s_item s_sid s_bit s_time s_link c 5 00100 18  s_item s_sid s_bit s_time s_link d 4 00010 13    s_item s_sid s_bit s_time s_link e 4 00001 15 5 00001 22   s_item s_sid s_bit s_time s_link f 4 00100 10 5 00010 20  6 01 19 null Figure 1 Data storage representation   2.2. An example  Give a time-vertical bitmap of database DB \(shown as figure 1\or counting compact support value, iff t m t 1 m_span s_sid of items are same\, the count is added by 1. For counting compact recent support value, iff I is a compact subsequence in S t m t R    the count is added by 1 Consider the partition P2 of database shown in figure 1, t R 11 and m_span=9. The compact support b\ 2 \(in the data sequences 4, and 5\hile the compact recent support b\ 1 \(in the sequence 5 b, 10\ontained in the sequence 4 cannot be counted, because the timestamp of item b is smaller than t R   3. Sliding-Windows Filtering with compactness and recency \(CR-SWF  By dividing the database into n partitions Algorithm CR-SWF processes orderly each partition under considering the co mpact constraint. Then compact frequent and compact recent filtering thresholds are used in each partition to deal with candidate itemsets  3.1. Processing original database procedure  Algorithm 2: processing original database Input: original database with time-vertical bitmap Output: compact frequent recent large k-itemsets 1. n = the number of partitions, CI   1 n k 1 n k db P  2.begin for k = 1 to n  // 1st scan of db 1,n  3.  begin for each 2-itemset I P k  4.   if \( I CI 5.     f.count=n_cf\(I\nt=n_cr\(I 6.     I.start=k 7.    if \( f.count k F P  r.count k R|P  8.      CI = CI I 9.   if \( I CI 10.     f.count + =n_cf\(I\nt + =n_cr\(I 11.    if \( f.count m=I.start,k m FP P  r.count m=I.start,k m R*P P   12.   CI = CI I 13.  end 14.end 15. keep C 1,n 2 in main memory 16. scan reduction technique 5  17. f.count =0, r.count=0 where I C 1,n h h 2 18.begin for k = 1 to n   //2nd scan of db 1,n  19.   for each itemset I C 1,n h  20.    f.count + =n_cf\(I\nt + =n_cr\(I 21. end 22. for each itemset I C 1,n h  23. if \( f.count 1,n F|db  r.count  1,n R|db   24.   L h L h  I 25. end In algorithm 2, each partition is processed under compact and recent constrains from Step 2 to Step 12  C stands for candidate itemset L stands for  large itemset. n_cf\(I\d n_cr\(I\e number of compact itemset I c and compact recent itemset I cr in P k   
667 


respectively. The total number of each compact itemset I c and compact recent itemset I cr are recorded in f.count and r.count respectively. And the starting partition of itemset I is recorded in I.start. Itemsets which meet constraints and support thresholds will be kept in CI After the scan reduction technique is employed, C 1,n h s h 3\aved in CI. By the last scan of database compact frequent recent itemsets are generated Consider the  original database shown in table 1 First, the database is divided into three partitions P 1 P 2  and P 3 The minimum support thresholds of frequency and recency F and R are set to 0.4. The maximum compact span is set to 9 \(m_span=9\d the minimum recent time is set to 10 \( t R 10 During partition P 1 is handled, each candidate 2itemset whose f.count 2 0.4*3 d r.count 2 0.4*3  will be save into CI. Similarly, after scanning partition P2 the support thresholds of itemsets which generated from the P 1 are 3 3 3 0.4 he support thresholds of new itemsets which generated from P2 are 2 0.4*3 fter the P3 is processed orderly, the support thresholds of itemsets which generated from P1 are 4 3 3 3 0.4 he generated 2-itemsets are shown in table 3. Finally, after scanning the database from P1 to P3, seven 2-itemsets of those eight candidate 2-itemsets meet the constraints and the support thresholds. The large 2-itemsets are {ad,ae,af bf, de, df, ef, }. Large k-itemsets are shown under the table 3  Table 2  Candidate 2-itemsets after scanning P 1  C2 start f.count r.count a,e 1 2 2 b,f 1 2 2 d,e 1 2 2 d,f 1 2 2  Table 3  Candidate 2-itemsets generated after orderly scanning partitions P 2 and P 3 C2 start f.count r.count a,d 3 3 3 a,e 1 4 4 a,f 2 5 4 a,b 2 3 3 d,e 1 4 4 d,f 1 6 6 e,f 2 3 3 b,f 1 6 6  Compact recent large k-itemsets in original database are{a},{b},{d},{e},{f},{a,d a,e},{af},{bf},{de},{df ef}, {adf},{bdf Table 4  Sequences which are added  partition sid sequence P4 10 <\(a,6\,\(f,13 11 <\(c,5\,\(f,11\,\(b,19 12 <\(f,3\,\(a,7\,\(c,10 P5 13 <\(b,12\,\(d,15 14 <\(d,4\,\(b,10\\(f,13\\(c,20 15 <\(b,6\,\(c,10\,\(e,11\,\(f,14  3.2. Processing incremental database procedure  Algorithm 3: processing incremental database Input: incremental database with time-vertical bitmap Output: compact frequent recent large k-itemsets 1.Original database = db m,n  k=m,n P k   2.New database = db i,j  k=i,j P k   3.Database removed  k=m,i 1 P k  4.Database added   k=n+1,j P k  5.db i,j db m,n      6.loading C m,n 2 of db m,n into CI where I C m,n 2  7.begin for k = m to i 1 // one scan of  8.  begin for each 2-itemset I P k  9.    if \( I CI && I.start k 10.     f.count n_cf\(I\r.count n_cr\(I 11.      I.start = k + 1 12.    if \( f.count m=I.start,n m F |P  r.count m=I start n m S|P  13.     CF = CF I 14.   end 15. end 16.  Call Algorithm 2 17.  end In the algorithm 3, the total number of compact itemset I c and compact recent itemset I cr are subtracted respectively in deleting partitions. Itemsets which don’t meet constraints or support thresholds are removed Then the algorithm 2 is called for adding partitions. By some updating operations, the partition P1 is removed from the original database db 1,3 and new partitions P 4  P 5 are added. The result is shown in table 5 and table 6  Table 5 Compact frequent recent 2-itemsets after deleting partition P 1 C2 start f.count r.count a,d 3 3 3 a,f 2 5 4 a,b 2 3 3 d,f 2 4 4 e,f 2 3 3 b,f 2 4 4  Table 6 Compact frequent recent 2-itemsets after adding partitions P 4 and P 5 C2 start f.count r.count a,f 2 7 5 b,d 5 2 2 b,f 2 7 7 c,f 4 4 4 
668 


d,f 2 5 5 Large k-itemsets of compactness and recency after scanning incremental database are {b},{d}, {f}, {af bf},{cf},{df  4. Experimental result   The experiment was performed on Intel Core\(TM\2 2.2GHz with 2GB memory, running on Windows XP. We use synthetic data to form database We use three datasets,T.I.D100K.C10%, T.I.D100K C30%, T.I.D 100K.C50% in the experiment where T is the mean size of a transaction, I is the mean size of potential maximal large itemsets, D is the number of transactions in units of K, and C is the correlation between items in terms of percentage. To conduct expediently the experiment, recent minimum support and frequent minimum support are set to the same value  From the figure 2, the number of candidate itemsets is increasing along with the support thresholds reducing gradually. Apparently, CR-SWF outperforms SWF under different support thresholds  Because the inputted data is stored with time-vertical bitmap, and the compact and recent constraints are employed in CR-SWF, a lot of dissatisfactory candidate itemsets can be avoided generating. So the running time can be saved      Figure 2  The performance comparison between CR-SWF with time-vertical bitmap and SWF   5. Conclusions   In this paper, the inputted data are represented with time-vertical bitmap in the incremental mining algorithm. The time-vertical bitmap employs link and bitmap. Link is  beneficial to handle data in the case of insertion or deletion. And the running time of computing frequent support and recent support can be reduced effectively. By employing constraints of recency and compactness, large numbers of dissatisfactory candidate itemsets can be avoided generating. The experiment result shows that algorithm CR-SWF outperforms algorithm SWF when the database is mined incrementally. The size of produced candidate sequential base can be much smaller than traditional mining algorithm along with supports reducing, so the time spending is much less  6. References  1 R A g ra w a l a nd R Srik a n t M ini n g s e que nt ia l pa tte r n s    In Proc.11thICDE Tai Pei, China, 1995, pp.3-14  Ch eun g D W an d J Han el a1  M ai nt en an ce o f  discovered association rules in large databases: An incremental updating approach The 12th IEEE International Conference on Data Engineering 1996 pp.106-114 3 D  C h e ung S.D  L e e  and B  K a o A  G e ne r a l  Incremental Technique for Maintaining Discovered Association Rules Proceedings of the Fifth International Conference on Database Systems for Advanced Applications Melbourne, Australia, 1997 pp.185-194 4 C h a n g H ung L e e  C h e n g R u L i n a n d Mi ng S y a n C h e n   Sliding-window filtering: An Efficient Algorithm for Incremental Mining  Proceedings of the tenth international conference on Information and knowledge management Atlanta, Georgia, USA, 2001,pp. 263-270 5 J  S  P a r k  M S C h e n a n d P  S Y u  U s i n g a H a s h Based Method with Transaction Trimming for Mining Association Rules IEEE Transactions on Knowledge and Data Engineering 1997, 9\(5\:813- 825  Ji an hu a L i and Xi ao f e n g W a n g  M i n i n g S e qu en ce  Patterns with Universal Temporal Constraints MNIMICRO SYSTEMS 2005, 26\(6\: 1004-1009  Yen L i an g Ch en an d Ya-Han Hu  Con st rai n t b a sed sequential pattern mining: The consideration of recency and compactness Decision Support Systems 2006 42\(2\: 1203-1215 8 J o s h ua H o L i or L u k o v  a nd Sa nja y C h aw la  S e que ntia l  Pattern Mining with Constraints on Large Protein Databases International Conference on management of Data COMAD 2005b Hyderabad,India,2005   j eev an As eerv a t h a m  A o m a r Os m a n i a n d E m  manuel Viennet, “bitSPADE: A Lattice-Based Sequential Pattern Mining Algorithm Using Bitmap Representation  Proceedings of the Sixth International Conference on Data Mining ICDM'06 2006, pp. 792-797   
669 


Finally, shows the extended association rules IV EXPERIMENTS For our experiments we used SQL Server2000 relational DBMS running on a 1.50GHz Pentium M with 504MB RAB The data we used is from the Northwind database of the SQL Server2000 and the relational structure is shown in Fig. 1 The sizes of the tables are as follows The Order Details table has 2155 tuples The Order table has 830 tuples The Customers table has 91 tuples We select 77 kinds of products as the chief attributes to mine the antecedent rules and select Quantity from Order Details table, OrderDate from Order table and ContactTitle and Country from Customers table as the relational attributes to mine the extended consequent rules The minimum support is set 0.5% and the extended minimum confidence is set 40 We will find these antecedent rules containing at least two products Thus, the antecedent rules mined by using our GA will answer the following question What products are frequently ordered together The extended consequent rules mined by using apriori algorithm and statistic method can answer these additional questions as follows How much quantities, what the possible order date is and how the customers are about these products frequently ordered together Figure 6 shows the generations and the number of rules during the GA mining process Table I shows the experimental results after incorporating and extracting the rules containing at least two products Table II shows fourteen extended association rules In the proposed method, the GA found most anteceden t rules. The matrix grouping method will extract anteceden t TABLE I T HE EXPERIMENTAL RESULT OF RULES MINED BY THE GA Quantity Accuracy of the support Proportion of total rules  Antecedent rules 14 92.86 77.78 TABLE II T HE EXTENDED ASSOCIATION RULES Antecedent rules Extended consequent rules Product21 Product51 0.60 Quantity21=81,Quantity51=66 Orderdate=\(1997\(60 80 Product16 Product31 0.84 Quantity16=243,Quantity31=182 Orderdate=\(1997\(42.86%\,1998\(42.86 42.86 Product16 Product60 0.72 Quantity16=196,Quantity60=280 Orderdate=\(1997\(50 Product31 Product51 0.60 Quantity31=186,Quantity51=225 Orderdate=\(1997\(80 Customers  Sales Associate, Ireland\\(40%\ \(Sales 40 Product21 Product61 0.96 Quantity21=176,Quantity61=156 Orderdate=\(1997\(62.5 50 Product16 Product62 0.72 Quantity16=120,Quantity62=89 Orderdate=\(1997\(66.67 50 Product56 Product65 0.60 Quantity56=197,Quantity65=101 40 60 Product31 Product72 0.72 Quantity31=194,Quantity72=199 Orderdate=\(1996\(50 Product60 Product71 0.72 Quantity60=235,Quantity71=197 50 Product10 Product77 0.60 Quantity10=130,Quantity77=132 Orderdate=\(1998\(80 Customers=\(\(Accounting Manager\\(40 Product30 Product54 0.72 Quantity30=67,Quantity54=120 Orderdate=\(1997\(66.67 Product17 Product33 0.60 Quantity17=158,Quantity33=86 Orderdate=\(1998\(60 40 40 Product55 Product62 0.60 Quantity55=135,Quantity62=61 40 40 Product59 Product76 0.60 Quantity59=131,Quantity76=80 Orderdate=\(1997\(80 40  40          Fig. 6. The number of rules and generations 2008 IEEE Congress on Evolutionary Computation CEC 2008 749 


rules through producing C 830 5 groups while our GA just uses 23968 groups to produce rules. Finally, the extracted extended association rules satisfy the condition of importance and have the qualification for commercial decision. Therefore we need not store retrieve, prune and sort a large number of rules for classification as traditional method V C ONCLUSIONS In this paper we introduce a new data mining method to mine rules from multi-relational databases Our approach has two novel points: One is the ideas of distributed mining by using GA and traditional apriori mining algorithm. Another is the definition of the new extended association rule for supporting the commercial decision better. In our experiment the GA based method is more efficient on the antecedent rules mining than the matrix method and the distributed mining method is also different with other multi-relational data mining methods that need query-processing using SQL language The extended association rule has finer pattern and contains more commercial information. Meanwhile, the number of rules is much less than the standard data mining method producing large candidate rules In the future research we will apply our method to the real world databases with large data and intend to combine the fuzzy logic and our method to predict data R EFERENCES  R. Agrawal and T  Im ielinski and A Swam i Mining Association Rules Between Sets of Items in Large Databases in Proc. of .ACM SIGMOD Conf., 1993, pp.207-216  R Agrawal and R. Srikant, Fast Algo r ithm f o r M in ing Association Rules,Ž in Proc. of .the 20 th VLDB Conf., 1994, pp.487-499  J Roberto and Jr  B a y ar d o a n d R  A g ra wa l Mining the M o s t Interesting Rules,Ž in Proc of the 5th ACM SIGKDD Conf. 1999 pp.145-154   Christia n Bo r g e lt and Rudolf Kr use  Introduction of As socia tion Rule s Apriori Implementation,Ž in Proc. of .the 15th Conf on Computational Statistics. 2002, pp.1  Sao Džeroski Multi-R elatio nal Data Mining: An Introduction in Proc. of .ACM SIGKDD Conf 2003, pp.1-16   Hendrik Blockeel an d Mich le Sebag, Scalability and ef f iciency in multi-relational data mining in Proc. of .ACM SIGKDD Conf 2003 pp.17-30  Svet lo z a r N es torov a n d Ne nad Jukic  A d-H o c A s socia tion-Rule Mining within the Data Warehouse,Ž in Proc. of .the 36th HICSS Conf 2002, pp.10  Hendrik Blockeel and Sao Džeros k i   M u ltiRelational Data Mining 2005: workshop report,Ž in Proc. of ACM SIGKDD Conf. 2005 pp.126-128   T. Fukud a Y. Morim oto S. M o r i shita and T. Tokuy am a  D ata Mining with Optimized Two-Dimensional Association Rules,Ž in Proc. of .the ACM TODS Conf. 2001, pp.179-219   Kaoru S him a da, Kotaro Hirasawa and Jinglu Hu  Class Association Rule Mining with Chi-Squared Test Using Genetic Network Programming in Proc of the Conf on Systems, Man, and Cybernetics 2006, pp.5338-5344   Ma nish Sagga r A s h i s h K u m a r A graw a l a nd A bhim any u L a d Optimization of Association Rule Mining using Improved Genetic Algorithms in Proc. of .the Conf on Systems, Man, and Cybernetics 2004, pp.3725-3729   S Muggle to n e d itor Inductive Logic Program m i ng  Academic Press London, 1992  S Ch a u dhri a n d U D a y a l An o v e r v ie w o f  D a t a  W a r e housi n g a n d OLAP Technology,Ž in Proc. ACM SIGMOD Conf 1997, pp. 65…74  Riyaz Sikora and Selwyn Piramuthu, Framework for efficient feature selection in genetic algorithm based data mining,Ž presented at the European Journal of Operational Research 2007 pp. 723…737  Ja naki Gopal an E r kan K o rkm az  Reda A l hajj and K e n B a rke r  Effective Data Mining by Integrating Genetic Algorithm into the Data Preprocessing Phase,Ž in Proc. of .the ICMAL Conf. 2005, pp.6  J.H Holland Adaptation in Natural and Artificial System,Ž Ann Arbor MI: Univ. Michigan Press, 1975  D.E Goldber g Genetic Algorithm Search, Optimization & Machine Learning Addison Wesley, 1989 750 2008 IEEE Congress on Evolutionary Computation CEC 2008 


TREC ONTO p-value Macro-FM 0.388 0.386 0.862 Micro-FM 0.356 0.355 0.896 MAP 0.290 0.284 0.484 Table 1 Other Experimental results downgrade For the average macroand microF 1 Measures also shown on Table 1 the TREC model only outperformed the ONTO model by 0.002 0.5 in macro F 1 and 0.001 0.2 in micro F 1  The two models achieved almost the same performance The evaluation result is promising The statistical test is also performed on the experimental results in order to analyze the evaluation's reliability As suggested by we use the Student's Paired T-Test for the signi\002cance test The null hypothesis in our T-Test is that no difference exists in two comparing models When two tests produce substantially low p-value usually  0.05 the null hypothesis can be rejected In contrast when two tests produce high p-value usually  0.1 there is not or just little practical difference between two models The T-Test results are also presented on Table 1 The pvalue s show that there is no evidence of signi\002cant difference between two experimental models as the produced pvalue s are quite high  p-value 0.484\(MAP 0.862\(macroFM and 0.896\(micro-FM far greater than 0.1 Thus we can conclude that in terms of statistics our proposed model has the same performance as the golden TREC model and the evaluation result is reliable The advantage of the TREC model is that the experimental topics and the training sets are generated by the same linguists manually They as users perfectly know their information needs and what they are looking for in the training sets Therefore it is reasonable that the TREC model performed better than the ONTO model as we cannot expect that a computational model could outperform a such perfect manual model However the knowledge contained in TREC model's training sets is well formed for human beings to understand but not for computers The contained knowledge is not mathematically formalized and speci\002ed The ONTO model on the other hand formally speci\002es the user background knowledge and the related semantic relations using the world knowledge base and local instance repositories The mathematic formalizations are ideal for computers to understand This leverages the performance of the ONTO model As a result as shown on Fig 2 and Table 1 the ONTO model achieved almost the same performance as that of the TREC model 6 Conclusions In this paper an ontology-based knowledge IR framework is proposed aiming to discover a user's background knowledge to improve IR performance The framework consists of a user's mental model a querying model a computer model and an ontology model A world knowledge base is used by the computer model to construct an ontology to simulate a user's mental model and the ontology is personalized by using the user's local instance repository The semantic relations of hypernym/hyponym holonym/meronym and synonym are speci\002ed in the ontology model The framework is successfully evaluated by comparing to a manual user model The ontology-based framework is a novel contribution to knowledge engineering and Web information retrieval References   C Buckley and E M Voorhees Evaluating evaluation measure stability In Proc of SIGIR 00  pages 33–40 2000   R M Colomb Information Spaces The Architecture of Cyberspace  Springer 2002   D Dou G Frishkoff J Rong R Frank A Malony and D Tucker Development of neuroelectromagnetic ontologies\(NEMO a framework for mining brainwave ontologies In Proc of KDD 07  pages 270–279 2007   S Gauch J Chaffee and A Pretschner Ontology-based personalized search and browsing Web Intelligence and Agent Systems  1\(3-4 2003   X Jiang and A.-H Tan Mining ontological knowledge from domain-speci\002c text documents In Proc of ICDM 05  pages 665–668 2005   J D King Y Li X Tao and R Nayak Mining World Knowledge for Analysis of Search Engine Content Web Intelligence and Agent Systems  5\(3 2007   D D Lewis Y Yang T G Rose and F Li RCV1 A new benchmark collection for text categorization research Journal of Machine Learning Research  5:361–397 2004   Y Li and N Zhong Mining Ontology for Automatically Acquiring Web User Information Needs IEEE Transactions on Knowledge and Data Engineering  18\(4 2006   H Liu and P Singh ConceptNet a practical commonsense reasoning toolkit BT Technology  22\(4 2004   A D Maedche Ontology Learning for the Semantic Web  Kluwer Academic Publisher 2002   S E Robertson and I Soboroff The TREC 2002 002ltering track report In Text REtrieval Conference  2002   M D Smucker J Allan and B Carterette A Comparison of Statistical Signi\002cance Tests for Information Retrieval Evaluation In Proc of CIKM'07  pages 623–632 2007   X Tao Y Li and R Nayak A knowledge retrieval model using ontology mining and user pro\002ling Integrated Computer-Aided Engineering  15\(4 2008   X Tao Y Li N Zhong and R Nayak Ontology mining for personalzied web information gathering In Proc of WI 07  pages 351–358 2007   T Tran P Cimiano S Rudolph and R Studer Ontologybased interpretation of keywords for semantic search In Proc of the 6th ICSW  pages 523–536 2007   Y Y Yao Y Zeng N Zhong and X Huang Knowedge retrieval KR In Proc of WI 07  pages 729–735 2007 
513 
517 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


