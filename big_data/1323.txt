html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Using  Association Rules for Completing Missing Data Chih-Hung Wuh       Chian-Huei Wun  Hung-Ju Choui hiDepartment of Electronic Engineering, National University of Kaohsiung, Kaohsiung, Taiwan  Department of Electronic Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan hjohnw@nuk.edu.tw  wendy@water.ee.nsysu.edu.tw     igemini.arashi@msa.hinet.net Abstract We present in this paper a new method for completing missing data using the concept of association rules. The basic idea is that association rules describe the dependency relationships among data entries in a dataset where all data, including the missing ones, should hold the similar relationships For a missing datum, we guess its possible value according to related association rules. A new completing procedure and a new evaluation function are developed and presented. The evaluation function is scored according to the support, confidence, and lift of association rules, which reasonably reflects the dependency relationships among existing and missing data. Experimental results show that our method is feasible in completing some incomplete datasets 1. Introduction With the explosive growth of data available from various sources, the issue on ensuring the quality of data becomes more and more important. Recently knowledge discovery from databases \(KDD   receives a lot of attentions, which tries to extract hidden but probably valuable information or knowledge from databases. Evidently, the mined results are useful and applicable only when the input data are of high quality. Unfortunately, most realworld databases contain  dirty  data [11]. Several types of  dirty  data are found in databases, such as incorrectness, duplication, inconsistency, and missing data [18]. This paper focuses on the cases when data are missing Missing data are introduced into databases because of the carelessness of human or the failure of machines in processing data. In a dataset, a normal record is complete if all its fields \(or attributes proper data. A missing datum indicates that a field of a record is empty. An incomplete case in a dataset is a record containing missing data. When KDD is performed on an incomplete dataset, two typical approaches are employed for handling missing data One approach is to develop robust algorithms which are tolerant of noisy or missing data. Usually, such algorithms can perform on limited types and amount of missing data and are costly to develop. The other approach is to try to improve the quality of input data and then applies existing KDD algorithms, which is more feasible There are two types of approaches being able to maintain the quality of data. One is to abandon or remove the whole incomplete case, such as list-wise deletion \(LD VD  LD and VD reduce the size of data samples for KDD and may lead to biased results. They usually remove critical data or variables and mislead the mining algorithms to improper conclusions. On the contrary, if all cases are needed, the missing data have to be fulfilled by some proper values before they are processed for KDD. There are several approaches being able to estimate the possible values for missing data [12]. Basically, they are all based on the missing information principle [15], i.e., the value for replacement is one of the existing data. Based on statistic or heuristic inference, these approaches usually produce satisfactory results. Clearly, if we can correctly guess what values are needed for completing 


correctly guess what values are needed for completing the missing ones, we can ensure a better quality of data This paper describes how association rules can serve as an effective mechanism for guessing what the missing data should be. A new completing procedure and a new evaluation function are developed and presented. Experimental results show that our method is feasible in completing some incomplete datasets 2. Related Work An intuitive but simple method for completing missing data is to substitute the missing data by Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE specific values. For example, zero substitution \(ZS  is to replace all missing data by  zero  which is usually the initial value of the field, e.g  0  for integral data  F  for Boolean data, etc. Mean substitution \(MS  mean value of the data defined under the same variable Random substitution \(RS a value randomly selected from the existing values defined under the same variable. ZS, MS, and RS are simple and fast; but unfortunately, usually produce misleading results. Method like that in [11] utilities the k-nearest neighbor approach and finds the most similar k-nearest neighbor cases to replace missing values Gibbs sampling \(GS   approach for Bayesian inference, which provides a sample from the posterior distribution of the unknown parameters and computes empirical estimates of posterior mean of input parameters. The Expectation Maximization \(EM  and the M step. The E step finds the conditional expectation of the missing values according to the observed data and current estimated parameters. The M step investigates maximum likelihood estimation as if there were no missing data. Bound &amp; Collapse \(BC 20] is a deterministic method that estimates some predefined parameters and computes the minimal and maximal possibilities of the possible values by bounding the set of possible values and then collapsing into a unique value via a convex combination of the extreme points with weights. Maximum likelihood estimation \(MLE  estimation, which can determine the parameters that maximize the probability of the sample data. MLE estimates the probability of obtaining a particular set of data given a chosen probability model. In addition they provide efficient methods for quantifying uncertainty through confidence bounds to replace missing data. Robust Bayesian Estimator \(RBE  based on Bayesian estimation, which learns conditional probability distributions from incomplete data sets by providing probability intervals that describe the upper and lower bounds of estimation of possible values for each missing datum. In many applications, RBE outperforms the others when data are closely related Other than completing missing data for data recovery, completing missing data is also needed in mining association rules, such as that in [13][14][19 There approaches develop robust mining algorithms or define new measure for evaluating support and confidence heuristically or probabilistically, and generate reliable association rules 3. Association Rules for Completing Missing Data 3.1. Association Rules for KDD Recently, the exploration on the associations among data in transactional datasets receives a lot of attentions. Identifying the associations among data can help data analysts understand the inter-relationships of 


help data analysts understand the inter-relationships of data in the underlying dataset. Here, we give a brief review on the definition of association rules. Let I = {i1 i2  im} be a set of distinct literals called items and D T1, T2  Tn} a set of transactions, where Tj ? I 1djdn. An association rule [1] is an implication of the form A  C, where A, C ? I are disjoint itemsets, A C = I. A and C are called the antecedent and consequence, respectively. Each itemset X has an associated measure of statistical significance s\(X called the support of X, where s\(X   D DddXd To measure the meaningfulness of an association rule r: A  C, the indicators, support, confidence, and lift, are defined as follows. The support, s\(A?C    D DddCAd of r reflects the statistical significance, the confidence c\(A  C     As CAs ?  of r indicates the statistical strength, and the lift lift\(A  C      CsAs CAs ?  describes the dependence between A and C. Finding association rules can be done in two steps: \(1 2 find the rules that satisfy the constraints which are usually controlled by the values of minimum support minsupp, and minimum confidence, minconf. Refer to 1][3][7][16][24], for algorithms of finding association rules 3.2. Data Associations The purpose of discovering association rules is to find items in a transaction that imply the presence of other items in the same transaction. The items appearing the same association rule reflect inter-relationships among the items. In the following, we define  data association  based on a similar concept of association rules as the dependency relationships among data entries in a dataset. Suppose D is a dataset defined over n variables X1  Xn. Let J = {Xi=xik| it1, xik is an Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE instantiation of Xi} be a set of items. A data association is an association rule of the form H  e defined over J, where {e}, H ?J and e?H. For example, in a dataset describing the payment status of purchase, data associations  Pay_Type = cash  Customer_Age = young  and  Pay_Type = credit  Customer_Age = senior  state that young customers usually pay by cash and the senior ones usually pay by credit. Note that, Pay_Type and Customer_Age are variables defined in the dataset The applicability of data associations can be scored in different domain problems 3.3. Obtaining Data Associations There are two sources where data associations can be obtained. One is to inquire from domain experts who 


obtained. One is to inquire from domain experts who explicitly describe the data associations related to the underlying dataset. In such data associations, the dependency relationships among data entities and their applicabilities are defined heuristically. If such relationships cannot be explicitly described, the other means is to obtain from the underlying dataset using the techniques of mining association rules. However the datasets discussed in this research are defined in the relational form. Mining for association rules are for transactional datasets. We first have to transform the dataset into the transactional form and then apply the algorithms for finding large itemsets. The following procedure can be used Suppose that D is a dataset containing m cases, c1 c2  cm, which are defined over n variables, X1  Xn For each case ci, 1d i dm, we convert D into a transactional dataset TD by taking ci as the transactional ID and the instantiations of X1  Xn in ci as the items That is, we let J = {Xj=xjk|1djdn, xjk is an instantiation of Xj found in D} be the itemsets and exclude missing data from J. In this transformation, a transaction in TD looks like  ci , {X1=x1ki  Xn=xnki  Next, given a specific threshold of minsupp, we explore the large itemsets, L1  Lg, gt1, in TD. The Apriori algorithm 3] is employed for finding large itemsets. Next, for each element euv in Lu, 1d udg, vt1, we compute, for each element duvw, wt1, of euv the confidence of  euv\\{duvw  duvw  If the confidence is large than a pre-defined minconf, we put euv\\{duvw  duvw} and associated support and confidence into the set of data associations. For convenience, the antecedent of a data association t is denoted as H\(t t,X consequence \(tail procedure is presented as pseudocodes in Figure 1 Note that, there are some other algorithms being able to find large itemsets effectively, such as the ones in 1][2][3][24], which can be used in the above procedure. Finding data associations from existing datasets is reasonable for relational databases, since the variables in such a database describe specific relations among data in the dataset. Such relations are held by the existing data and the missing ones. Also, data associations can be viewed as special association rules for transaction analysis. The differences between data associations and association rules are as follows INPUT: A dataset D of m cases defined over n variables, the value of minimal support minsup, and the value of minimal confidence minconf OUTPUT: A set T of data associations Let TD be an empty transactional dataset For each case ci  X1=x1ki  Xn=xnki  1d i dm, in D, Do TD = TD  ci , {X1=x1ki  Xn=xnki   Finding large itemsets from TD FindLargeItemsets\(Dt, minsupp  Lg}, gt1 Collect data associations from frequent itemsets Let T For each large itemset euv in Lu, 1dudg, vt1, with support supp\(euv For each element duvw of euv, wt1, Do Let conf\(t  euv\\{duvw  duvw   If \(conf\(t Return T Figure 1. The procedure for obtaining data associations x The items in data associations are variable instantiations in datasets; their variances are limited The items in association rules for transaction analysis are various goods The maximal length of transactions in data associations are the number of variables in the underlying dataset 


are the number of variables in the underlying dataset while the maximal length of transactions in association rules are the number of items. Usually, the former is less than the later 3.4. The Completing Procedure After data associations are obtained, we can utilize the data associations for completing missing data. Also, it is assumed that all data in similar datasets follow similar dependency models. Suppose that D0 is an incomplete dataset containing c cases, which are defined over n variables, X1  Xn. Let d1, d2  dm be m incomplete cases, mdc, in D0 and T is a set of data associations. In the incomplete case di, 1didm, for the missing data defined under the variable Xji, 1d j dn, i.e Xji=?, we find if a data association t=H\(t  Xji=xjk can be found in T. If there are x such links t1, t2  tx we may have x possible combinations for completing Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE Xji. Then, we investigate all H\(tv variable instantiations in H\(tv interpret the case di. The ones that can consistently interpret di are evaluated and one of them is used for completing Xji The applicability of the data association is determined by considering the associated support confidence, and lift since they indicate statistical significance, strength, and the co-relation between H\(t and {Xji=xjk}, respectively. Moreover, the length of H\(t and the degree of consistently interpreting the variables of di, should also be taken into account. Let wt be the number of elements in H\(t confidence, and lift values of t be supp\(t t lift\(t we use the following formulas, appl\(t t score the applicability of t appl\(t   u tw j i jk i jk xXappltI 1     1 where I\(t t H\(t t element Xijk= xijk of H\(t instantiation of Xijk in di. The applicability of each variable instantiation in t is defined as follows  ijk i jk xXappl       in?if0.5 inappearedhasif1 i i jk i i jk i jk dX 


dX dxX Then, we define score\(t w t t tlift w tappl    u    2 If di has more than one missing data, a1 and a2 there may have two or more data associations tj1, tj2   which are composed of the instantiations of different variables can complete a1 and a2. We select the combination that can consistently and simultaneously interprets all variable instantiations of di with the largest sum of scores for completing the missing data in di. If there is no data association containing Xji in its consequence can be found in T, we check the appearing frequency of the instantiation of Xji  in Lk kd2, and the instantiation of Xji that appear most frequently is selected to complete the missing data in di The completing procedure is presented as pseudocodes in Figure 2 and illustrated in Figure 4 The complexity of finding data associations depends on the settings of minsupp and minconf and the algorithm for finding large itemsets; which is similar as that in [1][2][3][24]. Suppose that there are c incomplete cases with h un-instantiated variables \(in average instantiated variables, the complexity of completing procedure can be calculated as follows. Eq.\(1 2 examines wt elements of a data association t to calculate appl\(t t than the number of variables of the underlying dataset they can be considered as constants. Therefore, the complexity of the proposed procedure is about O\(cur 4. An Example Suppose that a test dataset is given in Table 1 \(a dataset is defined by 4 variables, X1, X2, X3 and X4 Each variable can be instantiated by 1 or 2. The dataset can be converted into the transactional form, as shown in Table 1 \(b set as 30% and 75%, respectively. By applying the algorithm in Figure 1, the corresponding data associations can be obtained, as shown in Table  2 Suppose that we try to complete the missing datum X1=?,  in case c6. We search in Table  2 if some data associations include X1 in the consequence. In this case r4 is considered. In r4, we obtain lift\(r4 r4 I\(r4 appl\(X2=1 X1=2 1+0.5 and score\(r4 = \(appl\(r4 r4 1.5/2 1.186. Since r4 can consistently interpret c6, we use X1=2 for the missing value. In c1, two data are missing X3=? and X4=?, are found. Among the data associations 3 rules, r12, r13, r14, are examined for X3 and 3 rules, r15 r16, r17, are for X4. By applying the same procedure, we obtain score\(r12 r13 score\(r14 r15 r16 and score\(r17 have to select the one that can consistently and simultaneously interpret c1 with a highest score, we use X3=2 and X4=1 for completing c1 Figure 3. The completing procedure INPUT: An incomplete dataset D, a data association graph T, a score threshold s  OUTPUT: A completed dataset D  For each incomplete case ci in D, Do Let Xi1, Xi2  Xih be h un-instantiated variables in ci 


For each un-instantiated variable Xij of ci  Do For each link t = H\(t  Xij =d\(t, Xij interprets ci  Do Compute the score sijk of t according to Eq.\(1 2 If sijkts  Put \(t,sijk Find in each Sij an element \(tij, sij H\(ti1 ti2  H\(tih and the sum of scores 6j sij is maximal Replace the missing value of Xi1, Xi2  Xih in ci by Xi1=d\(ti1,Xi1 ti2,Xi2  Xih =d\(tih,Xih Return D  the updated D Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE Incomplete case di Incomplete case di Does t H\(t  Xj=zjk exist in T Does t H\(t  Xj=zjk exist in T For each missing datum Xj=? in di For each missing datum Xj=? in di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Compute score\(t according to Eq.\(1 2 Compute score\(t according to Eq.\(1 2 Does H\(t consistently interpret di Does H\(t consistently interpret di Apply the value to complete the missing data Apply the value to complete the missing data Figure 4. The proposed completing procedure Table  1. \(a b transactional form of the dataset Case X1 X2 X3 X4  TID Items c1 2 1 ? ?  t1 X1=2,X2=1 c2 2 1 2 1  t2 X1=2,X2=1, X3=2, X4=1 c3 2 1 2 1  t3 X1=2,X2=1, X3=2, X4=1 c4 1 2 1 ?  t4 X1=1,X2=2, X3=1 c5 1 2 1 2  t5 X1=1,X2=2, X3=1, X4=2 c6 ? 1 2 1  t6 X2=1, X3=2, X4=1 c7 1 ? 1 ?  t7 X1=1, X3=1 c8 1 2 1 1  t8 X1=1,X2=2, X3=1, X4=1 c9 1 ? 1 ?  t9 X1=1, X3=1 c10 1 2 1 2  t10 X1=1,X2=2, X3=1, X4=2 Table  2. Data associations of D ID Data association supp conf r1 {X3=1  X1=1} 0.60 1.00 r2 {X2=2, X3=1  X1=1} 0.40 1.00 r3 {X2=2  X1=1} 0.40 1.00 r4 {X2=1  X1=2} 0.30 0.75 r5 {X3=2, X4=1  X2=1} 0.30 1.00 r6 {X3=2  X2=1} 0.30 1.00 r7 {X1=2  X2=1} 0.30 1.00 r8 {X4=1  X2=1} 0.30 0.75 r9 {X1=1  X3=1} 0.60 1.00 


r9 {X1=1  X3=1} 0.60 1.00 r10 {X2=2  X3=1} 0.40 1.00 r11 {X1=2,X2=2  X3=1} 0.40 1.00 r12 {X2=1, X4=1  X3=2} 0.30 1.00 r13 {X2=1  X3=2} 0.30 0.75 r14 {X4=1  X3=2} 0.30 0.75 r15 {X2=1, X3=2  X4=1} 0.30 1.00 r16 {X3=2  X4=1} 0.30 1.00 r17 {X2=1  X4=1} 0.30 0.75 5. Experiments In order to test the effectiveness of our method, several experiments are performed and the results are compared with RBE. The datasets, DA and DB, are generated syntactically. In DA, data are randomly generated and the variables in DA have no particular dependency relations. In DB, three variables, X1, X2 and X3, are defined and each variable has three different instantiations, xij, i=1,2,3 and j=1,2,3. Cases in DB are generated \(1 X1=x11  X2=x22}, {X2=x21  X3=x32}, and X3=x33  X1=x13}, and \(2 associations do not hold. The datasets, Monk, TAE and Solaris are from [4]. Table 3 describes the basic information of these datasets. In these datasets, we randomly remove some data from completed cases and test if our method can successfully recover the missing ones. For nm missing data, the accuracy of recovery D is defined as 1- nw/nm if nw data are incorrectly guessed From the experimental results shown in Table 4, our method is more accurate than RBE is Table 3. Description of the test datasets Dataset ID cases variables niv missing ratio source DB -1 15 DB -2 30 3 3 20 DA -1 15 DA-2 100 6 2 20 Syntactic Monk -1 10 Monk -2 415 7 3 20 TAE -1 20 TAE -2 151 5 3 30 Solaris -1 5 Solaris -2 1066 13 4 10 UCI [4 niv: # of instances in each variables \(in average Table 4. Experimental results Our method RBEDataset ID \(s, c, nda s, c, nda DB -1 \(25,65, 6 15,50,8 DB -2 \(25,65, 6 15,50,8 DA -1 \(20,20,350 30,40,179 DA-2 \(20,20,350 30,40,179 Monk -1 \(15,15,115 10,35,181 Monk -2 \(15,15,115 10,35,181 TAE -1 \(10,30,73 5,60,115 TAE -2 \(10,30,73 5,60,115 Solaris-1 \(40,40,2926 60,60,988 Solaris-2 \(40,40,2926 60,60,988 s, c, nda data associations and nda  is the number of data associations obtained 6. Discussions &amp; Conclusion We presented in this paper a new method for completing missing data using data associations. The basic concept is that association rules describe the dependency relationships among data entries in a Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE 


0-7695-2291-2/04 $ 20.00 IEEE dataset and missing data should hold the similar relationships. One of the advantages of using this approach for completing missing data is that data associations can be easily and reasonably obtained Most reasonable datasets include reasonable association rules. Clearly, data associations truly describe the cross-relation among data instantiations more accurately. The score function that we presented in Eq.\(1 2 There may exist other measures, such as [16], that reveal more information for the applicability of data associations. The score function can be further improved. Unfortunately, the accuracy of the proposed method depends on the number of data associations When the threshold of minsupp and minconf are low there will be a large number of data associations being generated, resulting in inefficiency of the completing procedure. The future work includes developing a better informative score function and reducing the number of data associations and determining suitable values of minsupp and minconf. Currently, in this paper, only discrete data are discussed and the data existing in datasets are assumed to be correct and noise-free. We are extending the proposed method to handle non-discrete, noisy, and incorrect datasets 7. References 1] Agrawal, R., and Srikant, R. \(1994  Fast algorithm for mining association rules  in the Proceedings of the International Conference on VLDBases, pp. 487-499 2] Agrawal, R., Imielinksi, T., and Swami, A. \(1993  Dataset mining: a performance perspective  IEEE TKDE vol. 5, no. 6, pp. 914-925 3] Agrawal, R., Srikant, R., and Vu, Q. \(1997  Mining association rules with item constraints  in the Proceedings of the Third International Conference on KDD, Newport Beach, California, pp. 67-73 4] Black, C., Keogh, E., and Merz, C.J. \(1999 repository of machine learning databases, URL http://www.ics.uci.edu/~mlearn/MLRepository.html 5] Buntine, W.L. \(1994  Operations for Learning with Graphical Models  Journal of AI, vol. 2, pp. 159-225 6] Chen, M.S., Han, J., and Yu, P.S. \(1996  Data mining an overview from a database perspective  IEEE TKDE, vol 8, no. 6, pp.866-883 7] Coenen, F., Goulbourne, G., and Leng, P. \(2004  Tree structures for mining association rules  Data Mining and Knowledge Discovery, vol. 8, no. 1, pp.25-51 8] Dempster, A. P., Laird, N.M., and Rubin, D.B. \(1977  Maximum likelihood from incomplete data via the EM algorithm  Journal of the Royal Statistical Society, vol. 39 no. 1, pp.1-38 9] Frick, J.R., and Grabka, M.M. \(2003  Missing Income Information in Panel Data: Incidence, Imputation and its Impact on the Income distribution  in the Proceedings of the Workshop on Item-Non-response and Data Quality in Large Social Surveys, October 9-11 10] Giudici, P., and Castelo, R. \(2003  Improving Markov Chain Monte Carlo model search for data mining   Machine Learning, vol. 50, no. 1-2, pp.127-158 11] Hern  ndez, M.A., and Stolfo, S.J. \(1998  Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem  DMKD, vol. 2, no. 1, pp.9-37 12] Intelligent CAM Systems Laboratory  Methodologies for dealing with the Missing Data   http://www.eng.uc.edu/icams/resources/missing_data.htm 13] Kryszkiewicz, M., \(2000  Probabilistic Approach to Association Rules in Incomplete Databases  Web-Age Information Management, pp. 133-138 14] Kryszkiewicz, M., and Rybinski, H. \(1999 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





