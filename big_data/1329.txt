html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Proceedings  of  the Fourth International Conference on Machine Learning and Cybernetics, Guangzhou, 18-21  August 2005 INFORMATION FUSION OF AGENT BASED HETEROGENEOUS MULTI-CLASSIFIERS SHAO-DAN LIN, GUO-QIANG HAN, XIAO-YUAN XU School of Computer Science and Engineering, South China University of Technology, Guangzhou  510640, China E-MAIL: czt_linsd@gd.gov.cn, csgqhan@scut.edu.cn, xiaoyxu@yahoo.com.cn Abstract Traditional technology of classifier fusion can not make full use of the characteristics of heterogeneous classifiers to deal with various problems. This work suggests a new technology of information fusion using multiple agents, each of which uses a quite different classification algorithm such as decision tree algorithm, simple Na  ve Bayes algorithm and the newly emerging classification algorithm based on atomic association rules. Information fusion of these heterogeneous multi-classifiers is based on the classifier behavior, properties of training dataset and the instance to be classified. The proposed technology has following advantages: \(1 classification accuracy; \(2 3 fast learning and prediction. The experimental results on 10 UCI standard datasets show that accuracy of the proposed fusion technology is noticeably higher than that of traditional voting method Keywords Fusion of heterogeneous classifiers; Agent; associative classification; decision tree; Na  ve Bayes 1. Introduction Information fusion technology of multi-classifiers can surpass the capability limit of a single classifier. It can overcome the shortcomings of single classifier and realize the knowledge integration among multiple classifiers to achieve better performance. Especially, the information fusion of heterogeneous multi-classifiers can display their advantages of complementary among different classifier models and can make the classifying prediction more accurate and robust In this paper, we adopt multi-agents based technology to realize the knowledge fusion of heterogeneous classifiers Agents can imitate human beings  group behaviors to solve problems. According to this, every classifier is designed to be an agent with striking characteristics. Each agent has both strength and weakness. We use two types of agents model-responsible agent \(for short, Model Agent combining classifier agent \(for short, Combining Agent Model Agents learn a classification model using a specified algorithm on a dataset and predict an unlabelled instance using the learned model. Combining Agent realizes the information fusion of multiple Model Agents  prediction results and gives the final class label of the unlabelled instance When agents collectively do a classifying job, first of all, every Model Agent must complete its learning on its own and produce an independent classifying model. When a classifying prediction task is performed, the Combining Agent sends every Model Agent the instance to be forecast Model Agents use classifying model to forecast the label of the instance, meanwhile, it evaluates the instance  s properties and sends the overall evaluation results with the class label to the Combining Agent The combining Agent gives the final class label to the unlabelled instance based on the results of all Model Agent in terms of control logic. Thus, Combining Agent probably surpasses the capability limits of single classifiers. Hence the agent based system of heterogeneous multi-classifiers can achieve a high classifying accuracy without additional fusion training of classifier group 2. Agent based system of heterogeneous multiple classifiers 


classifiers 2.1. Choosing classifiers of different properties When choosing heterogeneous classifiers, two parameters must be taken into consideration: a high prediction accuracy and short time for model building. In order to structure the Model Agents, this work utilizes three different types of classifying algorithms, which meet the requirements mentioned above. Two of them, decision tree classification and simple Na  ve Bayes algorithm are quite popular in machine learning community. The third one is a new algorithm \(classification based on atomic associative rules, i.e., CAAR [2 2004 International Conference. The classifying mechanisms of these three algorithms are quite different. Decision Tree is based on entropy, simple Na  ve Bayes is based on 0-7803-9091-1/05/$20.00  2005 IEEE 1976  Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 post-pr classifi Am human outstan  easy-f CAAR of this 1 apply t confide 0.98*m 2 order b rules o rule. A which c 3 classify in the classify sequen ith to gh In of ial ng an y ur nt ers ne va es to obability, and CAAR is based on associative cation using confidence and support ong these, the newly emerging CAAR imitates behaviors in classifying things and makes use of ding features of the instances in dataset and the irst  strategy to classify. The details related to can be found in literature [2]. The three main steps algorithm  s principle are described below  he associative rules of strong rules with the highest nce \(maxConf axConf  ased on the rule  s confidence and support, test the 


ased on the rule  s confidence and support, test the n the dataset and the delete the records covered by a fter passing through the dataset, the redundant rules atch no instances in test are deleted  ing process again until the number of valid records dataset becomes zero. Finally, combine the ing rules generated at each partial classification in CAAR algorithm only uses strong atomic rules w the highest confidence and near-highest confidence classify. Thus, it has the following characteristics: a hi classifying accuracy, top speed and robust performance literature [2], there is a detailed description and analysis CAAR algorithm. Though CAAR uses several part classifications \(on average, with 7 passes speed, close to decision tree classification, is far faster th that of CBA algorithm [4 2.2. Multi-classifier system integrating JADE and Weka Agents possess many characteristics such as autonom intelligence, sociability and self-adaptation, etc. In o study, agents are constructed in Java Agent developme environment, that is, JADE platform [10]. The classifi are designed on the platform of Weka Java Machi Learning [9]. Decision tree algorithm J48 \(a ja implementation of C4.5 [1  ve Bay hereafter referred as Bayes  realize CAAR algorithm and combining classifier under  Figure 1. User interface of the heterogeneous multi-classifier system integrating Weka and  JADE 1977 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 Weka pla is to mo JADE p jade.core Weka h commun heteroge network file Ge weka\\gu be regist the GUI 3. Info clas 3.1. In The of the cla classifyin condition between ratio of distributi A C of each different advantag of classif dynamic For two cat propertie classifica as show the ome may tform. An easy way to integrate JADE and Weka dify the abstract class "Classifiers" by importing 


dify the abstract class "Classifiers" by importing ackage and make class "Classifiers" inherit Agent. In this way, all classifying algorithms in ave the basic functions of agent, such as 3.1.1. Influence of datasets The properties of the datasets directly influence capability of Model Agent [8]. For a given dataset, s algorithms may achieve high accuracy while others ication. So the agent based system of neous multi-classifiers can be used for distributed environment. After the construction of agents, the nericObjectEditor.props under the directory i" must be revised [6]. So the agent classifier can ered and integrated with Weka GUI. Figure 1 gives of our experiment dealing with dataset wine rmation fusion of heterogeneous multiple sifiers fluencing factors re are many factors that determine the performance ssification. For a given classification algorithm, its g capability will change according to different s such as properties of datasets, correlations the features or attributes and the class labels, the attributes missing values and uniformity of class on, etc ombining Agent needs to know the characteristics Model Agent and the complementary effects of classifying algorithms, and make full use of the es of all Model Agents. So it can break the limits ying capabilities of single Model Agent by using a fusion strategy [7 information fusion of multi-classifiers, there are egories of important influencing factors: the s of dataset, which have a great effect on the tion model, and that of the instance to be classified n in figure 2 perform unsatisfactorily. That is the algorithm  s adaptation to the specified problems. Up to now, there is no classification algorithm that can produce ideal model suitable for all problems, so some hybrid algorithms attract the interest of researchers [3][5 Therefore, if heterogeneous Model Agents can be used to deal with the specified problem complementarily, the chance for success will be much bigger. The key problem is how to measure the influencing factors related to dataset As for decision tree algorithm, two main factors that affect the classifying accuracy of decision tree is Ratio of the number of Missing Values \(RMV values of the training dataset, and the uniformity of the class distribution \(UCD UCD is given as follows 2 max C 1i imax C 11UCD  1 Where C is a set of class labels and di is the number of instances that belonging to ith class, and dmax=max{di C|i1 For CAAR algorithm, the internal associative relationship between dataset  s attributes and the class labels is the main factor influencing the classifiers  adaptation. As described in [2], in the first partial classification of CAAR based on mean confidence \(MC MS ten strongest atomic association rules, the CAAR divides datasets into two categories: type-P datasets that satisfy the constraining conditions of MC&gt;0.8 and MS&gt;0.02, and type-N datasets that do not satisfy the constraining conditions. The test on 26 datasets of UCI shows that 20 


conditions. The test on 26 datasets of UCI shows that 20 datasets belong to type-P and 6 ones belong to type-N. It is obvious that the classifying capability of CAAR on type-P datasets is much greater than on type-N z Parameters of CAAR model learned on a dataset \(MC and MS the dataset { z Uniformity of Class distribution and ratio of missing values in dataset \(UCD and RMV  Confidence of a CAAR rule r classifying an instance r.Conf An instance  s ratio of missing values \(IRMV Influencing factors  Influence of the instance to be predicted  z The max probability of an instance being a class \(maxP Figure 2. Factors that influence information fusion of heterogeneous multi-classifiers z z 1978 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 As for Bayes algorithm, it depends on the assumption that attributes are independent. This is the main factor that affects the adapting capability of Bayes classifiers. To quantatively evaluate the correctness of this hypothesis is very difficult. Thus, in terms of dataset-related factors, this paper only evaluates the influences that internal association features of datasets bring to CAAR and influences that the uniformity of class distribution of datasets and the ratio of missing values have on J48 3.1.2. Influence of the instance to be classified If classifying models are used to predict an unlabelled instance, the results of three Model Agents are directly relevant to the properties of the instances to be classified Sc for CAAR  amp; Sj for J48  amp RMV&lt;0.05 IRMV&lt;0.10 UCD&gt;0.60  maxP&gt;0.90 Sb for Bayes X S,S,S{X jbc X MC&gt;0.80 r.Conf&gt;0.70 MS&gt;0.02  Figure 3. Logic illustration of classifiers which satisfy the constrain conditions As to decision tree, the prediction accuracy is heavily affected by the instance  s ratio of missing values \(IRMV The more missing values, the worst case the prediction is As to CAAR, the classifying rule  s confidence is considered an important factor that influences the classifying accuracy For Bayes algorithm, the maximum value of probability distribution of the class labels for the instances to be classified, determines which class the instance belongs to. If the maximum probability is only 51%, the other 49% probability indicates the instance  s label is not 


other 49% probability indicates the instance  s label is not the chosen one. Under this condition, Bayes algorithm must produce inferior classification cS bS jS 3False Vote Pc=Pj= Pb Pc= Pj Pc Pc=Pb Pj=Pb bjc  PPP Pb true false 3True Sc Sb Pj maxP &gt;r.Conf 2True Sc Sb2True2True Sc Sj Pc Sj Sb Pb 1True 1True 1True Sc Sb Pj Sj bS cS jS bS S cS Sj cS bS j jS  Figure 4. Control logic of information fusion of three classifiers 1979 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 It is thus evid heterogeneous Model 3.2. Control flow o classifiers In order to contr classifiers, a logical Agent to see whether If the Model Agent ca datasets and of unlabe pass the constrain test true; otherwise, it  s conditions which are CAAR, obviously if it must be type-P \(i r.Conf&gt;0.7. If these co of Sc is false. As to uniformity of class di and the dataset  s ratio and the instance  s rati is true. As to Bayes c of classifying the insta true With regard to classifying prediction Model Agent CAAR respectively. The C 


information fusion o control logic \(shown method not only takes classifiers but also consideration. Those symbol is true have logic variables are s e e a t l f f Dataset Anneal 3 Auto 2 Breast 1 Diabetes Hepatic 1 Iono 3 Labor 1 Tic-tac Wine 1 Zoo 1 Ment that the information fusion of Agents is very important f information fusion of multi ol the information fusion of multiple symbol is designed for every Model the appointed constrains are satisfied n simultaneously satisfy constrains of lled instances, then the Model Agent and the value of its logical symbol is false. Figure 3 shows the constrain 3True i.e., "3False Model Agents have the same priority in determining th class label of the instance, so information fusion comes to voting state. If the dispute among three Model Agents can  be settled by voting \(for example class label is determined by directly comparing maxP o Bayes with r.Conf of CAAR as shown in figure 4 bjc  PPP 4. Experimental results and discussion 4.1. Experimental environment The agent-based system for information fusion oneeded by three Model Agents. As to s logical symbol Sc is true, the dataset e., MC&gt;0.80 and MS&gt;0.02 nditions are not met, the logical value decision tree J48, if the dataset  s stribution \(UCD of missing values is less than 0.05 o of missing values less than 0.10, Sj lassifiers, if the maximum probability nce is bigger than 0.90, then the Sb is the given unlabelled instance for the three class labels predicted by J48 and Bayes are Pc, Pb and Pj ombining Agent carries out the f multi-classifiers according to the in figure 4 advantage of the characteristics of all takes the classified instance into Model Agents whose constrain test the final say. When three classifiers   imultaneously true \(i.e., a state of heterogeneous multi-classifiers is realized on the platform of JADE and Weka. Different from literature [2], CAAR is integrated into the environment of Weka in this experiment Since Weka uses 10-fold stratified cross-validation. The datasets at first are sorted according to class labels before 


datasets at first are sorted according to class labels before the 10-fold stratified cross validation. Thus, there are some differences in classification results of CAAR algorithm. Ten datasets from UCI machine learning repository http://www.ics.uci.edu/~mlearn experiment \(indicated in Table 1 method is used to discrete all the continuous attributes in the datasets. The computer used in experiment is a PC with 2.0GB P4 and 256MB memory. The OS is Windows XP 4.2. Experimental results and discussion Table 1 provides properties of ten experimental datasets, and the prediction accuracy of 10- fold stratified cross validation of different classifying algorithms. In table 1, |A| and |C| respectively refer to the number of attributes and the number of class labels in datasets. The column Table 1. The properties of datasets and experimental results of classification Properties of datasets Accuracy of classifier A| |C| Type UCD RMV MMP CAAR J48 Bayes VF HCF 8 6 P 0.32 0.63 0.955 96.33 91.65 95.99 95.77 97.10 5 7 N 0.66 0.01 0.906 71.22 80.98 67.32 79.02 78.54 0 2 P 0.89 0.00 0.997 97.14 95.57 97.28 97.28 96.85 8 2 P 0.89 0.00 0.828 75.78 77.08 77.99 77.86 77.34 9 2 P 0.73 0.05 0.948 84.52 83.23 84.52 84.52 86.45 4 2 P 0.90 0.00 0.992 93.73 90.60 91.17 94.59 94.87 6 2 P 0.89 0.34 0.945 94.74 84.21 91.23 91.23 94.74 9 2 N 0.89 0.00 0.727 70.88 86.01 70.35 75.57 86.01 3 3 P 0.96 0.00 0.995 97.75 93.82 98.87 98.31 99.44 6 7 P 0.50 0.00 0.951 96.04 92.08 93.07 95.05 96.04 ean 0.76 0.10 0.924 87.81 87.52 86.78 88.92 90.74 1980 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 Type" stands for the dataset type which is corresponding to the classifying capability of CAAR on the specified dataset UCD stands for the uniformity of class distribution in datasets. RMV stands for the ratio of the number of missing values to the total number of attribute values in a dataset MMP is the average value of all the maximum probabilities maxP CAAR, J48, Bayes, VF and PF are corresponding to the prediction accuracy in 10-fold stratified cross-validation using CAAR algorithm, J48 algorithm, Bayes algorithm voting fusion algorithm and our proposed algorithm of heterogeneous classifier fusion, respectively After analyzing the properties of datasets, we can see that for the datasets of Anneal and Labor, their RMV values is bigger than others, and the prediction accuracy of J48 is clearly lower than that of CAAR and Bayes. On the dataset Tic-tac \(type-N comparatively weaker. At the same time, the MMP value of Bayes is only 0.727. Consequently, on this dataset, these two classifying algorithms are not as good as J48. On dataset Auto \(type-N is weaker, but that of J48 is better. But as to Bayes, though the parameter of MMP is 0.906, its classifying effects are comparatively worse. This illustrates the fact that it is unrealistic to assume that attributes of the dataset are independent one another Table 1 clearly shows that in the 10 datasets, as to single classifiers, the prediction accuracy of CAAR is the highest \(87.81 higher than that of any single classifier. Among these, the accuracy of our proposed fusion algorithm is 90.74 which is noticeably higher than 88.92% of traditional voting fusion method 5. Conclusions This work proposes a new information fusion technology of multi-classifiers. Using heterogeneous multi-classifiers can make full use of the characteristics of quite different classification algorithm. Our technology takes many factors into consideration, including the 


takes many factors into consideration, including the properties of classifiers, datasets and the classifying instance etc. The technology can effectively integrate the information from heterogeneous multi-classifiers. It has the following characteristics: a high accuracy for prediction robust performance, short time to build a classification model, and no need for fusion training Acknowledgements This paper was supported by the Natural Science Foundation of Guangdong Province of China \(grant No 31340 Guangzhou City in Guangdong Province of China \(grant No. 2004Z3-E0091 of Waikato in New Zealand. JADE was provided by Telecom Italia Lab, the R&amp;D branch of the Telecom Italia Group References 1] J. R.Quinlan. C4.5: Programs for machine learning Morgan Kaufmann, San Francisco, 1993 2] Xiaoyuan Xu, Guoqiang Han, and Huaqing Min  Construct Concise and Accurate Classifier by Atomic Association Rules  Proceeding of the Third International Conference on Machine Learning and Cybernetics \(ICMLC 2004 China, pp. 1604-1609 3] Ron Kohavi, "Scaling up the accuracy of Naive-Bayes classifiers: a decision-tree hybrid", Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996, pp. 202-207 4] B. Liu, W. Hsu, and Y. Ma, "Integrating Classification and Association Rule Mining", Proceedings of the KDD, New York, NY, August 1998, pp. 80-86 5] D. Meretakis, and B. Wuthrich, "Extending Nave Bayes classifiers using long itemsets", Proceedings of the 5th ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining \(KDD'99 San Diego, USA, 1999, pp. 165-174 6] Fabio B., Giovanni C., Tiziana T., et al, JADE Programmer's Guide. URL: Http://agent.cs.bath ac.uk/jade/programmersguide.pdf, 2004-12-21 7] Giancinto Giancinto and and Roli Roli, 01 , 01] G Giancinto and F. Roli  Dynamic Classifier Selection based on Multiple Classifier Behavior  Pattern Recognition, 2001, 34:1879-1881 8] M. Kamel and N. Wanas  Data dependence in combining classifiers  Multiple Classifier Systems 4th international workshop, MCS2003, Guilford, UK June 2003, Springer-Verlag, pp. 1-14 9] Berlin, LNCS2709.Weka Machine Learning Java Package, http://www.cs.waikato.ac.nz/~ml 10] Java Agent DEvelopment framework, http://sharon cselt.it/ projects/jade 1981 pre></body></html 





 0 100 200 300 400 500 600 1 21416181 Time \(epoch counts 0.6 0.9 1.0 a MinMaxLoad WC 0 1000 2000 3000 4000 5000 6000 7000 8000 1 21416181 Time \(epoch counts 0.6 b SS2 Figure 4 Space needed at node R to store answer synopsis S A  non-distributed single-stream case In contrast when SS2 is used to set the precision gradient Figure 4b the space requirement is almost an order of magnitude greater This difference in synopsis size occurs because in SS2 frequency counts are only pruned from synopses at leaf nodes so counts for all items that are locally frequent in one or more local streams reach the root node No pruning power is reserved for the root node and therefore no count in S A is ever discarded irrespective of the  value The same situation occurs if Algorithm 2b is used instead of Algorithm 2a This result underscores the importance of setting  1  in order to limit the size of S A  as discussed in Section 2.1 3.4 Communication Load Figure 5 shows our communication measurements under each of our two metrics for each of our three data sets under each of the ve strategies for setting the precision gradient listed in Table 3 First of all as expected the overhead of SS1 is excessive under both metrics Second by inspecting Figure 5a we see that strategy MinRootLoad does indeed incur the least load on the root node R in all cases as predicted by our analysis of Section 2.1.2 Under this metric MinRootLoad outperforms both simple strategies SS1 and SS2 by a factor of 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 247k 10k 296k 132k 20k counts transmitted \(per epoch a Load on root node R 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 43k    21k 52k   19k 23k   7k counts transmitted \(per epoch b Maximum load on any link Figure 5 Communication measurements k denotes thousands ve or more in all cases measured However MinRootLoad performs poorly in terms of maximum load on any link as shown in Figure 5b because no early elimination of counts for infrequent items is performed and consequently synopses sent from the grand-children of the root node to the children of the root node tend to be quite large As expected MinMaxLoad NWC performs best under that metric on all data sets For the AUCTION data set even though SS2 outperforms MinMaxLoad WC to be expected because of the high  value our hybrid strategy MinMaxLoad NWC is superior to SS2 by a factor of over two For the I NTERNET 2 and BBOARD data sets the improvement over SS2 is more than a factor of three On the negative side total communication not shown in graphs is somewhat higher under MinMaxLoad WC than under SS2 increase of between 7.5 and 49.5 depending on the data set 4 Related Work Most prior work on identifying frequent items in data streams 6 8 9 19 only considers the single-stream case While we are not aware of any work on maintaining frequency counts for frequent items in a distributed stream setting work by Babcock and Olston does adProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


dress a related problem In the problem is to monitor continuously changing numerical values which could represent frequency counts in a distributed setting The objective is to maintain a list of the top k aggregated values where each aggregated value represents the sum of a set of individual values each of which is stored on a different node The work of assumes a single-le v e l communication topology and does not consider how to manage synopsis precision in hierarchical communication structures using in-network aggregation which is the main focus of this paper The work most closely related to ours is the recent work of Greenwald and Khanna which addresses the problem of computing approximate quantiles in a general communication topology Their technique can be used to nd frequencies of frequent items to within a congurable error tolerance The work in focuses on providing an asymptotic bound on the maximum load on any link our result adheres to the same asymptotic bound It does not however address how best to congure a precision gradient in order to minimize load which is the particular focus of our work 5 Summary In this paper we studied the problem of nding frequent items in the union of multiple distributed streams The central issue is how best to manage the degree of approximation performed as partial synopses from multiple nodes are combined We characterized this process for hierarchical communication topologies in terms of a precision gradient followed by synopses as they are passed from leaves to the root and combined incrementally We studied the problem of nding the optimal precision gradient under two alternative and incompatible optimization objectives 1 minimizing load on the central node to which answers are delivered and 2 minimizing worst-case load on any communication link We then introduced a heuristic designed to perform well for the second objective in practice when data does not conform to worst-case input characteristics Our experimental results on three real-world data sets showed that our methods of setting the precision gradient are greatly superior to na  ve strategies under both metrics on all data sets studied Acknowledgments We thank Arvind Arasu and Dawn Song for their valuable input and assistance References  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB  1994  I Akamai T echnologies Akamai http://www akamai.com   A Ak ella A Bharambe M Reiter  and S Seshan Detecting DDoS attacks on ISP networks In PODS Workshop on Management and Processing of Data Streams  2003  A Arasu and G S Manku Approximate quantiles and frequency counts over sliding windows In PODS  2004  B Babcock and C Olston Distrib uted top-k monitoring In SIGMOD  2003  M Charikar  K  Chen and M F arach-Colton Finding frequent items in data streams In International Colloquium on Automata Languages and Programming  2002  E Cohen and M Strauss Maintaining time-decaying stream aggregates In PODS  2003  G Cormode and S Muthukrishnan What s hot and whats not Tracking frequent items dynamically In PODS  2003  E D Demaine A Lopez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space In European Symposium on Algorithms  2003  DynaServ er R UBis and R UBBos http://www.cs rice.edu/CS/Systems/DynaServer   eBay Inc eBay  http://www.ebay.com   C Estan and G V a r ghese Ne w directions in traf c measurement and accounting In SIGCOMM  2002  M F ang N Shi v akumar  H  Garcia-Molina R Motwani and J Ulmann Computing iceberg queries efciently In VLDB  1998  P  B Gibbons and Y  Matias Ne w sampling-based summary statistics for improving approximate query answers In SIGMOD  1998  P  B Gibbons and S T irthapura Estimating simple functions on the union of data streams In Symposium on Parallel Algorithms and Architectures  2001  M Greenw ald and S Khanna Po wer conserving computation of order-statistics over sensor networks In PODS  2004  J Han J Pei G Dong and K W ang Ef cient computation of iceberg queries with complex measures In SIGMOD  2001  Internet2 Internet2 Abilene Netw ork http abilene.internet2.edu   R M Karp S Shenk er  and C H P apadimitriou A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  2003  H.-A Kim and B Karp Autograph T o w ard automated distributed worm signature detection In Proceedings of the 13th Usenix Security Symposium  2004  A Manjhi V  Shkapen yuk K Dhamdhere and C Olston Finding recently frequent items in distributed data streams Technical report 2004 http://www cs.cmu.edu/manjhi/freqItems.pdf   G S Manku and R Motw ani Approximate frequenc y counts over data streams In VLDB  2002  Open Source De v elopment Netw ork Inc Slashdot http://slashdot.org  Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


