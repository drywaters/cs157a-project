The Study of Rough Relational Database Based on Granular Computing Qiusheng An Yusheng Zhang and WenXiu Zhang Abstract-In this paper the data querying and the rough functional dependency of rough relational database are studied by granular computing and rough sets theory First the bit representations of granular computing are used in order to represent attributes values of rough relational database and these equivalence granules attributes values are analyzed by rough sets theory Second data querying related to equivalence granules for rough relational database are discussed we divide the data querying into 
two types certain data querying and possible data querying in addition rough functional dependency based on bit representation is investigated and some important conclusions are presented Index Terms-bit representation data querying granular computing rough functional dependency roughly-redundant I INTRODUCTION J ough relational database model was introduced by eaubouef Theresa and Frederick E.Petry they defined the rough relational operators including difference union intersection select project join and divide and discussed their properties[l][2 The term granular computing GrC was first suggested by professor T.Y.Lin which the basic idea of granular computing is problem solving 
with different granularities 3 Granular computing may be viewed as a system of computing in which the objects of computation have a granular structure with a granule defined by a generalized constraint By construction granular computing subsumes interval analysis quantization and rough sets theory With granular computing the principal computational tool is the generalized extension principle 4 T.Y.Lin introduced the machine-oriented model for data mining by the bit representations with granular computing in paper 5 He used this method to investigate association rules and the primary objects he studied are relational database In 
his granular relation theory i.e machine oriented modeling primary processing unit is elementary granule so a relation use Manuscript received March 1 2005 This work was supported by 973 program of China No 2002CB312200 Qiusheng An is with the Institute for Information and System Sciences School of Science Xi'an Jiaotong University,710049 Xi'an China Tele phone:86-0357-2051517;e-mail:aqsaqsl 18@163.com Yusheng Zhang is with the Department of Mathematics Shanxi Datong University Datong 037009 China\(e-mail:ybsy_g@263.net WenXiu Zhang is with the Institute for Information and System Sciences School of Science Xi'an Jiaotong University,710049 Xi'an China e-mail:WXZhang\(xjtu.edu.cn granules bit patterns 
as attribute values is a model using entities as primitives it is called a machine oriented model In this model attributes names and attributes values are the meaningful names of binary relations and elementary granules respectively Up to now most of work related to rough relational database is theoretical research whereas only a few works deal with applications problems In this paper we use granular computing and rough sets theory to discuss the data querying and functional dependency of rough relational database II BASIC CONCEPTS Definition 1 The rough relational database has several features in common with 
the ordinary relational database Both models represent data as a collection of relations containing tuples These relations are sets These tuples of a relation are unordered and nonduplicated A tuple ti has the form di di2 
  dim where dy is a domain value of a particular domain set Dj In classical relational database dije Dj In the rough relational database however dy c Dj and duI does not have to be a singleton du 0 2 Definition 2 Information granules are viewed as linked collections of objects data points in particular drawn together by the criteria of indistinguishability 
similarity or functionality For an information system I=<U A elementary granule is defined by EFK\(u where EFK\(u is a conjunction of selectors of the form Ai=Ai\(u 11 EFK\(u 11 AA 4EK Ai Ai\(u III KcA ue U,II II is a function from 1D into power set 2u 6]-[10 Definition 3 An interpretation a a a2  am of a rough tuple ti=\(di1,di2,...,dim is any value assignment such that aje dy for all I  j m aj is called a sub-interpretation of du Definition 4 A rough relation is a subset of the set cross product P\(D1 x 
P\(D2 x P\(Dm  Definition 5 A binary granular structure consists of 4-tuple V,U,B,C V is called the object space U is the data space B={B',i=1,J...,n is a finite set of crisp/fuzzy binary relation and C is a finite set of concept spaces each of which consists of all the names attribute values of the elementary neighborhood B'p{ulu B p}.When V=U and the binary relations are equivalence relations i.e B=E then the triple U,E C is called rough granular structure where C in this case consists of the names\(attributes and attribute values of equivalence relations and equivalence classes of E 11 
 0-7803-9017-2/05/$20.00 C2005 IEEE 108 


III GRANULAR REPRESENTATIONS OF ROUGH RELATIONAL DATABASE In general relational database is human oriented processing of massive data according to their semantics e.g attribute values are its primary processing units On the other hand data mining is machine derivations of hidden semantics called patterns from the mathematical structure of stored data its primary processing unit is elementary granule so a relation use granules bit patterns as attribute values is a model using entities as primitives it is called a machine oriented model.In this model attributes names and attributes values are the meaningful names of binary relations and elementary granules respectively Proposition 1 Given a rough relational database R let t[AJ and x1,x2,..xJ be R's arbitrary attribute value and a set of objects respectively If t[AJ]exi Af t[A]j=1 then xi  R t A l jf tAJ]EXiA t[AJ I l,then xi c R A  where 1<i.n 1.-1.m is the cardinal number of sub-interpretations Definition 6 Let BIT be a mapping function BIT M bin1bin2...bini...bin ILI where MI{vvl vi vj vIq is the center of elementary granules bini=1 if viE M bini=O if vieM TABLE I SUBREGIONS is modified from 1 it is a rough relational database follow we first analyze its granular representations TABLE I SUBREGIONS OBJ ID COUNTRY FEATURE xI U123 US MARSH LAKE MARSH1 X2}b R MARSH IX,X2,X31 A LAKE=0 R LAKE-{Xj A PASTURE R PASTURE={X3 R RIVER=O X2 U124 US MARSH X3 U125 US MARSH PASTURE RIVER X4 U126 US FOREST,RIVER X5 U147 US SAND,ROAD,URBAN X6 U157 US MEXICO SAND,ROAD X7 M007 MEXICO SAND,ROAD x8 M008 MEXICO BEACH xg M009 MEXICO SAND Xio C039 BELIZE JUNGLE xI C040 BELIZE INT JUNGLE BEACH,SEA As can be seen from TABLE I the attribute value of ID is unique so the COUNTRY and FEATURE are mainly discussed Then according to proposition 1 the following results are obtained R US XhX2,X3,X4,X5b R us XjX2,X3,X4,X5,X6 R MEXICO-IX7,X8,X9 R MEXICO-{X6,X7,X8,X9 R BELIZE IXO BELIZE IXJ0,X1J R R RIVER-{X3,X4 R FOREST=4 R FOREST X4}S RS SD  X9 R SAND=I X5,X6,X7,X9 L ROAD R ROAD XS,X6,X7 A URBAN=0 R URBAN XS R BEACeI X8 R BEACH X8,X11 R JUNGLE={Xjo R JUNGLE X1O,XJI R SEA  R SEA Xjj Each attribute value above is corresponding to an equivalence granule and their bit representations are as follows according to definition 6 BIT\(R us IIIOOOOOO BIT R us BIT\(R ACO BIT R ssxco I I1OO BIT\(R BELIZE BIT\(R BELIZE BIT FM4RSH BIT\(R MARSH BIT R LAKE BIT R LKE BIT R PASTURE BIT R PASTURE BIT R VER BIT\(R RIVER BIT R FOREsT BIT R FOREST BIT\(R sAND BIT R sAND BIT R ROAD BIT R ROAD BIT\(R URBAN BIT\(UR RBAN BIT\(R BEACH BIT\(R BEACH BIT R juNGLE BIT R JUNGLE BIT R SEA BIT\(R SEA 1 IV DATA QUERYING OF RoUGH RELATIONAL DATABASE BASED ON BIT REPRESENTrATION Rough querying is the operation which make use of 109 


indiscernibility relation to finish In fact in the rough relational database model a query returns a rough relation based on indiscernibility relation of attribute values The rough relation is composed of a low approximation or those tuples which are certain responses to the query and an upper approximation tuples which are possible responses to the query 1 Given the granular representations of attributes values we can query them further Here we divide the querying based on bit representations into two types 1 Certain data quemng is that search these objects fully matching the querying conditions and the querying results are obtained by the low approximation of attributes values For example if we query those COUNTRY equal to US in TABLE I we can use R us whose BIT R us 1 1 1 1000000 to obtain the querying result i.e x1x2,x x4xs if we query the objects set that COUNTRY is equal to US and FEATURE is equal to MARSH then we can take the intersection of two bit representations namely BIT R us r BIT R mARsH 1 1 1100 OOrOn 01000000000  01000000000 and the result set is x2 The certain data querying is based on the proposition R XnY R X R Y of rough sets theory Proposition 2.2,4  then we will obtain the following proposition according the above conclusion Proposition 2 The result of certain data querying is the minimal set that satisfies querying conditions the result can be represented as  t[AJ]lt[AJ]eR Al t[AJ]l=l  where I.i.n 1.].fm R is a rough relational database  is the cardinal number of sub-interpretations 2 Possible data querying is rough querying and the querying results are obtained by the upper approximation of attributes values For example if we query those COUNTRY equal to US in Table I we will use R us whose BIT R us 11111000000 to obtain the result set is X1,X2,X3,X4,X5,X6 if we query the objects set that COUNTRY is equal to US and FEATURE is equal to AMRSH then we can take the intersection of two bit representations with two upper approximation of attributes values namely BIT R us r BIT R MARsH 1 1111 1 O00000r 11100000000 11100000000 the result set is  XI,X2,X3 The possible data querying is based on the proposition R XnY Y R X R Y of rough sets theory Proposition 2.2,8  the following proposition can be obtained according to the above conclusion Proposition 3 The result of possible data querying is the maximal set that satisfies querying conditions the result can be represented as tAJI t[Aj]eR Al t[AjI l  where 1 i n 1<j  m R be rough relational database 0/is the cardinal number of sub-interpretations V RoUGH FUNCTioNAL DEPENDENCY OF RoUGH RELATIONAL DATABASE BASED ON Brr REPRESENTATIONS Definition 7 Two sub-tuples X=\(ddx1,d.r and Y dNl d.2,...,d are redundant if dLxj=[dyj]I Definition 8 Two sub-tuples X=\(dx1,dx2.d and Y=\(dy1 dy2 dy are roughly-redundant if for some p c dj and q]5[dyj],[p]=[q]forallj=I,2,...,m 1 Definition 9 A rough fumctional dependency 276 Yfor a relation schema R exists if for all instances T\(R 1 Foranytwotuplest,t'E RT redundant\(t\(X X redundan\(t\(Y Y 2 For any two tuples s,s E R T roughlyredundant s\(X X roughly-redundant\(s\(Y Y 1 For a rough relational database the judge of rough functional dependency requires to use equivalence relation Assume that each attribute value is only included in one equivalence class then we can have following equivalence classes in TABLE I U/ID  xI  2 IX X4 X5 X6 X7 X8  x9 xjo xll  U/COLUNTRY XJ,X2,X3,X4,XS X6 X7 X8 X9 Xjo X/1 U/FEA TURE={iXI X2},{JX31 X4 X5 X6 X7 X8 X9 XJo X  According to the theory of functional dependency ID RlCOUNTRY and ID FEATURE hold Further we can obtain the following conclusion Proposition 4 Let t[AJ t[AJj be two attributes values belong to Xand Yrespectively x is an object of R if the following two conditions satisfy 1 t Ai thenxe Rt[Aj  2 R4Ai],thenxE R4Aj the rough functional dependencyX Yy holds Definition 10 redundant factor For definition 9,we call the similarity measure between t\(X t'\(X as antecedent lower redundant factor denoted as a we call the similarity measure between t\(Y and t'\(Y as consequent lower redundant factor denoted as B,so the definition 9 1 can denote as Xa Yp Similar we call the similarity measure between s\(X and s'\(X as antecedent upper redundant factor denoted as a the similarity measure between s\(Y and s'\(Y as consequent upper redundant factor denoted as f3',so the definition 9 2 can denote as Xa'4 card t\(X r t'\(X card t\(Y n t Y card t\(X u t x card t\(Y u t'\(Y Where a  E E 0,1 and card denotes as cardinal number of the set the definition of a',l is similar as a 5 In above example US and US MEXICO is roughlyredundant and aW=1/2=0.5 SAND,ROAD  and  SAND ROAD,URBAN  is roughly-redundant too so  t=2/3 For rough functional dependency we have following conclusions Proposition 5 For a RRDB if its arbitrary tuples t d1 d m2 dm and t=\(dy1,dy2 dym is tuple-redundant then they must be roughly-redundant If t,t is roughly-redundant it's unnecessarily tuple-redundant 110 


Proposition 6 In rough functional dependency Xa  Yp and 12 Z Pawlak Rough set Theoretical Aspects of Reasoning About Data Dordrecht Kluwer Academic Publishers l991,pp.12 Xaf R  YW if a=1 then fl=1,but when a 1,/3'is unnecessarily equal to 1 Proposition 7 Dissimilar antecedent attributes values don't influence the dependency Proposition 7 is determined by the properties of functional dependency itself VI CONCLUSION In this paper the bit representation of granular computing and rough sets theory are used to study the data querying and rough functional dependency of rough relational database Data querying related to equivalence granules for rough relational database are discussed we divide the data querying into two types certain data querying and possible data querying in addition a new judge method of rough functional dependency based on bit representation is given In the future we will research the theory and applications of rough functional dependency deeply REFERENCES 1 T Beaubouef Uncertainty processing in a relational database model via a rough set representation University Microfilms International A Bell&Howell Information Company PhD dissertation 1994 pp.67-76 2 T Beaubouef F Petry and B Buckles Extension of the relational database and its algebra with rough set techniques Computational Intelligence Vol 11 No 2 May 1995 pp 233-245 3 Yao Y.Y A partition model of granular computing  to appear in LNCS Transactions on Rough Sets 4 L.A.Zadeh Some Reflections on Information Granulation and its Centrality in Granular Computing,Computing with Words the Computational Theory of Perceptions and Precisiated Natural Language In Data Mining Rough Sets and Granular Computing T.Y.Lin Y.Y.Yao,and L.A.Zadeh,Eds Physics Heidelberg,2003,pp.1-19 5 Eric Louie T.Y.Lin A Data Mining Approach using Machine Oriented Modeling Finding Association Rules using Canonical Names In Proceeding of 14th Annual International Symposium Aerospace/Defense Sensing Simulation and Controls SPE Vol 4057 Orlando,2000 April 24-28 pp.148-154 6 T.Y.Lin,Eric Louie Data Mining Using Granular Computing:Fast Algorithms for Finding Association Rules  In Data Mining Rough Sets and Granular Computing T.Y.Lin,Y.Y.Yao,and L.A.Zadeh,Eds Physics Heidelberg,2003,pp.22-42 7 Andrzej Skowron,Jaroslaw Stepaniuk Information Granules:Towards Foundations of Granular Computing International Journal ofIntelligent Systems Vol 16,2001 pp.57-85 8 Andrzej Skowron Stepaniuk J Information Granules in Distributed Environment  New Directions in Rough Sets Data Mining and Granular-Soft Computing Yamaguchi Japan November 9-11 Lecture notes in Artificial Intelligence 1711 Springer-Verlag 1999 pp.357-365 9 Skowron A Stepaniuk J Towards discovery of information granules 3rd European Conference on principles and Practice of Knowledge Discovery in Databases September 15-18 1999 Prague.Czech Republic Lecture Notes in Artificial Intelligence 1704 Springer-Verlag,1999 542-547 10 Andrzej Skowron,Roman Swiniarskj and Piotr Synak Approximation Spaces and Information Granulation".In:4th International Conference RSCTC 2004 Uppsala,Sweden,June 2004 Proceedings pp.116-126 11 T.Y.Lin Association Rules in Semantically Rich Relations Granular Computing approach Proceedings of International Workshop on Rough Set Theory and Granular Computing Bulletin of International Rough Set Society Matsue Shimane Japan May 20-22 2001.Vol 5.No.1/2 pp 143-149 111 


5 appropriate minsup or minconf constraint the number of association rules and avoid the occurrence of some meaningless association rules. The results are as expected and quite consistent with our intuition. T he curves for large minconf \(or minsup moother than those for small ones in Figure 6 \(or Figure 7\meaning that the minsup or minconf had a large effect on the number of association rules when smaller minconfs \(or minsups were used, the above results are same to those in  Figure 6 The relationship between number of association rules and minsup using the proposed method 0 5 10 15 20 0.2 0.25 0.3 0.35 0.4 0.55 minsup minconf=0.55 minconf=0.65 minconf=0.75 minconf=0.85 minconf=0.95 minconf=1 the number of ation rules 0 2 4 6 8 10 12 14 16 18 0.55 0.65 0.75 0.85 0.95 1 minconf minsup=0.2 minsup=0.25 minsup=0.3 minsup=0.35 minsup=0.4 minsup=0.55 Figure 7 er of association rules and minconf  using the proposed method the number of ation rules Let minconf=0.65, the result of comparing the numbers of weighted \(weights are as that in section 3 large itemsets with those of the unweighted \(weights are actually equal to 1\ is shown in Figure 8.It is easy to see that the number of unweighted large itemsets dashed line\s always larger than that of the corresponding one of the weighted \(solid one\whether it is large 1-itemset\(L1\itemset\(L2\e 3itemset\(L3\ation of them, which are as we expect, so meaningless rules can be pruned early by assigning weights to some attributes, which can reduce the execution time of algorithm. In addition, we can find that the less of the items included in the large itemsets, the smoother the corresponding curve is which is natural because higher requirements are asked for the candidate itemsets that include more items more candidate itemsets can not become the corresponding large itemsets Figure 8 Comparison of the numbers of large itemsets of the weighted and unweighted methods 0 10 20 30 40 50 0.2 5 3 0.35 0.4 5 5 minsup unweighted L1 unweighted L2 unweighted L3 unweighted L1+L2+L3 weighted\( as example\L1 weighted\( as example\L2 weighted\( as example\L3 weighted\( as example\L1+L2+L3 Number of large itemsets Figure 9 How the number of large itemsets involvin g specific items var y with an increase in the item's weight 0 5 10 15 20 25 0.2 0.4 0.6 0.8 1 Weight R22 R33 R41 R51 Number of large itemsets involving some ecific item The relationship between the number of large itemsets involving some item and the item\220s weight is shown in Figure 9, when minsup=0.2, minconf=0.65 of the four items  22 R Income-middle 33 R Risk-middle 41 R Creditgood\d 51 R Result-on time\the different curves respectively in Figure 9} vary by an increase, weights of other items are 1 . Figure 9 shows the number of large itemsets with regard to some item increases along with an increase in the item\220s weight, which is also as we expected and verified that users\220 preference\( or the Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


6 ability of guidance\ be added into the procedure of data mining by adjusting weight Next, we selected 550 cases randomly used for inducing rules from 650 cases in the original data set the remaining 100 cases are used for testing accuracy of the induced rules of the proposed method by measuring the average percentage of correct predictions. Our experiment is based on the following conditions: randomly assign a set of weights to each fuzzy region of each attribute, let minsup 0.35, cut set of  the fuzzy set transforming quantitative value to fuzzy value is 0.7. Figure 10 presents how the accuracy of the proposed method varies along with minsup and minconf It is easily seen that the accuracy of the proposed method increases on the whole with an increase in minsup and minconf which is not counterintuitive and shows the contributions \(functions minsup and minconf to accuracy. But there is some variance or exception at the latter part of the curve because some statistics features disappear \(such as cases with only one or two rules left\when minsup is large enough, at the same time, it is just a roughly result of accuracy testing here, because our rule is with linguistic terms, we have to use the cut set technology percentage of correct rules predictions for testing accuracy. Although there is some information lost in the processing conducted some base features of the curves can be seen  Figure 10 The relation between accuracy and minsup for various  values of minconf 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0  35 0 4 0  45 0 5 0  55 0 6 0  65 minsup weighted minconf=0.7 weighted minconf=0.8 weighted minconf=0.9 Accuracy Figure 11 compares the accuracy of the proposed weighted method with an unweighted method \(that is thod in Hong et al h ic h s h o w s t h e  accuracy of unweighted method \(dashed line\ on the whole lower than that of the proposed weighted method \(solid line\o does every pair of corresponding curves under the same minconf In addition, the accuracy of crisp-partition method is lower than that of unweighted method, which has been verified in Hong et al o  w e don\220 t i llus t rate it  again here.  Figure 11 has the same features of Figure 10 for the latter parts of the curves, where the unweighted method has more rules selected than weighted method Figure11 Comparison of the accuracy of the unweighted and weighted methods 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0  35 0  4 0  45 0  5 0  55 0  6 0  65 minsup unweighted f=0.7 unweighted f=0.8 unweighted f=0.9 weighted f=0.7 weighted f=0.8 weighted f=0.9 Accuracy 5 Conclusions and future work The paper proposes a user-guided association rule mining method for quantitative or categorical data. For the proposed method, it is easy to add users\220 guide to data mining procedure, meaningless rules can be pruned early, so it is quicker to find association rules interested by users, and reduce the work burden of choosing interesting rules, in addition, the feature that the proposed method satisfies downward closure property is another reason to decrease computation time. The problem of data overflowing in Gyenes can be avoided in the proposed method because of the adoption of minimum operator. In the proposed algorithm, we assign fuzzy regions to each categorical attribute, which can describe categorical attribute in linguistic language more flexible according to situations especially when the number of categorical regions is large. The proposed method can also resolve conventional binary value problem by using a degraded membership functions and let all weights be 1. In a word, the proposed method is more flexible natural and understandable. Experimental results have showed its good performance, also The construction of membership functions in fuzzy association rules mining has always been a bottleneck our method assumes we have known the membership functions of attributes in advance. We could make the ction of membership functions more open or automatic according to distribution of data or users\220 opinions, based on some existing research on Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


7 automatically deriving membership functions  in Hong et al. [7 or m a k i ng probabil i t y an d fu zz y s e t t h eor y  work in concert such as that in Nozer et al [1 r th e issue of weight, we could consider different weight schemes such as centroid weights terv al-v alue  weights, assigning a weight for each time interval as  did in th eir alg orithm  w i th bin a r y v a l u e. It is also worthy to improve the proposed method to find fuzzy association rules for interval values. All of the above discussions will be interesting topics in near future References 1 R. Ag ra w a l T  I m ielinsk i an d A  S w a m i 001\000 Mining association rules between sets of items in massive databases 001\001 Proceedings of the ACM SIGMOD International Conference on Management of Data Washington DC, USA \(1993\, pp207-216 2 R. A g ra wal an d R Srik an   001\000 Fast algorithms for mining association rules in large databases 001\001 Proceeding of the 20 th VLDB Conference \(1994\, pp 487-499 3 P. Bo sc an d O Piv ert 001\000 On some fuzzy extensions of association rules 001\001 Proceedings of IFSA-NAFIPS 2001 Piscataway, NJ, IEEE Press \(2001\pp 1104-9 4 C.H. Cai, W  C. F u C  H. Ch e n g an d W  W  Kw o n g  001\000 Mining association rules with weighted items 001\001  Proceedings of 1998 International Database Engineering and Applications Symposium, Cardiff, Wales \(1998\ pp 68-77    Gyen esei  001\000 Mining weighted association rules for fuzzy quantitative items 001\001 Proceedings of  PKDD Conference, September 13-16, 2000, Lyon, France 2000\, pp 416-423 6 T  P  Ho n g  C.-S K u o an d S C. Ch i   001\000 Trade-off between computation time and numbers of rules for fuzzy mining from quantitative data 001\001 International Journal of Uncertainty,Fuzziness and Knowledge-based Systems 9 \(2001\, pp 587-604 7 T  P  Ho n g  S.-L W a n g  001\000 Determining appropriate membership functions to simplify fuzzy induction 001\001  Intelligent Data Analysis 4 \(2000\p 51-66 8  C  K uok A  Fu a nd H  W ong  001\000 Mining fuzzy association rules in databases 001\001 ACM SIGMOD Record 27 \(1998\pp 41-46 9 R  L a dne r, F.E P e try a nd M.A C obb 001\000 Fuzzy set approaches to spatial data mining of association rules 001\001  Transactions in GIS 7 \(2003\, pp 123-138 10 S.-F. L u H  H u a n d F. L i  001\000 Mining weighted association rules 001\001 Intelligent Data Analysis 5 \(2001 pp 211-225 11  N o z e r D  J  Si ng purw a lla a n d M B o ok e r  001\000 Membership function and probability measures of fuzzy sets 001\001 Journal of  the American Statistical Association 9 \(2004\, pp 867-877 1   Olso n   001\000 Comparison of Weights in Topsis Methods 001\001 Mathematical and Computer Modeling 40 2004\, pp 721-727 1 J S h u E  Tsan g an d D Yeu n g   001\000 Query fuzzy association rules in relational databases 001\001 In proceedings of IFSA-NAFIPS 2001 Piscataway, NJ IEEE Press \(2001\89-2993 14 R. Srik a n t a n d R. A g ra w a l 001\000 Mining quantitative association rules in large relational tables 001\001 The 1996 ACM SIGMOD International Conference on Management of Data. Montreal,Canada, June \(1996\p 1-12 15 L  A   Za de h 001\000 Fuzzy sets 001\001 Information and Control 8 1965\, pp 338-356 1 W  Z h an g  001\000 Mining fuzzy quantitative association rules 001\001 Proceedings of IEEE International Conference on Tools with Artificial Intelligence 1999 Piscataway NJ,IEEE Press \(1999\, pp 99-110 Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


8 Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


can test whether the hyperlinks on a page have been placed appropriately by analyzing significant navigational patterns \(association rules In our experiments the percentage of hyperlinks confirmed by rules was calculated by dividing the number of common items in hyperlink sets and whole ranking lists for a given page, by the number of hyperlinks on the page separately for direct, indirect and complex rules \(Fig. 8 Note that the number of hyperlinks was put in the denominator as opposed to calculations in section 7.4 where it was ranking length 48 89%87 0 20 40 60 80 100 direct indirect complex Figure 8. The average percentage of all hyperlinks confirmed by rules Proceedings of the 2005 5th International Conference on Intelligent Systems Design and  Applications \(ISDA  05 0-7695-2286-06/05 $20.00  2005 IEEE The average percentage of hyperlinks confirmed by direct rules amounted to only 48%, probably because there were too few of them. Indirect and complex rules, on the other hand, confirmed many more hyperlinks  87% and 89%, respectively, due to their larger quantity. These relatively great values that we received may have resulted from the enormous differences between the average number of hyperlinks on a page  10 and the average ranking length: 51, 177, 180 for direct, indirect and complex rules respectively. Concluding, indirect and complex rules appear to be better at assessing the usefulness of hyperlinks compared to direct rules Note that in any case at least 11% of hyperlinks were not confirmed by any rule, so they may be recommended to be removed from the content of pages 8. Conclusions and future work Complex rules combining both direct and indirect rules usually increase the length of rankings compared to those based on direct associations. This helps overcome the problem of a multitude of pages with too short rankings Fig. 5 quested ranking length \(Fig. 6 rules substantially change the order of ranking lists \(Fig. 2 and 3 greater extent only confirm hyperlinks existing on web pages compared to lists extracted from complex rules, for short and long ranking lengths \(Fig. 7 of rules, especially indirect and complex ones, can be useful for the assessment of hyperlinks Concluding, far more diverse indirect rules can significantly improve potential value of recommendation. Nevertheless, to confirm the usefulness of indirect rules for end users, some tedious tests with their participation are required Acknowledgements The authors are indebted to thank Marcin Pilarczyk for providing cleansed data about hyperlinks on WUT pages 9. References 1] Agrawal R., Imieli?ski T., Swami A.: Mining association rules between sets of items in large databases. ACM SIGMOD Int. Conference on Management of Data, ACM Press 1993 2] Boley D., Gini, M., Gross, R., Han, E.H., Hastings, K Karypis, G., Kumar, V., Mobasher, B., Moorey, J.: Document Categorization and Query Generation on the World Wide Web Using WebACE. Artificial Intelligence Review 


Wide Web Using WebACE. Artificial Intelligence Review 13 \(5-6 1999 3] Cho Y.H., Kim J.K., Kim S.H.: A personalized recommender system based on web usage mining and decision tree induction. Expert Systems with Applications 23 \(3 2002 4] Chun J., Oh J.-Y., Kwon S., Kim D.: Simulating the Effectiveness of Using Association Rules for Recommendation Systems. AsiaSim 2004. LNCS 3398, Springer Verlag 2005 5] G  ry M., Haddad M.H.: Evaluation of web usage mining approaches for user's next request prediction. WIDM 2003 ACM Press \(2003 6] Ha S.H.: Helping Online Customers Decide through Web Personalization. IEEE Intelligent Systems 17 \(6 2002 43 7] Hamano S., Sato M.: Mining Indirect Association Rules ICDM 2004. LNCS 3275, Springer Verlag \(2004 8] Kazienko P.: IDARM - Mining of Indirect Association Rules. IIS: IIPWM  05. Advances in Soft Computing Springer Verlag \(2005 9] Kazienko P.: Multi-agent Web Recommendation Method Based on Indirect Association Rules. KES  2004. Part II LNAI 3214, Springer Verlag \(2004 10] Kazienko P., Product Recommendation in E-Commerce Using Direct and Indirect Confidence for Historical User Sessions. DS  04. LNAI 3245, Springer Verlag \(2004 269 11] Kazienko P., Kiewra M.: Link Recommendation Method Based on Web Content and Usage Mining. IIS: IIPWM  03 Advances in Soft Computing, Springer Verlag \(2003 534 12] Kazienko P., Kiewra M., Personalized Recommendation of Web Pages. Chapter 10 in: Nguyen T. \(ed Technologies for Inconsistent Knowledge Processing. Advanced Knowledge International, Adelaide, South Australia 2004 13] Kazienko P., Kolodziejski P.: WindOwls - Adaptive System for the Integration of Recommendation Methods in Ecommerce. AWIC  05, LNAI, Springer Verlag \(2005 14] Kazienko P., Matrejek M.: Adjustment of Indirect Association Rules for the Web. SOFSEM 2005. LNCS 3381 Springer Verlag \(2005 15] Kendall, M. G.: Rank correlation methods. London: Charles Griffin &amp; Company, Ltd., London \(1948 16] Lu Z., Yao Y., Zhong N.: Web Log Mining. Chapter 9 in Zhong N., Liu J., Yao Y. \(eds Berlin, New York \(2003 17] Mobasher B., Cooley R., Srivastava J.: Automatic Personalization Based on Web Usage Mining. Communications of the ACM, 43 \(8 2000 18] Tan P.-N., Kumar V.: Mining Indirect Associations in Web Data. WEBKDD 2001. LNCS 2356, Springer Verlag \(2002 145-166 19] Tan P.-N., Kumar V., Srivastava J.: Indirect Association Mining Higher Order Dependencies in Data. PKDD 2000 LNCS 1910, Springer Verlag \(2000 20] Wan Q., An A.: Efficient Mining of Indirect Associations Using HI-Mine. AI 2003. LNCS 2671, Springer Verlag 2003  221 21] Wang D., Bao Y., Yu G., Wang G.: Using Page Classification and Association Rule Mining for Personalized Recommendation in Distance Learning. ICWL `02. LNCS 2436 Springer Verlag \(2002 22] Yang H., Parthasarathy S.: On the Use of Constrained Associations for Web Log Mining. WEBKDD 2002. LNCS 2703 Springer Verlag \(2003  118 Proceedings of the 2005 5th International Conference on Intelligent Systems Design and  Applications \(ISDA  05 0-7695-2286-06/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


lossless frequent patterns representations with borders. In practice, they are also much more concise than representations based on closed itemsets and approximate representations. In this paper, we offered three methods of deriving an upper bound on the length of sets in such representations. We proved that the upper bound on the length of a longest generalized disjunction-free set depends logarithmically on the number of records in the database. The obtained result is of high importance as it guarantees that any generalized disjunction-free set representation for all patterns \(both frequent and infrequent scans, where n is the number of records in the database irrespectively how strong or weak correlations among items in the database are and irrespectively of the lengths of records and number of distinct items The modifications of the basic estimation take into account the support threshold of the representation to be found or, additionally, the information on the length of longest sets of the representation already calculated for a higher support threshold. Though these estimations are more accurate than the basic one, they are still quite rough. Further improvements of the quality of estimating the length of longest itemsets in generalized disjunctionfree representations is subject to further research References 1] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, A.I Verkamo  Fast Discovery of Association Rules   Advances in Knowledge Discovery and Data Mining AAAI, CA, 1996 2] J. Baptiste, J.-F. Boulicaut  Constraint-Based Discovery and Inductive Queries: Application to Association Rule Mining  Pattern Detection and Discovery, Springer London, UK, September 2002, pp. 110-124 3] E. Baralis, S. Chiusano, P. Garza  On Support Thresholds in Associative Classification  SAC, ACM, Taipei Taiwan, March 2004, pp. 553-558 4] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, L. Lakhal  Mining Minimal Non-redundant Association Rules Using Frequent Closed Itemsets  Computational Logic, 2000 pp. 972  986 5] J.-F. Boulicaut, A. Bykowski, C. Rigotti  Approximation of Frequency Queries by Means of Free-Sets  PKDD Springer, Lyon, France, September 2000, pp. 75-85 6] A. Bykowski, C. Rigotti  A Condensed Representation to Find Frequent Patterns  PODS, ACM, Santa Barbara USA, May 2001, pp. 267-273 7] T. Calders, Axiomatization and Deduction Rules for the Frequency of Itemsets, Ph.D. Thesis, University of Antwerp, 2003 8] T. Calders, B. Goethals  Mining All Non-derivable Frequent Itemsets  PKDD, Springer, Helsinki, Finland August 2002, pp. 74?85 9] T. Calders, B. Goethals  Minimal k-free Representations of Frequent Sets  ECML/PKDD, Springer, CavtatDubrovnik, Croatia, September 2003, pp. 71-82 10] J. Han, M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers, 2000 11] S.K. Harms, J. Deogun, J. Saquer, T. Tadesse  Discovering Representative Episodal Association Rules from Event Sequences Using Frequent Closed Episode Sets and Event Constraints  ICDM, IEEE Computer Society, San Jose, California, USA, November-December 2001, pp. 603  606 12] M. Kryszkiewicz  Closed Set Based Discovery of Representative Association Rules  IDA, Springer Cascais, Portugal, September 2001, pp. 350-359 13] M. Kryszkiewicz  Concise Representation of Frequent Patterns based on Disjunction  free Generators  ICDM IEEE Computer Society, San Jose, California, USA 


IEEE Computer Society, San Jose, California, USA November-December 2001, pp. 305  312 14] M. Kryszkiewicz M  Inferring Knowledge from Frequent Patterns  Soft-Ware, Springer, Belfast, Northern Ireland April 2002, pp. 247  262 15] M. Kryszkiewicz  Concise Representations of Association Rules  Pattern Detection and Discovery, Springer London, UK, September 2002, pp. 92-109 16] M. Kryszkiewicz, Concise Representation of Frequent Patterns and Association Rules, Habilitation Thesis Publishing House of Warsaw University of Technology 2002 Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE 17] M. Kryszkiewicz  Reducing Infrequent Borders of Downward Complete Representations of Frequent Patterns  Proc. of The First Symposium on Databases Data Warehousing and Knowledge Discovery, Scientific Publishers OWN, Baden-Baden, Germany, July, 2003, pp 29-42 18] M. Kryszkiewicz  Closed Set Based Discovery of Maximal Covering Rules  International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems Vol. 11. World Scientific Publishing Company, Singapore September 2003, pp. 15-29 19] M. Kryszkiewicz  Reducing Borders of k-Disjunction Free Representations of Frequent Patterns  SAC, ACM Taipei, Taiwan, March 2004, pp. 559  563 20] M. Kryszkiewicz  Reducing Main Components of k-Disjunction Free Representations of Frequent Patterns   Proc. of IPMU  2004 \(in print 21] M. Kryszkiewicz, M. Gajek  Concise Representation of Frequent Patterns based on Generalized Disjunction  Free Generators  PAKDD, Springer, Taipei, Taiwan, May 2002, pp. 159-171 22] M. Kryszkiewicz, M. Gajek  Why to Apply Generalized Disjunction-Free Generators Representation of Frequent Patterns  ISMIS, Springer, Lyon, France, June 2002, pp 383  392 23] M. Kryszkiewicz, H. Rybi?ski, M. Gajek  Dataless Transitions between Concise Representations of Frequent Patterns  Journal of Intelligent Information Systems JIIS Netherlands, 2004, pp. 41-70 24] N. Pasquier, Data mining: Algorithmes d  extraction et de R  duction des R  gles d  association dans les Bases de Donn  es, Th  se de Doctorat, Universit  Blaise Pascal   Clermont  Ferrand II, 2000 25] N. Pasquier, Y. Bastide, R. Taouil, L. Lakhal  Efficient Mining of Association Rules Using Closed Itemset Lattices  Journal of Information Systems, Vol. 24, No. 1 1999, pp. 25  46 26] N. Pasquier, Y. Bastide, R. Taouil, L. Lakhal  Discovering Frequent Closed Itemsets for Association Rules  ICDT, Springer, Jerusalem, Israel, January 1999 pp. 398  416 27] J. Pei, G. Dong, W. Zou, J. Han  On Computing Condensed Frequent Pattern Bases  ICDM, IEEE Computer Society, Maebashi City, Japan, December 2002 pp. 378-385 28] V. Phan-Luong V  Representative Bases of Association Rules  ICDM, IEEE Computer Society, San Jose California, USA, November-December 2001, pp. 639-640 29] A. Savinov  Mining Dependence Rules by Finding Largest Itemset Support Quota  SAC, ACM, Taipei Taiwan, March 2004, pp. 525-529 30] M.J. Zaki, C.J. Hsiao  CHARM: An Efficient Algorithm for Closed Itemset Mining  SIAM, Arlington, 2002 


for Closed Itemset Mining  SIAM, Arlington, 2002 Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207–216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Int’l Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Int’l Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





