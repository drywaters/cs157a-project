A Low-cost Approach for Effective Shape-based Retrieval and Classification of Medical Images Joaquim C. Felipe 1 Olioti 1 Department of Physics and Mathematics University of S\343o Paulo at Ribeir\343o Preto - Brazil jfelipe@ffclrp.usp.br Agma J. M. Traina 2 Ribeiro 2 Sousa 2 Caetano Traina Jr 2 2 Department of Computer Science University of S\343o Paulo at S\343o Carlos - Brazil agma|mxavier|parros|caet  i c m c  us p br  Abstract This work aims at developing an efficient support for retrieval and classification of medical images introducing an approach that comprises techniques of image processing, data mining and fractal theory leading to an effective and direct way to compare images. A method of feature extraction and comparison is proposed, which uses Zernike moments for invariant pattern recognition as shape features of images' regions of interest. A new algorithm that generates statisticalbased association rules is used to identify representative features that discriminate the disease classes of images In order to minimize the computational effort, another new algorithm, based on fractal theory, is applied to reduce the dimensionality of the representative feature space. In essence, the proposed method determines the smallest set of relevant features that can properly represent images without loss of precision. In addition s the need of image segmentation leading to a simple but effective way to make image retrieval by content. Experiments executing k-nearest neighbor queries on medical images reveal that the process is robust and suitable to perform retrieval combined with classification of this kind of images 1. Introduction There are two main approaches to query an image riptive \(textual and the search procedure finds the related images based on these descriptions. A human specialist on the field must produce the descriptions, which is time consuming nds on the when he/she was describing the image, not on the objective when the query was issued. The second approach \205 the so-called Contentbased Image Retrieval \(CBIR\ \205 uses a set of algorithms to automatically extract relevant features d together with the image. CBIR techniques use the intrinsic visual features of images, such as color shape and texture to organize and retrieve them  based on structures that use similarity of features to make comparisons. CBIR techniques allow performing automatic indexing and retrieval of images, in most ng the intervening of the user In some applications, combining both textual and intrinsic information can lead to interesting results 4  Ev e n in th is c a s e th e  d e v e lo p m e n t o f  e f f i c i e n t methods to deal with intrinsic features and to perform similarity comparisons is crucial to reach good results An example of a large application of CBIR can be seen in Hospital Information Systems [7  w ith th e u s e o f  Picture Archiving and Communication Systems \(PACS where it is necessary to keep together all information concerning patients \(texts, images, graphs, videos, etc The huge amount of digital images produced in hospitals and health care centers demands the development of automatic mechanisms for their storage and retrieval from databases. Adding CBIR capabilities to PACS increases their value to assist diagnosis and allows easier and more efficient manipulation and organization of the stored images  In some contexts, the intrinsic features of images can be used to split them into a predefined number of s. An example is the classification of tumoral masses detected in mammograms, as benign or malignant. The radiologist makes this classification, at an initial evaluation, based on the shape presented by the lesion. Malignant tumors Proceedings of the Seventh IEEE Internat ional Symposium on Multimedia \(ISM\22205 0-7695-2489-3/05 $20.00 \251 2005  IEEE 


generally infiltrate the surrounding tissue, resulting in an irregular or hard-distinguishable contour, while benignant ones have a smooth contour \(Figure 1 ly employed to represent and analyze medical images us ua l l y  addressing specific contexts Figure 1. Typical breast tumor masses: benign \(left\nd malign \(right CBIR systems often face the challenge of dealing with the "dimensionality curse". Beyer [2 p r o v e s th at an  increase in the number of features \(consequently in the space dimensionality\ leads to a loss of significance in the ature values. Thus, in order to avoid a decrease in the accuracy, it is important to keep the number of features as low as possible This paper presents a new approach to determine a minimal representative set of features. Zernike moments calculated from the pixels values of images retain pattern information related to shape. We have chosen this technique  moments present advantages over other techniques for image atures \(see Section 2.1 Before the feature extraction, the images are preprocessed \(auto-leveled and filtered\ in such a way that the moments can be extracted without the need of previous image segmentation. This is an important goal, because the segmentation process, besides been very costly, adds imprecision to the resulting representation of the image A new algorithm \(StARMiner \205 Statistical identifies the most relevant features to discriminate images into for instance through mining association rules. The algorithm uses statistical measures that describe the behaviour of the attributes, considering the image categories, in order to find rules of interest. An experiment with images of tumoral masses of mammograms compares the accuracy of StARMiner with the well-known C4.5 decision tree inducer. The most relevant features, in this case, are the moments that hold the shape details responsible for the discrimination between classes In addition, a new technique based on the theory of fractals \(FD-ASE algorithm\ is used to perform ors. Considering tal dimension of the data set, the algorithm finds correlations between attributes and determines a set of independent ones discarding the others A feature extraction tool was developed to execute k nearest neighbor queries, where a query image is selected by the user and compared with images stored in a allowing the user to simultaneously retrieve and classify images The performance of the method is evaluated by an experiment where a 250 previously classified tumoral medical image database is used to verify the matching among the type of tumor of the query image and the retrieved ones The remainder of this paper is structured as follows Section 2 introduces background concepts and related techniques, while Section 3 presents the proposed method Section 4 discusses the experiments and results reached with the developed system. Finally, Section 5 presents the conclusions of this work 2 Background and Techniques This Section briefly presents concepts related to the techniques that support the method described in Section 3 2.1 Zernike Moments Moments and functions of moments can be s. Such features capture global information about the image and do not require closed boundaries as boundarych as Fourier descriptors do  Zernike polynomials provide a precise mathematical model that captures global shape while preserving enough information, by local harmonics. They form a complete rcle x 2 y 2 1 Zernike moments of an image are the projections of the image map of pixels onto these base functions and, if properly implemented, can be invariant with respect to the image size, rotation and translation The equations related to representation of the moments can be found in [5   T h e or d e r of on e m o m e n t c o r r e s p on d s t o  its capability to describe details \(high spatial frequency components\ of the pixel distribution of the image Zhang and Lu [12 c o mp ar e d s o me  s h ap e  ted for CBIR ace descriptors Zernike moments and grid descriptors. The strengths and limitations of them were analyzed against properties such as affine invariance, robustness compactness, low computation complexity and perceptual similarity measurement. Zernike moments have reached the best results of overall evaluation Proceedings of the Seventh IEEE Internat ional Symposium on Multimedia \(ISM\22205 0-7695-2489-3/05 $20.00 \251 2005  IEEE 


Tech and Chin s c r i b e d t h e e v a l ua t i o n o f  various types of moments, considering information ncy and noise sensibility, and concluded that Zernike moments represent images better 2.2 Relevant Features Selection Mining association rules is a task that has s. The majority of the works addressing this task considers the use of categorical items. However, in CBIR context, the mining process deals with images feature vectors, usually consisting of numerical and continuous values, and thus a suitable approach to find association rules in this context should consider quantitative data StARMiner algorithm extends the statistical rule ques presented in [1 to f i n d  p a tte r n s  o n  images. The goal of StARMiner is to implement statistical rules mining to find the features that best discriminate images into categorical classes. A rule has the format x 000\306 A i and it is identified only if the following conditions are satisfied 200 ute A i in images of category x must be different from its behavior in images of other categories 200 The attribute A i must present a uniform behavior in images of category x  The previous conditions are implemented in StARMiner algorithm incorporating restrictions of interest in the mining process. Let T be a database of medical images x an image category T x 001 T the subset of images of category x and A i an attribute. The restrictions of interest implemented in StARMiner algorithm are 1\ |AvgA i T x vgA i T-T x  002 mindif where AvgA i Z is the arithmetic average of A i values in the Z subset of images mindif cates the minimum allowed difference between the average of A i in images of category x and the average of A i in the remaining images of the database 2\ Hypothesis test. H0 should be rejected with a confidence equal our greater than minconf with: H0: AvgA i T x AvgA i T-T x  H1: AvgA i T x  000\217 AvgA i T-T x  where minconf cates the minimum confidence to reject the H0 hypothesis 3 0011 Ai\(T x  003 maxstd where 0011 Ai T x is the standard deviation of attribute A i values in the subset of images T x  maxstd is the input parameter that indicates the maximum standard deviation of A i values allowed in images of category x  The use of StARMiner algorithm to find rules in an image dataset allows finding rules involving attributes with high discrimination power, since the rule attributes have a particular and uniform behavior in images of a given category. The attributes that present a uniform independently of the image category, do not contribute to the category discrimination and should be eliminated 2.3 Attribute Correlations High-dimensional complex data, such as images and videos, usually suffer from the \215dimensionality grades a variety of algorithms s processes. Nevertheless, the existence of correlated attributes in high-dimensional datasets is very common, thus allowing the use of dimensionality reduction approaches to minimize the influence of the dimensionality curse This section presents an overview of an attribute selection technique, named FD-ASE Attribute Significance Estimator based on the Fractal Dimension  which finds a subset of relevant attributes of the dataset and also identifies groups of correlated attributes [9    Th e  c dimension D of a dataset, that is, the dimensionality of the object represented by the data, regardless of the dimension of the space where it is embedded. For instance, the intrinsic dimension of a set of points disposed along a line is one no matter if the set is embedded in any higher dimensional space. In essence, the FD-ASE finds groups of correlated attributes, identifying in each group a subset of base attributes, i.e., attributes to which every other attribute in the group is correlated ea supporting the discovery of correlated attributes is to calculate the intrinsic dimension of incremental sequences \(S i of attributes defined through forward attribute inclusion, and use rence between D nces to identify the existence or absence of attribute correlations. To put it simply, consider a dataset A={a 1 a 2 203 a E composed of E attributes and a sequence of attributes S i 001 A, such that D S i notes the intrinsic dimension of the dataset considering only the attributes in S i An attribute a k 004 A - S i s somehow correlated to at least one attribute of S i if Proceedings of the Seventh IEEE Internat ional Symposium on Multimedia \(ISM\22205 0-7695-2489-3/05 $20.00 \251 2005  IEEE 


adding a k to S i causes no meaningful change in D S i  The particular attributes to which a k are correlated to are discovered by comparing the values D S i 005 a k nd D S i 005 a k a i  006 a i 004 S i such that a significant difference means that attribute a i is not correlated to any attribute in S i A threshold in [0,1 is u s e d to  determine the significance of a change in the value of D s below the threshold are not considered meaningful. As a rule of thumb, lower thresholds are used to identify strong correlations, such as linear ones, while higher thresholds are used to identify weak correlations, such as non-polynomial 3. Proposed Method The proposed method aims at determining a set of moments\ that can be used as discriminators to classify images and, at the same time, as a minimal feature set to define a similarity measure to retrieve images based on shape resemblance. As shown in Figure 2, the method is applied in three steps Figure 2. Steps for feature characterization and CBIR 1 Extraction of moments An Image Training Set, consisting of images that represent each image class of the database, is submitted to a feature extractor, which generates a feature vector for each image, containing its Zernike moments In order to obtain invariance to translation, rotation and scale, the algorithm that calculates the moments considers the center of mass of the image and defines a normalizing it to the interval [0 ,1   2 Definition of relevant moments For order 30, a set of 256 moments is generated for each image in Step 1. How many of them are relevant to discriminate the images into the initial classes? To answer this question, it is necessary to verify the relevance of each moment to the classification of the images The set of feature vectors, together with the previously known class of each training image, is used by StARMiner to produce a set of statistical association rules, where the most relevant moments to discriminate the classes take part in the confident rules in the resulting set of rules. These ature vector of each image is constructed using these values 3 Determination of representative moments Despite being relevant to image classification, the ndencies on each other. Thus, in this step the set of dependent moments are determined, as well as the amount of ndency that they carry The relevant moments of the image set \(generated in Step 2\ are submitted to the algorithm FD-ASE. It returns a set of attributes that are representative of the dataset, based on the contribution of each attribute to the dataset intrinsic dimension. Attributes that do not significantly change the intrinsic dimension are ered dependent of others and then discarded Steps 2 and 3 are both necessary to reach the minimal  cation, but some of them can be dependent of others and thus not necessary to compare images. If we only apply Step 3, we get a set of independent moments, but not necessarily all of them are relevant to classify the images s is reduced in one order of magnitude \(from hundreds to dozens\. Only these selected moments will compose the image feature vectors to be used to index and retrieve the images in the CBIR environment. This approach, besides reducing the computational cost to extract the moments makes the searching time suitable to an operational ry large and the time spent with retrieval processes can be critical 4. Experimental Studies Initially, a feature extraction tool \205 Zernike Extractor s from images and to perform k nearest neighbor queries A database consisting of 250 images was used to test and validate the proposed method. The images are ROIs \(Regions of Interest\ comprising tumoral masses, taken from mammographies.  Although this amount of images is significantly smaller than the real size of a medical database, it is a common practice to use sample images during the task of evaluation and diagnosis in medical environments. Therefore, 250 images are enough to encompass a set of typical images from pathological and non-pathological  CBIR Process Relevant Moments Image Training Set Moments Training Set Zernike Extractor Representative Moments Same-class and Shape-similar images Zernike Extractor C4.5 MDE Image DB StARMiner FD-ASE Feature Extractor  STEP 1   2   STEP 3  Proceedings of the Seventh IEEE Internat ional Symposium on Multimedia \(ISM\22205 0-7695-2489-3/05 $20.00 \251 2005  IEEE 


groups, and to be used to choose the relevant moments and also to test the accuracy reached by the method given by a human specialist s are classified either as benign or s done by radiologists and eventually confirmed by complementary exams. These kinds of lesion are tightly related to the image shape:  benign masses have well-defined smooth contours, while the malignant ones have their contours spread over the mammary parenchyma 4.1 Pre-processing of Images regions and execute the extraction of moments without a previous segmentation, which is mandatory when objects shape is employed to retrieve images, the Zernike Extractor executes some basic pre-processing procedures a\ Auto-level: stretching of the maximum and minimum gray levels of the image to the maximum interval \(0-255\ This is important because the majority of mammography ROIs is dim b Reduction of the number of levels: the gray levels are reduced to 12. The complete interval [0 i s  partitioned into 12 intervals and gray values that are in the same interval receive the same new value new_gray 000O old_gray  12  256 000P c Median filter: this filter reduces white noise without affecting the object contours, making the image regions more homogenous  these three processes can be applied at a unique scan on the grid of pixels of the image 4.2 Representative Moments The 3-step method described in Section 3 was executed over an image training set consisting of 90 images selected from the initial database In Step 1, moments of order 30 were extracted nt of 256 moments for each image Step 2 was executed twice: first using the wellknown C4.5 decision tree generator a n d t h e n  using the StARMiner. It was done in order to compare the performance of both algorithms. C4.5 generated a decision tree, where 38 moments were identified as relevant. StARMiner generated a set of association rules, where other 38 moments were identified as the most relevant In Step 3, the 38 relevant moments from both C4.5 and StARMiner were processed by FD-ASE algorithm FD-ASE uses a threshold parameter that determines the limit of influence that makes a feature dependent or not of another. For this dataset, the value of threshold was 0.5 s of influence had determined a set of 18 representative moments from the set of C4.5 38 relevant moments and another set of 16 representative moments from the set of StARMiner 38 relevant moments 4.3 Results K nearest neighbor queries were applied to the images from the database, taking randomly the query images and varying the values of k for all feature sets generated in Steps 1, 2 and 3 Figure 3 presents curves of precision vs. recall obtained with 256 moments \(Step 1\ 38 moments from C4.5, 38 moments from StARMiner \(Step 2\ 18 moments from FD-ASE after C4.5 and 16 moments from FD-ASE after StARMiner \(Step 3 Figure 3. Precision x recall: 256, 38, 18 and 16 moments By analyzing the graph, we conclude that 200 The results with 38 moments are better than with 256 moments for both StARMiner and C4.5. The dimensionality reduction of Step 2 has provided an nd 0.8\in r 0.2. The regions of low recall are the most important into a PACS environment because k nearest queries es of k These results testify that the dimensionality curse really damages the results: the irrelevant features disturb nce of the relevant ones 200 StARMiner reaches a better performance than C4.5. It maintains higher precision with values over than 0.7 for all recall values less than 0.5            0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 6 0.8 1.0 recall p r e c is io n 256 MOMENTS 38 MOMENTS C4.5 38 MOMENTS StARMiner 18 MOMENTS C4.5 + FD-ASE 16 MOMENTS StARMiner FD-ASE 256 MOMENTS 38 MOMENTS StARMiner 16 MOMENTS StARMiner FD-ASE 38 MOMENTS C4.5 18 MOMENTS C4.5 + FD-ASE Proceedings of the Seventh IEEE Internat ional Symposium on Multimedia \(ISM\22205 0-7695-2489-3/05 $20.00 \251 2005  IEEE 


while with C4.5 this value of recall is around 0.3 200 The dimensionality reduction of Step 3 does not rformance with 16 moments is the same as with 38 moments for StARMiner. The same occurs with C4.5 In order to verify that the final feature set is the minimal set that maintains the precision, one moment were removed from each feature set, resulting in 15 moments for StARMiner and 17 moments for C4.5 Figure 4 shows that precision suffers a critical decrease proving that the feature sets that were generated in Step 3 are the minimal representative ones Figure 4. Precision x recall: 18, 17, 16 and 15 moments 5. Conclusions A new method that combines classification and shape similarity retrieval of images has been presented. This s to retrieve shape description, statistical association rules to identify relevant attributes and fractal theory to reduce dimensionality. The experimental approach applies a preprocessing procedure that discards the need of image and the tool developed allow the user to execute k nearest neighbor queries retrieving the most suitable images, that is, the ones that belong to the same class of the query image and at the same time, are similar in shape to it The results of searching a database consisting of ROIs of breast tumoral masses show that a significant reduction in the number of features \205 from 256 to 16 moments \205 can be applied, leading to an increase to the accuracy of the method. The precision values es under 20%. The experimental results also show that the final set consisting of 16 moments is the minimal one that maintains the accuracy for classification tasks 6. References 1  A u m a nn  Y  and Y  L i ndel l   A St at i s t i c al T h eo r y f o r  Quantitative Association Rules Fifth SIGKDD International Conference on Knowledge Discovery and o California. 1999 2  B e y e r   K   J G o dst e i n  R  R a m a kr i s hnan and U  Shaf t   When is "Nearest Neighbor" Meaningful?", In Proc. of International Conference on Database Theory \(ICDT Jerusalem, Israel, January 1999, pp. 217-235 3  B u eno  J M et al    H o w  t o  A dd C o nt ent based I m ag e Retrieval Capability in a PACS In IEEE International Conference on Computer Based Medical Systems  Maribor, Slovenia, 2002 4  I n o u e M and N  U e da  R et r i ev i n g L i g h t l y A nno t a t e d Images using Image Similarities", in ACM Symposium g 2005. Santa Fe - USA 5  K h o t anz ad A  and Y  H   H o ng    I nv ar i a nt  I m ag e Recognition by Zernike Moments IEEE Transactions on Pattern Analysis and Machine Inteligence vol. 12 no. 5, p. 489-497, 1990 6  M l s n a   P  A   a n d N  M  S i ra k o v   I n t e llig e n t S h a p e  Feature Extraction and Indexing for Efficient ContentBased Medical Image Retrieval". in IEEE Southwest Symposium on Image Analysis and Interpretation  2004. Lake Tahoe, Nevada, USA 7  M\374l l e r  H  et al    A R e v i ew o f C o nt ent based I m ag e Retrieval Systems in Medical Applications - Clinical Benefits and Future Directions International Journal of Medical Informatics vol. 73, pp. 1-23, 2004 8  Q u i n l a n J R    I nduc t i o n o f D e c i si o n T r ees   Machine Learning pp. 81-106, 1986 9  S o usa  E   P   M  et  al    H o w t o U s e F r ac t a l D i m e nsi o n t o  Find Correlations between Attributes", In rkshop on Fractals and Self-similarity in Data Mining in  ternational Conference on Knowledge Discovery & Data Mining  Canada. 2002. pp. 26-30   T e c h  C  H  and R  T  C h i n   O n I m ag e A n al y s i s  by  Methods of Moments IEEE Transactions on Pattern Analysis and Machine Inte lligence vol. 10 no. 4, pp 496-513, 1988 11  V a ila y a  A   e t  a l  I m a g e C l a s s i f i c a tio n f o r C o n t e n tbased Indexing IEEE Transactions on Image Processing vol. 10, no. 1, pp. 117-130, 2001   Z h ang  D  S  and G   L u    C o n t e nt B ased S h ape R e t r i e v a l  Using Different Shape Descriptors: A Comparative Study In Proc IEEE International Conference on Mu ltimedia and Expo Tokyo, Japan, pp.317-320, August 2001 Acknowledgments This research has been supported, in part, by the Brazilian National Research Council \(CNPq\ under grants 52.1685/98-6, 860.068/00-7 and 35.0852/94-4 and by the Sao Paulo State Research Foundation FAPESP\ under grant 04/02215-5         0.0 0.2 0.4 0.6 0.8 1.0 0.00.20.40.60.81.0 recall p r eci si o n 16 MOMENTS StARMing FD-ASE 15 MOMENTS StARMiner FD-ASE 18 MOMENTS C.45 + FD-ASE 17 MOMENTS C4.5 + FD-ASE 16 MOMENTS StARMiner FD-ASE 18 MOMENTS C4.5 + FD-ASE 17 MOMENTS C4.5 + FD-ASE 15 MOMENTS StARMiner FD-ASE Proceedings of the Seventh IEEE Internat ional Symposium on Multimedia \(ISM\22205 0-7695-2489-3/05 $20.00 \251 2005  IEEE 


I] Hu. K. and Xia S.W., Data mining based on large data warehouse, Journal of software, Vol 9,No.l,pp.53-63,Jan.1998 2] Agrawal. R., Mining association rules between sets of items in large database, Proc. ACM SIGMOD int  l conf. Management of data, Washington DC, pp 207-216.May.1993 3] Alex. B. and Stephen. IS., Data warebouse, Data mining and OLAP, McGraw-Hill Book Co. 1999 4] Chen. M.S. and Philip. S., Data mining: an overview from database perspective, IEEE Transaction on knowledge and data engineering pp.866-883 Aug.1996 Roberto. J., Efficiently mining long pattems from databases, Proceedings of the 1998 ACM-SIGMOD int  l conf. on management of data pp.85-93,1998 6] Duda. R.O., Hart. P.E. and Stork. D.J., Pattem recognition, Wiley, New York, 2001 7] Zaki, M.J., Ogibara. M. and Li. W., New algorithms for fast discovery of association rules, Proceedings of the third Int  l conf. on knowledge discovery in database and data mining, pp.283-286.1997 8] Goulboume. G.,  Coenen. F. and Leng. P., Algorithms for computing association rules using a partial-support tree, Knowledge-Based Systems, Vol 13 pp.141-149,2000 9] Yingwu Fang, Guangpeng Zhang, Dewei Wu and Wang Yi, Research on distributive data mining calculating process-DDCP algorithm, Joiirnal of university of elechonic science and technology, Vo132 No. 1, pp.80-84, Feb.2003 5 1660 pre></body></html 


It should be noted that after the above process the resulting support constraint set may become inconsistent Thus in the next round the value c   m i 1 z i may be larger If that happens the larger value c does not interpret as the privacy condence level Instead it should be interpreted as an indicator for inconsistency of the support constraint set Thus the above privacy deletion procedure should only be carried out one time We should note that even if the condence level is higher that is c   m i 1 z i is small there is still possibility that the condential information specied by  I,s,S  is leaked in theory That is for each transaction database D that satises the constraints S wehave support  I D    s S   However no one may be able to recover this information since it is NP hard to infer this fact Support constraint inference has been extensively studied by Calders in 2 3 It would be interesting to consider conditional privacypreserving synthetic transaction database generations That is we say that no private information is leaked unless some hardness problems are solved efciently This is similar to the methodologies that are used in public key cryptography For example we believe that RSA encryption scheme is secure unless one can factorize large integers In our case we may assume that it is hard on average to efciently solve integer linear programs Based on this assumption we can say that unless integer linear programs could be solved efciently on average no privacy specied in P is leaked by S if the computed condence level c   m i 1 z i is small 5 Conclusions In this paper we discussed the general problems regarding privacy preserving synthetic transaction database generation for benchmark testing purpose In particular we showed that this problem is generally NP hard Approximation algorithms for both synthetic transaction database generation and privacy leakage condence level approximation have been proposed These approximation algorithms include solving a continuous variable linear program According to 6 l i n ear probl ems ha vi ng hundreds of t housands of continuous variables are regularly solved Thus if the support constraint set size is in the order of hundreds of thousands then these approximation algorithms are efcient on regular Pentium-based computers If more constraints are necessary then more powerful computers are needed to generate synthetic transaction databases References 1 R  A g r a w al T  Imilien sk i an d A  S w a mi Min in g association rules between sets of items in large databases In Proc of ACM SIGMOD International Conference on Management of Database  pages 207216 1993  T  C a lders  Axiomatization and Deduction Rules for the Frequency of Itemsets  PhD Thesis Universiteit Antwerpen 2003  T  C a l ders  C omput at i onal compl e x i t y of i t e ms et frequency satisability In Proc 23rd ACM PODS 04  pages 143154 ACM Press 2004  R  F agi n J  Hal pern and N Me gi ddo A l ogi c f or reasoning about probabilities Information and Computation  87 1,2\78128 1990  G Geor gak opoul os  D  K a v v a di as  a nd C  P a padi mitriou Probabilistic satisability J of Complexity  4 111 1988  Li near P r ogrammi ng F r equent l y As k e d Q ues t i ons  http://www-unix.mcs.anl.gov/otc Guide/faq/linear-programming-faq html  T  M ielik  ainen On inverse frequent set mining In Proc of 2nd Workshop on Privacy Preserving Data Mining PPDM  pages 1823 IEEE Computer Society 2003  C  P o t t s  A nal ys i s of a l i n ear programmi ng heuri s t i c for scheduling unrelated parallel machines Discrete Appl Math 10 155164 1985 9 G  R amesh  W  Man iatty  a n d M Zak i Feasib le itemset distributions in data mining theory and application In Proc 22nd ACM PODS  pages 284295 2003  Y  W a ng X W u  a nd Y  Zheng P r i v ac y p res ervi ng data generation for database application performance testing In Proc 1st Int Conf on Trust and Privacy in Digital Business TrustBus 04 together with DEXA  LNCS 3184 pages 142-151 2004 Springer-Verlag  X W u  Y  W u Y  W a ng and Y  Li P ri v a c y a w are mar ket basket data set generation a feasible approach for inverse frequent set mining In Proc 5th SIAM International Conference on Data Mining  April 2005  Z Zheng R  K oha vi  a nd L Mas on R eal w o rl d performance of association rule algorithms In Proc of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 401 406 ACM Press 2001 8 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


Discovery, 8, 2004, pp. 7-23 20] W. Teng, M. Hsieh, and M. Chen. On the Mining of Substitution Rules for Statistically Dependent Items Proceedings of IEEE International Conference on Data Mining \(ICDM  02 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





