html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">A new  Classification Sentence Technique Using Intension Expressions Kazuhiro Morita, Elsayed Atlam, M,aao Fuketa, Yuki Kadoya, , Tom Sumitomo and Jun-ichi Aoe Department of Information Science and Intelligent Systems University of Tokushima Tokushima,770-8506, Japan E-mail: atlam@is.tokushima-u.ac.ip Abstract Although there are many text classification techniques depending on vector spaces, it is difficult to detect the meaning which are relating to the user  s intension complaint, encouragement, request, invitation, etc intension discussed in this paper is vety useful for understanding focus points in conversation. This paper presents a method of determining the speaker  s intention for sentences in conversation. The intension association expressions are introduced, and the formal rule descriptions using these expressions are defined to build intention classification knowledge. A set pattern matching algorithm is proposed to determine the intension class efficiently. From simulation results for 5.859 conversations, the presented set pattern-matching algorithm is about 44.5 times faster than Aho and Corasick method. Precision and recall of intension classifications are 90% and 95%. Moreover, precision and recall of unnecessary sentences extraction are 96 and 97 1. Introduction There are many conversation tools using computers such as, telephones, E-mail systems, mobile computers speech recognition devices and so on. It is very important techniques to determine the user  s intension in the communications. Many text classification techniques depending on vector spaces proposed by Ishida and Tsuji 7][8][9][12], but it is dificult to find the special sentences including the speaker  s intention independent of text classes. Finding the important sentences is relation to automatic summarization by Okumura and Nanba [14][5][6][11][13]. However, summarized sentences differ from the user  s intention in conversation because the purpose of these approaches is to extiact topics of news and focus points of documents. On the other hand, Fuketa et a1..[3][4][10] proposed Field Association \(FA expected passages in the whole document. FA words are very suitable for knowledge bases focusing on special sentraces becase they depend on word and phrase knowledge, not vector spaces depend on the whole text This paper extends the FA knowledge to Intention Association \(IA presents a method of determining the speaker  s intention for sentences in conversation. The formal rule descriptions using these IA expressions are defined to build intention classification knowledge. A set pattern matching algorithm is proposed to determine the intension class efficiently. The presented method is evaluated for many E-mail conversation sentences In this paper, chapter- 2 describes formal description rules for IA expressions. In chapter 3, IA expressions for E-mail conversations are defined by typical 8 classes and weight of rules are introduced to determine unnecessary sentences. Chapter 4 presents a efficient set pattern matching algorithm to find IA expressions form input sentences. In chapter 5 ,  the presented method is evaluated by experimental results for many conversation sentences. For the set-pattern matching machine, the time and space is estimated by comparing the traditional pattern matching methods 2. Formal Description Rules for Intension Association Expressions Intension understanding needs to build the detailed knowledge rules that can include  words    phrases   


knowledge rules that can include  words    phrases    part of speeches \(categories  concept    meaning   etc. Natural language has a huge ability expressing intension, and some expressions can be abstracted as conceptual representation, but there are many exceptions based on frozen expressions like idioms and proverbs On the formal description rules for intension association expressions, therefore, conceptual representations using  concept  and  meaning  are used for abstracted representations, and exceptions should be used the concrete information  words  and  phrases  Moreover there are many rules combining abstraction and exception. In this paper, a multi-attribute rule description is introduced in order to satisfy the above conditions These attributes include string \(words speeches \(categories semantic, or meanings Let ATTR be the attribute name and let VALUE be the attribute value. Let R be a finite set of pairs \(ATTR VALUE following attributes are considered as attributes STR: string, that is, word spelling CAT: category, or, a part of speech SEM: semantic information such as concepts If RULE is a multi-attribute rule consisting of a sequence of structures, then the p-th multi-attribute rule RULE \(p 0-7803-8623-x104/$20.00 02004 IEEE 98 RULE \(p I&lt;np For example RULE \(1 RI,, = \(\(SEM, EVENT SEM EVENT  meeting    seminar    party    festival  etc CAT, BE-VERB CAT, BE-VERB  is    are  etc Rl,3 = \(\(SEM, HOLD  carrying out    hold    open    perform    begin  etc Rule \(1 expressions such as  A campus festival is held    A seminar will be performed tomorrow  and so on. It is clear that the presented multi-attribute rules are an efficient semantic-based rule 3. Intention Association Classes and Weight of Rules 3.1. Intention Association Classes There are many communication tools using computers and it is very important to determine the user  s intension for all communications. This paper considers E-mail communications as one of the typical and practical communications The rule which classifies the contents of mail is described by combining a variety of words, phrases categories and semantics \(including concepts chapter defines I basic semantic information about important intension association \(1.4 1 Examples for intension association expressions of lt;Announce&gt; are  A campus festival is held    A seminar will be performed tomorrow    The schedule of a meeting was changed  and  Today  s concert was stopped  etc. An attribute \(SEM, EVENT meaning of the words  meeting    seminar    party    jestival  etc. An attribute \(SEM, SCHEDULE meaning of  schedule    plan    program  etc SCHEDULE is also including all words belonging to EVENT. Moreover, an attribute \(SEM, HOLD meaning of  carrying out    hold    open       


meaning of  carrying out    hold    open        begin  etc. An attribute \(SEM, CHANGE meaning of  change    cancel    stop    postpone  etc The sentence containing both EVENT and HOLD is important for IA expressions of &lt;Announce&gt;. It is also important for &lt;Announce&gt; expressions to contain both EVENT and CHANGE. In addition  becomes new  is also the meaning of  change  because  The curriculum became new  is the same meaning as  The curriculum was changed   2 Examples for intension association expressions of lt;Report&gt; are  We report the result to you  and  The operation was successful  etc. An attribute \(SEM lt;Announce&gt RESULT  resuIt    data    response  etc. An attribute \(SEM, REPORT the meaning of  report    announce    tell    convey   etc. An attribute \(SEM, OPERATION  operation    experiment    test    examination  etc An attribute \(SEM, SUCCESS  success    pass    good  etc. An attribute \(SEM FAILURE  tfailure    bag    unsuccessful  etc 3 Examples for intension association expressions of lt;Request&gt; are  Please submit these documents  and  Please gather not late  etc. An attribute \(SEM SUBMIT  submit    deliver    send    jhish    complete  etc. An attribute \(SEM, GATHER is the meaning of  gather    come    meet    assemble   etc  Submit  and  gather  are often utilized in request mails. An attribute \(SEM, DAY-TIME of  today    tomorrow    August 15th    Wednesday   etc. An attribute \(SEM, PLACE  room    floor    station    park  etc. There are more important for IA expressions of &lt;Request&gt; which described concrete DAY-TIME or PLACE, such as  Please submit these documents by tomorrow    Please come to the station at 10:00  etc 4 Examples for intension association expressions of lt;Question&gt; are  When is this seminar    Where is a meeting place  etc. An attribute \(CAT INTERROGATIVE  When    Where    What  and  How  An attribute \(SEM, TEACH meaning of  teach    ask    tell    answer  etc. For example  Please tell me your address  or  Please teach me how to solve this problem  are the same meanings as  Where is your address  or  How is this problem solved  so those are the expressions of &lt;Question&gt;. An attribute \(SEM, VISIT  visit    go    meet  etc. Examples are  May I visit to your house    When shall I visit you  etc. The expression belonging to VISIT is extracted together with an interrogative 5 Examples for intension association expressions of lt;ComplainB are  The program got yesterday is wrong    The received goods had broken    I cannot be satisfied of your service  etc. An attribute \(SEM, MISTAKE the meaning of  mistake    wrong    jailure    defect    bug  etc. An attribute \(SEM, SATISFY of  sat is   content    gratiw    complete  etc Expressions denying SATISFY have the meaning of a  complaint  An attribute \(SEM, HURRY meaning of  hurry    prompt    urgent    immediate    pressing  etc., and an attribute \(SEM, A  ITENTION denotes the expression  attention    measure    cope    correspond  etc. Expression with urgent demands also 


 correspond  etc. Expression with urgent demands also becomes &lt;Complaint&gt; expression. Examples are  Your prompt attention would be appreciated    We request an immediate refund   6 lt;Complain0 99 Examples for intension association expressions of lt;Reply&gt; are  It is O.K    I attend the meeting    I am absent from a meeting    I refuse this work  etc. An attribute \(SEM, ACCEPT expressions of &lt;Reply&gt; like  accept    upprove    O.K    understand    attend    present       etc. In contrary, to ACCEPT, an attribute SEM,DECLME lt;Reply&gt; like  refuse    decline    absent  etc. An attribute \(SEM, IMPOSSIBLE expressions like  impossible  or  cannot  The description containing both ACCEPT and IMPOSSIBLE i5 a refusal expressions of &lt;Reply&gt;. Examples are  I cannot attend the meeting    It became impossible to participate  etc. In addition, the meaning which combined ACCEPT and  next time  is also refusal expressions of &lt;Reply&gt;. For example    I will participate next time  is including the meaning of  I cannot participate this time   7 Examples for intension association expressions of lt;Invitation&gt; are  Let  s play together    Would you like to go with me  etc. An attribute \(SEM, TOGETHER is the meaning of  together    with    too  etc. An attribute \(SEM, HOPE  hope    want    wish    desire  etc. An attribute \(SEM PARTY  party    tfestival    entertainment  etc. PARTY is a sub concept of the EVEXT. An attribute \(SEM, JOIN  join    come    attend    participate  etc. There are lt;Invitation&gt; expressions combined with HOPE and JOIN and PARTY. Examples are  I hope you can come to our festival  and  I want you to participate in a dance party  etc 3.2. Weight of Rules An E-mail message can be classified according to rules as shown in Section 3.1 based on IA expressions Furthermore, an important message or an unimportant unnecessary weight, or point, to each classification rule. The important measurement of sentences is independent of classifications such as &lt;Announce&gt;, &lt;Report&gt;, etc. For example  A game is held on Sunday  is more important than  Do your best  For this reason, the sentence of lt;Announce&gt; is more important than &lt;Encouragement&gt generally. Consequently, the point of &lt;Announce&gt;. rule is 200 and the point of &lt;Encouragement&gt; rule is 70 in Table 1, where plus point means the important measurement is high and minus point is low. There are a variety of important measurements for IA expressions of same classification. For example, both RULE \(1 1 RULE \(14 points 150 and 15, respectively. RULE \(11 sentence asking schedule like  When is the meeting held  but RULE \(14 introduction of business like  How have you been  It is clear that the important measurement of latter case becomes low There are a variety of important measurements for IA expressions of same classification. For example, both RULE \(1 1 14 they have different points 150 and 15, respectively RULE \(1 I 


RULE \(1 I  When is the meeting held  but RULE \(14 sentence the introduction of business like  How have you been  It is clear that the important measurement of the latter case becomes low Furthermore, in the mails, there are unnecessary sentences no related to business. By deleting these sentences, the important measurement goes up very much relatively. The rules for judging unnecessary sentences are defined as follows 8 Examples for intension association expressions of lt;Salutation&gt; are  Hi, Mike    Dear Mr. Brown    Yours truly  etc. An attribute \(SEM, GREET meaning of the words  Hello    HP  thank    sorry   etc. An attribute \(SEM, CLOSING  Yours truly    Yours sincerely    Best wishes    See you  etc 9 Examples for intension association expressions of lt;Assumption&gt; are  If it rains, the game will be called off    If you have some questions, please ask to a service center  etc. The &lt;Assumption&gt; rules have the negative point. These rules are used in order to reduce the importance acquired with other rules. For example  The game will be called off  is extracted with 190 points of &lt;Announce&gt; by RULE \(3  If you have some questions, please ask to a service center  matches both RULE \(3 1 3 3 1 RULE \(3 is 140 lt;Assumption&gt 4. Determination Algorithm of Intension Classes 4.1. Set Pat tern Matching Machines Let X be a sequence of the input structures. A set pattern-matching machine SETM is a program which takes as input X and produces as the output the locations in input Xin which every RULE \(p subsequences of structures. The machine SETM consists of a set of states. Each state is represented by a number The machine M processes X by successively reading the input structure N in input X, making state transitions and occasionally emitting an output. The matching operation of the machine SETM is similar to multi-keyword string pattern-matching method of Aho-Corasick [ 1][2][ 151 however, the presented machine has the following distinctive features Let S be a set of states and let I be a set of the rule structures R, then the behavior of the machine SETM is 100 defined by next two functions: goto functiongoto : Sx I S U uail R-SET The function goto maps a set consisting of a state and a rule structure into a state or the message fail. Certain states are designated as output states which indicate that RULE \(p formalizes this concept by associating each RULE \(p R-SET \(possible empty A transition label of the goto function is extended to a set notation. Therefore, in the machine SETM, a confirming transition is decided by the inclusion relationship whether the input structure N includes the rule structure R or not Input structures to be matched by the matching rule are also defined by the same set representation. N is used as the notation for input structures to distinguish them from R. In order to consider the abstraction of the rule structure, matching of the rule structure R and the input 


structure N are decided by the inclusion relationship such that N includes R \(NOR Let R-SET be a set of RULE \(p following rule in Table 1 R-SET= {RULE \(1 1 RIJ = {\(SEM, EVENT CAT, BE VERB SEM, HOLD RULE \(1  A campus festival is held  where NI  = {\(STR  festival   CAT NOUN SEM, EVENT STR  is   N3 = {\(STR  held   CAT, VERB SEM, HOLD the corresponding rule structure as follows The machine becomes non-deterministic if there are two more labels R such that \(NOR goto\(STATE, R ambiguity must be solved before constructing the goto graph from R-SET. The goto graph is the tree structures sharing longest common prefixes for each rule, so it is easy to detect that ambiguity. Ambiguous rules are divided into unambiguous sub-rules Consider the following second rule in Table 1. RULE 3 SEM, SCHEDULE CAT, BE-VERB SEM, CHANGE two rules RULE \(1 3 structures are ambiguous, so the following sub-rules are obtained by dividing RULE\(3 RULE\(3 q+l maximum rule number. For q = 32 of Table 1, q+l = 33 CAT, BE-VERB N I ~ R I , I ,  NzDRI,z, N30R1,3 RULE \(3 SEM, SCHEDULE CAT, BE-VERB R3,3 = \(\(SEM, CHANGE SCHEDULE\( 1 28 R33,i Rn.zR33.i. Rm.1 = {\(SEM, EVENT R33.z = \(\(CAT, BE-VERB SEM CHANGE 4.2. Set Matching Algorithm It is clear that the deterministic machines can be constructed in the section 4.1, so i t i s  easy to analysis a sequence of structures. In natural language processing morphological, syntax and semantic analyzers are carried as the preprocessor in general, but the input must take a sequence of structures for skeleton sentences and phrases  Skeleton  means that the embedded sentences . and redundant words are ignored in the  preprocessor The reason is that the request of the mail messages is very simple expression For example, consider  Mr. Koizumi  s birthday party is held after the meeting on May 15    Mr. Koizumi  s birthday  which is the modifier of a subject is removed from the input. Moreover  after the  and  on  are also removed  Party  is taken as EVENT. Finally  party EVENT    is \(BE-VERB  and  held \(HOLD  are obtained from the input sentence and the following sequences of structures are prepared to the SETM machine. NI  = {\(STR  party   CAT, NOUN SEM EVENT STR  is   CAT, BE-VERB STR  held   CAT, VERB SEM, HOLD It is easy to produce a sequence of input structures because there are useful natural language processing modules. Therefore, the detail discussion is omitted in this paper The following algorithm summarizes the behavior of the machine SETM Algorithm: Set Matching Machine SETM Function SetMatch\(a, M A sequenceaof input structures is NI N2 ... N., where each N, \(O&lt;i&lt;n+l 


each N, \(O&lt;i&lt;n+l a which the first structure NI is removed. A SETM machine M with a goto function and an output function Step \(M-1 If a has no structures then terminate matching process Initial state STATE of the machine M is set to the start state number Step \(M-2 Matching Variable N of input structures is set to NEXT \(a each transition goto\(STATE, R is R such that NOR, then STATE = goto\(STATE, R goto the next step; Otherwise replace Nu to a and call SetMatch\(a,M Step \(M-3 Continue Matching If output \(STATE produced; Call SetMatch\(a,M End of Function Table 2 shows the flow of matching process for  This document is used for the next meeting, so please send it to the leader by tomorrow  First  meeting  becomes he input structure N = \(\(STR  meeting   CAT, NOUN 101 Table 2. Examples of Matching Process Table 3. Simulation Results of Machines SETM and AC SETM AC AC/SETM Information about rules and machines Number of rules 1,561 327,8 10 210 Number of concepts 588 Number of words in concepts 2,438 Number of transitions 895 982,55 1 1,097 Storages \(MB Speed \(ms SEM EVENT SEM EVENT Goto/Output column. The next structure N= {\(STR  please    t include a transition label {\(CAT, BE VERB STR  become   goto transition fails in Step \(M-2 function SetMatch is called recursively from the current structure. State transitions 0, 19, 20 and 21 are carried out for four structures as shown in Table 2. It turns out that rule 8 is matched to the input sentence by Output 2 1 5. Simulation Results 5.1. Space and Time Observations of Set Pattern Matching Algorithm Table 3 compares the presented machine SETM with the string pattern-matching machine AC [l]. The number of rules210 times more than that of the machine SETM because the machine SETM can be taken multi-attribute inputs, but the machine AC must take single attribute only. Therefore, the AC machine must be divided into the machines STR-AC \(for words for categories for semantics concepts and 2,438 words of SETM. The resulting number of transitions for AC is 1,097 times more than that of the machine SETM. The storage of the machine AC is 26.5 larger than that of the machine SETM, where the size of machine SETM includes dictionaries for concepts and their words The machine SETM can detect expected rules to be matched by only one scanning, but the machine AC needs to scan three times scanning for the above three kinds of rules. Moreover, the machine AC needs additional closure computation by adopting all results of the machines STR-AC, CAT-AC and SEM-AC. From the simulation results for the average matching time for 1,000 sentences, it turns out that the machine SETM is 44.5 times faster than the machine AC The worst-case time complexity of matching cost of 


The worst-case time complexity of matching cost of the machine SETM is proportional to the length of input structures. This theoretical estimation means that the presented method is practical 6. Conclusions This paper has presented a method for detecting user  s intention in communicatios and for calculating the important measuremt among the detected expressions. In order to extract the intension association expressions eitgh kinds of rule sets are classified and 51 concepts are defined. A multi-attribute set pattern-matching algorithm is presented. This multi-attribute enables us to fast detecting by using complex rule sets. From simulation results for 5,859 E-mail messages, the presented set pattern-matching algorithm is about 44.5 times faster than Aho and Corasick method. The precision and recall of intension classifications are 90% and 95% and 102 precision and recall of unnecessary sentences extraction are 96% and 97 The intension association expressions and the important measument is one of filtering information to determine the important E-mail messages. In the future study, the other measurement for important messages will be considered together with the time priority. The filtering measurement depends on individual criterion except business cases, so the leaning faculty for users should be also researched Texts. in Proceedings of the IEEE 7-th International Symposium on Computer VII, 1 1 , 1992, pp. 471 -474 References I ]  A.V. Aho &amp; M. J. Corasick  Efficient string matching: An aid to bibliographic search  Communications of the ACM 2] J. Aoe  An efficient digital search algorithm by using a double-array structure  IEEE Trans. Sof i .  Engr., SE 3] El-S. Atlam, M. Okada, M. Shishibori &amp; J. Aoe  An evaluation method of words tendency depending on time series variation and its improvements  An Journal of Information Processing and Management, 38\(2 4] M. Fuketa, S. Lee, T. Tsuji, M. Okada &amp; J. Aoe  A document classification method by using field association words  An International Journal of Information Sciences 5] T. Hasegawa &amp; S. Takagi  Extraction of Schedule Information in Communication through E-mails  IPS Japan SIC Notes, 98-NL-123-10, 1998, Cp. 73-80 6] M. Hatayama, Y. Matsuo&amp; S. Shira  Summarizing Newspaper Articles Using Extracted Informative and Functional Words  Journal of NLP, 9\(4 7] E. Ishida &amp; K. Tsuji  A Comparison of Feature Extraction for Japanese Text Categorization  IPS Japan SIC Notes 02-NL-151-12.2002, pp. 81-86 \(in Japanese SI 0. Kwon &amp; J. Lee  Text categorization based on k-nearest neighbor approach for Web site classification  An International Journal of Information Processing and Management, 39\( I 9] W. Lam, M. Ruiz &amp; P. Srinivasan  Automatic Text Categorization and Its Application to Text Retrieval  lEEE Transactions on Knowledge and Data Engineering, 11\(6 IO] S. Lee M. Shishibori, T. Sumitomo &amp; J. Aoe  Extraction of field-coherent passages  An International Journal of Information Processing and Management, 38\(2 18\(6 15\(9 157-1 71 126\(1 1999, pp. 865-879 173-207 I  1 J I .  Mani. Automatic Summarization. John Benjamins 1121 M. Moens &amp; C. Uyttendaele. Automatic Text Publishing Company, 200 1 


Publishing Company, 200 1 Structuring and Categorization as a First Step in Summarizing Legal Cases. An International Journal of Information Processing and Management, 33\(6 pp. 727-737 13] T. Mori. A Term Weighting Method based on Information Gain Ratio for Summarizing Documents retrieved by IR System. Journal of NLP, 9\(4 I41 M. Okumura, H. Nanba. New Topics on Automated Text Summarization. Journal of NLP, 9\(4 I51 K. Tsuda, M. Shishibori &amp; J. Aoe. An Efficient String Pattern-matching Algorithm. The Application of Reducing 103 pre></body></html 


 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   100  A   1  000  A   100  A   1  000 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   10  000  A   100  000  A   10  000  A   100  000 Figure 4 DataSet One with c  1 Figure 5 DataSet One with c  2 and therefore these itemsets participate in the implication count The number of tuples created by this step is S  50  c  1   2  4   The rest of the steps create itemsets that should not participate in the count We create three different kind of tuples that break one implication condition The relative weight of each kind is 1  3 Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A created before As in the previous step for each a i create at most c different b j  For each combination  a i  b j  write 50 tuples Then for each a i create eight b  j different than all b j s created before And write the eight tuples  a i  b  j   This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum top-condence level although they satisfy both the minimum support and the maximum multiplicity constraint The number of tuples created by this step is   A  S   3  50  c  1   2  8   Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated and each one appears with u different b j  where c  1  u  c  10 Write 50 such tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the maximum multiplicity condition The number of tuples created by this step is   A  S   3  50  c  5  5  Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated For each pair  a i  b j  write 40 tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum support requirement The number of tuples created by this step is   A  S   3  40 Shufe the output le This step just demonstrates that the operation of the algorithm is independent to the ordering of the tuples Estimate the implication count using algorithm NIPS/CI with a fringe size of four and also without a bounded fringe Perform one hundred such experiments and calculate the mean and the standard deviation of both estimations The total number of tuples for each experiment can be derived by adding the partial number of tuples created in each step For example for  A   10000 S  5000 and c  4 the average number of tuples for the corresponding experiment was 015 3  108  333 A minimum support of 50 tuples for this case corresponds to only 015  001 of the tuples demonstrating that in the implication count contribute even implications that hold for a very small number of tuples Figures 4,5 and 6 show the results for c  1  2  4 for varying cardinalities  A   The x-axis corresponds to the actual implication count of the dataset as that was imposed by the creation process The y-axis denotes the mean relative error as it is calculated by running one hundred experiments We used the following formula to estimate the mean relative error relative error   Actual S  Measured S  Actual S  Graphs Bounded Fringe express the experimental results for the case of a fringe with size F  4 while graphs Unbounded Fringe demonstrate the result for the case of an arbitrarily large fringe The error bars correspond to the statistical deviation of the mean error as that was computed by one hundred such experiments.The deviation is generally negligible which means that the error of the estimated S is always very close to the mean error We also observe that the difference between the estimation using a bounded fringe of size four and a unbounded one is negligible for a very wide range of implication counts and therefore a size of four for the fringe zone is sufcient to provide very accurate results for most applications 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe Figure 6 Dataset One with c  4 6.2 Real-world datasets  Algorithmic comparison We compare our estimates with the results taken using Distinct Sampling DS which has been sho wn pro vide highlyProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


accurate estimates for distinct value queries and event reports This algorithm outperforms other estimators that are based on uniform sampling[8 9 e v e n when using much less sample space We also provide comparison with our Implication Lossy Counting ILC algorithm described in Section 5.1 which is based in the Lossy Counting algorithm introduced in For this series of experiments we used a real dataset of eight dimensions which was given to us by an OLAP company whose name we cannot disclose due to our agreement The cardinalities of the dimensions are presented in Table 3 The parameters of the algorithms are presented in Table 5 For NIPS/CI we used 64 concurrent bitmaps with a fringe size of four thus requiring memory enough to hold  2 4  1   64  K  1920 itemsets We expect that the averaging\([5 o v e r these man y bitmaps will result in an error less than 10 We used the exact same sample space for DS The bound parameter t for DS was set to  1920  50  following the suggestion in F o r ILC we used an approximation parameter   0  01 which increases the memory requirements of ILC relative to those of NIPS/CI or DS On the average ILC used more than twice the memory that NIPS/CI and DS used For example for the experiment in Figure 7\(B it used more than 8,000 entries We evaluate the results of the algorithms with respect to the number of tuples the cardinality of the participating dimensions and the implication conditions To simulate a real data stream scenario we tracked the conditional implication counts of A  B  E  F and the unconditional B  E using the aforementioned algorithms The rst workload corresponds to quite large compound cardinality while the second to very moderate cardinalities Table 4 presents the actual aggregates for various instances of the stream for   5 and  1  60 We believe that most workloads fall somewhere in the middle with respect to the complexity of the wanted implications and the size of the returned counts Figure 7\(A depicts the relative error as the stream evolves for workload A using the algorithms DS NIPS/CI and ILC for different implication parameters In Figure a we show the results for minimum support   5 and  1  60 or  1  80 The different  1 are encoded in the parentheses next to identication of the algorithm in the legend of the graph In Figure b we increased the minimum support to   50 We observe that the behavior of DS varies widely while NIPS/CI remains always below the expected 10 error DS actually keeps a sample of the distinct elements seen so far and tries to scale the implication count that holds for that sample to the whole set of distinct elements In most cases the data in the sample is not representative of the implication The situation for DS is exacerbated when the minimum support increases where quite a lot of samples do not participate in the count making the scaling even more errorprone Algorithm ILC in all cases returned very erroneous results although it used much more space than NIPS/CI and DS since it tries to store not the implication counts but the actual implicated itemsets In these workload the implicated itemsets overwhelm its available memory which is actually larger than the amount given to NIPS/CI and DS In gure 7\(B we present the results of the algorithms for workload B The situation is still in favor of NIPS/CI whose relative error remains always close to the expected 10 unlike DS who returns highly skewed errors even though the domain cardinalities are much smaller and therefore keeps in the sample space much more data As expected from the analysis the error guarantees of NIPS/CI are virtually unaffected by changes in the cardinalities or the number of tuples seen so far in the stream ILC returns very erroneous results although now the cardinalities and the implicated items are much smaller compared to those of workload A The reason is not only because it keeps too much information in memory i.e all the implicated itemsets while both NIPS/CI and DS only hold a mantissa for the count but also because the constraint    rel is broken as the number of tuples increases 7 Related Work There are unique challenges in query processing for the data stream model Most challenges are the result of the streams being potentially unbounded in size Therefore the amount of the storage required in order to get an exact size may also grow out of bounds Another equally important issue is the timely query response required although the volumes of data the need to be processed is continually augmented at a very high rate Essentially the amount of computation per data item received should not add a lot of latency to each item Otherwise any such algorithm wont be able to keep up with the data stream In many cases accessing secondary storage such as disks is not even an option In  there is a discussion of what queries can be answered e xactly using bounded memory and queries that must be approximated unless disk access is allowed Sketching techniques\([14 ha v e been introduced to build summaries of data in order to estimate the number F 0 of distinct elements in a dataset In three algorithms that      approximate the F 0 are described with various space and time requirements Distinct is dri v e n by hashing functions similar to those studied in 14 and provides highly accurate results for distinct value queries compared to those taken by uniform sampling by using only a fraction of their sample size In the algorithms Stick y Sampling and Lossy Counting are introduced that estimate frequency counts with application to association rules and iceberg cubes In a framework for performing set expression on continuously updated streams based on sketching techniques is presented In a general framework over multiple granularities is presented for both range-temporal and spatio-temporal aggregations In a framework for identifying hierarchical heavy hitters i.e hierarchical objects like network addresses whose prexes denes a hierarchy with a frequency above a given threshold is described The effect of impications between columns has been emphasized in the system that identies correlated pairs of columns and soft-dependencies and has been proved very useful in query optimization 8 Conclusions We have presented a generalized and parameterized framework that can accurately and efciently estimate implication counts and can be applied to many scenarios To the best of our knowledge this is the rst practical and truly scalable approach to the problem Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


 Dimension Cardinality A 1557 B 2669 C 2 D 2 E 3363 F 131 G 660 H 693 Table 3 Cardinalities Workload A Workload B Tuples A  B  E  G E  B 134,576 608 50 672,771 12,787 125 1,344,591 34,816 152 2,690,181 84,190 165 4,035,475 132,161 182 5,381,203 187,584 188 Table 4 Impl counts w.r.t tuples NIPS/CI bitmaps 64 NIPS/CI K 2 DS sample size 1920 DS bound t 39 ILC  0.01 Table 5 Algorithm Parameters           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILS \(.8           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6 ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8   a   5 b   50 a   5 b   50 A Workload A B Workload B Figure 7 Relative Error vs stream size of online estimation within small errors of complex implication and non-implication counts between attributes of a data stream under severe memory and processing constraints and even in the presence of noise We prove that the complement problem of estimating non-implication counts can be      approximated when the size of the fringe zone is xed appropriately We demonstrate that existing algorithms for estimating frequent itemsets or sampling cannot be applied to the problem since they lose the cumulative effect of small implications In addition through an extensive set of experiments on both synthetic and real data we have shown that NIPS/CI always remains very close to the actual implication count capturing even very small implications whose total contribution is signicant References  R Agra w al A Evmie vski and R Srikant Information sharing across private databases In ACMSIGMOD  2003  N Alon Y  Matias and M Sze gedy  The space comple xity of approximating the frequency moments Journal of Computer and System Sciences  58:137 147 1999  A Arasu B Babcock S Bab u J McAlister  and J W idom Characterizing memory requirement for queries over continuous data streams In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  B Babcock S Bab u M Datar  R  Motw ani and J W idom Models and Issues in Data Stream Systems In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  Z Bar Y ossef T  Jayram R K umar  D  S i v akumar  and L T r e visan Counting Distinct Elements in a Data Stream In RANDOM  pages 110 2002  A Belussi and C F aloutsos Estimating the selecti vity of spatial queries using the correlation fractal dimension In VLDB95 Proceedings of 21th International Conference on Very Large Data Bases September 11-15 1995 Zurich Switzerland  pages 299310 1995  J Bunge and M Fitzpatrick Estimating the number of species A r e vie w  Journal of American Statistical Association  88:364373 1993  M Charikar  S  Chaudhuri R Motw ani and V  Narasayya T o w ards estimation error guaranties for distinct values In ACMPODS  pages 268279 2000  S Chaudhuri R Motw ani and V  Narasayya Random sampling for histogram construction How much is enough In ACMSIGMOD  pages 436 447 1998  G Cormode F  K orn S Muthukrishnan and D Sri v astana Finding hierar chical heavy hitters in data streams In VLDB  2003  M Datar  A  Gionis P  Indyk and R Motw ani Maintaing stream statistics over sliding windows In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms  2002  A Deshpande M Garof alakis and R Rastogi Independence is Good Dependency-Based Histogram Synopses for High-Dimensional Data In ACM SIGMOD  2001  C Estan S Sa v age and G V a r ghese Automatically inferring patterns of resource consumption in network trafc In ACMSIGCOMM  2003  P  Flajolet and N Martin Probabilistic Counting Algorithms for Data Base Applications Journal of Computer and System Sciences  pages 182209 1985  N Friedman L Getoor  D  K oller  and A Pfef fer  Learning probabilistic relational models In IJCAI  pages 13001309 1999  S Ganguly  M  Garof alakis and R Rastogi Processing set e xpressions o v e r continuous update streams In ACM SIGMOD  pages 265276 2003  P  Gibbons Distinct sampling for highly-accurate answers to distinct v alues queries and event reports In VLDB  pages 541550 2001  P  J Haas J F  Naughton S Seshadri and L Stok es Sampling-based estimation of the number of distinct values of an attribute In VLDB  1995  I Ilyas V Markl P  Haas P  Bro wn and A Aboulnaga CORDS Automatic Discovery of Correlations and Soft Functional Dependencies In ACMSIGMOD  2004  J Jung B Krishnamurthy  and M Rabino vich Flash cro wds and denial of service attacks Characterization and implications for cdns and web sites In ACMWWW  2002  J Ki vinen and H Mannila Approximate dependenc y inference from relations Theoritical Computer Science  149:129149 1995  G Manku and R Motw ani Approximate frequenc y counts o v e r data streams In VLDB  2002  V  Poosala P  J Haas Y  E Ioannidis and E J Shekita Impro v e d histograms for selectivity estimation of range predicates In ACMSIGMOD  pages 294305 1996  S Stolfo W  Lee P  Chan W  F an and E Eskin Data mining-based intrusion detectors An overview of the columbia ids project ACMSIGMOD Record  30\(4 2001  H W ang D Zhang and K G Shin Detecting SYN Flooding Attacks In INFOCOM 2002  2002  K Whang B V ander Zander  and H T aylor  A Linear Time Probabilistic Counting Algorithm for Database Applications ACM Transactions on Database Systems  pages 209229 1990  D Zhang D Gunopulos V  Tsotras and B  See ger  T emporal and spatiotemporal aggregations over data streams using multiple time granularities Information Systems  28\(1-2 2003 The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofcial policies either expressed or implied of the Army Research Laboratory or the U S Government Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





