Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 A FAST ALGORITHM OF MINING ASSOCIATION RULES IN NETWORK MANAGEMENT PEI-QI LIU ZENG-ZHI LI  YU-XI CHEN b YIN-LIANG ZHAO I a of Computer Architecture  Networks Xi'an Jiaotong University Xi'an China  Information  Network Center, Xi'an University of Architecture  Technology E-MAIL peiqiliu 163.com davidc@webmail.xauat.edu.cn Abstract This paper presents nowadays 
study and research of KDD points out the shortcoming of classical Apriori's algorithm Our research has put forward the AprioriNEW algorithm based on reducing database and analyses and appraises to this algorithm in progress This algorithm has been applied to mine the Trap's information in the network management Keywords Data mining; Association rule Network; Frequent itemset Transaction database 0 In-efficient utilization of data mining result in each cycle These lead to a serious decrease to the algorithm efficiency Many optimized methods of Apriori's algorithm have 
been put forward recently, most of the algorithms are based on memory optimization this is the limitation of the algorithm. This paper puts forward an improved algorithm on the basis of the database reducing It can reduce the searching space and composing branch It can also improve the efficiency of mining 1 Introduction 2 Basic concept and theory With the fast growing of network technology, especially the techniques in Internet fields, the network infer-structure becomes more and more complicated this leads to an urgent demand of efficient management solution to network Many study and research have been carried out in network  management field recent years 
i.e Neural Networks Artificial Intelligence of which Expert System has been testified as an efficient solution The Knowledge acquisition is the key problem in Expert System. We need a method that can mine knowledge from the Trap's information automatically and effectively KDD Knowledge Discovery in Database in network management is new technology, various methods, theories and tools have been put forward in the data processing fields For example classification clustering sequence pattern mining and association rule etc of which association rule is a very important and active mining method The association rule firstly puts forward by R.Agrawal in 1993"I It was used 
to mine association rule in the customer business transaction database and to discover relationship between the different items The nucleus of R.Agrawal's method is Apriori's algorithm It can find out frequent item set in the power sets on the principle of statistics This algorithm can be divided into two parts first Apriori's algorithm search frequent item sets in database, and then it creates association rules from those frequent item sets The major shortcomings of this algorithm are 0 power sets of items generation is needed cycling search of database occurred In the data mining the database can be transaction databases or relational databases. Relational databases can 
work as transaction databases by using the predicate logic The mining method of association rule in the transaction databases is suitable for relational databases. Here are some narration and definitions of conceptions I which will be used in this article later Definition 1 Let I={il ,i  im  be a set of items, then D c Tid, TA T c I is a transaction database, where Tid is an identifier which be associated with each transaction Definition 2 Let X E I Y 
C_ I and X n Y 4 we called X 3 Y as association rule Definition 3 Suppose c is the percentage of transactions in D containing A that also contain B then the rule XJY has confidence c in D Ifs is percentage of transactions in D that contain A U B then the rule X j Y has support s in D Definition 4 Hypothesis X E I minsup is the smallest support If the frequency of X is greater than 
or equal to minsup then X is called as frequent itemset, otherwise X is called non-frequent itemset Our purpose is searching for association rules with the smallest support degree and the smallest confidence degree The classical algorithms can mine those rules by scanning D repeatedly. Its efficiency is very low if the D is a huge database. Analyzing the process of mining rules we find out many properties of transactions and many transactions which can be deleted from D Those properties, included property 13 of Apriori, can be described as below 0-7803-7508-4/02/$17.00 02002 IEEE 91 5 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 Theorem 1 Supposed X c T,Y frequent itemset then X must be a frequent itemset. That is VX VY Y E TA X  Xmunt 2 mincount Where X.count and Y.count stands for X's count and Y's count in the database and mincount is the minimum support degree count Proof Let T tl  t   e tn be records of D According to the definition of the minimum support degree we get the formulation mincount=minsup*IDI Since Y is a frequent itemset there are at least mincount records included Y Since X E Y there are at least mincount records included X from the transitivity of the inclusive relationship.So  X.counter2mincount Theorem 2 Given XET Y cT and XEY If X is a non-frequent itemset then Y muse be a non-frequent itemset. That is VX VY Y c TAX c Y A X.count e min count  Y.count  min count Proof This theorem can be proved using disproof. Suppose Y is a frequent itemset that is Y.count 3 mincount Because of X Y X must be a frequent itemset by theorem 1, that is X.count2mincount This is contradictory with X.count  mincount Therefore if X were a non frequent itemset, Y must be non-frequent itemsets Theorem 3 Suppose we have got Lk and Lk is a set of k itemset If T C I and ITI=k then we can delete T from database Proof The conclusion can be founded obviously. Because the transactions in which the cardinal number is smaller than or equal to k  do not include in c i>k the transaction T is not used for data mining when i is greater than k Therefore, we can delete T from database Theorem 4 Suppose X c T Y C T X E Y and IYI=IXI+l If X is a non-frequent itemset Y is also a non-frequent itemset and can be delete from database Proof The proof can be divided into two steps First, Let's prove Y is a non-frequent itemset. Because X is a non-frequent itemset and XCY Y must be a non frequent itemset by theorem 2 Next proves that Y does not have any unknown frequent itemsets Suppose X til t    t,k  and IXI=k We can get Y=XU trm}, where t any set IYI=k+l The k itemsets of Y are T and X C Y If Y is a Y A Y.count 2 mincount That is,X must be a frequent set bll t,2    tlk 1 t,l tr2   t,k  t X kl 42 9    9 qk-2 tlk 7 t x   9 b12 9 t13 1    9 l1k 9 tIrn 1 Because frequency of Y's subsets has been known in mining Lk  Y does not have any unknown subsets Therefor we can delete itemset Y and its subsets from database Theorem 5  Let X C T and Y C T where IYI=2 and IXI 32 If X and Y are non-frequent XUY and its subsets can be deleted form database Proof  X and Y are non-frequent itemsets  X U Y is a non-frequent itemsets by theorem 2 Let X={tml,tm2,..-,tmk and Y={tnl,tn2},where IXI=k\(k 32 We can get xu Y={tm1 tm2 t t,l tn2 Where IX U YI=k+2 The k+l-itemsets of X U Y are tml 7 tm 9 9tmk 9 tn1}9 tml 9 tm2 9 tml tm td-1 t tu t,l t  tmk-2 9 t 9 t  t 1 9 tmk 9 tn2 tm19 tm2 9'.'9 d-3 9 t&-l 9 tmk 9 tnl 9 tn2},.'*9 itm2 9 tm3 9 9 tmk 9 tnl 9 tn2 1 Because those itemsets are supersets of X or Y According to theorem 2 they are non-frequent itemsets their k-itemsets are known after X has been mined Therefore X U Y and its subsets can delete from database For example suppose a,b,c and d,e are a non frequent itemsets We can get that la,b,c,d,e is a non frequent itemset and can erase a,b,c,d,e a,b,c,dJ,Ia,b,c,e and b,c,d,eJ from database Theorem 6 Suppose XGT Y GT X and Y are non frequent itemsets If all subsets of X \(or Y are non-frequent itemsets we can delete X U Y and its subsets from database Proof  X is a non-frequent itemset  X U Y is also a non-frequent itemset Let VW W C X VZ Z E Y All subsets of XU Y belong to one of following circumstances W U Y Z U X or W U Z Because X Y W and Z are non-frequent itemsets we can get that all subsets of X U Y are non frequent itemsets according to theorem 2 Therefore, they can be deleted from database When we want to delete records from database by this theorem we always judge a element of tree which root is X U Y and high is min{lXl,lYl For example suppose a,b,c,d  e,f,g} and all subsets of e,f,g} are non-frequent itemsets We can get that following subsets are non frequent itemsets a,b,c,d,e,f,gJ a,b,c,d,e,f Because subsets frequency of which the cardinal numbers is equal to 4 is known after a,b,c,d} has been mined it has no influence on the data mining when we f,g b,c,d,e,f,g 91 6 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 delete them form database So we can delete  a,b,c,d,e,f,g and its subsets from database Theorems above are the foundation of the new algorithm Taking into consideration of the complicated nature of construction algorithm we implement above theorems exclusive of theorems 6 in this article 3 The algorithm of mining association rules Because Apriori algorithm Scan database repeatedly, its efficiency is very low In recent years some optimization algorithms have been put forward by some scholar For example, Savaseres etc 14 has designed the algorithm based on Divides, Parks etc I has put forward to the algorithm of the Hash, Toivonen etc I has designed the algorithm based on Sampling This article put forward a algorithm by reducing redundancy records of database The reducing norms are based on the theorem 3 theorem 4 and theorem 5,here is a list of the norms Norm 1 The k-itemsets can be deleted from database after Lk has been mined Norm 2 The \(k+l which it include non-frequcnt k-itensets can be deleted from database after Lk has been mined Norm 3 The k+2 which include a non-frequcnt k-itenset and a non-frequcnt 2-itenset can be deleted from database after Lk has been mined According to above norms we design a algorithm which is similar to algorithm designed by R.Agrawa1 etc This algorithm can be divided into two parts 0 Find all frequent itemsets 0 Generate strong association rules from the frequent itemsets Because second part is simpler then first one we only give the AprioriNEWs algorithm to find all frequent itemsets The AprioriNEWs algorithm is listed as below Algorithm AprioriNEW Input DB minsup Output Result Result  k:=l ck  large 1-itemsets While Ck  do Begin Created a counter for all itemsets for \(i 1 k=IDB I;i begin if I Ti I=k-1 2 can be deleted from database f\(x C Sk-1 2 C DB,I 2 I=k+l q.-X c S2 can be deleted if c DB,X C Sk I 2 I-IXI=O q can be deleted if Ti DB q c ck  Ti counter end Lk tk ck and t.counter>minsup sk tit\200 ck and t.counteruninsup If k==2 s2  sk  The support degree of Lk has no change Result  Result U Lk k=k 1  End Where ck is candidate itemsets 2 is i-th record of database S is non-frequent k-itemsets S2 is non-frequent 2-itemsets 4 The algorithm performance analysis and evaluation Algorithm AprioriNEW can find all frequent itemsets and reduce database with non-frequent itemsets. Suppose 0 0 0 0 0 0 0 The number of records of database is n The minimum support degree is s The algorithm finished after circulation k times Average cardinal number of candidate items is p Average cardinal number of non-frequent itemsets is m Average cardinal number of frequent itemsets is 1 Average number of records which it reduces is r The complexity of time of Apriori is  where pkn is the complexity of time of mining database in k times  k is the complexity of time of creating all   2 candidate items can get the complexity of time is When we use AprioriNEW to mine same database we k c i=l p[n  r\(i 11  n i=l  r\(i  l  2 k Where C p[n  r\(i I is the complexity of time of scanning itemsets the complexity of time of reducing database with non-frequent itemsets is m[n  r\(i  I  Obviously algorithm AprioriNEW can effectively reduce records and Yo's frequency when T is a very large number It can also loading the fairly large database into memory In order to explain the merit of algorithm AprioriNEW we realize it with Visual C++6.0 on a Pentium 4 PC We have mined 10 transaction databases respectively in the test The record number of databases ranged from 1000,2000,to 10000 In order to compare mining process of the database i=l k i=l 917 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 we used the identical the minimum support degree 15 in itemsets and transaction database set forth 3 theorems of the figure 1 reducing the database and then put forward the A~rioriNEW algorithm In our test the alaorithmNEW is go aa 70 qa 30 io 60 50 20 n Fig 1 Compares AprioriNEW with Apriori Figure 1 shows that the AprioriNEW algorithm is more efficient than Apriori in mining 10 databases, the horizontal axle shows 10 different databases and the vertical axle shows the time of mining the unit is second Figure 2 shows mining of a database with 10000 records by the AprioriNEW with different minimum support degree, the vertical axle shows the number of reducing records in same database The unit is thousand The horizontal axle shows 10 differentsupportdegrees\(in percentage WVIVWt LnQoKlma  I   n-cd m I I 4 W md d Fig 2 Reduced database by AprioriNEW superior to the ilassical algorithm if transaction database can be reduced According to the running result of the algorithm the efficiency of mining is directly connected with the structure of record In case of database reducing records nature getting into worse that is it does not satisfied the condition in theorem 3,4,5 the efficiency of AprioriNEW descend quickly Therefore, we still need to improve algorithm AprioriNEW in the feature Acknowledgements Project Sponsor National Natural Scientific Foudation china N0.60 173066 References l R Agrawal T Imielinski A Swami Mining association rules between sets of items in large databases Proceedings of the ACM SIGMOD Conference on Management of data pp 207-216 1993 2 Jiawei Han Micheline Kamber Data Mining Concepts and Techniques Morgan Kaufmann Publishers Inc pp. 227-230,2000 3 Shi Zhong-zhi The knowledge is discovery. Tsinghua University publishers pp 60-78,2002 4 A Savasere E Omiecinski S Navathe An efficient algorithm for mining association rules in large databases Proceedings of the 21st International Conference on Very large Database 1995 3 S Park M S Chen P S Yu An effective hash-based algorithm for mining association rules Proceedings of ACM SIGMOD International Conference on Management of Data pp 175-186, San Jose, CA May 1995 6 H Toivonen. Sampling large databases for association rules Proceedings of the 22nd International Conference on Very Large Database, Bombay, India 5 1996 Generally, the efficiency of AprioriNEW is superior to the Apriori,\(showed in figure 1 The bigger the parameter r is the better result we get The efficiency of algorithm is related to record structure and record number \(showed in figure 2 The relationship between s and record number is demonstrated In fact AprioriNEWs efficiency is connected not only with above parameters, but also with the structure of transaction database and the opportunity of reducing the database If the algorithm can reduce records in the beginning loops it can gain a very good result in the complexity of time and space 5 Conclusions In this paper we analyzes the algorithm of mining association rule according to the property of frequent 91 8 


then for each non-empty subset a off whether the association rule a 3 If  a holds or.inot is determined by computing the confidence  supportv a If confidence 2 minimum confidence then the rule holds a is called the ascendant of the rule and If a is called the consequent of the rule The candidate ascendant are all possible subsets of the frequent itemset whose length are from 1 to the length of the frequent itemset minus 1 and the corresponding candidate consequents are those left in the frequent itemset 4.5 Example of Mining Association Rules For this specific project the end users are assumed to be doctors specialists or anyone interested in skin cancer who have knowledge about skin'cancer and know what kind of associations they are looking for Therefore to find association rules among which attributes is totally dependent on the end user The interface is designed such that user can find association rules among any combination of the attributes In the interface every attribute in the target table is listed as shown in Figure 7. Except the Malignant attribute a list box is used to select the number of intervals to discretize the attribute values The user can select a value between 1 and 10 for the number of intervals Different attributes can be discretized with different number of intervals The attribute Malignant in the table has only three values malignant benign and pre malignant so there is no need to discretize it Two sliders allow the user to determine the minimum support and the minimum confidence After the user specifies hidher requirements and clicks the Mining button the program begins Valid association rules are displayed to the user as shown in Figure 7 Mining Skin Cancer Database Boundary-Irregularity 1.0791 to 1.1707  Malignant  pre-malignant  Boundary-Irregularity 1.1707 to 1.708 1   Malignant  malignant  Asymmetry 0.0999 to 0.1467  Malignant  benign  Asymmetry 0.2318 to 0.3857  Malignant  malignant  From the rules above we can observe that high boundary irregularity or high asymmetry of the tumor is associated with malignant This coincides with the experts experience 5 Summary In this project we developed a web-based data browsing and content-based retrieval system for a skin cancer database Users can query the database by any combination of the feature attribute values or by synthesized image colors Range queries exact match queries and similarity-based queries are allowed based on any image feature Another contribution of this project is the implementation of an association rule mining algorithm which was originally developed for mining transaction databases Users can find association rules between different skin cancer feature values which can be every useful for skin cancer diagnosis and study References l National Cancer Institute NCI What You Need to Know about Skin Cancer 1995 2 L Xu M Jackowski A Goshtasby C Yu D Roseman and S Bines, "Segmentation of Skin Cancer Images," Image and Vision Computing, 17\(1\1999 pp. 65-74 3 J E Golston W V Stoecker R H Moss and I P S Dhillon Automatic Detection of Irregular Borders in Melanoma and Other Skin Tumors Computerized Medical Imaging and Graphics 16\(3\1992 pp 188-203 4 W V Stoecker W W Li and R H Moss Automatic Detection of Asymmetry in Skin Tumors Computerized Medical Imaging and Graphics 16\(3 1992 pp I91  197 SI R Jain R Kasturi and B G Schunck Machine Vision McCraw-Hill 1995 6 D. H. Ballard and C M. Brown Computer Vision Prentice Hall 1982  P Adriaans and D Zantinge Data Mining Addison Wesley 1996 8 R Agrawal and R Snkant Fast Algonthms for Mining Association Rules Proc of VLDB Conference 1994 pp 487-499 9 M Houtsma and A Swami Set-oriented Mining for Association Rules in Relational Databases Proc of Int Conference on Data Engineering 1995 pp. 25-33 Figure 7. Valid association rules are displayed Some examples of mined association rules are listed below when the minimum support is 12 and the minimum confidence is 60 Boundary-Irregularity 0.9022 to 1.0178  Malignant  benign  615 


4.2.1 The Round Robin Algorithm The main idea behind the Round Robin Algorithm denoted by RRA is rather than selecting a unique victim item per given restrictive association rule we select different victim items in turns starting from the rst item then the second and so on in each sensitive transaction The process starts again at the rst item of the restrictive rule as a victim item each time the last item is reached The rationale behind this selection is that by removing one item at a time from the sensitive transactions it would alleviate the impact on the sanitized database and the legitimate association rules to be discovered since this strategy tries to balance the decreasing of the support of the items in restrictive association rules Selecting the sensitive transactions to sanitize is simply based on their degree of conîict Given the number of sensitive transactions to alter based on   this approach selects for each restrictive rule the sensitive transactions whose degree of conîict is sorted in descending order The rationale is that by sanitizing the conîict sensitive transactions that share a common item with more than one restrictive rule this optimizes the hiding strategy of such rules in one step and consequently minimizes the impact of the sanitization on the discovery of the legitimate association rules The sketch of the Round Robin Algorithm is given as follows Round Robin Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i thei th item is assigned to item v mod k in round robin fashion Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of conîict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End The four steps of this algorithm correspond to the four steps described above in the beginning of this section The rst step builds an inverted index of the items in D in one scan of the database In step 2 the victim item Victim rr i is selected in a round robin fashion for each restrictive associationrule.Line1instep3showsthat  is used to compute the number NumbTrans rr i of transactions to sanitize This means that the threshold  is actually a measure on the impact of the sanitization rather than a direct measure on the restricted association rules to hide or disclose Indirectly  does have an inîuence on the hiding or disclosure of restricted association rules There is actually only one scan of the database in the implementation of step 4 Transactions that do not need sanitization are directly copied from D to D   while the others are sanitized before copied to D   In our implementation the sensitive transactions to be cleansed are rst marked before the database scan for copying The selection of the sensitive transactions to sanitize T ransT oSanitize is based on their degree of conîict hence the sort in line 1 of step 4 When a transaction is selected for sanitization only the victim items are removed from it line 3.1 in step 4 4.2.2 The Random Algorithm The intuition behind the Random Algorithm denoted by RA is to select as a victim item for a given restrictive association rule one item of such rule randomly Like the Round Robin Algorithm the rationale behind this selection is that removing different items from the sensitive transactions would slightly minimize the support of legitimate association rules that would be available for being mined in the sanitized database Selecting the sensitive transactions to sanitize is simply based on their degree of conîict We evaluated the sanitization through the Random Algorithm by selecting sensitive transactions sorted in ascending and descending order The approach based on descending order in general yielded the best results That is why we have adopted such an approach for our algorithm The sketch of the Random Algorithm is given as follows Random Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i  the item assigned to item v is random\(k Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of conîict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


The four steps of this algorithms correspond to those in the Round Robin Algorithm The only difference is that the Random Algorithm selects the victim item randomly while the Round Robin Algorithm selects the victim item taking turns 5 Experimental Results We performed two series of experiments the rst to measure the effectiveness of our sanitization algorithms and the second to measure the efìciency and scalability of the algorithms All the experiments were conducted on a PC AMD Athlon 1900/1600 SPEC CFP2000 588 with 1.2 GB of RAM running a Linux operating system To measure the effectiveness of the algorithms we used a dataset generated by the IBM synthetic data generator to generate a dataset containing 500 different items with 100K transactions in which the average size per transaction is 40 items The effectiveness is measured in terms of the number of restrictive association rules effectively hidden as well as the proportion of legitimate rules accidentally hidden due to the sanitization We selected for our experiments a set of ten restrictive association rules from the dataset ranging from two to ve items in length with support ranging from 20 to 42 and conìdence ranging from 80 to 100 in the database We ran the Apriori algorithm to select such association rules The time required to build the inverted le in main memory was 4.05 seconds Based on this inverted le we retrieved all the sensitive transactions in 1.02 seconds With our ten original restrictive association rules 94701 rules became restricted in the database since any association rule that contains restrictive rules should also be restricted 5.1 Measuring effectiveness In this section we measure the effectiveness of our algorithms taking into account the performance measures introduced in Section 3.3 We compare our algorithms with a similar one proposed in 4 t o h i d e r ul es by reduci ng s upport called Algo2a The algorithm GIH designed by Saygin et al 9 i s s imilar to Algo2a The bas ic dif ference is that in Algo2a some items are removed from sensitive transactions while in GIH a mark  unknowns is placed instead of item deletions Figure 4 shows a special case in which the disclosure threshold  is set to 0 that is no restrictive rule is allowed to be mined from the sanitized database In this situation 30.16 of the legitimate association rules in the case of RRA and RA 24.76 in the case of Algo2a and 20.08 in the case of IGA are accidentally hidden While the algorithms proposed in 4 9 h i d e r ul es reducing their absolute support in the database in our frame        0 5 10 15 20 25 30 35 IGA RRA RA A l g o2a Sanitizing Algorithms Misses Cost   IGA  RRA  RA  Al g o2a  Figure 4 Effect of  on misses cost work the process of modifying transactions satisìes a disclosure threshold  controlled by the database owner This threshold basically expresses how relaxed the privacy preserving mechanisms should be When  0  no restrictive association rules are allowed to be discovered When   100  there are no restrictions on the restrictive association rules The advantage of having this threshold is that it enables a compromise to be found between hiding association rules while missing legitimate ones and nding all legitimate association rules but uncovering restrictive ones Figure 5 shows the effect of the disclosure threshold  on the hiding failure and the misses cost for all three algorithms considering the minimum support threshold   5  Notice that RRA and RA yielded basically the same results That is why their curves are very identical at the scale of the gure As can be observed when  is 0 no restrictive association rule is disclosed for all three algorithms However 30.16 of the legitimate association rules in the case of RRA and RA and 20.08 in the case of IGA are accidentally hidden When  is equal to 100 all restrictive association rules are disclosed and no misses are recorded for legitimate rules What can also be observed is that the hiding failure for RA is slightly better than that for the other approaches On the other hand the impact of IGA on the database is smaller and the misses cost of IGA is the lowest among all approaches before   75  After this value all the algorithms yield similar results Regarding the third performance measure artifactual patterns one may claim that when we decrease the frequencies of some items the relative frequencies in the database may be modiìed by the sanitization process and new rules may emerge However in our experiments the problem artifactual pattern AP was always 0 with all algorithms regardless of the values of   Our sanitization indeed does not remove any transaction The same results can be observed for the algorithms presented in 4 9 We could measure the dissimilarity between the original and sanitized databases by computing the difference between their sizes in bytes However we believe that this Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 0255075100 Disclosure Threshold Hiding Failure IGA RRA RA 0 5 10 15 20 25 30 35 0255075100 Disclosure Threshold Misses Cost IGA RRA RA Figure 5 Effect of  on the hiding failure and misses cost dissimilarity should be measured comparing their contents instead of their sizes Comparing their contents is more intuitive and gouges more accurately the modiìcations made to the transactions in the database To measure the dissimilarity between the original and the sanitized datasets we simply compare the difference of their histograms In this case the horizontal axis of a histogram contains all items in the dataset while the vertical axis corresponds to their frequencies The sum of the frequencies of all items gives the total of the histogram So the dissimilarity between D and D denoted by dif  D D   isgiven by dif  D D   1  n i 1 f D  i   n  i 1  f D  i   f D   i  where f X  i  represents the frequency of the i th item in the dataset X 0 1 2 3 4 5 6 7 IGA RRA RA A l g o2a Sanitizing Algorithms Dissimilarity IGA RRA RA Al g o2a Figure 6 Difference in size between D and D Figure 6 shows the differential between the initial size of the database and the size of the sanitized database when the disclosure threshold  0  To have the smallest impact possible on the database the sanitization algorithm should not reduce the size of the database signiìcantly As can be seen IGA is the one that impacts the least on the database In this particular case 3.55 of the database is lost in the case of IGA 6 in the case of RRA and RA and 5.24 in the case of Algo2a 0 1 2 3 4 5 6 7 0 25 50 75 100 Disclosure Threshold Dissimilarity IGA RRA RA Figure 7 Difference in size between D and D Figure 7 shows the differential between the initial size of the database and the size of the sanitized database for our three algorithms with respect to the disclosure threshold   Again IGA is the one that impacts the least on the database for all values of the disclosure threshold   Thus as can be seen the three algorithms slightly alter the data in the original database while enabling exibility for someone to tune them 5.2 CPU Time for the Sanitization Process We tested the scalability of our sanitization algorithms vis a-vis the size of the database as well as the number of rules to hide Our comparison study also includes the algorithm Algo2a We varied the size of the original database D from 20K transactions to 100K transactions while xing the disclosure threshold  and the support threshold to 0 and keeping the set of restrictive rules constant 10 original patterns Figure 8A shows that IGA RRA and RA increase CPU time linearly with the size of the database while the CPU time in Algo2a grows fast This is due the fact that Algo2a requires various scans over the original database while our algorithms require only two Note that our algorithms yield almost the same CPU time since they are very similar Although IGA sanitizes less sensitive transactions it has an Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


overhead to group restrictive association rules that share the same items and optimizes this process We also varied the number of restrictive rules to hide from approximately 6000 to 29500 while xing the size of the database to 100K transactions and xing the support and disclosure thresholds to   0 Figure 8B shows that our algorithms scale well with the number of rules to hide The gure reports the size of the original set of restricted rules which varied from 2 to 10 This makes the set of all restricted rules range from approximately 6097 to 29558 This scalability is mainly due to the inverted les we use in our approaches for indexing the transactions per item and indexing the sensitive transactions per restrictive rule There is no need to scan the database again whenever we want to access a transaction for sanitization purposes The inverted le gives direct access with pointers to the relevant transactions The CPU time for Algo2a is more expensive due the number of scans over the database 6 Related Work Some effort has been made to address the problem of privacy preservation in association rule mining The class of solutions for this problem has been restricted basically to randomization data partition and data sanitization In this work we focus on the latter category The idea behind data sanitization was introduced in 1 Atallah et al considered the problem of limiting disclosure of sensitive rules aiming at selectively hiding some frequent itemsets from large databases with as little impact on other non-sensitive frequent itemsets as possible Specifically the authors dealt with the problem of modifying a given database so that the support of a given set of sensitive rules mined from the database decreases below the minimum support value The authors focused on the theoretical approach and showed that the optimal sanitization is an NPhard problem In 4 th e a u t h o r s i n v estig ated co n  d e n tiality issu es o f a broad category of association rules and proposed some algorithms to preserve privacy of such rules above a given privacy threshold Although these algorithms ensure privacy preservation they are CPU-intensive since they require multiple scans over a transactional database In addition such algorithms in some way modiìes true data values and relationships by turning some items from 0 to 1 in some transactions In the same direction Saygin et al 9 i nt roduced a method for selectively removing individual values from a database to prevent the discovery of a set of rules while preserving the data for other applications They proposed some algorithms to obscure a given set of sensitive rules by replacing known values with unknowns while minimizing the side effects on non-sensitive rules These algorithms also require various scans to sanitize a database depending on the number of association rules to be hidden Oliveira and Za ane 8 i nt roduced a uni  e d frame w o rk that combines techniques for efìciently hiding restrictive patterns a transaction retrieval engine relying on an inverted le and Boolean queries and a set of algorithms to sanitize a database In this framework the sanitizing algorithms require two scans regardless of the database size and the number of restrictive patterns that must be protected The work presented here differs from the related work in some aspects as follows First we extended our previous work presented in 8 b y addi ng t w o n e w al gori t h ms Round Robin and Random to the set of sanitizing algorithms Second the hiding strategies behind our algorithms deal with the problem 1 and 2 in Figure 3 and most importantly they do not introduce the problem 3 since we do not add noise to the original data Third we study the impact of our hiding strategies in the original database by quantifying how much information is preserved after sanitizing a database So our focus is not only on hiding restrictive association rules but also on maximizing the discovery of rules after sanitizing a database Another difference of our algorithms from the related work is that our algorithms require only two scans over the original database while the algorithms presented in 4 9 requi re v a ri ous s cans depending on the number of association rules to be hidden This is due the fact that our sanitizing algorithms are built on indexes and consequently they achieve a reasonable performance 7 Conclusions In this paper we have introduced two algorithms for balancing privacy and knowledge discovery in association rule mining Our sanitizing algorithms require only two scans regardless of the database size and the number of restrictive association rules that must be protected This rst scan is required to build the index inverted le for speeding up the sanitization process while the second scan is used to sanitize the original database This represents a signiìcant improvement over the previous algorithms presented in the literature 4 9 Our algorithms are integrated to the framework presented in 8 which combines three adv ances for ef ciently hiding restrictive rules inverted les one for indexing the transactions per item and a second for indexing the sensitive transactions per restrictive association rule a transaction retrieval engine relying on Boolean queries for retrieving transaction IDs from the inverted le and combining the resulted lists and a set of sanitizing algorithms The experimental results revealed that our algorithms for sanitizing a transactional database can achieve reasonable Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 140 20 40 60 80 100 DB Size \(transactions in thousands CPU Time \(sec IGA RRA RA Al g o2a 0 10 20 30 40 50 60 70 246810 Set of Restrictive Rules CPU Time \(sec IGA RRA RA Al g o2a AB Figure 8 Results of CPU time for the sanitization process results when compared with the other approaches in the literature Such algorithms slightly alter the data while enabling exibility for someone to tune them In particular the IGA algorithm reached the best performance in terms of dissimilarity and in terms of preservation of legitimate association rules On the other hand the results suggested that RA is slightly better than the other algorithms for hiding failure Although our algorithms guarantee privacy and do not introduce false drops to the data an extra cost is payed because some rules would be removed accidentally since there are functional dependencies between restricted and non-restricted rules The rationale behind this is that privacy preserving association rule mining deals with a tradeoff privacy and accuracy which are contradictory i.e improving one usually incurs a cost for the other It is important to note that our sanitization methods are robust in the sense that there is no de-sanitization possible The alterations to the original database are not saved anywhere since the owner of the database still keeps an original copy of the database intact while distributing the sanitized database Moreover there is no encryption involved There is no possible way to reproduce the original database from the sanitized one Currently we are investigating new optimal sanitization algorithms that minimize the impact in the sanitized database while facilitating proper information accuracy and mining In addition we are working on the optimization of the algorithms RRA and RA specially in terms of preservation of legitimate association rules since their results revealed they are promising 8 Acknowledgments Stanley Oliveira was partially supported by CNPq Conselho Nacional de Desenvolvimento Cient co e Tecnol ogico of Ministry for Science and Technology of Brazil under Grant No 200077/00-7 Osmar Za ane was partially supported by a Research Grant from NSERC Canada We would like to thank Y ucel Saygin and Elena Dasseni for providing us the code of their respective algorithms for our comparison study References 1 M  A tallah  E  Bertin o  A Elmag armid  M  I b r ah im an d V Verykios Disclosure Limitation of Sensitive Rules In Proc of IEEE Knowledge and Data Engineering Workshop  pages 45Ö52 Chicago Illinois November 1999  C  C l i f t on Usi ng S ampl e S i z e t o L i m i t E xposure t o Dat a Mi ning Journal of Computer Security  8\(4\:281Ö307 November 2000 3 C  C lifto n a n d D M a rk s Secu rity an d P ri v a c y Imp licatio n s o f Data Mining In Workshop on Data Mining and Knowledge Discovery  pages 15Ö19 Montreal Canada February 1996 4 E D a s s e n i  V S V e r y k i o s  A K E l m a g a r m i d  a n dE B e r t i n o  Hiding Association Rules by Using Conìdence and Support In Proc of the 4th Information Hiding Workshop  pages 369 383 Pittsburg PA April 2001 5 M  D ietzfelb in g e r  A R Karlin  K  M eh lh o r n  F  M  au f d er Heide H Rohnert and R E Tarjan Dynamic Perfect Hashing Upper and Lower Bounds SIAM Journal on Computing  23\(4\:738Ö761 1994  D E  OêL eary  Kno wledge Disco v e ry as a T hreat to Database Security In G Piatetsky-Shapiro and W J Frawley editors Knowledge Discovery in Databases AAAI/MIT Press pages 507-516 Menlo Park CA 1991 7 S  R M O l i v e i r a a n d O  R  Z a  ane A Framework for Enforcing Privacy in Mining Frequent Patterns Technical report TR02-13 Computer Science Department University of Alberta Canada June 2002 8 S  R M O l i v e i r a a n d O  R  Z a  ane Privacy Preserving Frequent Itemset Mining In Proc of the IEEE ICDM Workshop on Privacy Security and Data Mining  pages 43Ö54 Maebashi City Japan December 2002 9 Y  S aygi n V  S  V e r yki os and C  C l i f t on U s i n g U nkno w n s to Prevent Discovery of Association Rules SIGMOD Record  30\(4\:45Ö54 December 2001 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


