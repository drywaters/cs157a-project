 1 Survival Analysis Approach to Reliability, Survivability and Prognostics and Health Management \(PHM   Zhanshan \(Sam\ Ma  Axel W. Krings  sam@cs.uidaho.edu  krings@cs.uidaho.edu   Computer Science Department University of Idaho Moscow, ID 83844, USA  Abstract  Survival analysis, also known as failure time analysis or time-to-event analysis, is one of the most significant advancements of mathematical statistics in the last quarter of the 20th century. It has become the de facto  standard in biomedical data analysis. Although reliability 
was conceived as a major application field by the mathematicians who pioneered survival analysis, survival analysis failed to establish itself as a major tool for reliability analysis. In this paper, we attempt to demonstrate by reviewing and comparing the major mathematical models of both fields, that survival analysis and reliability theory essentially address the same mathematical problems Therefore, survival analysis should become a major mathematical tool for reliability analysis and related fields such as Prognostics and Health Management \(PHM paper is the first in a four part series in which we review state-of-the-art studies in survival \(univariate\ analysis  competing risks analysis and 
multivariate survival analysis  with focusing on their applications to reliability and computer science. The pres ent article discusses the univariate survival analysis \(survival analysis hereafter  I NDEX T ERMS Survival Analysis, Reliability, Network Survivability, Prognostic and Health Management \(PHM Software Reliability  T ABLE OF C ONTENTS 1  2   1  I NTRODUCTION 1   1.1 Important Definitions in Reliability Theory  1.2 Important Modeling Strategies   1.3 Does Reliability Exist Independently of 
Mathematical Modeling  1.4 Who Studies Reliability? -- Is There a Difference   between Reliability Theory and Reliability Engineering 2  E SSENTIALS E LEMENTS OF S URVIVAL A NALYSIS 5  2.1 A Brief History  2.2 Censoring and Statistical Models  2.3 Modeling Survivability with Random Censoring  2.4 Survival Distribution Models -- Parametric Models  2.5 Covariates Regression Models: Proportional   Hazards and Accelerated Failure Time Models  2.6 Counting Process and Survival Analysis  
 1  1-4244-14881/08/$25.00 ©2008 IEEE 2  IEEEAC paper #1618, Final Ve rsion Updated Dec. 27, 2007   2.7 Bayesian Survival Analysis  2.8 Spatial Survival Analysis  2.9 Survival Analysis and Artificial Neuronal Network 3. B RIEF A PPLICATION C ASE R EVIEW   16   3.1. Applications Found in IEEE Digital Library   3.2 Selected Papers in MMR-2004 4. S UMMARY   17 5.  R EFERENCES  
18     1  I NTRODUCTION   The modern mathematical theory of reliability was established more than half a century ago, predominantly based on probability theory \(e.g., Bazovsky 1961\eed the probability-based reliability theory has been considered a very successful field in app lied mathematics.  An example that shows the importance of reliability research in the mathematical literature is a survey conducted by Aven and Jensen \(1999\ey found that 1% of the literature \(then indexed by Zentralblatt/ Mathematical Abstracts and Mathematical Reviews\ are a ssociated with the keyword reliability In practice, the success of reliability engineering 
is obvious, embedded in every engineering endeavor performed by human being, ranging from the Apollo Moonlanding project, co mmercial airplanes, to consumer electronics.  However, like many scientific theories reliability theory is far from perfect. Computer networks and software, in particular, create serious challenges for traditional reliability theory. The challenges have been well recognized \(e.g., Krings 2008, Munson 2003, Shooman 2002, Xie 1991\r example, in software engineering, the notion of failure time, which is the core of any traditional reliability model, often becomes less relevant, since most failures are latent until the software features which cause the failure are triggered. An even more obvious difference is that software does not wear out 
nevertheless, the outdatedness caused by the environment updates \(such as Operating Systems\cause massive failures or even render the software useless under new environments. Similarly computer viruses or other security breaches render traditional reliability theory of limited value. In emerging technologies such as wireless sensors and ad hoc networks spatial topology or organizational geometry may not be ignorable due to the limited signal range. All these challenges point to the same conclusion, there is an urgent need to develop new approaches for modeling reliability and survivability for computer software and networks 


 2 The problems with software and computer networking are so serious that the paradigm of survivable network systems SNS\was proposed as a new discipline in the late 1990s e.g., Ellison et al. 1997, Krings and Ma 2006, Krings 2008\ivability can conceptually be considered as a system's capability to endure catastrophic failures, such as a network system under malici ous intrusions, but still preserve mission critical functionalities.  To some extent reliability is the foundation of security and survivability. A survivable system generally has to be reliable, and an insecure and/or unreliable system generally is not survivable. What makes surviv able network system \(SNS or survivability so important is the fact that today's computer networks control critical national infrastructures  An obvious point is that it is desirable to develop a theory of survivability that can be put into a unified framework with reliability. An intuitive idea could be to define both reliability and survivability in a single unified probability space with some mech anism to differentiate the malicious events over which the probability measures may be unknown. The difficulty lies in the reality that most events associated with survivability are generally unpredictable due to the nature of malicious intrusions. In addition, it appears that a probability definition for survivability alone, similar to that for reliability, may be equally infeasible, since mathematically the malicious intrusions correspond to the event points where probability measures  may not exist Indeed, despite its critical importance and significant efforts there is not a well-accepted mathematical definition for survivability. One of the objectives of this article is to propose treating unpredictable events such as malicious intrusions as censoring in a survival analysis model; in this way, survivability can be assessed with a survival \(survivor function, which has the exact same definition as tradition reliability. We argue that this unorthodox use of censoring to model survivability with survival analysis should be feasible at least for some computer networks, such as wireless sensor networks, given the essential similarity between a population  of patients  in a clinical trial and a population of sensor nodes in a wireless sensor network  1.1. Important Definitions in Reliability Theory  Let us recall some of the most essential definitions of reliability theory.  Assume we are concerned with a device that fails at an unforeseen or unpredictable random time \(or age T 0, with distribution function F  t   R t t T P t F     1 and probability density function \(pdf   t f   The reliability   t R is defined as    1   t F t R  2  This has the exactly same definition as the survival function in survival analysis  The failure rate   t is defined as the ratio of pdf to reliability   t  f  t  R  t  3  Failure rate  t  measures inclination to failure at time t given  t  t P  T t t  T t  for all t  It is also called as instantaneous failure rate  hazard rate  force of mortality  intensity rate or conditional failure rate  Obviously t t   is the conditional probability that a device surviving to time t will fail in the interval    t t t   The cumulative hazard rate \(fun ction function   t H is defined as  t t R ds s t H 0   ln      4 This derives     exp   exp   0 t ds s t H t R  5  The reliability modeling is then mainly concerned with collect information about the state of the system being investigated, and one should be alerted that different information level might lead to different reliability models Aven and Jensen 1991  1.2. Important Modeling Strategies  The following is a brief description of important modeling approaches for reliability, mainly based on Aven and Jensen 1999  1.2.1. Compounded Systems  Compounded Systems is what Aven and Jensen \(1991 called complex systems.  It is made of n components with positive random lifetimes T i  i 1 2 n  n N  Let 0,1 n 0,1 be the structure function of the system e.g., a parallel or series structure of n components.  The possible states of a component can be intact and failed  which is indicated by the number 1 or 0 respectively.  Let     t X t denote the state of the system at time t where X  t   X t 1  X t 2 X t  n  and X t  i   denotes the indicator function for component i at time t i.e X t  i   1, if T i  t and X t  i herwise.  The system lifetime is then given by  0    inf t R t T 6  For example, in simple series or parallel systems may take the  min   max   Under the assumption of independent component failures the random variables i T are i.i.d independent and identically distributed e standard problems such as 


 3 Aven and Jensen. 1999\: \(1\ inferring the system lifetime distribution from the lifetime distributions of its components; \(2\sing the effects of component reliability on the whole syst em; \(3\finding if certain properties of the component lifetime distribution, such as IFR \(increasing failure rate\are lost by forming a monotone  system. A monotone system is characterized by the property that its structure function   t is non-decreasing with each argument, which guarantees th at system reliability never gets hurt by improving the reliability of a component \(Aven and Jensen. 1999  1.2.2. Damage Models  Another reliability formulation can be based on the damage  or state observation of the system at time t Let X  t he damage or state random variable at time t and let S be the level of damage that leads to failure.  Then the system lifetime can be defined as       inf S t X R t T 7  where S can be a constant or a random variable independent of the damage process.  For example, a Wiener process with positive drift starting at 0 and the failure threshold of constant S may be used to describe the damage process, and the resulting system lifetime follows the in verse Gauss Distribution \(Aven and Jensen. 1999  1.2.3. Compound Point Process  This category of model describes the so-called shock  process where the shocks cause random amounts of damage to the system \(Aven and Jensen. 1999\ve occurrence times of shocks form an increasing sequence  0 2 1 T T of random variables.  Each time point T n  is assigned a real-valued random mark  V n which is the additional damage caused by the n th shock.  Let the marked point process be  T  V   T n  V n  n N  The resulting compound point process  X defined as   n n n V t T I t X     1 8  where I  T n  indicator function for T n Note that X  t  represents the accumulated damage up to time t The simplest form for X  t ompound Poisson process, in which the shock arrival time follows Poisson distribution and the shock amounts V n  i.i.d random variables Aven and Jensen. 1999\. The system lifetime T is the first hit time when the damage process   t X reaches level S  The level S may be constant or be a random variable.  A random failure level S is adopted when the observed damage process does not carry determ inistic information about the system failure state \(Aven and Jensen. 1999  The above process is elegantly described by the martingale  theory as exemplified in Aven and Jensen 1999. A martingale is the mathematical model for a fair game with constant expectation function eq ual to zero across the time domain. It provides an effect ive and flexible mathematical tool for studying lifetime random processes, which is often properly described by a point process. A point process can be described by an increasing sequence of random variables by a purely atomic random measure, or via corresponding counting process. The Poisson process is one of the simplest point processes with a semi-martingale representation Markov chains generally have smooth semi-martingale SSM\ representation. A very desirable property of a stochastic process with SSM repr esentation is that it can be decomposed into a drift or regression part and an additive fluctuation described by a martingale \(Aven and Jensen 1999\ve an expectation of zero across time domain.  With respect to survival analysis, the rigorous mathematical theory of surviv al analysis can be derived from the counting  process and its martingale models.  This again demonstrate that both relia bility and survival analysis address the same or very sim ilar mathematical problems  1.2.4. A General Failure Model  Aven and Jensen \(1999\ summarized a general failure model based on modern stochastic and Martingale theory Several terms need to be defined first  Path set A path set is a set of components whose functioning guarantees the functioning of the system  Smooth semimartingale SSM\hastic process Z Z  t ed SSM if it has a decomposition of the form   t t s M s d f Z t Z     0 0 9  Filtration Let     P F be the probability space, where  is the sample space F is the algebra of measurable sets and P is the probability measure.  The information up to time t is expressed by the pret history F t which consists of all events in F that can be distinguished up to and including time t The filtration F    R t t F is the family of increasing pret histories that follow th e standard conditions of completeness and right continuity  Let     t T I t Z denote the simple indicator process and T be the lifetime random variable defined on the basic probability space     P F  Z is the counting process corresponding to the si mple point process T n  1 T T and n T for  2 n  The paths of this indicator process Z are constant, but one jump from 0 to 1 at T  Assume that this indicator process has a smooth F semimartingale representation with an F martingale 0 M M  and a nonnegative stochastic process    t   


 4         0 R t M ds s s T I t T I t t  10  The filtration F  and the corresponding F SSM representation of the indicator process then define a general lifetime model  The process  t   t R in the above SSMrepresentation is termed the F failure rate or F hazard rate process   The compensator is defined as    ds s s T I t H t       0 11  which is also called F hazard process   A key point in the above general model is that the failure rate   t is interpreted as the limit of the conditional expectation with respect to the pret histories F    R t t F         lim   0 h F h t T P t t h  12  Aven and Jensen \(1999\demonstrated that this general failure model could conveniently derive all above models and resolve issues such as Simpson's paradox. They indicated that the main advantage of the semi-martingale representation is the random evolution of a stochastic process on different information levels They warned that the monotonicity properties in general are not preserved when changing the observation or information level  1.3. Does reliability exists independently of  mathematical modeling  Reliability, intuitively, should be the property of a physical system and exist independent of the mathematical theory adopted to measure it.  Altho ugh it is beyond the scope of this paper to argue whether or not mathematical objects are discovered or created, the an swer to the question seems elusive even if we choose to ignore the philosophical argument.  Let us use some examples to make the points  Assume that there is a well-accepted definition for reliability such as described ab ove.  However depending on the information available to evaluate system reliability, the results can be different Simpson's paradox is a revealing example. As Aven and Jensen 1999\ demonstrated that Simpson's paradox may occur in parallel systems in some parameter ranges, but not in series systems. They revealed that this has to do with the fact that the failure time of a parallel system consisting of components with constant failure rates does not follow ex ponential distribution and the system failure rate is non-monotonic. In the case of series construction, however the system lifetime still follows the exponential distribution. This problem is called change of information level in Aven and Jensen \(1999 Unfortunately, monotonicity is not preserved when changing the observation or information level. Therefore even with the well-defined pr obability theoretic definition the measurement of reliability is strongly influenced by the information collected, and by the assumptions made about the components. This implie s that the mathematically correct models supported w ith rational assumptions at particular levels may not be sufficient to determine the system level reliability \(Aven and Jensen 1999  1.4. Who studies reliability  Is there a difference betw een reliability theory and reliability engineering?  Resear chers involved in reliability theory come largely from three groups. The first is pure mathematicians who are mostly interested in mathematical problems such as reliability on graphs, reliability polynomial, and connectivity of abstract electric networks which leads to random cluster model a field that integrates probability, graph theory an d stochastic geometry Grimmett 2006 is an important topic in percolation theory, which st arted with the studies of disordered media \(Grimmett 19 99\ Similarly, reliability has been studied in random graphs \(e.g., Bollobás 2001 Graver and Sobel 2005\Overa ll, the reliability studied by pure mathematicians is often related to connectivity of graph models of networks.  A lthough the reliability models of this type are rarely direc tly applicable in reliability engineering, their studies often provide deep insights to the second group of researchers, who are applied mathematicians and statisticians They are responsible for much of the formal theory like what was described above The third group refers to reliability engineers whose engineering designs require modeling of specific engineered systems, which demands both the deep understanding of the physical system involved as well as the reliability theory Aven and Jensen \(1999\.  As expected, there are no clear boundaries among the three groups. Two excellent monographs, among many others, show the cross-boundary examples: Aven and Jensen \(1999\ay belong to the first and second groups, while Meeker and Escobar \(1998\ may belong to the second and third groups  The probabilistic nature and the mission criticality of applications, such as heart pacemaker, space shuttle commercial airplane, and large scale civil architectures such as bridges and dams, make re liability engineering among the most challenging and demanding profession Understandably, the high dependability demands the extreme caution when transferri ng theory into practice That may explain apparen tly occasional disconnection between reliability theory a nd practice, since reliability engineering may be hesitant to adopt untested reliability theory due to the huge stakes involved. This paper is an attempt to bring survival analysis or failure time analysis largely done by mathema ticians \(especially biomathematicians and bio-statisticians\dely adopted as 


 5 the de facto standard in biomedicine fields to the attention of engineering reliability research  In this introduction, we briefly introduced the essential models in reliability theory The discussion of survival analysis actually starts in the next section.  However, it is necessary to note two points. \(1\The major definitions in subsection 1.1 in reliability th 5 mathematically the same as th eir counterparts in survival  analysis.  The only difference is that reliability  R  t s equivalent to survivor function  S  t Therefore, we will not repeat these essential definitions, but they form the foundational definitions in survival analysis too. \(2\he modeling strategy, especially the general failure model of Aven and Jensen \(1999\, represe nts the state-of-the-art of theoretical modeling in reliability theory, which is based on measure theoretic stochastic processes.  This approach is very similar to the formal theory of survival analysis, which is based on the counting stochastic process and Martingale central limit theorem We will have a brief introduction on this in section 2.6  The remaining of this paper presents a review of the essential elements of survival analysis \(Section 2\nd advantages of survival anal ysis over tradition reliability theory. In Section 3, we bri efly survey and analyze the status of survival analysis applications in IEEE related engineering and computer science fields. Specifically, we analyze why survival analysis has been frequently associated with artificial ne uronal networks \(ANN\in engineering and computer science applications and its relationships with ANN.  The final section is a brief summary and perspective for its further applications in reliability engineering, su rvivability modeling, and computer science.  It should be noted that in this paper we limit the discussion of survival analysis to univariate  survival analysis. We postpone the discussion of competing risks analysis and multivariate survival analysis to Ma and Krings \(2008a, b & c   2  E SSENTIALS OF S URVIVAL ANALYSIS T HEORY   2.1. A Brief History  Survival analysis \(SA\, or failure time analysis, is a specialized field of mathemati cal statistics, developed to study a special type of random variable of positive values with censored observations, of which failure time or survival time events are the mo st common.  The events are not necessarily associated with failure or survival at all, and perhaps time-to-event is a better term.  Examples of time-toevent random variables are the lifetimes of organisms failure time of machine components, survival times of cancer patients, occurrence of the next traffic accident, or durations of economic recessions Survival or failure times are often used to represent time-to-event random variables However, the scope and gene rality of time-to-event random variables are much broader than the "survival" or "failure times in their strict literal meaning.  The generality and ubiquitousness of time-to-eve nt random va riables will become self-evident if we look into the nature of timedependent processes.  There are two major categories of observation data in studying time-dependent processes: one is the time-series data   which is well recognized, and the other is time-to-event data which is perhaps less recognized but equally important and ubiquitous  A particular challenge in an alyzing survival data is information censoring, i.e., the observation of survival times is often incomplete.  For exampl e, in a medical clinical trial of a new drug, some individuals under observation may be dropped from the experiment for various reasons, ranging from complications from other diseases to death caused by accident.  This kind of censor ing may be treated as random censoring. In other cases, the observation may be terminated before the process is fully developed, which leads to observation truncation.  One example is the information recorded in the black box of an airplane up to a crash.  This is a typical case of right  censoring or truncation where the observation is terminated at a fixed time.  Besides random and right censoring left  censoring or truncation is also possible.  For example, the tim e an enemy aircraft crosses a border may never be known exactly and thus the radar detection time may be left cen sored.  Traditional statistics and reliability theory simply do not have mechanisms to extract the partial information from the censored  observations.  Either including or excluding the censored observations from analysis will induce statistical bias, which could lead to incorrect infere nces.  In survival analysis unique mathematical models an d methodologies have been developed to extract the partial information from the censored observations maximally, without inducing unwarranted bias, as long as censoring is not too severe The ability to handle censoring in an effective manner is the most important and unique advantage of survival analysis However, observation censoring or incomplete information are not necessary for the appli cation of survival analysis  Survival analysis was initially advanced at the UC Berkeley in the 1960s to provide a better analysis method for Life Table data.  Chiang's \(1960\pers expanded Kaplan Meier's \(1958\ classic work on the survivor function Aalen's \(1975\ established rigorous mathematical theory of survival analysis based on the Martingale theory and Counting stochastic process.  It is interesting that Aalen's work probably was not fully appreciated until the late 1980s.  Fleming & Harrington \(1991\and Andersen Borgan, Gill & Keiding's \(1995\ to be the only expansion \(in the form of monographs\ of the theoretic foundation set by Aalen \(1975\. As expected, besides the monographs, there are numerous theoretic research articles which we choose to skip given our discussion is largely application-oriented  On the statistical science fr ontline, the development of statistical procedures and mode ls for survival analysis exploded in the 1970s and 1980s were mostly derived from 


 6 asymptotic methodology.  By the late 1980s and early 1990s, survival analysis ha d established itself as the de facto  standard statistical method in biomedical research.  In medical schools, survival analysis became a dominant part of the standard biostatistics curriculum, and survival analysis is almost universally adopted for data analysis in major medical journals such as JAMA and the New England Journal of Medicine Between the publishing of the first version of key monographs \(such as Kalbfleisch & Prentice 1980 and Lawless 1982\ their second editions Kalbfleisch & Prentice 2002 and Lawless \(2003\\, more than one dozen of monographs ab out survival analysis have been published.  Besides the classics, such as the abovementioned two, Cox and Oakes \(1984\, and Klein and Moeschberger  2003  two other monographs stand out  Analysis of Multivariate Survival Data by Hougaard 2000\ and Bayesian Survival Analysis by Ibrahim, Chen Sinha \(2004 the later two monographs indicate their special contributions  If asked to pick out a single fundamental model of survival analysis it would be the Proportional Hazard Model originally proposed by D. R. Cox \(1972\.  The Cox proportional model was initially developed as an empirical regression model.  Furthermore, it was found that the empirical model captures some of the most fundamental properties of hazard functions and the basic framework of Cox's model still holds after numerous extensions \(e.g Therneau and Grambsch 2000  Outside of biomedical fields, su rvival analysis has not been widely adopted, not even in ecology and other non-medical biology fields. Industry reliability analysis was actually envisioned as one of the most promising fields for survival analysis, besides the biomedi cal statistics. In 1991, NATO organized an Advanced Research Workshop on survival analysis and published pro ceedings from the workshop Klein & Goel 1992\di ng a section on the application of survival analysis in industr ial reliability.  Meeker and Escobar \(1998\ published an excellent monograph, with strong emphasis on survival analys is approach to reliability modeling  Two relevant international conferences are particularly worthy mentioning: the first is the biannual International Conference on Mathematical Methods in Reliability MMR and the latest conference proceedings was published in a volume edited by Wilson et al. \(2005\he second is the International Conference on Reliability and Survival Analysis ICRSA\, last held in 2005. It appears that no conference proceedings were published other than the abstracts posted at http://www.isid.ac.in/~statmath/smuconferences  conference05/abstracts.html   In our opinion, the potential of survival analysis in engineering reliability has not been fully expl ored yet. It appears that the application in computer science and IEEE related engineering is still limited to a handful of ad hoc  applications. For example, in a recent online search of the IEEE digital library, we found about 40 entries with the keyword survival analysis however only about a half of which is the application of surv ival analysis in engineering reliability and the others are  su rvival analysis application in biomedicine or other fields In addition, among the 40 papers, about one-fourth is the application of artificial neural networks \(ANN\o model fitting for survival analysis.  Furthermore, with the keyword competing risks analysis we obtained about 20 papers, but found no paper with the keywords multivariate survival analysis or shared frailty The latter two fields are the areas that most of the new advances in survival analysis are made \(which are discussed in Ma and Krings 2008a, b & c, and this article is limited to univariate survival analysis  2.2. Censoring and Statistical Models  It is largely accurate to treat survival analysis as the statistics for lifetime data with censoring.  Information or observation censoring is common in failure time data and reliability analysis.  Censoring refers to the situations in which exact lifetimes are fully observed for only a portion of the individuals in a statistical population sample Formally, an observation is right censored at C if the lifetime T known to be greater than or equal to C  Similarly, an observation is le ft censored if the lifetime is only known to be less than or equal to C Lawless 1982 2003\precise definitions can be achiev ed by further distinguishing censoring as type-I, type-II and random censoring each of which can be referred to as either left or right censoring. For simplicity, we only discuss their meaning for right censoring, which is the most common in failure time data  Type II censoring only knows the r smallest \(shortest lifetimes\ sample of n individuals  1  n r  The observation data consists of the r smallest lifetimes    2   1   r T T T out of a random sample of n lifetimes T 1  T 2 T n If T 1 T 2 T n are i.i.d and have a continuous distribution with pdf f  t survivor function S  t  joint pdf of right censoring    2   1   r T T T is  r n r r t S t f t f t f r n n                 2   1  14  With the pdf for censored sample individuals, statistical inference can be made based on it  In type-I  censoring observation is cut off at some prespecified time and an indivi dual's lifetime will be known exactly only if it is less than the pre-specified value.  One difference between the two types of censorings is that for type-I the number of fully observed individuals is a random variable, but for the type-II it is fixed.  In a sample of n individuals, each individual i e T i and a possible fixed censoring time C i The T i are assumed to be i.i.d with pdf f  t urvivor function S  t  The exact lifetime T i  of an individual will be observed only 


 7 if T i C i  When all the C i are equal, it is called singly Type-I censored.  The data from type-I censoring consists of n pairs of random variables  t i  i  where   min i i i C T t    i i i i i i C T if C T if 0 1  15 The joint pdf of    i i t is  i i i i C S t f 1     16  t i  is a mixed random variable with both discrete \(if censored\ and continuous parts  If the pairs    i i t are independent, th e likelihood function is  i i i n i i C S t f L 1 1     17 An observation from the above likelihood function is:  that each observed lifetime contributes a term f  t h censored time contributes a term S  C i This observation has wide applicability \(Lawless 2003, Kalbfleisch and Prentice 2002  Random censoring occurs naturally.  For example, in medical research, patients may be admitted into the study in somewhat random fashion contingent on their diagnostic times.   If the study is terminated at some pre-specified time then the censoring times, which start from the random entry points until the termination, are random. If C i is specified with different distributions, each with density mass at one fixed point, Type-I censoring can then be treated as a special case of random censoring. Often the likelihood function for type-I can be used to approximate that of random censoring  If censorings and failure events are dependent, although it might be difficult to write down a model that represent the process, the above likelihood function structure still largely holds \(Lawless 2003, Kalbfleisch and Prentice 2002\The above discussion of censoring is simplified greatly to highlight the principles of censoring and lifetime observations. The study of the failure-censoring pair random variables influences nearly every aspect of survival analysis  2.3. Modeling Survivability with Random Censoring  From the discussion of censoring in the previous section, in particular the random censoring mechanism, we propose to use the random censoring mechanism to describe the unpredictable events such as malicious intrusions in the modeling of network survivability A comparative analysis of random censoring and malicious intrusions to computer networks \(or any survivable systems arguments.  The most signifi cant similarity is that both random censoring and malicious intrusions are unpredictable.  In other words, we generally do not even know the prob ability that the event may happen Specifically, the striking time of malicious act can be considered largely random but the event is only describable after it occurs.  Therefore, the striking times can be treated as the censoring events. Similarl y, in a clinical trial of biomedicine, censoring should be unpredictable, otherwise the experiment is biased.   Furthermore, by introducing different levels of censoring \(e.g., percentage of censored individuals\one can simulate the effects of the strike intensity on the survivor function.  If one defines survivability as a threshold of reliability breakdown \(e.g survivor function crosses some threshold value kind of simulation can produce very important insights  2.4. Survival Distribution Models—Parametric Models   As we have mentioned previous ly, the basic definitions of survival analysis, such as hazard rate \(failure rate cumulative hazard function, p robability density function distribution functions are exactly the same as those in reliability theory introduced in Subsection 1 1 [Equations 1\\(5 T h e only di ffe re nce is the t e rm inology i e  survivor function vs reliability but both have exactly the same mathematical definitions. However, beyond the similarity at the definition level the two fields diverged significantly.  Through the remainder of this paper, the major concepts and models are drawn from the survival analysis literature  The following three concepts ex ist in both fields; however there are slight differences in their formulations mean residual lifetime  median lifetime and p-th quantile also known as the 100p-th percentile   The mean residual life mrl cted remaining lifetime for an individual of age t and is defined as mrl  t  E  T t  T t  It can be proved that the following equation holds               t S dx x S t S dx x f t T t mrl t t 18  The mrl is the area under the survival curve to the right of t  divided by S  t In contrast, the mean lifetime equivalent to the mean time to failure MTTF\reliability theory, is the total area under the survival curve  MTTF E  T  tf  t  dt S  t  dt 0 0   19  The variance of T is related to the survival function by  2 00     2   dt t S dt t tS T Var  20  The p th quantile of the distribution of T is the smallest t p  such that   1    inf    1   p t S t t e i p t S p p 21 


 8 If T is a continuous rando m variable, then the pth quantile can be found by solving p t S p 1     The median lifetime mlt h percentile t 0.5 of the distribution of T and is obtained by solving the following equation S  t p  1 0.5 0.5   Since failure time \(or lifetime\ply a non-negative random variable, the most natural approach is to study its probability distributions. Again the probability distributions used in both reliability and surv ival analysis are often the same.  The most commonly used distributions include exponential, Weibull extreme value distribution, gamma distribution, log-gamma, lognormal, generalized gamma logistic, log-logistic, and inverse Gaussian.  Distribution models discussed in this section are referred to as parametric models in the literature  We will look at two examples, exponential distribution and Weibull distribution.  The exponential distribution can serve as the baseline for more complex models, given its constant failure rate 0  0   t t h     22 with pdf   f  t  e t  23  The survivor function is  t e t S   24  and the mean and variances are  1 and 2  respectively  When 1 it is termed standard exponential distribution.  In addition, expo nential distribution is special cases of both the Weibull and gamma distributions  The Weibull distribution is perhaps the most widely used lifetime distribution.  Its hazard rate is   1     t t h 25  where 0 and 0 are parameters.  When 1  Weibull distribution becomes exponential distribution.  Its pdf and survivor functions are  0    exp     1 t t t t f  26   S  t  exp t   t 0  27  The hazard function of Weibull distribution is monotonic increasing if  1 decreasing if  1 and constant for  1  is therefore termed shape parameter. The is called scale parameter  Weibull distribution can be derived from uniform distribution U in the interval [0 1] conveniently       ln   1 U T  28  This formula can be used for Monte Carlo simulation of Weibull distribution  There is also a three-parameter version Weibull distribution by replacing the variable t with t–u where u is called location parameter  The following two distributions are with bathtub-shaped hazards functions   t t t h   29    exp   1 t t t h  30  Klein and Moeschberger \(2003\ummarized a near exhaustive list of distributions us ed in survival analysis. The distribution models described above are for continuous failure time data.  Any of the continuous failure time models can be used to generate a discrete model by introducing grouping on the time axis \(Kalbfleisch and Prentice 1980 2002, Klein and Moeschberger \(2003  The estimation and comparison of survival function S  t  which is equivalent to the reliability R  t y theory, with the above distribution models form a significant part of the early development  of  survival analysis.  Estimating survival function or fitting lifetime data to survival distributions was the earliest endeavor in this field.  Three approaches stand out: the Kaplan-Meier estimator for survivor function \(Kaplan and Meier 1958 Chiang's \(1960\he Nelson-Aalen estimator for cumulative hazard functions \(Nelson 1969 The life table approach is particularly important in population demography and actuarial study.  To compare the survival functions, several statistics have been devised or extended from classical statistics to censored data, such as the Wilcoxon test Savage log-rank test, and the KruskalWallis test \(Kalbfleisch and Prentice 1980, 2002\.  In recent years, great progress has been made in improving the estimations and comparisons of survivor functions, such as the integration of Bayesian approach and Markov Chain Monte Carlo \(MCMC\ simulation, and kernel-smoothed estimation. One can refer to Klein and Moeschberger \(2003 for comprehensive details  2.5.  Covariates Regression Models: Proportional Hazards  Models and Accelerated Failure Time Models  The previous section focu sed on using probability distributions to model survival of a homogenous population Translated into the reliability field, a homogenous population implies that devices experience exactly the same environmental covariates, a nd the covariates are not 


 9 correlated to failure times.  For example, this would imply that operation environment parameters such as temperature have no effects on the failure rates of devices.  The covariates regression approach introduced in this section eliminate this restriction.  Th e basic problem can be formed as follows: given survival time T 0, a vector     2 1 s z z z z of covariates that may influence T is observed.  Furthermore, the vector z itself can also be time dependent and may include bo th quantitative and qualitative indicative variables. In this approach, Cox's \(1972 proportional hazards model was initially treated as largely an empirical regression model, but later it was found that the framework of the model posse sses exceeding flexibility to capture major hazards effects and failure mechanisms.  The Cox model has been extended numerously and it forms the foundation for the covariates regression approach \(Therneau and Grambsch 2000  There are two ways to construct covariates regression models. The first is to extend survivor distributions such as the exponential and Weibull dist ribution, and this approach forms so-called mixture models. The second approach has no assumptions on the survival distribution, also called distribution-free approach, which is the approach adopted by Cox \(1972, 1975\. We briefly introduce the first approach by using exponential and Weibull distributions as examples. The approach can be extended to other distributions, including discre te distributions \(Kalbfleisch and Prentice 1980, 2002  2.5.1. Distribution-Dependent Regression Models Parametric Regression Models  With exponential distribution, the hazard rate is constant The hazard rate at time t for an individual with covariates z  can be written as    t  z   z  31  i.e, the hazard for a given vector z is constant; however, it varies with covariates z The  z ay be modeled in many ways.  In the simplest case it assumes a linear relationship  z and thus   t  z  g  z  32  where is the vector of regression coefficients for vector z and z is the  inner product of the two vectors.  Here g is a function form, for example   g  x  exp x  33  With g ing on an exponential form — the exponential distribution models under covariates z the hazard function is  t  z  exp z  34  The conditional pdf of T   given z is   f  t  z  e z exp te z  35  The above hazard function implies that the log failure rate is a linear function of covariates z Let Y ln T     ln   and W be a random variable with extreme value distribution  the log of the exponential failure time T nder covariates z  can be expressed as   Y z W  36  This is a log-linear model with many nice properties  Similar to the previous extens ion, Weibull distribution can be extended with covariates regression.  The conditional hazard for Weibull distribution is    t  z   t  1 e z  37  where  represent the covariate regression vector to avoid the confusion with the parameter of Weibull distribution The conditional pdf for Weib ull distribution is then     exp      1 z z e t e t z t f  38  The effects of covariates also act multiplicatively on the hazard in the Weibull distribution.  In terms of the log  failure time, the above conditi onal hazard function specifies the log-linear model. With Y ln T    ln     1 and   we get the log-linear Weibull model  W z Y   39  where W follows the extreme value distribution.  The above extensions of exponential and Weibull distributions demonstrate two important properties \(Lawless 2003 Kalbfleisch and Prentice 2002\. First, the effects of covariates act multiplicatively on the hazard function This general relationship inspired the general proportional hazards model.  Second, the lo g-linear models suggest that the covariates act additively on the log failure time This inspired the development of the accelerated failure time model.  Both models are described briefly below  2.5.2. Distribution-Free Regression Models Semi-parametric Regression Models  The next two subsections introduce two of the most important survival analysis models.  They do not assume any specific statistical distribu tions, and are therefore called distribution-free or semi-parametric approaches.  Both models have been extended numerously.  The Cox proportional hazard model is probably the most influential one in the entire survival analysis subject  Cox's Proportional Hazards Model \(PHM  Let  t  z  be the hazard function at time t for an individual with 


 10 covariates z The proportional hazards model \(PHM\ first proposed by Cox \(1972, 1975\, is    t  z  0  t  e z   40  where 0  t  is an arbitrary unspecified base-line hazard function for continuous failure time T In the PHM, the covariates act multiplicativel y on the hazard function.  By substituting 0  t  with the corresponding hazard function for exponential or Weibull distributions, the previous distribution-dependent models for exponential or Weibull distribution can be derived as special cases  The conditional pdf of T given z corresponding to the general hazard function    z t is     exp      0 0 0 t z z du u e e t z t f  41  The conditional surv ival function for T under z is   S  t  z   S 0  t  exp z   42  where     exp   0 0 0 t du u t S 43  The allowance of an arbitrary 0  t  makes the model sufficiently flexible for many applications  Extensions of Cox's Proportional Hazards Model PHM There are numerous extensions to Cox's PHM Therneau and Grambsch 2000\. Among those extensions two of the extensions strata and time-dependent covariates  are particularly important, but do not substantially complicate the parameters estimation \(Kalbfleisch and Prentice 2002, Lawless 2003   Suppose there is a factor that occurs on q levels and for which the proportionality assumption of PHM may be violated.  The hazard function for an individual in the jth stratum or level of this factor is    exp      0 z t z t j j 44 for j 1,2 q  where z is the vector of covariates for PHM The baseline hazard functions 01  0 q  for the q  strata are permitted to be arbitrary and are completely unrelated.  The direct product of the group \(stratum likelihood can be utilized to estimate the common parameter vector  Once the parameter is estimated, the survivor function for each stratum can be estimated separately  The second generalization to the proportional hazards model is to let the variable z depend on time itself.  For unstratified PHM, the hazard function is     exp       0 t z t t z t  45  and for stratified PHM it is      exp       0 t z t t z t j j     2  1 r j 46  The procedure used to estimate the is to maximize the socalled partial likelihood functions as described by Cox 1975\ and Kalbfleisch & Prentice \(1980, 2002  Time-dependent covariates may be classified into two broad categories: external and intern al \(Kalbfleisch and Prentice 2002, Lawless 2003, Klein and Moeschberger. 2003\. The failure mechanism does not directly involve external covariates. External covariates may be distinguished into \(1 fixed measured in advance and fixed during the observation\, \(2 defined determined in advance but not fixed, for example, contro lled stress levels\nd \(3 ancillary output of a stochastic process external to an observed object\n internal covariate is attached to the individual, and its existence depends on the survival of the individual. It is the output of a stochastic process that is induced by the individual under study.  The process is only observable if the individual survives and is uncensored   The Accelerated Failure Time \(AFT  The PHM represents the mul tiplicative influences of covariates z n the hazard.  However, without specific   0 t this model does not tell how z affects failure time itself. The previous log-linear models for T answer this question   Suppose that Y ln T  is related to the covariance z in a linear model  Y z W  47  where is a constant or intercept parameter z is a vector of covariates is a vector of parameters to be estimated is a scale parameter which is the reciprocal of the shape parameter, and W is a random variable with a specified distribution.  Many distributions, including the Weibull exponential, log-norma l, and log-logistic  can be used to describe W   Exponentiation of Y gives    exp T z T  48 where 0  exp  W T has a hazard function 0  t  that is independent of It follows that the hazard function for T  can be written in terms of this baseline hazard  0 as    exp  exp     0 z z t z t 49  The survival function is  S  t  z  exp 0  u  du 0 t exp z   50 and the pdf  is the product of    z t and S  t  z  


 11 This model specifies that covariates act multiplicatively on time t r than on the hazar d function.  That is, we assume a baseline hazard function to exist and that the effects of the covariates are to alter the rate at which an individual proceeds along the time axis.  In other words, the covariates z accelerates or decelerates the time to failure Kalbfleisch and Prentice 2002, Lawless 2003  It should be pointed out that the distribution-based regression models for exponential and Weibull distributions in the previous section are th e special cases of both PHM and AFT.  This correspondence is not necessarily true for models based on other distribu tions. Indeed, two-parameter Weibull distribution has the uniq ue property that it is closed under both multiplication of fa ilure time and multiplication of the hazard function by an arbitrary nonzero constant Lawless 2003, Kalbfleisch & Prentice 2002, Klein Moeschberger 2003  2.6. Counting Process and Survival Analysis   In the previous sections, we introduced censoring and survival analysis models th at can handle the censored information; however, we did not discuss how the censored information is processed.  Accommodating and maximally utilizing the partial information from the censored observations is the most challenging and also the most rewarding task in survival anal ysis.  This also establishes survival analysis as a unique fiel d in mathematical statistics Early statistical inferences for censored data in survival analysis were dependent on asymptotic likelihood theory Severini 2000\ Cox \(1972, 1975\ proposed partial likelihood as an extension to classical maximum likelihood estimation in the context of hi s proportional hazards model as a major contribution. Asymptotic likelihood has been and still is the dominant theory for developing survival analysis inference and hypothesis testing methods \(Klein and Moeschberger 2003, Severini 2000\. There are many monographs and textbooks of survival analysis containing sufficient details for applying survival analysis \(Cox and Oakes 1984, Kalbfleisch and Prentice 1980, 2002, Lawless 1982, 2003, Klein and Moeschberger, 2003\. A problem with traditional asymptotic lik elihood theory is that the resulting procedures can become very complicated when handling more complex censoring mechanisms \(Klein Moeschberger 2003\. A more elegant but requiring rigorous measure-theoretic probability theo ry is the approach with counting stochastic processes and the Martingale central limit theorems.  Indeed, this approach was used by Aalen 1975\ to set the rigorous mathematical foundation for survival analysis, and later further developed and summarized by Fleming and Harrington \(1991\, Andersen et al. \(1993\several research papers.  In reliability theory Aven and Jensen \(1999\ dem onstrated such an approach by developing a general failure model, which we briefly introduced in Section 1.2. However, the counting process and Martingale approach require measure theoretic treatments of probability and st ochastic processes, which is often not used in engineering or applied statistics.  A detailed introduction of the topic is obviously beyond the scope of this paper, and we only present a brief sketch of the most important concepts involved.  Readers are referred to the excellent monographs by Andersen et al. \(1993 Fleming and Harrington \(1991\ Aven and Jensen \(1999\ for comprehensive details, and Kal bfleisch and Prentice \(2002 Klein and Moeschberger \(2003\, Lawless \(2003\ for more application–oriented treatments The following discussion on this topic is drawn from Klein and Moeschberger \(2003  A counting stochastic process N  t  t 0 possesses the properties that N  0 ro and N  t   with probability one. The sample paths of N  t ht continuous and piecewise constant with jumps of size 1  step function In a right-censored sample, \(we assume only right censoring in this section N i  t  I  T i t  i   which keep the value 0 until individual i fails and then jump to 1  are counting processes. The accumulation of N i  t ocess     1 t N t N n i i is again a counting process, which counts the number of failures in the sample at or before time t   The counting process keeps track of the information on the occurrences of events,   for instance, the history information such as which individual was censored prior to time t and which individual died at or prior to time t as well as the covariates information This accumulated history information of the counting process at time t is termed filtration at time t denoted by F t For a given problem F t  rests on the observer of the counting process.  Thus, two observers with different recordings at different times will get different filtrations.  This is what Aven and Jensen 1999\ referred to as different information levels or the amount of actual available information about the state of a system may vary  If the failure times X i and censoring times C i  are independent,  then the probability of an event occurs at time t given the history just prior to t  F t\n be expressed as  t T if dt t h t C t X dt t C dt t X t P F dt t T t P i i i i i i r t i i r          1     t T if F dt t T t P i t i i r 0   1    51  Let dN  t be the change in the process N  t over a short time interval    t t t Ignoring the neglig ible chance of ties 1   t dN if a failure occurred and 0   t dN otherwise  Let Y  t denote the number of individuals with an observation time T i t Then the conditional expectation of dN  t   dt t h t Y F dt t C dt t X t with ns observatio of number E F t dN E t i i i t              52 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es — the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


States\nWAb-3.4: NEW RESULTS IN THE ANALYSIS OF DECISION-FEEDBACK 2118\nEQUALIZERS\nAhmed Mehana, Samsung Electronics, Co Ltd., United States; Aria Nosratinia, University of Texas at \nDallas, United States\nWAb-5: TARGET TRACKING II\nWAb-5.1: POSTERIOR DISTRIBUTION PREPROCESSING FOR PASSIVE 2125\nDTV RADAR TRACKING: SIMULATED AND REAL DATA\nEvan Hanusa, Laura Vertatschitsch, David Krout, University of Washington, United States\nWAb-5.2: DEPTH-BASED PASSIVE TRACKING OF SUBMERGED SOURCES  ............................................2130\nIN THE DEEP OCEAN USING A VERTICAL LINE ARRAY\nLisa Zurk, John K. Boyle, Jordan Shibley, Portland State University, United States\nWAb-5.3: GENERALIZED LINEAR MINIMUM MEAN-SQUARE ERROR 2133\nESTIMATION WITH APPLICATION TO SPACE-OBJECT TRACKING\nYu Liu, X. Rong Li, Huimin Chen, University of New Orleans, United States\nWAb-5.4: FEATURE-AIDED INITIATION AND TRACKING VIA TREE SEARCH ..........................................2138\nHossein Roufarshbaf Jill Nelson, George Mason University, United States\nxxxiii\nWAb-6: DIRECTION OF ARRIVAL ESTIMATION\nWAb-6.1: A SELF-CALIBRATION TECHNIQUE FOR DIRECTION 2145\nESTIMATION WITH DIVERSELY POLARIZED ARRAYS\nBenjamin Friedlander, University of California, Santa Cruz, United States\nWAb-6.2: CRAMER-RAO PERFORMANCE BOUNDS FOR SIMULTANEOUS  ..............................................2150\nTARGET AND MULTIPATH POSITIONING\nLi Li, Jeff Krolik, Duke University, United States\nWAb-6.3: COPY CORRELATION DIRECTION-OF-ARRIVAL ESTIMATION  .................................................2155\nPERFORMANCE WITH A STOCHASTIC WEIGHT VECTOR\nChrist Richmond, Keith Forsythe, MIT Lincoln Laboratory, United States; Christopher Flynn, Stevens nInstitute of Technology, United States\nWAb-6.4: LOCATING CLOSELY SPACED COHERENT EMITTERS USING 2160\nTDOA TECHNIQUES\nJack Reale, Air Force Research Laboratory / Binghamton University, United States; Lauren Huie, Air \nForce Research Laboratory, United States Mark Fowler, State University of New York at Binghamton, \nUnited States\nWAb-7: ENERGY- AND RELIABILITY-AWARE DESIGN\nWAb-7.1: LOW-ENERGY ARCHITECTURES FOR SUPPORT VECTOR 2167\nMACHINE COMPUTATION\nManohar Ayinala, Keshab K Parhi, University of Minnesota, United States\nWAb-7.2: TRUNCATED MULTIPLIERS THROUGH POWER-GATING FOR 2172\nDEGRADING PRECISION ARITHMETIC\nPietro Albicocco, Gian Carlo Cardarilli, University of Rome Tor Vergata, Italy; Alberto Nannarelli, \nTechnical University of Denmark Denmark; Massimo Petricca, Politecnico di Torino, Italy; Marco Re, \nUniversity of Rome Tor Vergata Italy\nWAb-7.3: A LOGARITHMIC APPROACH TO ENERGY-EFFICIENT GPU 2177\nARITHMETIC FOR MOBILE DEVICES\nMiguel Lastras Behrooz Parhami, University of California, Santa Barbara, United States\nWAb-7.4: ON SEPARABLE ERROR DETECTION FOR ADDITION ..................................................................2181\nMichael Sullivan, Earl Swartzlander, University of Texas at Austin, United States\nWPb-1: PAPERS PRESENTED IN 2012\nWPb-1.1 DYNAMICALLY RECONFIGURABLE AVC DEBLOCKING FILTER  .............................................2189\nWITH POWER AND PERFORMANCE CONSTRAINTS\nYuebing Jiang, Marios Pattichis, University of New Mexico\nxxxiv\n 


on science teams for numerous planetary missions including Magellan, Mars Observer, Mars Global Surveyor and Rosetta. He was the US Project Scientist for the international Mars NetLander mission, for which he was also principal investigator of the Short-Period Seismometer experiment, and is currently the Project Scientist for the Mars Exploration Rovers. He led the Geophysics and Planetary Geology group at JPL from 1993-2005, and is the JPL Discipline Program Manager for Planetary Geosciences. He has held several visiting appointments at the Institut de Physique du Globe de Paris. He has a BS in physics and a PhD in geophysics from the University of Southern California  David Hansen is a member of the technical staff in the Communications Systems and Operations Group at the Jet Propulsion Laboratory. Current work includes the development of the telecom subsystem for the Juno project. David received a B.S. in Electrical Engineering from Cornell University and an M.S. in Electrical Engineering from Stanford University  Robert Miyake is a member of the technical staff in the Mission and Technology Development Group at the Jet Propulsion Laboratory. Current work includes the development of thermal control subsystems for interplanetary flagship missions to Jupiter and Saturn missions to Mars and the Earth Moon, and is the lead Thermal Chair for the Advanced Project Design Team Robert graduated with a B. S. from San Jose State University, with extensive graduate studies at UCLA University of Washington, and University of Santa Clara  Steve Kondos is a consultant to the Structures and Mechanisms group at the Jet Propulsion Laboratory. He currently is generating the mechanical concepts for small Lunar Landers and Lunar Science Instrument packages in support of various Lunar mission initiatives. He also provides conceptual design, mass and cost estimating support for various Team X studies as the lead for the Mechanical Subsystem Chair. Steve is also involved with various other studies and proposals and provides mentoring to several young mechanical and system engineers. He graduated with a B.S. in Mechanical Engineering from the University of California, Davis and has 28 years of experience in the aerospace field ranging from detail part design to system of systems architecture development. He has worked both in industry and in government in defense, intelligence commercial and civil activities that range from ocean and land based systems to airborne and space systems. Steve has received various NASA, Air Force, Department of Defense and other agency awards for his work on such projects as the NASA Solar Array Flight Experiment, Talon Gold, MILSTAR, Iridium, SBIRS, Mars Exploration Rovers ATFLIR, Glory Aerosol Polarimeter System and several Restricted Programs  Paul Timmerman is a senior member of technical staff in the Power Systems Group at the Jet Propulsion Laboratory Twenty-five years of experience in spacecraft design including 22 at JPL, over 250 studies in Team-X, and numerous proposals. Current assignments include a wide variety of planetary mission concepts, covering all targets within the solar system and all mission classes. Paul graduated from Loras College with a B.S. in Chemistry in 1983  Vincent Randolph is a senior engineer in the Advanced Computer Systems and 


the Advanced Computer Systems and Technologies Group at the Jet Propulsion Laboratory. Current work includes generating Command and Data Handling Subsystem conceptual designs for various proposals and Team X.  He also supports Articulation Control and Electronics design activities for the Advanced Mirror Development project. Vincent graduated from the University of California at Berkeley with a B.S. in Electrical Engineering 18  pre></body></html 


i models into time and covariate dependent dynamic counterparts  ii models and reliability analysis in a more realistic manner  iii level  whether or not functional components \(loyal generals diagnose correctly and take proper actions such as fault mask of failed components \(traitors asymmetric  iv survivability analysis. Evolutionary game modeling can derive sustainable or survivable strategies \(mapped from the ESS in EGT such as node failures such as security compromise level modeling in the so-called three-layer survivability analysis developed in Ma \(2008a this article  v offer an integrated architecture that unite reliability survivability, and fault tolerance, and the modeling approaches with survival analysis and evolutionary game theory implement this architecture. Finally, the dynamic hybrid fault models, when utilized to describe the survival of players in EGT, enhance the EGT's flexibility and power in modeling the survival and behaviors of the game players which should also be applicable to other problem domains where EGT is applicable  5. OPERATIONAL LEVEL MODELING AND DECISION-MAKING  5.1. Highlights of the Tactical and Strategic Levels  Let's first summarize what are obtainable at both tactical and strategic levels. The results at both tactical and strategic levels are precisely obtainable either via analytic or simulation optimization. With the term precisely, we mean that there is no need to assign subjective probabilities to UUUR events. This is possible because we try to assess the consequences of UUUR events \(tactical level ESS strategies \(strategic level time prediction of survivability. The following is a list of specific points. I use an assumed Wireless Sensor Network WSN  i of UUUR events: \(a actions which can be treated as censored events; \(b Cont' of Box 4.2 It can be shown that the replicator differential equations are equivalent to the classical population dynamics models such as Logistic differential equation and LotkaVolterra equation \(e.g., Kot 2001 Logistic equation, or the limited per capital growth rate is similar to the change rate of the fitness  xfxfi which can be represented with the hazard function or survivor functions introduced in the previous section on survival analysis.  This essentially connects the previous survival analysis modeling for lifetime and reliability with the EGT modeling. However, EGT provides additional modeling power beyond population dynamics or survival analysis approaches introduced in the previous section. The introduction of evolutionary theory makes the games played by a population evolvable. In other words, each player \(individual 


other words, each player \(individual agent and players interact with each other to evolve an optimized system Box 4.3. Additional Comments on DHF Models  The above introduced EGT models are very general given they are the system of ordinary differential equations. Furthermore, the choice of fitness function f\(x complexity to the differential equation system.  The system can easily be turned into system of nonlinear differential equations. The analytical solution to the models may be unobtainable when nonlinear differential equations are involved and simulation and/or numerical computation are often required  In the EGT modeling, Byzantine generals are the game players, and hybrid fault models are conveniently expressed as the strategies of players; the players may have different failure or communication behaviors Furthermore, players can be further divided into groups or subpopulations to formulate more complex network organizations. In the EGT modeling, reliability can be represented as the payoff \(fitness, the native term in EGT of the game. Because reliability function can be replaced by survivor function, survival analysis is seamlessly integrated into the EGT modeling. That is, let Byzantine generals play evolutionary games and their fitness reliability function  The evolutionary stable strategy \(ESS counterpart of Nash equilibrium in traditional games ESS corresponds to sustainable strategies, which are resistant to both internal mutations \(such as turning into treason generals or nodes such as security compromises represent survivable strategies and survivability in survivability analysis. Therefore, dynamic hybrid fault models, after the extension with EGT modeling, can be used to study both reliability and survivability 13 risks such as competing risks which can be described with CRA; \(c captured with the shard frailty.  We believe that these UUUR events are sufficiently general to capture the major factors/events in reliability, security and survivability whose occurrence probabilities are hard or impossible to obtain  Instead of trying to obtain the probabilities for these events which are infeasible in most occasions, we focus on analyzing the consequences of the events.  With survival analysis, it is possible to analyze the effects of these types of events on survivor functions. In addition, spatial frailty modeling can be utilized to capture the heterogeneity of risks in space, or the spatial distribution of risks \(Ma 2008a d UUUR events introduced previously. These approaches and models that deal with the effects of UUUR events form the core of tactical level modeling  To take advantage of the tactical level modeling approaches it is obviously necessary to stick to the survivor functions or hazard functions models. In other words, survival analysis can deal with UUUR events and offer every features reliability function provides, but reliability function cannot deal with UUUR events although survivor function and reliability function have the exactly same mathematical definition. This is the junction that survival analysis plays critical role in survivability analysis at tactical level. However, we 


recognize that it is infeasible to get a simple metric for survivability similar to reliability with tactical level modeling alone. Actually, up to this point, we are still vague for the measurement of survivability or a metric for survivability. We have not answered the question: what is our metric for survivability? We think that a precise or rigorous definition of survivability at tactical level is not feasible, due to the same reason we cited previously  the inability to determine the probabilities of UUUR events However, we consider it is very helpful to define a work definition for survivability at the tactical level  We therefore define the survivability at tactical level as a metric, Su\(t t function or reliability function with UUUR events considered. In the framework of three-layer survivability analysis, this metric is what we mean with the term survivability. The "metric" per se is not the focus of the three-layer survivability analysis. It is not very informative without the supports from the next two levels  strategic and operational models.  However, it is obvious that this metric sets a foundation to incorporate UUUR effects in the modeling at the next two levels  Due to the inadequacy of tactical level modeling, we proposed the next level approach  strategic level modeling for survivability. As expected, the tactical level is one foundation of strategic level modeling ii objectives: \(a affect survivability which survival analysis alone is not adequate to deal with; \(b survivability at tactical level is necessary but not sufficient for modeling survivability, we need to define what is meant with the term survivability at strategic level  With regard to \(a behaviors or modes which have very different consequences. These failure behaviors can be captured with hybrid fault models. However, the existing hybrid fault models in fault tolerance field are not adequate for applying to survivability analysis. There are two issues involved: one is the lack of real time notion in the constraints for hybrid fault models \(e.g., N&gt;3m+1 for Byzantine Generals problem synthesize the models after the real-time notions are introduced. The solution we proposed for the first issue is the dynamic hybrid fault models, which integrate survivor functions with traditional hybrid fault models. The solution we proposed for the second issue is the introduction of EGT modeling  With regard to \(b modeling our problem at strategic level, EGT modeling is essentially a powerful optimization algorithm.  One of the most important results from EGT modeling is the so-called evolutionary stable strategies \(ESS We map the ESS in EGT to survivable strategies in survivability analysis.   Therefore, at the strategic level, our work definition for survivability refers to the survivable strategies or sustainable strategies in the native term of EGT, which can be quantified with ESS  In addition to integrating dynamic hybrid fault models another advantage for introducing EGT modeling at strategic level is the flexibility for incorporating other node behaviors \(such as cooperative vs. non-cooperative those behaviors specified in standard hybrid fault models, as well as anthropocentric factors such as costs constraints  Without UUUR events, both tactical and strategic level 


Without UUUR events, both tactical and strategic level models default to regular reliability models. This implies that, in the absence of UUUR events, reliable strategies are sustainable or survivable.  This also implies that three-layer survivability analysis defaults to reliability analysis however, the three-layer approach does offer some significant advantages over traditional reliability analysis, as discussed in previous sections. Nevertheless, when UUUR events exist, reliable strategies and survivable strategies are different. This necessitates the next operational level modeling  5.2. Operational Level Modeling and Decision-Making  When UUUR events are involved, we cannot make real time predictions of survivability at tactical and strategic levels This implies that the implementations of survivable 14 strategies need additional measures that we develop in this section.  Box 5.1 explains the ideas involved with possibly the simplest example  Figure 4 is a diagram showing a simplified relationship between action threshold survivability \(TS survivability \(ES view since both TS and ES are multidimensional and dynamic in practice. Therefore, the sole purpose of the diagram is to illustrate the major concepts discussed above The blue curve is the survivability when survivable strategies specified by ESS are implemented at some point before time s.  The system is then guaranteed to hold survivability above ES. In contrary, if no ESS implemented before time s, then the system quickly falls below to the survivable level at around 40 time units  T i m e 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 Su rv iv ab ili ty M et ric S u t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 E S S  i s  I m p lm e n t e d N o  E S S  is  I m p lm e n t e d ts E S T S  Figure 4. A Diagram Showing the Relationship Between TS and ES, as well as timing of s and t, with s &lt; t  6. SUMMARY  The previous sections discussed the major building blocks 


The previous sections discussed the major building blocks for the new life-system inspired PHM architecture. This section first identifies a few minor aspects that have not been discussed explicitly but are necessary for the implementation of the architecture, and then we summarize the major building blocks in a diagram  6.1. Missing Components and Links  Optimization Objectives  Lifetime, reliability, fault tolerance, and survivability, especially the latter two, are application dependent. Generally, the optimization of reliability and survivability are consistent; in that maximization of reliability also implies maximization of survivability. However, when application detail is considered, optimization of lifetime is not necessarily consistent with the optimization of reliability. Consider the case of the monitoring sensor network as an example. The network reliability is also dependent on connectivity coverage, etc, besides network lifetime. What may be further complicated is the time factor. All of the network metrics are time-dependent. A paradoxical situation between lifetime and reliability could be that nodes never 'sleep                                                   


          Box 5.1 Operational Level Modeling  Assuming that the ESS solution for a monitoring sensor network can be expressed with the following simple algebraic conditions: survivability metric at tactical level SU = 0.7, Router-Nodes in the WSN &gt; 10%, Selfish Nodes &lt; 40%. Even with this extremely simplified scenario, the ESS strategies cannot be implemented because we do not know when the actions should be taken to warrant a sustainable system.  These conditions lack a correlation with real time  The inability to implement ESS is rooted in our inability to assign definite probabilities to UUUR events, which implies that we cannot predict when something sufficiently bad will jeopardize the system survivability What we need at the operational level is a scheme to ensure ESS strategy is in place in advance  The fundamental idea we use to implement the ESS strategy is to hedge against the UUUR events. The similar idea has been used in financial engineering and also in integrated pest management in entomology. This can be implemented with the following scheme  Let us define a pair of survivability metrics: one is the expected survivability \(ES threshold survivability or simply threshold survivability \(TS ES is equivalent to the survivability metric at tactical level. ES corresponds to ESS at strategic level, but they are not equivalent since ESS is strategy and ES is survivability. TS is the survivability metric value \(at tactical level and TS can be obtained from strategic level models. For example, TS = SU\(s t condition for the implementation of ESS. In other words, the implementation of strategies that ensures TS at time s will guarantee the future ES level at time t.  To make the implementation more reliable and convenient multiple dynamic TSs can be computed at time s1, s2 sk, with si &lt; t for all i.  These TS at times s1, s2, ..., sk should be monitored by some evaluation systems  Unlike tactical and strategic levels, the operational level modeling is approximate. The term "approximate means that we cannot predict the real time survivability or we do not know the exact time an action should be taken. Instead, the action is triggered when the monitored survivability metric SU\(r survivability \(TS scheme of TS and ES, we ensure the ES by taking preventative actions \(prescribed by ESS and triggered by the TS consequences of UUUR events  Figure 4 is a diagram showing the above concepts and the decision-making process involved 15 This wakefulness \(never 'sleep short period but at the expense of network lifetime. Of course, when the network is running out of lifetime, network reliability ultimately crashes. This example reminds us that 


reliability ultimately crashes. This example reminds us that multi-objective optimization should be the norm rather than exception  Constraints and Extensions  Many application specific factors and constraints are ignored in this article. For example, we mentioned about spatial heterogeneity of environment, but never present a mathematical description The spatial heterogeneity can be modeled with the so-called spatial frailty in multivariate survival analysis \(Ma 2008a  Evolutionary Algorithm  Evolutionary game modeling when implemented in simulation, can be conveniently implemented with an algorithm similar to Genetic Algorithms \(GA ESS in the evolutionary game model with simulation is very similar to GA. Dynamic populations, in which population size varies from generation to generation \(Ma &amp; Krings 2008f of node failures. Another issue to be addressed is the synchronous vs. asynchronous updating when topology is considered in the simulation. This update scheme can have profound influences on the results of the simulation. Results from cellular automata computing should be very useful for getting insights on the update issue  6.2. Summary and Perspective  To recapture the major points of the article, let us revisit Figure 3, which summarizes the principal modules of the proposed life-system inspired PHM architecture. The main inspiration from life systems is the notion of individuals and their assemblage, the population. Population is an emergent entity at the next level and it has emergent properties which we are often more concerned with. Survival analysis, which has become a de facto standard in biomedicine, is particularly suitable for modeling population, although it is equally appropriate at individual level. Therefore, survival analysis \(including competing risks analysis and multivariate survival analysis comprehensively in the context of PHM in a series of four papers presented at IEEE AeroSpace 2008 \(Ma &amp; Krings 2008a, b, c, &amp; d proposed architecture. Survival analysis constitutes the major mathematical tools for analyzing lifetime and reliability, and also forms the tactical level of the three-layer survivability analysis  Besides lifetime and reliability, two other major modules in Figure 3 are fault tolerance and survivability. To integrate fault tolerance into the PHM system, Dynamic Hybrid Fault DHF 2008e, Ma 2008a make real-time prediction of reliability more realistic and make real-time prediction of fault tolerance level possible DHF models also unite lifetime, reliability and fault tolerance under a unified modeling framework that consists of survival analysis and evolutionary game theory modeling  DHG models also form the partial foundation, or strategic level, for the three-layer survivability analysis. At the strategic level, the Evolutionary Stable Strategies \(ESS which is mapped to survivable or sustainable strategies, can be obtained from the evolutionary game theory based DHF models. When there is not any UUUR event involved reliability and survivability are consistent, and reliable strategies are survivable. In this case, the strategic level modeling up to this point is sufficient for the whole PHM system modeling, and there is no need for the next level  operational level modeling  When there are UUUR events in a PHM system, the 


When there are UUUR events in a PHM system, the inability to determine the occurrence probabilities of UUUR events makes the operational level modeling necessary Then the principle of hedging must be utilized to deal with the "hanging" uncertainty from UUUR events. In this case reliability strategies are not necessarily survivable strategies At the operational level modeling, a duo of survivability metrics, expected survivability \(ES survivability \(TS the survivable strategies \(ESS level are promptly implemented based on the decisionmaking rules specified with the duo of survivability metrics then the PHM system should be able to endure the consequences of potentially catastrophic UUUR events. Of course, to endure such catastrophic events, the cost may be prohibitively high, but the PHM system will, at least, warn decision-makers for the potentially huge costs.  It might be cheap to just let it fail  Figure 3 also shows several other modules, such as security safety, application systems \(such as Automatic Logistics CBM+, RCM, Life cycle cost management, Real-time warning and alert systems architectures, but we do not discuss in this paper. Generally the new architecture should be fully compatible with existing ones in incorporating these additional modules. One point we stressed is that PHM system can be an ideal place to enforce security policies. Enforcing security policies can be mandatory for PHM systems that demand high security and safety such as weapon systems or nuclear plant facilities.  This is because maintenance, even without human-initiated security breaches, can break the security policies if the maintenance is not planned and performed properly  In perspective, although I did not discuss software issues in this paper, the introduced approaches and models should provide sufficient tools for modeling software reliability and survivability with some additional extension. Given the critical importance of software to modern PHM systems, we present the following discussion on the potential extension to software domain. Specifically, two points should be noted: \(1 architecture to software should be a metric which can 16 replace the time notion in software reliability; I suggest that the Kolmogorov complexity \(e.g., Li and Vitanyi 1997 be a promising candidate \(Ma 2008a change is because software does not wear and calendar time for software reliability usually does not make much sense 2 software reliability modeling.  Extending to general survivability analysis is not a problem either. In this article I implicitly assume that reliability and survivability are positively correlated, or reliability is the foundation of survivability. This positive correlation does not have to be the case. A simplified example that illustrates this point is the 'limit order' in online stock trading, in which limit order can be used in either direction: that stock price is rising or falling.  The solution to allow negative or uncorrelated relationships between reliability and survivability are very straightforward, and the solutions are already identified in previous discussions. Specifically, multiple G-functions and multi-stage G-functions by Vincent and Brown \(2005 very feasible solution, because lifetime, reliability and survivability may simply be represented with multiple Gfunctions. Another potential solution is the accommodation of the potential conflicts between reliability and survivability with multi-objective GA algorithms, which I previously suggested to be used as updating algorithms in the optimization of evolutionary games  


 The integration of dynamic hybrid fault models with evolutionary game modeling allows one to incorporate more realistic and detailed failure \(or survival individual players in an evolutionary game. This is because dynamic hybrid fault models are supported by survival analysis modeling, e.g., time and covariate dependent hazard or survivor functions for individual players. If necessary, more complex survival analysis modeling including competing risks analysis and multivariate survival analysis, can be introduced.  Therefore, any field to which evolutionary game theory is applicable may benefit from the increased flexibility in modeling individual players.  Two particularly interesting fields are system biology and ecological modeling.  In the former field, dynamic hybrid fault models may find important applications in the study of biological networks \(such as gene, molecular, and cell networks 2008g conjecture that explains the redundancy in the universal genetic code with Byzantine general algorithm. In addition they conducted a comparative analysis of bio-robustness with engineering fault tolerance, for example, the strong similarity between network survivability and ecological stability \(Ma &amp; Krings 2008g survivability analysis can be applied for the study of survivals or extinctions of biological species under global climate changes \(Ma 2008b  In this paper, I have to ignore much of the details related to the implementation issues to present the overall architecture and major approaches clearly and concisely. To deal with the potential devils in the implementation details, a well funded research and development team is necessary to take advantages of the ideas presented here. On the positive side I do see the great potential to build an enterprise PHM software product if there is sufficient resource to complete the implementation. Given the enormous complexity associated with the PHM practice in modern engineering fields, it is nearly impossible to realize or even demonstrate the benefits of the architecture without the software implementation. The critical importance of PHM to mission critical engineering fields such as aerospace engineering, in turn, dictates the great value of such kind software product  6.3. Beyond PHM  Finally, I would like to raise two questions that may be interested in by researchers and engineers beyond PHM community. The first question is: what can PHM offer to other engineering disciplines? The second question is: what kinds of engineering fields benefit most from PHM? Here, I use the term PHM with the definition proposed by IEEE which is quoted in the introduction section of the paper  As to the first question, I suggest software engineering and survivability analysis are two fields where PHM can play significant roles. With software engineering, I refer to applying PHM principles and approaches for dealing with software reliability, quality assurance, and even software process management, rather than building PHM software mentioned in the previous subsection. For survivability analysis, borrowing the procedures and practices of PHM should be particularly helpful for expanding its role beyond its originating domain \(network systems that control critical national infrastructures is a strong advocate for the expansion of survivability analysis to PHM. Therefore, the interaction between PHM and survivability analysis should be bidirectional. Indeed, I see the close relationships between PHM, software engineering, and survivability as well-justified because they all share some critical issues including reliability survivability, security, and dependability  


 The answer to the second question is much more elusive and I cannot present a full answer without comparative analysis of several engineering fields where PHM has been actively practiced. Of course, it is obvious that fields which demand mission critical reliability and dependability also demand better PHM solutions. One additional observation I would like to make is that PHM seems to play more crucial roles for engineering practices that depend on the systematic records of 'historical' data, such as reliability data in airplane engine manufacturing, rather than on the information from ad hoc events.  This may explain the critical importance of PHM in aerospace engineering particularly in commercial airplane design and manufacturing.  For example, comparing the tasks to design and build a space shuttle vs. to design and manufacture commercial jumbo jets, PHM should be more critical in the latter task  17    Figure 2. States of a monitoring sensor node and its failure modes \(after Ma &amp; Krings 2008e     Figure 3. Core Modules and their Relationships of the Life System Inspired PHM Architecture    REFERENCES  Adamides, E. D., Y. A. Stamboulis, A. G. Varelis. 2004 Model-Based Assessment of Military Aircraft Engine Maintenance Systems Model-Based Assessment of Military Aircraft Engine Maintenance Systems. Journal of the Operational Research Society, Vol. 55, No. 9:957-967  Anderson, R. 2001. Security Engineering. Wiley  Anderson, R. 2008. Security Engineering. 2nd ed. Wiley  Bird, J. W., Hess, A. 2007.   Propulsion System Prognostics R&amp;D Through the Technical Cooperation Program Aerospace Conference, 2007 IEEE, 3-10 March 2007, 8pp  Bock, J. R., Brotherton, T., W., Gass, D. 2005. Ontogenetic reasoning system for autonomic logistics. Aerospace Conference, 2005 IEEE 5-12 March 2005.Digital Object Identifier 10.1109/AERO.2005.1559677  Brotherton, T., P. Grabill, D. Wroblewski, R. Friend, B Sotomayer, and J. Berry. 2002. A Testbed for Data Fusion for Engine Diagnostics and Prognostics. Proceedings of the 2002 IEEE Aerospace Conference  Brotherton, T.; Grabill, P.; Friend, R.; Sotomayer, B.; Berry J. 2003. A testbed for data fusion for helicopter diagnostics and prognostics. Aerospace Conference, 2003. Proceedings 2003 IEEE  Brown, E. R., N. N. McCollom, E-E. Moore, A. Hess. 2007 Prognostics and Health Management A Data-Driven Approach to Supporting the F-35 Lightning II. 2007 IEEE AeroSpace Conference  Byington, C.S.; Watson, M.J.; Bharadwaj, S.P. 2008 Automated Health Management for Gas Turbine Engine Accessory System Components. Aerospace Conference 2008 IEEE, DOI:10.1109/AERO.2008.4526610 


2008 IEEE, DOI:10.1109/AERO.2008.4526610 Environment Covariates &amp; Spatial Frailty Applications: AL; Life Cycle Mgmt; Real-Time Alerts CBM+, RCM, TLCSM; Secret Sharing and Shared Control 18 Chen, Y. Q., S. Cheng. 2005. Semi-parametric regression analysis of mean residual life with censored survival data Biometrika \(2005  29  Commenges, D. 1999. Multi-state models in Epidemiology Lifetime Data Analysis. 5:315-327  Cook, J. 2004. Contrasting Approaches to the Validation of Helicopter HUMS  A Military User  s Perspective Aerospace Conference, 2004 IEEE  Cook, J. 2007. Reducing Military Helicopter Maintenance Through Prognostics. Aerospace Conference, 2007 IEEE Digital Object Identifier 10.1109/AERO.2007.352830  Cox, D. R. 1972. Regression models and life tables.  J. R Stat. Soc. Ser. B. 34:184-220  Crowder, M. J.  2001. Classical Competing Risks. Chapman amp; Hall. 200pp  David, H. A. &amp; M. L. Moeschberger. 1978. The theory of competing risks. Macmillan Publishing, 103pp  Ellison, E., L. Linger, and M. Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013  Hanski, I. 1999. Metapopulation Ecology. Oxford University Press  Hallam, T. G. and S. A. Levin. 1986. Mathematical Ecology. Biomathematics. Volume 17. Springer. 457pp  Hess, A., Fila, L. 2002.  The Joint Strike Fighter \(JSF concept: Potential impact on aging aircraft problems Aerospace Conference Proceedings, 2002. IEEE. Digital Object Identifier: 10.1109/AERO.2002.1036144  Hess, A., Calvello, G., T. Dabney. 2004. PHM a Key Enabler for the JSF Autonomic Logistics Support Concept. Aerospace Conference Proceedings, 2004. IEEE  Hofbauer, J. and K. Sigmund. 1998. Evolutionary Games and Population Dynamics. Cambridge University Press 323pp  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Huzurbazar, A. V. 2006. Flow-graph model for multi-state time-to-event data. Wiley InterScience  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis. Springer. 481pp  Kacprzynski, G. J., Roemer, M. J., Hess, A. J. 2002. Health management system design: Development, simulation and cost/benefit optimization. IEEE Aerospace Conference Proceedings, 2002. DOI:10.1109/AERO.2002.1036148  Kalbfleisch, J. D., and R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data. Wiley-InterScience, 2nd ed  Kalgren, P. W., Byington, C. S.   Roemer, M. J.  2006 Defining PHM, A Lexical Evolution of Maintenance and Logistics. Systems Readiness Technology Conference 


Logistics. Systems Readiness Technology Conference IEEE. DOI: 10.1109/AUTEST.2006.283685  Keller, K.; Baldwin, A.; Ofsthun, S.; Swearingen, K.; Vian J.; Wilmering, T.; Williams, Z. 2007. Health Management Engineering Environment and Open Integration Platform Aerospace Conference, 2007 IEEE, Digital Object Identifier 10.1109/AERO.2007.352919  Keller, K.; Sheahan, J.; Roach, J.; Casey, L.; Davis, G Flynn, F.; Perkinson, J.; Prestero, M. 2008. Power Conversion Prognostic Controller Implementation for Aeronautical Motor Drives. Aerospace Conference, 2008 IEEE. DOI:10.1109/AERO.2008.4526630  Klein, J. P. and M. L. Moeschberger. 2003. Survival analysis techniques for censored and truncated data Springer  Kingsland, S. E. 1995. Modeling Nature: Episodes in the History of Population Ecology. 2nd ed., University of Chicago Press, 315pp  Kot, M. 2001. Elements of Mathematical Ecology Cambridge University Press. 453pp  Krings, A. W. and Z. S. Ma. 2006. Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks Military Communications Conference, 23-25 October, 7 pages, 2006  Lamport, L., R. Shostak and M. Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4\(3  Lawless, J. F. 2003. Statistical models and methods for lifetime data. John Wiley &amp; Sons. 2nd ed  Line, J. K., Iyer, A. 2007. Electronic Prognostics Through Advanced Modeling Techniques. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352906  Lisnianski, A., Levitin, G. 2003. Multi-State System Reliability: Assessment, Optimization and Applications World Scientific  Liu, Y., and K. S. Trivedi. 2006. Survivability Quantification: The Analytical Modeling Approach, Int. J of Performability Engineering, Vol. 2, No 1, pp. 29-44  19 Luchinsky, D.G.; Osipov, V.V.; Smelyanskiy, V.N Timucin, D.A.; Uckun, S. 2008. Model Based IVHM System for the Solid Rocket Booster. Aerospace Conference, 2008 IEEE.DOI:10.1109/AERO.2008.4526644  Lynch, N. 1997. Distributed Algorithms. Morgan Kaufmann Press  Ma, Z. S. 1997. Demography and survival analysis of Russian wheat aphid. Ph.D. dissertation, Univ. of Idaho 306pp  Ma, Z. S. 2008a. New Approaches to Reliability and Survivability with Survival Analysis, Dynamic Hybrid Fault Models, and Evolutionary  Game Theory. Ph.D. dissertation Univ. of Idaho. 177pp  Ma, Z. S. 2008b. Survivability Analysis of Biological Species under Global Climate Changes: A New Distributed and Agent-based Simulation Architecture with Survival Analysis and Evolutionary Game Theory. The Sixth 


International Conference on Ecological Informatics. Dec 25, 2008. Cancun, Mexico  Ma, Z. S. and E. J. Bechinski. 2008. A Survival-Analysis based  Simulation Model for Russian Wheat Aphid Population Dynamics. Ecological Modeling, 216\(2 332  Ma, Z. S. and A. W. Krings. 2008a.  Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT, 20pp  Ma, Z. S. and A. W. Krings. 2008b. Competing Risks Analysis of Reliability, Survivability, and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008.  Big Sky, MT. 20pp  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(I Dependence Modeling", Proc. IEEE  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT. 21pp  Ma, Z. S. and A. W. Krings., R. E. Hiromoto. 2008d Multivariate Survival Analysis \(II State Models in Biomedicine and Engineering Reliability IEEE International Conference of Biomedical Engineering and Informatics, BMEI 2008.  6 Pages  Ma, Z. S. and A. W. Krings. 2008e. Dynamic Hybrid Fault Models and their Applications to Wireless Sensor Networks WSNs Modeling, Analysis and Simulation of Wireless and Mobile Systems. \(ACM MSWiM 2008 Vancouver, Canada  Ma, Z. S. &amp; A. W. Krings. 2008f. Dynamic Populations in Genetic Algorithms. SIGAPP, the 23rd Annual ACM Symposium on Applied Computing, Ceara, Brazil, March 16-20, 2008. 5 Pages  Ma, Z. S. &amp; A. W. Krings. 2008g. Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems, Proc. IEEE  AIAA AeroSpace Conference, March 1-8, Big Sky, MT, 2008. 20 Pages  Ma, Z. S.  and A. W. Krings. 2009. Insect Sensory Systems Inspired Computing and Communications.  Ad Hoc Networks 7\(4  MacConnell, J.H. 2008. Structural Health Management and Structural Design: An Unbridgeable Gap? 2008 IEEE Aerospace Conference, DOI:10.1109/AERO.2008.4526613  MacConnell, J.H. 2007. ISHM &amp; Design: A review of the benefits of the ideal ISHM system. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352834  Marshall A. W., I. Olkin. 1967. A Multivariate Exponential Distribution. Journal of the American Statistical Association, 62\(317 Mar., 1967  Martinussen, T. and T. H. Scheike. 2006. Dynamic Regression Models for Survival Data. Springer. 466pp  Mazzuchi, T. A., R. Soyer., and R. V. Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Millar, R.C., Mazzuchi, T.A. &amp; Sarkani, S., 2007. A Survey of Advanced Methods for Analysis and Modeling of 


of Advanced Methods for Analysis and Modeling of Propulsion System", GT2007-27218, ASME Turbo Expo 2007, May 14-17, Montreal, Canada  Millar, Richard C., "Non-parametric Analysis of a Complex Propulsion System Data Base", Ph.D. Dissertation, George Washington University, June 2007  Millar, R. C. 2007. A Systems Engineering Approach to PHM for Military Aircraft Propulsion Systems. Aerospace Conference, 2007 IEEE. DOI:10.1109/AERO.2007.352840  Millar, R. C. 2008.  The Role of Reliability Data Bases in Deploying CBM+, RCM and PHM with TLCSM Aerospace Conference, 2008 IEEE, 1-8 March 2008. Digital Object Identifier: 10.1109/AERO.2008.4526633  Nowak, M. 2006. Evolutionary Dynamics: Exploring the Equations of Life. Harvard University Press. 363pp  Oakes, D. &amp; Dasu, T. 1990. A note on residual life Biometrika 77, 409  10  Pintilie, M. 2006. Competing Risks: A Practical Perspective.  Wiley. 224pp  20 Smith, M. J., C. S. Byington. 2006. Layered Classification for Improved Diagnostic Isolation in Drivetrain Components. 2006 IEEE AeroSpace Conference  Therneau, T. and P. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. Springer  Vincent, T. L. and J. L. Brown. 2005. Evolutionary Game Theory, Natural Selection and Darwinian Dynamics Cambridge University Press. 382pp  Wang. J., T. Yu, W. Wang. 2008. Research on Prognostic Health Management \(PHM on Flight Data. 2008 Int. Conf. on Condition Monitoring and Diagnosis, Beijing, China, April 21-24, 2008. 5pp  Zhang, S., R. Kang, X. He, and M. G. Pecht. 2008. China  s Efforts in Prognostics and Health Management. IEEE Trans. on Components and Packaging Technologies 31\(2             BIOGRAPHY  Zhanshan \(Sam scientist and earned the terminal degrees in both fields in 1997 and 2008, respectively. He has published more than 60 peer-refereed journal and conference papers, among which approximately 40 are journal papers and more than a third are in computer science.  Prior to his recent return to academia, he worked as senior network/software engineers in semiconductor and software industry. His current research interests include: reliability, dependability and fault tolerance of distributed and software systems behavioral and cognitive ecology inspired pervasive and 


behavioral and cognitive ecology inspired pervasive and resilient computing; evolutionary &amp; rendezvous search games; evolutionary computation &amp; machine learning bioinformatics &amp; ecoinformatics                 pre></body></html 


