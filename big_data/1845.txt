Assessing Data Quality by a Cross-Layer Approach Mattia Monga Dip di Informatica e Comunicazione Universit 036 a degli Studi di Milano Via Comelico 39 I\22620135 Milano Italy Email mattia.monga@unimi.it Sabrina Sicari Dip di Informatica e Comunicazione Universit 036 a degli Studi dell'Insubria Via J.H.Dunant 3 I\22621100 Varese Italy Email sabrina.sicari@uninsubria.it Abstract Wireless sensor networks had a big diffusion in the last few decades and they are used in many application domains Services built on them require to handle a big amount of data and a fundamental requirement is their quality highly affected by the security of the whole system Some encryption techniques can be adopted but it is also necessary to verify the reliability of nodes that sense aggregate transmit data and share keys This paper proposes a methodology for assessing data quality Built on a cross layer approach our methodology exploits localization information Keywords Wireless sensor network security localization distributed aggregation 1 Introduction Wireless sensor networks 050WSNs\051 are becoming popular in many application domains indoor/outdoor surveillance systems traf\002c monitoring and control systems for urban and sub-urban areas systems supporting tele-medicine attendance to disable or elderly people environment monitoring localization and recognition of services and users monitoring and control of manufacturing processes in industry etc The amount of data that have to be transmitted and manipulated to provide such complex services is pretty 9781-4244-3941-6/09/$25.00 c 015 2009 IEEE high but sensors are tiny devices with limited computation and energy capabilities and the transmission of data is a very expensive operation from a power consumption perspective Thus one of the design goals of WSN systems should be the reduction of the need for transmission A step in this direction are publishsubscribe approaches and content-based routing  In se v eral situations ho we v er  the interest is just in some aggregated measure such as the average temperature of a region the average humidity etc In this cases a proper aggregation algorithm 050for examples see 5 6]\051 may reduce signi\002cantly the number of bytes exchanged across the WSN From a security point of view the wireless communications and the deployment in uncontrolled environments rise several issues in fact the con\002dentiality the integrity and the availability of data might be put at risk by malicious tampering of sensors and/or traf\002c Many of the solutions proposed in the literature are based on access control and strong authentication which are problematic to implement with limited resources and short battery life Moreover the approaches based on pre-shared encryption keys are prone to physical attacks in fact is pretty easy to clone a sensor device and its key Therefore in this paper we start from the assumption that the perfect protection of data is infeasible or too expensive to be effective and we propose to use an approach based on the assessment of data quality de\002ned as how likely is that the information to be elaborated by the application is reliable and trustworthy Therefore we suggest to combine two 227 in general unreliable but relatively easy to achieve 227 protection techniques while none of them is enough to guarantee the reliability of all the pieces of information consistency across them can 


be exploited to get a measure of the overall quality Applications can then leverage on this assessment to provide the level of service feasible in the current context The aim of our work is to de\002ne a methodology that de\002nes the fundamental steps for assessing data quality We use secure data aggregation and a secure localization protocol to gather information about the data generated by the sensors and we compute a crosslayer assessment of the overall quality of the data collected by the sink node The paper is organized as follows Section 2 provides a short state of art about data aggregation in hostile environments Section 3 shortly describes the reference scenario in order to clarify the application domain Section 4 introduces the main steps of the methodology and some implementation details Section 5 draws some conclusions and provides hints for future works 2 Related Work on Secure Aggregation Data con\002dentiality and integrity become vital when sensor nodes are deployed in a hostile environment Secure aggregation of data in a adversarial setting is the focus of several works They can be classi\002ed according to the approach used for encrypting data that can be done hop-by-hop or end-to-end In the former case the data are encrypted by sensing nodes and decrypted by aggregators The aggregator nodes then decrypt the data coming from the sensing nodes aggregate them and encrypt them again until eventually the sink node gets the 002nal encrypted aggregation result 050and decrypts it\051 In the end-to-end approach the intermediate aggregators manipulate only encrypted data and they have no keys to decrypt them Some hop-by-hop works 050for example 7 8  assume that data security is guaranteed by means of some key distribution schemes Such solutions are vulnerable to the tampering or cloning of the intermediate aggregator nodes End-to-end encrypted techniques  4]\051 partia lly o v ercome the weakness of hop-byhop techniques but they also need a key scheme Some approaches suggest to share a key among all sensing nodes and the sink however the aggregators have no keys to manage because they handle data without making any decryption Nevertheless sensing nodes keys are still critical and if they are shared among too many nodes a whole network region might be compromised by attacking a single sensing node An alternative is represented by the adoption of public key encryption b ut in this case the dra wback is represented by a high computational power consumption Most of the proposed solutions are based on the adoption of encryption techniques ad-hoc key distribution schemes 12 13 authentication access control solutions Our methodology proposes instead to combine some cheap protection techniques even if none of them is totally effective We leverage on cross-layer consistency to asses the overall quality of the collected information Thus by means of the veri\002cation of localization information it is possible to adopt an end-to-end secure data aggregation with a pre-shared key without wasting power with public key encryption techniques 3 Reference Scenario We consider a dense network composed of nodes n i  where n i 2 N 0  i 024 j N j and a base station b in which all the collected data sink We consider three subsets of N  017 S  composed by nodes s i  0  i 024 j S j which perform sensing functions 017 A  composed by nodes a i  0  i 024 j A j which aggregate data 017 V  composed by nodes v i  0  i 024 j V j which work as veri\002ers in the secure localization protocol N  S  A  V  however while V may overlap both S and A 050in principle every node whose position can be taken for granted might be used as a veri\002er\051 the effectiveness of secure aggregation requires 050see Section 4.1\051 that S and A are disjoint i.e S 134 A    Each s i node senses a given type of data 050 e.g temperature pressure brightness position and so on\051 while a i nodes combine the sensing data received from sensing nodes in their communication range Each 050sensing aggregator and veri\002er\051 node directly communicates with its closer neighbours 050at one hop distance\051 All the sensors s i whose data are to be collected by the node a j share a symmetric encryption key 024 a j with b  As we will discuss in Section 4.1 there is no need for storing any key on aggregator nodes 


4 Assessing the quality of collected data We combine secure data aggregation and a secure localization protocol to gather information about the data generated by the sensors Thus the sink node b is able to compute a cross-layer assessment of the overall quality of the collected data The proposed strategy is composed by four steps 1\051 Secure data aggregation s i nodes send their sensed data to the nearest a j  The data are encrypted with a key 024 a j not known to a j  The a j nodes aggregate all the received data without decrypting them 050see Section 4.1\051 and send them to b  At the end of the process b has the data in their aggregated form for example the average temperature of a region Encryption guarantees the integrity and con\002dentiality of the transmitted data while the key is not known to attackers However a clone of a sensor could have sent a forged datum that nonetheless was aggregated with the others 2\051 Node localization s i nodes provide to v j the pieces of information needed to localize them A beacon signal is normally enough the algorithm we chose 050see Section 4.2\051 is based on roundtrip time measurement and the v j nodes end up with a distance bound to s i  distance bounds are then sent to b  A malicious node might delay the beacon signal but not speed it up as a consequence the distance bounds could be larger than the actual ones but not shorter It is worth noting that the localization data cannot be transmitted as described in the previous step since no aggregation is performed 3\051 Assessment of localization b  thanks to the distance bounds received from the v j nodes 050at least three for each s i are needed\051 can now compute the position 1 050 x i  y i 051 of each sensing node s i  moreover the veri\002able multilateration 050see Section 4.2\051 provides a measure W i for the trustworthiness of the result each position can be marked as Robust  Malicious  or Unknown  At the end b has a table T   050 x i  y i 051  W i  that maps each position to its reliability measure 4\051 Cross-layer assessment of data quality b can use T to assess the quality of data aggregated on a 1 In this paper we consider nodes as points in the 2D space given region 004  in fact for each position within 004  a measure of its reliability is known thus according to the constraint of the application domain b may decide to discard the aggregated data if it is not considered reliable enough The next sections analyze in depth the previous steps 4.1 Secure Data Aggregation Limitations on power require a minimization of the amount of transmitted data from nodes to the sink Aggregation protocols may help in reducing the overall traf\002c among nodes At the same time since nodes are the attack goals of malicious users which try to violate the con\002dentiality and the integrity of data proper countermeasures are needed to perform a secure data aggregation Encryption can be used to secure node communication both hop-by-hop and end-to-end 050see Section 2\051 We chose an end-to-end secure aggregation solution in which an attack to any aggregator node is not able to compromise the whole system We adopted the algorithm described in because it is based on a simple and secure additively homomorphic stream cipher that allows ef\002cient aggregation of encrypted data The cipher algorithm uses modular additions and is therefore very well suited for CPU-constrained devices Aggregation based on this cipher can be used to ef\002ciently compute statistical values such as mean variance and standard deviation of sensed data while achieving a signi\002cant bandwidth gain A homomorphic encryption scheme enables arithmetic operations to be performed on encrypted data One example is a multiplicatively homomorphic scheme whereby the multiplication of two ciphertexts followed by a decryption operation yields the same result as say the multiplication of the two corresponding plain-text values Homomorphic encryption schemes are especially useful in scenarios where someone who does not have decryption keys needs to perform arithmetic operations on a set of ciphertexts A stream cipher is typically obtained by combining plain-text bits with a pseudorandom cipher bit stream 050key-stream\051 by an exclusiveor 050 010 051 operation Since 050 a 010 k 051 010 k  a  decryption is also easy to obtain The main idea of the homomorphic encryption described in is to replace the xor with modular addition 050 a  b 051 mod M  The main steps are the following 


017 Encryption 226 Each datum is represented by an integer 0 024 d 024 M 000 1  where M is a large integer 226 Let k be a randomly generated keystream where 0 024 d 024 M 000 1 226 The cipher-text c is given by c  Enc 050 d k M 051  050 d  k 051 mod M 017 Decryption d  Dec 050 c k M 051  050 c 000 k 051 mod M 017 Addition of cipher-texts 226 Let c 1  Enc 050 d 1  k 1  M 051 and c 2  Enc 050 d 2  k 2  M 051 226 then if M is suf\002ciently large Dec 050 c 1  c 2  k M 051  d 1  d 2  where k  050 k 1  k 2 051 mod M The scheme is made practical by generating encryption keys in each session with a pseudo-random function applied to a unique node id 050see for the details\051 from a logical point of view the sensor nodes and the sink share the encryption key of the aggregated datum However by attacking or cloning a single sensor node an enemy cannot compromise the whole system but at worst only aggregate a fake datum Aggregator nodes instead are immune from attacks since they handle only encrypted data 4.2 Secure Localization The node positions can be evaluated by using a multilateration technique which determines the node coordinates by exploiting a set of landmark nodes called the anchor nodes whose positions are known The position of the unknown node u is computed by using an estimation of the distances between the anchor nodes and the node itself The distance is not measured directly instead it can be computed by knowing the speed of the signal in the medium used in the transmission and by measuring the time needed to get an answer to a beacon message sent to u  If the computation is carried on without any precaution u might fool the anchors by delaying the beacon message However since a malicious node can delay the answer beacon but not speed it up under some conditions it is possible to spot malicious behaviors Veri\002able Multilateration 050VM\051 uses three or more anchor nodes to detect misbehaving nodes In VM the anchor nodes work as veri\002ers of the localization data and they send to the sink b the information needed to v 1 v 2 v 3 u   db 1  db 2  db 3 Figure 1 Veri\002able multilateration evaluate the consistency of the coordinates computed for u  The basic idea of VM is shown in Figure 1 each veri\002er v i computes its distance bound  to u  any point u 0 6  u inside the triangle formed by v 1  v 2  v 3 has necessarily at least one of the distance to the v i enlarged This enlargement however cannot be masked by u by sending a faster message to the corresponding veri\002er Therefore if the veri\002ers are trusted and they can communicate securely with b  the following algorithm can be used to check the localization data 1\051  Each veri\002er v i send a beacon message to u and records the time 034 i needed to get an answer 2\051  Each veri\002er v i 050whose coordinates  x i  y i  are known\051 send to b a message with its 034 i  3\051  From 034 i  b derives the corresponding distance bound db i and it estimates u s coordinates by minimizing the mean square error 017  P i 050 db i 000 p  050 x u 000 x i 051 2  050 y u 000 y i 051 2 051 2  where  x u  y u  are the coordinates to be estimated 2  4\051 b can now check if  x u  y u  are feasible in the given setting by two incremental tests a\051 016 test For all veri\002ers v i  compute the distance between the estimated u and v i  if it differs from the measured distance bound by more than the expected distance measurement error the estimation is affected by malicious tampering b\051 Point in the triangle test Distance bounds are reliable only if the estimated u is within at least one veri\002cation triangle formed by a triplet of veri\002ers otherwise the estimation is considered unveri\002ed 2 Such minimization is indeed far from trivial we used an approximation based on simulated annealing 


If both the 016 and the point-in-the-triangle tests are positive the distance bounds are consistent with the estimated node position which moreover falls in at least one veri\002cation triangle Thus the sink can consider the estimated position of the node as Robust  otherwise the information at hands is not suf\002cient to support the reliability of the data An estimation that does not pass the 016 test is considered Malicious  A sensible value of 016 depends on the expected error in time measurement and the number of available veri\002ers The simulation reported below should clarify the considerations involved in the choice of 016  If the 016 test is passed but the point-in-the-triangle one fails the sink marks the estimation as Unknown  meaning there is no suf\002cient information for evaluating the trustworthiness of node position Thus the localization phase ends up for each unlocalized node u i  with an estimation of the position of u i and a quality W i 2 Robust U nknown M alicious  4.2.1 Simulation We used OMN ET  050ver 3.3p1  17]\051 to set up a simulation of the secure localization algorithm A claimant node u to be localized resides at the center of a 100 m 002 100 m 002eld i.e at point  50  50   Since the best approach to lay out three veri\002ers is on the vertexes of an equilateral triangle we 002x ed their coordinates to be the points  1  1   99  1   50  85   If u is faithful it answers to veri\002ers beacons without any delay Otherwise if u is malicious it adds a variable delay to the answers in order to dissimulate a fake position u 0  i.e for each v i  if the distance 026 v i u 0 is greater than 026 v i u a proper delay is added by u to the answer beacon to v i  We assumed that signals travel at the speed of light and that time can be measured with an error whose standard deviation is 2 ns  As described above the timing information collected by veri\002ers v i can be used by the base station to classify the claimant as Malicious  Unknown  or Robust  Figure 2\050a\051 shows the effect of the choice of the 016 max in the 016 test on 10000 runs with 3 veri\002ers the only sensible value is 35 since lower levels have an overwhelming rate of false positives 050 i.e faithful nodes classi\002ed as Malicious 051 and a higher 016 gives too much false negatives 050 i.e malicious nodes classi\002ed as Robust 051 and unknowns About 50 of malicious claimants and 90 of faithful ones were classi\002ed as Unknown  the error in taking the estimated position instead of the real Figure 3 Empirical Cumulative Distribution Function 050ECDF\051 of deception for Unknown nodes Figure 4 Deception when a malicious node is classi\002ed as Robust one is pretty high as one can see from Figure 3 that plots the Empirical Cumulative Distribution Function of deception The situation is clearly improved when a fourth veri\002er is added 050see Figure 2\050b\051\051 the setting is now with a veri\002er at each corner of the 002eld and all the values less than 2.5 give acceptable results there are no Unknown s It is worth noting that the range of 016 considered is different since by increasing the number of veri\002ers the maximum acceptable error 016 max should decrease There are still some false negatives but the deception induced by a malicious node taken as Robust is always less than 1 m with 016 024 1  Figure 4 plots the density distribution of the deception 227 i.e the distance between the real position and the estimated one 227 at different values of 016  Adding a 002fth veri\002er randomly deployed signi\002cantly decreases the rate of false negatives as shown in Figure 2\050c\051 


050a\051 3 veri\002ers on a centered equilateral triangle 050b\051 4 veri\002ers on 002eld corners 050c\051 5 veri\002ers 4 on 002eld corners 1 randomly deployed Figure 2 Classi\002cation by secure localization 4.3 Cross-layer Assessment of Data Quality The sink receives a message encrypted by the scheme sketched in Section 4.1 Once decrypted the sink has the aggregated datum together with the guarantee of its integrity and con\002dentiality It could however embed fake data coming from malicious nodes which had mocked faithful nodes The sink has also data about the localization of nodes together with a marking of their trustworthiness quality Since each aggregated datum d refers to a given region 004 i  in order to assess the quality of d  the sink might use the localization information about any node n i 2 004 i and each node n j 2 004 j  j 6  i whose quality is Malicious or Unknown and the region 004 j con\002nes with the region 004 i  In other words the sink can use crosslayer information to guess how many possibly fake data were deceptively aggregated in the 002nal result It is still possible that an attacker decided to not lie on its position but only in the datum sent to the aggregator In that case however the position is faithful and other consistency properties could be exploited for example since temperature is a continuous quantity a datum of 40 016 C could be found as anomalous if its close to other measures that are about 20 016 C  Unfortunately this kind of considerations are application speci\002c and no general rule can be given In speci\002c cases a fake datum associated to a truthful localization can always snake in however at least the true position might be used to spot the malicious sensor All in all the crosslayer analysis enables a more careful assessment of the overall quality of the received data and data possibly affected by too many attackers can be discarded thus avoiding malicious poisoning Use case A simple numerical example should be suf\002cient to illustrate the application of the proposed approach A base station b has received the average temperature 05025 016 C 051 of a rectangular region de\002ned by the four points  0  0  000  10  0  000  10  10  000  10  0   b has built the following table listing all the information received about sensor nodes localization  x i y i W i  2 3 Robust 4 5 Robust 5 2 Malicious 11 3 Malicious 12 4 Robust  Therefore b may now asses the quality of the aggregated data received thus the average 25 016 C contains the data coming from the two sensors positioned in  2  3  and  4  5   and possibly from the two sensors positioned in  5  2  050maliciously asserting to be inside the region but probably outside\051 and  11  3  050maliciously asserting to be outside the region but possibly inside\051 Since the datum could be affected by two malicious nodes b decides to discard it 


5 Conclusions Data quality is a fundamental requirement in any wireless sensor network scenario Although it is very dif\002cult to provide data trustworthiness due to the distributed nature and the limited resource in terms of power of WSN the proposed methodology allows to analyze data trustworthiness by exploiting consistency on cross-layer information i.e node localization and data aggregation The proposed solution improves the knowledge about the security behavior of nodes that handle data More speci\002cally the trustworthiness about the node position information is used as a metrics for evaluating data trustworthiness In fact node position being target of different kind of attacks 227 i.e node malicious displacement or distance enlargement 227 is a good alarm for revealing malicious behavior Our methodology is 003exible in fact it results largely independent from the adopted routing protocols the veri\002cation localization algorithm and the secure data aggregation strategy Our approach can also be applied on wireless multimedia sensor networks due to its independence from the kind of sensing data 050multimedia or monomedia data\051 At the moment possible extensions for modelling privacy policies and the related enforcement mechanisms are under investigation Moreover the application of game theory for modelling malicious behavior and reason about rational choices represents a future goal Acknowledgment This research has been partially funded by the European Commission Programme IDEAS-ERC Project 227977-SMScom References   I F Akyildiz W Su Y Sankarasubramaniam and E Cayirci 223A survey on wireless sensor network,\224 IEEE Wireless Communications  vol 40 no 8 pp 102 226 114 August 2002   Patrick P A Felber R Guerraoui and A.-M Kermarrec 223The many faces of publish/subscribe,\224 ACM Comput Surv  vol 35 no 2 pp 114\226131 June 2003 A v ailable http://dx.doi.or g/10 1145/857076.857078   G Cugola A Margara and M Migliavacca 223Contextaware publish-subscribe Model implementation and evaluation,\224 in Proc of the IEEE Symposium on Computers and Communications 050ISCC'09\051  Sousse Tunisia 2009   C Castelluccia A C.-F Chan E Mykletun and G Tsudik 223Ef\002cient and provably secure aggregation of encrypted data in wireless sensor networks,\224 ACM Trans Sen Netw  vol 5 no 3 pp 1\22636 2009   L Hu and D Evans 223Secure data aggregation in wireless sensor networks,\224 in Workshop on Security and Assurance in Ad hoc Networks  2003   M Bagaa N Lasla A Ouadjaout and Y Challal 223Sedan Secure and ef\002cient protocol for data aggregation in wireless sensor networks,\224 in Proc of 32nd IEEE Conference on Local Computer Networks  2007   A Mahimkar and T Rappaport 223Securedav A secure data aggregation and veri\002cation protocol for sensor networks,\224 in Proc of IEEE Globecom  Dallas Tx 2004   B Przydatek D Song and A Perrig 223Sia Secure information aggregation in sensor networks,\224 in Proc of ACM SenSys  2003   J.Girao D.westhoff and M Schneider 223Cda concealed data aggregation for reverse multicast traf\002c in wireless sensor networks,\224 in Proc of IEEE ICC  Korea 2005   E.Mykletun J.Girao and D Westhoff 223Public key based cryptoschemes for data concealment in wireless sensor networks,\224 in Proc of IEEE ICC  Turkey 2006   L Eschenauer and V D Gligor 223A key-management scheme for distributed sensor networks.\224 in Proc of 9th ACM Conference on Computer and Communications Security  2002   R D Pietro A Mei and L V Mancini 223Random key assignment for secure wireless sensor networks,\224 in Proc of ACM Workshop on Security of Ad Hoc and Sensor Networks 050SASN\051  Fairfax-VA USA 2003   R D Pietro C Soriente A Spognardi and G Tsudik 223Collaborative authentication in unattended wsns,\224 in Proc of 2nd ACM Conference on Wireless Network Security 050WiSec  Zurich Switzerland 2009 


  S 020 Capkun and J Hubaux 223Secure positioning in wireless networks,\224 IEEE Journal On Selected Areas In Communications  vol 24 no 2 pp 221\226232 Feb 2006   S Brands and D Chaum 223Distance-bounding protocols,\224 in Proc of Workshop on the Theory and Application of Cryptographic Techniques  1994   OMNeT Community http://www.omnetpp.org   G Pongor 223OMNeT objective modular network testbed,\224 in MASCOTS'93 Proc of the International Workshop on Modeling Analysis and Simulation On Computer and Telecommunication Systems  San Diego CA USA The Society for Computer Simulation International 1993 pp 323\226326 


  Confidentiality measures directed toward the system itself may include authentication, authorization and directories. On the information side, single signon and identify management are common techniques Activities of the analyzed U.S. hospitals can be located in the upper quadrants increasing their focus on accountability including monitoring and auditing Early approaches checked on system layer monitoring activities; today application and domain layer objects are increasingly monitored and checked for compliance. While Swiss facilities attempt to improve accomplishments of the confidentiality property on the system layer, some unstructured monitoring attempts have been initialized. The upper right quadrants additionally require increased organizational IS security to be successful Administrative and clinical HIS adoption is further progressed at the two U.S. hospitals than at the Swiss ones. This seems to be influenced by market and organizational factors h ile th eir tech n i cal IS security capability is only slightly more advanced organizational IS security capabilities are significantly higher. Test plans and procedures are incorporated into operations at U.S. hospitals. Relative spending and budget for IT and IS security in relation to revenue are comparable between hospitals, except CH 2 which spends significantly less than the other facilities  4.6. Security in Heterogeneous IS  Most HIS today consist of a multitude of heterogeneous and fragmented components, accentuating the impracticability of completely monolithic systems. As a result, applications differ widely not only in business functionality but also in technical design and architecture, the rigor of applied security engineering techniques, and the support of security-related mechanisms and functions. Many vendors and service providers do not facilitate security interoperability due to a lack of standards. For example, authentication credentials are extractable from directory services with most contemporary applications; legacy systems do not provide such support. Generic authorization standards do not optimally model the workflow of multi-level, multi-user environments. Auditing trail data for applications and information is often not available and no interoperability protocol exists Many applications are interlinked using health care specific messaging protocols, disseminating sensitive information unauthenticated to a variety of distributed, possibly unauthorized applications that may be located beyond the security perimeter. Dependence of information flows and system components are fragmentarily identified To retain the combined system manageability of information, application, and system components, it is essential to obtain an aggregated representation for each information entity and its dependence on application and underlying computing and infrastructure components. Frequently, information entities are stored within multiple application components, which rely on their part on a variety of physical computing and storage devices. Due to regulations of health and personal information, it is essential to include application and domain layer elements in the representation and therefore go beyond the inventory lists of system and networking infrastructure, which can be easily obtained by commercial tools In addition to a heterogeneous application system environment, the components are used in different manners and settings. For example, clinical settings require authenticated access-levels dependent on the business function of the user, session lengths dependent on access patterns, and access to multiple applications for short intervals by a variety of personnel Often client terminals are shared by many professionals for practical reasons, but information access and modification needs to be accountable to each individual. The predicament of accessing vital health information lies in its dichotomy of obtaining timely access versus confidentiality and integrity concerns only allowing permitted and authenticated care providers to read and modify potentially life-threatening information. On the one hand, generic and unrefined authentication and authorization procedures may severely limit productivity and efficiency of clinicians On the other hand, lax security controls defeat the described requirements. Security controls and policies need to be specifically crafted and specified for each asset, resulting in the most effective protection of the information entity. The component owner carries the responsibility for the asset and makes decisions according to the organization s security specifications and budget  5. Conclusion  In this paper we discuss security aspects of Health Information Systems. In particular, four case studies on two U.S. and two Swiss hospitals are summarized and the results of the case studies compared with respect to different issues The empirical research showed that IS security in hospitals lacks some controls, in particular organizational measures. Size and IT diffusion as well as specific regulations influence organizational IS security Usage patterns of clinicians and heterogeneous application systems exacerbate security efforts Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


  U.S. organizations seem to have more controls in place than their Swiss counterparts. Technical security controls have become part of their system lifecycle. However, they still lack more advanced security and authentication controls  References  1 Ide ntity T h e f t Re sourc e Ce nte r  200 7 Da ta  Breach Stats," itrc, San Diego, 2008 http://idtheftmostwanted.org/ITRC%20Breach 20Stats%20Report%202007.pdf 2 P r ic e W a t e r hous e C o ope rs  2 00 8 Inf o rm a tion Se curity Breaches Survey", Technical Report PWC, London, 2008 http://www.pwc.co.uk/pdf BERR_ISBS_2008\(sml 3 M.F. C o llen  A History of Medical Informatics in the United States, 1950 to 1990 American Medical Informatics Association, Indianapolis 1995 4 B. Middle t on W  E. H a m m ond, P  F  Bre nna n   and G.F. Cooper, "Accelerating U.S. EHR Adoption: How to Get There From Here. Recommendations Based on the 2004 ACMI Retreat Journal of the American Medical Informatics Association 12 \(1\2005, pp. 13-19  T  U Dai m  R T  T a r m an  an d N Baso g l u   E x ploring Barriers to Innovation Diffusion in Health Care Service Organizations: An Issue for Effective Integration of Service Architecture and Information Proceedings of the 41st Hawaii International Conference on System Sciences IEEE Los Alamitos, 2008 6 C.S. G a rrard  H u m an Co m p u t er In teractio n s   Can Computers Improve the Way Doctors Work Schweizerische Medizinische Wochenschrift 130 \(42\, 2000, pp. 1557-1563 7 U  S. D e pa rtm e nt of He a lth H u m a n Se rv ice s   Health Insurance Reform: Security Standards HHS, Washington, 2003 http://www.cms.hhs.gov/SecurityStandard Downloads/securityfinalrule.pdf 8 G  F. K nolm a y e r a nd G  L oos li I T  G o v e rna n c e   in Handbuch Kompetenzmanagement R. Zaugg Ed., Haupt, Bern, 2006, pp. 449-457 9 Yin   Case Study Research: Design and Methods 3rd ed., Sage Publications, Thousand Oaks, 2003 10  Bunde s a m t f  r Sic h e r he it in de r Inf o rm a tions technik, "IT-Grundschutz-Kataloge," BSI, Kln 2005 http://www.bsi.bund.de/gshb/deutsch/index.htm 11 L  thi Information System Security in Health Information Systems - Exploratory Research in US and Swiss Acute-Care Hospitals Eul Verlag Lohmar, 2008   12  N a tiona l Ins tit ute of Sta nda rd a nd T e c hnolog y   An Introductory Resource Guide for Implementing the Health Insurance Portability and Accountability Act \(HIPAA\curity Rule National Institute of Standard and Technology, Gaithersburg, 2008 13  Eidg e ns s i s c he r D a te ns c hutz b e a u f t rag t e r ED S B Leitfaden zu den technischen und organisatorischen Massnahmen des Datenschutzes http://www.edoeb.admin.ch/dokumentation/0044 5/00472/00935/index.html?lang=de 14  N  M. L o re nz i a nd R  T  Rile y   Managing Technological Change: Organizational Aspects of Health Informatics 2nd ed., Springer, New York 2004 1 A  Bh attach erje e an d N Hi k m et  P h y sician s' R e sistance toward Healthcare Information Technologies: A Dual-Factor Model Proceedings of the 40th Hawaii International Conference on System Sciences IEEE, Los Alamitos 2007 16  R. Sp an jers, W  Hasselbring R. P e t e rso n and M  Smits, "Exploring ICT Enabled Networking in Hospital Organisations Proceedings of the 34th Hawaii International Conference on System Sciences IEEE, Los Alamitos 2001 1 T h e A d vi so r y Bo ard Co m p an y   Toward Integrated Identity and Access Management The Advisory Board Company, Washington, 2006 18  B  B l obe l A uthoris a t i on a nd A c c e s s C ontrol f o r Electronic Health Record Systems International Journal of Medical Informatics 73 \(3\, 2004, pp 251-257 1 J E  M  l l er an d H  V o segaard  E xp erien ces w ith  Electronic Health Records IT Professional 10 2 2008, pp. 19-23 20  S. K a hn a n d V She s ha dr i M e d ic a l R e c o r d P r ivacy and Security in a Digital Environment IT Professional 10 \(2 2008, pp. 46-52 21 R   A nde r s on Security Engineering: A Guide to Building Dependable Distributed Systems Wiley New York, 2001 22 ISO ISO/IEC 17799: Information Technology Security Techniques - Code of Practice for Information Security Management 2nd ed., ISO, Geneva, 2005 23  P. Me rte n s a nd G  K nolm a y e r Organisation der Informationsverarbeitung 3rd ed., Gabler, Wiesbaden, 1998 24  B.B. W a ng T  T  H. W a n  D.E Bu rk e, G  J. Bazzoli, and B.Y.J. Lin, "Factors Influencing Health Information System Adoption in American Hospitals Health Care Management Review 30 \(1 2005, pp. 44-51  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


several projects at NASA Glenn Research Center in Cleveland Ohio.  His career spans nearly twenty years involving the research implementation, and troubleshooting of network protocols and applications within satellite, mobile, and aeronautical environments pre></body></html 


common concern was compliance with the Healthcare Insurance Portability and Accountability Act \(HIPAA information privacy. Organizations were unsure whether DP products would be HIPAA compliant and even if the products themselves were secure whether their onboard information could be kept secure when the pens are small and easily lost  3.4.3. Implications for DP. Although there was substantial interest in DP technology among professional services and white collar organizations the ethnographic findings suggest that delivering appropriate value from DP technology would be difficult. The first barrier would be providing a platform that either conforms to or changes current behaviors around pen and paper: free availability of pens and unpatterned paper in the workplace behaviors of writing on environmental paper, and the need to keep up with a personal pen with one?s data The second barrier would be to meet expectations around text; much of the value was seen as being in retention and searching of textual information, but this would be made difficult by the frequent inclusion of acronyms, abbreviations symbols, and foreign languages. Many organizations saw value in centralized repositories that would manage DP information across many employees; this would require an IT infrastructure that was not available \(e.g., to manage the DP pattern space might be difficult and expensive to develop and maintain. It also poses questions about privacy, since employees might not wish the archive to retain personal information captured alongside work notes Finally, because we noted real or perceived barriers that were highly specific to various industries it appeared that DP solutions would be most likely to succeed if they were tailored to very specific settings Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 and use cases. This does not argue against DP usage and its value, but it suggests that delivering value to organizations may require substantially more design and development effort than a single general solution or platform would provide  4. Discussion  Our findings show that the interaction model of DP technology diverges substantially from common pen and paper usage. This divergence leads to unclear utility of DP for consumer note-taking purposes, and in a field trial, resulted in high abandonment rate of a consumer DP product. Among the most significant limitations were the low value seen in review of facsimile notes on a PC, the requirement with DP to use a special pen paired with patterned paper, and the low perceived value of capturing only notes taken on blank paper as opposed to handouts and other materials. In the enterprise space, the environmental issues were even more prominent due to the common infrastructure of freely available, communal pens and paper and the lack of IT infrastructure for DP document retention The note-taking research reported here suggests that writing behavior comprises many important areas besides capture, retention, and review of specific data. For instance, a single blank piece of paper or a one-word reminder may adequately serve as a reminder to do something that one intends, even though there may be little or no content as such Likewise, in interpersonal contexts such as attending 


Likewise, in interpersonal contexts such as attending a lecture, it may be socially desirable to take notes even if they are later simply disposed. Taking notes may also serve purposes of memory consolidation even when the content is never reviewed [8][9  4.1 DP and Writing Acts  Linguistic acts in social context have been described using a model of performative behavior commonly known as ?speech acts? theory [15 Chapman [2] extended that model to encompass writing, suggesting that many aspects of ?writing acts are unique and separate from spoken language.  In the extended model, writing acts may be described with a multidimensional taxonomy encompassing a writer?s context, aspects of the process, type of content, and linguistic features of the content Table 3 summarizes the dimensions of writing acts \(from [2 exemplify each dimension. For instance, ?separability denotes the extent to which items in a document are logically independent of one another; in a contract nothing is separable because the document is a single piece, while on a to-do list, many or all items may be independent of one another [2 DP technology may benefit from attention to the dimensions that are exemplified by writing acts embedded in social contexts. DP systems may be able to perform some kinds of writing acts quite well, but in other cases, DP may be inconvenient, unnecessary or inappropriate. Models such as the writing acts framework can be used both to understand user behavior broadly and systematically to explore the applicability of DP products across the general space of writing behavior A key problem for DP in note taking applications is that note taking can involve nearly every possible dimension of writing acts. Notes may be separable or not; they may serve as functional content or as contextual reminders; they may present just the facts? or be more interpretive; they may be transient or might be intended to be archival documents; and so forth. In short, notes are able to present a vast array of writing styles that pose substantially different value propositions and technological implications for DP products Delivering a general DP solution for note taking may therefore be expected to be difficult  Table 3: Dimensions of Writing Acts [2  Dimension Exemplars Separability a contract vs. a to-do list Function a typed inventory list vs calligraphy Emotionality a love letter vs. a packing slip Spatiality a transcript of speech vs. a diagram depicting concept relationships Associativity notes from class vs. doodling Linearity chronological notes vs. notes placed in a spatial ordering scheme Originality an essay vs. feedback on a manuscript Prescriptivity a signature vs. general notes Finality a document that will be archival vs. one that is a draft Structure a grocery list vs. concepts from a brainstorm Personality a letter to someone vs. a 


Personality a letter to someone vs. a journalistic essay Formality a business letter vs. a greeting card to a close friend  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 For product development purposes, the writing acts framework can be used to ensure coverage of use cases by generating possible scenarios combining various attributes of written documents. These potential combinations may then be explored evaluated, and prioritized for research attention development effort, or testing  4.2 Possible Directions for DP Technology  4.2.1. Note taking. What would make DP technology more popular for note taking? Our research suggests that this is a complex question because there are numerous behavioral barriers. The largest single problem for DP may be that it is a closed system comprising a special pen and patterned paper and does not function with the wide array of writing utensils and paper products that people use especially for environmentally available pens, paper and documents. If this problem could be solved or mitigated through an expanded DP technology platform, DP would avoid the large behavioral block posed by the current need for users to change their existing pen and paper habits DP effectively focuses on notes as data neglecting many aspects of the embedded social nature and process function of notes. If DP notes were easily integrated into a wider range of behavioral processes, adoption should increase. For example, if notes could be automatically handled for content such as phone numbers, appointment times reminders, temporary content, and the like, then the DP platform would come closer to matching current pen and paper usage. However, in many cases, there is a separate and larger issue: computer technology today also is not integrated into such processes. To take a simple example, consider a written grocery list Even if the problem could be solved to recognize extract, and transfer the list to a PC, it would be of little use because, for most people, the PC itself is not integrated into the grocery shopping process. Much information of this kind is transient; there is no need to manage or retain it once the paper has been used An example of potentially closer workflow integration is shown in the recently released Livescribe Pulse Smartpen [11], which couples the Anoto DP platform with audio recording such that note takers can review the audio of a meeting or lecture at the exact point that a note was written. We believe that these kinds of additions to base DP functionality are likely to appeal to specific niches of users, but as more use cases like these are enabled over time, DP may successively attain value for larger numbers of users In our field trials and organizational visits, one of the most common customer expectations was that DP notes should be converted from handwriting to text; respondents commonly noted that PC data is of little use unless it is transcribed to text. To meet customer demands, a DP product will need to address this expectation: the DP must either deliver text recognition with very high accuracy, which is a difficult problem, or it should manage the expectation in some other way that preserves customer perception of value from PC integration 


of value from PC integration The high cost of DP products \(approximately US 100 for a pen, plus the need for specially patterned paper demonstrable additional benefit. Transferring notes from paper to a PC today merely involves typing Unless the need for automation is great and the DP function is nearly perfect, users may simply prefer to type or to carry paper rather than to change behavior to use expensive and less flexible technology  4.2.2. Structured input. As noted in the Introduction above, another use case for DP technology is structured input of information. In particular, DP technology may be useful for form-based input into database and workflow systems, where information is initially recorded on paper forms and then automatically transferred from the pen to a database application. Although the present research report is concerned primarily with note taking applications, in the course of our enterprise research we discussed potential applications for forms-based DP usage We noted possible use cases that fell into five general areas: \(1 easier input of information by customers, such as clients filling out deposit or withdrawal slips at a bank; \(2 information from paper to database without needing to rekey or type the information, such as factory inspection and quality assurance logs, traffic tickets shipping manifests, and so forth; \(3 environments that were not suitable for handheld computing devices, such as construction sites and some kinds of manufacturing facilities; \(4 in which paper-based records are desirable for either employee compliance or customer comfort, such as medical settings; and \(5 based records are necessary, e.g., for legal reasons but a DP product could support faster turnaround and error correction. An example of such application involved financial forms that undergo offsite optical character recognition; a DP system might allow immediate recognition and error correction We plan to report this line of research fully in the future. For now, we note that each of those areas has various benefits and potential limitations with regard to DP technology. Forms-based use cases are more precisely defined and structured than note taking, in Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 terms of environment, workflow, and content; the information context, form design, and potential for systems integration with rapid feedback may mitigate issues with handwriting recognition; and enterprise customers may be less price sensitive than consumers Thus, structured input appears to be a more promising near-future application of DP technology than note taking  4.3 Research Discussion  As we noted above, the present research was primarily behavioral and qualitative. Thus, despite the strength and consistency of our results across multiple samples, contexts, and locations, we present no specific metrics or tested hypotheses on user behavior with DP products. Our findings could be used to inform directional hypotheses for future depth or quantitative research. For instance, one might formulate and test hypotheses about cultural differences, interest levels in DP between various groups such as professional and non-professional 


office workers, market research metrics such as price sensitivity, and the like It is important to underscore the value of ethnographic fieldwork in the present research. It would be possible to conduct design research that explores how to make DP technology better, e.g., in terms of usability and function, without investigating exactly what people would do with such products and why. It was only when we investigated behavior in depth that we discovered the divergence of DP products? limited current value for note taking, as opposed to the high value that one might presume in the absence of depth research  5. Conclusion  In our research, initial trials of digital pens in a controlled setting \(Research Series 2 above suggested potentially good fit between digital pen functionality and consumer note taking needs However, when we explored real world behavior in note taking \(Research Series 1, 3, and 4 above found many potential barriers to adoption of digital pens for note taking. In particular, traditional pen and paper offer advantages in terms of cost, widespread and ad hoc availability, flexibility to work with multiple sources seamlessly, behavioral workflow integration, and manageability of content The value of using digital pens will increase if manufacturers are able to expand their platform technology progressively to enable broader coverage of behavioral scenarios and habits, focusing on the broad range of writing behaviors rather than just needs for facsimile replication on a computer Alternatively, digital pen technology may be more easily applied to tasks involving structured input rather than unstructured note taking We suggest that DP development efforts should use existing linguistic frameworks \(e.g., [2 the space of writing acts of interest. This should allow DP products to target behavioral needs in a more focused manner, leading to higher customer adoption  6. Acknowledgements  We thank many people who participated in this research: first and foremost, the numerous research participants, firms, and organizations who so generously shared their time and insights, but who must remain confidential; and many colleagues and research partners at Microsoft who collaborated on the technology investigation and research efforts especially Jeff Staiman, Vince Ball, Brian Williams Yoshiyuki Moriya, Stephen Cooper, Dave Shen Setsuko Arimatsu, Benjamin Babcock, Dennis Meinhardt, John Chiloyan, Glen Larsen, Mehrdad Basseri, Lori Birtley, Ken Hinckley, and Jian Wang  7. References  1] Anoto Group. Digital pen and paper. Web page http://www.anoto.com/?id=158, last retrieved May 28 2008  2] Chapman, C. N. Writing acts: taxonomy and technological implications. Paper presented at North American Computing and Philosophy \(CAP Corvallis, OR, August 2005  3] Despont-Gros, C., B?uf, C., Geissbuhler, A., and Lovis C. \(2005 


C. \(2005 Implementation and Use in an Existing Clinical Information System. In Connecting Medical Informatics and Bio-Informatics: Proceedings of MIE2005 - The XIXth International Congress of the European Federation for Medical Informatics, 328-333. IOS Press, 2005  4] Fetterman, D. M. Ethnography, 2nd ed. Thousand Oaks CA: Sage, 1997  5] Furukawa, N.  Ikeda, H.  Kato, Y.  Sako, H. D-Pen: a digital pen system for public and business enterprises. In Frontiers in Handwriting Recognition 2004: Proceedings of the Ninth International Workshop on Frontiers in Handwriting Recognition \(IWFHR-9 2004  6] Guimbreti  re, F. 2003. Paper augmented digital documents. In Proceedings of the 16th Annual ACM Symposium on User interface Software and Technology Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 UIST ?03 November 2003  7] Holman, D., Vertegaal, R., Altosaar, M., Troje, N., and Johns, D. 2005. Paper windows: interaction techniques for digital paper. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems \(CHI ?05 Portland, Oregon. New York: ACM, 2005  8] Kiewra, K., DuBois, N., Christian, D., McShane, A Meyerhoffer, M., &amp; Roskelley, D. Note-taking functions and techniques. Journal of Educational Psychology, 83 240-245, 1991  9] Kobayashi, K. Combined effects of note-taking/reviewing on learning and enhancements through interventions: a meta-analytic review. Educational Psychology, 26, 459-477, 2006  10] Liao, C., Guimbreti  re, F., and Hinckley, K. 2005 PapierCraft: a command system for interactive paper. In Proceedings of the 18th Annual ACM Symposium on User interface Software and Technology \(UIST ?05 Seattle, WA. New York: ACM, 2005  11] Livescribe. Pulse Smartpen [electronic device http://www.livescribe.com/, last retrieved May 28, 2008  12] Logitech. io2 Digital Pen [electronic device http://www.logitech.com/index.cfm/mice_pointers/digital_ pen/devices/408&amp;cl=us,en, last retrieved May 28, 2008  13] Norrie, M. C., Signer, B., and Weibel, N. Print-n-link weaving the paper web. In Proceedings of the 2006 ACM Symposium on Document Engineering \(DocEng '06 New York: ACM, 2006  14] Randall, D., Harper, R., and Rouncefield, M Fieldwork for Design: Theory and Practice. London Springer-Verlag, 2007  15] Searle, J. R. Speech Acts: An Essay in the Philosophy of Language. Cambridge: Cambridge Univ. Press, 1969  16] Sellen, A. J. and Harper, R. H. The Myth of the Paperless Office. Cambridge, MA: MIT Press, 2003  17] Signer, B. and Norrie, M. C. 2007. PaperPoint: a paper-based presentation and interactive paper prototyping 


paper-based presentation and interactive paper prototyping tool. In Proceedings of the 1st international Conference on Tangible and Embedded interaction \(TEI ?07 Baton Rouge, Louisiana. New York: ACM, 2007  18] Tanabe, K., Yoshihara, M., Kameya, H., Mori, S Omata, S., Ito, T., Automatic Signature Verification Based on the Dynamic Feature of Pressure. Proceedings of the Sixth International Conference on Document Analysis and Recognition \(ICDAR ?01 Computer Society, 2001   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





