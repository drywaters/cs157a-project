    Computational Evaluation of Software Security Attributes   Gwendolyn H. Walton Florida Southern College and Software Engineering Institute/CERT Carnegie Mellon University   Thomas A. Longstaff Johns Hopkins University Applied Physics Laboratory    Richard C. Linger Software Engineering Institute/CERT Carnegie Mellon University   Abstract  
In the current state of practice, security properties of software systems are typically assessed through subjective, labor-intensive human evaluation Moreover, much of the quantitative security analysis research to date is characterized by the development of approximate solutions and/or based on assumptions that severely constrain the operational utility of the results. In order to achieve a dramatic increase in maturing the discipline of software security engineering, a fundamentally different approach to analysis and evaluation of security attributes is required The computational security attributes \(CSA\approach to software 
security analysis provides a new approach for specification of security attributes in terms of data and transformation of data by programs. This paper provides an introduction to the CSA approach provides behavioral requirements for several security attributes, and discusses possible application of the CSA approach to support analysis of security attributes during software development, acquisition verification, and operation   1. Introduction  Dynamic security analyses are necessary if an organization hopes to keep pace with the potential 
impacts from ongoing program maintenance and system evolution, changing threats in the operational environments, and changes in organizational security strategies However, in the current state of practice security properties of software systems are typically assessed through subjective, labor-intensive human evaluation  The behavior of users and administrators, the operating environment, and data transmission mechanisms are non-deterministic. Researchers have typically addressed this non-determinism by using a 
variety of analytical tools such as model checking concurrent sequential process modeling, and rulebased systems to model, analyze, and verify security protocols. In recent years, several research threads have emerged that address security metrics and quantitative and qualitative analysis and evaluation of security attributes.  Much of the security attribute analysis research to date is characterized by the development of approximate solutions and/or based on assumptions that severely constrain the operational utility of the results.  In order to achieve a 
dramatic increase in maturing the discipline of software security engineering, a fundamentally different approach to analysis and evaluation of security attributes is required The CSA approach to software security analysis provides theory-based foundations for precisely defining and computing security attribute values. The translation of a static security property expressed as an abstraction of external data to dynamic behavior of a program expressed in terms of its data and functions is key to the CSA approach to verification 
of behavior that meets a specific security property The ultimate goal of the CSA research is to develop mathematical foundations and corresponding automation to permit both rigorous evaluation and improvement of the security attributes of software during development and real-time evaluation of security performance during operation  2. The CSA Analysis Approach  The CSA approach to security attribute analysis illustrated in Figure 1, consists of three steps   Define required security behavior Specify security attributes \(such as authentication,  non 
repudiation, etc.\ terms of required behavior during execution expressed in terms of data and transformation of data by programs   Calculate program behavior Create a behavior database using the new technology of function extraction \(FX\hat contains the specification for Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 


  the complete as built functional behavior of the code   Compare program behavior to required security behavior Compare the computed behavior database with required security attributes to verify whether the software meets its security requirements  2.1. Define required security behavior   Requirements for security attribute behavior must explicitly define expected behavior of code in all circumstances of interest. Thus, the requirements for security attribute behavior must include a minimal definition of required behavior for all inputs of interest to the security attributes, including desired inputs \(for example, an authenticated user id\d undesired inputs \(for example: an unknown user id Usage environment conditions related to security attributes are specified in the same manner as inputs to the system. For example, availability of the network might be specified by a Boolean value that indicates whether or not the network is currently available. Security successes and failures are also specified in terms of data. For example, system control data can be used to indicate whether the current user has been authenticated using a trusted authentication mechanism The level of abstraction at which a security attribute is specified can depend on the specific situation. For example, if all available data transmission mechanisms have previously been certified to be trusted, the security attribute requirements would not need to include details about data transmission. If there is one trusted data transmission mechanism, X, and one or more data transmission mechanisms that may not be trusted, the security attribute requirements could specify that all data transmissions will be performed using X. If none of the data transmission mechanisms have been previously certified as trusted, the security attribute requirements will need to include required control data effects for transmission security A never responded and no output case for each external function call of interest must be considered, including a definition of correct behavior in the case of intentional and unintentional aborts and hangs.  In addition, security attribute requirements may specify a specific order in which certain functions can be called. For example: user authentication must occur before any data access functions can be called    Function Extractor Behavior Catalog Required Security Attribute Behavior CSA Analyzer Security property satisfied \(or not Input Program Calculate Program Behavior Define required security behavior Compare program behavior to required security behavior  CSA Analysis Function Extractor Behavior Catalog Required Security Attribute Behavior CSA Analyzer Security property satisfied \(or not Input Program Calculate Program Behavior Define required security behavior Compare program behavior to required security behavior  CSA Analysis  Figure 1. CSA analysis  The requirements may specify that a certain set of data transformations always occur. For example the control data that indicates that data transmission is secure must always be set by the trusted data transmission mechanism. The requirements may specify that a certain set of data transformation must never occur. For example, the control data that indicates that data transmission is secure must never be set by any code other than the trusted data transmission mechanism. If a user is authorized to only access specific data, the requirements may state that no data transformations other than a specific set of data transmissions can occur Any amount of traceability and control can be specified in the requirements for security attribute behavior.  For example, the requirements may include specifications of bounded behavior. \(i.e., the execution will proceed so long as the behavior is Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 


  within a specified domain\. Specifications for trusted mechanisms can be included in the requirements as a constraint. For example, one might specify that a call to method XXX that returns a value of y is sufficient to satisfy a requirement that a trusted mechanism must be used to perform authentication  A behavioral approach also supports dealing with some uncertainty in the specification of the security attribute requirements. For example, a security requirement might state that the code must guarantee security properties modulo some defined value. Some constraints might be specified using a stochastic component. For example The response history of component X must indicate that the component was available at least 94% of the time   2.2. Calculate program behavior   In this context, b ehavior means the as-built net functional effect of a program \(and its constituent control structures and instructions\rom entry to exit Behavior calculation means to create a behavior database that specifies the behavior of the code Software with unknown behavior cannot be certified as secure.  Thus, a behavior database containing all externally observable behaviors is an essential input to security attribute analysis. For software developed in-house, the behavior database might be derived from the specification from which the software is developed and verified. For procured software, a complete behavior database might be part of the acquisition requirements. For existing software for which a behavior database is not available intensive manual effort to understand the code may be necessary The CERT  organization of the Software Engineering Institute is developing Function Extraction \(FX\heory and technology for understanding program behavior. [1, 4, 5 T h e  objective of this work is to compute the functional behavior of programs with mathematical precision to the maximum extent possible.  The FX approach to program comprehension eliminates the need to study and understand code by manual means. The ultimate goal of the FX technology project is to develop theory and tools to automatically calculate program behavior based on precise semantics of the programming language, producing databases that represent the net behavior of programs in terms of disjoint conditional concurrent assignment statements FX technology is based on a function-theoretic view that treats programs and their parts as rules for mathematical functions or relations subject to compositional analysis to derive net effects. CERT   researchers are currently developing an FX system to support function extraction of assembly programs using complete semantics for Intel 32-bit assembly language code Behavior databases produced by the FX system provide several advantages for security attribute analysis   A behavior database lends itself to query and thus can facilitate risk assessments and other query-driven analyses of security attributes   Because the behavior database supports examination of the functional transformations on data and does not require examination of the state space, this approach is more scalable than formal methods approaches. For example, when formal methods are used to examine the integrity attribute, the entire state space must be computed. Integrity exists if the system can never map outside the state space. In contrast with CSA and the use of behavior databases only the calculated behavior is of interest Because only externally observable behavior is relevant to security attribute analysis, only the behavior at the boundary of the system is required for the calculation. Thus, there is no need to calculate the internal behavior of trusted components, and there is no need to expose the entire internal state space   If a property can be expressed in code, FX technology can be used to determine if that property holds within a program   FX technology can be used to describe the behavior of a function that combines two behaviors. Thus, FX can be used to give the exact description of the composition of the behaviors   Corporate policies and intentions can be defined in a behavioral format in advance of the design of the architecture and code. Queries to examine the behavior database for the presence or absence of desired properties can be developed in parallel with design of the architecture. If pre- and post-conditions are defined behaviorally, they can be used to evaluate all artifacts \(i.e., the behavioral databases, not just the code  2.3. Compare program behavior to required security behavior    The translation of a static security property expressed as an abstraction of external data to dynamic behavior of a program expressed in terms of Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 


  its data and functions is key to the verification of behavior that meets a specific security property Verification that a security property is satisfied requires verification of both the data at rest \(i.e. the control data values\d the data in motion \(i.e., the mechanisms used to perform the data transformations\me common tasks to verify data at rest include checking to make sure that a specific task \(for example, an audit task\ will always be carried out to check the contents of a specific control data structure The CSA approach to security attribute verification includes the use of constraints and boundary conditions to make any assumptions explicit. People and process issues can be handled by the CSA approach by using assumptions and constraints as part of the behavior databases. In addition, behaviors can embody requirements for a given security architecture. Thus, the attribute verification process will expose security vulnerabilities, making it easier to address evolution of code, environment, use, etc  3. Behavioral requirements of security attributes  The behavioral requirements for security attributes can be completely described in terms of data items and constraints on their processing. The processing can be expressed, for example, as logical or quantified expressions or even conditional concurrent assignments, which can be mechanically checked against the calculated behavior of the software of interest for conformance or nonconformance with security attribute requirements Security attributes are inter-related. For example authentication relies on non-repudiation as a strict subset of the required security behavior.   Thus quantitative reasoning about the attributes requires an integrated set of security attribute definitions  3.1. Use of a trusted mechanism  Each security attribute requires the use of one or more trusted mechanisms that are implemented in software components. A behavior database is needed for each mechanism to describe all cases of behavior in terms of the mechanism s data and the transformations it carries out on that data. This behavior database is analyzed in terms of control data and evidence data to ensure the following   The trusted mechanism sets the values of control data, which indicates whether the mechanism executed correctly   If control data indicates that the mechanism executed correctly, there exists evidence data to show that the data transformation was performed in a manner that satisfies the defined security specification Note that the implementation of a security attribute may include a trusted third party to acquire authenticate, and adjudicate evidence of transactions However, for the purposes of behavior specification of security attributes, the specific mechanism and actors are not relevant. All that is needed is a precise specification of the data and the transformations of the data and any constraints concerning those transformations  3.2. Trusted data transmission  Figure 2 illustrates the requirements for trusted data transmission   A trusted data transmission mechanism is used for all data transmissions. If the mechanism is not available or the mechanism fails, the requirement fails   No mechanism outside this trusted data transmission mechanism sets the value of the control data that indicates whether the data transmission mechanism executed correctly Figure 3 illustrates the process for determining whether data transmission security properties are satisfied by the data transmission components of a system. This process consists of the application of the CSA analysis process illustrated in Figure 1, where the input is the data transmission components of the system, and the output is a determination of whether the data transmission security requirements have been satisfied. Similarly, the process for determining whether the remainder of the system can modify the control data set by the data transmission components consists of application of the CSA analysis process of Figure 1, where the input to the process is the software for the entire system, and the output is a determination of whether the control data set by the data transmission components can be modified by any other part of the system The process for determining whether the remainder of the system can modify the control data set by the data transmission components, illustrated in Figure 4, consists of the application of the CSA analysis process of Figure 1, where the input to the process is the software for the entire system, and the output is a determination of whether the control data set by the data transmission components can be modified by any other part of the system Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 


  CSA Analysis data transmission security  CSA Analysis data transmission security Data Transmission mechanism Data Transmission mechanism security property satisfied CSA Analysis no modification of data transmission control data CSA Analysis no modification of data transmission control data Remainder of the System Remainder of the System security property satisfied both properties satisfied No. Data transmission is not trusted Yes. Data transmission is trusted CSA Analysis data transmission security CSA Analysis data transmission security Data Transmission mechanism Data Transmission mechanism security property satisfied CSA Analysis no modification of data transmission control data CSA Analysis no modification of data transmission control data Remainder of the System Remainder of the System security property satisfied both properties satisfied No. Data transmission is not trusted Yes. Data transmission is trusted  Figure 2. Requirements for trusted data transmission   Behavior Catalog Data transmission security requirements Analysis of security behavior Data Transmission Program Calculate programmed effects on control data Define required control data effects for transmission security Compare program behavior with required effects on control data Function Extractor CSA Analysis: Data transmission security  Security property satisfied \(or not Behavior Catalog Data transmission security requirements Analysis of security behavior Data Transmission Program Calculate programmed effects on control data Define required control data effects for transmission security Compare program behavior with required effects on control data Function Extractor CSA Analysis: Data transmission security  Security property satisfied \(or not  Figure 3. CSA analysis of data transmission security  As illustrated in Figure 4, to verify that a data transmission mechanism is trusted, it is necessary to verify the data that provides the evidence related to data transmission. For example, the specification for the data that provides evidence of valid data transmission may describe the mechanism by which each data message output incorporates a shared between sender and receiver\ that can be used to verify that the transformation worked correctly. Assignments to this shared data must not be reversible \(i.e., guaranteed encryption.\ As another example, suppose the behavior databases for all of the code have been examined to verify that all data transmissions in the system occur as a result of calls to function YYY. To verify the necessary security properties for data transmission, it is necessary to examine function YYY s behavior database to determine the net effect of the data transformations related to any conditions for which invalid data transmission could occur Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


    Behavior Catalog Requirements for no modification of specific control data Analysis of security behavior Security property satisfied \(or not Remainder of the system Calculate programmed effects on control data Define requirements for security property no modification of data transmission control data Compare program behavior with requirement for no modification of data transmission control data Function Extractor CSA Analysis: no modification of data transmission control data   Figure 4. CSA analysis of modification of data transmission control data  3.3. The authentication security attribute  Authentication requires that a trusted user has been bound to the behavior That is, the system will only allow the requested program to be executed if the user has previously been determined to be a trusted user. To verify authentication, one must examine the net effects on the control data related to authentication: verify the data that provides evidence that the binding took place, and verify that this evidence data was not changed before completion of any operation that required authentication. The requirements for authentication are as follows   A trusted data transmission mechanism is always used for every data transmission   A trusted identification mechanism is always used to provide proof of a user s identification Note that the user to be identified may be a person, a process, a program, or other entity   A trusted binding mechanism is always used to bind user data \(user identification, password, or other information to confirm the identification and the system information that provides the proof of the identification\ to an execution environment   No other mechanism outside the trusted mechanisms sets the value of any of the control data that indicates whether each of the trusted mechanisms executed correctly and that indicates the status of the bound data   If any of the above requirements or mechanisms fails, authentication fails The mechanism for using CSA to determine whether the data transmission is trusted was discussed earlier. The application of the CSA approach for analysis of the user identification mechanism and the user binding mechanism proceeds along the same lines as Figures 3 and 4 to determine whether each of these mechanisms is trusted  3.4. The authorization security attribute  Authorization requires that a user has the right to do the requested process To verify that an authorized operation took place, one must examine the net effects on the control data to verify that it provides evidence that authorization occurred before the operation, and that the evidence data for the authorization was not changed before that operation completed. The requirements for authorization are as follows   A trusted authentication mechanism \(subsection 3.3\ always used to authenticate the user.  Note that this requirement includes a requirement for trusted data transmission and authentication   A trusted lookup mechanism is always used to determine that the user has the right to complete the specified request   No other mechanism outside the trusted mechanisms sets the value of any of the control data that indicates whether each of the trusted mechanisms executed correctly and that identifies the authorized user and the scope of the authorization   If any of the above requirements or mechanisms fails, authentication fails Analysis of the authentication mechanism was discussed in the previous subsection. Analysis of the Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


  lookup mechanism applies the CSA approach by proceeding along the same lines as Figures 3 and 4 to determine whether this mechanism is trusted  3.5. The non-repudiation security attribute  Non-repudiation of data transmission requires that neither the sender nor the recipient of the data can later refute their participation in the transaction Non-repudiation of changes to a dataset requires that the means for authentication of changes cannot later be refuted For the purposes of this discussion we treat data change as a special case of data transmission, where receipt of the data transmission includes making and logging the requested change to the dataset. To verify non-repudiation, one must examine the net effects on the control data related to non-repudiation. The requirements for every data transmission that is subject to non-repudiation are as follows   Trusted authorization \(subsection 3.3\ always used for sender, receiver, and the scope of any data changed or transmitted. Note that this requirement includes a requirement for trusted data transmission, trusted authentication of users and trusted authorization of users and processes for the specific data scope   Trusted binding is used to bind the sender to the data sent and to bind the receiver to data received   The authorization, binding, and data transmission are handled as a single atomic operation within the boundary of the authorized secure process   A trusted mechanism is always used to provide traceability and audit. This trusted mechanism ensures data persistence of the audit data so the means of authentication and the data transmission cannot later be refuted   Every data transmission is preceded by an absolute definition of the data and identification that binds the data to the sender   Every data receipt is preceded by an absolute definition of the data and identification that binds the data to the recipient   No other mechanism outside the trusted mechanisms sets the value of any of the control data that indicates whether each of the trusted mechanisms executed correctly or the value of any of the control data generated by the trusted mechanisms   If any of the above requirements or mechanisms fails, non-repudiation fails Analysis of the authorization mechanism was discussed in the previous subsection. Analysis of the binding mechanism and the traceability and audit mechanism applies the CSA approach by proceeding along the same lines as Figures 3 and 4 to determine whether each of these mechanisms is trusted  3.6. The confidentiality security attribute  Confidential data access or confidential data transmission requires that unauthorized disclosure of one or more specific data items will not occur  Confidentiality is often described in terms of a security policy that specifies the required strength of the mechanisms that ensure that the data cannot be accessed outside the system. For example, the security policy may require verification that approved encryption mechanisms are used for the output. To verify confidentiality, one must examine the net effects on the control data related to confidentiality The requirements for confidentiality are as follows   A trusted non-repudiation mechanism subsection 3.5\ is always used to process requests for confidential data access and confidential data transmission.  Note that this requirement includes a requirement for trusted data transmission, trusted authentication of users and processes, and trusted authorization of users and processes for the particular data scope   A trusted mechanism is always used to ensure that the data cannot be read outside the system   No other mechanism outside the trusted mechanisms sets the value of any of the control data that indicates whether each of the trusted mechanisms executed correctly or the value of any of the control data set by the trusted mechanisms   If any of the above requirements or mechanisms fails, the request for confidential data access or confidential data transmission fails Analysis of the non-repudiation mechanism was discussed in the previous subsection. Analysis of the data access mechanism applies the CSA approach by proceeding along the same lines as Figures 3 and 4 to determine whether this mechanism is trusted  3.7. The privacy security attribute  Privacy requires that an individual has defined control over how his/her information will be disclosed. To verify privacy, one must examine the net effects on the control data related to privacy The requirements for privacy are as follows Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


    A trusted confidentiality mechanism subsection 3.6\ is always used for all accesses of a user s personal information Note that this requirement includes a requirement for trusted data transmission trusted authentication of users and processes, trusted authorization of users and processes for the specific data scope, and trusted non-repudiation for access to a user s personal information   All access to a user s personal information satisfies an existing privacy and confidentiality policy that includes control data that defines the scope of access for each user   A trusted non-repudiation mechanism subsection 3.4\ is used for all changes to the control data that defines the scope of access for each user. Note that this requirement includes a requirement for trusted data transmission, trusted authentication of users and processes and scope of data   No other mechanism outside the trusted mechanisms sets the value of any of the control data that indicates whether each of the trusted mechanisms executed correctly or the value of any data set by the trusted mechanisms   If any of the above requirements or mechanisms fails, the request for confidential data access or confidential data transmission fails Analysis of the confidentiality and nonrepudiation mechanisms was discussed in previous subsections. Analysis of the access mechanism applies the CSA approach by proceeding along the same lines as Figures 3 and 4 to determine whether this mechanism is trusted  3.8 The integrity security attribute  Integrity requires that authorized changes are allowed, changes must be detected and tracked, and changes must be limited to a specific scope. Integrity is defined as a property of the object, not of a mission. To verify integrity, one must examine the net effects on the control data related to integrity. That is, one must be able to: isolate the object, isolate all the behaviors that can modi fy the object, detect any modifications to the data, and ensure that all transformations of the data across the object are within the pre-defined allowable subset The requirements for integrity are as follows   A security policy exists that describes the scope of allowed changes as an invariance function: certain data transformations must hold; others must never hold   If the security policy data is changed to remove any element from the allowable subset, integrity of the data fails   A trusted non-repudiation mechanism subsection 3.4\ always used for changes to data and changes to policy to ensure that all changes to the security policy and changes to data are performed using a trusted non-repudiation mechanism. Note that this requirement includes a requirement for trusted data transmission, trusted authentication of users and processes trusted authorization of users and processes for the specific data scope, and trusted nonrepudiation for changes to the data. Every authorization for data changes must be restricted to the allowable subset as defined in the security policy   No other mechanism outside the trusted mechanisms sets the value of any of the control data that indicates whether each of the trusted mechanisms executed correctly or the value of the control data set by any of the trusted mechanisms   If any of the above requirements or mechanisms fails, integrity of the data fails Analysis of the non-repudiation mechanism was discussed in a previous subsection. Analysis of the data change mechanism applies the CSA approach by proceeding along the same lines as Figures 3 and 4 to determine whether this mechanism is trusted  3.9. The availability security attribute Availability requires that a resource is usable during a given time period, despite attacks or failures. To verify availability, one must examine the net effects on the control data related to availability  To avoid having to consider temporal properties, one can specify non-availability rather than availability i.e., specify under what conditions the program s behavior database do not apply.\veness properties are translated to the behavioral characteristics that are evident when the system is actually available Specific behaviors associated with non-availability due to denial of service must also be specified  4. Miniature illustration of CSA with FX  Consider the screen image of behavior generated by the CERT  Function Extraction for Malicious Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


  Code \(FX/MC\pe, as shown in Figure 5. This system is under development by CERT  to compute the behavior of malicious code expressed in Intel assembly language. FX/MC produces the net behavior of programs in terms of disjoint conditional concurrent assignment statements. That is, each case of behavior is expressed by a condition and a set of non-procedural assignments that define the final values of registers, flags, and memory locations on exit from an assembly language program This notional example illustrates application of the CSA approach to determine whether a security requirement is satisfied. Suppose that the specification of requirements for a security attribute includes a constraint that the value of the second argument to each call to function XXX must not be equal to 4. A constraint such as this, expressed in terms of concrete data operations and values, could be part of the requirements specification for any of the security attributes discussed above   A B A B  Figure 5 Computed behavior  Imagine that the behavior of Figure 5 defines the behavior of a fragment of code that executes immediately before the program calls function XXX with the second argument equal to the value stored in register EAX. The computed behavior highlighted as section A is the behavior with respect to register values for the condition 1 & EAX\ == 0 his condition means if the value of register EAX at the beginning of this fragment of code is even As shown on the figure, if this condition is true, the value of register EAX after executing this fragment of code is equal to the initial value of register ECX and therefore the value of the second argument to function XXX will be the initial value of register ECX. In contrast, the computed behavior highlighted as section B is the behavior with respect to register values for the condition 1 & EAX\ != 0 his condition means if the initial value of register EAX is odd As shown on the figure, if this condition is true, the value of register EAX is unchanged and therefore the value of the second argument to function XXX will be the initial value of register EAX. Thus, the security constraint is not satisfied because, under either of these conditions, it is not possible to certify that the value of the second argument will never be equal to 4 Note that, using FX technology, this analysis can take place in seconds through computational automation, thereby eliminating the need to study and understand the code by manual means  5. Implications  The behavioral requirements for security attributes provided in the previous section are consistent with the general descriptions of the eight security dimensions \(access control, authentication non-repudiation, data confidentiality, communication flow security, data integrity, availability, and privacy\ contained in standard ISO/IEDC 180282 Information Technology - Security Techniques - IT Network Security - Part 2: Network Security Architecture  d orig i n a ll y prov ided in t h e Bell  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


  Labs Security Framework [6  W e e x p e c t t h a t t h e CSA approach to security attribute analysis will be very supportive of work to address the challenges of security management and approaches to secure an enterprise and regulatory mandates \(Discussions of the challenges of security management, are provided in  Advantages of the CSA approach include   A rigorous method is used to specify security attributes in terms of the actual behavior of code and to verify that the code being evaluated is correct with respect to security attributes   The specified security behaviors can provide requirements for the security architecture   Traceability capabilities can be defined and verified outside of the code being evaluated   Vulnerabilities can be well understood, making it easier to address evolution of code, environment use, and users CSA technology can address specification of security attributes of systems before they are built specification and evaluation of security attributes of acquired software, verification of the as-built security attributes of systems, and real-time evaluation of security attributes during system operation It should also be possibl e to verify that the behavioral characteristics relating to security properties are consistent with a global security policy e.g., running the code in the context of a system with an expressed security policy\. The use of a system such as FX to perform this level of validation would require a behavioral specification of the global security policy in terms of system behavior, which could then be automatically checked against program behavior. As behavioral patterns scale through different layers of abstraction, high-level security behaviors can be expressed as a straight-forward composition of program behavior  6. Open Questions  Behavioral requirements for concurrency and parallel system issues must be addressed. In addition because computational security attributes and function extraction are emerging technologies, there has been limited experience in applying the technology to large systems. The computational effort involved in analyzing large, complex components and in analyzing large numbers of component compositions to yield a system security specification remains to be studied. However, it is clear that the CSA approach can be effective in addressing state space explosion while yielding complete, correct answers 7. Acknowledgement  CERT is a registered trademark and service mark of Carnegie Mellon University  8. References  1 R.W Co ll in s, A  R. Hev n er G  H. W a lto n  R.C. L i n g er The impacts of function extraction technology on program comprehension: A controlled experiment  Information and Software Technology 50\(2008\ 1165-1179.  Elsevier, 2008  2 A  K   G upta  U  Cha n d r a s he k h a r S. Sa bnis  a nd F  A   Bastry Building Secure Products and Solutions Bell Labs Technical Journal, 12\(3\23-38. Wiley Periodicals Inc. 2007  3 Inte rna ti ona l Org a niz a tion f o r Sta nda rdiz a tion a n d  International Electrotechnical Commission Information Technology - Security Techniques - IT Network Security Part 2: Network Security Architecture, ISO/IEC 18028-2  Feb. 2006   R L i n g er M   P l eszko c h   L  Bu rn s A  Hevn er  G   Walton Next generation software engineering: Function extraction for the computation of software behavior Proceedings of the 40 th Annual Hawaii International Conference on System Sciences \(HICSS40 Hawaii. IEEE Computer Society Press, Los Alamitos, CA 2007  5 R  L i ng e r S. P r ow e ll, R  B a rthol om ew  L  B u rns  T   Daly Funcation extraction: automated behavior computation for aerospace software verification and certification  Proceedings of the American Institute of Aeronautics and Astronautics \(AIAA\otech & Aerospace 2007 Conference California. IEEE Computer Society Press, Los Alamitos, CA.  May 2007  6 A  R. Mc G e e, S.R. V a sired d y C. X i e, D.D  Picklesimer, U. Chandrashekhar, and S.H. Richman A Framework for Ensuring Network Security  Bell Labs Technical Journal 8\(4\ges 7-27. Wiley Periodicals, Inc 2004  7 M Ple s z k oc h a nd R  L i ng e r  Improving Network System Security with Function Extraction Technology for Automated Calculation of Program Behavior Proceedings of 37th Hawaii International Conference on System Sciences Hawaii, January, 2004, IEEE Computer Society Press, Los Alamitos, CA, 2004   S  S a bn i s  U Ch and r ash e kh ar an d F  Bast ry   Challenges of Securing an Enterprise and Meeting Regulatory Mandates P roceedings of the 12 th  International Telecommunications Network Strategy and Planning Symposium, 2006. NETWORKS 2006 Nov. 2006 pages 1-6. IEEE Computer Society Press, Los Alamitos CA, 2002  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


selection is dominated by O   C k  2   Finally the space requirement is O  MAX  C k  2    Since the size of a community is small both the time and space are also small VII E XPERIMENTAL R ESULTS We now evaluate the performance of our algorithm for object connection discovery We run all experiments on an AMD Opteron 248 with 1GB RAM running Linux 64-bit We use the DBLP co-authorship dataset modeled as a graph The graph has approximate ly 316K nodes and 1,834K edges where a node represents an author and the edge weight is the number of papers co-authored between two authors A Performance of Community Partition We rst evaluate the performance of community partition by our greedy algorithm We test three settings of c local 10 50 and 100 and four settings of c global 10 100 1000 10000 We also compare with Newman’s algorithm 13 Figure 11\(a shows that our algorithm is about an order of magnitude faster than Newman’s when c local 10 Theresult also shows that when c local increases the efﬁciency decreases which demonstrates the effectiveness of Heuristic 1 10 100 1000 10000 10 2 10 3 10 4 c g lobal Running Time \(sec c local 10 c local 50 c local 100 Newman a Running Time 10 100 1000 10000 0 50 100 150 200 250 300 c g lobal Peak Memory Consumption \(MB c local 10 c local 50 c local 100 Newman b Memory Consumption Fig 11 Performance of Community Partition For the effect of Heuristic 2 i.e c global  the performance is the best when c global  1000 When c local 10  the running time is 682 613 570 and 667 seconds for c global  10 100 1000 and 10000 respectively The result can be explained as follows When c global is too small many items are not kept in the global max-heap and hen ce we need to rebuild the heap more often When c global is too large there are too many items in the heap and hence the update of the heap takes longer Figure 11\(b shows that the peak memory consumption of our algorithm increases when c local increases since the size of the local max-heaps increases when c local increases Increasing c global  i.e the size of the global heap from 10 to 10000 only increases the memory usage for less than 1 MB since we have only one global heap However in all cases our algorithm consumes considerably less memory than Newman’s We also record that the value of the modularity of the optimal community partition obtained by the greedy algorithm is 0.71 note that all the algorithms compute the same partition According to Newman 13 a m odul ari t y v a l u e o f g reat er t h an 0.3 indicates a signiﬁcant comm unity structure Therefore 0.71 is a very high value of modularity and indicates a highquality community partition B Semantics of Answer Graph A Case Study We conduct a case study to compare our answer graph with the center-piece subgraph  CEPS  10 Thi s s t udy ai ms to rst provide a more intuitive view on the answer graphs obtained by our algorithm and CEPS Then we perform a more systematic comparison in the following subsection We use the query  Jim Gray  Jennifer Widom  Michael I Jordan  Geoffrey E Hinton   The four scholars are from two different communities Gray and Widom are from the database community while Jordan and Hinton are from the machine learning community This is clearly captured by our answer graph as shown in Figure 1 which is displayed in Section I Figure 12 shows the answer graph of CEPS which is very similar to our answer graph The similarity is because both our algorithm and CEPS nd nodes that are closely related to the query nodes in order to connect them Thus the result shows that both algorithms are able to capture important nodes and paths related to the query nodes However our method not only nds a good connection between all query nodes but also for query nodes that are in the same context we put more emphasis on their connection than the existing methods Compare Figure 1 with Figure 12 we clearly see a stronger connection between Gray and Widom through both Ceri and Hellerstein in our answer graph than CEPS Michael I Jordan Jim Gray Alexander Aiken 1 3 Jennifer Widom Michael Stonebraker Tommi Jaakkola Lawrence K. Saul 3 Zoubin Ghahramani 9 Geoffrey E. Hinton 7 1 2 5 11 2 1 9 1 1 5 3 Joseph M Hellerstein Fig 12 The Answer Graph of CEPS Although the quality of the answer graphs is comparable our algorithm signiﬁcantly outperforms CEPS we take only 0.33 seconds to compute our answer graph while the computation of the CEPS takes 925 seconds We further compare the two methods using more systematic measures as follows C Performance of Object Connection Discovery We compare the performance of our algorithm PCquery with CEPS 10  W e s et t h e b udget t o b e t w i ce of t h e query size We also verify that the answer graphs obtained by PCquery and CEPS are of roughly the same size Other settings of CEPS are as its default We generate two types of queries in-community queries and random queries  which are abbreviated as cq and rq in the gures For in-community queries the nodes in a query are randomly selected from a randomly selected community For random queries the nodes in a query are randomly selected from the set of all nodes in the dataset We generate 100 queries for each type and test the query size from 2 nodes to 20 nodes Figure 13\(a reports the average running time of nding the connection for a query The result shows that PCquery is more 
866 
866 
 


than three orders of magnitude faster than CEPS for both query types We nd that PCquery takes more time to process an incommunity query than a random query This is because the computation of the intra-community connection by dynamic programming is more costly than that of the inter-community connection by tracing the c ommunity hierarchy tree 2 4 8 12 16 20 10 2 10 0 10 2 10 4 Quer y Size \(number of nodes Average Response Time \(sec PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq a Average Response Time 2 4 8 12 16 20 0 100 200 300 400 500 600 700 Quer y Size \(number of nodes Peak Memory Consumption \(MB PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq b Memory Consumption Fig 13 Efﬁciency of PCquery and CEPS Figure 13\(b reports the peak memory consumption during the entire running process The result shows that PCquery also consumes signiﬁcantly less memory than CEPS in all cases In addition to the comparison on efﬁciency we also compare the quality of the answer graphs obtained by PCquery and CEPS For the fairness of comparison We use the quality metrics proposed in CEPS 10  NRatio and ERatio which indicate the percentage of important nodes and edges that are captured by an answer graph respectively We report the result in Figure 14 2 4 8 12 16 20 0 20 40 60 80 100 Quer y Size \(number of nodes NRatio PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq a Average NRatio 2 4 8 12 16 20 0 20 40 60 80 100 Quer y Size \(number of nodes Eratio PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq b Average ERatio Fig 14 Quality of Answer Graph Figure 14 shows that both PCquery and CEPS obtain high-quality answer graphs Both NRatio and ERatio of our answer graphs are comparable to those of CEPS although on average those of CEPS are slightly better Considering our algorithm is three orders of magnitude faster and also consumes signiﬁcantly less memory we can conclude that our method is both efﬁcient and effective VIII C ONCLUSIONS We propose context-aware object connection discovery in a large graph We adopt a partition-and-conquer approach to achieve both high performance efﬁciency and high quality results Our method rst partitions a large graph into a set of communities The concept of community not only naturally deﬁnes the context of the nodes but also signiﬁcantly improves the efﬁciency of connection d iscovery since a community is much smaller than the original graph We compute the connection between query nodes rst at the intra-community level by maximizing the information throughput of the nodes and the information ow of the paths in the answer graph and then at the inter-community level by retaining the close relation between the commun ities as deﬁned by modularity The quality of both the intraand intercommunity connection is thus controlled by the integration of information throughput/ﬂow and modularity We verify by experiments that our community partition algorithm is efﬁcient and the set of communities obtained has high quality We also show that our method obtains comparable high-quality answers as the state-of-the-art algorithm but is more than three orders of magnitude faster and consumes signiﬁcantly less memory Acknowledgement This work is partially supported by RGC GRF under grant number CUHK419008 and HKUST617808 We thank Mr Hanghang Tong and Prof Christos Faloutsos for providing us the source code of CEPS R EFERENCES  X  Y an P  S  Y u and J  H an  Graph i nde xing bas e d o n d is crim inati v e frequent structure analysis ACM TODS  vol 30 pp 960–993 2005  J  C he ng Y  K e  W  N g a n d A  L u  F g-i nde x t o w a rds v e r i  c a t i on-fre e query processing on graph databases in SIGMOD  2007 pp 857–872  P  Z hao J  X Y u  a nd P  S Y u   Graph i nde xing T r ee  d elta   graph in VLDB  2007 pp 938–949 4 Y  K e J  Cheng and W  N g Cor r e lation s ear ch in gr aph d atabas es   in KDD  2007 pp 390–399 5 Y  K e J  Cheng and W  N g E f  cient c or r e lation s ear ch f r o m g r a ph databases To appear in TKDE  2008  A  I nokuchi T  W as hio and H  M ot oda An apriori-based algorithm for mining frequent substruc tures from graph data in PKDD  2000 pp 13–23  X  Y an and J  H an  Clos e g raph m i ni ng closed frequent graph patterns in KDD  2003 pp 286–295  J  H uan W  W a ng J  Prins  and J  Y ang Spin mining maximal frequent subgraphs from graph databases in KDD  2004 pp 581–586 9 C  F alouts o s  K  S  M c Cur l e y  a nd A  T o m k ins  F as t d is co v e r y of connection subgraphs in KDD  2004 pp 118–127  H  T ong and C  F alouts o s  Center piece s ubgraphs  p roblem de n ition and fast solutions in KDD  2006 pp 404–413  Y  K o ren S C North and C  V olins k y  Meas uring a nd e x tracting proximity in networks in KDD  2006 pp 245–255  M E  J  Ne wm an and M  G irv a n Finding and e v a luating c om m unity structure in networks Physical Review E  vol 69 p 066113 2004  M E  J  Ne wm an  F a s t algorithm f or detecting c om m unity s t ructure i n networks Physical Review E  vol 69 p 066133 2004  F  W u and B  A  H uber m an  F i nding com m unities i n linear tim e a physics approach The European Physical Journal B Condensed Matter and Complex Systems  vol 38 no 2 pp 331–338 2004  R K u m a r  P  Ragha v a n S Rajagopalan and A  T om kins   T r a w ling the web for emerging cyber-communities Comput Netw  vol 31 no 11-16 pp 1481–1493 1999  R K u m a r  P  Ragha v a n S Rajagopa lan and A Tomkins Extracting large-scale knowledge bases from the web in VLDB  1999 pp 639 650  R K u m a r  U Mahade v a n and D  S i v akum ar   A g raph-theoretic approach to extract storylines from search results in KDD  2004 pp 216–225  D Gibs on R K u m a r  and A  T om kins   Dis c o v e ring lar g e d ens e subgraphs in massive graphs in VLDB  2005 pp 721–732  Y  Douris boure F  Geraci a nd M Pe llegrini Extraction and classiﬁcation of dense communities in the web in WWW  2007 pp 461–470  A Claus e t M E  J  Ne wm an a nd C Moore Finding com m unity structure in very large networks Physical Review E  vol 70 p 066111 2004 
867 
867 
 


  13 false \(double\argets Figure 12 illustrates this situation. The conditional update correctly updates the tracker wh ich is tasked with following the target. However, the detector which is partially spatially coincident with the tracker also receives energy from the conditional update. This can lead the detector to initiate falsely\second target nearby the first target This effect can be countered a number of ways. First, we can adjust the speed at which the tracker re-centers itself The double initialization phenomenon occurs when the PDF peaks near the edge of the tracker grid. However, this method has the side effect of potentially allowing probability to fall off of the grid in low SNR environments causing track loss. Of course if the SNR is low enough or measurement outages occur tracks will be dropped. Second a guardband around the tracker that does not allow any detector sufficiently near the tracker to receive reinforcement via the conditional density can mitigate the double target problem. However, this has the side effect of preventing detection of closely spaced targets. Third increasing the spatial extent of the tracker has a similar effect as the using a guardband. It does require increased computation, but generates a better representation of the posterior There are several engineering tradeoffs. The first is that large tracker grids \(or large guard bands\ prevent falsely detecting new targets because of conditional probability spill over. However, if applied too aggressively, this will prevent correctly detecting cl osely spaced targets. Second quick tracker grid translation correctly centers the target mass, again preventing spillover into nearby detectors However, overly liberal trac ker repositioning may in fact move trackers to spurious energy locations and drop true targets off of the finite grid On Ambiguous Targets As discussed earlier, ambiguous targets will eventually move non-physically and this will cause the tracker to remove them via its natural prediction and update process Figure 13 illustrates this phenomenon. There are two real targets that create two persistent ambiguities. All four are detected and tracked automati cally. The ambiguous targets however, eventually move non-physically due to their reliance on the node bearing angles. The tracker automatically penalizes the non-physical motion and the targets\222 present hypothesis decrease quickly over time Ambiguous target removal is done automatically in the Bayesian framework as follows The PDF on target state is predicted forward in time according to the kinematic model True targets will have behavior consistent with the kinematic model \(note the kinematic model is a statistical model so it is predicting a range of possibilities for the future target state\biguous targets may behave consistently with this model for a period of time, but eventually they will appear to perform a non-physical maneuver \(these epochs typically come when the ambiguous target crosses a line of symmetry in the sensor\this point, the predicted target position will be in strong disagreement with the inco ming measurements on that target. This mismatch in predicted target position and measurements leads to a decr ease in the target present hypothesis as calculated in eq. \(4\long, only true targets remain   Figure 12 \226 Improper selection of grid resolution leads to multiple initializations on the same target. Left Measurement update of a Tracker \(red=highest likelih ood, blue=lowest\Right Measurement update of a detector which lies near the Tracker.  Since the track er size has been improperly chosen, some energy from the measurements of a single target leak s on to the detector. This can le ad to false double-initializations 


  14 8  C ONCLUSION  This paper has described a Bayesian approach to detecting and tracking multiple moving targets using acoustic data from multiple passive arrays In contrast to traditional undersea acoustic systems, which develop tracks at the single array level and require track association, our approach fuses data at the m easurement level and operates directly in the target state space We have detailed a well known nonlinear filtering approach to single target detection and tracking [1, 4 and desc ri be d our computationally efficient finite-grid approach to the required density estimation. We have furthermore extended this to the multiple target case by employing a bank of single target detector tracke rs and approximation methods that adjust for closely spaced targets. This approximate approach avoids fully treating the computationally complex joint multitarget problem Future work includes modified approaches to posterior estimation including dynamic grid extent, dynamic grid resolution, and particle filtering. It is anticipated that adaptive sampling of the posterior will lead to computational savings. Furthermore, future work includes more detailed modeling and estimation of closely spaced targets allowing a more accurate representation of the joint target density. Naively implemented, this implies exponential growth \(in the number of targets\r the probability state space being es timated. However, recent work in a related tracking domain on adaptive density factorization [5 c h a stic sa m p lin g  p article filtering    pr ovi de m e t h o d s t h at m i t i g at e t h i s com put at i o n gr owt h  when the full joint density is treated  A PPENDIX  This section discusses the details of how the single target probability density is time evolved on a discrete grid. This discussion is similar to that found elsewhere [15, 14, 13 We wish to compute the single target probability density at time      from the density at time     The relation between these two densities can be expressed using the law of total probability as                We expand     using a second order Taylor series as               where  is the vector of partial derivatives, i.e and is the matrix of second order partial derivatives Then the relation of \(23\ approximated as   Figure 13\226 Left: P h1 over time for four targets, two of which are real and two of which are ambiguous. Although the ambiguous intersections are persistent, eventually the false targets ha ve non-physical motion. The target present hypothesis quickly goes to zero for these targets and they are elimin ated. Right: the tracker estimate of target position and red circles indicating the removal point fo r the false targets 


  15             Where denotes the expectation with respect to the transition distribution    and the omitted terms involve similar terms involving and and cross terms between the and coordinates We use the nearly constant velocity \(NCV\model to specify the transition distribution    This assumption corresponds to one where the target moves at constant velocity except for random jump changes \(i.e nearly constant velocity\is is a plausible model when  is small as it is here Specifically, the NCV model assumes step changes in target velocity defined by the Ito Equations     This model implies  and likewise for  It is furthermore assumed that th e noise processes in each coordinate are independent Under this model, we can eval uate the required terms from 25\ as follows          And likewise for terms involving and Notice that all cross terms \(e.g  have expectation due to the assumption that the noise process is independent in the two coordinates This model simplifies \(25\ to         where the terms omitted are replicas involving the  coordinate Under the assumption that is small, this can be rewritten as    For implementation, this is approximated using an implicit Euler scheme wh ere      Where the indices  represent the discrete    locations where the probability mass is captured Likewise, using forward differencing      and          and similarly for the y coordinate system When substituted into \(28\is leads to a series of equations of the form                This series of equations defi ne the probability at each point at time  It can be efficiently solved via Thomas\222 algorithm \(rather than simply inverted\he matrix is tridiagonal     


  16 R EFERENCES    R o y E. Bet h el Benjam i n Shapo, C h r i st opher M   Kreucher, \223PDF Detection and Tracking\224, under review IEEE Transactions on Aerospace and Electronic Systems  2 y. E B eth e l an d G. J. Paras, \223A PDF Mu lt it arg et Tracker\224 IEEE Transactions on Aerospace and Electronic Systems vol. 30, no. 2, pp. 386-403, April 1994 3   R o y E  B e t h e l a n d G  J  P a r a s  223 A P D F M u l t i s e n s o r  Multitarget Tracker\224 IEEE Transactions on Aerospace and Electronic Systems vol. 34, no. 1, pp. 153-168 January 1998  L   D   Stone, C. A. Bar l ow, and T. L Corwin, \223Bayesian  Multiple Target Tracking\224  Boston: Artech House, 1999  l la and A. Hero, \223Multitarget Tracking using the Joint Multitarget Probability Density\224 IEEE Transactions on Aerosp ace and Electronic Systems  vol. 41, no. 4, pp. 1396-1414, October 2005  M  M o relande, C. Kreucher, K. Kastella, \223A Bay e sian  Approach to Multiple Target Detection and Tracking\224 IEEE Transactions on Signal Processing vol. 55, no. 5 pp. 1589-1604, May 2007  B  Shapo, and R  E B e t h el  223An Overvi ew of t h e Probability Density Function \(PDF\er\224 Oceans 2006 Boston, Sept. 2006  R oy L. St r e it 223M ult i s ensor M ul tit arget Int e nsit y Fil t er 224  International Conference on Information Fusion  Cologne, Germany July 2008  M  Ort on and W Fi t z geral d 223A B a y e si an approach t o  tracking multiple targets using sensor arrays and particle filters\224 IEEE Transactions on Signal Processing, vol. 50 no. 2, pages 216-223, Feb 2002  A. Doucet B Vo, C Andri e u, and M Davy 223Par t i c le filtering for multi-target tracking and sensor management\224, IEEE International Conference on Information Fusion, 2002  H Van T r ees, \223Det ecti o n Est i m a t i on, and M odul at i o n  Theory IV:  Optimum Array Processing\224  J. C St ri k w erda, Fi nit e  Di fference Sch e m e s and Partial Differential Equations, Ch apman & Hall, New York 1989   K. Kast el la and C Kreucher, \223M ult i p l e  M odel Nonl i n ear  Filtering for Low Signal Ground Target Applications\224 IEEE Transactions on Aerospace and Electronic Systems vol. 41, no. 2, April 2005, pp. 549-564  Z. Tang and \334. \326zg\374n er, \223Sensor Fu si on for Target Track Maintenance with Multiple UAVs based on Bayesian Filtering Method and Hospitability Map\224 Proceedings of the 42 nd IEEE Conference on Decision and Control pages 19-24, December 2003  K. Kast ell a 223Fi n it e di ff erence m e t hods for no nl i n ear filtering and automatic target recognition\224 MultitargetMultisensor Tracking: Applications and Advances vol III, pages 233-258, Artech House, 2000 B IOGRAPHY  Chris Kreucher received his Ph.D. in Electrical Engineering from the University of Michigan in 2005. He is currently a Senior Systems Engineer at Integrity Applications Incorporated in Ann Arbor, Michigan. From 1998 to 2007, he was a Staff Scientist at General Dynamics Advanced Information Systems' Michigan Research & Development Facility \(formerly ERIM\. His current research interests include nonlinear filtering \(specifically particle filtering Bayesian methods of multitarget tracking, self localization information theoretic sensor management, and distributed swarm management Ben Shapo earned his Ph.D. in El ectrical Engineering in 1996 from the University of Michigan.  He is currently a Senior Systems Engineer at Integrity Applications Incorporated in Ann Arbor, Michigan.  From 2003 to 2008 he was a Lead Engineer at General Dynamics, where he contributed to a number of RF and acoustics signal processing and tracking efforts.  Dr. Shapo has 12 years experience in the DoD research community in the areas of detection, tracking, and data fusion, with emphasis on highfidelity simulations and applying new methods to real data  Dr. Roy Bethel is currently employed at The MITRE Corporation in McLean, VA. He has been actively involved in development, testing, and evaluation of signal processing and detection and tracking systems. In particular, he has developed many systems that have been implemented on United States Navy airborne, surface, and submerged platforms. He is currently engaged in research and development of innovative approaches to multitarget detection and tracking  A CKNOWLEDGEMENTS  This work was partially funded by the Office of Naval Research contract N00014-08-C-0275. The authors would like to thank Dr. John Tague for his support, and Mr. Scott Spencer and Dr. Charles Choi for their assistance 


2 1 0 00                4 G ro w th 1  1 3 1 0 9 2 0 2 3 0 10  0 56                So ci ode m og ra ph ic c ha ra ct er is tic s 


s 5 A ge y ea rs   21 7 8 7 3 9 0 01 0 22  0 1 4 0 0 8              6 G en de r i s fe m al e2   0 2 4  0 0 6 0 0 2 0 00 0 0 3 0 10    


           7 C ur re nt ly n ot w or ki ng 2  0 0 5  0 0 8 0 04 0 04 0 0 1 0 16  0 16             8 C ur re nt ly in e du ca tio n2   0 6 


6 7  0 01 0 1 9 0 08  0 03 0 6 8 0 0 7 0 3 2           9 C ur re nt ly w or ki ng 2  0 2 8  0 03 0 18  0 1 1 0 0 3 0 64  0 00 0 1 4 0 8 9   


        10 E du ca tio n ac hi ev ed 3  3 5 7 1 5 2  0 04 0 02 0 2 1 0 1 2 0 16  0 02 0 1 6 0 13  0 0 6         11 D is pe ns ab le in co m e   


  21 0 9 2 72 7  0 14  0 0 1 0 09  0 08  0 2 0 0 00 0 0 4 0 18  0 1 6 0 0 1        In te rn et u sa ge                     


  12 A ct iv e in te rn et u sa ge 1  0 0 2 0 9 6 0 2 1 0 25  0 11  0 12  0 10  0 0 4 0 05  0 0 8 0 0 5 0 0 1 0 12        13 H ou rs o nl in e h ou rs 


rs   2 6 5 3 0 3  0 04 0 12  0 1 1 0 0 3 0 40  0 0 7 0 0 7 0 4 7 0 5 3 0 07  0 1 1 0 07       14 W illi ng ne ss to p ay 1  1 8 3 0 6 3  0 03 0 10 


10  0 07  0 08  0 0 2 0 0 4 0 0 1 0 01  0 00 0 0 5 0 14  0 04 0 05      G am e sp ec ifi c va ria bl es                      15 T en 


ur e w ee ks   2 8 2 3 5 2 0 2 6 0 31  0 0 9 0 01 0 12  0 0 4 0 02 0 0 9 0 0 9 0 07  0 02 0 13  0 08  0 0 4    16 C ro ss o ve r on o ffl in e 4  0 1 5 


5 1 1 1 0 1 9 0 11  0 13  0 18  0 2 0 0 1 4 0 0 7 0 14  0 1 1 0 0 4 0 08  0 15  0 0 5 0 01 0 07    17 S at is fa ct io n1   18 7 5 1 3 16  0 18  0 00 


00 0 44  0 52  0 1 4 0 0 3 0 02 0 07  0 0 9 0 1 4 0 10  0 08  0 0 6 0 09  0 0 1 0 13   18 C om m itm en t1  0 6 2 0 8 3 0 3 1 0 13  0 37  0 39  0 0 7 


7 0 0 6 0 02 0 03  0 0 4 0 1 3 0 14  0 17  0 0 5 0 09  0 07  0 19  0 58  S ou rc e O w n ca lc ul at io n N ot e N  1 3 89 o bs er va tio ns S ig ni fic an ce le ve ls 


ls  p  0 05 S D  S ta nd ar d de vi at io n 1 5 po in t L ik er t s ca le ra ng in g fro m 2 to 2  2 du m m y va ria bl e 3 o rd in al v ar ia bl e ra ng in g fro m v oc at io na l e du ca 


tio n to P h D 4 n um be r o f c on ta ct s   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


