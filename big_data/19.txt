Decisianmaking with Fuzzy Lily R Liang Dept of Computer Science University of the District of Columbia Washington DC 20008 USA LLiang@udc.edu Abstract An outstanding problem is how lo make decisions with uncertain and incomplete data from disparate sources without NP-hard algorithms Here we introduce a new reasoning methodology, fuzzy-belief-state-based reasoning to solve this problem. In this methodology weprst create a fuzzy-beliefstate base for a system from its historical data For any component n n  l,.,,,N of the set of empirical 
state vectors the values 0J that component are clustered into LOW MEDIUM and HIGH fuzzy sets. Then each state vector is fuzzified into a fuzzy-beliefstate vector ofNtriples where the n-th triple contains the fuzzy truths ofmembership ofthe variable value in these respective three Jiuzzy sets Each such vector of N triples is associated with a decision to form afuzzy-belief case andsuch cases comprise afurzy belief-state base. Then when given an observed state vector that is incomplete and uncertain we mine fuzzy association rules from 
the fuzzy-belief-state base and apply them to infer the missing values and their fuzzy beliefs based on that incomplete observation The completed observation is used io match fuzzy-beliefstate vectors in the fuzzy-beliefstate base. Decisions of the best malching cases are retrieved for use as in case-based reasoning 1 Introduction Decisions underlie any action that a problem solver may take in structuring problems reasoning about situations allocating resources, retrieving and displaying information or in controlling physical organizational or political activities \(see 271 Decisions are needed e.g in pattern recognition and classification, diagnosis, prognosis, product design marketing and military 
strategy public policy routing, scheduling and negotiations Given a situation where one of multiple competing choices classes actions strategies targets etc called decisions is to be selected some considerations are i\the completeness of the data upon which the decisionmaking is based ii the level of uncertainty of this data iii the 0-7803-881 9-4/04/$20.00 02004 IEEE Belief-State-Based Reasoning 553 Carl G Looney Dept of Computer Science  Engineering University of Nevada Reno Reno NV 89557 USA looney@cs.unr.edu consistency of the data that may be 
from disparate sources iv\the degree of belief in the decision and VI the optimal cost of the decision The expense of obtaining complete and essentially certain data makes it necessary and important to make decisions based on uncertain and incomplete data Each of the existing tools for decisionmaking under such conditions is either NP-hard its parameters are very difficult and costly to obtain or it lacks the flexibility of completing missing data Bayesian belief networks BBNs  12 241 fizzy belieJnelworks FBNs  171 andfuzzy belief Petri nets FBPNs I 51 also see 
ZO can accommodate uncertainty incompleteness and data from multiple and disparate sources However BBNs are NP-hard 6 71 and the necessary conditional probabilities are difficult or impossible to obtain But all types of these influence networks which originated in 192 1 28 require knowledge of the structure of the influence network Many BBN algorithms exist 22 29 301 Other methodologies use beliefs of some kind but do not work without one or more of certainty completeness and knowledge of the relationships between disparate sources Rule-based sysrems 2 
4 10 II attempt to deal with uncertainty by means of certainty factors but these are neither axiomatic nor consistent. Completeness is required by the theory ofevidence methodology S 261 operations research see 2 I for a connection to BBNs mathematical models and data mining 5 9 141 which associates disparate data in tabular columns Fuzzy logic methods require the completeness of the input data as do genetic algorithms. Rough sets 1231 require certainty as well Neural networks \(see 1191 generally require complete data as 
do pattern recognition and classification  1 SI Case-based reasoning CBR is powerful l 13 16 251 but requires certainty and incompleteness degrades its performance In this paper we introduce a new reasoning methodology that we callfuzzy-beliej*-siare-based reasoning to solve this problem. Section 2 introduces how a fuzzy-belief-state base is created and Section 3 introduces our new fuzzy-belief state-based reasoning approach We applied this approach on real world data and show the results in Section 4 Section 5 discusses conclusions and future directions 


2 Fuzzy-belief-state base creation S 0 Situations for a given system can be described by stute vectors s  sir s2  sN where s n  1  N is a state variable or parameter of the system The state can vary over time as is indicated by s\(t A basic assumption here as in other methodologies is that the system is stationary rather than evolutionary, that is the system at different times has the same relationships among the variables \(the correlations are constant over time For a set of Q state vectors s\(tl  s\(tQ\we consider the set of all Q values S sn\(tq q E Q for a fixed component n Considering the Q vectors as Q rows S is the set of all values in the nth column For each n  I  N we cluster the Q values in S into three clusters and find their Gaussian weighted centers  1 SI This process weights the prototype \(center of each cluster according to the density of the surrounding points and locates the center among the most densely situated points These three clusters of the S values are designated as the fuzzysets LOW,\(LJ MEDIUM,\(MA andHIGH,\(HJ with fuzzy set membership functions centered on their centers Figure 1 shows the components n across the horizontal axis the range of values and cluster centers on the vertical axis and the fuzzy set membership function values coming out of the plane in the third dimension I  I I   I.I........__ _.__ 4  k Figure 1 Fuzzy set membership functions of component n The fuzzy set membership functions FSMFs are Gaussians centered on the cluster centers except for the LOW and HIGH end functions where half Gaussians are used with the other half being a constant of unity The FSMFs for L M and H are where e.g fLn is the fuzzy truth that component value s belongs to fuzzy set L while pLn and uL are the center and the standard deviation for the FSMF for L Thus via fuzzification of each component value s,\(t a state vector s\(t  s!\(t s2\(t  sN\(t is mapped into a fuzzy-beliefistate FBS vector of N triples as shown in Equation 4 For any s there are two non-negligible fuzzy truths of membership called fuzzy beliefs\respectively for two ofthe three fuzzy sets L M and H These two sufficiently describes the fuzzy set memberships of s The lowest fuzzy truth is set to 0 Table 1 displays a tabular base of Q fuzzy-belief-state vectors B\(t with each having a decision adjoined as an extra component Such decisions are to have been previously selected so as to be optimal in some sense The decision column can actually be taken to be any column in the state vector or a new component but either way it represents the output of the decisionmaking process Such a table is called a fuzzy-belief-state base FBSB A row record in the FBSB is a fuzzy-belief-slate cnse Table 1 A fuzzy-belief-state base 21  SN t fLt-fi\\ll&,l  fLN&N&ly Decision t 0.0 0.5 0.8   0.7 0.5 0.0 Decisiond t2 0.3 0.7 0.0   0.0 0.5 0.9 Decision d tp 0.6 0.5 0.0   0.8 0.4 0.0 Decision d      3 Fuzzy-belief-s t ate-b ased reasoning In the real world it is often true that not all of the variables in a state can be observed at a given time t in this case we say the observation data is incompIete due to missing component values Also the present measurements of the observed variables contain noise uncertainty and come from disparate sources such as sensors reports archived data etc From an uncertain and incomplete observed input state vector we propose to use the knowledge that exists within the data in the fuzzy-belief-state base to complete the observation, account for the uncertainty with beliefs and perform a type of reasoning to select a decision as a response to that input The high level ulgorithm The fuzzy-belief-state-based reasoning FBSBR approach assumes no functional relationship between the output decisions and the inputs but uses only knowledge in the FBSB data Given the incomplete and uncertain observation state vector s-\(t we 554 


1 Fuzzification fuzzify the variables that are present to obtain an incomplete FBS vector B-\(t 2 Data Association perform fuzzy association rule mining 3 Belieflnferencing infer beliefs to get the complete the FBS vector B\(t 4 Decision Retrieval retrieve decisions of FBS cases from the FBSB that best match B\(t 5 Decision Adjuszment adjust retrieved decisions and use Figure 2 shows the above steps that make up fuzzy-belief state-based reasoning In the rest of this section we introduce these steps one by one  Fuzxification 1 Decision Adjustment 1 224 I Final Decision Figure 2 FBSBR flowchart Fuzzification For a given incomplete observation s-\(t we first fuzzify it to obtain an incomplete fuzzy-belief-state vector B-\(t The fuzzification process is done for each present value sk\(t by obtaining its fuzzy truths fLk fm and fm by putting it through the three FSMFs Lk Mk and H This step is similar to the fuzzification step in FBSB creation, which we introduced in Section 2 The lowest one of fU fm and fHk is set to 0 Duta ussociation In this step we implement our fuzzy association rule mining from the FBSB Let X be a statement such as skis L and Y be a statement such as sj is Mi The support of X in the FBSB is the set of all cases in that FBSB where X is true Thesupportofa rule ofthe form X  Y is the set of all cases in the FBSB where both X and Y are true i.e the cases that support XuY The rule confidence for X  Y is the ratio of the number of cases that support XuY to the number of cases that support X sometimes expressed as a percentage In our situation here X will be a pair of statements where each has a nonzero fuzzy truth and Y will be a single statement with its fuzzy belief Thus X  Y will have a form such as sk is L fu  sk is M fm  sj is Mj fMj 5 For each sk that is present in the observation we create rules ofthe above format with the pair of statements that are true for sk as X and a statement of each missing sj\(t being either Lj Mj or Hj as the statement in Y To determine the fuzzy truth of sj in a rule we examine all cases in the FBSB that support the statements of that rule and average their fuzzy beliefs for si The rule confidence is computed for each rule The rules and their confidences are to be used in the fuzzy-belief inferencing which is the next step Fuzzy-belief inferencing We now do the fuzzy belief inferencing to estimate each missing value sj\(t All fuzzy rules obtained for sj\(t are applied and each rule votes for its consequent statement Y with its confidence Letting F represent any of Lj Mj and Hj the fuzzy belief fF that sj\(t belongs to F is r=l,R\(F cr fFr fF  6 C r=l,R\(F r where R\(F  number of rules with statement Y  sj\(r is F fFr is the fF in Rule r c is the confidence of Rule r Only the two highest fF values for sj\(t are kept and the other one is set to 0 Thus we account for the missing datum si in s with a triple of fuzzy beliefs for B-\(t Each missing component of s is accounted for in this way to complete the fuzzy-belief-state vector B\(t Decision retrieval Next we measure the similarity of the completed B\(t to each FBS vector in the FBSB The similarity measurement h is determined by where 7 555 


is the similarity of the q-th vector in the FBSB to B\(t a is the vote of template matching of the q-th vector to B\(t on the n-th dimension, when either both of non zero L and M or both of non-zero H and M then a I otherwise aq\224  0 6 is the square root of the mean square error between the beliefs of the q-th vector and B\(t on dimension n We then retrieve the decisions of the cases with the and use either one decision or maximum similarity interpolated decisions 4 Computer runs on real world data We test the methodology on real world noisy and incomplete data The Wisconsin breast-cancer database was originally provided by Dr William H Woldberg and used by a number ofresearchers in pattern recognition This database contains 699 cases each of which is described by ten attributes in addition to the unique identification code number Attribute 10 is the class decision here The attributes are listed in table 2 Table 2 Wisconsin breast-cancer data attributes  Attribute Domain 0 Sample code number id number I Clump Thickness 1-10 2 Uniformity of Cell Size 1-10 3 Uniformity of Cell Shape 1-10 4 Marginal Adhesion 1  10 5 Single Epithelial Cell Size 1  10 6 Bare Nuclei 1-10 7 Bland Chromatin 1  10 8 Normal Nucleoli I  10 9 Mitosis 1  10 10 Class 2 for benign 4 for malignant Experiments with existing incomplete data There are 16 records in the Wisconsin breast-cancer database with the value of Feature 6 missing We use these 16 records as incomplete observations with their class label masked and use the rest of the data set for creating a FBSB The FBSB is created and FBSBR is applied to make decisions for these observations as to their membership in the disjoint classes benign or malignanl Decisions were compared against the masked labels and accuracy was computed The results are shown in Table 3 As can be seen 14 out of the 16 cases were labeled correctly by the FBSBR Cases 1096800 and 704 168 which are highlighted in Table 3 were mislabeled to mnlignont The accuracy for this test is 14/16, or 87.5 Table 3 Result with records of misslng Feature 7 Sample code  Label by FBSBR OriEinal Label I05701 3 malignant malignant 11 83246 benign benign 1 I84840 benign benign 1193683 benign benign 1197510 benign benign 1241232 benign benign 169356 benign benign 432809 benign benign 563649 malignant malignant 606 140 benign benign benign benign 61634 733639 benign benign 1238464 benign benign 1057067 benign benign 1096800 malignant iP.&l  i 704168 malignant\221 bFwn_ Experiments with created incomplete datu We also did tests on the Wisconsin breast-cancer with the missing data taken out We divided the 683 complete cases into 10 portions Each time we drew one portion out of the ten to use as an observation set and used the rest for creating a small fuzzy-belief-state base Table 4 Accuracy with created incomplete data missing one attribute each time 223Missine\224Attribute Dntaset 1 2 3 4 5 6 7 8 9 1 St 81 81 81 85 85 87 84 78 82 2nd 97 97 94 96 99 91 99 97 97 3th 97 97 99 99 97 94 99 97 99 4th 88 96 91 94 90 90 91 91 91 5th 88 97 96 97 97 91 97 96 97 6th 99 99 97 99 99 97 99 99 97 6th 99 99 97 99 99 97 99 99 97 7th 96 96 94 94 96 96 96 96 96 8th 97 99 97 96 97 99 97 96 97 9th 100 100 100 100 100 100 97 100 100 10th 100 100 100 100 100 100 100 100 100 For the observation set we deliberately masked their labels and also cleared their values of one or more features to simulate real world incomplete observations Then the FBSB was created and FBSBR was applied to infer about the missing data and to make decisions for these observations The decisions were compared against the masked labels and the accuracy was computed We repeated this for each different observation set Tables 4 and 5 show the results 556 


Table 5 Accuracy  with created incomplete data missing three attributes each time 223M issing\224Attribu tes Dataset 1.4 9 2 5 7 3 6 8 1 St 82 84 91 2nd 96 97 90 3th 97 97 94 4th 93 91 90 5th 91 96 87 6th 99 97 96 7th 96 96 96 8th 99 97 100 9th 100 100 100 10 th 100 100 99 5 Conclusions and future work A new methodology fuzzy-belief-state-based reasoning is introduced for making decisions with uncertain and incomplete data from disparate sources In this methodology a FBSB is created from historical data of a system by clustering and fuzzification When given observed incomplete and uncertain data the FBSB is then mined for fuzzy association rules to infer about the missing data After that each case in it is matched against the inference-completed observation to retrieve decision\(s of the best match\(es The test results on real world data prove the effectiveness of this methodology FBSBR leverages the strength of fuzzy clustering, belief inferencing and case-based reasoning to provide an innovative and intuitive way of reasoning under uncertainty and incompleteness Compared with the existing Bayesian network models of belief inferencing FBSBR has the following advantages 1 it is not NP-hard The fuzzy association rules are mined after an incomplete observation is made Only the association rules fur the observed ones and the unobserved ones are mined in a one-to-one manner The time complexity for the rule mining is 3N 2 it isflexible This post-observation mining not only prunes the miningspace but also makes FBSBR much more flexible than any other model that is predefined before observation 3 it avoids the graphical structure problem FBSBR reasons at the data level instead of using graphical representations for events or associations This avoids the big graphical representation problem which inhibits the utilization of existing network models 4 it is self-expandable As new cases are added and new rules are found and saved the FBSBR system grows and learns Compared with the case-based reasoning CBR technique FBSBR handles problems that CBR cannot handle without degrading the performance for retrieval of cases i.e FBSBR reasons under incompleteness Future work includes the investigation of evolutionary processes whose interrelationships change over time. It will also include searching for more complex rules Future applications will include diagnosis tracking and threat analyses for examples References I A Aamodt and E Pfatz, Case-based reasoning: foundational issues, methodological variations and system approaches IEEE AI Communications 7\(i 1994,39-59 Z A Barr and E A Feigenbaum Handbook ofAriificia1 Intelligence Morgan Kaufmann Los Altos, CA 198 1 3 K P Bennett  0 L Mangasarian 224Robust linear programming discrimination of two linearly inseparable sets\223 Optirnization Methods and Software 1 1992 23-34 4 B G Buchanan and E H Shortliffe Rule-based Expert Systems the MYCIN Experiment of the Stanford Heuristic Programming Project, Addison-Wesley, Reading MA 1984 SI K.C.C Chan and W.-H Au 224Mining Fuzzy Association Rules,\224 in Proc of the 6th Id1 Conf on Infirmation and Knowledge Management Las Vegas Nevada 1997 pp 209 215 161 G F Cooper Probabilistic Inference Using Belief Networks is NP-Hard Technical Report KSL-87-27 Medical Computer Science Group, Stanford University 1987 7 P Dagum and M Luby 223Approximating probabilistic inference in Bayesian belief networks is NP-hard,\224 Arfificiul Innrelligence 60\(1 1993, 141-153  81 A P Dempster 223A generalization of Bayesian inference,\224 J Royal Statistical Soc 30 1968,205-247 9J A Fu M.H Wong S.C Sze W.C Wong W.L Wong W.K Yu 224Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes\223 IDEAL 98 1st International Symposium on Intelligent Data Engineering and Learning, Pages 263-268 Hong Kong October 1998 I 01 D.E Heckerman, Probabilistic interpretations for MYCIN\222s certainty factors in L.N Kana1 and J.F Lemer Eds Uncertainly in Artipcial Intelligence 3  North-Holland Amsterdam 1986, 167-196  I11 Eric J Horvitz Johh S Breese, \223Decision Theory in Expert Systems and Artificial Intelligence \(Special Issue on Uncertainty in Artificial Intelligence International J Approximate Reasoning 2 1988,247-302 557 


12 Fin V Jensen Bayesian Networks and Decision Graphs Springer, New York 200 1 13 1 L Kolodner 223Maintaining Organization in a Dynamic Long-Term Memory,\224 Cognitive Science 7\(4 1983,243-280 14 C.M Kuok A Fu M.H Wong 223Mining Fuzzy Association Rules in Databases\223 ACM SIGMOD Record vol 27\(1 March 1998,4146 lS Lily Liang and Carl Looney 223Inference via fuzzy belief Petri nets,\224 Proc IEEE ACTAI 2003 Conf Sacramento 2003 5 10-5 14  Carl G Looney and Lily Rui Liang 223Cognitive Situation and Threat Assessment of Ground Battlespaces\224 Int J Information Fusion 4\(4 2003,297-308 1171 Carl Looney and Lily Liang 223Inference via Fuzzy Belief Networks\224 Proc ISCA International Conference San Diego Nov 2002  181 Carl G Looney, +\221Interactive clustering and merging with a new fuzzy expected value,\224 Pattern Recognition 35 2002 2413-2423 19 Carl G Looney Pattern Recognition Using Neural Networks Oxford University Press NY 1997 20 Carl Looney 223Fuzzy Petri Nets for Rule Based Decisionmaking,\224 3EEE Trans SMC 18 I Jan. 1988 22 A E Nicholson and J M Brady 223Dynamic belief networks for discrete monitoring,\224 ZEEE Trans SMC 24 I I 1994, 1593-1610 23 Z Pawlak Rough Sets Theoretical Aspects of Reasoning about Data Kluwer Academic, Dordrecht 1991 24 J Pearl Probabilistic Reasoning in intelligent Systems Networks of Plausible Inference Morgan Kaufmann Publishers San Mateo, Califomia 1988 25 R Schank Dynamic memory a theovy ofreminding and learning in computers and people Cambridge Universiv Press Cambridge UK 1982 26 G Shafer A Mathematical Theoy of Evidence Princeton University Press, Princeton 1976  Glenn Shafter and Judea Pearl Editors Uncertain Reasoning Morgan Kaufinan San Mateo,l990 Readings in 28 S Wright 223Correlation and causation,\224 J Agricultural Reseurch 20 1921,557-585 29 Y Xiang, \223Belief updating in multiply sectioned Bayesian networks without repeated local propagations,\224 Int J Approximate Reasoning 23,2000 1-21 30 Nevin L Zhang 223Computational properties of two exact algorithms for Bayesian networks,\224 Applied Intelligence 9 1998 173-183 1211 Sucheta Nadkami and Prakash Shenoy 223A Bayseian network approach to making inferences in causal maps,\224 European J Operations Research 128,2001,479-498 558 


tight t t= 1 Step 8: If  is null, then do the next step; otherwise, set r=r+1 and repeat Steps 6 to 8 1rL Step 9: Construct the association rules for all large q-itemset t with items \( ,  using the following substeps 1 2, ,...,t t a 1 1 1... ... qt t t? ? ? ? ? ?     \(8 b rules using 1 1 1 1  k k n j j j t t j n j t j           9 We take the fuzzy mining process of excessive air coefficient as an example. The excessive air coefficient value is taken when the unit is in 300MW stable load and the coal consumption is lower \(lower than 355g/kW.h history data. Then the data is standardized and is mined by fuzzy mining algorithm. The following rule is output: When the load is 100% and coal consumption eB  \(g/kW.h 37.50, 43.75], the excessive air coefficient  is [34.5 35.7], with a support value of 62% and a confidence of 81 It satisfies the requirement of minimum support and  Step 10: Output the rules with confidence value larger than or equal to the predefined minimum confidence After Step 10, the rules constructed are output and can act as the meta-knowledge for the given transaction. It is expressed based on reasoning and is easy to be understood 4.  The application of fuzzy data mining in operation optimization Based on the history data of a 300MW power plant unit, the typical parameters which are related to operation optimization are analyzed. Those parameters include load main steam pressure, main steam temperature, re-heated temperature, feed water temperature, exhaust gas temperature, excessive air coefficient and so on. The quantitative association rules are acquired by analyzing the history data. For example, load    1... nL L 1... nP P 1 1... mT T g gnB B related to low coal consumption are chosen as optimization values to optimal the electric industrial process. The rules to decide the optimal parameters values are expressed as 


to decide the optimal parameters values are expressed as 1 1   this way the optimal operation parameters are decided according to the load and the other related condition. The optimization values attained from data mining are reachable in operation and can reflect the actual status in operation According to the method mentioned above, the history data from recent three months are analyzed. A total of 4212 transactions consisted of the operation parameters of the typical stable load of 100%, 90% and 80% are obtained These transactions are standardized by formula \(1 minimum support value is set at 30% and the minimum confidence is set at 75%. The membership function of the parameters is shown in Figure 2 75 82 86 92686458  1 Low Middle High 0.5  Figure 2. The membership function 1645 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 minimum confidence. This rule can be expressed as The corresponding range is This rule means that when the load is 300MW and the coal consumption is lower the optimal range of the excessive air coefficient is 1.342~1.350. The average of the interval can be adopted to decide the optimization point. The optimization value of excessive air coefficient 100 ,34.5,35.7eM ?&lt; &gt;?&lt 300 ,1.34eM MW ?&lt; &gt;?&lt gt gt;2,1.350 is set at 1.346 when the load is 300MW \(100 the method mentioned above, by utilizing the fuzzy association algorithm to mine the optimization value when the typical load of 100%, 90% and 75%, etc, a set of optimization values is obtained. The optimal values attained from the fuzzy data mining and the reference values attained in traditional way are listed in Table 1 300MW 16.67 Referenc Optimal Parameters Main steam temperature Reheat temperature Feed water temperature Flue gas temperature Excessive air coefficient Main Steam Pressure MPa 537 537 270.5 138.4 137.1 1.352 1.346 16.71 538.3 538.1 


538.1 268.1 270MW e/ Referenc Optimal 16.67 537 537 264.2 135.2 133.6 1.432 1.431 16.66 538.1 537.4 264.6 Table 1. Optimization value analysis result of 300MW power unit 225MW e/ Reference Optimal 13.89 537 537 254.1 129.8 130.4 1.480 1.484 13.96 537.2 535.4 258.4  The controllable parameters are optimized based on the results of the fuzzy data mining in power plant. The performance of the boiler improved obviously. The average boiler efficiency improved about 0.924% and the coal consumption reduced about 3.72g/kW.h. The optimization value is close to the reference value in trend and can be used to guide the industry process. The newly founded rules and knowledge can be added to model base or the knowledge base. The operation optimization base on data mining is an effective method to improve the efficiency in power plant The execution times of fuzzy association mining in different minimum support with a computer Pentium 1.7G/256M are shown in Figure 3. For a total of about 4000 transactions in the data set and the minimum support is set at 20%, the execution time is about 150s. The fuzzy association mining is high efficiency. The execution time of fuzzy association mining increases linearly with the transactions in data set. So it is applicable to large data sets 80% 60% 40% 20 50 100 150 200 minimum support tim e s  Figure 3. The time of fuzzy association mining in different minimum support 5. Conclusion The operation optimization is the mainly method to improve the performance in power plant and the decision of optimization value is the key point in operation optimization. In this paper, we proposed the operation optimization based on data mining and applied the fuzzy 


optimization based on data mining and applied the fuzzy association rules to find the optimization value from the history data of the equipments in power plant. The fuzzy sets theory was introduced into the association mining process in order to soften the partition boundary of the domain and generalize and abstract the data. Base on the history data in power plant, the optimization values are reachable in operation and easy to guide operation. The rules mined out exhibit quantitative regularity in large database and can be used to provide guides and suggestions to the appropriate operator. Experimental results with the data in a 300MW power plant show that the algorithm base on fuzzy set operation performs very well and can be used to guide the operating process to achieve a good performance References 1] Wang X Z  Automatic classification for mining process operational data  Ind. Eng. Chem. Res., 37 pp.2215-2222, 1998 2] Tony Ogilvie, B W Hogg  Use of data mining techniques in the performance monitoring and operation of a thermal power plant  IEEE Colloquium on Knowledge Discovery and Data Mining, 1998 3] Ren HaoRen, Li Wei  The analyze of operation index for the power unit under different loads  Proceedings of the CSEE, Vol 19, No. 9, pp.50-52,56, 1999 4] Agrawal R, Imielinski T, Swami A  Mining 1646 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 association rules between sets of items in large database  Proc of the ACM SIGMOD conf on Management of data, Washingtong D.C, pp.207-216 May 1993 5] Agrawal R, Srikant R  Fast algorithms for mining association rules in large databases  The International Conference on Very Large Data Bases, Santiago, Chile pp. 487-499, 1994 6] Srikant R, Agrawal R  Mining quantitative association rules in large relational tables   Proceedings of the ACM SIGMOD International Conference on Management of Data, Montreal Canada, pp.1-12, 1996 7] Zou XiaoFeng, LU JianJian, Song ZiLin  Mining linguistic valued association rules  Journal of System Simulation, Vol 14, No. 9, pp.1130-1132, 2002 8] T. P. Hong, J. B. Chen  Finding relevant attributes and membership functions  Fuzzy Sets and Systems Vol 103, No. 3, pp.389-404, 1999 9] T.P.Hong, C.S.Kuo, S.C.Chi  Mining association rules from quantitative data  Intelligent Data Analysis, Vol 3, No. 5, pp.363-376, 1999   1647 pre></body></html 


0-7695-2263-7/05 $20.00  2005 IEEE pre></body></html 


n M L N n t n t n t n t L M L t L t L tt L t kkkk kkkk kkk kkkk kkkkkkkkP VK VK VK VK PP       kkP t 31 where L  s the error covariance associated with the state estimate t i    kkLX  tt kkk P1  00 0  0                     s s sss s s sss s s sssss N n t n t n 


n t n N n t n t n t n N n t n t n t n t n t n c t L kkkkkk kkkkk kP VKVK VKVK  32 4. Simulations One has run simulations comparing the sequential implementations of MSJPDA algorithm and the new algorithm here. A typical multisensor multitarget tracking environment is assumed in the simulations. According to article [1,3], One known that the performance of sequential MSJPDA is better than the performance of parallel MSJPDA. Therefore, the performance of parallel MSJPDA algorithm will not be compared here There are three sensors, which are fixed in three platforms. Regarding the 2nd sensor as fusion centre situation of the other sensors are: =?-500m?-500m 0m??N =?-500m? 500m?0m??The distance error of each sensor is: =300m, =200m, =100m?The bear error of each sensor is 0.03rad, =0.02rad, =0.01rad?The of sample is T=1s?The nonparametric model of clutter is used in the simulations, and expected number of false measurement is m=1.8 1 sN 3 s 1r 2 2r 3 3r 1 Simulations have been run for racking two targets. The true initialization state of the targets is X1?[-29500m,400m/s,34500m,-400m/s X ?[-26250m,296m/s,34500m,-400m/s]'? 2 The two targets will cross above 31seconds later. To evaluate tracking performance, 50 Monte Carlo runs were performed for three case of the target detection probability Pd=0.97 ? Pd=0.76 ? Pd=0.58. In every run, the total simulation time is 140 steps 


simulation time is 140 steps            Figure 1  RMS position error in case of Pd=0.97          Figure 2  RMS velocity error in case of Pd=0.97       Figure 3  RMS position error in case of Pd=0.76 567 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005         Figure 4  RMS velocity error in case of Pd=0.76         Figure 5  RMS position error in case of Pd=0.58          Figure 6  RMS velocity error in case of Pd=0.58  Table 1 The emanative times comparison for sequential MSJPDA and SD-CMSJPDA algorithm  Pd N A  0.97 0. 76 0.58 Sequential MSJPDA 2 11 17 SD-CMSJPDA 0 3 5 Pd denotes detection probability, N denotes emanative 


Pd denotes detection probability, N denotes emanative times, A denotes the kind of algorithm Table 1 shows the summation of emanative times for sequential MSJPDA and SD-CMSJPDA algorithm in 50 Monte Carlo simulations. From table 1 , it is shown that the stability of SD-CMSJPDA is better than that of sequential MSJPDA as the detection probability varied Figure 1,2 show the RMS errors for position and velocity in case of Pd=0. 97, respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.76 respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.58, respectively. From the figures we can see that the average RMS position error is lower for the SD-CMS JPDA algorithm. We also see that the state estimation precision of sequential MSJPDA get worse as the detection probability decreases The reasons for these simulation results lies:1 state estimation precision will get worse when the detection probability decrease;2 algorithm is to process measurement from each sensor using single sensor JPDA algorithm sequentially. Therefore the estimation error from each sensor will be accumulated Moreover, the sequential MSJPDA algorithm can  t improve the joint detection probability of the multisensor system The estimation error of the SD- CMSJPDA  algorithm will not be accumulated for it processes the measurement from each sensor directly in the mean time .What  s more the new method can greatly improve the joint detection probability of the multisensor system. Therefore, the tracking performance of SD-CMSJPDA algorithm is better than that of sequential MSJPDA. Algorithm All of the simulations are run in the personal computer with a 2.0G CPU and a 256M memory. The average cost time per step is 0.0251 in the sequential implementations of MSJPDA algorithm. And the average cost time per step is 0.0282 in the sequential implementations of MSJPDA algorithm. According to the results we can see that there is few difference in real time between the new method and the sequential   MSJPDA when there is not so many sensors and targets 568 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  5. Conclusion In order to solve the problem of multisensor multi target tracking, a new centralized multisensor  joint probabilistic data association  algorithm is proposed in this paper. The simulation results shows that the tracking performance of the new algorithm is better than that of the sequential MSJPDA algorithm The computational complexity of the new method will increase as the number of sensors and targets grow Therefore, how to improve the real time of SD- CMSJPDA algorithm will be pay attention References 1] He You, Wang Guohong, Lu Dajin, Peng Yingning Multisensor Information Fusion With Application[M Publishion House of Electronics Industry. 2000, Beijing.  [11] B..Zhou and N.K.Bose Multitarget  Tracking in Clutter:Faste Algorithms for Data Association .IEEE Transaction on Aerospace and Electronic Systems 1993,29\(2 2] Bar-shalom,Y\(Ed Applications and Advances,2: Norwood,MA Artech  House, 1992 3] L.Y. Pao, C.W.Frei. A Comparison of Parallel and Sequential Implementation of a Multisensor Multitarget Tracking Algorithm. Proc. 1995 American Control Conf. Seattie, Washington,June 1995 1683~1687 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





