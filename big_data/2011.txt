html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Space Charge  Oscillations in Gyrotron Magnetron-Injection Guns with High Pitch-Factor Vladimir N.Manuilov Radiophysical Department of Nizhny Novgorod State University 23, Gagarin Avenue, Nizhny Novgorod, Russia, 603950 manuilov@rf.unn.ru Abstract: The data of numerical simulation of gyrotron magnetron-injection guns \(MIGs beams \(HEBs HEBs parameters for the regimes with big value of the locked into the adiabatic trap space charge are found. It is shown that if the magnetic compression ratio or the anode potential is high enough to cause the intense space charge oscillations in the beam, the values of the pitch-factor and velocity spread are saturated while the oscillations of the space charge and potential in the beam continue to increase Keywords: gyrotron, magnetron-injection gun, adiabatic trap, space charge, low frequency space charge and potential oscillations, numerical simulation Introduction Helical Electron Beams \(HEBs the energy source of radiation. The efficiency of gyrotrons strongly depends on the achievable level of the pitch-factor g  v  0  v  0   H e r e  v  0    v    0   a r e  t h e  m e a n  v e l o c i t i e s  o f  rotational and longitudinal motion in the working space The most reliable and frequently used formation system for HEBs is the magnetron injection gun \(MIG reasons [1] MIGs form HEBs with the sufficient oscillatory v e l o c i t y  s p r e a d   v   o f  a b o u t  2 0 3 0    A t  t h e  s a m e  t i m e   a  characteristic feature of MIG is the trapped configuration of the magnetic field distribution \(Fig.1 essential part of particles reflect from the magnetic mirror e v e n  f o r  m o d e r a t e  v a l u e s  o f  t h e  p i t c h f a c t o r   g    1  3   1.5 Such electrons are accumulated in the adiabatic trap between the cathode and the cavity, make the beam quality worse and can initiate the beam instability [1]. So, studying the process of accumulation of the particles in the trap and development of space charge oscillations in the beam makes it possible to predict beam parameters with more accuracy and might specify the ways to improve the beam quality Below the process of establishing of HEBs parameters oscillatory velocity distributions, time dependencies of locked charge, passing to the cavity current and some others beam topology \(regular intersecting, boundary and laminar beams  cold  pitchfactor \(calculated without taking into account the space c h a r g e  f o r c e s   g 0   2  a r e  c o n s i d e r e d   Figure 1. MIG with regular intersecting beam. The ratio of the current magnetic field intensity to the operating magnetic field B0 in different planes is specified. All dimensions are normalized to the cathode radius Short Description of Physical Model The most adequate way to take into account the reflected particles is to use the numerical dynamic model based on the PIC-method. The numerical procedure takes into account two-dimensional magnetic- and electric field distributions including the own beam space charge fields and the action of the secondary emission from cathode caused by the reflected from the magnetic mirror electrons bombarding cathode surface. The following conditions were assumed a b c the quasi-static approach is valid The combined action of such factors as thermal velocities 


The combined action of such factors as thermal velocities and emitter roughness was described by introducing the G a u s s i a n  f u n c t i o n  o f  i n i t i a l v e l o c i t y  d i s t r i b u t i o n  f  v    a t  t h e  particles   s t a r t  p o i n t s   2    T h e  w i d t h  o f  f  v    w a s  c h o s e n  i n  such a manner as to provide the relative velocity spread v a l u e   v  2 0   i n   cold  regime Results of Simulation MIGs of a centimeter-wave gyrotrons with different angles     1 2  18  and 26   field line to provide different HEB topology, was chosen for the numerical simulation \(see Fig.1 in the regime close to the critical one. The value of the beam current I was assigned such as to ensure that t j  I  I L  0  1   I L  i s  t h e  L a n g m u i r  c u r r e n t  o f  t h e  g u n    Due to the velocity spread particles with high oscillatory velocities are reflected from the magnetic mirror and then captured into the adiabatic trap between the cathode and the cavity. The accumulation of locked space charge Qr leads to significant deviation of the pitch-factor calculated from dynamic model in comparison with the data of static one which neglects the reflected electrons [1]. As an example table 1 shows the evolution of HEB parameters in t r a d i t i o n a l  M I G  f o r m i n g  r e g u l a r  i n t e r s e c t i n g  b e a m    2   Here gstat , gdyn are values of pitch-factor according to static a n d  d y n a m i c  m o d e l s  c o r r e s p o n d i n g l y    v  s t a t  a n d   v  d y n  a r e  values of velocity spread defined by the levels of 0.1 and 0.9 of the cut-off collector curve [1, 2], and kR=Iref/I is the ratio of the reflected current to the emission current calculated from the static function of oscillatory velocity distribution. Moreover, dynamic model shows that if g 0  2  5   t h e  f u r t h e r  i n c r e a s i n g  o f  p i t c h f a c t o r  i s  n o t  p o s s i b l e   In such regimes the accumulation of the space charge in the trap is so strong that caused by this factor additional depression of the emitter electric field totally compensates the attempts to increase pitch-factor by increasing the anode potential or magnetic compression ratio. It is n e c e s s a r y  t o  n o t e  t h a t  j u s t  i n  t h e  r e g i m e s  w i t h  g 0  2  5  t h e  locked space charge Qr exceeds to the charge of the primary beam Q0 \(the charge of the particles moving to the cavity and not reflected yet the locked charge as well as potential inside the beam are developed Table 1. HEB parameters calculated by different models Static model         Dynamic model g 0  g s t a t   v  s t a t  k R  g d y n   v  n 1.0 0.93 0.360 0 0.91 0.364 1.5 1.45 0.314 0.04 1.38 0.308 2.0 1.80 0.302 0.15 1.45 0.291 2.5 2.08 0.292 0.22 1.51 0.292 3.0 2.38 0.276 0.26 1.50 0.293 3.5 2.65 0.282 0.33 1.48 0.288 4.0 2.88 0.271 0.37 1.50 0.293 In such regimes not only g value, but also velocity spread becomes close to constant and does not depend on the cold pitch-factor because the magnetic mirror does not let pass the  bad  electrons to the cavity. In the operating space the dynamic model gives the velocity spread values which are only a little bit higher than the prediction of the static one But in the intermediate plane before the magnetic mirror t h e  q u a l i t y  o f  t h e  H E B  i s  m u c h  w o r s e   h e r e   v   r e a c h e s  0.40-0.45. The data of table 1 allow to make the conclusion that the critical value of static reflection coefficient kR when the beam becomes unstable and big deviation between predictions of static and dynamic models occurs is close to 0.05. So, when the optimization of MIG for millimeter or submillimeter wave gyrotron is performed on the basis of static model only \(we have to use such approach because of too big computation time to provide kR 0.02-0.03 The quality of HEB can be improved if we turn to the 


The quality of HEB can be improved if we turn to the boundary of laminar beam. Table 2 contains the normalized ratio of locked charge for three beam types which differ by t h e  a n g l e    o n  t h e  e m i t t e r   S o   w e  s e e  t h a t  t h e  l o c k e d  charge in boundary and laminar beams is essentially lower e s p e c i a l l y  f o r  m o d e r a t e  v a l u e s  o f  g 0   Table 2. Value of the normalized locked charge Qr/Q0 in HEBs with different topology g0 Regular intersecting beam Boundary beam Laminar beam 2.0 1.2 1.0 0.7 4.0 2.8 2.3 2.4 I n c r e a s i n g  o f  t h e  a n g l e    e l i m i n a t e s  t h e  o s c i l l a t i o n s  o f  space charge and potential as well till the values g0 3.5 and so strengthens the HEBs stability. Besides that, it grows up the maximum achievable value of the operating pitch-factor see Table 3 Table 3. Value of the calculated by dynamic model pitch-factor in HEBs with different topology g0 Regular intersecting Boundary beam Laminar beam 2.0 1.45 1.43 1.58 2.5 1.51 1.55 1.65 3.0 1.50 1.63 1.69 3.5 1.48 1.63 1.61 4.0 1.50 1.63 1.65 Conclusion Accumulation of the trapped particles in the beam leads to saturation of the operating value of the pitch-factor and development of strong oscillations of the space charge and potential in HEBs. The way to improve the HEB stability and increase maximum achievable pitch-factor is to use MIGs forming boundary or laminar beams References 1. P.V. Krivosheev, V.K.Lygin, V.N. Manuilov Sh.E.Tsimring. Numerical Simulation Models of Focussing Systems of Intense Gyrotron Helical electron Beams, Int. J. of Infrared and MM waves, vol 22, no. 8, 1119-1146, 2001 2. V.K. Lygin. Numerical simulation of intense helical electron beams with the calculation of the velocity distribution functions. Int. J. Of Infrared and MM waves, vol.16, no.2, pp.363-376, 1995 pre></body></html 


central server, and sends the data to the central server. The data which don  t need to transmit to the central server will be stored in the observation server so that it can be used in the future. We also can control the equipments by sending some commands to them from the central server. The central server sends the commands to the observation server T, and then T redirects the command to the equipment S and S will respond according to different kinds of commands which sent from the central server C.  Data Management There are two kinds of data, the data stored in the central server and the data stored in the observation server. Data collection module collects the camera and sensor data from the camera and sensor equipments. If the data don  t need to transmit to the central server, they will be stored in the field station server database. To manage these heterogeneous data located in different field stations, we use the SDSC Storage Resource Broker \(SRB  them. SRB is client-server middleware that provides a uniform interface for connecting to heterogeneous data resources over a network and accessing unique or replicated data objects and in conjunction with the Metadata Catalog MCAT based on their logical names or attributes rather than their names and physical locations. Irods [7] is middleware based on the SRB technique, and we use it to manage the heterogeneous data distributed in different field station in the FEDC framework. We need to manage two kinds of data, equipment status monitor data and data generated by sensors and cameras in ChinaFlux project. We have an iRods iCAT server in the central server node, and each field station has an iRods data server which used to index all the data in the field station server. The iCAT is the iRODS CATalog, stored in a database using a DataBase Management System \(DBMS data management of FEDC is described by figure 2   Figure 2. Data Management Architecture in FEDC 1 In the FEDC framework, all the metadata information will be registered in the metadata database located in the central server so that every query just needs to send to the central server, and it will return the results every quickly. In order to register all the metadata [8] in the central server, we proposed a metadata self-registry method in the FEDC framework. To accomplish the metadata self-registry, we need to do three steps as below a metadata information and this information exists in their file name, file attribute, and other related objects. We use a module to extract all the metadata related to the file in the FEDC framework b special format so that it can be registered in the central server c the iRods server can accept, we use the micro-services which are small, well-defined procedures/functions that perform a certain task in the iRods, to register the metadata into the irods iCAT server 2 All the data information in the central server and field stations are stored in the metadata database in central iRods server. The query is as below Step1, the user asks for data. The user sends the query to the central server S to get the data information, including its location and metadata information Step2, data request goes to SRB Server. The central server S process the query sent by user and redirect the query to the SRB server to get the results 


SRB server to get the results Step3, SRB Server looks up information in database. When the SRB server gets the queries, it will query its metadata database to see whether the metadata information of the query data is there, and then tell the central server S which field station server has the data 411 Step4, get the data from field station server. After get the query results, the central server will return the results to the user U, and the user can send the request to the field station server T according to the query results from central server If the user has the enough rights, the field station server T will send the data to the user D. Data Analysis and Visualization After we have transmitted the data to the central server we do some analysis based on the data stored in the central server database. In FEDC framework, we used GIS technique to visualize the monitor data, and some flash charts to visualize the real-time flux data. Figure 3 shows that results of using the flash chart to show the real-time wind speed and 2co  flux data, and figure 4 shows that using the map to visualize the status of local area network in the field station   Figure 3. The real-time visualization of flux data    Figure 4.  The visualization of monitor data  IV. CONCLUSIONS In this paper, we proposed a framework named FEDC which can be used in large scale field ecological data collection for the ChinaFlux project. This framework can be used to manage and transmit heterogeneous data generated by different kinds of collection equipments. We implemented the data collection layer, transportation layer management layer, analysis and visualization layer in FEDC framework, and every layer has different kinds of tasks in the field ecological data collection, and the result shows that it has good performance in field ecological data collection and management ACKNOWLEDGEMENTS We would like to thank Professor Honglin He, Doctor Xuefa Wen from Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences for their open idea, discussion, cooperation, and contribution. This work was supported by the Knowledge Innovation Program of the Chinese Academy of Sciences No.O815021108  REFERENCES 1] GR Yu, XF Wen, XM Sun, BD Tanner, X Lee, JY Chen. Overview of ChinaFLUX and evaluation of its eddy covariance measurement Agricultural and Forest Meteorology, 2006  2] F Vernon, T Hansen, K Lindquist, B Ludaescher. ROADNET: A Realtime Data Aware System for Earth, Oceanographic, and Environmental Applications.  Eos Transactions \(American Geophysical Union fall meeting  3] BM Howe, T McGinnis. Sensor networks for cabled ocean observatories. Underwater Technology, 2004  4] C Cotofana, L Ding, P Shin, S Tilak, T Fountain. An SOA-based Framework for Instrument Management for Large-scale Observing Systems \(USArray Case Study Conference on Web  5] MT Ritsche, DJ Holdridge, R Pearson. New and Improved Data Logging and Collection System for Atmospheric Radiation Measurement 


Climate Research Facility, Tropical Western Pacific, and North Slope of Alaska Sky Radiation, Ground Radiation, and MET Systems. Fifteenth Atmospheric Radiation Measurement, 2005  6] C Baru, R Moore, A Rajasekar, M Wan. The SDSC storage resource broker. Proceedings of the 1998 conference of the Centre for Advanced Studies on Collaborative research,1998  7] A Rajasekar, R Moore, F Vernon. iRODS: A Distributed Data Management Cyberinfrastructure for Observatories. American Geophysical Union, Fall Meeting 2007  8] S Weibel. Metadata: the foundations of resource description portal.acm.org, 1995 412 pre></body></html 


our idealized scenario, we have a unimodal fitness landscape, and thus our local best fit will be the global best fit; however, this will not be the case for more complex situations, having multimodal landscapes.  Such scenarios are to be expected for implementations that are more realistic Substitutions are another way of injecting diversity into the gene pool. With substitution, values for individual genes are swapped. In the present scenario, this is akin to changing a latitude value while fixing longitude, or vice versa. Lastly a small fraction of guesses by the ECM are  clones  of the parent, having identical genetic structure. The combination of these four processes \(crossover, mutation, substitution and cloning optimal genotype while eliminating weaker genetic structures.  The ECM design allows the user to select the ratio of guesses that are produced via crossover vs. other processes Some of these genetic processes can be seen operating in Figure 3, which shows a typical example of the ECM  Figure 3:  Location of 100 plume source location guesses in each of four different generations  spacecraft observations of the circular analytical plume with 0% noise is the reference case; crossover/mutation is 50/50  see text and Figure 4 cross represents a single guess. Guesses are essentially random in Generation 1, but begin to  converge on a solution by Generation 5.  Convergence continues through Generation 10, and by Generation 20, most guesses  are within a fraction of a degree of the correct solution  behavior over several generations \(spacecraft observations of the circular analytical plume with 0% noise are assumed In each generation of this example, the ECM makes 100 guesses of the plume center location, with a 50/50 ratio of genetic crossover to the other processes. Each of the crosses in Figure 3 represents the location of a single guess \(of the center of an individual test plume region in which our steady state plume is located \(as illustrated in Figure 1 region to search only a 20  x 20  box that encompasses the observations Generation 1 shows the arbitrary distribution of the first 100 guesses. For each of these guesses, the plume model prepares an idealized plume centered at the latitude longitude of the guess, measurements of which are then compared to the set of spacecraft observations. Following the algorithm discussed above, those guesses that most closely fit the observations have their genetic information carried forward into the subsequent generation to seed a portion of the next 100 guesses, with the remaining portion being generated through mutation, substitution and cloning As quickly as in Generation 5 \(Figure 3, top right 7 significant number of guesses are seen converging on a solution. This convergence improves through Generation 10, although we still see the same fixed number of mutations \(the point at 9  latitude, -6  longitude is an example of a mutation have converged to within a fraction of a degree of the correct solution at \(0  0   substitution of a single gene on a number of guesses in Generation 20, which produce the  cross  shape apparent in the figure. For these guesses, either latitude or longitude is replaced, with the other gene held at its prior best-fit value By adjusting the fraction of mutations and substitutions correctly, one can balance the need for diversity with speed of convergence. In the absence of any mutations or substitutions, the solution will converge on the first local best fit it obtains, which may or may not be the global best fit. At the other extreme, a 100% mutation/substitution rate yields scattershot, or random, guesses, and provides no 


yields scattershot, or random, guesses, and provides no computational advantage over a brute-force solution, since determination of the global best fit only can be made after the full parameter space has been explored. A optimal medium exists somewhere between these two extremes.  A series of tests were performed to identify the best ratio to use in our ECM simulations \(Figure 4 tests was run out for 40 generations with 100 guesses in each generation, and the best-fit solution \(the lowest value of f from the 100 guesses ratio is ~50% crossover, 50% mutation \(including cloning and substitution search, but also provides efficient convergence to the global best fit  Figure 4: Demonstration of improved fitness for various ratios of crossover to mutation in a  sample ECM run \(spacecraft observations of the circular analytical plume with 0% noise is the reference case  identical plume models were run for 40 generations, but with each having the indicated percent of crossover and mutation. Lower  values of the ordinate indicate a better match between the plume model and observations. We see that a 50/50 split between  crossover and mutation provides the quickest path to a best-fit solution  We use this ratio for all the tests performed in Section 4 Generally, we obtain good convergence in 25 or fewer generations, each containing 100 test plumes \(2500 total compared with the more than one million required for a brute-force search over the 1440 x 720 points in the original grid Other minimization algorithms, such as LevenbergMarquardt \(L-M G-N significant disadvantages over the ECM for this type of problem.  First, because L-M and G-N are gradient-based algorithms, they require the landscape of the fitness function being minimized to be smooth, with a well-defined gradient. The function, f, being minimized in our tests however, has a sharp  spike  at the best-fit value, and a flat gradient most everywhere else. \(In other words, test plumes that do not exactly match the observations generally have the same poor fitness value regardless of the selection of genes global, minimizers.  If a guess is made and falls close to a 8 local minimum, the algorithm can be trapped around the local minimum and never find the global best fit.  The design of the ECM alleviates both of these problems  it functions well even with poorly defined gradients, and is designed to search for and identify the global best-fit solution There are other alternative approaches to identifying global minima such as the simulated annealing \(SA do not suffer the aforementioned problems.  The SA approach, as with the ECM, iteratively selects new solutions to the minimization problem; however, the decision to  accept  the new solution in SA is typically made in a probabilistic sense, independent of the new solution  s fitness.  With the ECM, a new solution is only accepted if the fitness is improved over the prior best fit There are two additional drawbacks to applying an approach like SA to this particular problem, although they best manifest themselves under less idealized conditions than those used here.  First, SA cannot handle problems that involve multiple competing objectives.  For example, we may be interested in identifying not only the source location of a plume, but also the surface emission flux or duration of emission. For such a problem, SA is forced to use an aggregated single objective by combining the competing objectives into a single weighted objective function, which is then iteratively optimized.  The ECM, on the other hand 


is then iteratively optimized.  The ECM, on the other hand can handle these multiple, simultaneous objectives without compromising accuracy in achieving each objective, and hence, is significantly more flexible Second, SA cannot find multiple local optimal solutions simultaneously.  In future implementations of our plume identification algorithm, we will likely have environments with multiple plumes, and minima corresponding to each solution.  The ECM is capable of efficiently handling this problem by partitioning the search space into multiple parts and having sub-populations, each of which focuses on a different part of the domain.  Since SA is a one-solution optimization algorithm, it cannot find multiple local solutions simultaneously For the present task, SA provides a viable alternative to obtaining the location of our simulated, idealized plume however it quickly becomes impractical when the problem becomes more complex and more realistic. Under those conditions, the flexibility of the ECM becomes a great asset 4. RESULTS A number of tests were performed to quantify accuracy in determining plume surface location as a function of the number of observations, level of instrument noise observation footprint size, and knowledge of actual plume shape. For most of the tests discussed in this section, we have assumed the simple, circular plume shape for both the actual and test plumes. This allows us to focus specifically on the singular objective of each test without dealing with differences in plume shape.  The exception to this broad assumption, is, of course, when we evaluate the role of our knowledge of actual plume shape itself, in which case we retain the circular shape for our actual plume and employ the zonally asymmetric one for our test plume Number of Observations We have shown previously \(Figure 1 circular plume appears like when observed for different lengths of time. We have performed a series of tests without random noise, for varying observation periods \(1 day, 10 days, 100 days, 1000 days using only observations that fall within  10  of what appears to be the plume center. Because of the Gaussian shape of the plume, tracer strengths at locations distant from the center are inconsequential and their contribution to f in Eq. 3 can be ignored.  Consequently, we can reduce our search space \(i.e. the region in which the ECM makes its guesses  x 20  box, which encloses the region with the strongest observations \(again, our steadystate approximation assures us that the plume source location must be within the region of strongest observations We have found that the accuracy in finding the correct plume center location generally increases as the number of measurements increases, as shown in Table 1. The results for the best-fit surface source location \(latitude and longitude with corresponding error in km generations. This demonstrates that, while it is not apparent from a cursory glance at Figure 1 that the source location of the true plume can be gleaned from the spacecraft coverage with any precision \(as the blurring of the original plume is quite evident amount of information to allow the best-fit latitude and longitude to determined by the ECM with high accuracy With observations of such a surface source over several tens of days, we should be able to decrease the source location uncertainty to levels that fall within the mobility range of future surface rovers \(20-40 km days or less of coverage \(about 1.5 martian years length of time of the primary science phase of most Mars missions Table 1:  Best-fit latitude and longitude positions for the circular plume model for varying lengths of observation 


circular plume model for varying lengths of observation campaigns Campaign Length days Best-Fit Latitude   Best-Fit Longitude  Error [km 1 -0.3476 -0.6190 42.0 10 -0.2522 -0.4613 31.1 100 -0.1862 -0.4960 31.3 1000 -0.1834 -0.2507 18.4 9 Instrument Noise Every observational measurement includes some level of noise in addition to the  true  signal. As such signal-to-noise problems are unavoidable, we must attempt to interpret their influence on the measurement result by introducing random noise into each observation and gauging the effect this has on our best-fit solution. To emulate, qualitatively, noise of various amounts, in separate tests we have added a noise term with a 3? distribution of either 10, 100 or 1000% of the local observation strength to each individual spacecraft observation. The impact of this noise term on the circular plume, relative to the perfect \(0% error Figure 5 There are 100 days of observations in each example. The results for the determination of the plume source location are presented in Table 2. While Figure 5 shows only one instance in which random noise at a specific amount has been applied, for Table 2  at each level of measurement uncertainty  the evaluation of plume source location was done ten times to account for the random factor and the variance among these ten cases is what is reported For low levels of measurement noise, in the tens of percent range, the results are largely indistinguishable from the noise-free standard, and uncertainty in position remains low relative to the noise-free instance from 10% to 100%, and then to 1000%, the ability to identify the plume source location becomes significantly diminished. With 100% uncertainty, individual measurements can vary by up to a factor of two, thus introducing  ghost  local maxima in the signal which are then interpreted as being close to the plume source location This phenomenon is significantly increased in the 1000 case. Note that, while Figure 5 shows only one case of random noise applied to the observations, the ghost local maxima do move around at this level of noise in different samplings, thus partly explaining the high uncertainty in source location  Figure 5:  Illustration of varying levels of instrument noise on the noise-free plume  measurement \(upper left of observations. As instrument noise increases, the plume source location is even less obvious  so that the accuracy to which the plume source location can be isolated is reduced 10 Table 2:  Variance in best-fit latitude and longitude solutions produced by the ECM for varying measurement uncertainty.  Source location uncertainty is relative to 0 measurement uncertainty value Measurement Uncertainty Source Location Uncertainty [km 0 0 10 0.6 100 31.5 1000 96.5  


 Footprint Size We performed a series of experiments demonstrating the role of measurement footprint size on source location uncertainty. As the footprint size decreases, the spatial extent over which the signal is averaged decreases and, in principle, provides better isolation of the peak signals from the adjacent, weaker  wings  of the plume \(Figure 6 footprint size is decreased from 200 km to 10 km, the retrieved shape from the observations better matches the  truth  although at the expense of a decrease in the area sampled in the same spatial domain. Table 3 bears out the competing influence of increased signal isolation and decreased spatial coverage for a 100-day observation campaign.  From a 200 km \(cross-track footprint, there is a ~45% decrease in uncertainty, but beyond this, there is no further gain. Figure 6 illustrates the cause of this strange behavior.  At 200 km \(upper left have near complete surface coverage, with substantial overlap, over the course of 100 days.  At 100 km \(upper right beginning to see small gaps in coverage, suggesting that for 100 days of observations, the amount of  repeat  coverage is small.  Hence, by reducing the cross-track footprint from 200 km to 100 km, we are provided the benefits of a smaller footprint  Figure 6:  Comparison of different observational footprint sizes.  Top left panel \(200 km  the nominal footprint size used in this study.  As footprint size is reduced, the shape of the plume is better defined, but at  the expense of spatial coverage Additionally, the peak plume strength in the observation set approaches that of the  true   plume \(cf. Figure 2b in black space indicates the fraction of the surface not observed by the instrument 11 Table 3:  Best-fit latitude and longitude positions for the derived plume source locations and consequent error for varying footprint widths Footprint Width km Best-Fit Latitude   Best-Fit Longitude  Error [km 200 -0.1862 -0.4960 31.3 100 -0.1523 -0.2485 17.2 50 -0.1178 -0.2390 15.8 20 -0.1195 -0.2414 15.9 10 -0.1276 -0.2391 16.0  but are not sacrificing overall surface coverage. For footprint sizes smaller than 100 km \(Figure 6, lower row the two effects counter each other relatively equally, and there is generally no improvement to the best fit. However as was discussed previously, for a fixed footprint size, an increase in the number of observations \(up to the point where coverage begins to substantially overlap improve the best fit. Footprint size is not an easily adjustable parameter for a spacecraft instrument and appears not to be as significant a factor in the plume source location error budget as is measurement spatial coverage Plume Shape In previous tests, we used the idealized case of a circular plume and assumed the same shape for the test plumes However, in reality, we will have no knowledge of the actual shape of the plume from our limited, defocused measurements, so we will need to make an educated guess as to the plume shape that is being observed. While knowledge of the meteorological context of the plume observations may be available, which would allow a 


observations may be available, which would allow a simulation of the plume shape using a GCM \(plume lifetime can be inferred from the plume  s spectroscopically determined composition discrepancy between the real plume shape and the GCMderived shape used for the test plumes in the ECM analysis of the observational data. We simulated this uncertainty by adopting our real plume shape from Eqn. 1 \(circular the test plume from Eqn. 2 \(zonally asymmetric expect this difference to result in a generally poorer solution. In fact, the results in Table 4 for lower surface coverage \(1 and 10 day campaigns error with coverage is not certain to be monotonic Interestingly, the errors in localizing the source when there is a reasonably high degree of surface coverage by the measurements are comparable to the results in Table 1 \(both cases assuming noise-free measurements reasonable, but not perfect, understanding of the observed plume shape, a campaign length of several tens of days or more, again, brings the uncertainty in plume source location to within the range of a landed rover Table 4:  Best-fit latitude and longitude positions for different campaign lengths, but assuming different actual and test plume shapes Campaign Length days Best-Fit Latitude   Best-Fit Longitude  Error [km 1 -0.5361 -0.2151 34.3 10 -0.1574 1.1961 71.5 100 -0.1692 0.2477 17.8 1000 -0.1389 0.2569 17.3  5. FUTURE PLANS A more ambitious implementation of this scheme would incorporate output of a full GCM in lieu of our basic analytic test plume model in the ECM framework. With this approach, the set of ECM guesses in a particular generation would spawn a suite of GCM model runs, with the  best guess  solutions from the completed GCM runs identified by the ECM and used to prepare the next generation of guesses.  Such an approach would be entirely autonomous and the best application of the GCM to the problem In the simplified demonstration herein, the importance of the ECM is somewhat understated.  Indeed, for simple plume shapes, a reasonable guess of plume location can be made directly by identifying the peak value in the observation set. This is an unfortunate side effect of this simple, two-gene demonstration.  The real value of the ECM manifests itself when we consider a parameter space of multiple genes.  Under more complex scenarios \(multiple plumes, various lifetimes, etc certainty, isolate individual plume locations. If the steadystate assumption is not valid, and the plume is evolving with time, the age of the plume becomes an additional gene we must consider.  Preliminary exploration of plume age as a third gene has begun, and we have obtained positive results from the ECM.  Further work is necessary to incorporate additional genes, and will be pursued in future work 6. CONCLUSIONS We have discussed the framework of a novel new approach to finding atmospheric plume source locations on planetary surfaces from orbit by using existing numerical analysis methods. This approach relies on the ability of genetic algorithms to quickly identify global best-fit solutions to a multivariate problem. We have found that, under idealized conditions, a source location can be identified to within tens of km on the surface within a couple dozen iterations of the ECM system using idealized plume shapes, even with 


ECM system using idealized plume shapes, even with limited spacecraft observations to constrain the system Error is minimized by increasing the number of observations, and by reducing the instrument uncertainty 12 Additionally, development of instruments with narrower footprints may contribute to a decrease in the source location error. Error due to uncertainty in the knowledge of the observed plume shape is reduced when observations are acquired in a long duration campaign. Plans for future development of this approach have been outlined, which involve the incorporation of general circulation models to supplant the idealized plumes used in the present study. The benefits of the genetic algorithms used here are maximized when multiple genes are employed, rather than just latitude and longitude. Examples of these genes would be parameters such as plume age, species lifetime and time evolution of source emission flux This approach can be of great value to planetary exploration programs, which seek high-precision identification of regions of interest on planetary surfaces to target future landed missions. By applying the methods presented here to realistic atmospheres, we can augment the direct observations made by orbiting spacecraft with a robust indirect approach, saving both time and money 7.  ACKNOWLEDGEMENTS The research described in this publication was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration REFERENCES 1] V.A. Krasnopolsky, J.P. Maillard and T.C. Owen  Detection of Methane in the Martian Atmosphere Evidence for Life  Icarus 172, 537-547, 2004 2] M.J. Mumma, G.L. Villanueva, R.E. Novak, T Hewagama, B.P. Bonev, M.A. DiSanti and M.D. Smith  Absolute Measurements of Methane on Mars: The Current Status  Proceedings of the American Astronomical Society Division of Planetary Sciences Bull. Amer. Astron. Soc. 38, 471, 2007 3] M. Mumma, G. Villanueva, R.E. Novak, T. Hewagama B.P. Bonev, M.A. DiSanti and M.D. Smith  Absolute Measurements of Methane on Mars: The Current Status   Mars Atmosphere: Modeling and Observations Workshop, #9099, 2008 4] V. Formisano, S. Atreya, Th. Encrenaz, N. Ignatiev and M. Giuranna  Detection of Methane in the Atmosphere of Mars  Science 306, 1758-1761, 2004 5] D.W. Beaty, M.A. Meyer and the Mars Advance Planning Group  2006 Update to Robotic Mars Exploration Strategy: 2007-2016  24 pp., 2006 6] D.J. McCleese and the Mars Advance Planning Group  Robotic Mars Exploration Strategy: 2007-2016  33pp 2006 7] MEPAG  Mars Scientific Goals, Objectives Investigations and Priorities: 2006  J. Grant, ed., 31pp 2006 8] R.J. Terrile, et al  Evolutionary Computation Technologies for Space Systems  IEEE Aerospace Conference Proceedings, Big Sky, MT, March 2005 9] S. Lee, C.H. Lee, S. Kerridge, C.D. Edwards and K.-M Cheung  Orbit Design and Optimization Based on Global Telecommunication Performance Metrics  IEEE Aerospace Conference Proceedings, Big Sky, MT, March 2006 13 BIOGRAPHY Michael A. Mischna is an atmospheric scientist with the Jet 


atmospheric scientist with the Jet Propulsion Laboratory focusing on the atmospheres and climates of terrestrial planets. He was the EDL Atmosphere Team Lead for the Mars Phoenix Mission. His research interests include longterm evolution of the martian climate, the role of greenhouse gases in providing habitable environments and surfaceatmosphere interactions. Dr. Mischna has a B.S. in atmospheric science from Cornell University, an M.S. in meteorology from the Pennsylvania State University and an M.S. and Ph.D. in Geophysics and Space Physics from the University of California, Los Angeles  Seungwon Lee is a member of the technical staff at the Jet Propulsion Laboratory. Her research interests include genetic algorithms, lowthrust trajectory design nanoelectronics, quantum computation, parallel cluster computation and advanced scientific software modernization techniques. Dr. Lee received her Ph.D. in Physics from the Ohio State University in 2002 Her work is documented in numerous journals and conference proceedings  Mark Allen is supervisor of the Earth and Planetary Atmospheres Group and Principal Scientist at the Jet Propulsion Laboratory. His research interests include remote detection of trace atmospheric species, and the role of atmospheric chemistry on the atmospheres of Mars, Venus and Titan. He was the PI of the MARVEL Mars Scout proposal, and is a co-investigator on the MIRO microwave instrument on the Rosetta Orbiter. Dr Allen has a B.A. from Columbia University and received a Ph.D. in Chemistry from the California Institute of Technology in 1976     Richard J. Terrile created and leads the Evolutionary Computation Group at NASA  s Jet Propulsion Laboratory. His group has developed genetic algorithmbased tools to improve on human design of space systems and has demonstrated that computer aided design tools can also be used for automated innovation and design of complex systems. He is an astronomer, the Mars Sample Return Study Scientist, the JIMO Deputy Project Scientist and the co-discoverer of the Beta Pictoris circumstellar disk. Dr. Terrile has B.S. degrees in Physics and Astronomy from the State University of New York at Stony Brook and an M.S. and a Ph.D. in Planetary Science from the California Institute of Technology in 1978 14  pre></body></html 


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





