 The Research of Improved Apriori Algor ithm for Mining Association Rules Sheng Chai 1, 3 Jia Yang 2 Yang Cheng 1  1 College of Computer, Sichuan University, Sichuan Chengdu, 610064, China 2 University of Electronic Science and Technology of China, Sichuan Chengdu, 610054, China 3 Dept. Computer, Chengdu Aeronautic Vocational and Technical College, Sichuan Chengdu, 610021 China Chai_sheng@hotmail.com  ABSTRACT The efficiency of mining association rules is an important field of Knowledge Discovery in Databases. The Apriori 
algorithm is a classical algorithm in mining association rules. This paper presents an improved Apriori algorithm to increase the efficiency of generating association rules This algorithm adopts a new method to reduce the redundant generation of sub-itemsets during pruning the candidate itemsets, which can form directly the set of frequent itemsets and eliminate candidates having a subset that is not frequent in the meantime. This algorithm can raise the probability of obtaining information in scanning database and reduce the potential scale of itemsets Keywords data mining, association rule, Apriori algorithm, frequent itemset  
 1. INTRODUCTION  Mining association rule is one of main contents of data mining research at present, and emphasizes particularly is finding the relation of different items in the database How to generate frequent itemsets is the key and core. It is an important aspect in improving mining algorithm that how to decrease itemsets candidate in order to generate frequent itemsets efficiently   In classical Apriori algorithm, when candidate generations are generated, the algorithm needs to test their occurrence frequencies. The manipulation with redundancy result in high frequency in querying, so tremendous amounts of resources will be expended whether in time or in space  
 In this paper, an improved algorithm is proposed for miming the association rules in generating frequent k-itemsets. Instead of judging whether these candidates are frequent itemsets after generating new candidates this new algorithm finds frequent itemsets directly and removes the subset that is not frequent, which based on the classical Apriori  2. ASSOCIATION RULES DESCRIPTION  The association rule mining were first proposed by Agrawal and etc. It can be formal defined as follow Given a transaction database DB, I={i l i 2 i 3 i n is a 
set of items with n different itemsets in DB, each transaction T in DB is a set of item \(i.e.itemsets T  I  Definition 1: Let I={i l i 2 i 3 i n be a set of items then D={<Tid, T>|T  I} is a transaction database, where Tid is an identifier which be associated with each transaction  Definition 2: Let X  I, Y  I, and X Y  we called X Y as association rule  Definition 3: Suppose c is the percentage of transaction 
in D containing A that also contain B, then the rule X Y bas confidence c in D. If s is percentage of transactions in D that contain A B, then the rule X Y has support s in D  Definition 4: Hypothesis X  I, minsup is the smallest support. If the frequency of X is greater than or equal to minsup, then X is called as frequent itemset, otherwise X is called non-frequent itemset  Classical Apriori algorithm  1\ C l candidate I-itemsets 2\ L 1 c  C 
1 c.count minsup 3\OR \(k=2;L k-1  k++\ DO BEGIN 4\  C k apriori-gen\(L k-1  5  FOR all transactions t D DO BEGIN 6 C t subset \(C k t 7  FOR all candidates c C t DO 8 c.count 9  10 k c C k c.count minsup 11\D  12\nswer 
L k   The association rule mining is a two-step process   1.Find all the frequent itemsets in the transaction database. If support of itemset X, support\(X minsup then X is a frequent itemset. Otherwise, X is not a frequent itemset   2.Generate strong association rules from frequent itemsets. For every frequent itemset A, if B  A, B   and support \(A\port\(B minconf, then we have 1-4244-0885-7/07/$20.00 ©2007 IEEE  


 association rule B A-B\ The second step is relatively easy, and its algorithm generation can be found in the reference  The present focuses in research are to find highly efficient algorithms in the first step  3. PROBLEM DESCRIPTION  In Apriori algorithm, from C k to L k have two steps : \(1 Pruning itemsets according to L k-1 2\uning itemsets according to minsupport  First, the set of frequent 1-itemsets is found. This set is denoted L 1 L 1 is used to find L 2 the set of frequent 2-itemsets, which is used to find L 3 and so on, until no more frequent k-itemsets can be found, and then algorithm ceases. In the cycle k, a set of candidate k-itemsets is generated at first. This set of candidates is denoted C k Each itemset in C k is generated by joining two frequent itemsets that belong to L k-1 and have only one different item. The itemsets in C k are candidates for generating frequent sets, and the ultimate frequent itemsets L k must be a subset of C k Every elements in C k  should be identified in business database to decide wither to join L k   The question of Apriori algorithm is that the set of candidate itemsets C k is generated from L k-1 Every itemset in C k is tested whether its all k-l subsets constitute a large k-1 itemset or not, if one of k-1 itemsets is not in L k-1 itemsets, the super itemset of this k-l itemset can be deleted. That is, every time a k itemset is constituted, Apriori must scan all L k-1 itemsets Because of may scan large database many times in this way, the identifying processes are the bottleneck of the Apriori algorithm  Apriori algorithm may need to generate a huge number of candidate generations. Each time when candidate generations are generated, the algorithm needs to judge whether these candidates are frequent item sets. The manipulation with redundancy result in high frequency in querying, so tremendous amounts of resources will be expended whether in time or in space  4. IMPROVED ALGORITHM   4.1 Principle of the Improvement  The improvement is mainly way of reducing query frequencies and storage resources. We design an improved Apriori algorithm that mines frequent itemsets without new candidate generation. For example, in this algorithm we compute the frequency of frequent k-itemsets when k-itemsets are generated from k-1\-itemsets. If k is greater than the size of transaction T, there is no need to scan transaction T which is generated by \(k-1\-itemsets according to the nature of Apriori algorithm, and we can remove it  4.2. Algorithm in Detail  To implement the improvement, the improved algorithm is described as follow steps   1\ function apriori-gen\(L k-1 to generate candidate k-itemsets by frequent \(k-1\sets  2\Judging whether C is joined into candidate k-itemsets. It is processed by calling function has_infrequent_subset\(C, L k-1 If the return value is true it means the sets aren’t frequent itemsets and should be remove in order to raise efficiency. Otherwise, scan database D  3\ frequency of frequent k-itemsets is computed when k-itemsets are generated by \(k-1\itemsets. If k is greater than the size of transaction T, there is no need to scan transaction T which is generated by \(k-1\itemsets according to the nature of Apriori algorithm, and we can delete it   4\If the size of transaction T is greater than or equal to k, then function subset\(C k t\ is called, which finds frequent itemsets using an iterative level-wise approach based on candidate generation  Algorithm1: Apriori. Find frequent itemsets  L 1 large 1-itemsets FOR \(k=2; L k-1  k++\ do BEGIN C k apriori-gen\(L k-1 min_sup new candidate generation END RETURN L=U k L k   Algorithm2: apriori-gen\(L k-1 enerate candidate generation FOR all itemset p L k-1 DO BEGIN   FOR all itemset q L k-1 DO BEGIN IF p.item 1 q.item 1 p.item k-2 q.item k-2  p.item k-1 q.item k-1 THEN BEGIN c= p q Put the k-1 elements in q join to the back of q IF has_infrequent_subset\(c, L k-1 HEN delete c Delete the candidate generation that include non-frequent subtests ELSE FOR all transactions t  D DO BEGIN// Scan D and accumulate itemsets IF Count\(t\<k THEN  delete t ELSE BEGIN C t subset\(C k t Take out the subsets of candidate generation included in transaction t 


 FOR all candidates c  C t DO  c.count END END L k c  C k c.count  minsup END END END END Return C k   Algorithm3: has_infrequent_subset\(c, L k-1 Judge the elements of candidate generation FOR all \(k-1\-subset s of c DO  IF s L k-1 THEN Return TURE Return FALSE  Owing to the improved algorithm mines frequent itemsets without candidate generation, the query frequency falls to about half level comparing with Apriori algorithm. The size of database is reduced meanwhile, the storage space and computing time are saved  5. EXPERIMENT RESULTS  This is an example based on the transaction database, D of Figure 1. There are nine transactions in this database that is, |D|=9.We use the improved Apriori algorithm for finding frequent itemsets in D   Figure 1 Generation of candidate itemsets and frequent itemsets  1.Scan D for count of each candidate  In the first iteration of the algorithm, each item is a member of the set of candidate 1-itemsets, C 1 The algorithm scans all of the transactions in order to count the number of occurrences of each item  2.Compare candidate support count with minsup  Suppose that the minimum transaction support count required is 2. The set of frequent 1-itemsets, L 1 can then be determined. It consists of the candidate 1-itemsets satisfying minimum support  3.Generate C 2 candidates from L 1 and scan D for count of each candidate  To discover the set of frequent 2-itemsets, L 2 the algorithm generates a candidate set of 2-itemsets, C 2  And then the transactions in D are scanned and the support count of each candidate itemset in C 2 is accumulated, as shown in the table of C 2 in Figure 1  4. Compare candidate support count with minsup  The set of frequent 2-itemsets, L 2 is then determined consisting of those candidate 2-itemsets in C 2 having minimum support. Then D 2 was determined from L 2   5. Generate C 3 candidates from L 2 and scan D 2 for count of each candidate  First let C 3 L 2 L 2 I 1 I 2 I 3 I 1 I 2 I 5 I 1 I 3 I 5  I 2 I 3 I 4 I 2 I 3 I 5 I 2 I 4 I 5 Based on the Apriori property that all subsets of a frequent itemset must also be frequent, it can determine that the four latter candidates cannot possibly be frequent, therefore remove them from C 3 thereby saving the effort of unnecessarily obtaining their counts during the subsequent scan of D 2 to determine L 3   6. Compare candidate support count with minsup  The transactions in D 2 are scanned in order to determine L 3 consisting of those candidate 3-itemsets in C 3 having minimum support  7.The algorithm uses L 3 L 3 to generate a candidate set of 4-itemsets, C 4 Although the join results in {{I 1 I 2 I 3  I 5 this itemset is pruned since its subset {{I 2 I 3  I 5 is not frequent. Thus, C 4   and the algorithm terminates, having found all of the frequent itemsets  6. CONCLUSIONS  In this paper, the improved Apriori algorithm is proposed to update the classical Apriori algorithm Through pruning candidate itemsets by the infrequent itemsets, the present algorithm can reduce the number of database scanning and the redundancy while generating subtests and verifying them in the database Validated by the experiments, it can obtain higher efficiency   


 REFERENCES   R. A g raw a l T   I m ielins k i, a nd A  Sw ami M in i n g  Association Rules Between Sets of Items in Large Databases”, In Proceedings of the ACM SIGMOD Conference on Management of data 207-216, May 1993  R  A g r a w a l and R  S r ika n t F a s t A l gor ithm s for  Mining Association Rules”, In Proc. VLDB 1994 pp 487-499  A  S a va sere E. Om iec i n s k i  and S  N a vat h e  A n  Efficient Algorithm for Mining Association Rules in Large Databases”, In VLDB’95 pp.432-443, Zurich Switzerland  P i n gpi ng W e i  Cu i r u W a ng, Ba oyi W a ng  Zhe n x i n g  Zhang, “Data Mining Technology and Its Application in University Education System Computer Engineering June 2003, pp.87-89   a oro ng Q i u, X i a o mi n g Ba i, Lip i ng Zhan g A n Apriori algorithm based on granular computing and its application in Library management system Control & Automation 2006, pp.218-221   


the cut, and this cu t exists in any cases Therefore, when there are some elements infrequent, all its ch ildren nodes are also infrequent; they are all below this cut and can be trimmed out of the search space. By doing so we can reduce the search space dramatically thus improve the mining efficiency. Lets consider node es search tree in figure 3, the node e will be counted firstly by the depth-first way, then the node eb is counted, based on the cut in figure 3 the two nodes are frequent, thus they are stored into LFCI \(storing the local FCI then count ebds support, because T\(ebd 1 is less than the minimum support threshold which is 2  it is infrequ ent, and all its children nodes rooted by ebd need to be pruned away. In the rest of the search tree, ebc will be counted then, using the same way until all frequent closed itemsets are mined. Finally, we can get LFCI = {e: 3, ebc: 2, edf: 2   Figure 3. The search space tree of node e 3.3 Algorithm of FCI-Miner Algorithm Mining frequent closed itemsets algorithm FCI-Miner  Input The minimum support threshold and the improved FP-Tree  Output FCI 1. FCI = NULL 2. For each x Htable //x is element in Htable 3. {Construct xs bit object group 4 x_Miner Bitsets Begin LFCI CurItem mine all frequent close d itemsets with suffix x 5. GetFCI \(LFCI, FCI  6. Return FCI Procedure x_Miner \(Bitsets, Begin, LFCI CurItem\ //Begin is the cu rrent nodes position in bit object, End is the position having the top position value, CurItem is the current items position value 1.   Tempfci = CurItem; // Tempfci store the position value of the current item 2   for \( i = Begin; i End; ++ i   3.      {if \(\(T\(i\T\(i+1\ount  4.          {GetFCI \(Tempfci, LFCI 5.           x_Miner \(Bitsets, i+1, LFCI Tempfci +item \(i+1\s position value Procedure GetFCI \(TFCI, FCI 1. if \(each M TFCI\ isnt the subset of some elements in FCI with the same support 2.      {FCI = FCI M 3.        Delete elements which are the subset of M in FCI with the same support 3.4 Experimental Evaluation  We conduct all experiments on a notebook of TM\Duo 1.6 GHz CPU and 1024MB of ma in memory, running Microsoft windows XP. All the codes are written in Microsoft Visual C ++ 6.0. We compare FCI-Miner with Mafia \(Mafia can download from the website http://fimi.cs.helsinki.fi e chose several real and synthetic databases for testing the performance of FCI-Miner. The real databases are all taken from the website of UCI Machine Learning Database Repository  http://www.ics.uci.edu/~ml earn/MLRepository html real databases are very dense. We also chose a few synthetic databases, which can be generated by the generator from IBM Almaden Centre website http://www.almaden.ib m.com/cs/quest/demos.ht ml Database T25I20D10K has 50 items, T25 shows that the average transaction length is 25 I20 shows that the average maximal potentially frequent itemset size is 20; D10K shows that there are 10K transactions. These databases are sparse Table4. Database characteristics Table 4 above shows the characteristics of all databases used in our evaluation. It shows the number of items, the average transaction length Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


and the number of transaction in each database The following figures show the testing results on these databases   0 0.2 0.4 0.6 0.8 1 1.2 0.1 0.5 1 5 10 25 40 60 65 90 Min Support Total time \(s Mafia FCI-Mine r Figure 4. FCI-Miner versus Mafia on Mushroom 0 0.3 0.6 0.9 1.2 75 80 85 90 95 Min Support Total time \(s Mafia FCI-Miner Figure 5. FCI-Miner versus Mafia on Chess  0 5 10 15 20 75 80 85 90 95 Min Support T otal tim e \(s Mafia FCI-Mine r Figure 6. FCI-Miner versus Mafia on Connect-4 0 0.2 0.4 0.6 0.8 1 1.2 1.4 40 50 60 70 80 90 Min Support Total time \(s Mafia FCI-Miner Figure 7. FCI-Miner versus Mafia on T25I20D10K 0 5 10 15 20 70 75 80 85 90 95 Min Support T otal tim e \(s Mafia FCI-Miner Figure 8. FCI-Miner versus Mafia on T50I20D100K  As shown in figure 4 to figure 8 above, we tested the total execution time of Mafia and FCI-Miner on five databases described in table 4 In figure 4, FCI-Miner outperforms Mafia range from 0.001 to 1. When te sted on dense databases such as Chess and Connect-4, FCI-Miner performs well when minimum support threshold is greater than 0 75. On T25I20D10K and T50I20D100K, FCI-Min er also outperforms Mafia in most cases 4 Conclusions In this paper, we presented and evaluated an efficient algorithm for mining frequent closed itemsets in transaction database. FCI-Miner adopts a novel data structure for compressing crucial information abou t frequent itemsets which can reduce the database greatly, and also uses an effective method to generate candidate itemsets and count their support without generating conditional FP-Trees. The experimental results show that it is an effective and efficient algorithm  5. References  Agrawal R. Srikant Fast algorithm for mining  In: Proc of the 20th In tl Conf. on VLDB94. Santiago: Morgan Kaufmann 1994: 487-499  R. Bay ardo Ef ficiently mining long patterns from datab Proc. of 1998  AC M SIGMOD Intl Conf. on Management of Data. New York: ACM Press, 1998: 85 93  N. Pasquier  Y  Bastide, R   T a ouil and L. Lakhal  Discovering frequent closed itemsets for association rules. Proceeding of the 7th Intl Conference on Database Theory. 1999: 398-416  D. Burd ick M. Calimlim J. Gehrk e. Maf ia: A  maximal frequent i temset algorithm for transactional da Proceeding of the 17th  Intl Conference on Data Engineering. Heidelberg IEEE Press, 2001: 443-452   Hsiao CJ. CH ARM: An ef ficient algorithm for closed itemset mining. Proceeding of the 2nd SIAM Intl Conference on Data Mining Arlington: SIAM, 2002: 12-28  R. CLOSET: An efficient algorithm for mining frequent closed itemsets Proceeding of the 2000 ACM SIGMOD Intl Workshop on Data Mining and Knowledge Discovery. Dallas: ACM Press, 2000: 21-30  R y mon R. Search thro ugh s y stematic set enumeration. Proceeding of the 3rd Intl Conference on Principles of Knowledge Representation and Reasoning. 1992: 539-550  Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


                                                                                                                 
456 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efÞcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efÞciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efÞciency When the clustering model is available it is a signiÞcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques SufÞcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efÞcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efÞcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conÞdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conÞdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conÞdence The sufÞcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efÞcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efÞciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufÞciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70Ð81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207Ð216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487Ð499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145Ð154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146Ð153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9Ð15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597Ð600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern ClassiÞcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155Ð162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512Ð521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1Ð12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265Ð278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283Ð304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476Ð482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559Ð563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220Ð231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13Ð24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10Ð17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549Ð550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188Ð201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259Ð283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909Ð921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305Ð345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432Ð444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conÞdence Intelligent Data Analysis  9\(4\:381Ð395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49Ð73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407Ð419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1Ð12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki EfÞciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642Ð658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483Ð490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194Ð203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344Ð353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efÞcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103Ð114 1996 
618 
618 


