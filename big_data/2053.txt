Session F4H 978-1-4244-1970-8/08/$25.00  2008 IEEE  October 22 – 25, 2008, Saratoga Springs, NY 38 th ASEE/IEEE Frontiers in Education Conference F4H 1 Work in Progress - Programming Misunderstandings Discovering Process Based On Intelligent Data Mining Tools  Paola Britos, Elizabeth Jiménez Rey, Darío Rodríguez, Ramón García-Martínez CAPIS-ITBA, III-FI-UNLP, LSI-FI-UBA, {pbritos, drodrigu, rgm}@itba.edu.ar, ejimenez@fi.uba.ar   Abstract We present research work in progress that focuses on data mining tools used for helping teachers to apply a three step knowledge discovering process to diagnose students’ misunderstandings \(and their causes related to their programming errors  Index Terms TDIDT algorithms, bayesian networks, data mining, students’ misunderstandings  diagnosis I NTRODUCTION  Data mining has been addressed as an effective way of discovering new knowledge from data sets of educational processes, data generated by learning systems or experiments, as well as how discovered information can be used to improve adaptation and personalization [1   Am ong interesting problems data mining can help to solve determining which are common learning styles or strategies   4 p r e d i c t i ng t h e k n o w l e dge an d i n t e rest s of a use r  based on past behavior, partitioning a heterogeneous group of users into homogeneous clusters or detecting misconceptions in learning processes One of the most common techniques of data mining are the decision trees \(TDIDT\ used for discovering knowledge in rule format which constitutes a model that represents the knowledge domain subjacent to th e available examples of it A Bayesian network is a directed acyclic graph in which each node represents a variable and each arc represents a probabilistic dependency which specifies the conditional probability of each variable given its parents; the variable to which the arc points to is de pendent \(cause-effect variable in the origin of this one [5 To discover common learning misconception of learners research described in  em pl oy t h e associ a t i on r u l e t o dat a  mine learner profile for diagnosing learners’ common learning misconception during learning processes. The association rules that occurring misconception A implies occurring misconception B can be discovered utilizing the proposed association rule learning diagnosis approach In this paper we present research work in progress that focuses on data mining based t ools for helping teachers to diagnose students’ misunderst andings \(and their causes related to their programming errors. To do this some considerations of what to me asure and why are presented in section problem domain a three step knowledge discovering process is presented in the homonymous section, a real case taken from an introductory programming course students population is presented in section case example and finally preliminary conclusions and future research are drawn P ROBLEM D OMAIN  To model student’s misconceptions a list of components to be evaluated has been id entified. Some of these components, its justification and the rank of possible values are described in the following paragraphs Student applies refinement method this component looks for to diagnose if student has acquired the analytical ability of decomposing a complex problem into more simple parts divide and conquer strategy\e tabulated answers may be: <yes, no, incomplete Student discovers algorithm this component looks for to evaluate if student has developed the ability to put in sequence programming primitive sentences in a logic way according to problem’s objective to be solved. The tabulated answers may be: <yes, no Student obtains a generalized solution this component looks for to evaluate if st udent has incorporated the algorithm concept as a procedure of resolution of a problem family class to which the proposed problem belongs. The tabulated answers may be: <yes, no Student uses begin/end correctly this component looks for to evaluate if student has incorporated the concept of primitive sentences block or compose sentence block that have to be executed conceptua lly as a unique sentence. The tabulated answers may be: <yes, no Students obtain a logic solution this component looks for to evaluate if student has the maturity to obtain a solution design with grater quality. The tabulated answers may be good, regular, bad Student controls repe titive cycle end condition this component looks for to evaluate if student has incorporated the algorithm concept as a procedure that has to provide a solution in a finite time verifying that an iterative structure effectively ends. The tabulated answers may be: <yes, no Student uses logic connectors correctly this component looks for to evaluate if student has detected the relation among the type of problem and the conditions which rule the problem solution generalization. Blocking conditions is an issue to be evaluated in this component. The tabulated answers may be: <yes, no 


Session F4H 978-1-4244-1970-8/08/$25.00  2008 IEEE  October 22 – 25, 2008, Saratoga Springs, NY 38 th ASEE/IEEE Frontiers in Education Conference F4H 2 Student develops an infinite cycle this component looks for to evaluate if student controls iterative cycles due to verify the value change of variables used in conditional expression associated to iterative cycle en d. The tabulated answers may be: <yes, no K NOWLEDGE D ISCOVERING P ROCESS  To discover programming misconceptions of students, this research focuses on using intelligent systems based data mining tools: rules induction by TDIDT algorithms and Bayesian networks. These tools are used in a three step knowledge discovering process  First step deals with building a data base \(manually by instructor\n a standard ch aracterization of each student and her/his programming style and misconceptions. This characterization focuses on as pects of: student program development methodology, student developed program functionality, student program design quality Second step deals with discovering rules \(by using TDIDT algorithms\ch establish a relation among programming misconceptions with possible cau ses. The obtained rules are applied to confirm or deny courseware structure teachers hypothesis. Experiment results indicate that applying this step, teacher can correctly discover learners’ common misconception causes Third step deals with discover ing the weight each cause has on each misconception \(by using Bayesian networks\his allows establishing a rank of importance of the causes of misconceptions in order to propose recovering teaching strategies C ASE E XAMPLE  We have carried on a preliminary experiment on a first course of programming with a population of 45 students Professor of this course wish to explore which misconceptions are related to th e fact that student doesn’t discover correctly the exer cise associated algorithm student discovers algorithm = no  From the database developed in Step 1 of the course on programming examinations, we apply Step 2: and using TDIDT algorithm we obtain the following set of rules  IF student applies refinement method = no THEN student develops an infinite cycle =yes  IF student uses logic connectors correctly = no THEN student develops an infinite cycle =yes  IF student develops an infinite cycle =yes THEN student uses logic connectors correctly = no  IF student uses begin/end correctly =no THEN student uses logic connectors correctly = no  IF student uses logic connectors correctly = no THEN student controls repetitive cycle end condition = no   IF student controls repetitive cycle end condition = no  THEN student obtains a logic solution = bad  IF student obtains a logic solution = bad THEN student obtains a generalized solution = no  IF student obtains a generalized solution = no THEN student discovers algorithm = no  From this set of rules we can identified that “student discovers algorithm = no” has three possible causes student applies refinement method = no”, “student uses logic connectors correctly = no” and “student uses begin/end correctly =no”. We apply Bayesian Networks BN\n Step 3 to determine which of the three identified causes has the greater impact on the considered problem student doesn’t discover correc tly the exercise associated algorithm”. The results obtained are CAUSE  BN WEIGHT  student applies refinement method = no 84,88  student uses logic connectors correctly = no 56,49  student uses begin/end correctly =no 09,34  P RELIMINARY C ONCLUSIONS AND F UTURE R ESEARCH  Data mining applied to help teachers to discover students misunderstandings causes is a promising issue to explore as a new diagnosis tool area. Current results are promising but not conclusive on a basis of a set of 45 student’s examination records of a Pascal first level course. Next step will be to tune the process over a population integrated by 300 students of the introductory programming course Pascal programming language\of the Engineering School of the University of Buenos Aires R EFERENCES    Beck, J. , Calde r s, T P echenizkiy, M., Viola, S. 2007 Workshop on Educational Data Mining ICALT'05: 933-934   Schulte C  Bennedsen J 200 6  What do teachers teach in introductory programming ICERW´06: 17-28   Salgueiro, F., Cat a ldi, Z., Britos, P Sierra E y Garc a Martínez R  2006 Selecting Pedagogical Protocols using SOM Research in Computing Science Journal, 21: 205-214   Br itos P  Cataldi Z   Sier ra, E., García-Martínez, R. 2008 Pedagogical Protocols Selection Automatic Assistance LNAI 5027 \(in press    Felgaer  P  Britos P and García-Martínez, R. 2006 Prediction in Health Domain Using Bayesian Network Optimization Based on Induction Learning Techniques International Journal of Modern Physics C 17\(3\: 447-455    Chen C  Hsieh Y 2005  Mining Learner Profile Utilizing Association Rule for Common Learning Misconception Diagnosis  ICALT'05: 588-592 A UTHOR I NFORMATION  Paola Britos Associate Professor, Software & Knowledge Engineering Center, Buenos Aires Institute of Technology Elizabeth Jiménez Rey Assistant Professor, Engineering School; Postgraduate Student, Intelligent Systems Laboratory, University of Buenos Aires Darío Rodriguez Assistant Professor, Software Knowledge Engineering Center Buenos Aires Institute of Technology Ramón García-Martínez Full Professor, Software Knowledge Engineering Center Buenos Aires Institute of Technology; Director, Intelligent Systems Laboratory University of Buenos Aires   


little ones as flexibly as possible. Furthermore, theorem1 can be used to prune the search space The search space for item set I  a  b  c  d is S  I  I t  can be partitioned into some little ones step by step. The iterative results of a hierarchy of search space are shown in figure 1 For a given item set I in database D and the minimum support threshold min_sup the tas k of mining FI or MFI is in the follow: finding the set          minsup X Supp and I X X   in the search space S  I   4.   Discovering Max-itemsets 4.1 Search and Pruning Strategy In general, it is possible to search for the maxitemsets either top-down or bottom-up The bottom-up approach is good for the case when all max-itemsets are short, and the top-down approach is good when all  maxitemsets are long. If some max-itemsets are long and some are short in the mining database, then both search approaches will not be efficient. A key idea of our hybrid approach is the use of information gathered in the search in the bottom-up to prune search space during the topdown search. It uses infrequent itemsets found in the search in the bottom-up to prune search space during the top-down search. Some ef ficient prune technologies will be discussed later Lemma1 Let I Y X   and I u  and    uY X S  be a search space on X If the itemset R  Xu  is infrequent, the space S can be pruned by R And the remained space is   Y X S    The theorem2 shows how to prune the search space  X  Y  b y t h e inf req u e nt it em set R  Theorem2 Let X and Y be distinct itemsets, and    Y X S  be a search space. If there exists an item Y u  satisfied that itemset Xu R  if infrequent the partition of           u Y X u Y Xu  012  is the most efficient ones  Proof For such search space    Y X S  the size of S lies on value  Y Let      2 1 k i i i Y   there are 2 k  subsets in S that is S 2 k The original search space S  can be decomposed into   012 k j k j j i i Xi 1 1     by theorem1. For the j th subspace    1 k j j j i i Xi S  012  if the assumable infrequent itemset is Xi j then S j can be pruned entirely by lemma1. Obviously, when j 1\(i.e i 1  u  value of j S comes to the top. The result follows by theorem1 Theorem3 Let    Y X S  be a search space, and X  be a k 1\-itemset. Let k L  be the set of all infrequent k itemsets. If there exists such a itemset k L Z   satisfied with X Z  search space S can be pruned by k L  And the resulting space     R Y X S           k L Z and X Z X Z R      Proof  015 Y u  there exists              u Y X u Y Xu Y X  012   by Corollary1 If there has no k L Z   satisfied with X Z  i.e k L Xu   the search space S can not be pruned any part by Z  Let      2 1 M i i i R   the result follows by pruning S via R i j   M j   1 uentially as in Corollary1 Theorem4 Let   Y X S  be a search space, and X  be a k 1\-itemset. Let k L  be the set of all infrequent k itemsets, and    k L Z Z X M     The pruned search space S  is only 2 M of its original search space S  Proof When M 0\(2 M 1\y itemset in k L  can not used to pruned the original search space S When  M 0, assume       k L Z and X Z X Z R      R Y U   and J U  then      RU X Y X S   As we have known M R  and J M S 012  2 holds. By theorem3, we conclude that S can be pruned by k L  to   U X S    i.e J S 2    M S S    2  For search space S  X  Y   i f t h e num be r  of  infrequent itemsets containing X is large, the scale of S  will be reduced rapidly  4.2 The Hybrid Max-Itemsets Search Algorithm  There are two phases in this hybrid approach: the search in bottom-up direction and the other in top-down direction for every pass. Max-itemsets are enumerated in both bottom-up and top-down directions Consider a pass k the set of frequent k itemsets L k  and the set of infrequent k itemsets L k are to be classified in the bottom-up direction. This procedure repeatedly uses Apriori-gen algorithm to generate candidates like the Apriori u r i ng th e k th pass, every search space S  X:Y w h e r e  X is an itemset of size k 1 can be decomposed into some little pieces, whose ancestors are k itemsets. For search space S  X top-down procedure check whether the border element   ac:d   a:d bc:d b  ab:cd   abcd      d       Figure 1 Hierarchy of search space on I    a,b,c,d  K 0 K 2 K 1 K 3 
548 


i.e Y X  f S is frequent firstly, if not S is decomposed. Implementation of this hybrid approach is shown in algorithm2 Algorithm2. Algorithm for Max-itemsets Mining Procedure MFI Search \(Transaction Set D Item Set I  Begin   T MFI    B MFI  k 1  1 I    C k  I  L k Partition C k  min_sup   L k Partition C k  min_sup   While    k do Top-Down Search    012 1 k  Forall k S   do B S bor  If countsupport B  min_sup then If          T MFI R and B R R then   Decompose S  k  L k  L k        012 012 1 1 k k   Endif Else B MFI MFI T T    Endif Endfor Bottom-Up Search C k 1 Apriori-gen L k  L k 1 Partition C k 1  min_sup   L k 1 Partition C k 1  min_sup  Forall k L X  do If     012    1 k L Y Y X then X MFI MFI B B     Endfor  k  End Return B T MFI MFI   End Procedure: Partition C k  min_sup  classify L k and L k from set of candidate k itemsets C k  Begin   k L    k L  Forall k j C X  do if CountSupport X j   min_sup then j k k X L L    else j k k X L L      Endif Endfor End Procedure: Decompose S  X  Y   k  L k  L k  Begin       Z    k k L and X Z Z P      U   k k L and X U U P        k P V V Y R        k P T T R R     R n   Forall i 1 i  n 1 i o If k i P Xu  and k u u Xu k i i  012  1 then    1 k i i u u Xu  012      Endfor Return   End  5.  Experimental and Analytical Results  The test databases are generated synthetically by an algorithm designed by the IBM Quest project in IBM Almaden Recearch Center, referring to Http://www.almaden.ibm.com/cs/quest/syndata#AssocSynData    In this algorithm bitmap technology is used to count the support of every itemset instead of scanning the entire transaction database. Figure 2 shows the relative times at varying numbers of transactions for databases where the average size of transactions is 10 and the average size of potential max-itemsets is 4            When the average size of transactions or the average size of max-itemsets increase, there has much more itemsets \(or search spaces\e tested. therefore the total time will increase. Figure 3 shows the relative times of this hybrid algorithm at varying minimal supports on the datasets of T15.I8.D10K            To illustrate expandability of this algorithm, we performed an experiment varying the database size from 5K to 20K. The average size of transactions is 10, and the average size of potential max-itemsets is 6. For the experiment we fixed a minimum support of 4%. Figure 4 shows the result for the datasets 1000 3000 5000 7000 9000 7.0 6.0 5.5 5.0 4.5 Minimal Support\(T 15I8D10K Response Time\(sec Two-Way Figure 3 Times varying support thresholds Figure 2. Relative times varying databases 0 50 100 150 200 250 300 350 50K 100K 150K 200K Number of Transactions Responsed times\(Sec List BM 2-BMC 
549 


           As we have shown, there are three search strategies for discovery MFI. Figure 5 shows the relative times of the three approaches for the tests at varying minimal supports on T10.I6.D10K. From the experiments, we could see that when minimal support is greater than 2 the performances of the bottom-up is little better than the hybrid. The main reason is that the number of itemsets generated is small with the increasing of minimal support As the minimal support decreases, MFI becomes longer which results in an increase in the number of counting itemsets. In such a case, the hybrid has performances. We can also see performance of the bottom-up approach is lower than the others. The most primary factor is almost all the max-itemsets are expected to not be long in this T10.I6.D10K dataset. The experiment illustrates the fact that top-down search might be efficient for the  long maxitemsets           6.  Conclusions  We use an improved compacting bitmaps database format. Support of itemset can be counted by means of binary bit vectors intersections, which minimizes the I/O and computing cost. To reduce the disk and main memory space demanding, we break the bitmap down into some little blocks, which can be encoded as a shorter code. The blocks of bitmaps are fairly adaptable. Hence the additional space decreases rapidly The hybrid approach exploits key advantages of both the top-down strategy and the bottom-up ones, which can discovery both longer max-itemsets and the shorter ones in earlier passes. And the infrequent \(or frequent\itemsets discovered in the bottom-up can also be used to prune the search space in the other top-down direction. Furthermore this algorithm can be parallelized easily on this hierarchical search space organization. We note that using L k to prune the search space is not the only technique. If  k k L L   it would be more efficient to decompose and prune the search space using L k rather than ~L k   Acknowledgements This paper is supported by the National Science Foundation \(No.70571057, No. 70771074  References  1   R. Agrawal, T. ImielinSki, A. Swami, Mining association rules between sets of items in large database. Proc. of the ACMSIG2 MOD International Conference on Management of Data Washington, DC.1993, 2 : 207-216 2   Jia wei Han, et al, Frequent pattern mining: current status and future directions, Data mining and Knowledge Discovery, 2007 V\(15\5-86 3   Bayardo R. Efficiently mining long patterns from databases. In: Proc. of the ACM SIGMOD, Intl Conf. On Management of Data. New York: ACM Press. 1998. 85-93 4   Dao-I Lin, Zaki M. Kedem, Pincer Search: A New Algorithm for Discovering the Maximum Frequent Set, Proceedings of the 6th International Conference on Extending Database Technology 1998. 105-119 5   R. Agrawal, C. Aggarwal, Depth first generation of Long patterns, 7th International conference on Knowledge discovery and Data mining. 2000. 108118 6   D. Burdick, M. Calimlim and J. Gehrke, MAFIA: A Maximal Frequent Itemset Algorithm for Transactional Databases, Proc. of the 17th Int'l Conf. on Data Engineering. 2001. 443-452 7   J.P. Han, Y. Yin. Mining frequent patterns without candidate generation. In ACM SIGMOD Conf May 2000. 1-12 8   Guizhen Yang. The complexity of mining maximal frequent itemsets and maximal frequent patterns.In Proceeding of the 2004 ACM SIGKDD international conference on kowledge discovery in databases \(KDD04\Seattle,WA, 344…353 9   Ramesh G, Maniatty WA, Zaki MJ \(2003\ Feasible itemset distributions in data mining: theory and application. In: Proceeding of the 2003 ACM symposium on principles of database systems PODS03\San Diego, CA, 284…295 1  Mikolaj Morzy, Hierarchical Bitmap Index An Efficient and Scalable Indexing Technique for SetValued Attributes, ADBIS 2003, LNCS 2798, 2003 236…252  0 200 400 600 5K 10K 15K 20K Num bers of T ransactions Response Time sec Fi g ure 4. Relative times var y in g  numbers of transaction 0 500 1000 1500 2000 2500 5 4 3 2 1 Minimal Support\(T 10I6D10K Total Time\(sec Two-W ay Bo ttom U p To p D ow n Fi g ure 5. Total times var y in g  Su p p ort thresholds 
550 


0 0.05 0.1 0.15 0.2 0.25 0.3 0 20 40 60 80 100 Error k Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0 100 200 300 400 500 600 700 800 Error k Synthetic data set tau=0.10  tau=0.05  Figure 5 Support estimation error increasing k  high when k is low We now turn our attention to the synthetic data set where we built models with much higher k  In this case error shows a slower rate of decrease The trend also seems asymptotic We can also see there is a clear gap between 002 0  05 and 002 0  10  error roughly grows 100 when 002 decreases by 50 Interestingly enough results are much better for real data sets than for the synthetic data set Error decreasing minimum support Figure 6 shows error growth as 002 decreases The left graph analyzes error for real data sets We can see error growth shows different behavior for each data set Error growth is slow for the Votes data set Error grows almost linearly for the Chess data set Error grows fast for the Mushroom data set The trend indicates we need to build more accurate models with higher k for Mushroom probably not for Chess and not necessary for the Votes data set The right graph analyzes error growth for the synthetic data set TheÞrstmodelat k  800 is twice as accurate as k  400  We can see error grows linearly for high 002 values but it start growing faster at 002 0  1  The trend indicates the growth is not linear but it does not seem exponential 4.4 Comparing Speed and Scalability We rst compare our proposal versus the standard algorithm to mine association rules We then study time complexity and scalability Comparing clustering and A-priori Table 2 compares the efÞci ency of the model with the standard A-priori algorithm We must stress our proposal does not intend to substitute fast association rule algorithms Table 2 Comparing model and A-priori 002 from model clustering+model A-priori 0.20 1 1672 24 0.15 1 1672 43 0.10 1 1672 156 0.05 3 1674 645 0.02 11 1683 3347 0.01 36 1708 14806 32 16 b u t w e i ncl ude t h es e c ompari s ons t o pro v i d e a rel ative performance benchmark We used the synthetic data with n 1 M  The clustering model used had the number of clusters set to k  100  These times include the time to compute exact support on a nal pass The rst column varies 002 the minimum support threshold The second column shows the time to discover frequent itemsets from the model excluding the time to compute the model The third column adds the time to compute the clustering model and the time to mine frequent itemsets Finally the fourth column shows the time for the traditional algorithm As can be seen the clustering model r epresents an efÞcient mechanism to produce all frequent itemsets assuming the model is already tuned and stored That is we assume the model is computed a few times or even once The second column shows clustering the data set takes most of the time In this case the standard algorithm is faster at high support levels but the model becomes faster at low support levels Notice the third column represents a pessimistic case in which the model is recomputed every time Finally we can see the A-priori algorithm suffers scalability problems due to the exponential growth of patterns The basic reason the model is faster is because it uncovers long itemsets and it is efÞciently manipulated in main memory 
614 
614 


0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.1 0.2 0.3 0.4 0.5 0.6 Error tau Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0 0.05 0.1 0.15 0.2 Error tau Synthetic data set k=400  k=800  Figure 6 Support estimation error decreasing 002  0 100 200 300 400 500 600 700 0 200 400 600 800 1000 1200 1400 1600 Time in seconds n x 1000 Data set size d= 100  d=1000  0 50 100 150 200 250 300 0 20 40 60 80 100 120 140 160 Time in seconds k Number of clusters d= 100 n=100k  d=1000 n=100k  Figure 7 Time complexity for clustering large data sets with K-means 
615 
615 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efÞcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efÞciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efÞciency When the clustering model is available it is a signiÞcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques SufÞcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efÞcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efÞcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conÞdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conÞdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conÞdence The sufÞcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efÞcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efÞciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufÞciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70Ð81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207Ð216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487Ð499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145Ð154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146Ð153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9Ð15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597Ð600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern ClassiÞcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155Ð162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512Ð521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1Ð12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265Ð278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283Ð304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476Ð482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559Ð563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220Ð231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13Ð24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10Ð17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549Ð550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188Ð201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259Ð283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909Ð921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305Ð345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432Ð444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conÞdence Intelligent Data Analysis  9\(4\:381Ð395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49Ð73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407Ð419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1Ð12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki EfÞciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642Ð658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483Ð490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194Ð203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344Ð353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efÞcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103Ð114 1996 
618 
618 


