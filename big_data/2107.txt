A Hybrid Method fo r Discovering Maxi mal Frequent Itemsets  Fu-zan Chen, Min-qiang Li School of Management, Tianjin University Tianjin, 300072, China fzchen@tju.edu.cn, mq li@tju.edu.cn   Abstract A novel hybrid method included two phases for discovering maximal frequent itemsets is proposed. A flexible hybrid search method is given, which exploits key advantages of both the top-down strategy and the bottomup strategy. Information gathered in the bottom-up can be used to prune in the other top-down direction. Some efficient decomposition and pruning strategies are 
implied, which can reduce the original search space rapidly in the iterations. The compressed bitmap technique is employed in the counting of itemsets support According to the big space requirement for the saving of intact bitmap, each bit vector is partitioned into some blocks, and hence every bit block is encoded as a shorter symbol. Therefore the original bitmap is impacted efficiently. Experimental and analytical results are presented in the end  1. Introduction  Frequent itemsets mining was first proposed by Agrawal  
f or market basket analysis in the form of association rule mining A major challenge in mining frequent itemsets from a large data set is the fact that such mining often generates a huge number of patterns satisfying the min _ sup threshold, especially when min _ sup is set low. This is because if a pattern is frequent each of its subsets is frequent as well. A large pattern will contain an exponential number of smaller, frequent subpatterns. To overcome this problem closed frequent itemsets\(CFI\ and maximal frequent itemsets\(MFI 
mining were proposed[2  An i t em s e t   is a maximal frequent itemset or max-itemset n set D if  is frequent and there exists no super-pattern  such that     and  is frequent in D For the same min _ sup threshold, the set of max-itemsets, which is more compact, contains the complete information regarding to its corresponding frequent itemsets  1.1 Related Works  Mining max-itemsets was first studied by 
Bayardo[3 wh er e an  Ap rior i-b ased levelwise, b r eadth first search method was proposed to find max-itemset by performing superset frequency pruning and subset infrequency pruning for search space reduction. PincerSearch[4 s es ho rizo ntal d ata f o r m at I t n o t o n l y  constructs the candidates in a bottom-up manner like Apriori, but also starts a top-down search at the same time maintaining a candidate set of maximal patterns DepthProject [5 d s lon g i t em sets us i n g a d e pth  firs t  search of a lexicographic tree of itemsets, and uses a counting method based on transaction projections along 
its branches. Mafia [6 is an o t h er ef ficie n t m e tho d  fo r mining the MFI, which uses three pruning strategies to remove non-maximal sets and a vertical bit-vector data format to improve performance. Both DepthProject and Mafia mine a superset of the MFI, and require a postpruning to eliminate non-maximal patterns. FP-Growth[7  uses the novel frequent pattern tree \(FP-tree\structure which is a compressed representation of all the transactions in the database, and a recursive divide-andconquer and database projection approach to mine long patterns. Guizhen Yang r o v id ed th eo reti cal an al ysis of the \(worst-case\mplexity of mining max-patterns 
The complexity of enumerating maximal itemsets is shown to be NP-hard. Ramesh et al.[9 aracterized the length distribution of frequent and maximal frequent itemset collections  1.2 Contributions  Firstly, support counting is a confessed bottleneck in the association rules mining, which requires a great I/O and computing cost. A novel compressed bitmap index technique to speed up the counting process is employed The presented algorithm could reduce both the number of times the database is scanned and the search space rapidly Secondly, a flexible hybrid search method is given, which 
exploits key advantages of both top-down and  bottom-up strategies The rest of this paper is organized as follows Section 2 discusses the problem of itemset support counting based on compressed bitmap technology Section 3 introduces the description and decomposition strategies of search space. Section 4 presents the algorithm and some pruning strategies. The feasible optimizations and experimental results are presented Section 5. Finally, we conclude this paper in Section 6  
Fifth International Conference on Fuzzy Systems and Knowledge Discovery 978-0-7695-3305-6/08 $25.00 © 2008 IEEE DOI 10.1109/FSKD.2008.347 546 


2.  Itemset Support Counting The bitmap technique was proposed in the 1960ês and has been used by a variety of products. A typical context is the modern relational DBMS[10  I t al so  h a s been applied in association mining. The key idea of the approach is to use a bitmap index to determine which transactions contain which itemsets. Each transaction has one unique offset position in the bitmap. A bit vector       2 1 n b b b Bit X   is associated to each itemset X  In Bit X  the i th bit i b is set to 1 if the transaction i  contains the itemset X and otherwise i b is set to 0. It should be noted that i b is set to null if itemsets or transaction does not exist. The ordered collection of these bit vectors  composes the bitmap. In a bitmap, a line represents a transaction, while a column corresponds to a given k itemset During the supports counting, Intensively manipulates bit vectors requires a lot of disk space and main memory. So, it is necessary to compress the bitmap in advance. A new block strategy is proposed to encode and decode the bitmap, which is similar to the pagination technology in operating systems. In this approach, every bit vector is partitioned into fractions, called blocks, that can be encoded respectively, so that a bitmap is divided into granules. Each block should have an appropriate size if the size is too small, the impact is not remarkable otherwise the encoding is not straightforward. In order to take full advantage of Logical Calculation Units, the block size should be an exponential to 2. Each block is represented as    W p  p is the number of the block, and W is the block bit vector. Let l be the block size m the number of transactions in database D In this way each bit vector       2 1 m i b b b b B    can be partitioned into    INT l m p  blocks INT is the integer function\. The k th block vector        2 1 l j k w w w w W      l*\(k i l i j 1   MOD      MOD is the mode function Encoding each block as a shorter code can reduce the space demanding. The encoding principle which should be conformed is that each block can be represented uniquely. As a part of the initial bit vector B  each block vector W is also a binary bit vector. The conversion between binary, octal, decimal and hexadecimal can be implemented conveniently, hereby every block can be represented a binary, octal, decimal or hexadecimal code. We use a hexadecimal code, i.e. every four bits in a block encode a hexadecimal code As each itemset is associated to a binary bit vector the support of a given itemset is the total number of 1 in the vector. For the sake of efficient counting the number of 1, we previously store the binary block in a bit array  1  l Bit   l is the block size\, and the hexadecimal blocks in an array  1  p ABit   p is the number of blocks\he value in   i ABit is the hexadecimal  code of the i th block. Implementation of this support counting algorithm follows Algorithm 1  Itemsets Support Counting Algorithm Countsupport  X 1  X 2  Begin Support 0 For i 1 i  p  i o If X 1  ABit  i  an d X 2  ABit  i   0 th en  For j 1; j l  j do   X  Bit  j  X 1  Bit  j  X 2  Bit  j     Support  X  Bit  j    Endfor  X  ABit  i  X 1  ABit  i  X 2  ABit  i   Else  X  ABit  i   endif end  3.  Description of the Search Space  Definition1 Let Y X T   be an itemset in database D where I Y X   and    Y X And let P  Y  be the power set of Y denoted by     Y Z Z Y P   For this itemset T then set of all the itemsets obtained by concatenating X with the elements in P  Y is called the search space of T That is        Y P Z Z X Y X     denoted by X Y S or S We call X is an ancestor element of S denoted by S  anc  Y is a child element of S denoted by S  chi and Y X  is the border of S denoted by S  bor  Definition2 Let S and S i  k i   1  be the search spaces. Iff k S S S S      2 1 and    j i S S we call      2 1 k S S S  is partition of S denoted by k S S S S 012 012 012   2 1  Theorem1 Let I R Y X    be the distinct itemset and    RY X S  be a search space. For itemset      2 1 k i i i R   where I i j  and Y X i j   the search space S can be partitioned into the following subspaces   012 012 k j k j j Y i i Xi Y X 1 1        by R That is   012 012  k j k j j k Y i i Xi Y X Y i i i X 1 1 2 1             Proof Let V be a itemset 015 I w  the power set of wV is     wV Z Z wV P         wV w V Z Z   It follows from the definition1 and definition2, that the search space          V Xw V X wV X 012  The above search space X  RY  c a n b e pa rt i t i one d v i a i t e m  R i j   sequentially. The result follows With only a limited amount of main memory in practice, we should decompose the original search space into some smaller pieces, such that each one can be solved independently in main memory. Following the above description and partition mechanisms, the original enormous search space could be partitioned into some 
547 


little ones as flexibly as possible. Furthermore, theorem1 can be used to prune the search space The search space for item set I  a  b  c  d is S  I  I t  can be partitioned into some little ones step by step. The iterative results of a hierarchy of search space are shown in figure 1 For a given item set I in database D and the minimum support threshold min_sup the tas k of mining FI or MFI is in the follow: finding the set          minsup X Supp and I X X   in the search space S  I   4.   Discovering Max-itemsets 4.1 Search and Pruning Strategy In general, it is possible to search for the maxitemsets either top-down or bottom-up The bottom-up approach is good for the case when all max-itemsets are short, and the top-down approach is good when all  maxitemsets are long. If some max-itemsets are long and some are short in the mining database, then both search approaches will not be efficient. A key idea of our hybrid approach is the use of information gathered in the search in the bottom-up to prune search space during the topdown search. It uses infrequent itemsets found in the search in the bottom-up to prune search space during the top-down search. Some ef ficient prune technologies will be discussed later Lemma1 Let I Y X   and I u  and    uY X S  be a search space on X If the itemset R  Xu  is infrequent, the space S can be pruned by R And the remained space is   Y X S    The theorem2 shows how to prune the search space  X  Y  b y t h e inf req u e nt it em set R  Theorem2 Let X and Y be distinct itemsets, and    Y X S  be a search space. If there exists an item Y u  satisfied that itemset Xu R  if infrequent the partition of           u Y X u Y Xu  012  is the most efficient ones  Proof For such search space    Y X S  the size of S lies on value  Y Let      2 1 k i i i Y   there are 2 k  subsets in S that is S 2 k The original search space S  can be decomposed into   012 k j k j j i i Xi 1 1     by theorem1. For the j th subspace    1 k j j j i i Xi S  012  if the assumable infrequent itemset is Xi j then S j can be pruned entirely by lemma1. Obviously, when j 1\(i.e i 1  u  value of j S comes to the top. The result follows by theorem1 Theorem3 Let    Y X S  be a search space, and X  be a k 1\-itemset. Let k L  be the set of all infrequent k itemsets. If there exists such a itemset k L Z   satisfied with X Z  search space S can be pruned by k L  And the resulting space     R Y X S           k L Z and X Z X Z R      Proof  015 Y u  there exists              u Y X u Y Xu Y X  012   by Corollary1 If there has no k L Z   satisfied with X Z  i.e k L Xu   the search space S can not be pruned any part by Z  Let      2 1 M i i i R   the result follows by pruning S via R i j   M j   1 uentially as in Corollary1 Theorem4 Let   Y X S  be a search space, and X  be a k 1\-itemset. Let k L  be the set of all infrequent k itemsets, and    k L Z Z X M     The pruned search space S  is only 2 M of its original search space S  Proof When M 0\(2 M 1\y itemset in k L  can not used to pruned the original search space S When  M 0, assume       k L Z and X Z X Z R      R Y U   and J U  then      RU X Y X S   As we have known M R  and J M S 012  2 holds. By theorem3, we conclude that S can be pruned by k L  to   U X S    i.e J S 2    M S S    2  For search space S  X  Y   i f t h e num be r  of  infrequent itemsets containing X is large, the scale of S  will be reduced rapidly  4.2 The Hybrid Max-Itemsets Search Algorithm  There are two phases in this hybrid approach: the search in bottom-up direction and the other in top-down direction for every pass. Max-itemsets are enumerated in both bottom-up and top-down directions Consider a pass k the set of frequent k itemsets L k  and the set of infrequent k itemsets L k are to be classified in the bottom-up direction. This procedure repeatedly uses Apriori-gen algorithm to generate candidates like the Apriori u r i ng th e k th pass, every search space S  X:Y w h e r e  X is an itemset of size k 1 can be decomposed into some little pieces, whose ancestors are k itemsets. For search space S  X top-down procedure check whether the border element   ac:d   a:d bc:d b  ab:cd   abcd      d       Figure 1 Hierarchy of search space on I    a,b,c,d  K 0 K 2 K 1 K 3 
548 


i.e Y X  f S is frequent firstly, if not S is decomposed. Implementation of this hybrid approach is shown in algorithm2 Algorithm2. Algorithm for Max-itemsets Mining Procedure MFI Search \(Transaction Set D Item Set I  Begin   T MFI    B MFI  k 1  1 I    C k  I  L k Partition C k  min_sup   L k Partition C k  min_sup   While    k do Top-Down Search    012 1 k  Forall k S   do B S bor  If countsupport B  min_sup then If          T MFI R and B R R then   Decompose S  k  L k  L k        012 012 1 1 k k   Endif Else B MFI MFI T T    Endif Endfor Bottom-Up Search C k 1 Apriori-gen L k  L k 1 Partition C k 1  min_sup   L k 1 Partition C k 1  min_sup  Forall k L X  do If     012    1 k L Y Y X then X MFI MFI B B     Endfor  k  End Return B T MFI MFI   End Procedure: Partition C k  min_sup  classify L k and L k from set of candidate k itemsets C k  Begin   k L    k L  Forall k j C X  do if CountSupport X j   min_sup then j k k X L L    else j k k X L L      Endif Endfor End Procedure: Decompose S  X  Y   k  L k  L k  Begin       Z    k k L and X Z Z P      U   k k L and X U U P        k P V V Y R        k P T T R R     R n   Forall i 1 i  n 1 i o If k i P Xu  and k u u Xu k i i  012  1 then    1 k i i u u Xu  012      Endfor Return   End  5.  Experimental and Analytical Results  The test databases are generated synthetically by an algorithm designed by the IBM Quest project in IBM Almaden Recearch Center, referring to Http://www.almaden.ibm.com/cs/quest/syndata#AssocSynData    In this algorithm bitmap technology is used to count the support of every itemset instead of scanning the entire transaction database. Figure 2 shows the relative times at varying numbers of transactions for databases where the average size of transactions is 10 and the average size of potential max-itemsets is 4            When the average size of transactions or the average size of max-itemsets increase, there has much more itemsets \(or search spaces\e tested. therefore the total time will increase. Figure 3 shows the relative times of this hybrid algorithm at varying minimal supports on the datasets of T15.I8.D10K            To illustrate expandability of this algorithm, we performed an experiment varying the database size from 5K to 20K. The average size of transactions is 10, and the average size of potential max-itemsets is 6. For the experiment we fixed a minimum support of 4%. Figure 4 shows the result for the datasets 1000 3000 5000 7000 9000 7.0 6.0 5.5 5.0 4.5 Minimal Support\(T 15I8D10K Response Time\(sec Two-Way Figure 3 Times varying support thresholds Figure 2. Relative times varying databases 0 50 100 150 200 250 300 350 50K 100K 150K 200K Number of Transactions Responsed times\(Sec List BM 2-BMC 
549 


           As we have shown, there are three search strategies for discovery MFI. Figure 5 shows the relative times of the three approaches for the tests at varying minimal supports on T10.I6.D10K. From the experiments, we could see that when minimal support is greater than 2 the performances of the bottom-up is little better than the hybrid. The main reason is that the number of itemsets generated is small with the increasing of minimal support As the minimal support decreases, MFI becomes longer which results in an increase in the number of counting itemsets. In such a case, the hybrid has performances. We can also see performance of the bottom-up approach is lower than the others. The most primary factor is almost all the max-itemsets are expected to not be long in this T10.I6.D10K dataset. The experiment illustrates the fact that top-down search might be efficient for the  long maxitemsets           6.  Conclusions  We use an improved compacting bitmaps database format. Support of itemset can be counted by means of binary bit vectors intersections, which minimizes the I/O and computing cost. To reduce the disk and main memory space demanding, we break the bitmap down into some little blocks, which can be encoded as a shorter code. The blocks of bitmaps are fairly adaptable. Hence the additional space decreases rapidly The hybrid approach exploits key advantages of both the top-down strategy and the bottom-up ones, which can discovery both longer max-itemsets and the shorter ones in earlier passes. And the infrequent \(or frequent\itemsets discovered in the bottom-up can also be used to prune the search space in the other top-down direction. Furthermore this algorithm can be parallelized easily on this hierarchical search space organization. We note that using L k to prune the search space is not the only technique. If  k k L L   it would be more efficient to decompose and prune the search space using L k rather than ~L k   Acknowledgements This paper is supported by the National Science Foundation \(No.70571057, No. 70771074  References  1   R. Agrawal, T. ImielinSki, A. Swami, Mining association rules between sets of items in large database. Proc. of the ACMSIG2 MOD International Conference on Management of Data Washington, DC.1993, 2 : 207-216 2   Jia wei Han, et al, Frequent pattern mining: current status and future directions, Data mining and Knowledge Discovery, 2007 V\(15\5-86 3   Bayardo R. Efficiently mining long patterns from databases. In: Proc. of the ACM SIGMOD, Intêl Conf. On Management of Data. New York: ACM Press. 1998. 85-93 4   Dao-I Lin, Zaki M. Kedem, Pincer Search: A New Algorithm for Discovering the Maximum Frequent Set, Proceedings of the 6th International Conference on Extending Database Technology 1998. 105-119 5   R. Agrawal, C. Aggarwal, Depth first generation of Long patterns, 7th International conference on Knowledge discovery and Data mining. 2000. 108118 6   D. Burdick, M. Calimlim and J. Gehrke, MAFIA: A Maximal Frequent Itemset Algorithm for Transactional Databases, Proc. of the 17th Int'l Conf. on Data Engineering. 2001. 443-452 7   J.P. Han, Y. Yin. Mining frequent patterns without candidate generation. In ACM SIGMOD Conf May 2000. 1-12 8   Guizhen Yang. The complexity of mining maximal frequent itemsets and maximal frequent patterns.In Proceeding of the 2004 ACM SIGKDD international conference on kowledge discovery in databases \(KDDê04\Seattle,WA, 344Ö353 9   Ramesh G, Maniatty WA, Zaki MJ \(2003\ Feasible itemset distributions in data mining: theory and application. In: Proceeding of the 2003 ACM symposium on principles of database systems PODSê03\San Diego, CA, 284Ö295 1  Mikolaj Morzy, Hierarchical Bitmap Index An Efficient and Scalable Indexing Technique for SetValued Attributes, ADBIS 2003, LNCS 2798, 2003 236Ö252  0 200 400 600 5K 10K 15K 20K Num bers of T ransactions Response Time sec Fi g ure 4. Relative times var y in g  numbers of transaction 0 500 1000 1500 2000 2500 5 4 3 2 1 Minimal Support\(T 10I6D10K Total Time\(sec Two-W ay Bo ttom U p To p D ow n Fi g ure 5. Total times var y in g  Su p p ort thresholds 
550 


0 0.05 0.1 0.15 0.2 0.25 0.3 0 20 40 60 80 100 Error k Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0 100 200 300 400 500 600 700 800 Error k Synthetic data set tau=0.10  tau=0.05  Figure 5 Support estimation error increasing k  high when k is low We now turn our attention to the synthetic data set where we built models with much higher k  In this case error shows a slower rate of decrease The trend also seems asymptotic We can also see there is a clear gap between 002 0  05 and 002 0  10  error roughly grows 100 when 002 decreases by 50 Interestingly enough results are much better for real data sets than for the synthetic data set Error decreasing minimum support Figure 6 shows error growth as 002 decreases The left graph analyzes error for real data sets We can see error growth shows different behavior for each data set Error growth is slow for the Votes data set Error grows almost linearly for the Chess data set Error grows fast for the Mushroom data set The trend indicates we need to build more accurate models with higher k for Mushroom probably not for Chess and not necessary for the Votes data set The right graph analyzes error growth for the synthetic data set Theﬁrstmodelat k  800 is twice as accurate as k  400  We can see error grows linearly for high 002 values but it start growing faster at 002 0  1  The trend indicates the growth is not linear but it does not seem exponential 4.4 Comparing Speed and Scalability We rst compare our proposal versus the standard algorithm to mine association rules We then study time complexity and scalability Comparing clustering and A-priori Table 2 compares the efﬁci ency of the model with the standard A-priori algorithm We must stress our proposal does not intend to substitute fast association rule algorithms Table 2 Comparing model and A-priori 002 from model clustering+model A-priori 0.20 1 1672 24 0.15 1 1672 43 0.10 1 1672 156 0.05 3 1674 645 0.02 11 1683 3347 0.01 36 1708 14806 32 16 b u t w e i ncl ude t h es e c ompari s ons t o pro v i d e a rel ative performance benchmark We used the synthetic data with n 1 M  The clustering model used had the number of clusters set to k  100  These times include the time to compute exact support on a nal pass The rst column varies 002 the minimum support threshold The second column shows the time to discover frequent itemsets from the model excluding the time to compute the model The third column adds the time to compute the clustering model and the time to mine frequent itemsets Finally the fourth column shows the time for the traditional algorithm As can be seen the clustering model r epresents an efﬁcient mechanism to produce all frequent itemsets assuming the model is already tuned and stored That is we assume the model is computed a few times or even once The second column shows clustering the data set takes most of the time In this case the standard algorithm is faster at high support levels but the model becomes faster at low support levels Notice the third column represents a pessimistic case in which the model is recomputed every time Finally we can see the A-priori algorithm suffers scalability problems due to the exponential growth of patterns The basic reason the model is faster is because it uncovers long itemsets and it is efﬁciently manipulated in main memory 
614 
614 


0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.1 0.2 0.3 0.4 0.5 0.6 Error tau Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0 0.05 0.1 0.15 0.2 Error tau Synthetic data set k=400  k=800  Figure 6 Support estimation error decreasing 002  0 100 200 300 400 500 600 700 0 200 400 600 800 1000 1200 1400 1600 Time in seconds n x 1000 Data set size d= 100  d=1000  0 50 100 150 200 250 300 0 20 40 60 80 100 120 140 160 Time in seconds k Number of clusters d= 100 n=100k  d=1000 n=100k  Figure 7 Time complexity for clustering large data sets with K-means 
615 
615 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


