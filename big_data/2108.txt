Scalable Relational Query Results Handling in Service Oriented Architectures Alexander W ohrer Thomas Lustig and Peter Brezany Institute of Scienti“c Computing Faculty of Computer Science University of Vienna Austria  woehrer  brezany  par.univie.ac.at e0026005@student.tuwien.ac.at Abstract The focus of Grid computing shifted in recent years towards data-intensive applications Additionally it became more standard-based and the adherence to and usage of them has to be further fostered in order to enable and ease exchange and interoperability among the research community The OGSA-DAI framework for data access and integration together with its default XML output format for relational data namely Javas WebRowSet are two important representatives of these developments The research reported in this paper is being conducted in the context of the EU project ADMIRE The papers contribution is twofold and targeted on improving the scalability of query result handling in SoA First an XML indexing approach is elaborated to provide an out-of-core Java implementation of the standard Java WebRowSet interface to handle big XML les with low main memory consumption Second the concept of ne grained data statistics for data preprocessing tasks calculated on-the-”y data service side is presented 1 Introduction Thanks to the Internet scienti“c research has become much more open and collaborative in many areas over the last decades Interdisciplinary large scale research eorts so called e-Science are w orking with data in dierent ways than traditional approaches from gathering and analyzing only your own data towards using second-hand public available and synthetic data The Grid is the infrastructure of c hoice to enable the needed exible secure coordinated resource sharing among dynamic collections of individuals institutions and resources for e-Sciences The importance of databases most of them are relational database management systems RDMBS in accessing data on the Grid is constantly increasing over the traditionally used huge binary les The research presented in this paper is conducted in the context of the Advanced Data Mining and Integration Research for Europe ADMIRE pro ject which is pioneering architectures and models that will deliver a coherent extensible and exible framework to make the best use of a wide range of distributed data resources for knowledge discovery process KDD  and in tegrate them as adv anced service-orien ted Grid application The KDD process typically consists of the following steps data selection preprocessing transformation mining and interpretation/evaluation As mentioned before e-Science requires data from dierent domains to be integrated This data integration commonly happens before the data preprocessing phase Due to the immense size of the resulting dataset and the fact that one is not familiar with its characteristics additional support to allow scalable query result handling for data preprocessing is required The de-facto standard for database access and integration on the Grid is the OGSA-DAI framework Its default output format is the Java WebRowSet  XML format Figure 1 sho ws an example OGSADAI based data access and integration sub-task of some bigger e-Science work”ow incorporating our developed components A client instructs an OGSA-DAI service to integrate data from three dierent databases and deliver the results back to the requestor via the speci“ed delivery method In order to pre-process the data for subsequent data mining the client needs to know some details about it e.g number of rows min/max values etc and mechanisms to read and change it Ideally the dataset should not have to be read additionally to gain the required statistics and no further transformations e.g temporal table in database are necessary to enable pre-processing of big datasets Copyright © 2009 by the Institute of Electrical and Electronics Engineers, Inc. All rights reserved 
 


 Database X Database Y Database Z Client OGSA-DAI JDBC JDBC JDBC SQL Activity 1...3 Delivery Activity MetaStatistics Activity Workflow data request data statistics data response in WebRowset format Integration Activity Novel WebRowSet Impl Figure 1 Overview of developed components and their relation to OGSA-DAI and WebRowSet in a typical data mining and integration work”ow in ADMIRE This are exactly the issues our research eort addresses The main contribution to the above described work”ow consists of two components 1 An out-of-core partly disk-based implementation of the WebRowset interface based on a novel XML indexing approach allowing to read and modify huge les in WebRowSet format This component is shown in Figure 1 on the left client side 2 A possibility to de“ne required statistics for the results of a data request in a ne-grained way via a novel OGSA-DAI activity These extendable set of statistics are calculated on-the-”y tuples are bypassed on the way to the delivery activity This component is pictured in Figure 1 in the middle service side The rest of the paper is organized as follows Section 2 introduces related work and the required background for our contribution such as details about OGSA-DAI and WebRowSet The kernel sections elaborate on the requirements design and implementation of serviceside on-the-”y statistics in Section 3 and XML indexing for scalable WebRowSet handling in Section 4 while their evaluation is discussed in Section 5 We nish the paper with an outline of future work and our conclusions in Section 6 2 Background and Related Work Our earlier work on D 3 G fo cuses on gathering a xed set of data statistics and using them in remote DPP methods service side on query results The current work extends this approach by allowing a very ne-grained way of specifying needed statistics as well as de“ning dependencies between them in order to reuse already calculated statistics internally Additionally it focuses on extensibility to support an additional standard operate on single column and aggregate operate on more than one column statistical functions A more general approach to index based XML parsing than ours is described in IBP Due to its closed source approach we could not get a license for deeper performance comparison nor do we know their API IBP seems to focus on read-only scenarios and keyvalue searches This requires some mandatory initial complete scan of the target XML document for index building while our method implemented for the WebRowSet format allows for inserts and updates and reads the XML document on demand in an iterative manner A short overview of XML parsing APIs is given below to show that none of the existing parsing APIs allows random access and minimal memory consumption at the same time DOM constructs the whole document structure in-core providing an easy to use way of direct navigation to XML elements This allows random access navigation within the XML document data but causes a direct relation between memory consumption and document size SAX avoids the memory consumption by a sequential parsing approach For backward navigation the document has to be read again STAX places a read cursor in the XML document to control the XML parser Early implementations still have problems with memory consumption of huge XML documents Additionally it is not possible to deactivate the XML validation in the Sun Java im 
 


Listing 1 Example WebRowSet XML le  1 webRowSet 2 properties>....</properties 3 metadata 4 column count>1</column count 5 column definition>...</column definition 6 metadata 7 data 8 currentRow><columnValue>ID1234</columnValue></currentRow 9 data 10 webRowSet plementation of STAX JAXB is a framework for Java applications that allows the mapping of XML le data to Java objects at high memory cost OGSA-DAI is an integral component of the ADMIRE infrastructure It supports the exposure of data resources such as relational or XML databases on the Grid and includes a collection of basic building blocks so called activities for de“ning inter-service work”ows including querying transforming and delivering data in dierent ways The framework has been designed from the beginning to be extendable so users can provide their own additional functionality OGSA-DAI is heavily standards based e.g GridFTP GSI and WebRowSet as standard output format It is one of the reference implementations of Open Grid Forums WSDAI sp eci“cations and a v ailable for v arious service environments like Axis OMII Globus Toolkit 4 gLite and UniCore The WebRowSet interface and format w a s introduced in J2SE Version 1.4 Since J2SE Version 5.0 reference implementation classes of the interfaces ship with the Java run time The interface implementation eases the task of converting data back and forth between database and XML representations and is heavily used in business and research An example WebRowSet XML le is shown in Listing 1 It contains three parts properties metadata and data The properties section provides details about RDBMS speci“c things like synchronization provider isolation level rowset type etc The metadata section provides information about the underlying database query result such as column numbers their name and type etc The data section holds actual data within the currentRow section shows the values of each column of a row of a WebRowSet instance currently mapped to an underlying data source without any modi“cation Any data manipulation operations such as insert update or delete are marked appropriately in the output XML via deleteRow  insertRow and updateValue tags respectively Javas reference implementation is always validating checking if a XML follows a given certain DTD or schema before actually reading it and main memory based both features are restricting scalability Listing 2 demonstrates how to typically work with Listing 2 Code snippet of how a WebRowSet implementation is typically used 1 WebRowSet wrs  new WebRowSetImpl new FileInputStream\(WEBROWSET_FILE_NAME  2 wrs.first 3 while wrs.next  4 for  int i=1;i<=wrs.getMetaData\(\.getColumnCount\(\;i  5 String x  rsmd.getString\(i 6 if rsmd wasNull wrs.deleteRow 7 else System.out.print  rsmd.getString\(i   8  9  10 wrs.writeXML new FileOutputStream\(UPDATED_WEBROWSET_FILE_NAME 11 wrs.close WebRowSet on the client side It rst speci“es the le to be accessed an then iterates over it outputting each column as string if value available but deleting the row if one is NULL until EOF is reached Then writing the pre-processed dataset in this case cleaned from rows containing NULL values to a new le which serves as input for a data mining task 3 On-the-”y Statistics Collection Let us rst elaborate on the rational behind on-the”y statistics collection In the relation b e t w een data transfer and computation in distributed environments is described The code-to-data approach is considered bene“cial for many scenarios We are going to compare a typical data management work”ow with one exploiting on-the-”y statistics Let T DD be the time needed to deliver a dataset from one service to the requester T SC be the time to compute statistics T RD be the time to re-read the dataset requester side and T DS be the time needed to deliver the statistics about a dataset to the requester Given the assumption that the statistics computation does not include very complex functions and that metadata about a given dataset in our case statistics is typically just a tiny fraction in size of the original dataset the relations between these times are as follows T DD T SC and T RD T DS Without service side on-the-”y statistics the overall formula to get the data from the service re-scan it locally and calculate the statistics is given in Function 1 With service side statistics the re-scan can be avoided instead the statistics are calculated in parallel to the at latest mandatory data delivery task of the data service resulting in Function 2 T old  T DD  T RD  T SC 1 T new  max  T DD  T SC  T DS 2 It should be noted that T RD is dependent on the number of tuples of the dataset while T DS is not Two types of statistic functions can be distinct and should be supported service side 
 


 Standard functions can be applied and calculated in one dimension on a single column e.g maximum value and do not need any additional input to ful“ll their task  Aggregator functions are typically using the results and values from more than one column e.g covariance and need additional parameters to work Aggregator functions are calculated after the standard functions because of the need of their results Now lets have a closer look on the requirements of a service-side on-the-”y statistics collection process which we de“ne as follows  con“gurable it should be possible to de“ne which statistics to be calculated on which column in a very ne-grained way in order to avoid unnecessary computation  extendable the inclusion of special and just locally needed functions is enabled and anticipated  dependent dependencies between statistic functions can be speci“ed and are resolved automatically by including required functions for some statistical calculation in the statistics collection process  scalable large datasets should be supported by the implementation of each statistical function Examples of demanding functions are those requiring sorting e.g median and non-iterative functions where this will be harder to achieve as in simple and not memory critical functions e.g min/max Our implementation of this on-the-”y statistics calculation concept de“ned above is based on OGSADAI This novel approach creates useful statistics of  MatchedIterativeActivity MetaStatisticsActivity StatisticProcessor     uses Standard function Aggregator function Figure 2 Architecture of MetaStatistics activity Listing 3 Example PMML input for MetaStatistics activity 1 PMML version  3.0  2 Header 3 Application name ADMIRE version  1.0  4 Header 5 DataDictionary 6 DataField name column 1 displayName column 2 dataType double 7 taxonomy SUMSQUAREDDELTA  FREQ  QUARTILE  optype continuous  8 DataField name column 2 displayName column 1 dataType double 9 taxonomy MIN MAX AVERAGE  STDDEV  optype continuous  10 DataDictionary 11 Extension extender AFc1c2 name column 1/column 2 12 value SUMXYDELTA  COVARIANCEXY   13 PMML the dataset during the data ow inside the OGSA-DAI framework and provides them for subsequent data preprocessing and data management decisions e.g cleaning method to use or if this result set is of interest at all Therefore we implemented an OGSA-DAI activity named MetaStatistics ful“lling the requirements speci“ed above The architecture of this activity is shown in Figure 2 MatchedIterativeActivity is abstract activity base class provided by OGSA-DAI that provides a template for an iterative processing algorithm and validates the activitys inputs are matched The MetaStatistics activity requires two inputs one for specifying the wanted statistics column by column and one for specifying the tuple stream on which the calculations are made on An example on how to specify required statistics in a ne grained way is given in Listing 3 This mechanism is based on PMML an XML standard being developed by the Data Mining Group to represent and describe data mining and statistical models as well as some of the operations required for cleaning and transforming data prior to modeling The MetaStatistics activity uses itself a dynamic class loading and function applier engine called StatisticProcessor Statistical functions need to implemented the following interface de“nition next\(Object n  getResult and getName  next\(Object n is called by the StatisticProcessor when it iterates through the tuples of the input Inside the next-method we are calculating the values in a sequential way to optimize the memory consumption and to avoid another iteration through all values At the end of this method the result is available and can be read by calling getResult  The getName method is used to resolve the statistical function name after instantiating an object Standard functions have a local variable called mapFP which is a HashMap that provides function references to other objects within the same column This reference map is set inside the constructor when the statistic objects are instantiated by the StatisticProcessor This allows to get dependent calculation results e.g the standard deviation object need the results from the average object 
 


Listing 4 Example XML output from MetaStatistics activity  1 metastatistics DataSourceID XY  2 stdfunctions 3 column name column 1  4 function name MIN 20000.0</function 5 function name MAX 150000.0</function 6 function name AVERAGE 86734.25350480416</function 7 function name STDDEV 1.4015262028244455E9</function 8 column 9 column name column 2  10 function name SUMSQUAREDDELTA 6.098014970632999E11</function 11 function name FREQUENCY total 1000</function 12 function name FREQUENCY missing 0</function 13 function name FREQUENCY distinct 384</function 14 function name QUARTILE Q1 0.0</function 15 function name QUARTILE Q3 36870.74118725</function 16 function name QUARTILE Median 0.0</function 17 column 18 stdfunctions 19 aggrfunctions 20 aggrkey name column 1/column 2  21 function name SUMXYDELTA 6.960866142676609E11</function 22 function name COVARIANCEXY 7.004466187493039E8</function 23 aggrkey 24 aggrfunctions 25 metastatistics In order to specify which function needs what other results a dependency table is speci“ed above all currently available functions The output of the statistics activity provides the calculated metadata in a simple XML format an example is given in Listing 4 For reading this XML format we created helper Java classes that can be integrated into existing data mining applications An OGSA-DAI work”ow using our MetaStatistics activity is shown in Figure 3 The result tuples of a SQLQuery activity are used within two parallel tasks while the left part creates the WebRowSet le from tuples and delivers it via GridFTP the right part calculates speci“ed statistics and returns them to the client Standard statistics implemented so far include minimal value maximal value average frequencies total missing distinct quartiles Q1 Q3 median and standard deviation The currently implemented statistical aggregator functions are covariance and sumXYdelta Distinct values are calculated by using a HashMap to SQL TEE Tuples Client request SOAP response DS OGSA-DAI service Tuple2WRS MetaStatistics Tuples Tuples Deliver2GFTP Deliver2Response XML XML host Figure 3 Usage of MetaStatistics activity store already observed values while median calculation is using a buer based sorting algorithm 4 Scalable XML Handling XML is widely used in distributed systems but the existing parser models are inecient and resource intensive for applications to handle random access on large XML documents as discussed in Section 2 Using XML with databases typically results in a structure containing what we call block tags  Block tags are containing the similar structured payload data tags and are close direct or 2nd level child nodes of the root element of a XML document Examples are Book tag enclosed by a BookSet root tag or also the currentRow tag in the WebRowSet format shown in Listing 1 A typical usage of an XML le for data mining and integration is shown in Listing 2 for a WebRowSet le namely iterating over the items block tags of some set and modifying it update delete insert The observation that one block is the basis for navigation and access lead us to consider to use the block tags of an XML le as target index value By this our index size is just dependent on the number of blocks N umber blocks in an XML le while more ne grained indexes would imply a much bigger index size e.g N umber blocks  N umber subtags  The iterative work style of most data mining and integration applications together with the target to avoiding long startup times time before rst block is accessible lead us to consider on-demand parsing of the target XML le omitting initial validation instead of pre-reading and indexing the whole le in advance Our implementation of the above described XML indexing approach is done for the WebRowSet format and API as illustrated in Figure 4 The low level component that is accessing the WebRowSet XML le is a simple non-validating SAX parser steered by our Random Access Parser maintaining two look-up tables for remembering the already read/update/inserted/deleted rows Normally SAX parsers are only used for forward parsing and do not support random access Therefore the starting positions of found rows have to be stored as parser snapshots using an implementation of the Memento pattern If the parser has to be moved to a relative previous row of the WebRowSet le compared to the actual position its internal state is restored again One table keeps the information of the original le while the other handles updates inserts and deletes A table contains the row identi“cation number and the Memento object to access it again Updated and inserted rows are serialized to disk and read-in again when either one of two events occur writ 
 


Figure 4 Architecture of scalable WebRowSet implementation ing the updated XML le or accessing the row again In order to use our novel WebRowSet implementation in existing applications just one minor change is required namely to change the used class on line 1 in Listing 2 to NovelWebRowSetImpl  Methods which are requiring a working connection to a database e.g acceptChanges should commit changes to the underlying database are not supported 5 Evaluation The two main measurements of the novel WebRowSet implementation were memory consumption and read/write performance Measuring the memory consumption was done by using functions provided by the JVM we used Sun Java JVM 1.5.09b for Microsoft Windows XP 32-bit edition with heap size increased to 256 MB each client and service side For test data generation we utilized the data generator that is delivered with Weka version 3.5.8 The generated table consisted of 10 columns 9 numerous and 1 string having the following characteristics 2 nullable 4 holding continuous and 6 discrete values Accesses to two les resulting from queries against this table where investigated one consisting of 100.000 rows ending up in 41  7 MB WebRowSet le while the other consisted of 250.000 rows resulting in 92  0 MB To evaluate the memory consumption over time on the server side we used the Apache Tomcat 5 JMX feature hosting our extended OGSA-DAI service This was done during a typical data mining and integration work”ow targeted by ADMIRE and pictured in Figure 1 During server side processing including statistics calculation the amount of reserved heap space stays constant indicating good scalability behavior On the client the rst row of the smaller 41  7 MB result le is available after 0,043 seconds with our novel indexbased implementation instead of 23 seconds with Suns reference implementation Trying to read the larger 92  0 MB le with Suns in-core implementation results in an out-of-memory exception Further increasing the heap allowed the standard implementation to read also the large le although it took 9.6 minutes before it was possible to navigate within the XML le elapsed time of the readXML function During this phase over 150 MBytes were used for the needed data structures After this was done the navigation through the data started and took about 9 seconds for iteration and used again over 70 MB additional heap space In contrast the test with our modern implementation showed that the memory consumption stays below 2 MB and is not increasing constantly over time The time to access the rst row is around 17 ms while the iteration through the rows was nished in 22 seconds The read/write scalability of the current WebRowSet implementation is basically dependent on how many Memento objects equal to the number of rows in the WebRowSet le can be handled We investigated the size of a Memento object via the method introduced in resulting in a deep memory usage of one object between 1000 and 2500 bytes This value might be further optimised by reducing the number of member variables currently 14 of the Memento object The scalable XML index based non-validating WebRowSet implementation and the OGSA-DAI MetaStatistics activity including JUnit tests can be downloaded from http://www.gridminer.org/dppms.html  
 


6 Conclusions and Future Work The focus of Grid computing shifted in recent years towards data-intensive applications The importance of databases most of them are relational database management systems RDMBS in accessing data on the Grid is constantly increasing over the traditionally used huge binary les OGSA-DAI is the de-facto standard middleware to assist with access and integration of databases via the Grid and an integral component of the ADMIRE infrastructure The research eort described in this paper focuses on mechanisms to support the handling of large relational results in SoA First an XML indexing approach is elaborated to provide an out-of-core Java implementation of the standard Java WebRowSet format and interface to handle big XML les with low main memory consumption and fast time to rst row Its implementation is based on a rowwise indexing approach on-demanding parsing of the target XML document and the omittance of validation The introduced extended SAX parser based on Memento objects can be applied to other XML formats as well Second the concept of ne grained data statistics for data preprocessing and management tasks calculated onthe-”y data service side is presented This novel approach creates useful statistics of the dataset during the data ow inside the OGSA-DAI framework and provides them for subsequent data pre-processing and data management decisions e.g cleaning method to use or if this result set is of interest at all The current implementation as an OGSA-DAI activity supports dierent types of statistic functions a very ne-grained way to de“ne which statistics to be calculated on which column in order to avoid unnecessary computation the inclusion of special and just locally needed functions is anticipated and dependencies between statistic functions can be speci“ed and are resolved automatically The initial performance tests presented in this paper are indicating good scalability for both contributions Our future work include the following thorough performance and scalability analysis of both components generalizing our index based XML parsing component and extending the set of supported standard and aggregator statistic functions Acknowledgments This work has been supported by the ADMIRE project which is nanced by the European Commission via Framework Program 7 through contract no FP7-ICT-215024 References  R Agra w al T Imielinski and A Sw ami Database mining A performance perspective IEEE Transactions on Knowledge and Data Engineering  5\(6 925 1993 Special issue on Learning and Discovery in Knowledge-Based Databases  M An tonioletti A Krause S La ws N W P aton S Malaika D Pearson A Eisenberg and J Melton The WS-DAI family of speci“cations for web service data access and integration SIGMOD  35\(1 2006  M A tkinson P  Brezan y  O Corc ho L Han J v a n Hemert L Hluchy A Hume I Janciak A Krause D Snelling and A Woehrer ADMIRE White Paper Motivation Strategy and Impact 2008  J Gra y  Distributed computing economics MSR-TR2003-24 2003  J Gra y and A S Szala y  Where the rubb er meets the sky Bridging the gap between databases and science IEEE Data Engineering Bulletin  27\(4 2004  R Grossman M Hornic k and G Mey er Data mining standards initiatives Communications of the ACM  45\(8 2002  J Han and M Kam b er Data Mining Concepts and Techniques  Morgan Kaufmann Publishers Inc San Francisco CA USA 2005  Ja v amex UK Classmexer agen t http://www.javamex.com/classmexer  W Kim B.-J Choi E.-K Hong S.-K Kim and D Lee A taxonomy of dirty data Data Min Knowl Discov  7\(1 2003  Mario An tonioletti et al The design and implementation of grid database services in ogsa-dai Concurrency and Computation Practice and Experience  17\(2-4 2005  D P earson Data requiremen ts for the grid Global Grid Forum  2002  D D Roure M A Bak er N R Jennings and N R Shadbolt The evolution of the grid In Grid Computing making the global infrastructure a reality  pages 65…100 John Wiley  Sons 2003  R Sc hro eder e-sciences as researc h tec hnologies recon“guring disciplines globalizing knowledge Social Science Information  47\(2 2008  Sun W ebRo wSet XML sc hema http://java.sun.com/xml/ns/jdbc/webrowset.xsd  A W o ehrer P  Brezan y  L No v a k o v a  and A M Tjoa D3G Novel approaches to data statistics understanding and preprocessing on the grid In AINA 06 Proceedings of the 20th International Conference on Advanced Information Networking and Applications Volume 1 AINA06  pages 313…320 IEEE Computer Society 2006  H Zhang X Zhou Y Gang and X W u  IBP An index-based XML parser model In H Jin D A Reed and W Jiang editors NPC  volume 3779 of Lecture Notes in Computer Science  pages 65…71 Springer 2005 
 


               The graphs above were created by users of Mycrocosm from the general public; earlier on in the development process we invited a small set of people to use Mycrocosm.We shall discuss the experiences of three of these users in this next section 11 These early users come from the authors academic community; two are members of the authors research group though they are not directly involved in developing Mycrocosm. The third was an undergraduate student working for another research group over the summer. She came to know of mycrocosm via word of mouth and was interested in trying it out. The length of time they spent using mycrocosm varied going from approximately one week for two of the users up to 47 days for the third. None of these users were given any specific instruction when directed to the Mycrocosm site. A little while after their activity on the site reduced they were sent a questionnaire to gauge their experience. While this is quite a small sample it does shed some light on possible motivations to using mycrocosm The most commonly described use of mycrocosm amongst these users was introspection. Users tracked sports scores in personal competition, eating destinations, number of dinner companions, commute time, meals consumed and other eccentricities of life such as teeth brushing behaviour and fashion choices. This is not surprising as Mycrocosm provides a way to record and quantify one s behaviour One user, Jane, says I always felt that my commute was long and out of personal curiosity I collected numbers just to see a\ how long it is, b\ how big a chunk of my time every day is eaten up by commuting although I had a general sense of what the pattern was, I thought it would be interesting to find out precisely  Peter \(our longest running user\ writes I'm primarily interested in introspection - it's fun to see what the variety of places I eat is ... Because eating is   11 Names used here are pseudonyms  Figure 14 Figure 16 Figure 15 Figure 17 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 
 


something that happens every day, it's easy to make updating mycrocosm a part of my routine. I keep it on my bookmark bar and when I'm looking for ways to procrastinate, I'll log on and add the previous day's information  Related to the above use is that of communicating something to others about yourself, using data Jane recounts that she created a graph to track what she eats in order to rebuff a complaint that her roommate had that she ate too skewed a diet. However this use can take an interesting turn, as users are careful of what they put in their graphs and how it makes them look. For example Peter says For all my graphs, I'm always excited to enter in new data that increases the diversity of my chart, or grows a category that's rare. For instance, if I eat dinner somewhere I don't usually eat, I like to enter that because it increases the diversity of places it looks like I've been He continues I also think a little bit about how other people will interpret it. For instance, I don't want to over-represent the times when I eat alone. Since that's already a reasonably frequent category, I probably end up under-reporting those instances because they're not unique and I don't really want it to seem like I'm often eating alone  Peter engages in the practice of impression manageme m o n t o on li n e s o cial s p aces an d i n deed almost any social space in which we are perceived by others There is also use of Mycrocosm for more general non-egocentric communication. We have already described the graph commenting on Facebook as memory prosthesis, but we also observed graphs made to comment on others behaviour and habits, for example one of our users made a graph to track/comment on how often one of their acquaintances was late for their meetings as a gentle reminder to that acquaintance Two of our users also commented on how they felt the investment of adding data to the graphs over time gave them a certain weight and credibility that differed from more the more ephemeral text based microblogging systems. Peter says I really like my charts on mycrocosm have some depth to them. I can show to people that I really care about something by how much I update it, as opposed to facebook where they kind of have to take my word that I've always liked a certain band or book. In mycrocosm, I could have a chart that shows how many pages of certain authors I've read, or how many movies I watch a month  While it is still too early to draw strong conclusions from the above user accounts, they provide promising indicators that Mycrocosm provides a flexible,  expressive and unique communication tool  4.3. Truth, Truthiness and Statistical Graphs  Mycrocosm can be, and often has been, used as a tool to keep track of factual data the length of one s commute, one s waking time, diet info, exercise records, etc.  Yet it can also be used for more subjective narratives, and there is a certain irony in using statistical graphs to represent thoughts, ideas and fictional data. Statistical representations are often seen as representations of truth or fact and in our context this association provides a interesting juxtaposition to what may actually be communicated; that feel of truth and authority adds a certain irony when what is being represented is, say, the amount of Haterade 12 consumed amongst a group of friends. Just as we can express truth, opinions and non-truths in text, Mycrocosm presents users with the opportunity to use statistics association with credibility in representing who they are but also allows one to express partial/non truths. This odd juxtaposition actually adds to the humanity of the graphs, as users push on these associations of charts with facts, they turn data into stories, and collections of graphs into portraits  5. Conclusion & Future Work  In this paper we have introduced Mycrocosm, a new web service for social communication and identity representation that allows users to use simple statistical graphs as an expressive communicative medium Some of our future work includes further exploration of what this visual vocabulary should contain as we scale the web site up in use we will have a better chance to evaluate what the different kind of visualizations that people need are as well as examine what types of communication emerge within this medium. We also want to expand and examine the usefulness of Mycrocosm in the context of users other lifestreaming/personal publishing data. We plan to enable the embedding of live Mycrocosm graphs on external web pages \(such as a user s blog\nd to hopefully integrateMycrocosm with social data aggregation systems such as FriendFeed 13 or Facebook and see whether it can augment these existing forms in a meaningful way. Additionally we also plan to provide APIs to allow easy automation of data entry on the site    12 A slang term that rhymes with Gatorade a popular sports drink\, for a drink purportedly drunk by and fuelling those who hate on are jealous of\hers http://www.urbandictionary.com/define.php?term=haterade 13 http://friendfeed.com  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 
 


5. References  1. Felton, N. 2007 Feltron Annual Report Retrieved from http://www.feltron.com/index.php?/content/2007_annual_r eport   2. Goffman, E. 1959 The Presentation of Self in Everyday Life New York: Doubleday   3. Graphjam Graphjam Retrieved from http://graphjam.com   4. Harrison, E Ellie Harrison Portfolio Retrieved from http://www.ellieharrison.com/_homepage.htm   5. Java, A., Song, X., Finin, T., & Tseng, B. \(2007\ Why we twitter: understanding microblogging usage and communities. In Proceedings of the 9th WebKDD and 1st SNAKDD 2007 workshop on Web mining and social network analysis pp. 56-65\San Jose, California  6. Lenhart, A., Fallows, D., & Horrigan, J. \(2004 Content Creation Online ew Internet & American Life Project Retrieved June 15, 2008, from http://www.pewinternet.org/PPF/r/113/report_display.asp   7. Lewiston, C. Personal Pies. . Retrieved from http://www.flipflopflyin.com/personalpies   8.D.W. McDonald Visual Conversation Styles in Web Communities In Hawaii International Conference on System Sciences vol. 40, 2007, p. 1238   9. MK12. \(2007 Stranger than Fiction Intro Sequence  Retrieved June 14, 2008, from http://www.youtube.com/watch?v=9iWbAw-L1lg   10. Nardi, B. A., Schiano, D. J., & Gumbrecht, M. \(2004 Blogging as social activity, or, would you let 900 million people read your diary? In Proceedings of the 2004 ACM conference on Computer supported cooperative work pp 222-231\ Chicago, Illinois, USA  11. Z. Pousman, J.T. Stasko, and M. Mateas Casual Information Visualization: Depictions of Data in Everyday Life in IEEE Transactions on Visualization and Computer Graphics 2007, pp. 1145-1152  12. Serena Williams's Professional Career. . \(2007, August 19 The New York Times Retrieved June 14, 2008, from http://www.nytimes.com/interactive/2007/08/19/sports/200 70819_SERENA_GRAPHIC.html?adxnnl=1&adxnnlx=11 90759125-TH8+4WEzUOKHVLWjY9Hwkg   13. Superfad Gotta Go", Ad for Toyota Retrieved from http://www.superfad.com/clientlist.php?project=30   14. Viegas, F., Wattenberg, M., Mckeon, M., Ham, F Kriss, J. \(2008\. Harry Potter and the Meat-Filled Freezer A Case Study of Spontaneous Usage  of Visualization Tools. In Hawaii International Conference on System Sciences, Proceedings of the 41st Annual p. 159\trieved June 15, 2008, from http://dx.doi.org/10.1109/HICSS.2008.188 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 
 


Service Description Service Data Entity Replica Processing Unit Network Device Invoke Derive Specific Quality Non-Functional Performance Requirement Storage Unit Workstation User Profile Application Constraint Availability Transition Functional Data Entity Operation Behavior Utilization Network Process Capacity Module Initiate Invoke Initiate Server Role Load Site Node Call Requirements satisfied by Network Infrastructure View Requirements satisfied by Topology View Requirements satisfied by Functional View 1 1 1 1 1 1 1 1 1 1 source-outgoing source-outgoing target-incoming 1 1 1 1 outgoing1-source1 target -incoming1 source outgoing target incoming target1 incoming source outgoing1 1 1 source outgoing target 


target incoming target incoming target incoming source outgoing Figure 6. EIS System Network View Meta-model operations are those used for service description and must be ultimately decomposed into elementary ones \(i.e. data processing, storing and transferring are estimated through parameter values that are propagated by service invocation parameters to parameters describing application operations constituting the service description which are further propagated to parameters describing elementary operations 4.3. EIS Architecture Design Task Implementation EIS Architecture Design tasks may be supported by existing tools [20]. Systems Modeling Language \(SysML 15] is considered as the most appropriate for EIS System Network model representation and requirement engineering, since it supports the concepts of requirements and resource allocation. As a direct consequence, SysML allows the representation of requirements as model elements which means that requirements are part of the system architecture. For representation purposes, a SysML pro?le for EIS System Network meta-model \(?gure 6 mented as a plugin to MagicDraw modeling tool [2]. In order to facilitate model exchangeability, EIS System Network model is being realized in XML, which is a standard exchangeable format. In order to exchange data with speci?c software tools, model transformations will be accomplished through appropriate XSLTs developed for each tool for example as the one transforming XMI to the EIS System Network document type description \(DTD 5. Case Study In the following we discuss the case of renovating a legacy information system supporting a large-scale public organization based on the proposed concepts. The organization supports more than 350 interconnected regional of?ces and its main purpose is to provide services to the public both citizens and businesses. Regional of?ces are divided into three categories according to their size and information infrastructure requirements \(large, medium and small More than 15.000 employees work in the organization having on-line access to the legacy system. There are more than 300 different services provided to the public, while each citizen is required to register in the one belonging to his/her residential area, called residential of?ce. Some of them require the actual presence of citizens in their residential of?ce Existing system architecture is based on a fat clientserver architecture. All application logic is programmed within the client platform, while data is distributed in local database servers located in each regional of?ce. A Central database is supported in the Datacenter for data synchronization and lookup purposes. The Datacenter and all regional of?ces participate in a private TCP/IP network to facilitate ef?cient data replication. Most data related to a speci?c citizen are maintained as local data in his/her residential of?ce. Client programs access the local database to store data, while they access the central database mostly for lookup purposes. Local data are asynchronously replicated in the central database using a transaction management system \(TMS Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 to facilitate communication with the central database. The central database provides the overall view of each citizen  s record 


record To enhance the level of services provided by the organization, we decided to establish an enhanced e-services environment through an e-government portal. The main target of the portal is to minimize the need for citizen  s presence in regional of?ces and intents to deal with all the drawbacks of the current e-service platform. It provides easy access to citizens and businesses twenty four hours per day, seven days per week. It also promotes the increase of e-services to users, facilitating the organization to accomplish its strategic goals. The portal facilitates on-line transactional services and ensures on-line access to the databases of the legacy information system Provision of transactional e-services re?ects the operation of the legacy system and thus results in its renovation. In order to effectively support both systems \(e.g the portal and the legacy system able to apply the same policies and minimize maintenance cost. Thus, it was decided to explore the renovation of the legacy information system by adopting modern technological trends, such as multi-tiered application architecture server-based computing and light clients. It was decided also to rewrite application code based on J2EE architecture to develop a web interface for the legacy information system in order to support a uni?ed environment for both the legacy system and the portal. This decision affected the legacy system architecture, described in System Network View. Some of issues raised included: \(a database architecture? It is currently distributed. Should it become centralized? What are the implications in the network infrastructure? \(b complished to minimize maintenance cost Though EA was never fully described, the organization had already decided to establish an EA based on Zachman framework a few years ago. RUPmethodology was used for software development, thus application description models were developed within Rational Rose platform. In order to be able to apply the proposed tasks identi?ed in section 4, relative information had to be extracted from the corresponding cells. Application description \(e.g. applications and modules tracted from corresponding Rational Rose ?les. Though the process was not automated, the provision of System Network meta-model, helped architecture designer to identify the information needed to obtain from software designers Detailed service description in terms of load requirements could not be extracted from software description. This was crucial in order to decide upon Intranet and Datacenter architecture. This information was collected by interviewing software developers The new system has to deal with a number of require&lt;&lt;Site&gt;&gt Central Organization Building lt;&lt;ServerProcess&gt;&gt Oracle Central database lt;&lt;Site&gt;&gt Medium Regional Office lt;&lt;Site&gt;&gt Datacenter server room lt;&lt;Site&gt;&gt Large Regional Office lt;&lt;Site&gt;&gt Small Regional Office oracle L.R.O lt;&lt;ServerProcess&gt;&gt tuxido L.R.O lt;&lt;ServerProcess&gt;&gt lt;&lt;Site&gt;&gt L.R.O. department tuxido Central 


tuxido Central lt;&lt;ServerProcess&gt;&gt lt;&lt;Site&gt;&gt R.O. server room lt;&lt;ClientProcess&gt;&gt oracle net lt;&lt;Site&gt;&gt Organization lt;&lt;ClientProcess&gt;&gt tuxido lt;&lt;ClientProcess&gt;&gt application lt;&lt;UserProfile&gt;&gt officer lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;initiate&gt;&gt Figure 7. Topology View - Existing System ments, with security and availability being the most important ones. Security issues have to do with the security of the network, security of data, authentication control, etc. Availability requirements deal with the backup subsystem, the recovery system and high availability UPS. Privacy must be enforced with the use of cryptography and compression techniques. All these requirements were identi?ed during the System Architecture design process and consequently exported in System Motivation cell where all system requirements are gathered using a simpli?ed text-based requirement description method System Architecture design tasks were performed by existing tools already described in [20]. The existence of System Network meta-model and its implementation in XML facilitated tool integration and interoperability. The identi?cation of primary EIS engineering activities served by Zachman matrix rows and columns facilitated a better understanding between software developers, architecture designers and organization management and enhanced discrete methodology integration. Existing and renovated application architecture of the legacy system de?ned by Topology View are presented in ?gures 7 and 8 respectively. The screenshots are from the MagicDraw [2] tool, enhanced with EIS pro?le to provide the appropriate functionality 6. Conclusions &amp; Future Work MB-EISE process based on Zachman framework was explored in the paper. The designer may adjust basic MB-EISE activity model for each cell, formulate a methodology-independent EIS cell-related view, and ?nally identify methods and tools appropriate for implementing each speci?c task. One could argue that in such a case, 36 distinct EIS sub-views should be de?ned, each of them being rather complex, while basic MB-EISE activity should be adjusted 36 times, resulting in a very complicated process Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 lt;&lt;Site&gt;&gt Central Organization Building lt;&lt;ServerProcess&gt;&gt Oracle Application Server lt;&lt;ServerProcess&gt;&gt Oracle Central database lt;&lt;Site&gt;&gt Medium Regional Office lt;&lt;Site&gt;&gt Datacenter server room lt;&lt;Site&gt;&gt Large Regional Office lt;&lt;Site&gt;&gt 


lt;&lt;Site&gt;&gt Small Regional Office lt;&lt;Site&gt;&gt L.R.O. department lt;&lt;Site&gt;&gt Organization lt;&lt;ClientProcess&gt;&gt web browser lt;&lt;ServerProcess&gt;&gt Web Server lt;&lt;UserProfile&gt;&gt officer &lt;&lt;initiate&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt lt;&lt;invoke&gt;&gt Figure 8. Topology View - Renovated System However, EIS engineering process, as enterprise architecture, is itself complex. The bene?t of the proposed approach is that all aspects \(simple or complex form and modular fashion. Cell-related sub-views and corresponding meta-models, as well as cell-related MB-EISE activity model may be progressively formed according to the designer  s priorities and perspectives Having a black-box view of each Zachman cell, the proposed approach focuses on EIS view integration and interview consistency. The notion of external entities when de?ning EIS cell-related views provides the means for interoperability with external cells, while at the same time facilitates atomicity within the limits of each cell. We are currently emphasizing Business and System rows, and especially Function and Network cells, exploring in parallel Motivation column and the way non-functional requirements are managed Having a white-box view of each Zachman cell, it is evident that the de?nition of a technology neutral metamodel and the identi?cation of basic engineering tasks, corresponding to EIS cell-related views, contributes to the integration of different methodologies and tools. A library of EIS System Network models has been already implemented in XML. Emphasis is given to requirements management and especially requirements derivation References 1] Institute For Enterprise Architecture Developments http://www.enterprise-architecture.info 2] MagicDraw UML. http://www.magicdraw.com 3] A. Aurum and C. Wohlin. Engineering and Managing Software Requirements. Springer, 2005 4] F. S. d. Boer, M. M. Bonsangue, J. Jacob, A. Stam, and L. W N. v. d. Torre. A Logical Viewpoint on Architectures. In EDOC, pages 73  83. IEEE Computer Society, 2004 5] E. R. Byrne. IEEE Standard 830: Recommended Practice for Software Requirements Speci?cations, 1998 6] B. Dave and D. Jim. The new, improved RUP SE Architecture Framework, 2005. IBM Rational Edge 7] D. J. de Villiers. Using the Zachman Framework to assess RUP. Rational Edge, 2001 8] J. A. Estefan. Survey of Model-based Systems Engineering \(MBSE May 2007 9] A. Fatolahi and F. Shams. An investigation into applying UML to the Zachman Framework. Information Systems Frontiers, 8\(2  143, 2006 10] M. Glinz. On non-functional Requirements. 15th IEEE International Requirements Engineering Conference, 2007 11] F. Goethals, W. Lemahieu, M. Snoeck, and J. Vandenbulcke. An overview of enterprise architecture framework deliverables. In Banda RKJ \(ed Introduction. ICFAI University Press., 2006 12] H.-P. Hoffmann. Harmony-SE/SysML Deskbook: ModelBased Systems Engineering with Rhapsody, Rev. 1.51 Telelogic/I-Logix white paper. Telelogic AB, May 2006 


Telelogic/I-Logix white paper. Telelogic AB, May 2006 13] IEEE. IEEE Recommended Practice for Architectural Description for Software-Intensive Systems - Std 1471. Technical report, oct 2000 14] O. M. G. Inc. UML 2.0 Superstructure Speci?cation, October 2004 15] O. M. G. Inc. Systems Modeling Language \(SYSML i?cation. Version 1.0, September 2007 16] INCOSE. INCOSE Handbook SE Process Model, September 2003. http://g2sebok.incose.org 17] Institute for Electrical and Electronic Engineers. IEEE Std 15288 -2004, Systems Engineering -System Life Cycle Processes, June 2005 18] A. v. Lamsweerde. Goal-Oriented Requirements Engineering: A Guided Tour. In Fifth IEEE International Symposium on Requirements Engineering \(RE  01 19] S. Leist and G. Zellner. Evaluation of current architecture frameworks. In H. Haddad, editor, SAC, pages 1546  1553 ACM, 2006 20] M. Nikolaidou and N. Alexopoulou. Enterprise Information System Engineering: A Model-Based Approach Based on the Zachman Framework. In HICSS  08. IEEE Computer Society, 2008 21] M. Nikolaidou and D. Anagnostopoulos. A systematic approach for con?guring web-based information systems Journal of Distributed and Parallel Databases, 17\(3  290, May 2005 22] C. M. Pereira and P. Sousa. A method to de?ne an Enterprise Architecture using the Zachman Framework. In H. Haddad A. Omicini, R. L. Wainwright, and L. M. Liebrock, editors SAC, pages 1366  1371. ACM, 2004 23] K. Pohl and E. Sikora. Supporting the Co-Design of Requirements and Architectural Artifacts. In 15th IEEE International Requirements Engineering Conference \(RE  07 pages 258  261, India Habitat Center, New Delhi, 2007 24] J. Schekkerman. How to Survive in the Jungle of Enterprise Architecture Frameworks: Creating or Choosing an Enterprise Architecture Framework. Trafford, 2003 25] J. F. Sowa and J. A. Zachman. Extending and Formalizing the Framework for Information Systems Architecture. IBM Systems Journal, 31\(3  616, 1992 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


C. \(2005 Implementation and Use in an Existing Clinical Information System. In Connecting Medical Informatics and Bio-Informatics: Proceedings of MIE2005 - The XIXth International Congress of the European Federation for Medical Informatics, 328-333. IOS Press, 2005  4] Fetterman, D. M. Ethnography, 2nd ed. Thousand Oaks CA: Sage, 1997  5] Furukawa, N.  Ikeda, H.  Kato, Y.  Sako, H. D-Pen: a digital pen system for public and business enterprises. In Frontiers in Handwriting Recognition 2004: Proceedings of the Ninth International Workshop on Frontiers in Handwriting Recognition \(IWFHR-9 2004  6] Guimbreti  re, F. 2003. Paper augmented digital documents. In Proceedings of the 16th Annual ACM Symposium on User interface Software and Technology Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 UIST ?03 November 2003  7] Holman, D., Vertegaal, R., Altosaar, M., Troje, N., and Johns, D. 2005. Paper windows: interaction techniques for digital paper. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems \(CHI ?05 Portland, Oregon. New York: ACM, 2005  8] Kiewra, K., DuBois, N., Christian, D., McShane, A Meyerhoffer, M., &amp; Roskelley, D. Note-taking functions and techniques. Journal of Educational Psychology, 83 240-245, 1991  9] Kobayashi, K. Combined effects of note-taking/reviewing on learning and enhancements through interventions: a meta-analytic review. Educational Psychology, 26, 459-477, 2006  10] Liao, C., Guimbreti  re, F., and Hinckley, K. 2005 PapierCraft: a command system for interactive paper. In Proceedings of the 18th Annual ACM Symposium on User interface Software and Technology \(UIST ?05 Seattle, WA. New York: ACM, 2005  11] Livescribe. Pulse Smartpen [electronic device http://www.livescribe.com/, last retrieved May 28, 2008  12] Logitech. io2 Digital Pen [electronic device http://www.logitech.com/index.cfm/mice_pointers/digital_ pen/devices/408&amp;cl=us,en, last retrieved May 28, 2008  13] Norrie, M. C., Signer, B., and Weibel, N. Print-n-link weaving the paper web. In Proceedings of the 2006 ACM Symposium on Document Engineering \(DocEng '06 New York: ACM, 2006  14] Randall, D., Harper, R., and Rouncefield, M Fieldwork for Design: Theory and Practice. London Springer-Verlag, 2007  15] Searle, J. R. Speech Acts: An Essay in the Philosophy of Language. Cambridge: Cambridge Univ. Press, 1969  16] Sellen, A. J. and Harper, R. H. The Myth of the Paperless Office. Cambridge, MA: MIT Press, 2003  17] Signer, B. and Norrie, M. C. 2007. PaperPoint: a paper-based presentation and interactive paper prototyping 


paper-based presentation and interactive paper prototyping tool. In Proceedings of the 1st international Conference on Tangible and Embedded interaction \(TEI ?07 Baton Rouge, Louisiana. New York: ACM, 2007  18] Tanabe, K., Yoshihara, M., Kameya, H., Mori, S Omata, S., Ito, T., Automatic Signature Verification Based on the Dynamic Feature of Pressure. Proceedings of the Sixth International Conference on Document Analysis and Recognition \(ICDAR ?01 Computer Society, 2001   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207–216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Int’l Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Int’l Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





