A Research about Indepe ndent Tasks Scheduling on Tree-Based Grid Co mputing Platforms    LI Jun 1 Li Chunlin 2 Li Qingqing 2  Institute of Computer Science Wuhan University of Technology Wuhan, CHINA Email :{ kevin.leejun@gmail.com 1 chunlin74@yahoo.com.cn 2 liqingqing2884@163.com 2    Abstract Task scheduling has been one of the hot and difficult problems in grid computing, it is a big challenge to design an efficient scheduling algorithm. This paper discusses the problem 
of independent tasks scheduling on tree-based grid computing platforms, a small heap tree \(virtual resource tree\model was proposed and on the model we propose a tasks scheduling heuristic algorithm based-on linear programming. In this algorithm, we consider the computing power and bandwidth for each node in the model and assign task for each node in an integrated manner. The algorithm analysis shows the proposed algorithm is rational and effective Keywords-grid; task scheduling; small heap tree; linear programming; heuristic algorithm I   I NTRODUCTION  In the grid system, tasks scheduling is an important part of 
it. According to the tasks information, tasks scheduling should make some appropriate strategies to assign the different tasks to the appropriate resources node to run it. The target of the grid tasks scheduling is to achieve optimal scheduling and to improve the overall throughput of grid system. Reference [1 described the characteristics and the main objective of grid task scheduling. Reference i s c usse d t he  c o m p l e xi t y o f  independent tasks scheduling in the tree-based multi-processor computing platform. In referenc  a t r e e P e t r i m o de l  fo r grid resource scheduling is presented. References [4-5 discussed the tasks scheduling problem in the tree-based grid 
computing platforms, and the outcome of the simulation in [4  proved that the tree structure can simplify the implementation of network platform and reduce the communication complexity of the nodes to facilitate the management of grid resource. In reference [5  th e au th o r a n aly zed th e in d e p e n d en t  ta sk s scheduling problem in the tree-based grid computing platforms turn the independent tasks scheduling into linear programming problem. And based-on the linear programming, two demanddriven and dynamic heuristic algorithms for task allocation are proposed According to the gird task scheduling goal i   a nd based-on the tree-based grid computing platforms, a small heap 
tree for virtual resource model was proposed, using the model we proposed a heuristic algorithm for task allocation. In this algorithm, the computing and bandwidth of the node was considered in the model in an integrated manner II  T ASKS SCHEDULING  ON TREE BASED GRID COMPUTING PLATFORMS  A  A Tasks Scheduling Model on Tree-based Grid Computing Platforms Let tree structure as a communication model in the grid computing environment, this can simplify the realization of grid computing, reduce the complexity of the node 
communication. Because in the model, each node is only need to communication with the father node and the son node, root node to each node has only one rout  C o nsi d er o f t h ese  characteristics on the tree-based grid computing platforms, we proposed a small heap tree for virtual resource model According to the computing power of each node on the treebased grid computing platforms \(the computing power of a node is measured by the time to execute a unit of the task, the computing power of a node is more strong, it takes less time to execute a unit of the task\ hierarchically traverse the treebased platforms to build a small heap tree for virtual resource model dynamically. The model features as follows 
200  Consider costs of the task relocation, it takes time to transfer the task 200  Consider the nodes in the tree-based grid computing platforms can communication with each other. When we create the virtual resource model, transfer a unit of task in the small heap tree to obtain the relative communication capabilities between the father nodes and the son nodes 200  The tasks are independent to each other 200  One task can only be completed by one node; one node can only compute one task at one time 200  Single-port master / slave mode task scheduling 
978-1-4244-5874-5/10/$26.00 \2512010 IEEE 
200  All the tasks import from the root node A small heap tree virtual resource model is shown in Figure 1; it\222s a complete binary tree The work was supported by the National Natural Science Foundation of China \(NSF\ under grants \(No. 60773211, No. 60970064\, Program for N ew Century Excellent Talents in University, China \(NCET-08-0806\, and the National Science Foundation of  Hubei Province under Grant N o.2008CDB335 
 


  Figure 1. A small heap tree virtual resource model The symbols in Fig.1 are described below   P 0 P 1  P k-1 indicate the nodes in the virtual resource model   c 0 c 1  c k-1 indicate the computing power of nodes, the time cost to execute a unit of task   t 1 t 2  t k-1 indicate the communication capabilities between the father node and its son nodes the time cost to transfer a unit of task x i is defined as the number of task which is assigned to the P i node, so the P i node will take c i x i time to complete the x i  tasks, and will take t i x i time to receive the x i tasks; M is defined the total number of the tasks which need to scheduling, T is defined the time to complete all the tasks Based-on the tree-based grid computing platforms, the independent tasks scheduling can be turned into the linear programming problem [5  s o m e s y m bol s a b ou t  linear programming equations as follows   x i is the number of task which is assigned to a node and M is the total number of the tasks, so we can get 1   The number of assigned tasks\(x i or each node should no more than the total number of the tasks M, so we can get \(2   The time of the root node to execute the assigned tasks should no more than the total tasks completion time so we can get \(3   The time of each node to receive and execute the assigned tasks should no more than the total tasks completion time, so we can get \(4   Each sub-tree has a bandwidth constraint, so we can get \(5\. For example, the time of the root node spend to transfer all the tasks should no more than the total tasks completion time T    0 j t Z jY p p j T x x j  Z 0 is a node set which contain the root nodes son nodes,  Y j is a node set which contain the node behind node P j the number of the tasks which root node transfer to its son nodes is   j Y p p j x x x j is the number of tasks which the node P j would compute  j Y p p x is the total number of tasks that the node P j s descendants nodes should compute, M, T, x i c i Y j all are integer. So the linear programming equations as follows                j i i Z jY p p j k i i i i k i i Y c x T M T x x T c x T c x k i M x M x j  6  t  5    4    3  1 0  0  2   1  0 j 1 0 0 0 1 0  In the above equations, M, t j c i Y j are known parameters, x i is a variable, Minimize T is the objective function B  An Example of Independent Tasks Scheduling on the platform For example, a tree-based grid computing platform is shown in Figure 2  Figure 2. A tree-based grid computing platform Based-on the platform in Fig.2, a small heap tree can be created. The virtual resource model based-on Fig.2 is shown in Figure 3  Figure 3. The virtual resource model \(small heap tree 
 


In the above model, the set of the node computing power C={5, 7, 6, 8, 10}, the set of the bandwidth t={1, 2, 1, 3 Using the above linear programming to solve the tasks scheduling problem, with Lingo tool, when M=8, the optimal result is {x 0 x 1 x 2 x 3 x 4 2, 2, 2, 1, 1}, Minimize T=15 when M=50, the optimal result is {x 0 x 1 x 2 x 3 x 4 15, 11 13, 4, 7}, Minimize T=80. Minimize T to meet \(T, x 0 x 1 x 2 x 3  x 4 are integer                     T x x x x T x T x T x T x T x M x x x x x 4 3 2 1 4 3 2 1 0 4 3 2 1 0 4 2 2 1  7  3 1 10  6  1 1 8  5  2 6  4  1 7  3  5  2   1   III  H EURISTIC T ASKS S CHEDULING A LGORITHM BASED ON S MALL H EAP T REE M ODEL  According to the linear programming model above, a heuristic tasks scheduling algorithm based-on the small heap tree model was proposed. In this algorithm, using the linear programming model to get the assigned task number for each node, then considered the computing and bandwidth of the node in the model and in an integrated manner to assign task for node. For each non-leaf node, we make the following deal when a son node requests task from the father node, first checkout whether it can accept more task, if yes, assign a task to it; when more than one nodes request task, checkout their task number and select the best node to assign a task to it Because bandwidth has a greater impact on task scheduling than computing power [5 s o w h e n se l e c t  t h e be st nod e w e  adopt the following strategy: first, compare the bandwidth between the son nodes, if they have the same bandwidth then select the node which computing power is more strong, else select the node which bandwidth is more wide. The heuristic task scheduling algorithm code described as follows Procedure task_assign_OPCHATA CreateHeapTree\(\      //create the small heap tree PreAssign\(\    //using the linear programming model to pre-assign task to each node While\(Receive_Task\(\\  //when receive task from the father node Add_task\(Queue\    //put the task into the queue End While While\(Receive_req\(\  //receive the task request from son node Add_req\(Queue_Req\(P x put the task request into the request queue End While If\(Exist_task\(Queue\\ //task queue is not null If\(array\(P i 0\   //node P i can accept more task Execute_task\(Queue\ //execute one task in the task queue End If Select_best\(Queue_Req \(P x select the best node from the request queue  If\(array\(P x 0\  // node P i can execute one more task Trans_task\(P x transfer a task to node P x  Del_req\(Queue_Req \(P x delete node P x from the request queue End If Else   //send the task request to its father node Send_req\(Pi End If End The code of Select_best\(\ function Select_best\(Queue_Req\(Px  Compare_bandwidth\(\   // compare the bandwidth between the son nodes If\(bandwidth_is_equal\(\  //they have the same bandwidth Compare_compute Select_maxCompute Else Select_maxBandwidth EndIf  CreateHeapTree\(\as used to create a small heap tree based-on the computing power of the nodes in the tree-based grid computing platforms. In the PreAssign\(\ function, using Karmarkar algorithm to solve the linear programming equations, so that to get the task scheduling for each node, and then put the task scheduling result into an array which is called array, such as array m ean s t h e n u m ber  of tas k s th a t th e n o de  P i would process IV  A LGORITHM P ERFORMANCE A NALYSIS  In our paper, we use the SimGrid 7 to simulate and analyse the algorithm which is proposed. SimGrid offers a series of core functions to create and simulate the heterogeneous distributed environment 7 so its suitable for simulating the independent task scheduling algorithm in the grid environment For example, The optimal solution of the model in the Fig.3 is x 0 x 1 x 2 x 3 x 4 2, 2, 2, 1, 1}, the task scheduling is shown in Figure 4  Figure 4. The  optimal solution task scheduling The level of the solid line with arrows indicate the execute time of the task, the slash with arrows indicate the transfer of the task. When M=8, The time to complete all tasks is 18, so T=18, which is close to the Minimize T\(T=15  0  1   2   3  4   5  6  7   8   9 10 11 12 13 14 15 16 17 18  19  P 0 P 1  P 2  P 3  P 4  
 


From the Fig.4, we can get the calculate completion time\(T\(P i of each node: T\(P 0 10, T\(P 1 15, T\(P 2 8 T\(P 3 14, T\(P 4 17. The computing time of each node\(T c P i  is equal to the calculate completion time minus the transfer time of each node, also we can get the computing time of each node from the Fig.4: T c P 0 10, T c P 1 15, T c P 2 4 T c P 3 12, T c P 4 14. Because the linear programming model doesnt consider the node cannot transfer two tasks at one time so the Minimize T should equal to the maximum of the computing time of each node in the model. The time of each node spend on the computing is equal to the completion time minus the transfer time, the computing time of each node is T c P 0 10, T c P 1 15, T c P 2 14, T c P 3 12, T c P 4 14, so the Minimize T=max\(T c P I 15, this verify the correct of the model, the scheduling in the Fig. 4 is the optimal scheduling V  S UMMARY AND O UTLOOK  Scheduling algorithm has been one of the hot and difficult problems in grid computing. In this paper, we proposed a small heap tree model and a heuristic task scheduling algorithm. In this algorithm, we consider the computing power and bandwidth for each node in the model and assign task for each node in an integrated manner, then use the SimGrid [7  to  simulate and analyze the algorithm, and get good result. But the algorithm still has some problems, such as didnt consider the tasks computing and bandwidth requirements, the load balancing, dependencies between tasks. There are all the problems we will concentrate on next A CKNOWLEDGEMENTS  The work was supported by the National Natural Science Foundation of China \(NSF\nder grants \(No. 60773211, No 60970064\, Program for New Century Excellent Talents in University, China \(NCET-08-0806\, and the National Science Foundation of Hubei Province under Grant No. 2008CDB335 Any opinions, findings, and conclusions are those of the authors and do not necessarily reflect the views of the above agencies R EFERENCES  1  LUO Hong, MU De-jun, DENG Zhi-qun, WANG Xiao-dong, A Review of Job Scheduling for Grid Computing\(in Chinese\ Computer Application and Rearch, 2005, 5, pp. 529551 2  Dutot P, Complexity of master- slave tasking on heterogeneous trees European Journal on Operational Research, 2005, 164\(3\, pp.690-695 3  ZHOU Juan, LIU Jue-fu, LI Pei-song, MA Feng-wei, Grid Resource Scheduling Model Based on Tree-Petri Net\(in Chinese\, Computer Engineering, 2008, 34\(17\ pp. 271350 4  TAN Yi-ming, ZHANG Miao, ZHANG De-xian, Adaptive Tasks Scheduling Algorithm in Tree Grid Computing Environment\(in Chinese\ Computer Engineering, 2008,34\(17\ , pp. 62-64 5  LIN Wei-Wei, QI De-Yu, LI Yong-Jun, WANG Zhen-Yu, ZHANG ZhiLi, Independent Tasks Scheduling on Tree-Based Grid Computing Platforms\(in Chinese\ Journal of Software, 2006, 17\(11\, pp. 2352-2361 6  GUI Xiaolin, Grid Computing Technology\(in Chinese\, Beijing University of Posts and Telecommunications Press, 2004, pp. 148163 7  XIA Jingbo, LIU Ying, WANG Shengrong, Theory and Development of Grid\(in Chinese\, Xi'an University of Electronic Science and Technology Press, 2006, pp.250-263  
 


SRB server to get the results Step3, SRB Server looks up information in database. When the SRB server gets the queries, it will query its metadata database to see whether the metadata information of the query data is there, and then tell the central server S which field station server has the data 411 Step4, get the data from field station server. After get the query results, the central server will return the results to the user U, and the user can send the request to the field station server T according to the query results from central server If the user has the enough rights, the field station server T will send the data to the user D. Data Analysis and Visualization After we have transmitted the data to the central server we do some analysis based on the data stored in the central server database. In FEDC framework, we used GIS technique to visualize the monitor data, and some flash charts to visualize the real-time flux data. Figure 3 shows that results of using the flash chart to show the real-time wind speed and 2co  flux data, and figure 4 shows that using the map to visualize the status of local area network in the field station   Figure 3. The real-time visualization of flux data    Figure 4.  The visualization of monitor data  IV. CONCLUSIONS In this paper, we proposed a framework named FEDC which can be used in large scale field ecological data collection for the ChinaFlux project. This framework can be used to manage and transmit heterogeneous data generated by different kinds of collection equipments. We implemented the data collection layer, transportation layer management layer, analysis and visualization layer in FEDC framework, and every layer has different kinds of tasks in the field ecological data collection, and the result shows that it has good performance in field ecological data collection and management ACKNOWLEDGEMENTS We would like to thank Professor Honglin He, Doctor Xuefa Wen from Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences for their open idea, discussion, cooperation, and contribution. This work was supported by the Knowledge Innovation Program of the Chinese Academy of Sciences No.O815021108  REFERENCES 1] GR Yu, XF Wen, XM Sun, BD Tanner, X Lee, JY Chen. Overview of ChinaFLUX and evaluation of its eddy covariance measurement Agricultural and Forest Meteorology, 2006  2] F Vernon, T Hansen, K Lindquist, B Ludaescher. ROADNET: A Realtime Data Aware System for Earth, Oceanographic, and Environmental Applications.  Eos Transactions \(American Geophysical Union fall meeting  3] BM Howe, T McGinnis. Sensor networks for cabled ocean observatories. Underwater Technology, 2004  4] C Cotofana, L Ding, P Shin, S Tilak, T Fountain. An SOA-based Framework for Instrument Management for Large-scale Observing Systems \(USArray Case Study Conference on Web  5] MT Ritsche, DJ Holdridge, R Pearson. New and Improved Data Logging and Collection System for Atmospheric Radiation Measurement 


Climate Research Facility, Tropical Western Pacific, and North Slope of Alaska Sky Radiation, Ground Radiation, and MET Systems. Fifteenth Atmospheric Radiation Measurement, 2005  6] C Baru, R Moore, A Rajasekar, M Wan. The SDSC storage resource broker. Proceedings of the 1998 conference of the Centre for Advanced Studies on Collaborative research,1998  7] A Rajasekar, R Moore, F Vernon. iRODS: A Distributed Data Management Cyberinfrastructure for Observatories. American Geophysical Union, Fall Meeting 2007  8] S Weibel. Metadata: the foundations of resource description portal.acm.org, 1995 412 pre></body></html 


our idealized scenario, we have a unimodal fitness landscape, and thus our local best fit will be the global best fit; however, this will not be the case for more complex situations, having multimodal landscapes.  Such scenarios are to be expected for implementations that are more realistic Substitutions are another way of injecting diversity into the gene pool. With substitution, values for individual genes are swapped. In the present scenario, this is akin to changing a latitude value while fixing longitude, or vice versa. Lastly a small fraction of guesses by the ECM are  clones  of the parent, having identical genetic structure. The combination of these four processes \(crossover, mutation, substitution and cloning optimal genotype while eliminating weaker genetic structures.  The ECM design allows the user to select the ratio of guesses that are produced via crossover vs. other processes Some of these genetic processes can be seen operating in Figure 3, which shows a typical example of the ECM  Figure 3:  Location of 100 plume source location guesses in each of four different generations  spacecraft observations of the circular analytical plume with 0% noise is the reference case; crossover/mutation is 50/50  see text and Figure 4 cross represents a single guess. Guesses are essentially random in Generation 1, but begin to  converge on a solution by Generation 5.  Convergence continues through Generation 10, and by Generation 20, most guesses  are within a fraction of a degree of the correct solution  behavior over several generations \(spacecraft observations of the circular analytical plume with 0% noise are assumed In each generation of this example, the ECM makes 100 guesses of the plume center location, with a 50/50 ratio of genetic crossover to the other processes. Each of the crosses in Figure 3 represents the location of a single guess \(of the center of an individual test plume region in which our steady state plume is located \(as illustrated in Figure 1 region to search only a 20  x 20  box that encompasses the observations Generation 1 shows the arbitrary distribution of the first 100 guesses. For each of these guesses, the plume model prepares an idealized plume centered at the latitude longitude of the guess, measurements of which are then compared to the set of spacecraft observations. Following the algorithm discussed above, those guesses that most closely fit the observations have their genetic information carried forward into the subsequent generation to seed a portion of the next 100 guesses, with the remaining portion being generated through mutation, substitution and cloning As quickly as in Generation 5 \(Figure 3, top right 7 significant number of guesses are seen converging on a solution. This convergence improves through Generation 10, although we still see the same fixed number of mutations \(the point at 9  latitude, -6  longitude is an example of a mutation have converged to within a fraction of a degree of the correct solution at \(0  0   substitution of a single gene on a number of guesses in Generation 20, which produce the  cross  shape apparent in the figure. For these guesses, either latitude or longitude is replaced, with the other gene held at its prior best-fit value By adjusting the fraction of mutations and substitutions correctly, one can balance the need for diversity with speed of convergence. In the absence of any mutations or substitutions, the solution will converge on the first local best fit it obtains, which may or may not be the global best fit. At the other extreme, a 100% mutation/substitution rate yields scattershot, or random, guesses, and provides no 


yields scattershot, or random, guesses, and provides no computational advantage over a brute-force solution, since determination of the global best fit only can be made after the full parameter space has been explored. A optimal medium exists somewhere between these two extremes.  A series of tests were performed to identify the best ratio to use in our ECM simulations \(Figure 4 tests was run out for 40 generations with 100 guesses in each generation, and the best-fit solution \(the lowest value of f from the 100 guesses ratio is ~50% crossover, 50% mutation \(including cloning and substitution search, but also provides efficient convergence to the global best fit  Figure 4: Demonstration of improved fitness for various ratios of crossover to mutation in a  sample ECM run \(spacecraft observations of the circular analytical plume with 0% noise is the reference case  identical plume models were run for 40 generations, but with each having the indicated percent of crossover and mutation. Lower  values of the ordinate indicate a better match between the plume model and observations. We see that a 50/50 split between  crossover and mutation provides the quickest path to a best-fit solution  We use this ratio for all the tests performed in Section 4 Generally, we obtain good convergence in 25 or fewer generations, each containing 100 test plumes \(2500 total compared with the more than one million required for a brute-force search over the 1440 x 720 points in the original grid Other minimization algorithms, such as LevenbergMarquardt \(L-M G-N significant disadvantages over the ECM for this type of problem.  First, because L-M and G-N are gradient-based algorithms, they require the landscape of the fitness function being minimized to be smooth, with a well-defined gradient. The function, f, being minimized in our tests however, has a sharp  spike  at the best-fit value, and a flat gradient most everywhere else. \(In other words, test plumes that do not exactly match the observations generally have the same poor fitness value regardless of the selection of genes global, minimizers.  If a guess is made and falls close to a 8 local minimum, the algorithm can be trapped around the local minimum and never find the global best fit.  The design of the ECM alleviates both of these problems  it functions well even with poorly defined gradients, and is designed to search for and identify the global best-fit solution There are other alternative approaches to identifying global minima such as the simulated annealing \(SA do not suffer the aforementioned problems.  The SA approach, as with the ECM, iteratively selects new solutions to the minimization problem; however, the decision to  accept  the new solution in SA is typically made in a probabilistic sense, independent of the new solution  s fitness.  With the ECM, a new solution is only accepted if the fitness is improved over the prior best fit There are two additional drawbacks to applying an approach like SA to this particular problem, although they best manifest themselves under less idealized conditions than those used here.  First, SA cannot handle problems that involve multiple competing objectives.  For example, we may be interested in identifying not only the source location of a plume, but also the surface emission flux or duration of emission. For such a problem, SA is forced to use an aggregated single objective by combining the competing objectives into a single weighted objective function, which is then iteratively optimized.  The ECM, on the other hand 


is then iteratively optimized.  The ECM, on the other hand can handle these multiple, simultaneous objectives without compromising accuracy in achieving each objective, and hence, is significantly more flexible Second, SA cannot find multiple local optimal solutions simultaneously.  In future implementations of our plume identification algorithm, we will likely have environments with multiple plumes, and minima corresponding to each solution.  The ECM is capable of efficiently handling this problem by partitioning the search space into multiple parts and having sub-populations, each of which focuses on a different part of the domain.  Since SA is a one-solution optimization algorithm, it cannot find multiple local solutions simultaneously For the present task, SA provides a viable alternative to obtaining the location of our simulated, idealized plume however it quickly becomes impractical when the problem becomes more complex and more realistic. Under those conditions, the flexibility of the ECM becomes a great asset 4. RESULTS A number of tests were performed to quantify accuracy in determining plume surface location as a function of the number of observations, level of instrument noise observation footprint size, and knowledge of actual plume shape. For most of the tests discussed in this section, we have assumed the simple, circular plume shape for both the actual and test plumes. This allows us to focus specifically on the singular objective of each test without dealing with differences in plume shape.  The exception to this broad assumption, is, of course, when we evaluate the role of our knowledge of actual plume shape itself, in which case we retain the circular shape for our actual plume and employ the zonally asymmetric one for our test plume Number of Observations We have shown previously \(Figure 1 circular plume appears like when observed for different lengths of time. We have performed a series of tests without random noise, for varying observation periods \(1 day, 10 days, 100 days, 1000 days using only observations that fall within  10  of what appears to be the plume center. Because of the Gaussian shape of the plume, tracer strengths at locations distant from the center are inconsequential and their contribution to f in Eq. 3 can be ignored.  Consequently, we can reduce our search space \(i.e. the region in which the ECM makes its guesses  x 20  box, which encloses the region with the strongest observations \(again, our steadystate approximation assures us that the plume source location must be within the region of strongest observations We have found that the accuracy in finding the correct plume center location generally increases as the number of measurements increases, as shown in Table 1. The results for the best-fit surface source location \(latitude and longitude with corresponding error in km generations. This demonstrates that, while it is not apparent from a cursory glance at Figure 1 that the source location of the true plume can be gleaned from the spacecraft coverage with any precision \(as the blurring of the original plume is quite evident amount of information to allow the best-fit latitude and longitude to determined by the ECM with high accuracy With observations of such a surface source over several tens of days, we should be able to decrease the source location uncertainty to levels that fall within the mobility range of future surface rovers \(20-40 km days or less of coverage \(about 1.5 martian years length of time of the primary science phase of most Mars missions Table 1:  Best-fit latitude and longitude positions for the circular plume model for varying lengths of observation 


circular plume model for varying lengths of observation campaigns Campaign Length days Best-Fit Latitude   Best-Fit Longitude  Error [km 1 -0.3476 -0.6190 42.0 10 -0.2522 -0.4613 31.1 100 -0.1862 -0.4960 31.3 1000 -0.1834 -0.2507 18.4 9 Instrument Noise Every observational measurement includes some level of noise in addition to the  true  signal. As such signal-to-noise problems are unavoidable, we must attempt to interpret their influence on the measurement result by introducing random noise into each observation and gauging the effect this has on our best-fit solution. To emulate, qualitatively, noise of various amounts, in separate tests we have added a noise term with a 3? distribution of either 10, 100 or 1000% of the local observation strength to each individual spacecraft observation. The impact of this noise term on the circular plume, relative to the perfect \(0% error Figure 5 There are 100 days of observations in each example. The results for the determination of the plume source location are presented in Table 2. While Figure 5 shows only one instance in which random noise at a specific amount has been applied, for Table 2  at each level of measurement uncertainty  the evaluation of plume source location was done ten times to account for the random factor and the variance among these ten cases is what is reported For low levels of measurement noise, in the tens of percent range, the results are largely indistinguishable from the noise-free standard, and uncertainty in position remains low relative to the noise-free instance from 10% to 100%, and then to 1000%, the ability to identify the plume source location becomes significantly diminished. With 100% uncertainty, individual measurements can vary by up to a factor of two, thus introducing  ghost  local maxima in the signal which are then interpreted as being close to the plume source location This phenomenon is significantly increased in the 1000 case. Note that, while Figure 5 shows only one case of random noise applied to the observations, the ghost local maxima do move around at this level of noise in different samplings, thus partly explaining the high uncertainty in source location  Figure 5:  Illustration of varying levels of instrument noise on the noise-free plume  measurement \(upper left of observations. As instrument noise increases, the plume source location is even less obvious  so that the accuracy to which the plume source location can be isolated is reduced 10 Table 2:  Variance in best-fit latitude and longitude solutions produced by the ECM for varying measurement uncertainty.  Source location uncertainty is relative to 0 measurement uncertainty value Measurement Uncertainty Source Location Uncertainty [km 0 0 10 0.6 100 31.5 1000 96.5  


 Footprint Size We performed a series of experiments demonstrating the role of measurement footprint size on source location uncertainty. As the footprint size decreases, the spatial extent over which the signal is averaged decreases and, in principle, provides better isolation of the peak signals from the adjacent, weaker  wings  of the plume \(Figure 6 footprint size is decreased from 200 km to 10 km, the retrieved shape from the observations better matches the  truth  although at the expense of a decrease in the area sampled in the same spatial domain. Table 3 bears out the competing influence of increased signal isolation and decreased spatial coverage for a 100-day observation campaign.  From a 200 km \(cross-track footprint, there is a ~45% decrease in uncertainty, but beyond this, there is no further gain. Figure 6 illustrates the cause of this strange behavior.  At 200 km \(upper left have near complete surface coverage, with substantial overlap, over the course of 100 days.  At 100 km \(upper right beginning to see small gaps in coverage, suggesting that for 100 days of observations, the amount of  repeat  coverage is small.  Hence, by reducing the cross-track footprint from 200 km to 100 km, we are provided the benefits of a smaller footprint  Figure 6:  Comparison of different observational footprint sizes.  Top left panel \(200 km  the nominal footprint size used in this study.  As footprint size is reduced, the shape of the plume is better defined, but at  the expense of spatial coverage Additionally, the peak plume strength in the observation set approaches that of the  true   plume \(cf. Figure 2b in black space indicates the fraction of the surface not observed by the instrument 11 Table 3:  Best-fit latitude and longitude positions for the derived plume source locations and consequent error for varying footprint widths Footprint Width km Best-Fit Latitude   Best-Fit Longitude  Error [km 200 -0.1862 -0.4960 31.3 100 -0.1523 -0.2485 17.2 50 -0.1178 -0.2390 15.8 20 -0.1195 -0.2414 15.9 10 -0.1276 -0.2391 16.0  but are not sacrificing overall surface coverage. For footprint sizes smaller than 100 km \(Figure 6, lower row the two effects counter each other relatively equally, and there is generally no improvement to the best fit. However as was discussed previously, for a fixed footprint size, an increase in the number of observations \(up to the point where coverage begins to substantially overlap improve the best fit. Footprint size is not an easily adjustable parameter for a spacecraft instrument and appears not to be as significant a factor in the plume source location error budget as is measurement spatial coverage Plume Shape In previous tests, we used the idealized case of a circular plume and assumed the same shape for the test plumes However, in reality, we will have no knowledge of the actual shape of the plume from our limited, defocused measurements, so we will need to make an educated guess as to the plume shape that is being observed. While knowledge of the meteorological context of the plume observations may be available, which would allow a 


observations may be available, which would allow a simulation of the plume shape using a GCM \(plume lifetime can be inferred from the plume  s spectroscopically determined composition discrepancy between the real plume shape and the GCMderived shape used for the test plumes in the ECM analysis of the observational data. We simulated this uncertainty by adopting our real plume shape from Eqn. 1 \(circular the test plume from Eqn. 2 \(zonally asymmetric expect this difference to result in a generally poorer solution. In fact, the results in Table 4 for lower surface coverage \(1 and 10 day campaigns error with coverage is not certain to be monotonic Interestingly, the errors in localizing the source when there is a reasonably high degree of surface coverage by the measurements are comparable to the results in Table 1 \(both cases assuming noise-free measurements reasonable, but not perfect, understanding of the observed plume shape, a campaign length of several tens of days or more, again, brings the uncertainty in plume source location to within the range of a landed rover Table 4:  Best-fit latitude and longitude positions for different campaign lengths, but assuming different actual and test plume shapes Campaign Length days Best-Fit Latitude   Best-Fit Longitude  Error [km 1 -0.5361 -0.2151 34.3 10 -0.1574 1.1961 71.5 100 -0.1692 0.2477 17.8 1000 -0.1389 0.2569 17.3  5. FUTURE PLANS A more ambitious implementation of this scheme would incorporate output of a full GCM in lieu of our basic analytic test plume model in the ECM framework. With this approach, the set of ECM guesses in a particular generation would spawn a suite of GCM model runs, with the  best guess  solutions from the completed GCM runs identified by the ECM and used to prepare the next generation of guesses.  Such an approach would be entirely autonomous and the best application of the GCM to the problem In the simplified demonstration herein, the importance of the ECM is somewhat understated.  Indeed, for simple plume shapes, a reasonable guess of plume location can be made directly by identifying the peak value in the observation set. This is an unfortunate side effect of this simple, two-gene demonstration.  The real value of the ECM manifests itself when we consider a parameter space of multiple genes.  Under more complex scenarios \(multiple plumes, various lifetimes, etc certainty, isolate individual plume locations. If the steadystate assumption is not valid, and the plume is evolving with time, the age of the plume becomes an additional gene we must consider.  Preliminary exploration of plume age as a third gene has begun, and we have obtained positive results from the ECM.  Further work is necessary to incorporate additional genes, and will be pursued in future work 6. CONCLUSIONS We have discussed the framework of a novel new approach to finding atmospheric plume source locations on planetary surfaces from orbit by using existing numerical analysis methods. This approach relies on the ability of genetic algorithms to quickly identify global best-fit solutions to a multivariate problem. We have found that, under idealized conditions, a source location can be identified to within tens of km on the surface within a couple dozen iterations of the ECM system using idealized plume shapes, even with 


ECM system using idealized plume shapes, even with limited spacecraft observations to constrain the system Error is minimized by increasing the number of observations, and by reducing the instrument uncertainty 12 Additionally, development of instruments with narrower footprints may contribute to a decrease in the source location error. Error due to uncertainty in the knowledge of the observed plume shape is reduced when observations are acquired in a long duration campaign. Plans for future development of this approach have been outlined, which involve the incorporation of general circulation models to supplant the idealized plumes used in the present study. The benefits of the genetic algorithms used here are maximized when multiple genes are employed, rather than just latitude and longitude. Examples of these genes would be parameters such as plume age, species lifetime and time evolution of source emission flux This approach can be of great value to planetary exploration programs, which seek high-precision identification of regions of interest on planetary surfaces to target future landed missions. By applying the methods presented here to realistic atmospheres, we can augment the direct observations made by orbiting spacecraft with a robust indirect approach, saving both time and money 7.  ACKNOWLEDGEMENTS The research described in this publication was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration REFERENCES 1] V.A. Krasnopolsky, J.P. Maillard and T.C. Owen  Detection of Methane in the Martian Atmosphere Evidence for Life  Icarus 172, 537-547, 2004 2] M.J. Mumma, G.L. Villanueva, R.E. Novak, T Hewagama, B.P. Bonev, M.A. DiSanti and M.D. Smith  Absolute Measurements of Methane on Mars: The Current Status  Proceedings of the American Astronomical Society Division of Planetary Sciences Bull. Amer. Astron. Soc. 38, 471, 2007 3] M. Mumma, G. Villanueva, R.E. Novak, T. Hewagama B.P. Bonev, M.A. DiSanti and M.D. Smith  Absolute Measurements of Methane on Mars: The Current Status   Mars Atmosphere: Modeling and Observations Workshop, #9099, 2008 4] V. Formisano, S. Atreya, Th. Encrenaz, N. Ignatiev and M. Giuranna  Detection of Methane in the Atmosphere of Mars  Science 306, 1758-1761, 2004 5] D.W. Beaty, M.A. Meyer and the Mars Advance Planning Group  2006 Update to Robotic Mars Exploration Strategy: 2007-2016  24 pp., 2006 6] D.J. McCleese and the Mars Advance Planning Group  Robotic Mars Exploration Strategy: 2007-2016  33pp 2006 7] MEPAG  Mars Scientific Goals, Objectives Investigations and Priorities: 2006  J. Grant, ed., 31pp 2006 8] R.J. Terrile, et al  Evolutionary Computation Technologies for Space Systems  IEEE Aerospace Conference Proceedings, Big Sky, MT, March 2005 9] S. Lee, C.H. Lee, S. Kerridge, C.D. Edwards and K.-M Cheung  Orbit Design and Optimization Based on Global Telecommunication Performance Metrics  IEEE Aerospace Conference Proceedings, Big Sky, MT, March 2006 13 BIOGRAPHY Michael A. Mischna is an atmospheric scientist with the Jet 


atmospheric scientist with the Jet Propulsion Laboratory focusing on the atmospheres and climates of terrestrial planets. He was the EDL Atmosphere Team Lead for the Mars Phoenix Mission. His research interests include longterm evolution of the martian climate, the role of greenhouse gases in providing habitable environments and surfaceatmosphere interactions. Dr. Mischna has a B.S. in atmospheric science from Cornell University, an M.S. in meteorology from the Pennsylvania State University and an M.S. and Ph.D. in Geophysics and Space Physics from the University of California, Los Angeles  Seungwon Lee is a member of the technical staff at the Jet Propulsion Laboratory. Her research interests include genetic algorithms, lowthrust trajectory design nanoelectronics, quantum computation, parallel cluster computation and advanced scientific software modernization techniques. Dr. Lee received her Ph.D. in Physics from the Ohio State University in 2002 Her work is documented in numerous journals and conference proceedings  Mark Allen is supervisor of the Earth and Planetary Atmospheres Group and Principal Scientist at the Jet Propulsion Laboratory. His research interests include remote detection of trace atmospheric species, and the role of atmospheric chemistry on the atmospheres of Mars, Venus and Titan. He was the PI of the MARVEL Mars Scout proposal, and is a co-investigator on the MIRO microwave instrument on the Rosetta Orbiter. Dr Allen has a B.A. from Columbia University and received a Ph.D. in Chemistry from the California Institute of Technology in 1976     Richard J. Terrile created and leads the Evolutionary Computation Group at NASA  s Jet Propulsion Laboratory. His group has developed genetic algorithmbased tools to improve on human design of space systems and has demonstrated that computer aided design tools can also be used for automated innovation and design of complex systems. He is an astronomer, the Mars Sample Return Study Scientist, the JIMO Deputy Project Scientist and the co-discoverer of the Beta Pictoris circumstellar disk. Dr. Terrile has B.S. degrees in Physics and Astronomy from the State University of New York at Stony Brook and an M.S. and a Ph.D. in Planetary Science from the California Institute of Technology in 1978 14  pre></body></html 


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





