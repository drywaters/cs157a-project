ACN An Associative Classi\002er with Negative Rules Gourab Kundu Md Monirul Islam Sirajum Munir Department of Computer Science and Engineering Bangladesh University of Engineering and Technology Dhaka-1000 f gourabkundu mdmonirulislam g cse.buet.ac.bd sirajum.munir@gmail.com Abstract Classi\002cation using association rules has added a new dimension to the ongoing research for accurate classi\002ers Experiments have shown that these classi\002ers are significantly more accurate than decision tree classi\002ers The 
idea behind most of the existing approaches has been the mining of positive class association rules from the training set and then selecting a subset of the mined rules for future predictions However in most cases it is found that the 002nal classi\002er contains some weak and inaccurate rules that were selected for covering some training instances for which no better rules were available These rules make poor predictions of unseen test instances and only for these rules the overall classi\002cation accuracy is drastically reduced The idea of this paper is to eliminate these weak 
and inaccurate positive rules as far as possible by accurate negative rules The generation of negative associations from datasets has been attacked from different perspectives by various authors and this has proved to be a very computationally expensive task This paper approaches the problem of generating negative rules from a classi\002cation perspective how to generate a suf\002cient number of high quality negative rules ef\002ciently so that classi\002cation accuracy is enhanced We extend the Apriori algorithm for this and show that our classi\002er 223Associative Classi\002er 
with negative rules\224\(ACN is not only time-ef\002cient but also achieves signi\002cantly better accuracy than four other state-of-the-art classi\002cation methods by experimenting on benchmark UCI datasets Keywords associative classi\002cation data mining negative rule 1 Introduction Classi\002cation is a very important problem that has been studied for years now The goal of the classi\002cation algorithms is to construct a model from a set of training data whose target class labels are known and then this 
model is used to classify unseen instances Many different types of classi\002cation techniques have been proposed in literature that includes decision trees na 250 021ve-Bayesian methods statistical approaches[10 etc Recently a new classi\002cation technique has come into existence In this technique at 002rst classi\002cation rules are mined from training data using an association rule mining algorithm and then a subset of these rules are used to build a classi\002er This technique which uses association rules for classi\002cation is called 223Associative Classi\002cation\(AC A 
number of associative classi\002cation algorithms have been proposed in literature but most of them use only positive association rules and differ mainly in rule mining and classi\002er construction from the set of mined rules One of the 002rst algorithms to use association rules for classi\002cation was CBA\(Classi\002cation based on Association CB A uses the Apriori algorithm[1 in order to discover all frequent ruleitems Then it converts any frequent ruleitem that passes the minimum con\002dence threshold into a rule After that it sorts the rules based on 
a rule ranking criteria and selects a subset of the rules that are needed to cover the dataset These ordered rules are later used for classi\002cation An AC approach that uses multiple rules for making a single prediction is CMAR Classi\002cation based on Multiple Association Rules  Instead of using Apriori this method adapts the FP-tree t o mine the class association rules and makes use of a CR-tree structure to store and retrieve the mined rules But the major distinguishing feature from CBA is that here the classi\002cation is 
performed based on a weighted 037 2 analysis using multiple association rules Some other associative classi\002ers based on positive association rules are discussed in 8 18 A positive association rule is of the form X  Y where 


X  Y both are a set of items and X T Y is 036  A negative association rule is of the form X  Y where in addition to being a set of items X or Y will contain at least one negated item An interesting approach that uses both positive and negative rules for classi\002cation is ARC-P It examines the correlation of each frequent itemset with the class label If the correlation is positive a positive rule is discovered If the correlation is negative two negative rules are discovered The negative rules produced are of the form X   Y or  X  Y which the authors term as 223con\002ned negative association rules\224 Here the entire antecedent or consequent is either a conjunction of negated attributes or a conjunction of non-negated attributes The problem with this approach is that it results in a very small set of rules which may not be adequate to provide classi\002cation for all training and test instances In this paper we introduce a new method for associative classi\002cation named 223Associative Classi\002er with Negative Rules\224\(ACN that extends the Apriori algorithm to mine a relatively large set of negative association rules and then uses both positive and negative rules to build a classi\002er The set of mined negative rules and the way ACN generates them are totally different from ARC-PAN The bene\002t of our approach is that the number of negative rules generated is much larger and so in general a lot of good high con\002dence and high support negative rules are found that can be used in place of some weak positive rules As a result the number of inaccurate positive rules in the 002nal classi\002er is greatly reduced and classi\002cation accuracy is increased The major bottleneck in associative classi\002cation is the computational cost for the discovery of association rules and if a classi\002er uses negative rules together with positive rules this cost can increase further ACN tries to address this issue by mining a relatively large set of negative rules but with as low overhead as possible So it adopts a variant of Apriori algorithm and generates a set of negative rules from the available frequent positive itemsets These negative rules come almost free-of-cost since their support and con\002dence can be readily calculated from available positive rules and no extra database scan is required The rest of the paper is organized as follows.In Section 2 we discuss the related work in negative association mining Section 3 presents ACN in details Section 4 presents our experimental results and 002nally in section 5 we conclude the paper with some remarks 2 Negative Association Mining Ef\002cient mining of negative association rules has recently received much attention from a large group of researchers Negative associations can inform us of facts like 223If a customer purchases Coke he is not likely to purchase Juice.\224 etc Such associations are quite natural and occur frequently in practice However mining transactional databases for negative associations is really challenging because typically a super-store contains thousands of items and each transaction contains only a few of them As a result each transaction has a large number of negated or absent items and the number of possible negative associations under the support-con\002dence framework turns out to be overwhelmingly large Moreover a large portion of the discovered rules may be uninteresting from the user's perspective These dif\002culties have forced researchers to incorporate additional rule interestingness measures[14 o v er the support-con\002dence frame w ork or incorporate domain knowledge[13 to reduce the search space However it is evident that important differences exist between the mining of negative associations in transactional databases and the mining of them in classi\002cation datasets Firstly in classi\002cation datasets typically the number of attributes is not large and each attribute has a small number of possible values So the value for an attribute in a record indicates the absence of other possible values for that attribute This is much smaller compared to the number of absent items in a transaction in a super-store database Secondly the purpose of mining is on increasing classi\002cation accuracy rather than presenting the set of mined rules to the user and satisfying his interest Thirdly the process of negative rule generation must be as cheap as possible to avoid further increasing the complexity of the mining phase of an associative classi\002er Keeping the above challenges in mind we designed an ef\002cient algorithm for generation of both positive and negative rules Using both sets of rules we were able to obtain classi\002cation accuracy higher than other methods A number of negative rule mining algorithms have already been proposed in the literature[19 16 3 17 13 15 However to the best of our knowledge the algorithm for negative rule mining in ACN best meets the challenges described above This approach is novel and has not been introduced in literature before 3 ACN This section describes ACN in more details First we discuss how ACN generates rules then we present the classi\002er builder for ACN and 002nally we discuss different pruning strategies to reduce the number of generated rules 


3.1 ACN Rule Generator Let a relational schema contains n data attributes A 1 A 2     A n  and one class attribute Z Each attribute has a set of possible values Each record is of the form a 1 a 2    a n  z where a 1 a 2    a n are the values of the corresponding data attributes and z is the class label Given a set of training instances the task of a classi\002er is to learn the relation between set of attribute values to class label and later use this relation to predict class labels for test instances from the values of their data attributes only For mining class association rules using Apriori each itemset is considered to be of the form conditionset z which represents a rule conditionset  z where conditionset is a set of attribute value pairs and z is a class label The terms rule and itemset will be used interchangeably afterwards and should be understood from context The rule has con\002dence equal to ruleSupportCount  conditionSupportCount  100 where conditionSupportCount is the number of cases in the dataset D that contain conditionset and ruleSupportCount is the number of cases in the dataset D that contain conditionSet and are labeled with class z The rule has support equal to ruleSupportCount j D j  100 where j D j is the size of the dataset The idea behind the Apriori algorithm is the property that all subsets of any frequent itemset must also be frequent This allows Apriori to generate the frequent itemsets in a level wise manner In the following discussion we assume that the notation C k represents the candidate itemsets of length k and L k represents the frequent itemsets of length k generated by Apriori We also assume that the notation l i  refers to the j th item in l i where l i represents an itemset Obviously for all k L k is a subset of C k and each member of L k has support higher than the user speci\002ed support threshold In Apriori there are two steps the join step and the prune step  In the join step a set of candidate itemsets C k is generated by joining L k 000 1 with itself If we assume that the items in the itemset are sorted in lexicographic order then members l 1  z and l 2  z of L k 000 1 are joinable if l 1 1]=l 2 1  l 1 2]=l 2 2  l 1 3]=l 2 3     l 1 k-2]=l 2 k-2  l 1 k-1  l 2 k-1 The resulting rule generated by joining l 1 and l 2 is l=l 1 1  l 1 2      l 1 k-1  l 2 k-1  z Now comes the prune step of Apriori It checks to see whether the all subsets of the generated candidate itemsets of length k-1 are frequent If not that itemset cannot be frequent and can immediately be discarded Otherwise a full database scan must be made to count the support for the new itemset to decide whether it is actually frequent or not We call each candidate itemset of length k for which all subsets of itemsets of length k-1 for all k 025 2 are frequent a 223legal candidate\224 For each item of this legal candidate ACN replaces this item with the corresponding negated item creates a new negative rule and adds it to the negative ruleSet The generation of positive rules continues without disruption and the abundant but valuable negative rules are produced as by-products of the Apriori process For the trivial cases the set of frequent 1 positive rule items are generated without any join step Frequent 1 negative rules are generated by simply considering the negation of the single antecedent of each positive rule item Example We will explain the rule generation of ACN using an example Let us consider a case with 5 data attributes A,B,C,D,E and one class attribute Z The domains for the four data attributes are respectively f a1,a2,a3 g  f b1,b2,b3,b4 g  f c1,c2,c3 g  f d1,d2,d3 g  f e1,e2,e3,e4 g  Z can take on the possible values of y and n Suppose in the Apriori rule mining process the set of frequent ruleitems of length 3 are found to be a1  b1  c1  y a1  b1  d1  y b1  c1  d1  y a2  b2  c2  n a2  b2  d4  n b1  c1  e3  n Now the set of candidate ruleitems of length 4 after join step is a1  b1  c1  d1  y a2  b2  c2  d4  n Out of these two candidates only the 002rst one a1  b1  c1  d1  y possibly can be frequent since all its subsets are frequent as found from the set of frequent ruleitems of length 3 So according to our de\002nition it will be a legal candidate and from this ruleitem four rules of the form  a1  b1  c1  d1  y a1   b1  c1  d1  y a1  b1   c1  d1  y and a1  b1  c1   d1  y will be generated In this way negative rules will be generated in all phases of the Apriori algorithm These negative rules will not take part in generation of any new rule but they will compete for a place in the 002nal classi\002er with the positive rules Please note that each legal candidate ruleitem with n number of items in the antecedent will generate n new negative rules even if the candidate ruleitem turns out to be infrequent L 1 frequent-1-Positive-itemsets\(D N 1 frequent-1-Negative-itemsets\(D for\(k=2;L k 000 1 empty;k PC k  legal candidates generated for level k for each legal candidate generated for each item on the candidate create a new negative rule by negating that item 


add this rule to NC k  end end calculate support for each rule of PC k by scanning the database calculate support for each rule of NC k from supports of members of PC k and L k 000 1  L k candidates in PC k that pass support threshold N k candidates in NC k that pass support threshold end Return L=L 1  L 2      L k 000 2  N 1  N 2      N k 000 2 Figure 1 ACN Rule Generator 3.2 Classi\fer Builder ACN sorts the set of positive and negative rules on the following rule ranking criteria a rule Ri will have higher rank than rule Rj if and only if 1 conf\(Ri  conf\(Rj or Ri  conf\(Rj but correlation\(Ri  correlation\(Rj or 3 conf\(Ri  conf\(Rj and correlation\(Ri  correlation\(Rj but sup\(Ri  sup\(Rj or 4 conf\(Ri  conf\(Rj and correlation\(Ri  correlation\(Rj and sup\(Ri  sup\(Rj but size of conditionset of Ri  size of conditionset of Rj or 5 conf\(Ri  conf\(Rj and correlation\(Ri  correlation\(Rj and sup\(Ri  sup\(Rj and size of conditionset of Ri  size of conditionset of Rj but Ri is a positive rule and Rj is a negative rule After sorting ACN builds a classi\002er based on database coverage similar to CBA except for the fact that before a negative rule is taken ACN calculates the accuracy of the rule on the remaining uncovered dataset and if it is greater than a speci\002c threshold only then the rule is taken in the 002nal classi\002er There is no such restriction for positive rules Sort rules based on rule ranking criteria for each rule taken in order If the rule classi\002es at least one remaining training example correctly If the rule is a negative rule and accuracy on remaining data  threshold Include that rule in classi\002er and delete those examples end if the rule is a positive rule Include the rule in classi\002er and delete those examples end end end If database in uncovered select majority class from remaining examples else select majority class from entire training set  end Figure 2 ACN Classi\002er Builder 3.3 Pruning Strategies ACN adopts several pruning strategies to cut down the number of generated rules In the 002rst strategy only negative rules are pruned Consider two rules l and m from which a negative rule n is produced Let l   l[2      l[k  z and m   l[2      l[i-1  l[i+1      l[k  z and n   l[2       l[i      l[k  z If con\002dence of l  con\002dence of m  it can be proved that con\002dence of n  con\002dence of m So according to our rule ranking criteria m will precede n and n can be pruned because the coverage of m is a superset of coverage of n Secondly ACN prunes all rules that have con\002dence less than the minimum con\002dence Thirdly ACN prunes both positive and negative rules based on a correlation coef\002cient threshold 3.4 Time-E\016ciency of ACN In this section we theoretically prove that ACN does not perform any extra dataset scan to count the support or con\002dence of the generated negative rules So there is no big I/O overhead for generating the negative rules Theorem ACN performs no extra dataset scan than normal Apriori Process  Proof The proof is by contradiction Suppose at some pass of Apriori ACN generates a negative rule l 1  l 2      l i 000 1   l i      l p  z for which the support and con\002dences cannot be calculated without a database scan This rule can only be generated from a candidate ruleitem l 1  l 2      l i 000 1  l i      l p  z According to the rule generation method of ACN this ruleitem must be a legal ruleitem i.e all subsets of this ruleitem of length p-1 must be frequent As a result the ruleitem of the form l 1  l 2      l i 000 1  l i 1     l p  z must be frequent and its support and con\002dences must have been calculated in the previous pass of the Apriori algorithm For the legal candidate ruleitem l 1  l 2      l i 000 1  l i      l p  


z support and con\002dence will be calculated via database scan at current pass Now for the rule l 1  l 2     l i 000 1   l i      l p  z we can obtain supp l 1  l 2     l i 000 1   l i      l p  z supp\(l 1  l 2      l i 000 1  l i 1     l p  z supp\(l 1  l 2      l i 000 1  l i      l p  z supp\(l 1  l 2      l i 000 1   l i     l p  supp l 1  l 2      l i 000 1  l i 1      l p  000 supp  l 1  l 2      l i 000 1  l i  l i 1      l p  conf l 1  l 2     l i 000 1   l i      l p  z supp l 1  l 2     l i 000 1   l i      l p  z  supp\(l 1  l 2      l i 000 1   l i     l p  So no such negative rule will be generated by ACN for which a database scan will be needed to count the support and con\002dence So ACN performs exactly the same number of dataset scans as Apriori Figure 3 Support and Con\002dence calculation of negative rule items from other positive rule items So ACN does not need to perform any extra database scan to calculate support and con\002dences of the generated negative rules For any rule to gather the support and con\002dence values we need to consult O\(r records where r is the number of records in the database But here the support and con\002dence of a negative rule can be calculated in O\(1 time and no I/O operation is needed As a result the mining phase of ACN remains time-ef\002cient 4 Experimental studies In this section we present some experimental facts regarding ACN and also compare it with other state-of-the-art classi\002cation algorithms in terms of accuracy Table 1 Number of positive and negative rules generated for two data sets Con\002dence Diabetes Diabetes Heart Heart  rules rules  rules rules 50-60 223 571 3438 10496 60-70 200 546 1474 4823 70-80 163 520 1770 5201 80-90 133 448 952 2917 90-100 142 474 999 2900 Table 1 gives the number of positive and negative rules generated in experiments on two data sets Diabetes and Heart From the table it is evident that the number of generated negative rules of high con\002dence\(90-100 several orders of magnitude in number than the number of generated positive rules of that con\002dence In general a rule is good if it has high con\002dence So taking the negative rules into account ACN generates more good rules than other classi\002cation methods that generate only positive rules So the class association rule set for ACN is much larger and richer Table 2 gives the comparison of accuracy among ACN C4.5 CBA and CMAR The accuracy of ACN was obtained by 10 fold cross validation over 16 datasets from UCI ML  e ha v e used C4.5 s shuf 003e utility to shuf 003e the datasets Discretization of continuous attributes is done using the same method in CBA For ACN the minimum con\002dence was set to 50 the correlation coef\002cient threshold was set to 0.2 and the remaining accuracy threshold for negative rules was set to 55 These were the best values for the parameters that we obtained experimentally From Table2 we see that the average accuracy of ACN is better than CBA CMAR and C4.5 Moreover out of the 16 datasets ACN achieves the best accuracy on more than half 9 datasets Table 3 gives the comparison of accuracy among ACN and ARC-PAN We have used only 6 datasets for comparison here since the accuracy of ARC-PAN was available for these 6 datasets only 


Table 2 Comparison of C4.5 CBA CMAR and ACN on accuracy Dataset ACN CMAR CBA C4.5 diabetes 76.3 75.8 74.5 74.2 pima 75.1 75.1 72.9 75.5 tic-tac 99.7 99.2 99.6 99.4 iris 95.3 94 94.7 95.3 heart 82.2 82.2 81.9 80.8 lymph 83.1 83.1 77.8 73.5 glass 73.8 70.1 73.9 68.7 austra 85.5 86.1 84.9 84.7 led7 71.9 72.5 71.9 73.5 horse 83.7 82.6 82.1 82.6 sonar 79.8 79.4 77.5 70.2 hepati 83.2 80.5 81.8 80.6 crx 85.2 84.9 84.7 84.9 cleve 81.5 82.2 82.8 78.2 hypo 98.9 98.4 98.9 99.2 sick 97.3 97.5 97 98.5 Average 85.4 84.6 84.3 83.0 Table 3 Comparison of ACN and ARC-PAN on accuracy Dataset ACN ARC-PAN diabetes 76.3 74.9 pima 75.1 73.1 iris 95.3 94.0 heart 82.2 83.8 led7 71.9 71.1 breast 95.3 96.2 Average 82.68 82.18 For ARC-PAN we consider the results obtained when all rules both positive and negative are used for classi\002cation From table 3 we see that the win-loss-tie record of ACN against ARC-PAN is 4-2-0 5 Conclusions In this paper we proposed a novel classi\002cation algorithm ACN that mines both positive and negative class association rules and uses both sets for classi\002cation We showed that the number of generated negative rules is large and so using them in place of some weak positive rules can enhance classi\002cation accuracy Our experiments on UCI datasets show that ACN is consistent highly effective at classi\002cation of various kinds of databases and has better average classi\002cation accuracy compared to C4.5,CBA,CMAR and ARC-PAN References  R Agra w al and R Srikant F ast algorithms for mining association rules In VLDB  Chile September 1994  M Antonie and O R Za 250 021ne An Associative Classi\002er based on Positive and Negative Rules In DMKD  Paris France June 2004  M Antonie and O R Za 250 021ne Mining Positive and Negative Association Rules an Approach for Con\002ned Rules In Principles and Practice of Knowledge Discovery in Databases  2004  E Baralis and P  Garza A Lazy Approach to Pruning Classi\002cation Rules In ICDM  2002  C Blak e and C Merz UCI repository of m achine learning databases  R Duda and P  Hart Pattern Classi\002cation and Scene Analysis  John Wiley and Sons 1973  J Han J Pei and Y  Y i n Mining frequent patterns without candidate generation In SIGMOD  2000  G K undu S Munir  M F  Bari M M Islam and K Murase A Novel Algorithm for Associative Classi\002cation In International Conference on Neural Information Processing  Japan 2007  W  Li J Han and J Pei Accurate and ef 002cient classi\002cation based on multiple class association rules In ICDM  San Jose CA November 2001  T  Lim W  Loh and Y  Shi h A comparison of prediction accuracy complexity and training time of thirty-three old and new classi\002cation algorithms Machine Learning  39 2000  B Liu W  Hsu and Y  Ma Inte grating Classi\002cation and Association Rule Mining In KDD  New York August 1998  J Quinlan C4.5:Programs for Machine Learning  Morgan Kaufmann 1993  A Sa v asere E Omiecinski and S Na v athe Mining for Strong Negative Associations in a Large Database of Customer Transactions In ICDE  1998  S.Bri n R Motw ani and C.Silv erstein Be yond mark et baskets Generalizing association rules to correlations In ACM SIGMOD  1997  D Thiruv ady and G W ebb  Mining Ne g ati v e Association Rules using GRD In PAKDD  2004  X W u C Zhang and S Zhang Ef 002cient Mining of Both Positive and Negative Association Rules ACM Trans on Information Systems  22\(3 2004  P  Y an G Chen C Cornelis M D Cock and E K erre Mining Positive and Negative Fuzzy Association Rules In LNCS 3213  2004  X Y in and J Han CP AR Classi\002cation based on Predicti v e Association Rules In SDM  2003  X Y uan B P  Buckles Z Y uan and J Zhang Mining Ne gative Association Rules In Seventh International Symposium on Computers and Communications  Italy June 2002 


Table 10 Overview of different error classes in DC1 Algorithm All Features Multi-word features Single-word features False negatives False positives False negatives False positives None of the terms extracted Some of the terms extracted All extracted but not combined On-topic features Off-topic features Not extracted Extracted falsely as multi-word On-topic features Off-topic features Association Mining 594 14 62 25 23 0 45 39 260 259 Likelihood Test 594 138 1 0 1 0 295 3 29 0 002ed the algorithm in order to consider only the subsequence of the extracted feature which consists of nouns We refer to these two modi\002cations as Subsequence Similarity SsS An evaluation of these modi\002cations is shown in columns 5 and 6 of Table 7 We observe an average increase of recall by 2 and an increase of precision by 10 4.3.2 Analysis of the Association Mining Approach The precision of the Association Mining approach is fairly low since it returns any noun as a feature if it often occurs in the documents For example the term week is extracted as a frequent feature There is no distinction between dataset speci\002c terms and common vocabulary terms Setting the minimum-support threshold higher will not solve this problem as it would lead to decreased recall Note that in our evaluation Table 8 the infrequent features hardly affect the algorithm's results since they are only very seldomly extracted at all For example in the DC1 dataset of the 597 sentences only 12 contain an opinion word but no frequent feature Of those 12 cases the infrequent feature identi\002cation leads to 7 correct and 5 false features being extracted In some cases see Column 9 of Table 10 the association mining falsely attributes nouns occurring in a sentence to a single feature set For example in 4 recent price drops have made the g3 the best bargain in digital cameras currently available g3 is extracted as a feature set since the two terms occur together as one entity in multiple other sentences The compactness pruning will therefore not remove this feature set Sentences as 4 will hence result in an error during extraction The large amount of false positives in the single-word feature extraction see Table 10 Columns 6  7 is due to the fact that many sentences in the DC1 dataset consist of comparisons of the DC1 camera to other camera models The features of these other camera models are also mentioned in the reviews and therefore falsely extracted by the association mining since the algorithm is not capable of distinguishing between references to features of the DC1 camera and any other camera model 4.3.3 Comparison of the Approaches As outlined in Table 10 the two approaches have their strengths and weaknesses in different tasks If the Likelihood Ratio Test approach fails to extract a multi-word feature the tendency is that none of the feature terms are being extracted while this is not the case in the association mining approch This is due to the fact that the association mining algorithm will return any feature combination occurring in a given sentence while the Likelihood Ratio Test approach requires that a multi-word feature occurs in the same ordering in several sentences in order to achieve a high likelihood ratio and therefore be extracted The threshold of the Likelihood Ratio Test approach in combination with the Subsequence similarity calculation will therefore prevent that a subset of a multi-word feature is extracted instead the feature will not be extracted at all At the same time the association mining extracts several false multi-word features none of them belonging to the general vocabulary We observe similar results in the analysis of the single-word errors The Likelihood Ratio Test approach fails to extract many of the features which is again due to the threshold while the Association Mining approach extracts less false features but has the problem of wrongly extracting actual single-word features as a multi-word expression as analyzed in Section 4.3.2 The inability of the Association Mining approach to recognize whether a certain candidate feature is an attribute of the current topic as de\002ned in Section 2.1 is observable in Columns 10  11 of Table 10 The Association Mining approach extracts a large number of false features compared to the Likelihood Ratio Test approach The low number of falsely extracted on-topic features of the Likelihood Ratio Test approach could be attributed to the dBNP method Apparently if a candidate BNP is preceded by a de\002nite article an on-topic feature follows However the low number of false positives during the feature extraction re\003ects the tradeoff between recall and precision of this approach 5 Conclusions In this paper we provide a comprehensive analysis of two state-of-the-art algorithms for extracting features from product reviews based on the Likelihood Ratio Test and on 
159 
150 


association mining The Likelihood Ratio Test fails to extract features also belonging to common vocabulary and it makes the extraction dependent on the feature position in the sentence leading to low recall The dBNP and bBNP based methods yield low recall due to the fact that the product features do not occur with the article the in front of them very often The Association Mining approach returns all frequent nouns which decreases precision Our results suggest that the choice of algorithm to use depends on the targeted dataset If it consists of mainly on-topic content the results of Table 10 indicate that the Association Mining algorithm is better suited for this task due to its high recall If the dataset consists of a mixture of onand off-topic content our results suggest that the Likelihood Ratio Test based algorithm would perform better due to its ability to distinguish and 002lter out the off-topic features For future work we plan to extend the Likelihood Ratio Test methods especially the dBNP based approach by other determiners such as a or this  which should increase the recall of this method Another possibility which we will investigate regards the BNP patterns The current Likelihood Ratio Test approach is not capable of dealing with discontinuous feature phrases for example in 5 the quality of the pictures is great the feature would be picture quality  This problem could be addressed by introducing wildcards in the BNP patterns We will also investigate whether there are any methods in order to calculate an optimal threshold for the candidate feature extraction in order to increase the recall of the Likelihood Ratio Test based algorithm We plan to investigate whether a deeper linguistic analysis e.g with a dependency parser can improve the feature extraction Acknowledgements The project was funded by means of the German Federal Ministry of Economy and Technology under the promotional reference 01MQ07012 The authors take the responsibility for the contents The information in this document is proprietary to the following Theseus Texo consortium members Technische Universit  at Darmstadt The information in this document is provided as is and no guarantee or warranty is given that the information is 002t for any particular purpose The above referenced consortium members shall have no liability for damages of any kind including without limitation direct special indirect or consequential damages that may result from the use of these materials subject to any liability which is mandatory due to applicable law Copyright 2008 by Technische Universit  at Darmstadt References  R Agra w al and R Srikant F ast algorithms for mining association rules Proc 20th Int Conf Very Large Data Bases VLDB  1215:487–499 1994  K Bloom N Gar g and S Ar g amon Extracting a ppraisal expressions In HLT-NAACL 2007  pages 308–315 2007  R Bruce and J W iebe Recognizing subjecti vity a case study in manual tagging Natural Language Engineering  5\(02 1999  K Da v e S La wrence and D Pennock Mi ning the peanut gallery opinion extraction and semantic classi\002cation of product reviews In Proceedings of the 12th International Conference on World Wide Web  pages 519–528 New York NY USA 2003 ACM  T  Dunning Accurate methods for the statistics of surprise and coincidence Computational Linguistics  19\(1 1993  O Feiguina and G Lapalme Query-based summ arization of customer reviews In Canadian Conference on AI  pages 452–463 2007  C Fellbaum Wordnet An Electronic Lexical Database  MIT Press 1998  A Ferraresi Building a v ery lar ge corpus of english obtained by web crawling ukwac Master's thesis University of Bologna Italy 2007  M Gamon A Aue S Corston-Oli v er  and E Ringger  Pulse Mining customer opinions from free text In Proceedings of the 6th International Symposium on Intelligent Data Analysis IDA-2006  Springer-Verlag 2005  N Glance M Hurst K Nig am M Sie gler  R Stockton and T Tomokiyo Deriving marketing intelligence from online discussion In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining  pages 419–428 New York USA 2005 ACM  M Hu and B Liu Mining opinion features in customer reviews In Proceedings of 9th National Conference on Arti\002cial Intelligence  2004  N K obayashi K Inui K T atei shi and T  Fukushima Collecting evaluative expressions for opinion extraction In Proceedings of IJCNLP 2004  pages 596–605 2004  S Morinag a K Y amanishi K T ateishi and T  Fukushima Mining product reputations on the Web In Proceedings of KDD-02 8th ACM International Conference on Knowledge Discovery and Data Mining  pages 341–349 Edmonton CA 2002 ACM Press  A.-M Popescu and O Etzioni Extracting product features and opinions from reviews In Proceedings of HLT-EMNLP-05 the Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing  pages 339–346 Vancouver CA 2005  H Schmid T reetagger a language independent part-ofspeech tagger Institut fur Maschinelle Sprachverarbeitung Universitat Stuttgart  1995  J W iebe R Bruce and T  O'Hara De v elopment and use of a gold-standard data set for subjectivity classi\002cations In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics  pages 246–253 Association for Computational Linguistics Morristown NJ USA 1999  J Y i T  Nasuka w a R Bunescu and W  Niblack Sentiment analyzer Extracting sentiments about a given topic using natural language processing techniques In Proceeding of ICDM-03 the 3ird IEEE International Conference on Data Mining  pages 427–434 Melbourne US 2003 IEEE Computer Society 
160 
151 


Figure 4 Expected and real number of extracted patterns using two promoter sequence datasets Horizontal axis minimum support vertical axis number of patterns 
85 
85 


a frequency constraint and according to the structure of the dataset These proposals are all based on a global analytical model i.e an interesting approach that needs however to develop complex and speci\036c models As a result they cannot be easily extended to handle complex conjunctions of constraints to incorporate different symbol distributions or different semantics for pattern occurrences To the best of our knowledge no method has been proposed to estimate the number of patterns satisfying a constraint while avoiding to develop a global analytical model Our approach requires only to know how to compute for a given pattern its probability to satisfy the constraint this can be obtained in many situations and it remains ef\036cient in practice by adopting a pattern space sampling scheme 6 Conclusion Using constraints to specify subjective interestingness issues and to support actionable pattern discovery has become popular Constraint-based mining techniques are now well studied for many pattern domains but one of the bottlenecks for using them within Knowledge Discovery processes is the extraction parameter tuning This is especially true in the context of differential mining where domain knowledge is used to provide different datasets to support the search of truly interesting patterns From a user perspective a simple approach would be to get graphics that depict the extraction landscape i.e the number of extracted patterns for many points in the parameter space We developed an ef\036cient technique based on pattern space sampling that provides an estimate on the number of extracted patterns This has been applied to non trivial substring pattern mining tasks and we demonstrated by means of many experiments that the technique is effective It provides reasonable estimates given execution times that enable to probe a large number of points in the parameter space Notice that domain knowledge is also exploited here when selecting the distribution model Future directions of work include to adapt the approach to other pattern domains and to different constraints Another interesting aspect to investigate is the use of more sophisticated sampling schemes e.g that could b e incorporated in the approach when more complex syntactical constraints are handled e.g a grammar to specify the shape of the patterns Acknowledgments This work is partly funded by EU contract IQ FP6-516169 Inductive Queries for Mining Patterns and Models and by the French contract ANR-MDCO14 Bingo2 Knowledge Discovery For and By Inductive Queries We thank Dr Olivier Gandrillon from the Center for Molecular and Cellular Genetics CNRS UMR 5534 who provided the DNA promoter sequences References  J F  Boulicaut L De Raedt and H  M annila e ditors Constraint-Based Mining and Inductive Databases  volume 3848 of LNCS  Springer 2005  C  B resson C K e ime C F a ure Y  Letrillard M  B arbado S San\036lippo N Benhra O Gandrillon and S GoninGiraud Large-scale analysis by SAGE revealed new mechanisms of v-erba oncogene action BMC Genomics  8\(390 2007  L  C ao and C  Z hang Domain-dri v e n actionable kno wledge discovery in the real world In Proceedings PAKDDÕ06 volume 3918 of LNCS  pages 821–830 Springer 2006  G  D ong and J  L i Ef 036cient mining of emer ging patterns discovering trends and differences In Proceedings ACM SIGKDDÕ99  pages 43–52 1999  F  Geerts B  G oethals and J  V  d en Bussche T ight upper bounds on the number of candidate patterns ACM Trans on Database Systems  30\(2 2005  U  K eich and P  A  P e vzner  S ubtle motifs de\036ning the limits of motif 036nding algorithms Bioinformatics  18\(10 2002  S  K ramer  L De Raedt and C  Helma M olecular f eature mining in HIV data In Proceedings KDDÕ01  pages 136 143 2001  L  L hote F  Rioult and A  S oulet A v e rage number of frequent closed patterns in bernouilli and markovian databases In Proceedings IEEE ICDMÕ05  pages 713–716 2005  I  M itasiunaite a nd J.-F  B oulicaut Looking for monotonicity properties of a similarity constraint on sequences In Proceedings of ACM SACÕ06 Data Mining  pages 546–552 2006  I Mitasiunaite and J F  Boulicaut Introducing s oftness i nto inductive queries on string databases In Databases and Information Systems IV  pages 117–132 IOS Press 2007  I Mitasiunaite C Rigotti S Schicklin L  M e yniel J F  Boulicaut and O Gandrillon Extracting signature motifs from promoter sets of differentially expressed genes Technical report LIRIS CNRS UMR 5205 INSA Lyon France 2008 23 pages Submitted  G Ramesh W  M aniatty  a nd M J Zaki F easible itemset distributions in data mining theory and application In Proceedings ACM PODSÕ03  pages 284–295 2003  F  Zelezn  y Ef\036cient sampling in relational feature spaces In Proceedings ILPÕ05  volume 3625 of LNCS  pages 397 413 Springer 2005 
86 
86 


