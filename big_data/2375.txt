A Weighted Utility Framework for Mining Association Rules   M. Sulaiman Khan 1, 2 Maybin Muyeba 1 Frans Coenen 2  1 School of Computing, Liverpool Hope University, UK 2 Department of Computer Science University of Liverpool, UK khanm@hope.ac.uk  muyebam@hope.ac.uk  frans@liv.ac.uk    Abstract  Association rule mining \(ARM\entifies frequent itemsets from databases and generates association rules by assuming that all items have the same significance and frequency of occurrence in a record i.e. their weight and utility is the same \(weight=1 and utility=1\ which is not always the case. However, items are actually different in many aspects in a number of real applications such as retail marketing, nutritional pattern mining etc. These differences between items may have a strong impact on decision making in many application unlike the use of standard ARM. Our framework, Weighted Utility ARM \(WUARM considers the varied significance and different frequency values of individual items as their weights and utilitie s. Thus, weighted u tility mining focuses on identifying the itemsets with weighted utilities higher than the user specified weight ed utility threshold. We conduct experiments on synthetic and real data sets using standard ARM, weighted ARM and Weighted Utility ARM \(WUARM and present analysis of the results  1. Introduction  Data mining and knowledge discovery in databases is an interesting research area only developed in the last fifteen years. Association rule mining [1  popular data mining technique because of its wide application in marketing and retail communities as well as other more diverse fields. Researchers from the data mining community are more concerned with qualitative aspects of attributes \(e.g. significance utility\ as compared to considering only quantitative ones \(e.g. number of appearances in a database etc because qualitative properties are required in order to fully exploit the attributes present in the dataset Classical association rules mining techniques treat all items in the database equally by considering only the presence within a transaction without taking into account their significance to the user or business and also their utility as frequency of occurrences in each record. Although standard ARM algorithms are capable of identifying distinct patterns from a dataset they sometimes fail to associate user objectives and business values with the outcomes of the ARM analysis. For example, in a retail mining application frequent itemsets identified by the standard association rule mining algorithm may contribute only a small portion of the  overall company profit because high profit and luxury items normally do not frequently appear in transactions and thus do not appear in rules with high support count  values Given weighted items in table 1, we see from table 2 that the rule [jeans 000 suit, 50 be more  important than [shirt 000 suit, 75 ev en th o u g h th e  former holds a lower support. This is because those items in the first rule usually come with more profit per unit sale and jeans appear twice in transaction 2, which doubles the profit for jeans in that transaction. In contrast, standard ARM simply ignores this difference Table 1 Weighted items table ID Item Profit Weight  1Shir t 10 0.1 2 Jean 25 0.3 3 Jacket 50 0.6 4Sui t 80 0.9 Table 2 Customers transactions Tid Shirt Jean Jacket Suit 1 1 1 0 1 2 0 2 1 0 3 1 1 2 1 4 1 0 1 1  Many techniques and algorithms have been proposed for mining association rules that consider the qualitative properties of attributes in the databases However, proposed techniques mostly compromise either on quality of rules or efficiency of algorithms 
Second UKSIM European Symposium on Computer Modeling and Simulation 978-0-7695-3325-4/08 $25.00 © 2008 IEEE DOI 10.1109/EMS.2008.73 87 


The main challenge in mining weighted and utility association rules is that the anti-monotonic property [2  does not hold. Also the rules generated using these techniques are not guaranteed as high quality rules These issues give rise to a new approach for identifying correct patterns from databases considering their significance and utilities as quality constraints. To our knowledge, there seems to be no work addressing both weighted and utility mining frameworks in a hybrid fashion Weighted Utility association rule mining WUARM\ is the extension of weighted association rule mining in the sense that it considers items weights as their significance in the dataset and also deals with the frequency of occurrences of items in transactions Thus weighted utility association rule mining is concerned with both the frequency and significance of itemsets. Here weighted utility mining is helpful in identifying the most valuable and high selling items which contribute more to the companyís profits Weighted Utility of an item set depends upon two factors Transactional Utility It is the frequency of occurrences or quantity of an item in a transaction Item significance It is the value representing significance of an item \(value, profit etc\d it holds across the dataset Items weights are stored in a weighted table \(see table 1\.  Using transactional utilities and item weights we can extract weighted utility rules The paper is organized as follows: section 2 gives a background and related work, section 3 gives a problem definition, section 4 discusses the downward closure property \(DCP\d weighted utility property section 5 shows experimental evaluation and a conclusion in section 6  2. Background and related work  One major issue in association rule mining with weighted or utility settings is the invalidation of antimonotonic property of itemsets. Previous works [3, 4 5, 6 co n s id ered ite m  w e ig h t s as t h eir u tilit y  to ref l ect  their significance in the dataset. Our approach is different from all these in that we define utility differently by considering the frequency of occurrences of database attributes in a single record. The weight shows the significance of an item in a dataset e.g profit margin of an item or items under promotional offers etc. We define item weight as a weighting function to signify an item differently in different domains \(see next section, definition 2\. This way we can extract those rules that have significant weight and high utility In ob j ect ori e n t ed  m i ni ng approach i s  proposed that takes into account the items utilitiesí as the objective defined by the user to generate top-K high utility association rules, where K is the number of user specified rules. Standard DCP is not valid in the proposed model but instead a condition based weaker DCP approach is used. Also, the significance of items is not taken into account while generating the utility association rules A most recent framework for mining weighted ARs is presented in w h ere a g e n e ralis ed w e i g h t ed A R M  model is given that uses a modified Apriori approach 1 o r b i n a ry an d q u a n tita tiv e attrib u t es. T h e ap p r o ach  also has a valid DCP. But this model only considers an items significance and not their utilities. In real world applications, transactional databases hold item utilities as well but classical and weighted ARM simply ignores these  3. Problem definition  In this section, a formal description of the weighted utility mining problem is given and related concepts are described Definition 1 \(Weighted Utility Mining Let the input data D have transactions       3 2 1 n t t t t T   with a set of items         3 2 1 I i i i i I  and a set of positive real number weights        2 1 I w w w W   associated with each item in I  Each th i transaction i t is some subset of I and a weight w is attached to each item j i Thus each item j i will have associated with it a weight from the set W i.e. a pair    w i is called a weighted item where I i  and W w  Weight for the th j item in the  th i transaction is given by    j i i w t with u as the utility \(frequency of occurrence\f an item in a transaction from a set U and represented with non negative integers. Weighted Utility mining is thus a triple  U W I    Definition 2 Item Weight  IW is a non-negative real value   j i w given to each item j i ranging in   w i t h s o m e de g r ee of i m portan ce, s u ch t h at    j i w    j i W where W is a weighting function, a function relating specific values in a domain to user preferences. The weight reflects the significance of an item that is independent of transactions 
88 


Definition 3 Weight Table is a two dimensional table    W I WT over a collection of items I where W is the set of positive real numbers   j i w given to each item I i   Definition 4 Item Utility of an item j i in a transaction q t is denoted as    u i t j q Item utility reflects the frequency of an item in a transaction and is transaction dependent Definition 5 Item Weighted Utility IWU is the integrated weight w and utility u value of an item j i in a transaction i t denoted by     u i w t j i  Definition 6 Transaction Weighted Utility  TWU is the aggregated weighted utilities of all the items present in a single transaction. Transaction weighed utility can be calculated as              1 i t j j i i t u i w t t twu i     Definition 6 Weighted Utility Support  wus of an itemset X Y is the fraction of transaction weighted utilities that contain both X and Y relative to the transactional weighted utility of all transactions. It can be formulated as      1   1       T i i S i i t twu t twu XY wus  where     S Y X T S S S       By this means, weighted utility support is modeled to measure the actual contribution of an itemset in the dataset in weighted utility association rule mining scenario  4. Downward Closure Property \(DCP  In classical ARM algorithm, it is assumed that if the itemset is large, then all its subsets should be large, a principle called downward closure property \(DCP anti-monotonic property of itemsets. For example, in standard ARM using DCP, it states that if AB and BC are not frequent, then ABC and BCD cannot be frequent, consequently their supersets are of no value as they will contain non-frequent itemsets. This helps the algorithm to generate large itemsets of increasing size by adding items to itemsets that are already large In the weighted utility framework where each item is given a weight with several occurrences, the DCP does not hold in a straightforward manner. Because of the weighted support, an itemset may be large even though some of its subsets are not large and we illustrate this in table 5 In table 5, all frequent itemsets are generated using 30% support threshold. In column two \(i.e. Standard ARM\mset {ACD} and {BDE} are frequent with support 30% and all of their subsets {AC}, {AD CD} and {BD}, {BE}, {DE} respectively are frequent as well. But in column 3 with weighted settings, itemsets {AC} and {BE} are no longer frequent and thus violate the DCP  4.1. Weighted Utility anti-monotonic property  We argue that the DCP with weighted utility framework can be validated. We prove this by showing that if an itemset is not frequent, then its superset cannot be frequent and is always true \(see table 1 column 4, Weighted Utility ARM, only the itemsets are frequent with frequent subsets We also briefly prove that the monotonic property of itemsets is always valid in the proposed framework and is stated using the lemma as follows  Lemma If an itemset is not frequent them its superset cannot be frequent and     sueprset wus subset wus  is always true  Proof Given an itemset X not frequent i.e wus X wus min_    For any itemset Y X where Y   i.e. superset of X if a transaction t has all the items in Y, i.e t Y  then that transaction must also have all the items in X, i.e t X  We use tx to denote a set of transactions each of which has all the items in X, i.e       t X tx t T tx tx      Similarly we have       t Y ty t T ty ty      Since Y X   we have ty tx  Therefore     ty wus tx wus  According to the definition of weighted utility support, the denominator stays the same, therefore we have     Y wus X wus   Because wus X wus min_    we get wus Y wus min_    this then proves that Y is not frequent if its subset is not frequent 
89 


 4.2. Simulated Example  We demonstrate an example to simulate the process of weighted utility mining framework with valid DCP Table 3 is the weighted items table with weights associated with each item according to some profit margin  Table 3 Weighted items table Items i Profit Weights w  A £60 0.6 B £10 0.1 C £30 0.3 D £90 0.9 E £20 0.2  Table 4 is a transaction database with 10 records The last column in table 4 shows the transaction weighted utilities for each transaction and the last row shows the total transactional utilities sum Table 4 Transaction database with transactional weighted utilities of items Items A B C D E twu  1 1 1 4 1 0 0.700 2 0 1 0 3 0 1.400 3 2 0 0 1 0 1.050 4 0 0 1 0 0 0.300 5 1 2 0 1 3 0.575 6 1 1 1 1 1 0.420 7 0 2 3 0 1 0.433 8 0 0 0 1 2 0.650 9 7 0 1 1 0 1.800 10 0 1 1 1 1 0.375 Weighted Utility count 7.703  Table 5 shows all possible itemsets generated using table 3. Itemsets with classical ARM support are shown in column 2, itemsets with weighted ARM support are shown in column 3 and column 4 shows itemsets with weighted utility ARM support. Column 1 in table 5 shows the itemsets ids. Support threshold for classical ARM and weighted ARM is set to 30% and for weighted utility ARM it is set to 0.3 \(as equivalent to 30%\emsets with highlighted background are frequent itemsets This simulation illustrates the effect of an itemís utility and its weight on the generated rules. Using a standard ARM technique without considering items utilities and their weights, rules generated with 30 support are shown in column 2. It is interesting to note that the rules generated with 30% support using weighted ARM framework \(column 3\o frequent using the classical ARM technique. This is due to the fact that WARM uses already generated frequent itemsets with standard ARM approach and thus misses many potential ones as shown in table 5 But the proposed framework overcomes this problem by using weights and utilities for itemsets pruning using the Apriori approach, thus considers potential itemsets which WARM ignores Table 5 Weighted utility mining comparison  Standard ARM Weighted ARM Weighted Utility ARM 1 A \(50 A \(30 A  0.59  2 A B  30  A B  21  A B  0.22  3 A B C  20  A B C  20  A B C  0.14  4 A B C D  20  A B C D  38  A B C D  0.14  5 A B C D E  10  A B C D E  21  A B C D E  0.05  6 A B C E  10  A B C E  12  A B C E  0.05  7 A B D  30  A B D  48  A B D  0.22  8 A B D E  20  A B D E  36  A B D E  0.13  9 A B E  20  A B E  18  A B E  0.13  10 A C  30  A C  27   A C  0.38  11 A C D  30  A C D  54   A C D  0.38  12 A C D E  10  A C D E  20  A C D E  0.05  13 A C E  10  A C E  11  A C E  0.05  14 A D  50  A D  75   A D  0.590  15 A D E  20  A D E  34  A D E  0.13  16 A E  20  A E  16  A E  0.13  17 B \(60 B \(6 B  0.51  18 B C  40  B C  16  B C  0.25  19 B C D  30  B C D  39  B C D  0.19  20 B C D E  20  B C D E  30  B C D E  0.10  21 B C E  30  B C E  18  B C E  0.16  22 B D  50  B D  50   B D  0.45  23 B D E  30  B D E  36  B D E  0.18  24 B E  40  B E  12  B E  0.23  25 C \(60 C \(18 C  0.52  26 C D  40  C D  48   C D  0.43  27 C D E  20  C D E  28  C D E  0.10  28 C E  30  C E  15  C E  0.16  29 D \(80 D \(72 D  0.90  30 D E  40  D E  44  D E  0.26  31 E \(50 E \(10 E  0.32   Rules {A C}, {AC D} and {A D} in column 4 are frequent under Weighted Utility framework because of their high weight and utility in transactions But it is interesting to get a rule B D, because B has least weight and low utility count. Justification for this kind of rule is that, though B has low weight \(0.1\, it has the second highest count support \(i.e. 60%\d it appears more with item D than any other item \(i.e 50%\nother aspect to note is that D has the highest weight \(0.9\d count support \(80%\These kinds of rules can help in ìCross-Marketingî and ìLoss Leader Analysisî in real world applications Further, the rules generated using our approach holds a valid DCP and the monotonic property of itemsets as proved in section 4.1 and table 5 illustrates a concrete example of this. Itemset BD appears in transaction 1, 2, 5, 6, 9 and 10 with high utility therefore the 45  0    BD wus Intuitively, the occurrence of its superset BDE is only possible when 
90 


BD appears in that transaction. Itemset BDE only appears in transactions 5, 6 and 10, thus 18  0    BDE wus were     BD wus BDE wus   Summatively, if BD is not frequent, itís superset BDE is impossible to be frequent; hence there is no need to calculate its weighted utility support  5. Experimental Evaluation  In this section we report our performance study for the WUARM approach. In particular, we compare the quality and efficiency of WUARM algorithm with Apriori version of standard and weighted ARM, a well known algorithm for mining frequent itemsets Experiments were undertaken using three different association rule mining techniques. Three algorithms were used for each approach, namely Standard ARM as classical Apriori ARM, Weighted ARM \(WARM as post processing Apriori weighted ARM and Weighted Utility ARM \(WUARM\ proposed approach We performed two types of experiments based on quality measures and performance measures. For quality measures, we compared the number of frequent itemsets generated using three algorithms described above with real and synthetic data. In the second experiment, we showed the scalability of the proposed WUARM algorithm by comparing the execution time of three algorithms with varying support thresholds Both real and synthetic datasets are used in experiments. For real data we used Retail dataset, a real market bask an d T10I4D100K synthetic data is obtained from the IBM dataset generator [8   5.1. Frequent Itemsets Comparison  For quality measure, both the dataset described above were used. Each item is assigned a weight range between [0-1 g to th eir s i gn i f ican ce in t h e  dataset We generated artificial frequencies of items range 1o r bot h real an d sy n t h e t i c dat a t o obt ai n i t e m s utilities in transactions. In figure 1 and 2, the x-axis shows support thresholds from 1% to 6% and on the yaxis the numbers of frequent itemsets are shown. Three algorithms as described above are compared. Weighted Utility ARM algorithm uses weighted datasets with items utilities; Standard ARM using binary dataset and WARM using weighted datasets and applying a post processing approach. Note that the weight of each item in classical ARM is 1 i.e. all items have equal weight and utilities of each item in Standard ARM and WARM is 1 i.e. all items with utility exactly one which is not the case in real applications The results show quite similar behavior of the three algorithms to classical Apri ori ARM. As expected, the number of frequent itemsets increases as the minimum support decreases in all cases. The number of frequent itemsets generated using the weighted ARM algorithm are always less than the number of frequent itemsets generated by standard ARM because weighted ARM uses  frequent itemsets generated by standard ARM This generates less frequent itemsets and misses many potential ones   Retail Dataset 0 40 80 120 160 123456 Minimum Support Frequent Itemsets Standard ARM Weighted ARM Weighted Utility ARM  Figure 1 No. of frequent Itemsets generated using varying support threshold \(real data  T10I4D100K 0 100 200 300 400 123456 Minimum Support Frequent Itemsets Standard ARM Weighted ARM Weighted Utility ARM  Figure 2 No. of frequent Itemsets generated using varying support threshold \(synthetic data WUARM generated fewer rules than standard ARM but more rules than weighted ARM because it not only considers the items weight but also take into account the items utilities in each transaction and considers potential itemsets which weighted ARM ignores. Also we do not use standard ARM approach to first find frequent itemsets and then re-prune them using weighted utility support measures. Instead all the potential itemsets are considered from beginning for pruning using Apriori approach to validate the DCP 
91 


Results of the proposed WUARM approach are better than weighted ARM because we consider all the possible itemsets and uses items weight and their utilities. Moreover, WUARM, Standard ARM and WARM utilises binary data  5.2. Performance  For performance study, we compare the execution time of WUARM algorithm with classical Apriori ARM and WARM algorithms using both real and synthetic data   Retail Dataset 40 60 80 100 120 123456 Support Threshols Execution Time \(sec Standard ARM Weighted ARM Weighted Utility ARM   Figure 3 Execution time \(real data  We investigated the effect on execution time caused by varying the support threshold with fixed data size number of records\.  In figure 3 and 4, a support threshold from 1% to 6% is used again  T10I4D100K 0 10 20 30 40 123456 Support Threshold Execution Time \(sec Standard ARM Weighted ARM Weighted Utility ARM  Figure 4 Execution time \(synthetic data With both real and synthetic data WUARM has comparatively low execution time due to the fact that it generates fewer rules than standard ARM and do not use pre or post processing as mentioned earlier Weighted ARM has slightly higher execution time due to the fact that WARM initially uses classical ARM approach and then use already generated frequent sets for pruning, which takes computation time  6. Conclusion  In this paper, we have presented classical and weighted Association Rule Mining, in particular, the weighted utility framework which has the ability to deal with item weights and utilities in a hybrid fashion This framework can be integrated in the mining process, which is different to most utility and weighted ARM algorithms. To solve this problem, we identified the challenge faced while using weights and utilities together, in particular the invalidation of downward closure property Using a simulated example, we proved that weight and utility can be used together to steer the mining focus to those itemsets with significant weight and high utility. This is further proven by experiments conducted on real and synthetic datasets. We have showed that efficient WUARM algorithms can be developed by modifying the standard Apriori algorithm with weighted utility settings. The experiments also show that the algorithm is scalable  7. References  1 Bod o n  F A Fa st A p riori im ple m e n ta tion  In ICDM Workshop on Frequent Itemset Mining Implementations vol 90, Melbourne, Florida, USA \(2003  M  S u l a i m an Kh an  M  M u y e b a  F  Co en en  F u zzy Weighted Association Rule Mining with Weighted Support and Confidence Framework to appear in ALSIP \(PAKDD  2008, Osaka, Japan 3 H ong Y a o, H o w a r d J  H a m ilton  Mining itemset utilities from transaction databases Data & Knowledge Engineering pp. 603- 626, Volume 59, Issue 3 \(2006 4 H  Y a o, H  J  Ham ilton, a nd C  J  B u tz A Founda tio na l  Approach to Mining Itemset Utilities from Databases 4 th  Intl. conf. on Data Mining Florida, USA, \(2004 5 Jia n y i ng Hu A l e k s a ndra Moj s ilov i c   H ig h-utility pa tte rn mining: A method for discovery of high-utility item sets Pattern Recognition Elsevier Science Inc, Volume 40 Issue 11, pp. 3317-3324, \(2007 6 R Cha n  Q. Ya ng a n d Y-D  She n   M ini n g Hig h Uti lity  Itemsets In proc. of 3 rd IEEE International Conference on Data Mining \(ICDM'03 pp. 19-26, Melbourne, FL, \(2003 7 FI MI Fr e q ue nt I t e m s e t Mining I m plem e n ta tion  Repository http://fimi.cs.helsinki.fi  8 IBM Sy n t h e tic Data Generator http://www.almaden.ibm.com/s oftware/quest/resources/index html 
92 


J+R-? ?-R3-J -#\\E J+#I\(L I#9\**-#- JI-L   n n\r%R&J!%W&S?:"\r  n\n\r 1	\n	17!\n  r+\n-\r\n*2 B%BB C       71    E2   n    r  n\r\r\n!\r	#\n1\n  n\r\r\n\r  3  r	\r9\r	\n*\r2\n\n0\r r n+\n	8\n\n#6\r!\r n\r\r\n r445	*'+,*$-\r\n\n\r\r\r\n*\r\r\n   BBB  I\n\r%9&R	%W&%S&S:-*\n\r n-	\r n7	 \n\r\nJ	"\r*\r2\n\n0\r n n\r\r\n\r r\n+\r\n	\n\r\r\r\n.\n/\r\r  n!\r0     E  R      I\n\r  E  E\n  9  r n\r	;\r\r"3GG000&\n&\n&GX\n\rG""\nGI    95  E      1    60    r\n  2\r r7\r  0.13  Y\n\n\n  B""&%3CB  R!2%&\(\n\n  r 8\r\n\r n\n29 r	\n\r\r  r\n-\r\n*\n\r  WZ\n%"\r 3BB  R!2%W& \n\r  r-8\r\n\r: \n\n29 r7\r  n    r  n\n    r!\r       W  Z  n  r  CBC  3 CCC B J%#&J	%,&J0:\r 3-10	2 n\r	\r r      n\r\r  n  r       r\n\n\r\r\r\n.\n/\r\r\n!\r0   BB    1    60  r\r  n  2\r\n 3BB  I&\(!%9&*:*\r 	&2!\n \r n   r\n\r\r	\n\r\r\n-:\r\r  MB&%*!2B C   71        9  L        J1    n\r\r  n  r    n\r\r\r  n r0\r\r\r  R&, :\r\n\r\r\n n\n n\r\r#6\r  n\r\r\n\r'\r\n	\n\r\r\r\n  r# \r\\n\r  W  22  n\n\r  n    r  n\n2  n     IA  J0  I2  n\n\r\n    3 CCC%B""%""BC    1  7  r\n  E    J1    n\r\r\n\r'\r\n\n'!\r0\r\r*\r\n  r  n\n  pp 467-472     1    r\r      J1    r   n 2!\n n\r\r\n\r'!\r0*\r\n'\r\n    n\r0\n  


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


