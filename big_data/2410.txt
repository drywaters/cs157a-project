Research on Application of Improved Association rules Algorithm in intelligent QA system Youfu Du 002 Ming Zhao 002 Guanjun Fan College of Computer Science, Yangtze University, Jingzhou, Hubei, 434023, China Abstract With increasingly prevalent of E-learning, the intelligent Q/A system arise at the historic moment. In this paper we  proposed  an improved text cluster algorithm with the improved asso ciation rules algorithm 002 It can classify the information in the database accurately according to the questions, locate the userí question fast and therefore speeds up the inquiry rate. The experimental result indicates t hat this plan has the merits of intelligence, self-renewing, and efficient Q/A and so on 1. Introduction With the development of Internet technology, Elearning becomes increasingly popular. Facing the difficult questions appearing in study, most learners hope to obtain a satisfied answer fastly through a learning platform. There is no doubt that the intelligence Q/A system can meet request of learners. In this paper we designed an intelligence Q/A system with text data mining algorithm , which applies the improved text cluster algorithm and effectively speeds up the inquiry rate 2. Association rules algorithm and its improvement The association rules and its algorithm always count for much in data mining as a result of its huge commercial value and the theoretic value since being proposed According to the defin ition of association rules, its discovery duty may be defined as: Assigning a business database D, extract all associati on rules which satisfy the smallest support--Smin and the smallest confidence Cmin 2.1 The classical association rules mining algorithm 001j the Apriori algorithm and its flaw The Classical Apriori algorithm is an association Rules Mining algorithm with Width first Step 1: when scanning the database, calculate the support of all single projects in the database and form a single dimension frequent ite m-set with the projects whose support is bigger than the minsupp, namely L1 Then repeat scanning the database. When scanning the kth time, namely L k, the frequent item set is produced with k length. When scanning the \(k+1\e, firstly produce the candidate item set Ck+1 with k+1 length from L k, secondly use Hash tree to scan the C k+1 002 it produces a frequent item set with k+1 length 002 namely L k+1, until no production of frequent item set. The final frequent item set is 001 L k. Its merit lies in the association rules, which can effectively prune item sets and does not produce and calculate those candidat e item sets which are impossible to become frequent item-set. And thus we can obtain the smaller candidate item sets But the Apriori algorithm has some shortcomings as follows 1\The algorithm has some problem in the efficiency possibly producing the massive candidate sets. Too many scanning times are the primary cause, in that the database needs to be scanned once when seeking every k frequent item set \(k=1, 2,Ö, K\es. In addition, the produced candidate item sets are too many when the pattern is too long. Therefore when the database or K is too large, the running time of the algorithm would be too long or is unable to be completed. So the algorithm has not great extendibility and is difficult to be promoted 2\ produces too many false redundancy\hen the database is too large or the support or the confidence threshold value is too low, too many rules will be produced. And it is very difficult for users to discriminate and judge these rules and find truly useful information 2.2 The improved association rules algorithm the Apriori+ algorithm 2.2.1 The basic conceptions Aiming at the two major faults above, We propose an improved association rules algorithm 002 the Apriori 
Second International Conference on Genetic and Evolutionary Computing 978-0-7695-3334-6/08 $25.00 © 2008 IEEE DOI 10.1109/WGEC.2008.74 414 


algorithm. According to the deci sion-making attribute in mining, the insignificant records in data source are eliminated through the reduction operation. Thus we can reduce the running time of programs, save the space, and thereby raise the mining efficiency greatly and solve the redundancy in a certain extent To describe the algorithm well, there are several conceptions as follows Definition 1 the weighting function Supposing D is a record set of historical transaction data ti is the ith record and also a vector constituting a group of attributes    t1   bm   aking attribute a i when in record t i ; tm[bm is the value of condition attribute bm when in record tm. Take weight W i as a new attribute to t i and form a new record tw i. The record set of historical data added with weight attribute is called Dw twi  ti [ai], t1 [b1], t2  tm bm  wi The correspondent weight wi of the ith record is defined as   ai The weight is defined as a kind of function of one or several decision-making attributes in the record Definition 2 the weight support The weight support of the Apriori+ algorithm is 1 1 1   n m ijj j i n i i wXt a Supp x y w 002 002 002 002 003 003 002 001\002\001\002\001\002\001\002\001\002\002 1 002 Definition 3 the confidence the confidence of th e Apriori+ algorithm is 002    Supp X Y Conf X Y Supp X 004\002 002 001\002\001\002\001\002\002 2 002 2.2.2 The description of the Apriori+ algorithm There are two main functions 1\source, clean those records whose items are less than the experiential rules requested, and obtain a subset 2\to each record according to the function of certain decisionmaking attributes. Compute the support and the confidence through weight and delete the records with low support The Algorithm flow is shown as Figure 1 Input : database: attri_supp 002 Overall mini_supp Overall mini_conf Fig.1 The Apriori+  algorithm flow chart The algorithm is d escribed as follows Output: large item set {kLk k 0011 1 andLk 001  DBset is the regular subset nRulesItemCount is the least number of items needed to meet the rules   is the weighti ng function of decision-making attribute w i  is the weighting function value Lwk is the large item set of the weighting function Cwk is the candidate set of the weighting function Wfoun-tionsup is the support of the weighting function fCWConf is the confidence of the weighting function Produce regular subset database with experiential rules --DBset D= {order by the number of items in each record Hk= {take rules from the training storehouse and obtain the least number of items-- Item-Count while \(! DBsettable.IsEOF if\(nRecordsItemCount<nR ulesItemCount DBsettable.Delete\(\thus obtain the subset storehouse --DBset discover the large item set from DBset for\(i=0;! DBsettabl e.IsEOF 
415 


   Establish the weighting function of decision-m aking attributes in DBset total-weight=   // Sum the weighting function in the subset database Lw1={large1-itemsets of subdatabase Cw1=Lw1 of subdatabase for\(k=2;L 001\031 k-1 begin Cwk=apriori-gen\(Lwk-1\Cwk=Lwk-1*Lwk-1 when the first k-2 items of Lwk-1 are the same. For each produced candidate k- item set, inspect wether its all \(k1\ subsets are frequent \(k-1 m sets without rules If they are not, delete the candidate item set for all twi 001\031 DBset do begin // determine which item sets in candidate ones are contained in transactions that take twi as symbol 002 Cwt=subset\(Cw2,t\all \(k+1\ subsets and for each such subset,judge whether its all k- item subsets are candidate item sets.And filter the possible candidate \(k+1\ sets. If there are some items which have never appear in these candidate \(k+1\ sets then delete these items, de posit the others in t for all candidatesc 001\031 Cwk do begin if \(Cwt 001  begin c.weight+=tw.weight end end Wfountionsup=c.weight/total-weight Lwk={c 001\031 Cwk \( Wfountionsup\inisup} // delete the records whose support is smaller than the minsupp end judge whether Lw2 is the largest item set. If it is compute its confidence and produce rule model if \(Lw2 is the large item set fCWConf= { calculate the confidence based on Wfountionsup if \(fCWConf> Wfountionsup ValuabeRules= {the produced valid rules fCWConf=0.0f else fCWConf=0.0f else Wfountionsup=0.0f end This algorithm has remarkable merits Firstly it cleans the records whose nRecordsItemCount is smaller than nRulesIt emCount--the least number of items needed for known rules.And thus obtain the subset storehouse-- DBset Also use Weight to express the orginal weight Substitute n \(the sum of record s in traditional algorithm with total-weight 000 the sum of weights; Substitute c.count\(the sum of transactions with tradition the accumulation of weights and its weight support is c.weight/total-weight Then delete the records whose weight support is smaller than minsupp so that we can reduce the time and the complexity of space 3.  The design of intelligent Q/A system On the basis of the above association rules algorithm this system firstly segments the questions of learners with data mining text cluster algorith m. The cut words will be saved together with their correspondent documents in a table. Then carry out the association rules algorithm and the weight calculating al gorithm separately, and obtain weight value of each words and association rules based on the key words in each document. Secondly write the first component the consequent of association rules in the association table, then calculate similarity of the text, and thus get the similarity of sentences. After obtaining the similarity, use it to discover th e current best answer to the question. Thus meet the request of learners and the intelligence Q/A system can be realized This function frame can be designed as Figure 2 000\003 Fig.2 The function frame of intelligence QA system 4. The experimental data and analysis In this experiment, there are 300 records about questions and 1035 records about answers 0017 Figure 3 shows the comparison of mining time with two algorithms, with different smallest complete weighting support threshold values 0018 Figure 4 shows the change of the number of the candidate item sets about the test algorithm,under 
416 


different smallest complete weighting support threshold values 000\023 000\024\000\030\000\023 000\026\000\023\000\023 000\027\000\030\000\023 000\023\000\021\000\023\000\034\000\023 000\023\000\021\000\023\000\032\000\023 000\023\000\021\000\023\000\030\000\023 000\023\000\021\000\023\000\026\000\023 000\023\000\021\000\023\000\024\000\024 000\023\000\021\000\023\000\023\000\034 000\023\000\021\000\023\000\023\000\032 000\023\000\021\000\023\000\023\000\030 000W\000\022\000V Fig 3. Mining time comparison under differentsupport thershold 000\023 000\024\000\023\000\023 000\025\000\023\000\023 000\026\000\023\000\023 000\027\000\023\000\023 000\030\000\023\000\023 000\031\000\023\000\023 000\032\000\023\000\023 000\023\000\021\000\023\000\034\000\023 000\023\000\021\000\023\000\032\000\023 000\023\000\021\000\023\000\030\000\023 000\023\000\021\000\023\000\026\000\023 000\023\000\021\000\023\000\024\000\024 000\023\000\021\000\023\000\023\000\034 000\023\000\021\000\023\000\023\000\032 000\023\000\021\000\023\000\023\000\030  000D\000W\000H 000L\000W\000H\000P\000V\000H\000W\000V\000\022\000Q\000X\000P\000E\000H\000U\000V Fig 4 Size of the candidate items under different support threshold From the above experiment, we can know that the mining time of the Apriori algorithm is 21.53% more than that of the Apriori+ algorithm in average, 38.42% most With the same quantity of mined frequent item sets, the number of candidate item set s produced by the Apriori algorithm is 27.02% more than the Apriori+ algorithm produces in average, 45.61% most. The experimental results indicate that the Aprio ri+ algorithm truly has higher mining efficiency than the Apriori algorithm 4. The conclusion In this paper we adopt the improved association rules in text cluster,it can reduces the cluster time, and raises the data mining efficiency. Thus can locate the learner's question fast and speed the Q/A to a certain degree. It has the merits of intelligence, self-renewing and high Q/A efficiency and so on. As a result of scientific research condition and limited time 002 it has some fault yet 001 the accurate and fast Q/A is still the aim of the system References  Shou-ning QU. Intelligent Question Answering Sy stem  000\003 Journal of Zhengzhou University Nat.Sci.Ed  Savasere A, Om iecinski E, Navathe S. An efficient algorithm for mi P roceedings of the 001 21st International Conference on Very Large Database. Zurich Switzerland, 1995:432-444  Meiling Liu, Zhangy an Xu, Jingli Lu, et al.An improved Apriori algorithm by Guangxi Normal University: Natural Science Edition, 2004,22\(1  Agrawal R, Im ielinski T, Swam i A. Mining association rules between sets of item  Proceedings 001 of the ACM SIGMOD International Conference Management of Date. Washington, 1993:207-216  Shouning Qu, Qin Wang. Research and application in supply chain management based on correlation analysis of   Materials Science Forum, 2006 532/533  Margaret H D. Data Mining Introductory  and Advanced  Beijing: Tsinghua University Press, 2005: 107-110 
417 


                                                                                     2400 2008 Chinese Control and Decision Conference CCDC 2008 


B II*L\E#9I #&M\J+I%B  r r!0\n\n\rB\n\n\n   r  n    r"\n    n   n	\r\n	\n	\r  r"\r\r	\r!0\n r 0 r  n\r   r\r2\r r  r  0\n  n!"\r\r\n\r\n\n\r	BC n&*	70\r\r2\r7\n0\r n\r\r\n\n\r\n 7$\r"!\r	\r!%!\n\n    r r	\r$\r7\n   r"\r r$\r7\n   r"\r	7\n r\r%\n"7\n\n\r\n 2M  E	B n0\r\r\n"2\r0\r n\n\r  r  r    r 0\n2\r$\n\r\r 2M 2\r0 r r\n!\n\n r6\r"\r	\r r               B B  B      B B  C     B     B     B&B   B B&B   B&B    n\r\n\n0\r\r\r!27\n!1\n r\r\n	0	0\r\r\n\r  r r& \r$\r\r\n!2\n7\r\n r6\r"\r	%\r\n!27\n D  n  7\n  r\r  n  r\r2\r\n  r\r   r  n\r n!\r%\r\r\r6\r\r r r2\r\n0\r\r\n!&#"\r	\r  1\r\n\n\r\n"1\r r r 4\r&C&#\n\r\n2\r\r\r\r\n\r6\r r	\n\r!2\r$\r7 n\r\n7\r"!\r\n4\r\r\r\r  2\r7\n&#\n\n"7\n\r\r\r6\r r		\r!"!\n2\r\r\r\r r  r\n     2    r  r  7\n  n  0	0\r\n\r"!\r&907%7  r r r2\r\r\r	7\n"\r n\r\n%&&\r!2\r$\r7\n\n  r!2\r$\r7\n\n0&\n  r\n6"!\r\r7\r"!\r\n&C n!"7!\r\r\n!\r!$2\r    n\n2      r  r    77       r    r  n    r n&#\n"!\r\n\r\n!!	\n\r n r\r-"	\r!&#\n02 r:!"22;\n!\r\r!2 r$\r7\n&-\n	\r	\r\n\n  r0\n\r2\r\r	\r   J   n\r\n    r  6"!\r\n  7  r\n\n   2\r	\r!\n\r\r2\r\r"7\r\r\n	\r r\n\r	\n\r\r\n r\r\r\n2\n2/\r\r\n r!	!\r	\n""\r\r 2\n\n\r\r\n0\r n 7    n n\n  r\n  r    2    r r\r\n\r I6"!\r\n\r\n2\r	\r!\n\r2$\r\r n0\r\r\r\r2\r\r\n"\n\n20\r\r6\r  r	7	\n\r\n&907%\r  r\n\n2\r$\n\n\r\r!\n  r\n\r\r\r$\r  7\n&-\n\r0\n\n7\r6"!\r\n%\r r7\n\n!\r"\n\n\r	$\r\n"!\r\n r!"!\r\n\n7!$	7"\r!\n\r\n r\r6!\r\n7$1$\r\r r r\n\r\n0\r7\n\r"!\r\n\r 7	\r\r$\r 276"!\r\n\r5$\n\r	!\r n\r@J7\n\r\n\rA0\n\n&\r\n"\n\n2\r\r r\r\n\n\r2\r\r\r"\r\r\n\n 26"\r6"!\r\n 7%\r\n""\r\r\r r4\n\r\r	\n  IEI\(I I    0    n1    0    n\n r  n 2\r0\n\r\n\r!\nJ	*\r2\n\n  n\r\r\n   r\n  n\r\r\r  n  r\r  n    BB B       r  9\r\n  r  I\r"\n    r    r\r"3GG\n!&\r\r	\r&!G0\nG\rG%BB%\nV C B%&\r    1       r7  r  r\r  n  J2\r	!\r$\n\n  n\r\r n\r     r\n n\r\r\r n  n/\r\r  n!\r0  1 E	B3#"$2\r0\r\n\r#\n"!\r n\n\r6\r"\r	\r                  n n        n       r     


J+R-? ?-R3-J -#\\E J+#I\(L I#9\**-#- JI-L   n n\r%R&J!%W&S?:"\r  n\n\r 1	\n	17!\n  r+\n-\r\n*2 B%BB C       71    E2   n    r  n\r\r\n!\r	#\n1\n  n\r\r\n\r  3  r	\r9\r	\n*\r2\n\n0\r r n+\n	8\n\n#6\r!\r n\r\r\n r445	*'+,*$-\r\n\n\r\r\r\n*\r\r\n   BBB  I\n\r%9&R	%W&%S&S:-*\n\r n-	\r n7	 \n\r\nJ	"\r*\r2\n\n0\r n n\r\r\n\r r\n+\r\n	\n\r\r\r\n.\n/\r\r  n!\r0     E  R      I\n\r  E  E\n  9  r n\r	;\r\r"3GG000&\n&\n&GX\n\rG""\nGI    95  E      1    60    r\n  2\r r7\r  0.13  Y\n\n\n  B""&%3CB  R!2%&\(\n\n  r 8\r\n\r n\n29 r	\n\r\r  r\n-\r\n*\n\r  WZ\n%"\r 3BB  R!2%W& \n\r  r-8\r\n\r: \n\n29 r7\r  n    r  n\n    r!\r       W  Z  n  r  CBC  3 CCC B J%#&J	%,&J0:\r 3-10	2 n\r	\r r      n\r\r  n  r       r\n\n\r\r\r\n.\n/\r\r\n!\r0   BB    1    60  r\r  n  2\r\n 3BB  I&\(!%9&*:*\r 	&2!\n \r n   r\n\r\r	\n\r\r\n-:\r\r  MB&%*!2B C   71        9  L        J1    n\r\r  n  r    n\r\r\r  n r0\r\r\r  R&, :\r\n\r\r\n n\n n\r\r#6\r  n\r\r\n\r'\r\n	\n\r\r\r\n  r# \r\\n\r  W  22  n\n\r  n    r  n\n2  n     IA  J0  I2  n\n\r\n    3 CCC%B""%""BC    1  7  r\n  E    J1    n\r\r\n\r'\r\n\n'!\r0\r\r*\r\n  r  n\n  pp 467-472     1    r\r      J1    r   n 2!\n n\r\r\n\r'!\r0*\r\n'\r\n    n\r0\n  


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


