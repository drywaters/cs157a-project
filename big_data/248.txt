An Efficient Clustering Algorithm for Market Basket Data Based on Small Large Ratios Ching-Huang Yun and Kun-Ta Chuang and Ming-Syan Chen Department of Electrical Engineering National Taiwan University Taipei, Taiwan, ROC E-mail chyun@arbor.ee.ntu.edu.tw, doug@arbor.ee.ntu.edu.tw mschen@cc.ee.ntu.edu.tw Abstract In this papec we devise an eflcient algorithm for cluster ing market-basket data items In view of the nature of clus tering market basket data, we devise in this paper a novel measurement, called the small-large \(abbreviated 
as SL ra tio, and utilize this ratio toperform the clustering. With this SL ratio measurement we develop an eficient clustering algorithm for data items to minimize the SL ratio in each group. The proposed algorithm not only incurs an execution time that is sign@cantly smaller than that by prior work but also leads to the clustering results of very good quuliv Keywords Data mining, clustering analysis, market basket data small-large ratios 1 Introduction Mining of databases has attracted a growing amount of 
attention in database communities due to its wide applica bility to improving marketing strategies 3][4 Among oth ers, data clustering is an important technique for exploratory data analysis 5][6 In essence, clustering is meant to di vide a set of data items into some proper groups in such a way that items in the same group are as similar to one another as possible Market-basket data analysis has been well addressed in mining association rules for discovering the set of large items Large items refer to 
frequently pur chased items among all transactions and a transaction is rep resented by a set of items purchased 2 Different from the traditional data, the features of market basket data are known to be high dimensionality sparsity, and with mas sive outliers 7 The authors in 8 proposed an algorithm for clustering market-basket data by utilizing the concept of large items to divide the transactions into clusters such that similar transactions are in the same cluster and dissimilar B C U 
U F It B,O,I  il C TlD IlemScf 210 13.1 220 A,KI 230 B,E.I 2411 U.C,E,I 2su c.1 40 40 I n n  Figure 1 An example database for clustering market-basket data transactions are in different clusters This algorithm in 8 will be referred to as algorithm Basic in this paper, and will be 
used for comparison purposes. An example database for clustering market-basket data is shown in Figure 1 In view ofthe nature ofclustering market basket data we devise in this paper a novel measurement called the small large \(abbreviated as SL ratio and utilize this ratio to per form the clustering. The support of an item i in a cluster C Supc\(i is defined as the percentage of transactions which contain this item 
i in cluster C For the clustering U0 shown in Figure 1 the support Supc A is 20 and Supc B is 80%. An item in a cluster is called a large item if the sup port of that item exceeds a pre-specified minimum support S i.e an item that appeared in a sufficient number of trans actions On the other hand an item in a group is 
called a small item if the support of that item is less than a pre specified maximum ceiling E i.e an item that appeared in a limited number of transactions To model the relation ship between minimum support S and maximum ceiling E the damping factor X is defined as the ratio of E to S i.e  0-7695-1372-7101 10.00 0 2001 IEEE 505 


Minimum Support S  60 Maximal Ceiling E  30x1 A C F G H 1 B C G Intra\(U  7 Inter U  2 Cost Uo  9 S1.R Threshold U  3/2    350 I D,G,H I 1/2 2.1 Large Items and Small Items The concept of large items is first introduced in mining association rules 2 In 8 using large items as similar ity measure of a cluster is utilized in clustering transac tions Specifically, large items in cluster Cj are the items frequently purchased by the customers in cluster Cj In other words large items are popular items in a cluster and thus contribute to similarity in a cluster While rendering the clustering of fine quality it is noted that the execution efficiency of 1 he algorithm in 8 could be fkther improved due to its relatively inefficient steps in the refinement phase This could be partly due to the reason that the similarity measurement used in 8 does not take into consideration the existence of small items To remedy this a maximal ceiling E is proposed in this paper for identifying the items of rare occuirences If an item whose support is below Figure 2 The large middle, and small items in clusters, and the corresponding SL ratios of transactions X  f In addition an item is called a middle item if it is neither large nor small For the supports of the items shown in Figure I if S  60 and E  30%, we can obtain the large, middle and small items shown in Figure 2 In C2  210,220,230,240,250 B and I are large items In addition C and E are middle items and A is a small item Clearly, the portions of large and small items represent the quality ofthe clustering. Explicitly the ratio ofthe num ber of small items to that of large items in a group is called small-large ratio of that group Clearly, the smaller the SL ratio, the more similar the items in that group are With this SL ratio measurement, we develop an efficient clustering al gorithm algorithm SLR standing for Small-Large Ratio for data items to minimize the SL ratio in each group It is shown by our experimental results that by utilizing the SL ratio, the proposed algorithm is able to cluster the data items very efficiently This paper is organized as follows Preliminaries are given in Section 2 In Section 3 an algorithm, referred to as algorithm SLR Small-Large Ratio is devised for cluster ing market-basket data. Experimental studies are conducted in Section 4. This paper concludes with Section 5 2 Preliminaries a specified maximal ceiling E that item is called a small item Hence small items in a cluster contribute to dissimi larity in a cluster In this paper the similarity measurement of transactions is derived from the ratio of the number of large items to that of small items. In the example shown in Figure 1 with the minimum support S  60 and the max imum ceiling E  30 we can obtain the large middle and small items by counting their supports In Cl item B is large because its support value is 80 appearing in TID 1 10 120,130 and 150 which exceeds the minimum sup port S However item A is small in C1 because its support is 20% which is less than the maximum ceiling E 2.2 Cost Function We use La1 Cj  S to denote the set of large items with respect to aictribute I in Cj and Srn~\(Cj E to denote the set of small items with respect to attribute I in Cj For a clustering U  C  Ck the corresponding cost for attribute I has two components the intra-cluster cost Intra1 U and the inter-cluster cost Inter1 U which are described in detail below Intra-Cluster Cost The intra-cluster item cost is meant to represent for intra-cluster item-dissimilarity and is mea sured by the total number of small items where a small item is an item whose support is less than the maximal ceiling E Explicitly we have We investigate the problem of clustering market-basket data where the market-basket data is represented by a set of transactions A database of transactions is denoted by D  tl t2  tlL where each transaction ti is a Note that we did not use x&iISm~\(Cj as the intra set of items  il  i2    ih For the example shown in cluster cost since the use of  Smr Cj  E I may cause Figure 1 we are given a predetermined clustering U0  the algorithm to tend to put all records into a single or C1 C2 C3  where C1  110,120,130,140,150 few clusters even though they are not similar For exam C2  210,220,230,240,250 and C3  ple suppose that there are two clusters that are not sim 310,320,330,340,350 If large items remain ntral  I Smr\(Cj E ilar but share some small items 5 06 


large after the merging merging these two clusters will reduce ISmr\(Cj E because each small item previ ously counted twice is now counted only once However required The goal of this paper focuses on designing an efficient algorithm for the refinement phase this merging is incorrect because sharing of small items should not be considered as similarity For the clustering U0 shown in Figure 2 the small items of C1 are A C F G H I In addition, the small item of Cz is A and the small items of C3 are B C G Thus, the intra-cluster cost Intra1 Uo is 7 Inter-Cluster Cost The inter-cluster item cost is to rep resent inter-cluster item-similarity and is measured by the duplication of large items in different clusters where a large item is an item whose support exceeds the minimum support S Explicitly we have Interr\(U  E~=,IL~~\(c,,s  1 LU~\(C,,S Note that this measurement will inhibit the generation of similar clusters For the clustering U0 shown in Figure 2 the large items of C1 are  B D In addition the large items of Cz are  B I and the large items of Cs are  D H Asaresult E;=,ILUI\(C,,S  6 and 1 Lnl\(C,,S  4 Hence, the inter-cluster cost Interr\(U0  2 Total Cost Both the intra-cluster item-dissimilarity cost and the inter-cluster item-similarity cost should be consid ered as the total cost incurred Without loss of generality a weight w is specified for the relative importance of these two terms The definition of item cost Cost~\(Uo with re spect to attribute I is Cost1  Uo  w  intra Uo  Iderr Uo  If the weight zu  1 I?1t?Xr\(Uo is more important than I,nterZ\(Uo and vice versa In our model we let w  1 Thus for the clustering U0 shown in Figure 2 the CostI\(U0 is 7  2  9 3 Algorithm SLR for Clustering Market Basket Data In this section we devise algorithm SLR \(Small-Large Ratio which essentially utilizes the measurement of the small-large ratio SL ratio for clustering market-basket data For a transaction t with one attribute I ILl\(t rep resents the number of the large items in t and ISz\(t I repre sents the number of the small items in t The SL ratio oft with attribute I in cluster Ci is defined as For the clustering shown in Figure 1    110,120,130,140,150 Cz  c1  210,220,230,240,250 and C3  310,320,330,340,350 Figure 2 shows that the minimum support S  60 and the maximal ceiling E  30 For TID 120 we have two large items B D and one small item A Thus the SL ratio of TID 120 is SLR1tem\(C1 120   0.5 Similarly the SL ratio of TID 240 is SLR1t,,,\(Cz 240    1 because TID 240 has 2 large items  B I and 2 small items  C E As mentioned before although algorithm Basic utilizes the large items for similarity measurement algorithm Basic is exhaustive in the decision procedure of moving a transac tion t to cluster Cj in current clustering U   CI    ck  For each transaction t algorithm Basic must compute all costs of new clusterings when t is put into another cluster In contrast by utilizing the concept of small-large ratios algorithm SLR can efficiently determine the next cluster  for each transaction in an iteration where an iteration is 2\2213 Objective Of Market-Basket Data a refinement procedure from one clustering to the next The objective ofclustering market-basket data is 223We are given a database of transactions a minimum support and a maximum ceiling Then we would like to determine a clustering U such that the total cost is minimized\222 The procedure of clustering algorithm we shall present includes two phases namely the allocation phase and the refinement phase In the allocation phase the database is scanned once and each transaction is allocated to a cluster based on the purpose of minimizing the cost The method of allocation phase is straightforward and the approach taken in 8 will suffice In the refinement phase each transaction will be evaluated for its status to minimize the total cost Explicitly a transaction is moved from one cluster to another cluster if that movement will reduce the total cost of clustering The refinement phase repeats unti no further movement is clustering 3.1 Description of Algorithm SLR Figure 3 shows the main program of algorithm SLR which includes two phases the allocation phase and the re finement phase Similarly to algorithm Basic 8 in the al location phase, each transaction t is read in sequence Each transaction t can be assigned to an existing cluster or a new cluster will be created to accommodate t for minimizing the total cost of clustering For each transaction the initially allocated cluster identifier is written back to the file How ever, different from algorithm Basic algorithm SLR com pares the SL ratios with the pre-specified SLR threshold cr to determine the best cluster for each transaction Note that 507 


some transactions might not be suitable in the current clus ters Hence we define an excess transaction as a transac tion whose SL ratio exceeds the SLR threshold a In each iteration of the refinement phase algorithm SLR first com putes the support values of items for identifying the large items and the small itenis in each cluster Then, algorithm SLR searches every cluster to move excess transactions the excess pool where all excess transactions are collected to gether After collecting all excess transactions we compute the intermediate support values of items for identifying the large items and the small items in each cluster again Fur thermore empty clusters are removed In addition we read each transaction t from the excess pool In line 8 to line 14 of the refinement phase shown in Figure 3 we shall find for each transaction the best cluster that is the cluster where that transaction has the minimal SL ratio after all clusters are considered If that ratio is smaller than the SLR thresh old we then move that transaction from the excess pool to the best cluster found However if there is no appropriate cluster found for that transaction t t will remain in the excess pool If there is no movement in an iteration after all transactions are scanned in the excess pool the refine ment phase terminates Otherwise the iteration continues until there is no further movement identified After the re finement phase completes, there could be some transactions still in the excess pool that are not thrown into any appro priate cluster These transactions will be deemed outliers in the final clustering result In addition it is worth mentioning that algorithm SLR is able to support the incremental clus tering in such a way that those transactions added dynam ically can be viewed as new members in the excess pool Then algorithm SLR will allocate them into the appropri ate clusters based on their SL ratios in existing clusters By treating the incremental transactions as new members in the excess pool algorithm SLR can be applied to clustering the incremental data efficiently 3.2 Illustrative Example of SLR Suppose the clustering U0  C1 C C3  shown in Figure 1 is the clustering resulted by the allocation phase The cost of U0 examined by the similarity measurement is shown Figure 2 In this experiment, the minimum sup port S  60 the maximal ceiling E  30 and the SLR threshold a   In the refinement phase shown in Figure 4 algorithm SLR computes the SL ratio for each transac tion and reclusters the transactions whose SL ratios exceed a Figure 5 is the final clustering U1  Ci C CA  obtained by applying algorithm SLR to the clustering U0 First algorithm SLR scans the database and counts the sup ports of items shown in Figure 1 In C1 the support of item A is 20 and the support of item B is 80 Then algorithm SLR identifies the large and small items shown Figure 3 The overview of algorithm SLR in Figure 2 In C1 item A is a small item and item B is a large item For the transactions in each cluster algorithm SLR compuies their SL ratios in that cluster In C1 the large items are B D and the small items are A C F G H I For transaction TID 120 item A is a small item and items B D are large items Thus the SL ra tio of TID 12!0 is SLR1tC,,\(C1,120  which is smaller than a However, for transaction TID 140, items  F H are small items and item  D is the only large one The SL ra tio of TID 140 is SLRI~~,,\(C 140   larger than a After the SL ratios of all transactions are determined algo rithm SLR shall identify the excess transactions and move them into the excess pool Three transactions i.e TIDs 140 150 and 330 are identified as the excess transactions as shown in Figure 2 After collecting all excess transac tions we compute the intermediate support values of items for identifying the large items and the small items in each cluster again The intermediate clustering of U0 is shown in Figure 4 For each transaction in the excess pool algorithm SLR will compute its SL ratios associated with all clusters except the cluster that transaction comes from Note that an item that is not shown in the cluster C can be viewed as a small item because its support will be one when the corresponding transaction is added into C For transaction TID 140 moved from C1 SLRlt,,\(C2,140   CO with three small items D F H in C2 On the other hand SLRrtem\(C3,140  with one small item F and two large items  D H in C3 For transaction TID 140 508 


C C C Item A B c E G 330 EC V,F L Support 167 83.3 33 3 33 3 167 BCPI CI ao I Figure 4 Using small-large ratio to recluster the transactions by algorithm SLR the smallest SL ratio is SLRrtem\(C3 140  which is smaller than cx  4 Thus, transaction TID 140 is reclus tered to C Figure 4 shows that algorithm SLR utilizes the SL ratios to recluster transactions to the most appropriate clusters The resulting clustering is U1  Ci Ci CA  In the new clustering, algorithm SLR will compute the sup port values of items for all clusters Figure 5 shows the supports of the items in C C4,and Ci Algorithm SLR proceeds until no more transaction is reclustered The clus tering U1 is also the final clustering for this example and the final cost Custr\(U1  5 which is smaller than the initial cost Costr\(Uo  9 4 Experimental Results To assess the performance of algorithm SLR and algo rithm Basic we conducted several experiments for cluster ing various data We comparatively analyze the quality and performance between algorithm SLR and algorithm Basic in the refinement phase 4.1 Data Generation We take the real data sets of the United States Congres sional Votes records in 1984 I for performance evalua tion The file of 1984 United States congressional votes contains 435 records each of which includes 16 binary at tributes corresponding to every congressman's vote on 16 key issues e.g the problem of the immigration the duty of export and the educational spending, and so on There are 168 records for Republicans and 267 for Democrats We Fl 100 25 I I 100 100 Minimum Support S  60 Maximal Ceiling E  30 Middle Small C E A G Ir3.I n I F I c I Intra\(U  3 Inter U  2 cost U  5 Figure 5 The clustering Ul  Ci Ci C  obtained by algorithm SLR set the minimum support to 60 which is the same as the minimum support setting in 8 for comparison purposes To provide more insight into this study we use a well known market-basket synthetic data in 2 as the synthetic data for performance evaluation This code will generate volumes of transaction data over a large range of data char acteristics These transactions mimic the transactions in the real world retailing environment The size of the transaction is picked from a Poisson distribution with mean TI which is set to 5 in our Experiments In addition, the average size ofthe maximal potentially large item sets denoted by 111 is set to 2 The number of maximal potential large item sets denoted by LI is set to 2000 The number of items, denoted by IN is set to 1000 as default 4.2 Performance Study In the experiment for the real data S  0.6 and cy  2.5 and X varies from 0.4 to 1 where X is the damping factor Figure 6 shows the results of two clusters, cluster 1 for Re publicans and cluster 2 for Democrats It shows that these two results are similar to each other in the percentages of the issues in cluster 1 and cluster 2 Recall that an iteration is a refinement procedure from one clustering to the next clustering. Figure 7 shows the comparison of the execution time between algorithm SLR and algorithm Basic in each it eration It can be seen that although algorithm SLR has one more iteration than algorithm Basic, the execution time of algorithm SLR is much shorter than that of algorithm Basic in every iteration We next use the synthetic data mentioned above in the following experiments It is shown by Figure 8 that as the database size increases the execution time of algorithm Basic increases rapidly whereas that of algorithm SLR in 509 


                  p 0.5 d 0.4 0.1 I 2 3 15 6 7 8 9 101l1113111516 Cluster I ISSUCl a For Republicans 1 0.9 08 07 9 06 5 05 j 0.4 03 02 0.1 0 I2 3 J 5 6 7 8 910111213IJ1516 Clur1cr 2 ISSUE-l b For Democrats Figure 6 The percentage of the issues in clus ter 1 and cluster 2 creases linearly indicating the good scale-up feature of al gorithm SLR 5 Conclusion In view of the nature of clustering for market basket data we devised in this paper a novel measurement called the small-large ratio We have developed an efficient clustering algorithm for data items to minimize the SL ratio in each group The proposed algorithm is able to cluster the data items very efficiently This algorithm not only incurs an I 2 3 4 lterawn Figure 7 Execution time of algorithm SLR and algorithm Basic in each iteration 3mm Figure 8 Execution time of algorithm SLR and algorithm Basic as the number of transac tions ID1 varies execution time that is significantly smaller than that by prior work but also leads to the clustering results of very good quality Acknowledgments The authcas were supported in part by the Ministry of Education Project No 89-E-FA06-2-4-7 and the National Science Council Project No NSC 89-221 8-E-002-028 and NSC 89-22 1!3-E-002-028 Taiwan Republic of China References l UCI Machine Learning Repository http www. ics uci eddmleardMLRepository html 2 R Agranal and R Srikant Fast Algorithms for Mining As sociation Rules in Large Databases Proceedings ofthe 20th International Conference on Very Large Data Bases pages 478-499 September 1994 3 A G Buchner and M Mulvenna. Discovery Internet Market ing Intelligence through Online Analytical Web Usage Min ing ACM SIGMOD Record 27\(4 Dec 1998 4 M.-S Chen, J Han, and P S Yu Data Mining An Overview from a Database Perspective IEEE Transactions on Knowl edge and Data Engineering 8\(6 1996 5 A K Jain M N Murty and P J Flynn Data Clustering: a Review 4CMCornputingSurveys 31\(3 Sep 1999 6 D A Keim and A Hinneburg Clustering Techniques for the Large Data Sets  LFrom the Past to the Future Tutorial notes for ACM SIGKDD 1999 international conference on Knowl edge discovery and data mining pages 141-181 Aug. 1999 7 A Strehl and J Ghosh A Scalable Approach to Balanced High-dimensional Clustering of Market-baskets Proceed ings of the 7th International Conference on High Performance Computing December 2000 8 K Wang C Xu, and B Liu Clustering Transactions Using Large Items ACM CIKM International Conference on Infor mation and Knowledge Management pages 483490 Nov 1999 510 


         1 manufacture.html 2 samples.html Figure 8 Music Machine Web  Visits and Traversal Distribution  H Hofmann A Siebes and A W ilhelm V isualizing association rules with interactive mosaic plots In the sixth ACM SIGKDD international conference on Knowledge discovery and data mining  pages 227Ö235 August 2000  E H hsin Chi and S K Card Sensemaking of e v olving web sites using visualization spreadsheets In Proceedings of the Symposium on Information Visualization  pages 18Ö25 San Francisco California 1999  N Jok ob  The art of na vigating through hyperte xt Communications of the ACM  33\(3 1990  J Lamping R Rao and P  Pirolli A focus+conte xt technique based on hyperbolic geometry for visualizing large hierarchies In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems  pages 401Ö408 New York May 1995  B Liu W  Hsu and Y  Ma Pruning and summarizing the discovered associations In International Conference on Knowledge Discovery and Data Ming  pages 125Ö134 San Diego CA August 1999  Y  Ma B Liu and C K W ong W e b for data mining Or ganizing and interpreting the discovered rules using the web SIGKDD Explorations  2\(1 July 2000  Y  S Maarek M Jaco v e  M  Shtalhaim S Ur  and D Zernik Webcutter A system for dynamic and tailorable site mapping In Proceedings of WWW6 Conference  pages 713Ö722 Santa Clara California April 1997  N Minar and J Donath V isualizing the cro wds at a web site In Proceedings of CHIê99  1999  T  Munzner  Dra wing lar ge graphs with h3vie wer and site manager In Symposium on Graph Drwaing  pages 384Ö393 August 1998  J E Pitk o w and K A Bharat W e b viz a tool for w orldwide web access log analysis In Proceedings of First International WWW Conference  pages 271Ö277 May 1994  A W e x elblat History-based tools for na vigation In Proceedings of the Hawaii International Conference On System Science  IEEE Computer Society Press January 1999  A W e x elblat and P  Maes F ootprints History-rich tools for information foraging In Proceedings of CHIê99  pages 270Ö277 May 1999  G J W ills Niche w orks interacti v e visualization of v ery large graphs Journal of Computational and Graphical Statistics  8\(2 1999  T  Zheng Y  Niu and R Goebel W ebframe In pursuit of computationally and cognitively efìcient web minging In Proceedings of the 6th Paciìc-Asia Conference PAKDD  pages 264Ö275 May 2002 Proceedings of the IEEE/WIC International Conference on Web Intelligence \(WIê03 0-7695-1932-6/03 $17.00 © 2003 IEEE 


References 141 111 121 31 R Agrawd and R Srikant 223Fast Algorithms for Mining Association Rules,\224 Proc of Int\222l Conf on Very Large Data Bases pp.487-499 1994 R Srikant and R Agrawal 223hslining Generalized Asso ciation Rules,\224 Proc of Int\222l Conf on Very Large Data Bases pp.407-419 1995 N Megiddo and R Srikant 223Discovering Predictive As sociation Rules,\224 Proc ofthe 4th Int\222l Conf on Knowl edge Discovery an Databeses and Data Mining 1998 I51 161 E.H Han G Karypis and V Kuniar 223Scalable par allel data mining for association rules;\224 Proc of ACM SIGMOD Int\222l Conf pp.277-288 1997 T Shintani and M Kitsuregawa 223Iniplenientation of Parallel Mining Association Rules and their Evalua tion,\224 JSPP\22296 pp.97-104 June 1996 L Harada N Akaboshi K Ogihara and R Take 223Par allel Algorithm with Load Balancing for Mining Associ ation Rules,\224 IEZCE Trans on Info and Syst V-ol.J82 D-1 No.1 pp.70-81 January 1999 368 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


