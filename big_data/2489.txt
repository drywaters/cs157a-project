Mining Views Database Views for Data Mining Hendrik Blockeel t1 Toon Calders 2 Elisa Fromont 3 Bart Goethals T Adriana Prado T5 Katholieke Universiteit Leuven Belgium tLeiden Institute of Advanced Computer Science The Netherlands 1hendrik.blockeel,3elisa.fromont}@cs.kuleuven.be Technische Universiteit Eindhoven The Netherlands 2t.calders@tue.n1 t Universiteit Antwerpen Belgium 4bart.goethals 5adriana.prado}@ua.ac.be AbstractWe present a system towards the integration of data mining into relational databases To this end a relational database model is proposed based on the so called virtual mining views We show that several types of patterns and models over the data such as itemsets association rules and decision trees can be represented and 
queried using a unifying framework I MOTIVATION Data mining is not a one-shot activity but rather an iterative and interactive process During the whole discovery process typically many different data mining tasks are performed their results are combined and possibly used as input for other data mining tasks To support this knowledge discovery process there is a need for integrating data mining with data storage and management The concept of inductive databases IDB has been proposed as a means of achieving such integration 1 In an IDB one can not only query the data stored in the database but also the patterns that are implicitly present 
in these data The main advantages of integrating data mining into database systems are threefold first of all the data are mined where they are located in the database Hence the need for transforming data into an appropriate format is completely removed Second in database systems there is a clear separation between the logical and the physical level This separation shields the user from the physical details making the technology much more accessible for a nonspecialist Ideally the user of an inductive database should not be involved with selecting the best algorithms the parameter settings the storage format of the patterns etc but 
should instead be able to specify in a declarative way the patterns in which he or she is interested The third advantage of an IDB is the flexibility of ad-hoc querying That is the user can specify new types of constraints and query the patterns and models in combination with the data itself and so forth Notice that the functionality of an inductive database goes far beyond that of data mining suites such as e.g Weka 2 and Yale 3 These systems typically only share the first advantage of inductive databases by imposing one uniform data format for a group of algorithms In this work we focus 
our attention on determining how such an inductive database can be designed in practice II DESCRIPTION OF THE SYSTEM The system proposed in this paper builds upon our preliminary work in 4 5 In contrast to the numerous proposals for data mining query languages we propose to integrate data mining into database systems without extending the query language Instead we extend the database schema with new tables containing for instance association rules decision trees or other descriptive or predictive models As far as the user is concerned these tables contain all possible patterns trees and models that can be learned over the data Of course 
such tables would in most cases be huge Therefore these tables are in fact implemented as views called virtual mining views Whenever a query is formulated selecting for instance association rules from these tables a run of a data mining algorithm is triggered e.g Apriori 6 that computes the result of the query in exactly the same way that normal views in databases are only computed at query time and only to the extent necessary for answering the query The complete system is illustrated in Figure 1 When the user formulates his or her mining query the parser is invoked by 
the DBMS creating an equivalent relational algebra expression At this point the expression is processed by the Mining Extension which extracts from the query the constraints that can be pushed into the data mining algorithms The output of these algorithms is then materialized in the virtual mining views After the materialization the work-flow of the DBMS continues as usual and as a result the query is executed as if all patterns and models are stored in the database Observe that this system can possibly cover every mining techniques whose output can be completely stored in relational tables This approach also integrates constraint-based mining in 
a natural way Within a query one can impose conditions on the kind of patterns or models that one wants to find In many cases these constraints can be pushed into the mining process In 4 Calders et al present an algorithm that extracts from a query a set of constraints relevant for association rules to be pushed into the mining algorithm In this way not all possible patterns or models need to be generated but only those required to evaluate the query correctly as if all possible patterns or models were stored We have extended 978-1-4244-1837-4/08/$25.00 251 2008 IEEE 1608 ICDE 2008 


G U Fig 1 The integration of data mining into a DBMS Play Tennis Day  Outlook T      Fig 2 The PlayTennis data table and its corresponding supp its support and sz its size Other attributes such as x2 or any correlation Ii 1 Temp Humidity Wind Play DI  Yes measure could also be added to the view to describe the itemsets Similarly association rules can be represented by a view Rules\(rid cida cidc cid conf where rid is the rule identifier cida and cidc are the identifiers for the concepts representing the antecedent and the consequent of the rule respectively cid is the union disjunction of these and conf 1609 DBMS SQL Mining Query Query Output Strong No 10 11 0.5      Outlook I 11 40 2 2 3 Ml 4 Humidity MI overcast raijt strong we k Fig 3 Framework for an Inductive Database on that the Concepts table presented above is implemented as a virtual mining view In this context given a data table T and its corresponding virtual mining view T Concepts the virtual mining views for itemsets association rules and decision trees based on T Concepts are given in Figure 3 Although all virtual mining views are defined over T in this text we omit the prefix T when it is clear from the context Notice again that Figure 3 only gives one possible instantiation of the proposed approach As pointed out in Section II virtual mining views for other mining techniques can be added to the system The proposed framework can be described as follows 1 Itemsets and Association Rules The result of frequent itemset mining can therefore be represented by a view Sets\(cid supp sz For each itemset there is a tuple with cid the identifier of the itemset concept supp sz I Rain Cool Normal Weak Yes D6 Rain Cool Normal Strong No       PlayTennis Concepts cid 60 3 5 high nor al Treescharac Play treeid T acc sz 1 Concepts table this constraint extraction algorithm to extract constraints from queries over decision trees The user can refer to 7 for more details on the algorithm III FRAMEWORK REPRESENTATION Given a table T\(Al  An let Dom T  Dom Al x  x Dom\(An denote the domain of T We create a Concepts table T Concepts\(cid A1   An such that for every tuple t in T there exist 2n unique tuples t  t>n  in ConceptsT such that t'.Aj  t.Aj or t'.Aj  for all i C 1 2n andj C 1 n We denote the special value  as the wildcard value and assume it doesn't exist in the domain of any attribute As each of the concepts can actually cover more than one tuple in T a unique identifier cid is associated to each concept A tuple or concept cid,a an C T Concepts represents all tuples from Dom\(T satisfying the condition Ailai:A Ai  ai Figure 2 shows a data table for the classic PlayTennis example 8 together with a sample of its corresponding Concepts table A Representing models as sets of concepts In this section we explain how a variety of models can be represented using virtual mining views We assume from now Sets cid Day Outlook Temp Humidity Wind Play  1   Weak Yes 6 3  Overcast   Sunny Hot High Weak No D2 Sunny Hot High D3 Overcast Hot High Weak Yes D4 Rain Mild High Weak Yes D5  Sunny  High  No 2  Sunny  Normal  Yes 4  Rain   Strong No 5  Rain Trees Play treeid cid ml un-Ml 2 sunl ml 0.7 8     Rules cida cidc cid conf  8 


Weak D12 Sunny Cool High Strong  r r T1 se s C d d d d d d r cid  C.cid DS Outlook  C S Temp    Humidi Wind Fig 4 Prediction A select T treE D s Treesc L~re Dl.s2 D lect disti Dm Trees_E Treesch Concept ere T.cid C2 and id  T2.treeid and  Cl.cid and ook Sunny se eid   5 Kists Dm Trees_Play Tl Treescharac_Play DI Concepts Cl L~re Tl.cid  Cl.c and Tl.treeid  Dl.t and Dl.sz  5 and Cl.cid  C.cid and Cl.Temp   s S nd nd nd nd nd nd nd Fig 5 Example mining queries Figure 5 illustrates several mining queries that can be posed in our inductive database shown in Figures 2 and 3 Some constraints can be directly imposed using the tables Sets Rules and Treescharac as shown in queries A and B Query A asks for association rules having support of at least 30 confidence of at least 80 and size of at most 4 Query B selects decision trees having the attribute Play as the target attribute and having maximal accuracy among all possible decision trees of size  5 The user can also constrain the concepts by which the models are described For example query C asks for decision trees having a test on Outlook=Sunny and on Wind=Weak while query D asks for trees where the attribute Temp is never a wildcard 1610 i d   C C S.Wind  d treeid  E emp iumidit find reeid 1u lis the confidence of the rule Many other attributes such as lift conviction or gini index could be added to describe the rules 2 Decision trees We represent all decision trees that can be learned from T for one specific target attribute Ai by the view Trees Ai\(treeid cid A unique identifier treeid is associated with each decision tree and each of the decision trees is described using a set of concepts there is at least one concept describing one leaf If the user prefers to build a decision tree from only a subset of the attributes he should first create a view on the data table containing exactly those attributes such that the mining view Concepts associated with this view can then be used to describe the tree We added a view Treescharac Ai\(treeid acc sz to represent several characteristics of a decision tree learned for one specific target attribute Ai For every tree there is a tuple with a tree identifier treeid acc its accuracy and sz its size number of nodes Again other attributes could be added to describe the decision trees Figure 3 shows a decision tree built to predict the attribute Play using all other attributes in the data table and its representation in the mining view Trees using the first five concepts of the mining view Concepts from Figure 2 IV MODEL QUERYING In this section we give some concrete examples of common data mining tasks that can be expressed with SQL queries over the virtual mining views These examples support our claim that the virtual mining views provide an elegant way to incorporate data mining capacities to database systems without changing the query language A Prediction In order to classify a new example using one or more of the learned decision trees one simply looks up the concept that covers the new example More generally if we have a test set S all predictions of the examples in S are obtained by equi-joining S with the semantic representation of the decision tree given in the virtual mining view Concepts We join S to Concepts using a variant of the equi-join that requires that either the values are equal or there is a wildcard value Consider the PlayTennis example of Figure 2 Figure 4 illustrates a query that predicts the attribute Play for all unclassified examples in table Test Set considering all possible decision trees of size  5 B Constraints For itemsets and association rules we consider constraints such as minimal and maximal size minimal and maximal support minimal and maximal confidence plus the constraints that a certain item must be in the antecedent in the consequent of the rules and Boolean combinations of these For decision trees we consider constraints on size and accuracy In addition to these we also consider constraints posed on the concepts that describe the trees Next to these well-known constraints in our approach the user has also the ability to come up with new ad-hoc constraints Test Set Day  D Humidity  z Weak D8 Rain Hot High Strong D9 Overcast Hot High Weak D1O Overcast Mild High Weak DII Overcast Cool Normal C cid L~ct max E Dm Sunny Hot High S      S Rules R Concepts Concepts se  Treesch Trees_E Concept re T.cid and and and se and Cl.cid and C2 cid and S.supp and R.conf and S.sz C lect disti Dm Trees_E Trees_E Treesch d C rac_PlaG ay T Temp  C R.rid C1 c R.conf from Sets Outlook Temp Humidity Wind D7 


TABLE I EXECUTION TIMES FOR THE QUERIES IN FIGURE 5 A B C D lime s 0,60 Time s 4,35 1,36 4,59 Concepts 2397 Concepts 652 76 652 Rules 11525 Trees 439 33 439 output rows 11525 Output rows 31 2 36 7 value the attribute is present in every leaf of the tree Hence many well-known and common constraints can be expressed quite naturally in our model The declarative nature of the queries also improves the ability to extract and exploit constraints in the queries imposed by the user for making the underlying mining operations more efficient V IMPLEMENTATION The system was developed into PostgreSQL 9 written in C as follows When the user writes a query PostgreSQL generates a data structure representing its corresponding relational algebra expression After this data structure is generated our Mining Extension is called see Figure 1 Here we process the relational algebra structure extract the constraints trigger the data mining algorithms and materialize the results in the virtual mining views Just after the materialization the workflow of the DBMS continues and the query is executed as if the patterns or models were there all the time The system is currently linked to algorithms for association rule discovery and exhaustive decision tree learning 5 All the constraints listed in Section IV-B and represented as attributes of the mining views size accuracy support confidence can be extracted and efficiently exploited by the integrated data mining algorithms Some types of constraints however are currently not extracted by our implementation for instance the constraint maximal accuracy in query B others are extracted but cannot be exploited by the mining algorithms This may affect the efficiency of the system but not its correctness the remaining constraints can be used to filter the results afterwards A Experiments We now present a set of experiments which were conducted for the UCI dataset ZOO 10 with 101 examples for the SQL queries showed in Figure 5 For queries B C and D the target attribute was the attribute Class the tests Outlook=Sunny and Wind=Weak were replaced by Hair=true and Feathers=false respectively and the test Temp was changed to Feathers Our platform was an AMD ATLON 3.2 GHz processor with 1 GB of memory using Linux Table I presents the total execution times in seconds the number of intermediate generated concepts rules when applicable trees when applicable and the size of the output in rows for the example queries For query A the constraints supp>=30 conf>=80 and sz<=4 were all exploited by the system Observe that the number of rows in its output corresponds to the exact number of rules that were intermediately generated For query B the constraint sz<=5 was also exploited This was not the case of the constraint max\(accuracy however Yet this query was correctly computed The output consisted of 9 trees with 31 concepts in total Regarding queries C and D the constraints sz<=5 and acc>=70 were both exploited The constraints on the concepts that describe the decision trees are examples of constraints that were extracted by the system but not exploited by the data mining algorithms The results were nevertheless correctly computed As can be seen in table I the execution times of the queries are rather low which shows the usefulness and elegance of the proposed approach The execution times consist mainly of the time spent by the data mining algorithms plus the time for materializing the results VI DEMONSTRATION The demonstration will focus on showing how the system works on different datasets using a set of constraints such as those presented in Figure 5 Every time a data table T is created in the system all virtual mining views associated with T are automatically created and the user can immediately query for itemsets association rules or decision trees over T ACKNOWLEDGMENT Hendrik Blockeel is a post-doctoral fellow from the Research Foundation Flanders FWO-Vlaanderen This research was funded through K.U.Leuven GOA project 2003/8 Inductive Knowledge bases FWO project Foundations for inductive databases and the EU project Inductive Queries for Mining Patterns and Models REFERENCES 1 T Imielinski and H Mannila A database perspective on knowledge discovery Communications of the ACM vol 39 no 11 pp 58-64 1996 2 I H Witten and E Frank Data Mining Practical machine learning tools and techniques 2nd ed Morgan Kaufmann 2005 3 I Mierswa M Wurst R Klinkenberg M Scholz and T Euler Yale Rapid prototyping for complex data mining tasks in Proc 12th ACM SIGKDD Int Conf  1on Knowledge discovery and data mining ACM 2006 pp 935-940 4 T Calders B Goethals and A B Prado Integrating pattern mining in relational databases in Proc 10th European Conf on Principles and Practice of Knowledge Discovery in Databases PKDD Springer 2006 5 E Fromont H Blockeel and J Struyf Integrating decision tree learning into inductive databases in Knowledge Discovery in Inductive Databases KDID 5th International Workshop Revised Selected and Invited Papers S Dzeroski and J Struyf Eds 2007 6 R Agrawal and R Srikant Fast algorithms for mining association rules in Proc 20th Int Conf Very Large Data Bases VLDB J B Bocca M Jarke and C Zaniolo Eds Morgan Kaufmann 1994 pp 487-499 7 H Blockeel T Calders E Fromont B Goethals and A Prado Mining views Database views for data mining in ECML/PKDD2007 International Workshop on Constraint-Based Mining and Learning CMILE 2007 8 T M Mitchell Machine Learning New York McGraw-Hill 1997 9 Online Available http://www.postgresql.org 10 D N A Asuncion UCI machine learning repository 2007 Online Available http://www.ics.uci.edu mlearnll\\4LRepository.html 1611 11 r-/\\11j_ II T 


824 812 108 1135 189 M1 884 757 118 0 0 0 85.6 93.2 M2 809 50 654 2 122  0 80.8 100.4 M3 100 14 40 97 24 0 97.0 108.0 M4 1168 3 0 9 989  4 84.7 97.2 M5 212 0 0 0 0 185  87.3 89.2 Table 7   Estimation results of KBNN on test dataset   Predicted Mode Choice r i r a  M1 574 M2 540 M3 70 M4 793 M5 146 M1 589 528 37 0 0 0 89.7 97.3 M2 540 32 461 0 82 0 85.4 100.0 M3 67 0 0 66 57 0 98.5 104.0 M4 778 14 42 4 654 18 84.1 101.9 M5 142 0 0 0 0 128 90.2 102.8  5. Discussion and conclusions  With the capacity of learning driven by pattern recognition and effort correction mechanisms, the KBNN method based on the similarity between neural networks and decision tree, which combines the rule extraction and the accurate approximation of these two algorithms was used to construct commuter mode choice model for Shanghai, China. Its capacity for generalization and estimation performance was compared with the classical discrete choice model namely the nested logit model. The research results demonstrated that the KBNN model has fast convergence and high precision, which is of great importance for travel mode choice prediction. The model offers considerable advantages on predictive power and comparatively appeal in transferability from training dataset to test dataset over NL model. The proposed KNBB model performs with its structure flexibility to adapt to the training data and has the capability to produce explainable if-then rules through hyperplanes in the first hidden layer It is found in this research that a behavior rich teaching data is very important to construct and train a KBNN model. And one important property of neural networks models is their capability of processing missing data and noise. So further research should be carried out to find if the KBNN model is able to work well with noise and missing data  6. Acknowledgements  This research is funded by the national high technology research and development program of China \(2007AA11Z203\d National Natural Science Foundation of China \(50578094\The authors appreciate the Shanghai City Comprehensive Transportation Planning Institute for providing the data used in this study. The contents of the paper reflect the views of the authors who are responsible for the facts and accuracy of the information presented herein. The contents do not necessarily reflect the official views of the Antai Collge of Economics and Management Shanghai Jiao Tong University  References  1 M  C Fa dde n C on ditio na l L o g it A n a l y s is of Qua lita tiv e  Choice Behavior”, in Frontiers in Econometrics Academic Press, New York, 1973 2 L  C a o A Mode l f o r T r a v e l Mode Sw itc hing   N e w  Jersey Institute of Technology: New Jersey, 1998  H S a sch a   V  N R o b H S e rge et al  Ho m eAct i v i t y Approach to Multi-Modal Travel Choice Modeling the 85th Transportation Research Board Annual Meeting  Washington, D. C., 2006 4  M  E r i c  J  R  M a t t h e w  a n d A  C  J u a n   A  T o u r B a s e d  Model of Travel Mode Choice the 10th International Conference on Travel Behavior Research Lucerne, 2003 5 D  A  H e ns he r  a n d T  T  T on  A C o m p a r is on of the  Predictive Potential of Artificial Neural Networks and Nested Logit Models for Commuter Mode Choice Transportation Research Part E Elsevier, 2000, pp. 155-172  G  W e t s  K V a n h o o f  T  A r en t ze an d H T i mmer m an s  Identifying Decision Structures Underlying Activity Patterns: An Exploration of Data Mining Algorithms Transportation Research Record Transportation Research Board, Washington, D. C., 2000, pp. 1-9 7 J  C T h ill a n d A  W h e e l e r   T r e e I nduc tion of Spa tia l  Choice Behavior Transportation Research Record  Transportation Research Board, Washington, D. C., 2000, pp 250-258 8 T  Ya m a m o to, R. Kita m u ra a nd J. Fu jii D riv e r’s Route  Choice Behavior: Analysis by Data Mining Algorithms Transportation Research Board Transportation Research Board, Washington, D. C., 2002, pp. 59-66 9 M A bolf a z l a nd J   M. Er ic   N e s te d L o g it Mode ls a n d  Artificial Neural Networks for Predicting Household Automobile Choices: Comparison of Performance Transportation Research Record Transportation Research Board, Washington, D. C., 2002, pp. 92-1000 10 D  A  H e ns he r  a n d T  T  T on A C o m p a r is on of the  Predictive Potential of Artificial Neural Networks and Nested Logit Models for Commuter Mode Choice Transportation Research Part E Elsevier, 2000, pp. 152-172 1 T  A r en t ze H  T i mm er man s  P aram et ri c Act i o n  Decision Trees: Incorporating Continuous Attribute Variable 
495 


into Rule-Based Models of Discrete Choice Transportation Research Part B Elsevier, 2007, pp. 772-783 12  P. J  W e r bos T h e R o ots  of B a c k pr opa g a tion: Fr om  Ordered Derivatives to Neural Networks and Political Forecasting, Wiley, New York 1994 1  P  Lau r et  E  F o ck and R  N R a n d r i a n a r i von y   Bayasian Neural Network Approach to Short Time Load Forecasting Energy Conversion and Management Elsevier 2008, pp. 1156-1166 14 C.P L i m  J  H Le ong a nd M.-M. K u a n  A H y brid Neural System for Pattern Classification Tasks with Missing Features IEEE Transactions on Pattern Analysis and Machine Intelligence 2005, pp. 648-653 15 K  Pola t, a n d S  G une s A Nov e l H y brid Inte llig e n t Method Based on C4.5 Decision Tree Classifier and OneAgainst-All Approach for Multi-Class Classification Problems Expert Systems with Applications Elsevier, 2008 16 G  Se rpe n D  K  T e k k e dil, a nd M  O rra  A K now le dge Based Artificial Neural Network Classifier for  Pulmonary Embolism Diagnosis Computers in Biology and Medicine  Elsevier, 2008, pp. 204-220 17 G  G  T o w e ll, a nd J  W  Sha v lik  K now le dg e B a se d Artificial Neural Networks Artificial Intelligence Elsevier 1994, pp. 119-165 18 Y  Y a o, Y   V Ve nk a t e s h, a n d C  C  K o   A K now le dg e Based Neural Network for Fusing Edge Maps of MultiSensor Images Information Fusion 2001, pp. 121-133 
496 


7 5769 l00 97.67 l005 I00 Avg Accuracy ______ __95.59 95.98 89.5 8954 BSTC/RCBT To keep comparisons fair we ran SVM and randomForest on the same genes selected by our entropy discretization except with their original undiscretized gene expression values SVM was run with its default radial kernel We ran randomForest 10 times with its default 500 trees for ALL LC and OC and its accuracy was constant For PC we had to increase randomForest's number of trees to 1000 before its accuracy stabilized over the 10 runs Table III contains the number of class 0/1 samples in the clinically determined training set the number of genes selected by our entropy discretization and our experimental results As shown in this table the overall average accuracies of BSTC and RCBT are again best at about 96 each When compared against RCBT SVM and randomForest on the individual tests we can see that BSTC is alone in having 100 accuracy on the majority of datasets However BSTC's performance on the preliminary AML/ALL dataset test is relatively poor This is likely due to over fitting Every error BSTC made mistook a class 0 AML test sample for a class 1 ALL test sample i.e all errors were made in this same direction And the ALL training data has both i about 2.5 times as many class 1 samples as class 0 samples and ii a small number of total samples/genes When the training set is more balanced and the number of samples/genes is larger we can expect that cancellation of errors will tend to neutralize/balance any over fitting effects in BSTC And BSTC is a method meant primarily for large training sets where CAR-mining is prohibitively expensive As we will see below in Section V-B.1 BSTC s performance is much better for larger AML/ALL training set sizes B Holdout Validation Studies Holdout validation studies make comparisons less susceptible to the choice of a single training dataset and provide performance evaluations that are likely to better represent program behavior in practice We next present results from a thorough holdout validation study completed using 100 different training/test sets from each of the ALL LC PC and OC data sets For these holdout validation tests we benchmark BSTC against Top-k/RCBT because i BSTC/RCBT perform hest in our preliminary experiments ii Top-k/RCBT is the fastest/most accurate CAR-based classifier for microarray data and iii we are interested in BSTC's CAR-related vs Topk/RCBT s CAR based scalability For the holdout validation study we generated training sets of sizes 40 60 and 80 of the total samples Each training set was produced by randomly selecting samples from the original combined dataset We then used the standard R dprep package's entropy minimized partition 17 to discretize the selected training samples Finally the remaining dataset samples were used for testing the two classifiers after rule/BST generation on the randomly selected training data For each training set size we produced 25 independent tests In addition to these training sets we created an additional 25 1-x/0-y tests To create these tests we chose training data by randomly selecting x class 1 samples and y class 0 samples to be used as training data As before the remaining samples were then used to test both classifiers For each dataset the x and y values are chosen so that the resulting 25 classification tests have the exact same training/test data proportions as the single related dataset test reported in section V-A For each training set size we plot our results using a boxplot Boxplot Interpretation Each boxplot that we show in this section can be interpreted as follows The median of the measurements is shown as a diamond and a box with boundaries is drawn at the first and the third quartile The range between these two quartiles is called the inter-quartile range IQR Vertical lines a.k.a whiskers are drawn from the box to indicate the minimum and the maximum value unless outliers are present If outliers are presents the whiskers only extend to 1 5 x IRQ The outliers that are near i.e within 3 x IRQ are drawn as an empty circle and further outliers are drawn using an asterisk 1 ALIJAML ALL Experiment Figure 4 shows the classification accuracy for the ALL/AML dataset As can be seen in this figure BSTC and RCBT have similar accuracy across the ALL/AML tests as a whole BSTC outperforms RCBT in terms of median and mean accuracy on the 40 and 80 training set sizes while RCBT has better median/mean accuracy on the 1-27/0-11 training size tests And both classifiers have the same median on the 60 training set size Over the 100 ALL/AML tests we see that BSTC has a mean accuracy of 92.13 while RCBT has a mean accuracy of 91.39 they are very close It's noteworthy that BSTC is 100 accurate on the majority of 80 training size tests However BSTC appears to have slightly higher variance than RCBT on all but the 40 training tests Considering all the results together both BSTC and RCBT have essentially equivalent classification accuracies on the ALL/AML dataset 2 Lung Cancer LC Experiment The results for the Lung Cancer dataset are reported in Figure 5 Here again both BSTC and RCBT have similar classification behavior RCBT has higher mean and median accuracies on the 40 and 60 tests while BSTC outperforms RCBT on the 1-16/0-16 tests Meanwhile both classifier have the same median on the 80 training test Over all 100 LC tests we find that BSTC has a mean accuracy of 96 32 while RCBT has a mean accuracy of 97.08 again they are very close As before BSTC is alone in having 100 accuracy more 1068 TABLE III USINC GIVEN RES Ul TS TRAINING DATA  Class I  Class 0 Genes random Training Training After BSTC RCBT SVM Forest Dataset Samnples Samnples Discr Ac or Accr Ac o Accuracy ALL 27 11 866 82.35 91 18 91 18 85.29 LC 16 16 2173 100 97.99 93.29 99.33 PC 52 50 1554 100 97.06 73.53 73.53 OC 133 


TABLE IV AVERACGE RUN TIMLES FOR THE PC TESTS IN SEC-NI t INDICATES nl WAS LOWERED TO 2 Training Median  Mean 260 Near outliers  25/25t T O Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 80 Training 1 27/0-11 Training 1.0 1.0 1 0]0.95 0.9 0.9 0.7 0.7 0.850.8 0.80.80.80.750.7 0.70.70.7 650.6 0.60.60.6Holdout Validation Results 25/25t cJ CZ C O Far outliers 40 Training 60 Training 80 Training 1-16/0-16 Training h0 1.0 T 1.0 1.00.95 0.9 0.9 0 0.8 0.8 0.80.80.8LC Holdout Validation Results THE PC TESTS THAT RCBT FINISHED Training BSTC RCBT 40U 75.08 79.27 60 78.18 85.45 80 84.98 1-52/0 50 81.65%o 1069 cJ CZ C 5.06 120.63 21.32 RCBT  7110  7200  7200  RCBT DNF 0 P 00 24/25 t per training set test Finally the  RCBT DNF column gives the number of tests RCBT was unable to finish in  the cutoff time over the number of tests for which Top-K finished mining rule group upper bounds Explanation for varying nd values Run time cutoffs were necessary to mitigate excessive holdout validation CARmining times Even with a cutoff of 2 hours these 100 PC experiments required about 11 days of computation time with most experiments not finishing For the 80 and 1-52/0-50 training set sizes RCBT with nl  20 failed to finish lower bound rule mining for all 50 tests within 2 hours Thus RCBT's nl parameter was lowered from the default value of 20 to 2 in an attempt to improve its chances of completing tests Not surprisingly decreasing nl i.e mining fewer lower bound rules per Top-k rule group decreases RCBT s runtime However RCBT was still unable to finish lower bound rule mining for any tests Classification Accuracy Figure 6 contains accuracy results for BSTC on all four Prostate Cancer test sets Prostate Cancer boxplots for RCBT weren't constructed for training set sizes that RCBT was unable to complete all 25 tests within the time cutoffs In contrast BSTC was able to complete each of the 100 PC classification tests in less than 6 seconds Table V contains mean accuracies for the PC dataset with 40 60 80 and 1-52/0-50 training For each training set the average accuracies were taken over the tests RCBT was able to complete within the cutoff time Hence the 40 row means were taken over all 25 results Since RCBT was unable to complete any 80 or 1-52/0-50 training size tests we report these BSTC means over all 25 tests RCBT has slightly better accuracy then BSTC on 40 training For 60 training TABLE V MEAN AcCURACIES FOR 40 60 80 1.52/0.50 BSTC 3 4.93 5.78 5.57 Top0.09 BSTC F a RCBT BSTC RCBT BSTC RCBT b c Fig 5 BSTC RCBT BSTC RCBT b c ALL BSTC RCBT a Fig 4 BSTC RCBT d then half the time for any training set size see Figure 5 d However RCBT has smaller variance for 3 of the 4 training set sizes Therefore as for the ALL/AML data set both BSTC and RCBT have about the same classification accuracy on LC 3 Prostate Cancer PC Experiment RCBT begins to run ilnto a comiputational difficulties on PC's larger training set sizes This is because before using a Top-k rule group for classification RCBT must first mine nt lower bound rules for the rule group RCBT accomplishes rule group lower bound mining via a pruned breadth-first search on the subset space of the rule group's upper bound antecedent genes This breadthfirst search can be quite time consuming In the case of the Prostate Cancer PC dataset all 100 classification tests 25 tests for each of the 4 training set sizes generated at least one top10 rule group upper bound with more than 400 antecedent genes Due to the difficulties involved with a breadth-first search over the subset space of a several hundred element set RCBT began suffering from long run times on many PC classification tests Table IV contains four average classification test run times in seconds for each PC training size The BSTC column run times reflect the average time required to build both class 0 and class I BSTs and then use them to classify all the test samples Each Top-k column run time is the average time required for Top-k to mine the top 10 covering rule groups with minimum support 0.7 for each training set Table IV's RCBT column gives average run times for RCBT using a time cutoff value of 2 hours for all the training sets For each classification test if RCBT was unable to complete the test in less than the cutoff time it was terminated and it s run time was reported as the cutoff time Hence the BSTC RCBT d RCBT column gives lower bounds on RCBT s average run time 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


