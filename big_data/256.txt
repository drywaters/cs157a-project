TRACKING MULTIPLE CLOSELY SPACED TARGETS USING AN ADAPTIVE FOVEAL SENSOR Fengjun Xi and Darryl Morrell Department of Electrical Engineering Arizona State University Fengjun.Xi@asu.edu morrell@asu.edu ABSTRACT We address the problem of con  guring a foveal sensor to track multiple closely spaced moving targets The foveal sensor has a high acuity region whose center and extent can be con  gured surrounded by a low acuity region We study three heuristic approaches to extend a near-optimal greedy con  guration rule for a single target to multiple targets simultaneously observe all targets SO center the 
foveal region on each target in turn TO and center the foveal region on the target with the worst position estimate WO The target tracker is implemented using a particle  lter with joint probabilistic data association JPDA Additionally we implement two different independent-partition proposal distributions using JPDA and global nearest neighbor GNN Monte Carlo simulations show that the WO rule outperforms the other rules and that the IP-JPDA proposal gives better tracking performance I INTRODUCTION Tracking multiple closely-spaced targets using an attentive sensor in heavy clutter is a challenging problem Previous 
work 1 2 d e v eloped a ttenti v e s ens o r c ontrol s t rate gies to track single target In this paper we investigate strategies to track multiple targets moving in one dimension with a foveal sensor The foveal sensor has a high acuity region whose center and extent can be con  gured surrounded by a low acuity region target positions within the high acuity foveal region are observed more accurately The control strategies are obtained by ext ending a near-optimal greedy algorithm from single target to multiple targets using three approaches simultaneously observe all targets SO cen 
ter the foveal region on each target in turn TO and center the foveal region on the target with the worst position estimate WO The target tracker is implemented using a joint multi-target probability density JMPD particle  lter with joint probabilistic data association JPDA Our simulation results show the best performance is obtained by the WO rule This work supported by AFOSR under grant F49620-03-1-0117 A second contribution of this paper is a novel independent partition IP proposal scheme for the particle  lter Previous work 3 4 d e v eloped t he independent partition method to reduce the number of particles and hence the 
computational burden necessary to implement the particle  lter That work used sensor arrays and uni  ed target tracking i.e tracking with sensor models that do not require data association We have adapted the IP proposal method to include data association using two different methods GNN and JPDA IP-JPDA provides better performance in the presence of clutter The rest of this paper is organized as follows Section II describes the target dynamics and observation models Section III brie  y outlines the tracker algorithm We present three foveal sensor con  guration algorithms in Section IV 
and introduce the IP-JPDA and IP-GNN proposal schemes in Section V Section VI compares the three con  guration algorithms as well as IP-GNN and IP-JPDA through Monte Carlo simulations Conclusions are given in Section VII II TARGET DYNAMICS AND SENSOR MODELS We consider T max targets moving in one dimension Let x t k denote the state vector of target t at discrete time k  x t k   X t k  X t k  
  where X t k is position and  X t k is velocity The state vector for a given target is also called a partition We use a constant velocity target motion model x t k   1 t 01  x t k  1  n t k  1  where  t is the time difference between measurements and n t k  1 is a zero-mean Gaussian process with covariance 
Q  The multitarget state vector for T max targets is composed of the individual partitions x k   x 1 k   x 2 k   x T max k     V - 941 0-7803-8874-7/05/$20.00 2005 IEEE ICASSP 2005 


The foveal sensor provides observations of the target position corrupted by noise The sensor has two adjustable parameters d k sets the location of the foveal region while c k sets the gain at the center of the foveal region The observation for target t is obtained from its position as z k arctan  c k  X t k  d k    v t k  where v t k is white Gaussian noise with variance R  Once con  gured the foveal sensor provides measurements of target positions it detects each target with a known probability P D  The measurement vector at time k is z k   z 1 k z 2 k z M k k    Its elements consist of observed target positions and clutter We model clutter as uniformly distributed over the surveillance region volume V  the number of false alarms is Poisson distributed with parameter  V  where  is the spatial false alarm density and is known to the tracker III TRACKER ALGORITHM The nonlinearity of the foveal sensor observation model necessitates the use of a particle  lter in this application We use Monte Carlo joint probabilistic data association 5 an d an independent partition proposal distribution described in Section V to estimate the target state and provide information necessary to con  gure the foveal sensor The posterior distribution of x k given z 1 through z k is approximated by T max sets of N part particles  x i k  N part i 1 and associated weights   i k  N part i 1  Table 1 outlines our tracking algorithm IV SENSOR CONFIGURATION We improved the performance of the single-target con  guration rule in 1 by e x amining t he characteris tics of the near-optimal solution obtained using SPSA 7 f o r R  0  0  4  The foveal center d is positioned at the predicted target position The foveal gain c is set to c     P    1  8log 10 R   0  4  R  0  0146  3  30  P   0  R 0  0146 1 where P  is the predicted position error variance From this single target rule we have developed three foveal sensor con  guration rules for multi-target tracking SO is designed to observe all targets at the same time In order to let all targets fall within the high acuity foveal region d is set to the point midway between the two most widely separated targets say i and j  Denoting the distance between targets i and j as D ij  we set the gain as c  012     D ij 0  9  P  i  q P  j    log 10 R   0  4  R  0  0146   D ij 1  65  P  i  q P  j   0  R 0  0146 Ta bl e 1  Multiple Target Tracking Algorithm 1 Generate  x i 0  N part i 1 set   i 0  N part i 1  1 N part  2 Compute  x 0  015 N part i 1  i 0 x i 0 and  P 0  015 N part i 1  i 0  x i 0   x 0   x i 0   x 0  T  3 Set k 1  4 Compute the predicted state estimate  x  k  F  x k  1 and predicted error covariance P  k  F  P k  1 F   Q 5 Con  gure d k and c k as in Section IV 6 Obtain z k using the con  gured sensor 7 For each target t  propose a set of partitions  x t,i k  N part i 1 and associated bias terms  b t,i k  N part i 1 via the IP-GNN or IP-JPDA algorithm of Section V concatenate all partitions to form particles x i k   x 1 i k   x 2 i k   x T max i k     8 For i 1 N part   i k   i k  1 015 N H l 1 p  z k  l k  x i k   T max t 1 b t,i k  where   l k  N H l 1 is the set of feasible associations of observations to targets N H is the total number of feasible hypotheses and p  z k  l k  x i k  is given in   9 For i 1 N part  normalize weights  i k   i k 015 N part i 1  i k  10 Permute particles by the K-means algorithm[4  11 Compute the state estimate  x k  015 N part i 1  i k x i k  error covariance  P k  015 N part i 1  i k  x i k   x k   x i k   x k    12 Calculate  N ef f  1 P N p art i 1   i k  2 and perform resampling if  N ef f N T  13 Set k  k 1 andgotostep4 where P  i and P  j are the predicted position error variances of targets i and j  In TO the foveal region is centered on each target in turn while the gain is computed using 1 with P  of the centered target In WO the foveal region is centered on the target that has the largest predicted position error variance and the gain is also computed using 1 with P  of the centered target V INDEPENDENT PARTITION PROPOSALS The independent partition approach proposes a collection of particles for each partition  x t,i k   N part i 1  weights  w t,i k  N part i 1  which depend on the observation z k arecomV - 942 


 0 10 20 30 0 1 2 3 4 5 6   time index, k ASE of target position estimate of converged tracks IP JPDA t1\(SO t2\(SO t3\(SO t1\(TO t2\(TO t3\(TO t1\(WO t2\(WO t3\(WO   0 10 20 30 0 1 2 3 4 5 6   time index, k ASE of target position estimate of converged tracks KP t1\(SO t2\(SO t3\(SO t1\(TO t2\(TO t3\(TO t1\(WO t2\(WO t3\(WO   Fig 1  Average squared position error ASE for three targets t1 t2 and t3 for the SO TO and WO heuristics with no clutter puted for each partition Computation of the weight requires that the elements of z k be associated with each partition The proposed particles for each partition are resampled according to these weights to obtain a collection  x t,i k  N part i 1  These partitions are arranged into particles x i k   x 1 i k   x 2 i k   x T max i k     We investigate two association approaches to implement IP proposal distributions global nearest neighbor IP-GNN and joint probabilistic data association IP-JPDA The GNN method makes one-to-one assignments between the elements of z k and the partitions x t,i k of particle x i k to minimize the total distance between measurements and predicted measurements 6 The s et of as s i gnments is then used to compute the weight assigned to each partition The JPDA method computes weights for a given partition x t,i k by applying JPDA as if it were the only target present treating observations from other targets as false alarms This requires enumerating all association hypotheses between x t,i k and the elements of z k  we denote the set of possible hypothesis as   t,l k  N H part l 1  Here N H part is the total number of association hypotheses for the partition x t,i k  Table 2 shows the details of the IP-JPDA proposal scheme VI SIMULATION RESULTS We evaluated performance by examining the percentage of Monte Carlo runs in which the tracker converges i.e position errors for all targets remain below a given threshold during the last ten time steps of each run and for converged runs the average squared error ASE in the estimated posiTa bl e 2  IP-JPDA Subroutine for Target t 1 For each particle i 1 N part  a Sample x t,i k   p  x k  x t,i k  1  b For each  t,l k  compute p  z k  t,l k  x t,i k   c Compute w t,i k  015 N H part l 1 p  z k  t,l k  x t,i k   2 Normalize  w t,i k  N part i 1 to sum to 1 3 For each particle i 1 N part  sample an index j according to the distribution  w t,i k  N part i 1 set x t,i k  x t,j k  and b t,i k  w t,j k  0 10 20 30 0 1 2 3 4 5 6   time index, k ASE of target position estimate of converged tracks IP JPDA t1\(SO t2\(SO t3\(SO t1\(TO t2\(TO t3\(TO t1\(WO t2\(WO t3\(WO   0 10 20 30 0 1 2 3 4 5 6 KP time index, k ASE of target position estimate of converged tracks   t1\(SO t2\(SO t3\(SO t1\(TO t2\(TO t3\(TO t1\(WO t2\(WO t3\(WO   Fig 2  ASE for the SO TO and WO heuristics in clutter tion of each target All evaluations used 1000 Monte Carlo simulation runs with q 0  01  R 0  05   t 1 and N part  400  The initial particles were sampled from a Gaussian distribution whose mean is the true targets state x 0 and whose covariance is P 0  diag\(1  0  1  We compared the performance of the SO TO and WO approaches for both IP-JPDA and kinematic prior KP proposals for no clutter  P D 1   V 0  and clutter  P D 0  9   V 0  5  Table 3 and Fig 1 show percent convergence and ASE as a function of time for the SO TO and WO heuristics tracking thr ee targets in no clutter Table 4 and Fig 2 show the corresponding performance with clutter The performance of IP-JPDA is always better than that of KP and WO and TO with IP-JPDA proposal perform better than SO Also WO appears to be generally slightly better than TO in the presence of clutter We also compared IP-JPDA and IP-GNN Table 5 and Fig 3 show that IP-JPDA has the same performance tracking 2 targets as IP-GNN when P D 1 and P FA 0  while IP-JPDA has better performance than IP-GNN when V - 943 


Ta bl e 3  Percentage of runs that converged with no clutter Proposal method SO TO WO IP-JPDA 90.5 99.5 99.7 KP 88.5 97.1 96.4 Ta bl e 4  Percentage of runs that converged in clutter Proposal method SO TO WO IP-JPDA 81.1 90.7 92.5 KP 80.6 75.8 83.1 P D 0  9 and  V 0  5  This is because IP-JPDA can counteract the negative in  uence of false measurement-totarget assignments To further test the robustness of IPJPDA we applied it to two different scenarios in clutter in which three moving targets have different initial positions and velocities Fig 4 shows the true and estimated trajectories the IP-JPDA proposal scheme works for both cases VII CONCLUSIONS In this paper we introduce an attentive tracker for multiple closely spaced targets using an adaptive foveal sensor Three foveal sensor con  guration rules are studied and compared by Monte Carlo simulations The WO rule generally outperforms the TO and SO rules for either IP-JPDA or KP proposals In addition both IP-JPDA and IP-GNN proposal approaches are presented and investigated Monte Carlo simulations show better performance is obtained by IP-JPDA than by IP-GNN in the presence of clutter VIII REFERENCES  Y  Xue a nd D Morrel l   A dapt i v e F o v eal S e ns or for Target Tracking 36th Asilomar Conf on Sig Sys and Comp pp 848-852 Nov 2002  H S h ah and D  M orrel l   A n A dapt i v e Z oom Al gori t h m For Tracking Targets Using Pan-Tilt-Zoom Cameras ICASSP04 May 2004  M Orton a nd W  F itzgerald  A B ayes ian Approach to Tracking Multiple Targets Using Sensor Arrays and Particle Filters IEEE Tran Sig Proc vol 50 no 2 pp 216-223 Feb 2002  C  K reucher  K Kastella A O Hero  T r acking Multiple Targets Using a Particle Filter Representation of the Ta bl e 5  Percentage of runs that converged using IP-JPDA and IP-GNN Proposal method no clutter clutter IP-JPDA 100 98..9 IP-GNN 100 84.1  0 10 20 30 0 0.5 1 1.5 2 2.5 3 time index, k ASE of target position estimate of converged tracks P D 1  V 0 t1\(IP GNN t2\(IP GNN t1\(IP JPDA t2\(IP JPDA 0 10 20 30 0 0.5 1 1.5 2 2.5 3 time index, k ASE of target position estimate of converged tracks P D 0.9  V 0.5 t1\(IP GNN t2\(IP GNN t1\(IP JPDA t2\(IP JPDA Fig 3  Comparison between IP-JPDA and IP-GNN proposal schemes for clutter and no clutter 0 10 20 30 10 0 10 20 30 40 50 time index, k True and estimated target position t1\(True t2\(True t3\(True t1\(Estimated t2\(Estimated t3\(Estimated 0 10 20 30 5 0 5 10 15 20 25 30 35 40 45 50 time index, k True and estimated target position t1\(True t2\(True t3\(True t1\(Estimated t2\(Estimated t3\(Estimated Fig 4  True and estimated trajectories using IP-JPDA Joint Multitarget Probability Density SPIE Intl Symposium on Opt Sci and Tech San Diego CA Aug 2003  R  Karls s on and F  G us tafs s on Monte C arlo data as s o ciation for multiple target tracking Proc of IEE Target Tracking Algorithms and Applications vol 1 pp 13/1-13/5 Oct 2001  S  B l ackman and R  P opol i  Design and Analysis of Modern Tracking Systems Artech House 1999  J  C  S p al l   A n O v e rvi e w of t h e S i m ul t a neous P e r turbation Method for Ef  cient Optimalization Johns Hopkins APL Technical Digest vol 19 No 4 1998 V - 944 


structure. Every node in the tree represents the a n t e c e d e n t  i t e m  s e t  o f  t h e  r u l e  I  C   W h e n e v e r  w e  create a new node, n, in the tree and label it with an item set, I, we apply the pruning conditions listed next to check whether we can terminate the processing at n pre-maturely thus reducing the space. By terminating a node, we stop further processing at that node and no longer list any item sets under it. We exploit the following three pruning conditions   Z e r o c o n f i d e n c e  p r u n i n g   I f  t h e  c o n f i d e n c e  I  C     0   t h e n  t e r m i n a t e  n  Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE   O n e s u p p o r t  p r u n i n g   I f  t h e  s u p p o r t   I  C     1   then terminate n   M i n i m a l i t y  p r u n i n g   I f  t h e  c o n f i d e n c e   I  C    g t    minconf, then terminate n Zero-confidence pruning is a simple and straight forward pruning condition based on the observation that if the support of a certain item set, I, is zero then every superset of I has a zero support; as a result, any rule formed by replacing the antecedent of a rule having a zero confidence by a superset of that antecedent will also have a zero confidence. To justify t h a t    s u p p o s e  a  r u l e  I  C  h a s  a  s u p p o r t  o f  z e r o   i  e   s u p p o r t  I  U  C     0    a s  a  r e s u l t   t h e  c o n f i d e n c e  o f  I  C is also zero   r e c a l l  t h a t  c o n f i d e n c e  I  C     s u p p o r t  I  C     s u p p o r t  I    S u p p o s e  t h a t  I  is a superset of I; we know that support of \(I  U C I  U C I U C I   C   i s  z e r o  a n d  t h u s  c o n f i d e n c e  o f  I   C  i s  a l s o  z e r o   I n  s h o r t   i f  w e  k n o w  t h a t  a  r u l e  I   C  h a s  a  z e r o  confidence, then there is no need to process the supersets, I  of I and calculate the confidence values of the rules I   C  Rules with support equal to 1 are based on a single sample from the database and thus are not rules per se N o t e  t h a t  i f  t h e  s u p p o r t  o f  a n y  r u l e  I  C  i s  1   t h e n  a n y  non-minimal rule formed from this rule can not have a support greater than 1. In practice we can easily combine the previous two pruning conditions by pruning a node if its support is less than or equal to 1 this is because the confidence of a rule is equal to zero if and only if its support is equal to zero. Hereinafter we shall refer to those two pruning conditions combined as the support-less-than-two pruning The minimality pruning condition is based on our previous argument presented in section 2.3 for the need to remove non-minimal rules. Once we generate a rule R   I  C  w i t h  c o n f i d e n c e  v a l u e  e x c e e d i n g  m i n c o n f   w e  terminate the node, n, that generated R because all successor nodes of n may only generate non-minimal rules of R according to our definition of minimality As noted earlier, other pruning conditions that pertain to special situations can be easily added to our set of conditions and tested in a similar manner. Each pruning condition when satisfied results in node termination Our algorithm employs the two pruning conditions support-less-than-two pruning and minimality pruning to prune the SE tree. Every time we generate a new item set, I, at some node, n, three scenarios may transpire   S u p p o r t  o f   I  C   i s  l e s s  t h a n  2   w e  t e r m i n a t e  n   C o n f i d e n c e  o f   I  C   i s  i n  t h e  r a n g e   0   m i n c o n f  exclusively; we continue processing without terminating n   C o n f i d e n c e  o f   I  C   i s  g r e a t e r  o r  e q u a l  t o  m i n c o n f   w e  t e r m i n a t e  n  a n d  o u t p u t  I  C   a s  a  r u l e  The formal algorithm is depicted in Figure 2. Note 


that our algorithm is complete in terms of producing all confident minimal rules. We omit the proof due to space limitations Figure 2. The formal algorithm 4.2 Ensuring rule minimality Suppose we have a set of items {1, 2, 3  11} with 11 being the fixed consequent. Suppose further that we are currently working with node 1 and that to the right of 1 we have two un-terminated nodes, 9 and 8,9. Both nodes will be considered with 1 because node 8,9 falls under node 8 in the tree and is thus independent of the result of the join of 1 and 9. A possible scenario would b e  f o r  b o t h  r u l e s   1  9  1 1  a n d  1  8  9  1 1   t o  s u c c e e d  thus we end up generating a non-minimal rule 1  8  9  1 1  i n  t h i s  c a s e    Another scenario we need to consider is when rule 9  1 1  h a s  s u p p o r t  0  o r  1   t h e n  t h e r e  i s  n o  n e e d  t o  t r y  r u l e  1  8  9  1 1  b e c a u s e  i t s  s u p p o r t  i s  a l s o  g o i n g  t o  b e  0  Input: an ordered list of items, minconf, and the consequent item set Output: set of minimal rules satisfying minconf Algorithm   C r e a t e  t h e  r o o t  n o d e   t h e  e m p t y  s e t     F o r  e v e r y  i t e m   I   i n  o u r  o r d e r e d  l i s t  o f  i t e m s   d o  the following o Generate a new node, n o Insert n under the root from right to left o Label n with I o Depending on the resulting confidence and s u p p o r t  o f   I  C    p r o c e e d  w i t h  o n e  o f  t h e  following   I f  s u p p o r t  o f   I  C   i s  l e s s  t h a n  2  t h e n  terminate n   I f  c o n f i d e n c e  o f   I  C   i s  g r e a t e r  o r  e q u a l  to minconf then terminate n and output I  C   a s  a  r u l e    I f  c o n f i d e n c e  o f   I  C   i s  i n  t h e  r a n g e   0   minconf terminated node, Unter-n, to the right n working from highest level in the tree down, and at each level processing nodes right to left   G e n e r a t e  a  n e w  n o d e   n n     I n s e r t  n n  u n d e r  n  f r o m  r i g h t  t o  l e f t      L a b e l  n n  w i t h  t h e  l a b e l  o f  U n t e r n   Note that the item set represented by nn is the union of the items in n and Untern   R e c u r s i v e l y  r e p e a t  t h e  c o n f i d e n c e  testing process for nn Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE or 1. In this scenario, our algorithm will not produce undesired rules but will test for rules whose outcome is known beforehand which undesirable because this involves the time-consuming process of scanning a large database to compute the support and confidence of the rule  in cases where a non-vertical database layout is adopted To rectify this problem, we associate a temporary taboo list \(TL the nodes that are pruned under I. For example, if 1  9  1 1  i s  n o t  a  c o n f i d e n t  r u l e  o r  i t s  s u p p o r t  i s  l e s s  than 2, then we append 9 to 1  s taboo list \(TL1 for short r u l e  u n d e r  n o d e  1   1  X  1 1   w e  c h e c k  i f  a n y  s u b s e t  o f  X is in TL1. If so, we directly terminate X under 1 and add X to TL1. This may seem unfeasible at first however, an efficient implementation has been devised 


as we shall discuss in more details in section 5.2 5. Implementation &amp; performance analysis 5.1.  The P-tree technology2 Our implementation for this work is based on an efficient data structure, the P-tree \(Predicate or Peano tree relational data in a loss-less compressed column-wise format by splitting each attribute into bits, grouping bits at each bit position, and representing each bit group by a P-tree. To create P-trees from transactional binary data, we store all bit values in each binary attribute for all the transactions separately. In other words, we group all bits, for all transactions t in the table, in each binary attribute, separately. Each such group of bits is called a bit group. Figure 3 shows the process of creating P-trees from a binary relational table. Part b groups, one for each attribute in a show the resulting P-trees, P1 and P2, one for each of the bit groups in b recursively partitioning the bit groups into halves Each P-tree will record the total number of 1s in the corresponding bit group on the root level. The second level in the tree gives the number of 1s in each of the halves of the bit group. The first node from the left on the second level will give the number of 1s in the first half of the bit group; similarly, the second node will give the number of 1s in the second half of the bit group. This logic is continued throughout the tree with each node recording the number of 1s in either the first or the second half \(depending on whether it is the left or right node 2 Patents are pending for the P-tree technology parent node. In practice, we do not directly store the number of 1s, but a 1 if the node contains only 1s and a 0 otherwise. Nodes containing only 1s or only 0s are considered pure \(otherwise they are mixed partitioned further  called pure-1 and pure-0 respectively. This aspect is referred to as P-tree compression [10] [16] and is one of the most important characteristics of the P-tree technology. Note that we can easily differentiate between pure-0 nodes and mixed nodes by the fact that pure-0 nodes have no children \(i.e. they are leaf nodes P1-trees corresponding to the P-trees in Figure 3 P1-trees are manipulated using operations such as AND, OR, NOT and ROOTCOUNT \(the count of the number of  1  s in the bit group represented by the tree in order to query the underlying data. The reader is referred to [10] [16] for more details on P-trees and their logical operators   Figure 3. Construction of basic P-trees Figure 4. Pure-1 Trees Note that by using P-trees, we can compute the confidence of rules in a quick and efficient manner without any database scans represented by a P-tree, Pi; to get the support of an item, we issue a ROOTCOUNT operation on the P-tree of that item. To get the support of an item set of size more than one, we AND the P-trees of the items in the a 0 00 0 1 1   0 0 01 0 1 b 


b c 3 30 1 2 1   0 6 24 0 2 d Column1   Column2 0                 1 0                 1 0                 1 0                 1 1                 0 0                 0 1                 1 1                 1 b bit groups 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 a Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE item set and then issue a ROOTCOUNT operation on the result 5.2.  Taboo lists For every new item I, we create a new node under the root and test for potential rules with all possible unterminated nodes lying to I  s right. The actual node creation order of the SE tree is the lexicographic subset ordering of the items in the given item list. For example, the node creation order for the set {1,2,3 would be: {1}, {2}, {1,2} ,{3}, {1,3}, {2,3}, {1,2,3 This ensures that every item set has all its subsets before it For every item, I, we maintain a TLI which saves the item sets whose outcomes, when joined with I, are known beforehand and thus need not be tested. In our implementation, each TLI is a P-tree having a size equal to the number of un-terminated nodes to the right of I \(i.e. all nodes that I will be tested with is used for nodes which, when joined with I, result in the firing of some pruning condition; thus, none of their supersets need to be tested with I. The remaining TLI entries will be 0s. For example, for the set of items 1,2,3,4}, suppose that the nodes created so far are 1}, {2}, {1,2} ,{3}, {1,3}, {2,3}, {1,2,3}. For node 4 we initialize a TL4 having 7 entries all containing 0s initially. If the joining of item 4 with node {2} results in firing at least one pruning condition, then the second entry pointing to item set {2} is flagged with 1, and so are all entries containing 2 \(i.e. entries pointing to 1,2}, {2,3} and {1,2,3 


1,2}, {2,3} and {1,2,3 tell which other entries contain item 2? We maintain for each item an index list as a P-tree \(called an index P-tree item exists in. For example, item 1 will have the following index list \(it will be stored as a P-tree but we are just listing the entries in a list for convenience 1,0,1,0,1,0,1. In other words, viewing the nodes of the SE tree in node creation order, item 1 occurs in node positions 1, 3, 5 and 7. Every new node added to the SE tree results in the expansion of all index P-trees by either a 1, if the corresponding item is in the new node added, or a 0, otherwise Going back to the previous scenario where the joining of item 4 with node {2} results in firing at least one pruning condition and thus node {2} need to be added to TL4, we simply OR the index P-tree of item 2 with TL4. In general, if we want to add node {x y  z} to some taboo list, TLI, we AND the index Ptrees for all items in the node \(i.e. AND index P-tree of x with that of y  with that of z us where item set {x, y  z} occurs. We then OR the resulting P-tree index with TLI which results in appending node {x, y  z} to TLI We maintain the taboo lists and index lists as Ptrees as this will give us faster logical operations and compression. In addition, in the case of taboo lists, it could speed the node traversal especially in cases where there many consecutive 1  s. For example suppose the entries in a taboo list are: 1111 0011 Figure 5 below shows the corresponding P-tree of the given taboo list Figure 5. The resulting taboo list in P-tree format In this example, instead of going through the first four nodes sequentially and then skipping them because they are flagged, using a P-tree to represent the taboo list, we can directly skip the first 4 entries because they form a pure-1 node on 2nd level of the Ptree It is worth mentioning that our traversal through the itemset space using taboo lists is very similar to a popular approach used in AI literature and known as Tabu search [13]. The idea in Tabu search is to traverse the space in a more effective manner by avoiding moves that result in revisiting points in the space previously visited whose outcome is known not to be acceptable \(hence the name "tabu   union of I and X produces an infrequent itemset implies that future joins of I with any superset of X will produce an infrequent itemset; a scenario similar in essence to revisiting a point in the search space whose outcome is known to be unsatisfactory and which could be circumvented by putting the point on a Tabu list. Because of the difference in context and problem definition, we refer to our lists as taboo lists instead of Tabu lists 5.3.  Comparison analysis To the best of our knowledge, no previous work has attempted to mine the type of rules we are considering We find [5] to be particularly interesting as it proposes an algorithm called Dense-Miner which is capable of mining association rules with fixed consequents at very low support thresholds. For the lack of a better benchmark, we will compare our approach with DenseMiner; however, we have to emphasize that a number 1                  0 0                    1 0 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 


ICTAI 2004 1082-3409/04 $20.00  2004 IEEE of fundamental differences exists between the two approaches which we briefly outline next   D e n s e M i n e r  m i n e s  a l l  a s s o c i a t i o n  r u l e s  w h i l e  w e  only mine minimal, confident rules   D e n s e M i n e r  u s e s  s u p p o r t  a s  a  p r u n i n g  m e c h a n i s m  while this is not the case in our work \(In reality we also use support pruning, but in our case, the support threshold is always set to 2  as an absolute threshold In terms of rule overlap between the two approaches, all rules produced by our approach that have a support value greater than the minimum support threshold used for Dense-Miner will be produced by Dense-Miner also Table 2. Data sets description Table 3 Connect-4 Dataset 0 500 1000 1500 2000 2500 3000 3500 20 30 40 50 60 70 80 90 100 Confidence Threshold Ti m e s  P-tree based Dense Miner PUMSB Dataset 0 500 1000 1500 2000 2500 40 50 60 70 80 90 100 Confidence Threshold T im e s  P-tree based Dense Miner Figure 6. Speed comparison All experiments were conducted on a P-II 400 with 128 SDRAM running Red hat Linux 9.1.  C++ was used for coding. We experimented on two real-life dense data sets, Connect-4 and PUMSB, which are available at the UCI data repository. Table 2 below briefly describes the two datasets by listing the number of transactions, items, and items per transaction for each data set Connect-4 Dataset 0 50 100 150 200 250 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 99 99 5 Confidence Threshold 


N um be r of ru le s PUMSB-4 Dataset 0 20000 40000 60000 80000 100000 120000 140000 160000 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 91 91 5 Confidence Threshold N u m be r of ru le s Figure 7. Number of rules produced Figure 6 shows the time in seconds needed to mines rules at different confidence thresholds by our approach \(P-tree based aforementioned, Dense-Miner mines all rules using support pruning. It uses a variant of support referred to as coverage and defines the minimum coverage threshold as the minimum support divided by support of the fixed consequent. Results for Dense-Miner are observed with minimum coverage threshold fixed at 1% and 5%, respectively Our approach, on the other hand, mines only minimal rules without using any support pruning. It is very clear from the figure that users interested in minimal rules without support would prefer our approach as the time needed is many orders of magnitude less than that of Dense-Miner The user might notice from both parts of Figure 6 how the two approaches differ in the way they produce the rules. Dense-Miner takes more time at lower confidence while our approach takes more time at higher confidence thresholds. This is mainly because our approach mines minimal rules using only confidence pruning and the higher the confidence threshold, the more difficult it would be to get confident rules high in the SE tree; as a result, the SE tree grows deeper and thus requires more time to traverse. Dense-Miner, on the other hand, mines all Trans. Items Items per trans Connect-4 67557 129 43 PUMSB 49046 7117 74 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE rules and it is very logical to have more rules satisfying lower confidence thresholds \(and vice versa obviously requires more time to mine The number of rules produced by Dense-Miner ranges from around 500,000 rules to less than 10 rules over both data sets. Figure 7 shows the number of rules produced by our approach over the two data sets at the 


produced by our approach over the two data sets at the different confidence thresholds. The same discussion presented in the previous paragraph applies here regarding the larger \(smaller produced at higher \(lower 6. Conclusion In this paper we proposed a framework based on SE-trees and the P-tree technology for extracting minimal, confident rules using fixed-consequent ARM Our methodology relieves the user from the burden of specifying a minimum support threshold by extracting the highest support rules that satisfy user confidence threshold. Albeit, to the best of our knowledge, no previous work has attempted to mine minimal confident rules with fixed consequents, we provide a comparison analysis study showing how well we compare to other close approaches in the literature In terms of limitations, we acknowledge that our approach suffers in situations where the desired rules lie deep in the tree because a large number of nodes and levels need to be traversed then. A future direction in this area targets finding measures for estimating the probability of rule availability along certain branches and quitting early in cases where such probability is low 7. References 1] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases Proceedings of the ACM SIGMOD \(Washington, D.C 1993 2] R. Agrawal and R. Srikant, Fast algorithms for mining association rules. Proceeding of the VLDB \(Santiago, Chile 1994 3] K. Ahmed, N. EI-Makky and Y. Taha  A note on  Beyond Market Baskets: Generalizing association rules to correlations  ACM SIGKDD Explorations, Vol. 1, Issue 2 pp. 46-48, 2000 4] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, and L Lakhal, Mining Minimal Non-Redundant Association Rules using Frequent Closed Item sets. Proceedings of the First International Conference on Computational Logic \(London UK 5] R. Bayardo, R. Agrawal, and D. Gunopulos, ConstraintBased Rule Mining in Large, Dense Databases. Proceedings of the IEEE ICDE \(Sydney, Australia 6] R. Bayardo and R. Agrawal, Mining the most interesting rules. Proceedings of the ACM SIGKDD \(San Diego, CA 1999 7] C. Becquet, S. Blachon, B. Jeudy, JF. Boulicaut, and O Grandrillon  Strong-association-rule mining for large-scale gene expression data analysis: a case on human SAGE data   Genome Biology, 3\(12 8] F. Coene, P. Leng, and S. Ahmed  Data Structure for Association Rule Mining: T-Tree and P-Trees  IEEE transactions on Knowledge and Data Engineering 16\(6 778, 2004 9] E. Cohen, M. Datar, S. Fujiwara, A. Gionis, P. Indyk, R Motwani, J. D. Ullman, and C. Yang  Finding interesting associations without support pruning  IEEE Transactions on Knowledge and Data Engineering, 13\(1  78, 2001 10] Q. Ding, M. Khan, A. Roy, and W. Perrizo, The p-tree algebra. Proceedings of the ACM SAC, Symposium on Applied Computing \(Madrid, Spain 11] Qin Ding, Qiang Ding, and W. Perrizo, Association Rule Mining on Remotely Sensed Images Using P-trees Proceedings of the PAKDD, Pacific-Asia Conference on Knowledge Discovery and Data Mining, Springer-Verlag Lecture Notes in Artificial Intelligence 2336, 66-79, May 2002 12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


