Audit-trail-based modelling of the decision-making process in Management and Accounting using sensitivity analysis  Serafeim Fragos Department of Communication Systems, Lancaster University, Lancaster LA1 4YR, UK Dr Lampros Stergioulas Lampros.Stergioulas@brunel.ac.uk  Reshma Gandecha reshma.gandecha@brunel.ac.uk  Brunel University, Department of Information Systems and Computing, Uxbridge, UB8 3PH    Abstract  In a behavioural and organisational context 
complex problems that reflect the multidimensional attributes of human activity inevitably arise and have to be addressed. In an attempt to model human decision-making behaviour, the vast number of potential parameters raises the question of how this complexity can be harnessed.  This paper proposes a data-driven approach, with which dependencies or associations are extracted from the data itself The complex and dynamic nature of modern business processes makes this approach more suitable, as the design of competent rule-based models or expert systems would be cumbersome expensive or even infeasible. A hybrid behaviour 
modelling method, based on both statistical component analysis and sensitivity analysis, is proposed to directly model decision behaviour The derived model is the outcome of an optimisation process, where model-data matching is maximised in terms of known, pre-defined criteria. The implementation of the proposed intelligent system as an integral part of real-life business/accounting activity is discussed, and its capability to provide intelligent support to the decision making and internal control processes in management and accounting is demonstrated using realistic data from a business procurement 
application  1. Introduction    In a behavioural and business context, it is inevitable to have to cope with a lot of different information attributes which by and large reflect the multidimensionality of the human nature Resource-Based-Agents accounting information systems and user navigation in management information systems are two indicative areas in which the vast amount of potential parameters 
raise the question of how all this complexity can be harnessed The answer can be two-fold. First, the accounting, auditing or managerial staff can design appropriate models which could act as rule-based systems and give solutions to well-defined problems. The second approach concerns datadriven methods which intend to extract the appropriate rules based on the data itself. The complex and dynamic nature of the contemporary business processes makes this approach more 
suitable, as the design of appropriate models or expert systems would be cumbersome, expensive or even infeasible. Thus, classification and prediction in a behavioural context can be valuable alongside a necessary data reduction or some kind of feature selection procedure The main aim of this work is to identify a suitable algorithmic framework that can maximize classification and prediction in organisational domains that are examined under a behavioural context. A second objective is to propose an 
exemplar for audit trail design, analysis and modeling in management information systems  1.1.  Audit trail analysis  Audit trail is a term used in both accounting and computing disciplines. Literally, it reflects the chronological order of the events which take place in an organisation or in a computing server Behavioural analysis \(of user or interactions\n be carried out by examining the records of the audit trail file. The prevalence of electronic forms of data has forced originally disparate Computing 
and Management disciplines \(especially that of information systems auditor\ converge According to current practice, the audit trail analysis and initial setup can be carried out by computer experts or internal auditors In computer systems, audit trail are produced by application software systems. They record details of all user activity, in a chronological order 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 1 


including the input transactions, the processing paths they traverse, and the resultant effects on database records.  Weber [1 s t i n gu i s h e s t w o  types of audit trail: \(a\ accounting audit trail which maintains a record of events within a subsystem and \(b\ operations audit trail, which maintains a record of the resource consumption associated with each event in the subsystem. The audit trail must be analysed by auditors or management to detect possible control weaknesses in the system. According to OLeary 2 intrusiondetection systems can be employed. These systems monitor users to determine whether current behaviour conforms with past behaviour. Denning  ied th e role a n d u s e of a u dit trail in  th e  context of intrusion detection. This model is based on the hypothesis that security violations could be detected from abnormal patterns of system usage   1.2. Aim of computational algorithms  Our aim is to produce appropriate algorithms for behavioural data classification capable of coping with  a The multidimensionality of data. To do this, feature selection methods are applied in order to select the optimal feature subset, which maximises the classification and prediction performance of the algorithm b. Multicollinearity issues. This is an important step, since the large number of features in a behavioural analysis context make the probability of correlated each other features very high c To cope with overfitting. Overfitting is the result of an over-trained sample which is damaging as to the proper classification of the rest of the data. The complexity and uncertainty of human-generated data can cause problems. Methods for overcoming such difficulties are proposed  The large number of features is often a restrictive factor. The algorithms should be able to perform almost equally well with datasets containing fewer features that exhibit a high sensitivity to the classification target. Determining the sensitivity of features is most important, since the sensitivity score can be used as a metric by which a features worthiness is evaluated  2 Some background on Behavioural Accounting  According to Belkaou n d th e C o m m i t t ee of Behavioural Scien n t i n g can be considered to be a behavioural process. The behavioural accounting approach applies behavioural science to accounting. The objectives of this approach are very similar to that of behavioural science. As a main objective of behavioural science is to analyse and predict human behaviour, the corresponding task in behavioural accounting is to analyse and generalise the behaviour of financial informations users in all possible accounting contexts. Users of accounting information can be accountant or nonaccountant individuals. Behavioural accounting is a multidisciplinary field that includes accounting of financial theory, statistics or other computational methods, cognition and psychology and increasingly elements of information theory and computing In terms of human information processing in accounting there are two main components that can be considered as the main objectives. The first deals with the improvement of the quality of the data \(including representation\he second concerns the ability/skills of users processing business data. Several approaches have been proposed in contemporary literature based on probabilistic judgments, computational aspects and cognitive attributes The Brunswicks Lens Model i s bas ed on  the recognition of the interaction between the environmental and individual-specific variables. A regression or ANOVA model is used where the personal attributes are categorical values. Other methods include conjoint measurement multidimensional scaling techniques, and discriminant analysis. The expected benefits come in the form of improved decision making in terms of  1 The importance of attributes in the judgment process examining the consensus between decision makers 2 The accuracy of judgments 3. Effects of task characteristics on achievement and learning  In accounting, especially in the auditing areas the judgment accuracy is assessed by two main criteria. One is the degree of consensus regarding the decision made and the other is a mathematical or statistical model 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 2 


2.1.  General behavioural aspects  An Accounting Information System \(AIS\n an organisation plays the role of supporting the decision making process that is carried out by internal or external users of the organisation. This can be done through the appropriate process and presentation of financial information. Ko and Mock u e t h at to des i g n a s u cces s f u l  A I S the integration of human behaviour in the system is important In support of this argument, Dickson advocates that the decision makers are different having obviously different needs\ and the information system requirements should vary per type of decision being considered. Thus, it might be expected that behavioural studies in AIS can support not only the design of effective accounting systems but also the provision of new theoretical insights into the nature of accounting information The decision-facilitating function of AIS is emphasised in the AIS behavioural research. In their review, Ko and Mock u e th at t h es e  studies investigate the parameters that influence the decision makers effectiveness in using accounting information. These parameters can be divided in three categories including a Information structures b. Individual difference characteristics and c. Task characteristics  2.1.1 Information structures and presentation format  Information structures that communicate the information to the users play an important role and they can include: a. the way information is presented, b. the degree of aggregation of information and c. the types and frequencies of feedback to the decision-maker. The report format of financial information is investigated by several studies, whereas salient effects seem to be relevant. Stock and Watson 9 investigated the subject by examining different formats of financial accounting ratios revealing classification differences due to different treatment of these ratios  2.1.2 Individual difference characteristics and cognition  Individual difference characteristics refer to cognitive issues as well as the experience of the users. The cognitive characteristics can be considered as the independent variables and the information processing behaviour or task performance as the dependent variables According to Ko and Mock, some AIS researchers hypothesise that a decision makers performance is likely to be affected by these characteristics Derm es ti g ated t h e eff ect o f deci s i on makers cognitive characteristics on the perceived importance of information. The results suggested the cognitive characteristics of users need to be considered in the design of accounting information systems  2.1.3 Task characteristics  The decision task characteristics play an important role. Behavioural research in AIS investigates the effects or relationships of task parameters \(e.g. task complexity, ambiguity or time and financial constraints\ on the decision maker performance  2.2.  Information Systems Audit and Security  Multidimensional accounting systems seem to take into account non-financial information and be more adjusted to behavioural issues. Current ERP applications include not only human resources subsystems, but also tools of recording the user behaviour for security and change-management purposes 12 S ecu ri t y i s on e of t h e m o st  critical areas in Internal Auditing. A definition  of com p u ter au dit \(or I n f o r m ation S y s t e m s  Audit\the application of the principles of internal auditing to the practices of computing and information systems  2.3. The rationale for data reduction  Contemporary accounting and management information systems deal with huge amounts of data which must be processed and evaluated on an annual, monthly, daily or other basis. The dominance of internet, intranets, and other networking environments have increased the complexity of current databases and made the need for data reduction tools more imperative. Human behaviour also plays a distinctive complex role since it is unpredictable and much more difficult to follow specific rules According to In e n s i o n alit y reduction offers the following advantages for a classification system  Cost reduction in data acquisition 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 3 


 Improvement of the comprehensibility of the classification model  Faster induction of the classification model  Improvement of classification accuracy   3. The proposed audit-trail modeling system  3.1. The data analysis system and simulation data  The simulation is relevant to internal agents modelling while they access and explore information data for completing a task and/or making a decision. This experiment aims to achieve maximum generalisation using a neural network model The high-level architecture of the proposed audit-trail data analysis system includes a user behavior pattern generation stage, where the key features of the user \(user model\ calculated and a user classification stage. The performance of such a system depends crucially on the effectiveness of the chosen behavior modeling process. Existing data-driven models of human behavior employ some form of correlation analysis, usually via calculation of the covariance matrix i a Prin c ipal C o m pon e n t An al y s i s  PCA  26 postgraduate students ran a simulation of an REA-oriented reporting system. The students were asked to make a decision about the purchase of a software package \(choice of 10 packages in total They were prompted to execute some queries which concerned relevant information of financial or non-financial qualitative data. This report system involves the view of queries based on disaggregated and localized data. Thus, the viewed queries belong into groups with respect to the operating business functions To model human decision behavior with a limited set of parameters, the proposed method uses PCA to reduce the dimensionality of the input data and provide a simple behavioral model. Thus the method is a hybrid of statistical and selflearning methods, comprising a Pre-processing, which includes feature extraction and selection \(redundancy reduction b Statistical component analysis \(PCA c. Sensitivity analysis through neural network bootstrapping, and finally d Supervised nonlinear classification by a neural network  The result is a compact, robust behavioral model \(user representation in terms of a small number of parameters/features\. Given this model the data analysis system will be able to generate the behavioral pattern for a particular user and then characterize/classify it by employing the derived neural network model The method intends to produce a classification and prediction tool for decision makers identifying the important features and removing redundant information. In this experiment, the decision makers are classified into 2 categories according to if their selected purchase \(software\ncludes high level of support service \(provided by suppliers not  3.2. Feature Design   Time-based features can be extracted regarding 3 time-based characteristics. The features are designed according to  The contextual environment of decision making process  The level of interest which can be interpreted, furthermore, as motivation  The salient effects which can identify focal points or sensitive information  The cognitive characteristics of users which reveal users exploration style cognitive style\ or cognitive strategies  1. Duration of activity \(37 features Features of this type are pertinent to how long a user stays in a file \(where he can execute several queries duration aspects based on the overall simulation like total duration of the experiment longest/shortest file duration, start time etc  2. Activity frequency \(37 features Features of frequency are relevant to the number of times the user visits the files \(that correspond to operating functions\ or how many times he/she executed queries within each file. The number of files or queries used by users make up the rest of this feature type  3. Order-based \(44 features These features are relevant to the cognitive characteristics of the user The time he/she accesses a file is recorded and an order score is calculated. The earlier a user views a file the bigger the order score value. For 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 4 


each file:  S = T/A, where S is the order score value, T is the total simulation time for a user and A the files access time. Different features are extracted for each file considering either the average of these values or the \(unique earliest/latest access time. 118 features per user were extracted from the raw log data. Table 1 depicts the contextual and time structure of features Table 1: Structure of features   Duration Frequency Order Sum Files Activity 33 33 44 110 Overall Activity 4 4 - 8 37 37 44 118 3.3. Principal Components Analysis  The number of features designed is inevitably high. Some of them are possibly redundant adding more noise or delay to the process, making the need for data reduction imperative. PCA is a classical method which has been used mainly for data reduction purposes. This study involves PCA as a step of transforming the input of the neural network \(NN\oid NN overfitting or noise errors. The original features will be selected or discarded using a NN boot-strapping method  3.4. The NN Bootstrapping Approach  NN cross-validation and bootstrapping is considered to be well performing in terms of defining the generalisation error and best model selection    In t h i s st u d y   NN  bootstrapping is used as the core method to  1 Perform a Sensitivity Analysis of inputs which can be the original features or the Principal Components Scores variables  2 Remove the redundant features from the system and keep only those which appear to have the greatest sensitivity to output giving simultaneously the minimum generalisation error  3 Select the best model that minimises the classification error and gives maximum generalisation value  3.4.1. Description of the NN Bootstrapping In fig. 1, the NN Bootstrapping method is depicted There is one hidden layer with 4 nodes and the training error is defined as .05.  The classification target vector takes 2 values which are 0 for selection of packages type A and 1 for type B Type A = software packages with moderate or low service support, type B packages with high service support\The maximum number of the models is 1500. In each model a data validation subset is defined at random. Consequently 23 users are treated as the training set and 3 as the validation set.  The target is to find 100 models with low generalisation error, which is taken to be equal to the lowest error of the first 10 models.  In case that the instances of this error are fewer than 100, we select the 100 best models out of 1500     Figure 1: NN Bootstrapping  Furthermore a random variable/feature is added which can be used as a metric of sensitivity. For each successful model we run a simulation where for each variable of an input vector we assign 10 values from 0 to 1 increasing by step s = 0.1. Then the variance of output is calculated regarding the 10 variables values. For each variable \(including the random variable\he procedure is repeated for all the input vectors of the data and we calculate the variance mean of all outputs  The final sensitivity of each variable is calculated as the average of all 100 models as S V/M where   V  =  The average of variance of all models and M  =  The average of all models simulated outputs means  It should be mentioned that the variance is divided by the overall mean of the outputs to smooth any distortions that can occur due to 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 5 


different magnitude of the output values. The variables with lower sensitivity than random are excluded from further analysis. The same happens when the variables have sensitivity less than 110 of the random gauge  3.4.2 The feature selection algorithm The method involves PCA and 3 stages of sensitivity analysis \(see fig.3\hese are  1. PCA and extraction of Principal Components \(PC\ Scores 2 NN Sensitivity Analysis of  PC Scores 3 NN Sensitivity Analysis of  Selected Features 4 NN Sensitivity Analysis to define the best Model  3.4.3  Principal Component Analysis \(PCA Initially the Principal Components \(PCs\which contribute to 95% of the covariance are computed This number is 18. As a second step we carry out Varimax on th e P C loadin g s T h is i s  a necessary step to achieve maximisation of variance within each loading and therefore a bigger differentiation of components in terms of variables significance within each loading Finally the PC Scores are calculated.  PC Scores indicate the score of user on each component and they can be expressed as S = A  L where A Standardised data and L = PC Loadings  3.4.4. Using PC Scores as Input to the Bootstrapping NN NN Overffiting is a critical issue which should be tackled if adequate generalisation and less output variance are to be achievec. Causes of overfitting include large number of input variables as well as multicollinearity problems [21 2 2    Since the number of original variables is large \(in respect to the training cases\, PC Scores can be used as inputs to the NN instead of the original variables and overcome respective problems. The task in this phase is to assign a degree of sensitivity to PC Scores. The number of selected significant components with the greater sensitivity is 7 PCs are then converted into features again through loadings after being weighted according to the result of the PC Scores sensitivity. In this way, the system runs an intermediate process offering more reliable results and avoiding overfitting. 80 \(out of 118\eatures with the greater sensitivity \(over 0.1\re selected The selected features from the sensitivity analysis process feed a similar NN. Following the same NN procedure, 13 features are finally extracted \(Table 2 Table 2: Final features  N o Description File Time Character istic Sensitivity  1 9 Idle time between file opening Package Functionality Info Duration  11.35 2 6 Idle time after file process Budget Duration 9.84 4 6 Times that file was accessed Package Performance Info Frequency  5.48 6 4 Number of different queries Suppliers Offers Frequency 5.22 6 6 Number of different queries Suppliers pricing discounts price policy etc Frequency 5.71 6 8 Number of different queries Package Performance Info Frequency 5.54 6 9 Number of different queries Package Functionality Info Frequency 4.55 7 0 Number of different queries  Package Service Info Reports Frequency 7.38 7 4 Number of different queries  Logistics Department Files Frequency 5.67 7 6 First Time Access Order Score Budget Order 6.8 7 8 First Time Access Order Score Suppliers Stability/Relia bility Information Order  11.35 7 9 First Time Access Order Score Package Performance Info Order 4.91 8 0 First Time Access Order Score Package Functionality Info Order 6.4 1 0 1 Average Order Score of Access Package Performance Info Order  4.71 1 1 2 Average Order Score of Access End Package Performance Info Order  5.2  0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 6 


3.5. Assessing the Generalisation Performance  Best Model Selection  The NN process is repeated without the generalisation restriction. A cross-validation procedure with 100 random samples is carried out and the overall performance of the 100 models is calculated. Finally the one with maximum performance in terms of classification and generalization is selected  The criteria for selecting the best model out of 100 are \(in order of importance  1 The lowest total \(training and validation error for both users classes 2 The lowest validation error for both users classes 3 The most equal representation of both classes in the validation set  The selection of best model is determined mainly by the low generalisation \(or validation error for the two classes, since the training misclassifications are equal to zero most of the times. The third criterion concerns the equal prediction capability for both classes Table 4 shows the average performance of the 100 models for 3 different combinations of features. Comparing this performance to that of the 1 st row which is to do with the 1 st  Bootstrapping stage where all features are included \(via components\n that the contribution of data selection/reduction stage to performance is extremely important. It can be noted that the average performance of the data with the final set of features outperformed significantly the original representation of the data Table 3: Average Performance \(100 models   Features Class 0 Class1 Overall All features included \(118 47% 46% 46 Selected features with extreme values impact \(15 81% 71% 76 Selected Features without extreme values impact \(15 90% 87% 88  The last row of table 4 shows the average performance of the 3 rd final bootstrapping.  It can be concluded that there is actually a pattern in users behaviour, which can validate and give an interpretation of the users cognitive styles and interest 3.6. Discussion As is shown in Table 2, feature 70 related to the Package Service Info Reports file appears to have a significant level of sensitivity \(or discrimination value\garding the users classification. The value of this feature depends on the frequency of queries activities within the file This makes sense if we consider that the more queries the users use the more interest they have for the software support after purchase Apart from this obvious observation, salient attributes can be considered on features pertinent to Package performance Info file. That probably indicates that the interest of decision makers for the packages technical performance appears to have a real information value regarding the users final judgment. This is a salient effect which since the presentation format of the respective queries menu do not have any distinct characteristics in the prototype interface  can be considered as a genuine information effect  4. Conclusion The proposed method can be considered as a hybrid of data reduction and classification algorithm techniques. The main advantage of the method is that the users can be modelled using only the most significant variables which give the maximum prediction value Abnormal behaviour detection techniques can be based on the selected features with the highest sensitivity to the decision made. Identification of features non-compatible with the task could reveal bias or deviations from normal procedures Moreover, misclassification of a new user by the NN model could indicate a users different behaviour which should be examined and further analysed. Thus the method can assist in verifying that the organisational procedures are followed and compliance is met. Also it could have the form of an intelligent agent possibly embedded in 1 Executive Information System \(EIS 2 Auditing software \(Computer Assisted Auditing Tools and Techniques CATTS 3 Control self assessment tools and 4 Decision aid tools  Contribution to task performance is also relevant, since decision making bias or information effects k acc u r ac y ca n be identified. Problems pertinent to the complexity of the contextual environment can be resolved by removing redundant information 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 7 


The method can have equal success in analyzing multidimensional accounting information that includes human and non-human economic\mponents mixed together Regardless the accounting/auditing context, the proposed algorithm can be a significant contribution to supervised classification, nonlinear regression and data reduction techniques. It represents a procedure for variable elimination according to generalisation and non-linear regression criteria, which is ideal for analyzing human behaviour due to the complexities of \(a information load \(too many variables\d \(b nature of human cognition  Acknowledgements One of the authors \(SF gratefully acknowledges financial support from the State Scholarships Foundation \(I.K.Y.\of Greece   5. References   W e b b  G  L    P azzan i  M  J  and D Bi l l s u s  M ach i n e  Learning for User Modeling, User Modeling and UserAdapted Interaction, Vol. 11, 2001, 19-29  OL ear y  D E  In tru s io n-Detectio n S y ste m s  Journal of Information Systems Spring\, 1992, 63-74 3 D e nning D  E A n intrus ionde te c tion M ode l   IEEE Trans. Soft. Eng., SE-13 1987, 222-232 4 B e lk a oui, A A cc ounti ng the ory  2nd Inte rna tio na l  Edition, Harcout Brace Jovanovich, Inc, 1985 5 R e port of the C o m m itte e o n B e ha v i oura l Sc ie nc e  Content of the Accounting Curriculum The Accounting Review supplement, 1971, p.247  Brun sw i c k E   T h e Co n cep t u al F r am e w o r k o f  Psychology, Chicago: University of Chicago Press 1952  Ko C-E  and T h eod o r e J M o ck Beh avi o ural  Accounting Research: A Critical Analysis, Kenneth Ferris, Century Publishing Co., Chapter 8, 1987, pg 171-201 8 D i c k s on, G  W Se n J  A  a nd N  L  C h e r v a ny   Research in Management Information Systems: The Minnesota experiments Management Science May 1977, 913-923 9 Stoc k  D  a nd C  J  W a ts on H um a n J udgm e n t  Accuracy, Multidimensional Graphics, and Humans Versus Models Journal of Accounting Research 22 1 1984, 192-206 10 De rm e r J.D C og nitiv e  Cha ra c t e r istic s a nd the  Perceived Importance of Information The Accounting Review July 1973 511-519 11 B a rne s M C us tom iz a tion of ER P a pps re quire s  development skills, Michael Barnes. Informationweek Manhasset, 722, 1999, pg. 9A 12 Be s t  P   S A P R/3 A udit T r a il A n a l y s is 4th  Annual SAP Asia Pacific Institute of Higher Learning Forum, SAPHIRE 2000 13 C h a m be r s A  D  a nd J  M. C our t C om pute r  Auditing, London, Pitman Publishing, 1991 1 In za I  L a rraa ga P   an d B  S i erra F eatu r e S u b s et Selection by Bayesian Networks: A Comparison with Genetic and Sequential Algorithms International Journal of Approximate Reasoning 27\(2\, 2001, 143164 15 L a m  K  Y H u i, L a nd S.L Chu ng   Multivariate Data Analysis Software for Enhancing System Security, J. Systems Software, 31, 1995, 267275 16 C h e n H  M. a n d M.D C o o p e r  U s i ng C l us te ring  Techniques to Detect Usage Patterns in a Web-based information System J. Am. Soc. Info. Sci. Tech., 52  2001, 888-904 17 B a x t W  G  a nd H  W h ite  B oots tra ppi ng  Confidence Intervals for Clinical Input Variable Effects in a Network Trained to Identify the Presence of Acute Myocardial Infarction Neural Computation, 7 1995 624-638 1  E f ro n  B T h e Jack kn i f e t h e Bo ot st rap and Ot h e r  Resampling Plans, Philadelphia: SIAM, 1982 19 Ef ro n  B E stim atin g t h e Erro r Rate o f  a Prediction Rule: Improvement on Cross-validation J of the American Statistical Association, 78 1983, 316331 20 Kaiser, H. F., T h e V a ri m a x Criterio n f o r A n al y t i c  Rotation in Factor Analysis Psychometrica, 23 1958 187-200 21 Hu rv ich C  M. an d Ch i h L in g T s ai, Re g res sio n  and Time Series Model Selection in Small Samples Biometrika, 76 2\, 1989, 297-307 22 Sa rle  W  S S to ppe d T r a i ning a nd O t he r Remedies for Overfitting", Proceedings of the 27th Symposium on the Interface of Computing Science and Statistics, 1995, 352-360 2 Hay n es C M   S  J Kach el meier  T h e E f f e cts o f  Accounting Contexts on Accounting Decisions: A Synthesis of Cognitive and economic Perspectives in Accounting Experimentation Journal of Accounting Literature  Gainesville  17 1998, 97        0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 8 


  n?1 2 j HT  n?3 exp  n?1 2 j 2 \(?2 j HT  2 j HT n?1  n?1 2 n?1 ds2?j 41 PDET    n?1 2 j HL  n?3 exp  n?1 2 j 2 \(?2 j HL  2 j HL n?1  n?1 2 n?1 ds2?j 42 where  2 n? 1 2?j |HT 2?j |HL 2?j |HL 2?j |HT 43  ln    2?j |HL 2?j |HT  n?1 np   Thus all four probabilities are governed by two equations in four unknowns: n, ?np, \(?2?j |HT 2?j |HL 


in four unknowns: n, ?np, \(?2?j |HT 2?j |HL The Neyman-Pearson decision rule is said to maximize the probability of detecting the track-lost regime, PDET for a given n, subject to the constraint that PFA is less than or equal to some user-de?ned probability. [6]. However, it is convenient in this context to instead ?x both PDET and PFA at some desired values, and use \(41 43 the corresponding minimum \(integer n that must be used in order to meet these goals. Typically there is a range of values of ?np that will achieve them Appropriate values of n and ?np can be determined for each pe element ?j using \(41 43 determinations for each element can then be made using 38 39 for each pe element; if a single overall track-loss metric is desired, Wishart distributed versions of \(33 35 also be used in the development of the Neyman-Pearson decision rule. However, because of the approximate nature of the distribution in \(35 additional covariance information is useful in the matrix likelihood test [5]. Further, a Wishart implementation has the disadvantage of being more computationally intensive than the implementation outlined above D. Application to the PDAF In order to implement the Neyman-Pearson rule, \(15 be rewritten x  k|k  k|k ? 1 k k 44 where, for the PDAF e?\(k  mk i=1 i zi\(k 1? ?0  k|k ? 1   45 This  effective  measurement innovation is then the single lter prediction error for purposes of calculating the sample variances s2?j . The j-th diagonal of the innovations covariance for the tracking regime, ST?sirf , and track-lost regime SL?sirf , can then be used for \(?2?j |HT 2?j |HL respectively Some method of handling the case of no gated measurements must be implemented. As suggested earlier, it is at least mathematically consistent in this case to de?ne the measurement innovations as zero. However, when there is a signi?cant probability of zero measurements gating the assumption that the prediction errors have a Gaussian distribution will be a poor one; in practice, this probability is likely to be signi?cant both when PD &lt; 1 and when in the track-lost regime. So the goals for PDET and PFA will not generally be met unless the innovations from timesteps when no measurements gate are excluded, and it is necessary to use the previous nC timesteps to ?nd n nonzero innovations with which to compute s2?j . The average number of timesteps n  T used when the ?lter is operating in the tracking regime is approximately n/\(PDPG average number of timesteps n  L used to when the ?lter is in the track-lost regime is problem speci?c E. Extensions to Other Data Association Methods This strategy is suitable for other data association methods. For the NN ?lter, the KF measurement innovations are the prediction errors. For MAP and maximum likelihood ML determined by similarly deriving a relationship between the estimated state, predicted state, and Kalman gain IV. EXAMPLE: PDAF TRACK REGIME TEST In this section, an example system using the PDAF is constructed. The SIRF approximation steady-state innovations covariances for the tracking and track-lost regimes are calculated and compared to simulation results. An example 


calculated and compared to simulation results. An example Neyman-Pearson decision rule is determined. Theoretical and simulation results for the probabilities of detection of the track-lost regime, PDET, and of false alarm while in the tracking regime, PFA, are given. An estimate of the number of timesteps required to detect track-loss is also provided 4320 A. Dynamics The kinematic model system \(with timestep x\(k + 1  1 0 1  x\(k  2/2   w\(k 46 y\(k  1 0  x\(k 47 is the standard zero-order hold discrete approximation to a continuous double-integrator system. For ? = 0.1 x\(k + 1  1 0.1 0 1  x\(k  0.005 0.1  w\(k 48 B. Kalman Filter System For Q = Q = 1000, R = R = 0.1 P  kf  0.3000 1.9998 1.9998 19.9965  Skf = 0.4000 are the steady-state covariances C. Clutter and Gating Though the example system is dependent on Q,R and ?, it can be described in the tracking regime using just three independent parameters [12]: the probability of detecting the truth measurement, PD, the normalized target acceleration, \(NTA sity, \(NCD the target is maneuvering, and NCD is a measure of how dif?cult it is to localize the target from the measurements For the example system, NTA = 1. Choosing PD = 1 0.02 results in NCD = 0.002, and an average track-lifetime of approximately 500 timesteps using the PDAF [8], [12 this is a tracking problem of  moderate  dif?culty for the example dynamics Using a four standard deviation gate \(? = 16 PG = 0.99994. For the example system, nz = 1, cnz = 2 and Vk = 8 |S\(k D. Experimental Tracking and Track-lost Regimes The following experimental setup allows for  controlled  track-loss and is used to verify operation of the trackloss detector. Until timestep 1000, the ?lter is tracking in the sense that PD = 1 \(the truth measurement is always 


available to the gating test 2000, PD = 0 \(the truth measurement is never available to the gating test sense that the truth measurement is never gated. PFA can then be calculated using measurement innovations from the rst 1000 timesteps, and PDET from using measurement innovations from the second 1000 timesteps, with roughly 1000 values of s2?j being tested by the decision rule in each regime. Note that PDET is thus calculated using both data points from the transient of  controlled  track-loss as well as from the  steady-state  operation of the ?lter in the tracklost regime E. PDAF SIRF Tracking Approximation For the example system with tracking regime assumptions: PG ? PD = 1, q1 ? PD = 1 [3], and \(25 T\(?Vk cnz 2  mk=1 exp\(??Vk Vk mk ? 1   nz nz/2  I2\(mk 49 Figure 1 shows the resulting function for ?T\(?Vk 0 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Vk   T  L Fig. 1. ?T and ?L as functions of ?Vk for example system ating \(4 6 7 11 24 using \(49 T\(?Vk  T = 0.9602, ?V  k 0.1047, and P  T?sirf  0.3286 2.1122 2.1122 20.5492  ST?sirf = 0.4286 Simulating and then averaging P\(k|k? 1 k example PDAF tracking regime over 1000 timesteps gives P  T?sim  0.3210 2.0831 2.0831 20.3698  ST?sim = 0.4210 Thus ST?sirf has only 1.7% error when compared to the more time-consuming simulation results F. PDAF SIRF Track-lost Approximation Figure 1 shows the function for ?L\(?Vk system with track-lost regime assumptions. Iterating \(4 6 


7 11 30 31  L = 0.3264 V  k = 0.4594, and P  L?sirf  8.1431 15.8765 15.8765 56.2960  SL?sirf = 8.2431 Simulating and then averaging P\(k|k? 1 k example PDAF track-lost regime over 1000 timesteps gives P  L?sim  9.1253 17.3318 17.3318 54.3980  SL?sim = 9.2253 SL?sirf has 10.8% error when compared to the more time-consuming simulation results. In general, it has been observed that the approximation for the tracking regime is more accurate than for the track-lost regime when compared to the simulation results 4321 G. Neyman-Pearson Decision Rule PDET ? 0.99 and PFA ? 0.01 are reasonable targets for the decision rule. The number of effective innovations n necessary to meet these goals can be calculated by incrementing n until, for some value\(s and PFA ? 0.01 \(see Figure 2 the slope of the curve is ?np 0 0.005 0.01 0.015 0.02 0.975 0.98 0.985 0.99 0.995 1 target PD &amp; PFA PFA P D n=6 n=7 n=8 Fig. 2. Neyman-Pearson Threshold Curves As can be seen from the ?gure, the goals require n ? 7 timesteps. A good value for ?np \(one that exceeds the goals for both PDET and PFA 41 43 the means of 25 trials of the example experimental setup and the decision rule \(38 39 ST?sirf = 0.4286, and SL?sirf = 8.2431, the experimental results were PDET = 0.9997 and PFA = 0.0132. Though the actual process of track-loss is dif?cult to quantify, the experimental results n  T = 7.0 and n  L = 24.9 do provide some insight into the number of timesteps necessary for track-loss detection, as the average number of timesteps necessary, n  Loss, should be bounded by n  T ? n  Loss ? n  L V. SIMULATION RESULTS Table 1 provides a comparison of experimental values for PDET, PFA, n  T, and n  F across large ranges of NTA and NCD for PDAF simulation data when PD = 1. For each NTA-NCD combination, n and ?np have been chosen such that, theoretically, PDET ? 0.99 and PFA ? 0.01, with n as small as possible. The mean track-lifetime of all NTA-NCD combinations is approximately 100 timesteps where NCD is labeled  High  1000 timesteps where NCD is labeled  Medium  and 10,000 timesteps where NCD is labeled  Low  12]. The experimental values of PDET, and PFA were calculated using the means of 25 trials of the example experimental setup; trials were selected from realizations where track-loss did not occur until forced at timestep 1001 


where track-loss did not occur until forced at timestep 1001 Having the tracking and track-lost variances of the innovations spaced well apart is the condition for small n As Table 1 shows, this condition is met less often for low values of NTA In the track-lost regime, the assumption that the validated prediction errors are Gaussian distributed is usually quite conservative since the sample variances are generally larger than for a Gaussian distribution. This contributes to generally exceeding the PDET goals. Conversely, the sample variances in the tracking regime, while more Gaussian are still somewhat larger than would be expected from a Gaussian distribution, meaning that the PFA goals may not always be met. However, because n is restricted to integer values, the theoretical values of both PDET and PFA often exceed the desired values signi?cantly for low values of n Unlike the tracking regime, the track-lost regime cannot be described solely in terms of PD , NTA, and NCD. SL is non-linearly dependent on ?, so n, PDET, and PFA vary with ? as well. This can be seen by comparing the Medium NTA results in Table 1 with those in Table 2, where identical values of PD , NTA, and NCD constructed from different values of Q, R, and ? yield different results for n In general, lowering PD increases the tracking regime innovations variance ST?sirf , reducing the separation from SL?sirf and thus having the tendency to raise the required n to meet the goals for PDET and PFA. This can be seen by comparing the Low NTA data in Table 1 \(PD = 1 Table 3 \(PD = 0.9 Over the parameter space explored, the test \(decision rule VI. CONCLUSION AND ONGOING WORK A strategy has been laid out for creating a two-class decision rule to determine the regime of operation for the PDAF in the absence of truth data. Scalar information reduction factors can be used in an iterative scheme to predict the steady-state innovations covariance for both the tracking and track-lost regimes, which results in lower computational burden when compared to Monte Carlo simulation. Then a distribution can be assumed for the sample variances of the prediction errors. Together, these pieces of information constitute a model around which a Neyman-Pearson decision rule can be constructed, where the con?dences in both the probability of track-loss detection and of false alarms are explicitly chosen. Good performance of the test as a trackloss detector was demonstrated for an example system over a large range of tracking dif?culties Ongoing work includes modeling the effective innovations \(45 theoretical distributions for the prediction errors used in the decision rule \(38 desirable to more accurately model the sample distribution of the innovations variance, and it has been shown that the prediction errors of many data association algorithms can be well approximated by a Gaussian mixture [13 REFERENCES 1] Y. Bar-Shalom and T. Fortmann, Tracking and Data Association, Academic Press Inc., 1988 2] T. Fortmann, Y. Bar-Shalom, and Y. Scheffe  Sonar Tracking of Multiple Targets Using Joint Probabilistic Data Association  IEEE J. of Oceanic Engineering, July 1983 4322 TABLE I SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.152e3 1.153e3 1.073e4 5.963e3 11 11.0 336 0.591 0.9900 0.0081 1.0000 0.0099 5e-5 1e-4 0.1 Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





