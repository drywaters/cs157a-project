DASWIS'02 1 Discovering Associations in XML Data Amnon Meisels Michael Orlov and Tal Maor Department of Computer Science Ben-Gurion University of the Negev Beer-Sheva 84105 Israel am,orlovm,maort@cs.bgu.ac.il Abstract\227 Knowledge inference from semi-structured data can utilize frequent sub structures in addition to frequency of data items In fact the working assumption of the present study is that frequent sub-trees of XML data represent sets of tags objects that are meaningfully associated A method for extracting frequent sub-trees from XML data is presented It uses thresholds on frequencies of paths and on the multiplicity of paths in the data The frequent sub-trees are extracted and counted in a procedure that has 
000\002\001\004\003\006\005\b\007 complexity The data content of the extracted sub-trees in the form of attribute values is cast in tabular form This enables a search for associations in the extracted data Thus the complete procedure uses structure and content to extract association rules from semistructured data A large industrial example is used to demonstrate the operation of the proposed method u 1 I NTRODUCTION There are three main features unique to XML 002les that can be explored for inferring knowledge t 
The topology of the data 227 its graph structure t The frequency of tags and of graph-connected tags t The values of tags 227 attributes that are attached to tags The main idea is that tags that are connected topologically i.e belong to some subtree of the XML data are 224meaningfully\224 related For example the tag 224 staff-member 224 appears frequently with a subtree that includes three branches with tags 224 name 224  224 phone 224 and 224 of\002ce 224 By discovering the fact that this subtree repeats itself in the data one can infer that it is an object with a meaning Tags that are topologically connected can be investigated further in two combined directions One di 
rection looks at the meaning i.e natural-linguistic meaning of the tags found to be related The other direction is based on the statistical distribution of attributes value within the set of related tags Values of Tags typically take the form of attributes and it is common to have more than one attribute per tag Our proposed procedure for discovering sets of related tags is composed of two parts One part calculates frequencies of tags within paths on the XML data It prunes its 224pathswith-frequencies\224 by thresholding these frequencies relatively to their topological connections It also achieves a useful pruning of paths by analyzing the distribution of frequencies of attributes values As will be seen in Sec 3 the extraction of attributes and values involves complex steps sometimes using 
text strings as key words for comparison of values The product of the 002rst part of our proposed procedure is a set of paths of tags that are frequent their attribute values frequent and topologically connected on a path These paths can now be combined into subtrees and the frequency of these subtrees in the XML data can be counted Our proposed interpretation is that frequent subtrees represent related objects that contain some knowledge The issue of counting of subtrees is computationally complex and has been attempted by a few researchers in the last two years We believe that we improve on former proposals for locating frequent subtrees in XML data by using the above three features together Our proposed extraction and counting algorithm behaves like the square of the number of 
Tags Former proposed algorithms for counting subtrees creating all potential frequent k-trees were clearly exponential 2 R ELATED WORK There are two recent papers that address the search for frequent objects in XML data 10 W ang and Liu used the same algorithm in both papers In the y searched on HTML data that was constructed from a movies database IMDB at http://us.imdb.com  by an SQL query 223Take the 5000 movies that were made from 1950 to 1998 in the US\224 The resulting tree is assumed to be known  so that counts subtrees that start from a selected node in the tree of records The present study assumes no knowledge about the structure of the 
XML data and looks for frequent subtrees that start anywhere on the data tree Consequently the de\002nition of the term frequent is different in 10 and in the present study  WL start from frequent paths with one leaf and construct all possible trees with two leaves three leaves etc This constructive algorithm is obviously exponential in the number candidates and counts all possible embeddings of subtrees In the proposed algorithm of the present study we construct subtrees bottom up starting with tags that were found frequent and adding to them more frequent tags see Sec 3.3 As a result of this difference our proposed algorithm is not exponential Another recent study 7 b uilds on top of the frequent tree counting method of It looks for multi-le v el associa 
tion rules extracted from the frequent trees of Frequent subtrees have also been derived from HTML data by Tags of HTML 002les are considered both as an attribute and as a value For example in the tag td>Position</td  Proceedings of the Third International Conference on Web Information Systems Engineering \(Workshops 0-7695-1754-3/02 $17.00 \251 2002 IEEE  


2 DASWIS'02 td>IT Specialist</td  the attribute name is Position and its value is IT Specialist  With this approach names construct trees of hierarchical classes where leafs are text between two tags Starting from HTML data and removing structures that are assumed to be irrelevant such as images or sound Laur et al  also propose to count frequent subtrees In 5 trees are constructed from paths i.e one leaf and combined to form trees The term frequent similarly to is de\002ned to be the number of appearances of a path that is higher than a given threshold Each transaction contributes at most one count of the tree frequency In a subsequent stage nodes of frequent trees are organized and indexed according to their levels in the tree The most recent published method for 002nd frequent sub-trees in semistructured data operates on either XML or HTML data  It 002nds a frequent pattern tree in the original data tree by starting with one frequent node and expanding it recursively A pattern is de\002ned to be frequent by according to the count of its root instances Support is de\002ned as the ratio of this count to the number of nodes in the data and a threshold for support is used to extract frequent patterns The path of a 223web-surf\224 that follows the list of sites a user moved through can also be analyzed for frequent sub paths Here two somewhat different approaches appeared recently  4 One approach generates a tree per user  where all paths generated by a single user over time are combined into a tree The other approach generates a tree for each instance of 223websur\002ng\224 thus creating a very large number of trees paths Chen et al look for frequent instances of trees among users Here frequent is taken to mean that the number of instances is above a given threshold Lin et al on the other hand de\002ne frequency and support on web paths Each visit contributes one count to the support of a node in a given path Frequent paths are combined to construct frequent trees The method proposed in combines tw o paths or trees if the y dif fer by at most tw o nodes 3 E XTRACTING FREQUENT TREES The proposed extraction procedure for frequent subtrees starts with the extraction of frequent paths All statistics use the tags of the XML data and some thresholds on the frequency of attribute values within tags The paths are pruned and written to a meta-data XML 002le Next a pruning tree is constructed from the extracted paths It includes all of the frequent paths but its parts do not necessarily exist in their simple form in the data see Sec 3.2 The pruning tree is used to prune the data tree Each frequent path is scanned top-down on the full data tree to 002nd all paths on the pruning tree that are connected to it The result of this procedure is a pruned data tree A tree that actually appears in its full form in the XML input data which is composed only of frequent paths The third and 002nal stage is to count frequent subtrees on the pruned data In this stage the frequencies of subtrees that are composed only of frequent paths and that appear as a whole in the real data are counted This is done by a low complexity counting algorithm that uses the table of frequent trees created during the second stage The three stages are described in detail in the following subsections 3.1 Frequencies of paths Statistics over tags their attributes and attributes values generate our de\002nition of frequent paths as follows t For each tag on a distinct path identi\002ed uniquely by the list of tags/nodes from root to the tag being counted the count of its appearances is computed for example tag book appears 000\002\001\004\003\004\005 times on path library/shelf/book  t The total count is calculated for each attribute of each tag For example the book above could have attributes date and ISBN  the former with count of 000\006\001\007\003\004\005  and the latter with count of 000\b\003\t\000\b\005  t The counts for each value of an attribute are also calculated For example date has the value 1984 forty times and the value 1899 only a single time For the purpose of the statistics above text nodes are considered as attribute nodes with automatically generated names such as text-node-1  Here is a sample of typical results of the path frequencies statistics the actual result contains a variety of additional statistics that are omitted here Note that the number of occurances of the tag 223SPN\224 of the attribute 223text-node-1\224 within this tag and the number of occurances of the speci\002c value 223SEL FROM\224 of this attribute are all equal to 239 This particular real example re\003ects the fact that in an 223industrial\224 XML database it frequently occurs that in all occurances of a speci\002c tag there occurs also the same attribute and the same value for this attribute xml version="1.0 encoding="UTF-8 frequencies xmlns:kite="http://www.kite.org.il tag name="SFN count="239  path-per-tag name="SFN path="/EIPC/FIGURE/PARTLIST_SECTION/PART/NOM_COL/SFN count="239 attr-per-path name="text-node-1 count="239 value-per-attr-per-path value="SEL FROM count="239 attr-per-path path-per-tag  tag  frequencies 3.2 Pruning tags and paths For our 002nal goal we want to combine the discovery of frequent XML subtrees and of interesting relations among the values of attributes of those frequent subtrees To this end we must determine in some way which tags and attributes are 223interesting\224 This is best done by using both the topology of the XML data and the distribution of values within its tags In the present discovery procedure we apply two types of criteria one looks at the content of the tag and the other examines the relative frequency of the tag i.e relative to frequencies on its path The criterion for the content of tags examines the distribution of values of attributes and retains values that are more frequent than some threshold The criterion for frequency of tags uses a threshold on the ratio of frequencies of tags along paths The criteria are applied in the form of a pruning procedure by the algorithm that searches for frequent tags The algorithm prunes the frequent tags by using the three following steps 1 For each tag the ratio of its count determined in Sec 3.1 to its parent tag count is calculated If the ratio exceeds Proceedings of the Third International Conference on Web Information Systems Engineering \(Workshops 0-7695-1754-3/02 $17.00 \251 2002 IEEE  


DISCOVERING ASSOCIATIONS IN XML DATA 3 a given threshold the tag is not pruned Tags that are not pruned are checked in the following step for 223interesting\224 values to their attributes Pruned tags are removed together with the rest of their paths Note that frequency ratios of over 000\b\005\004\005 001\000 signal multiplicity of descendants on the average If on the average for each path A-B there is a corresponding path A-B-C  it probably means that many B on the path A-B have a child C  and thus C is essential for the frequent subtrees counting procedure 2 Attribute values that have a relative frequency i.e relative to their total count exceeding a given threshold are considered 223interesting\224 For example for a threshold of 002 000  the value 1984 in the example of Sec 3.1 would be considered 223interesting\224 and the value 1899 in the same example would not Attributes must have at least two 223interesting\224 values in order to be considered 223interesting\224 This is because if an attribute has only a single dominant value it cannot yield associations or knowledge Tags that are not 223interesting\224 are pruned by a recursive procedure that is described as part of the next step The reason for this step is that if an attribute has either many 223insigni\002cant\224 values or one value only it will not generate any meaningful associations Since we want 223interesting\224 values for each attribute if there are none we prune the tag The meaning of 223insigni\002cant\224 is de\002ned by a suitable threshold 3 Tags that are not 223interesting\224 are pruned only if they do not posses an 223interesting\224 descendent tag In other words tags remain in the data tree and thus participate in the counting of subtrees only if there is a descendant tag with 223interesting\224 attribute\(s or the tag has an interesting attribute itself The example in Fig 1 demonstrates the results of applying the pruning principles described above In the 002gure of the data tree boxed nodes are nodes that have 223interesting\224 attributes determined by a threshold on the distribution of their values  library book book book journal journal author ISBN author author date date Fig 1 Visualizing determination of 223interesting\224 tags Boxed nodes are those that have 223interesting\224 attributes Let us assume that the threshold which is used in Step 1 above is 003 005 004\000  Following the procedure of determining interesting tags we 002nd that the path library/book/ISBN is below the 003 005 004\000 threshold this path occurs only once while its ancestor library/book occurs three times Next library/journal/date has no 223interesting\224 attributes it's not boxed in the 002gure and it also has no descendants therefore this path is also 223non-interesting\224 It can be seen that for the other paths all the requirements hold and thus the paths that remain are t library t library/book t library/book/author t library/journal From all the paths that were kept as part of frequent tags by the procedure in Sec 3.1 a tree is constructed All 223noninteresting\224 tags and attributes are not included in the tree In other words tags that were not found to be 223interesting\224 by the recursive procedure of Sec 3.2 are pruned together with subtrees that are rooted at those tags We term the result the pruning tree  The pruning tree for the data tree in Sec 3.2 is shown in Fig 2 The algorithm for building the pruning tree is Alg 1  library book journal author Fig 2 Pruning tree resulting from uniting the 223interesting paths\224 from the data tree in Sec 3.2 Algorithm 1 B UILD P RUNING T REE  P A T H L I S T  Pre-condition PathList is a list of all different paths in the data tree Post-condition 005 is the pruning tree that adheres to the principles described in this section 1  Next step before the beginning of the recursion  2 005\007\006 Tree built by uniting the paths in PathList 3 if 005 count b 005 parent count t given threshold then 4  Erase 005 from its parent's list of child subtrees  5 else 6 for all 005\013\n\r\f child subtrees of 005 do 7 B UILD P RUNING T REE  005 n  8 for all 016\017\f attributes of 005 do 9 if 016 doesn't have at least two 223interesting\224 values then 10  Erase 016 from the list of attributes of 005  11 if 005 has no child subtrees 020\021\005 has no attributes then 12  Erase 005 from parent's list of child subtrees  The pruning tree is a combination of frequent paths which satisfy some requirements In order to be able to count real subtrees that actually appear in the data we have to use the original XML tree which we will call the data tree  The pruning tree is utilized for the purpose it was built for 227 pruning nodes in the data tree  Alg 2 presents the way to do it traverse the data tree simultaneously with the pruning tree  and throw from the data tree all the tags and their subtrees that do not appear in the pruning tree  The algorithm needs one pass from top to bottom on the data tree  to produce the pruned data tree  At the end of this step all the nodes of the pruned data tree contain tags which are 223frequent\224 This means that some of these nodes have Proceedings of the Third International Conference on Web Information Systems Engineering \(Workshops 0-7695-1754-3/02 $17.00 \251 2002 IEEE  


4 DASWIS'02 223interesting\224 attributes and/or values or one of their descendant nodes does Algorithm 2 P RUNE D ATA T REE  D A T A T R E E  P R U N I N G T R E E  Pre-condition dataTree is the original data tree  pruningTree is the constructed from all frequent paths  Post-condition Pruned data tree is returned In this tree each node contains a representation of the subtree rooted at this node as well as the representation of the values of nodes on that subtree We use such representations so that order of children in the subtree won't matter and for the purpose of saving space as well 1 processedTrees 006 001\000 2 dataSubTrees 006 dataTree.children 3 for all 002 f 004\003\006\005\b\007\006\005\n\t\f\013\n\r\006\016\020\017\022\021\n\021\\024\023 do 4  we only consider Element nodes of the XML data tree ignoring Comment and Text nodes  5 if 002 is an Element node 020 002\026\025 027\024\030\020\003\006\021\032\031\034\033 f 035 017\020\013\020\027\037\036 \027\006!\n\016\020\017\022\021\n\021"\025$\#&%\037\036\f'\(\003\020\017\022\021\(\027\037\031 then 6 processedTrees 006 processedTrees   P RUNE D ATA T REE  002  002 n   7 return New subtree with corresponding representation and values 3.3 Finding frequent subtrees The counting of subtrees is a complex process that performs two tasks at the same time counting of subtrees and storing of all appearances of the counted subtrees The different appearances of the same subtree relates to the existence of different values of attributes in the same tags of the two sub-trees The counting algorithm keeps two global tables that store subtrees in tabular one row per sub-tree form For ef\002ciency these tables are hash-tables that store and retrieve similar subtrees in 031 000 033  In our implementation these are standard Java HashMaps using the standard Java hashCode for string representation The 002rst table stores every possible subtree in the pruned data tree  It has one row per each different subtree in the pruned data-tree The second table stores all the appearances of the data of each subtree in the pruned data tree  Each subtree that has a count of  in the data will have  different rows in the second table This will enable the retrieval of the actual attribute values for algorithms that look for meaningful associations in the frequent sub-trees The counting algorithm traverses the pruned data-tree from top to bottom recursively Each visited node tag causes the algorithm to insert the relevant structure of the sub-tree into the two tables For example if the string 0 A 0 B 120 C 0 D 12121 is inserted into the 002rst table this represents the traversal of a sub-tree in which B and C are children of A  and D is a child of C  The entry is inserted when the algorithm has traversed this sub-tree and is pointing at node A for the second time If the subtree already exists in the counting table the counter will be updated At this point in the run of the algorithm the subtree's attributes and their values are inserted into the second table For example 0 id="123 0 attribute1="b1",attribute2="b2 12020 no="5 12121  In the description of Alg 3 the term SubtreesRegistry refers to the two global tables described above One for counting subtrees and the other for lists of subtrees 223valuetuples\224 In the above example when a subtree is represented as 0 A 0 B 120 C 0 D 13121  the representation is unique for all topologically equivalent subtrees the order of children is not important Assume that in the 002rst table this subtree has the count of 003  In the second table it might have the following list of values 0 id="123 0 attribute1="b1",attribute2="b2 13020 no="5 12131 0 id="456 0 attribute1="b1",attribute2="b3 13020 no 12131 0  1 In the 002rst list entry C has no value or attribute while B has two attributes For completeness a missing attribute value at some tag is stored as the value none  Here the term 223missing\224 relates to attribute values that appear at some of the appearances of a frequent sub-tree and are 223missing\224 in others This will be useful later when all attribute values are transferred to some data mining algorithm for extraction of interesting associations and empty 002elds are problematic The worst case complexity of this algorithm is 0314/65&\033 and this happens when the data tree becomes one path The algorithm scans the data tree bottom-up When the algorithm starts it picks up a leaf and updates the SubtreesRegistry tables Its next step is to combine the leaf with the immediate subtree it belongs to The algorithm updates the SubtreesRegistry accordingly and repeats this step until it reaches the root of the data tree The worst complexity of each step is 0314/7\033  writing the whole data The number of steps is the depth of the tree which is  at the worst case Algorithm 3 C OUNT S UBTREES  P R U N E D D A T A T R E E  S U B T R E E S R E G I S T R Y  Pre-condition PrunedDataTree is the pruned data tree data with eliminated 223non-interesting\224 nodes and SubtreesRegistry is structure with two tables as described before Post-condition The two tables in SubtreesRegistry  one with subtrees counts and another one with subtrees values list are updated for all subtrees in PrunedDataTree  1 8\020\017\020\013\n\027\024\021\020\003\b9\022\005\b\007\022\005\n\t\(\013\\n\r\006\016\n\017\022\021\020\021\022\023 006 8\n\017\020\013\n\027\024\021\b\003\0209\022\005\b\007\022\005\f\016\n\017\\022\021\n\021"\025$#&%\037\036\026'\b\003\020\017\006\021\(\027\037\031\\034\033 2 for all 002 f 8\n\017\020\013\n\027\024\021\b\003\0209\022\005\b\007\022\005\020\t\f\013\\n\r\006\016\n\017\006\021\020\021\024\023 do 3 C OUNT S UBTREES  002  S U B T R E E S R E G I S T R Y  4  rep is this subtree representation and valueRep is the corresponding subtree values representation  5 SubtreesRegistry.addCount\(rep 000  6 SubtreesRegistry.addValue\(rep,valueRep 4 I MPLEMENTATION AND E XPERIMENTATION To test the proposed algorithms and their implementation we ran some experiments on XML data The standard XML datasets are no good for our approach Take the famous IMDB for example it's data is composed of a few original large tables translated into XML format This means that all subtrees are similarly frequent  In other words all subtrees of tags correspond to tuples in an original database table Correspondingly they all have the exact same frequency for all sub-tuples hence Proceedings of the Third International Conference on Web Information Systems Engineering \(Workshops 0-7695-1754-3/02 $17.00 \251 2002 IEEE  


DISCOVERING ASSOCIATIONS IN XML DATA 5 no 223interesting\224 subtrees with widely different frequency of appearance in the data Our experiments were run on a large industrial XML data 002le that stores a large manual of an aviation company This particular dataset generated probably mechanically has little hope for extraction of meaningful associations but is at least a true XML data tree with subtrees widely differing in their frequency of appearance The input data includes a total of 382,373 instances of 62 different tags in a tree of depth 8 The parameters of the XML data tree are given in table I Depth 000 001 002 001 Num of Tags 002 000\003\001 003 003 005\004\007\006 b\000 002 001 002 003 003\001\b\006 001 002 003 001\t\004\007\006 003 001 003 Depth 003 003 002 Num of Tags 002 000 005\006 002 000\b\001 000 n\004\013\000 003 002 002 000 TABLE I D ISTRIBUTION OF TAGS IN THE INPUT XML DATA TREE The result of running Algorithms 3 for 4 different cases of threshold selections are presented in Table II For all the cases in Table II the similarity threshold for identifying the discovered frequent subtrees is 90 and the required minimal number of trees is 5 The table presents the number of subtrees with different topologies that were found to be frequent in the data case attribute threshold tags threshold different subtrees 1 f\016\r\020\017 f\016\r\020\017 021 2 f\016\r\020\017 022\020\017 023\024\f 3 025\020\022\020\017 f\016\r\020\017 023 4 f\016\r\020\017 025\020\022\020\017 021 TABLE II R ESULTS OF RUNNING A LGORITHMS 1 2 3 The exact frequency distribution of the resulting subtrees that were found to be frequent is detailed in Table III Entries in Table III give the number of instances of frequent subtrees with a given number of Tags Tags are counted in leafs of the subtree case 1 tag 2 tags 3 tags 4 tags 1 000 n\004 000 000 026\001 000\002\005 001 003 003\004 000 004\003 013\002\005\000 000 027\001 007\001 2 004\013\004 001 004\005\000 000 003 000 000 002 002 002 003 002 3 001 t\002 003 4 000 n\004 000 000 026\001 000\002\005 001 003 003\004 000 004\003 013\002\005\000 000 027\001 007\001 case 5 tags 8 tags 9 tags 16 tags 1 001 2 002 001 004 3 4 001 TABLE III D ISTRIBUTION OF FREQUENT SUBTREES To illustrate the process let us take a look at a particular subtree that was found to be frequent Figure 3 The subtree in Figure 3 has 4 tags/leaf-nodes and 175 instances of this subtree were found in the data Taking the multiple instances of the data i.e the values of its attributes one can attempt to discover meaningful associations Analyzing the 175 data tuples of the frequent tree of Figure 3 for association rules and using a support threshold of 002 005 004\000 and a con\002dence threshold of 004 004\005 001\000 we get 29 association rules Examples of association rules are 000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000 Fig 3 Example1 description 1 If PART-->UPA-->SUBUPA=1 Then PART-->NOM\\_COL-->DOT support 32.39 confidence 95 2 If PART-->NOM\\_COL-->DOT Then PART-->NOM\\_COL-->REFINT\(2 support 81.25 confidence 91.67 3 If PART-->NOM\\_COL-->REFINT\(1 Then PART-->NOM\\_COL-->DOT  PART-->NOM\\_COL-->REFINT\(2 support 81.25 confidence 94.08 The analyzed XML data describes presentation modes of text and graphics of a large manual for some sub systems of an airplane The attribute DOT describes modes of presentation of parts whether they are composed of several sub-presentations that can be zoomed-in or not The value 223.\224 means that the presentation has no zoom-in capabilities The 002rst rule means that when only one subpart is present then the presentation does not include capabilities for presenting sub-parts The second rule means that when the presentation is 223simple\224 then no pointer to a second sub-part exists The third rule is similar to the 002rst and second one but connects the fact that there is no sub-part with the 223simple\224 mode of presentation and the non existence of a second pointer to a second sub-part A much more frequent subtree that was found has 8126 instances in the data has 3 tags leaf-nodes and is presented in Figure 4 Using a threshold support of 002 005 004\000 and con\002dence of 004 005 004\000 we get from the 8126 instances of 3-tuples of data 8 rules As can be seen the 2 example rules are very similar to the previous 3 rules 000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\\000\000\000\000 Fig 4 Example2 description 1 If PART-->UPA-->SUBUPA=1 Then PART-->NOM\\_COL-->REFINT=null support 23.38 confidence 93.09 2 If PART-->NOM\\_COL-->DOT Then PART-->NOM\\_COL-->REFINT=null support 82.66 confidence 96.25 Proceedings of the Third International Conference on Web Information Systems Engineering \(Workshops 0-7695-1754-3/02 $17.00 \251 2002 IEEE  


6 DASWIS'02 5 C ONCLUSIONS The top-down scheme of our frequent trees extraction method and its resulting value-tuples for mining associations can be listed as follows 1 Perform counts of all tags and their paths on the XML 002le and create a new XML 002le that includes the frequencies of paths and tags This generates useful meta-data  2 Prune 224non-interesting\224 attribute values Attributes that are either 223rare\224 or 223too-frequent\224 3 Use thresholds on relative frequencies to prune tags/paths This will prune subtrees that have low multiplicity less frequent instances 4 Combine frequent paths into subtrees generating a 224pruning tree\224 from the data 5 Prune the data tree retaining only subtrees that are in the pruning tree 6 Count subtrees in the pruned data-tree using an 031  5 033 algorithm 7 Construct tables of values of the frequent objects tuples thereby enabling procedures of data mining on the attribute values to be performed The above scheme for extracting and counting frequent subtrees in XML data differs from former studies in two aspects 1 The extraction and counting procedure has low computational complexity 2 The selection criteria for 223interesting\224 sub-trees involves considerations that relate to the statistics of the content of the data in the XML tags For the counting of sub-trees a viable comparison of the proposed method is with that of WL WL construct and k eep entire paths and the subtrees that are constructed from the frequent paths This procedure is clearly exponential in the number of data nodes In the proposed method subtrees are constructed from the bottom up Two paths are combined if their leaves are brothers in the XML tree As explained in section 3.2 this computation is linear in the input XML data The counting of sub-trees in the proposed method also differs from that of WL Here the number of appearances of combined paths in the XML tree is calculated and its frequency determined If it is frequent it is inserted into the table of frequent trees This procedure is performed recursively until a maximal sub tree is achieved The algorithm of WL removes sub trees that can in principle be just partial trees to other frequent trees or frequent trees with a different order of some branches The second major difference of the present extraction method from former studies is its consideration of discovery An important part of the pruning procedure relates to the content of the XML tags Using a threshold on the distribution of values of attributes  is clearly a non structural criterion Its rational is based on our assumption that the 002nding of frequent sub-trees is part of a wider goal discovering forms of knowledge in semi structured data Our speci\002c choice is to attempt to discover association rules cf 2 in the e xtracted data that w as found to be frequent i.e frequent sub-trees The proposed method assumes that the values of attributes are the content of the XML data This is the reason for pruning paths and potential sub-trees that have a value distribution of attributes that will not yield any meaningful associations The method proposed in the present paper has been implemented in Java and has an interactive part that enables the user to select the relevant thresholds It also outputs the resulting pruning trees and frequent sub-trees for the user to inspect and interact with The graphical user interface of the implemented system for extracting frequent tags and constructing the pruning tree is shown in Fig 5 As can be seen the frequencies of attribute values are presented on the screen The pruning tree appears on the screen as a combined set of frequent paths that resulted from the thresholds selected by the user in the top part of the form This pruning tree will be combined with the data in order to produce real counts of subtrees in the data as described in Sec 3.3 Fig 5 Graphical User Interface showing a pruning tree R EFERENCES  Rak esh Agra w al and T omasz Imielinski and Arun N.Sw ami Mining Association Rules between Sets of Items in Large Databases  Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data pp.207\226216 1993  Rak esh Agra w al and Ramakrishnan Srikant Fast Algorithms for Mining Association Rules  Proc 20th Int Conf Very Large Data Bases VLDB pp.487\226499 1994  T atsuya Asai et al Ef\256cient Substructure Discovery from Large Semistructured Data  Proc 2nd SIAM Intern Conf on Data Mining SDM'02  pp 158-174 Arlington VA April 2002  Ming-Syan Chen and Jong Soo P ark and Philip S Y u Ef\256cient Data Mining for Path Traversal Patterns  Knowl Data Eng  vol 10 pp 209-221 1998  P A Laur  F  Masse glia P  Poncelet and M T eisseire A General Architecture for Finding Structural Regularities on the Web PS  Proc 9th Intern Conf on Artif Intell AIMSA'2000 Varna Bulgaria September 2000  K Maruyama and K Uehara Mining Association Rules from SemiStructured Data  ICDCS Workshop on Knowledge Discovery and Data Mining in the World-Wide Web 2000 F23-F30   K Maruyama and K Uehara Knowledge Integration of Rule Mining and Schema Discovering  Proc Third Intern Conf on Discovery Science DS2000  pp.285-289 2000  X Lin C Liu Y  Zhang and X Zhou Ef\256ciently Computing Frequent Tree-Like Topology Patterns in a Web Environment  31st Tool's Asia  IEEE cs press pp 440-447 1999  K e W ang and Huiqing Liu Discovering typical structures of documents a road map approach  Proc 21st ACM SIGIR Conf on Res Dev in Inform Retrieval SIGIR'98 pp 146-154 Melbourne Austrailia August 1998  K W ang and H Liu Discovering Structural Association of Semistructured Data  IEEE Trans Knowl Data Engin  vol 12 pp 353-371 2000  W ai-ching W ong and Ada W ai-Chee Fu Finding Structure and Characteristics of Web Documents for Classi\256cation  ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery pp 96-105 2000 Proceedings of the Third International Conference on Web Information Systems Engineering \(Workshops 0-7695-1754-3/02 $17.00 \251 2002 IEEE  


Comparielon of Seif-Similarity CUNOS Gazallt 0 10 20 30 40 50 BO 70 80 Sample size a Gazelle Camperision Ot Sell-Similarity Curves T1014D5M s 0.8 E 0.75 I 0.7 0.55 0.55  0.1 RC-S  0.6 O.l%A-S  O.l%A-SS  r 0.1 RC.SS 0 10 20 30 40 50 BO 70 80 Sample size b VBwk Cornparision 01 Self-Similatity Curves: TBISDZSM 0.05 RC-S  0.05 A-S  0.05 A-SS D  0.75 0.7 0.65 0.05 RC-SS  0 10 20 30 40 50 BO 70 80 Sample Sire tc T1014D5M  I 0.75 0.08 RC-S  0.08 A-S  0.08 A.SS 0  0.6 0.55 0.5 0.08 RC-SS   0 10 20 30 40 50 BO 70 Bo Sample size d TBI5D25M Figure 3 Evaluation of Proposed Method Similarity plots PerbmanceCawkkn RCn A VT!d P&manceCanpamlon UCn A:T815025M IC f F s 9 Y if E 5 0.1  C 9  0.1XA  k _ o.l%Rcs  wm 0.lXRCSS  0.IXRC-S  I  IO M B U M MI 70 80 WIW 0 10 M 30 10 M MI 70 80 90100 saw IO sanp1e sa0 Sam sa a VBook b T1014D5M c T815D25M Figure 4 Impact of Representative Set on Performance Sampling MethOdolcgy Performance T1014D5M:0.05 sup SBrnpling Melhodobgy Breakdown T1014D5M:0.05 0 10 20 30 40 50 80 70 80 90 1W 0 10 20 30 40 50 60 70 80 90 100 Sample Sue Sample Size a Overall Performance b Breakdown Figure 5 Sampling Methodology on Performance Breakdown 360 


of 73.17 125 for RC-SS seconds to compute the results The approach using the entire association set requires 140 seconds to compute the same result When compared with the baseline execution time \(1730s the representative class approach\222s improvement factor is around 25 Impact of Sampling Methodology In Figure 5 we con sider the performance of the sampling methodology pre sented in Section 3 Figure 5a compares the cumulative execution time performance of our the approach using the representative class to identify the ideal sample size while overlapping the sampling and U0 operations of the next stage with the computation of the current stage In Figure 5a we compare the performance of ideal situation where there is no sampling overhead TC-ideal with the actual perfor mance with overlapping \(TC-overlap and without overlap ping \(TC-nooverlap For the overlapping computation we used two scenarios, one where the processor handling the WO and sampling was a 300Mhz Pentium 113 i.e a more realistic scenario for active disks with a much slower pro cessor than the compute processor\and the other where the U0 processor was lGHz ideal baseline On viewing the graphs it is clear that the TC-ideal graph and the TC-overlap \(1GHz\are almost identical, reflecting the fact that almost all of the sampling overhead is over lapped with useful computation \(computing the association set for the previous sample size On relaxing the assump tion and allowing for the fact that the processor doing the sampling and WO is often much slower 300 Mhz we ob serve a marginal drop in performance \(slightly under 4 so still most of the sampling overhead is overlapped with useful computation Note that if we did not overlap the sampling overhead with computation then we perform 10 12% worse than the ideal \(comparing the TC-Ideal and TC noverlap graphs This experiment assumes the best pos sible sampling algorithm the Sample A algorithm We further broke down the performance of the sampling over heads in Figure 5b for the two configurations \(1GHz and 300Mhz The performance of the naive algorithm \(see Sec tion 3 is much worse than the Sample A algorithm 5 Conclusions and Future Work We have presented an efficient method to progressively sample for association rules Our approach relies on a novel measure of model accuracy \(self-similarity of associations across progressive samples\the identification of a represen tative class of frequent itemsets that mimic extremely accu rately\the self-similarity values across the entire set of asso ciations and an efficient sampling methodology that hides 3We did not have a dual processor system where one processor was fast and the other was slow We timed the sampling overheads for each of the different sample sizes for his dataset on the slower machine and we used these times by precomputing the samples and busy waiting for the necessary amount of time while evaluating the performance on this configuration the overhead of obtaining progressive samples by overlap ping it with useful computation We evaluated the results on a set of real and synthetic datasets We extensively benchmarked each aspect of our algorithm and obtained uniformly good performance several factor-fold execution time improvements across both real and synthetic datasets In the current work we have considered each sample to be independent of the other We would like to see if the pro posed method can be improved by using adaptive sampling techniques[S Other directions of future work have been outlined already in the text References I Anurag Acharya et 01 Active disks Programming model algorithms and evaluation In ASPLOS 1998 2 C Aggawal and P Yu Online generation of association rules In ICDE 1998 3 R Agrawal and R Srikant Fast algorithms for mining asso ciation rules In 20th VLDB Conf September 1994 4l Doug Burdick Manuel Calimlim and 1 E. Gehrke Mafia A maximal frequent itemset algorithm for transactional databases In ICDE 2001  5 Jason Catlett Megainduction A test flight In Machine Learning pages 59k599 1991 61 W G Cochran Sampling Techniques I Wiley  Sons-1977 171 G Das H Mannila and P Ronkainen Similarity of at tributes by external probes In KDD 1998 SI Carlos Doming0 and Osamu Watanabe Scaling up a boosting-based learner via adaptive sampling In PAKDD 2wO 9 1 Han and 1 Pei Yiwen yin Mining frequent patterns with out candidate generation In SIGMOD 2ooO IO Haussler Kearns Seung and Tishby Rigorous leaming curve bounds from statistical mechanics In COLT 1994 1 I George H John and Pat Langley Static versus dynamic sam pling for data mining In KDD 1996 I21 F Olken and D. Rotem Random sampling from database files  a survey In 5th Intl Conf Staristical and Scientific Dorubose Manugement April 1990 I31 S Parthasarathy Efficient progressive sampling for associ ation rules OSU CIS Technical Repon Number TR-OSU CISRC-5/02-TR13 November 2001 revised March 2002 I41 Foster 1 Provost David lensen and Tim Oates Efficient progressive sampling In KDD 1999 I51 Foster 1 Provost and Venkateswarlu Kolluri A survey of methods for scaling up inductive algorithms Duta Mining and Knowledge Discovery 3\(2 1999 I61 E Riedel G Gibson and C Faloutsos Active storage for large-scale data mining and multimedia In VLDB 1998 I71 Pradeep Shenoy et al Turbo-charging vertical mining of large databases In SIGMOD pages 22-33.2wO 1181 H Toivonen Samding large databases for association rules  In 22nd VLDB c 1996 1191 I S Vitter An efficient algorithm for sequential random sampling In ACM Trans Mathematical Siftware volume 13\(1 pages 58-67 March 87 ZO M I Zaki S Parthasarathy er al Evaluation of sampling for data mining of association rules In RIDE 1997 211 M 1 Zaki S Parthasarathy et al New algorithms for fast discovery of association rules In 3rd Inrl Conf on Knowl edge Discovery and Dora Mining August 1997 361 


  9 operation. In actuality the control of the hardware elements extends even further back to the TSX-5 and STEP heritage busses. These have over a dozen years more on-orbit success for the Air Force. Figure 9 shows several of these heritage satellites   The Glory hardware draws its heritage from the same series of satellites, namely the Defense Systems Inc. \(DSI CTA Space Systems \(CTASS\P bus architecture. TSX5 was an evolution from the STEP heritage, upgrading the electronics for radiation hardness. The TSX-5 mission is flying in a highly elliptical orbit that passes through the Van Allen belts. It has been flying for over 4 years in this environment. ACRIMSAT, OV-3 and VCL made only minor improvements to the TSX-5 designs  The reaction wheels are built by Orbital and are flying on the OV-3 satellite. The propulsion system was assembled and tested by Orbital and is identical to the OV-3 system More than 75% of the bus co mponents were built by Orbital at either the Dulles campus, or previously at the McLean or Germantown facilities. Orbital also manufactured the bus structure elements, including the aluminum honeycomb decks. The same methods were used on the extremely successful ACRIMSAT structure  In terms of the purchased components, most also have flight heritage. The GPS receivers have more than 100 years of combined on-orbit satellite years of operations on the Orbcomm constellation alone. The LN-200S is flying on the TSX-5 mission, and flew briefly on QuikToms \(launch vehicle problem\ RF transmitters and receivers are from L3-Conic and have significant flight heritage on both Orbital satellites and others across the industry. The Orbital-built antennas have extensive flight heritage. The torque rods are the industry standard units from Ithaco. The Solar Array Drive Assemblies SADAs\re the standard Type 2 gimbals from Moog. Orbital has flown these same drives on our geo-synchronous satellites    Figure 9 - ACRIMSAT, STEP-0 and VCL Satellites  The arrays are populated with single junction GaAs cells from Emcore, which was Tecstar when the cells were made and laid down on the panels. These cells are of the same vintage as those flying on the Orbcomm and GALEX satellites. The NiH2 battery is an SPV from Eagle-Picher. It is the same technology, although a smaller capacity, as the batteries flying on the Iridium constellation. The baseline star trackers are the ESA flight heritage Advanced Stellar Compass from the Danish Technical University \(DTU This unit has more than 5 years on orbit with the Oersted satellite, and has flown on more than 8 missions to date with no known failures  The Taurus launch vehicle has had 6 successful flights in various configurations. ACRIMSAT was successfully 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


