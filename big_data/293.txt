mBAR: A Materialized Bitmap Based Association Rule Algorithm Woon-Hak Kang Dong-Hyun Kim Sang-Won Lee Sungkyunkwan University woonagi319, tube, swlee}@skku.edu Abstract With the rapid progress in information technology, the data mining technique has been exploited in various applications. The association rule\(hereafter, AR\ mining one of the most popular data mining techniques, is to find the frequent itemsets which occur commonly in transaction database. Of the various AR algorithms, the Apriori is most popular, and it has been continuously improved during the past decade. Even with recent version, however, it is very time consuming for the Apriori-based algorithms to count frequent itemset since, basically for each k-size item set, we need to compute its support on-the-fly In this paper, we propose the mBAR approach to AR mining, which drastically improves the Apriori algorithm by exploiting materialized bitmaps. First, we present a bitmap-based Apriori algorithm. And, we suggest, in order to boost the performance of finding the frequent itemsets, how to store\(i.e. materialize\ the bitmaps instead of computing the bitmaps on-the-fly. Related to the materialized bitmaps, we suggest a way to choose the bitmaps selectively, instead of full bitmaps, and propose an incremental maintenance technique for materialized bitmaps against the changes in transaction database 1. Introduction With the advances in information technology, most information is stored in a form of database, and by analyzing the information, we can find trends or patterns which may be very useful in business decision making. The process of finding useful, non-trivial, but unknown knowledge from the mine of data is called data mining This technique is now widely used in various mission critical applications such as marketing, sales, stock prediction and et al Depending on types of applications and the knowledge to be mined, various data mining algorithms have been developed. Among them, the association rules\(hereafter, AR\mining is most popular, and the famous Apriori algorithm is the de-facto standard AR algorithm Since its introduction in 1994, the Apriori-based AR algorithm has been continuously enhanced in performance. However, it has some intrinsic disadvantages, such as repeated database scan unpredictable computation cost and memory requirement To overcome these problems,  various algorithms have been proposed. In spite of the huge enhancements, the Apriori-based AR algorithm does not still, we believe meet the strict performance requirements in near realtime data mining In this paper, we propose a materialized bitmap-based AR algorithm under the Apriori philosophy, which dramatically improves the performance. The algorithm is very intuitive and fast in computing the frequent itemsets The contributions of this paper are four folds: 1\To improve the frequent itemset counting by exploiting the bitmaps, and further more 2\far as we know, to firstly suggest to use the materialized bitmaps in processing the AR query - a kind of index for data mining, 3 an algorithms for choosing the materialized bitmaps selectively under the space constraints, and finally 4\to develop an incremental maintenance technique for the materialized bitmaps according to the changes in base transaction database. With the mBAR algorithm, near real-time AR will be possible. To sum up, we propose a comprehensive bitmap-based index framework in the AR field. We believe that the bitmap concept is very suitable as a materialized view framework for AR mining The remainder of this paper is organized as follows Section 2 gives a brief overview of the Apriori algorithm Section 3 proposes a bitmap based AR algorithm\(hereafter BAR BAR technique by materializing the bitmaps and thus avoiding the overhead of the repeated on-the-fly computations of bitmaps, which we call the mBAR technique. Finally, Section 5 discusses the conclusion and future works 2 Related Work Apriori Algorithm The Apriori algorithm ch is the m o st popular AR algorithm, has broken new ground for large scale data mining, and since its introduction, most of the AR algorithms is based on it. Because our mBAR algorithm is also based on the Apriori algorithm, we briefly review the Apriori algorithm The main principle of Apriori is that any subset of a frequent itemset must also be frequent. Using this principle Proceedings of the 21st International Conf erence on Data Engineering \(ICDE \22205 1084-4627/05 $20.00 \251 2005  IEEE 


the Apriori generates much smaller number of candidate itemsets than the previous algorithms. The Apriori algorithm performs as follows. In the first pass, it counts the support of all items over the database and determines which of them are frequent. In the subsequence passes generating candidate itemsets and finding frequent itemsets are iterated until no new large itemsets are found Candidate itemsets are generated by joining frequent itemsets founded in the previous pass and pruning. Then by scanning database, the support of each candidate itemset is counted However, if any frequent k itemset exists, it scans the database k times for frequent itemset counting. This excessive database scan is the main bottleneck of the Apriori algorithm. Another bottleneck is to test whether a candidate itemset is contained in the transaction database. Thus, we need to do many item containment test in transaction The AprioriTid is also e disadvantage of na\357ve Apriori. In the AprioriTid, the database is not scanned after the first pass by buffering the scanned data as a compressed format in memory. The AprioriTid uses the buffered data to prevent database scan, but these data can be larger than original database Thus, it may not be a viable option for moderate size of real data Compared to the traditional Apriori algorithms, we employ bitmaps as the internal representation format of frequent itemset counting, and this brings many advantages which will boost the performance of Aprioribased AR algorithms. You may understand the advantages of our approach as you proceed this paper 3. BAR: A Bitmap-based AR Algorithm The bitmap technique is very popular in data warehouse DW\fields, where very large database and complex analytical query are handled. The bitmap index is one of the main performance boosts in DW field. We had started our research by wondering whether it is possible to enhance the Apriori algorithm using the bitmap index technique Our BAR algorithm is basically based on the Apriori idea, and is in the same vein with AprioriTid in that it maintains a compressed database within memory. But, its uniqueness comes from the fact that it uses bitmap as the data structure for internal processing. By bitmap representation, we expect to avoid costly database scans\(high disk I/Os\CPU intensive item containment test In order to count support for each itemset, compared to the previous frequent itemset counting approaches where we need to scan every transactions, the BAR algorithm uses the efficient bitmap AND operation, thus being very intuitive and very fast. The compressed bitmap representation also enables as much as item information to be retained in memory, thus reducing disk I/O very much Even though the definitions of transaction table and related notations are generally sim in this paper C k set of candidate k-itemsets\and L k set of frequent k-itemsets\are represented as bitmaps. Firstly, we scan the transaction DB and construct a bitmap for each item in the database,  in which a bit is set to \2171\220 if the item is contained in the transaction ID, and otherwise, the bit is set to \2170\220 L 1 a}, {b}, {c}, {e C 2  a b a c}, {a e}, {b c}, {b e}, {c e TID a b c d e  100 1 0 1 0 200 0 1 1 1 300 1 1 1 1 400 0 1 0 1 supp 2 3 3 1 3 a 1 0 1 0 b 0 1 1 1 a b 0 0 1 0 1  C 1 L 1 bit-apriori-gen minsup = 2 TID Items 100 200 300 400 a c d b c e a b c e b e 000G D TID a b c e 100 1 0 1 Figure 1. An Illustrative Example of BAR Algorithm For the exam the figure 1 shows how the BAR algorithm represent the transaction DB in bitmaps and how to calculate the frequent itemset just using bitmap AND operations While scanning the transaction database, we calculate the support for each 1-item, and construct table C 1  which will be the basis for candidate itemset generation. After constructing table C 1 we create frequent 1-itemset L 1 by removing the item which does not meet the minimum support. After this step, we can generate next level candidate itemsets\(that is, bitmaps and support value for each bitmap\without disk scanning and string matching for item containment test. From the candidate itemsets we remove the itemsets whose support value is less than the minimum support. By repeating this process until no more new itemset are found, we can completely find the frequent itemsets. We call this module bit-apriori-gen 4. mBAR: Materialized BAR Algorithm As previously described, the BAR technique enables fast calculations of frequent itemsets over the large volume of data. Nevertheless, because the BAR algorithm follows the same steps of the Apriori algorithm it suffers from repetitive on-the-fly calculations of bitmaps and their support values. To overcome this problem, we propose the mBAR technique, which materializes the bitmaps for itemsets and stores the support value using B+ tree, thus enhancing the Aprioribased AR algorithms furthermore. Of course, this technique requires storage overhead for storing large 0 200 0111 300 1111 400 0101 supp 2 3 3 3 Proceedings of the 21st International Conf erence on Data Engineering \(ICDE \22205 1084-4627/05 $20.00 \251 2005  IEEE 


bitmaps and incremental bitmap maintenance overhead However, the performance benefit beats this overhead by far. In this respect, our mBAR is a kind of index for AR data mining 4.1. Overview In contrast to relational views which are virtual relations a materialized view\(MV Because we need to store the pre-computed result, it requires the space overhead. However, if a given query is relevant to the materialized view - i.e. rewritable to the view, we can answer the query very fast. Thus, the MV technique is widely exploited in DW applications, and all the major commercial DBMSs support the MV functionality with some variations In this paper C k means bitmaps for k itemsets, and L k refers to bitmaps for frequent k-itemsets. The mBAR stores all the bitmaps of C k to C n and stores each bitmap\220s support value in the leaf nodes of B+ tree. Thus, with our mBAR technique, the problem to find all the association rules for the given confidence and support, is simply to find the itemsets whose support value is greater than the given support value, which is just an index traversal operation. To sum up, the mBAR can accelerate the AR calculation at the cost of some space overhead 4.2. Overhead of Materialized Bitmaps If we would like to store all the bitmaps for real medium size transactional databases, the storage requirement is too huge. For our mBAR to be practical, we should devise a technique which stores minimal bitmaps but achieve the AR performance as close as in the case of full bitmap materialization. For example, let us consider a sample transaction database, where number of transactions is one million t ber of distinct items is one thousand S for C 1 will be approximately 125MByte, according the following formula e.g. O n   t 1 000\335 S bits = 10 6 000\335 10 3 bits 1Gbit + some overheads 000 125MBytes C 1 which stands for 1-itemsets of bitmap, consumes large spaces, but the bitmap compression technique will reduce the space much. The problem lies in that space requirement is not limited only to C 1 but the bitmaps for upper level itemset also need to be materialized. More precisely, up to k level, where k = # of itemsets / 2 the space requirement exponentially increases. Thus, the total space overhead for bitmap materialization can be approximated as follows  11 ofTIDs ofTIDs  nn nr nr rr P C r 000 \000 000\003\000u\000\003\000\013 000\003 000\003 000 000\003\000u\000\003\000\013 000\003 000\003 000\246\000\246 However, in practice in case k 002\373 of itemsets / 2 the number of frequent k-itemset will be reduced, in particular, it will be close to 0 as k approaches to n Nevertheless, in the worst case, the storage requirements for bitmap materialization, we need the space of a couple times of the original transactional database size 4.3. Bitmap Selection to Materialize Because it is not practical to materialize all the bitmaps for every k itemset, we investigate a technique to reduce the itemsets to materialize. Please note that the superset-subset relationships between all itemsets make a lattice structure. By exploiting this lattice structure, we can materialize bitmaps selectively with a little performance compromise. Our idea mainly comes from the cube reduction technique found in [3][4 Here we assume that it is possible to find almost association rules from C 1 upper 20% of L k in terms of support value\all the itemsets which are maintained in B+ tree\Our assumption about the upper 20% is arbitrarily chosen, and in practical usage it will be given as storage constraint B users, where the B should be larger than the minimum space. Based on the given B, we can calculate the number of bitmaps at each level of frequent itemset counting in Apriori algorithm. If we are given the space constraint B e.g 1GBytes\, the num ber of transactions t ber of distinct items n  representation\(8 bits\calculate the minimal storage which is the summation of 1\1-itemset bitmaps C 1 and 2 all the itemsets The space for 1-itemset bitmap C 1  bits. And the space for B+tree can be estimated as 1 n Ct 000u 1 8 n nr r C 000 000u 000\246 bits according to combination formula. Consequently, we need the space of 1 1  n nn r Ct C 000 8 r 000u\000\016 000u 000\246 bits for C 1 If the space constraint B is larger than this value, the remaining space for upper k-itemset\(where  2 k 000t 1  8  n nr r P Bnt r 000  000\020\000u\000\020 000u 000\246 We can calculate the ratio between the remaining space and the total expected space as 12  8    nn nr nr rr P  B nt C t r 000 \000 000\020\000u\000\020 000u 000u 000\246\000\246 By multiplying this ratio with the number of items generated in each k-itemset we can estimate the upper level bitmaps to materialize Figure 2 shows a pseudo code of our mBAR algorithm. In the line 1, every 1-itemset C 1 are generated and materialized in disk, that is L 1 The line 2 calculates the size of the available remaining space which can be used for storing the upper level bitmaps in the lattice. The Proceedings of the 21st International Conf erence on Data Engineering \(ICDE \22205 1084-4627/05 $20.00 \251 2005  IEEE 


line 3 to 11 generates bitmaps for candidate itemsets and calculates their support values, which are same with basic BAR algorithm. The line 13 to line 18 materializes the bitmaps of the most frequent M k k-itemsets for each klevel in the lattice, and also creates B+tree for storing the support value for every candidate itemsets 1 C 1  L 1 1-items bitmap table Store MV 2  12  8          nn nr nr rr P B nt C t ofTIDs r 000 \000 000\020\000u\000\020 000u 000u\000u\000\003\000\003\000\003 000\246\000\246  Ratio 3 for  k 2 L k-1 000z\000\207  k  do begin 4 C k bit-apriori-gen L k-1  New candidates 5 forall transaction t do 6 count support and sum_of_items 7 forall candidates c 000\217 C k do 8 if  c bit t  1 then 9 c count 10 end 11 end 12 prune column 13 knk MCR 000 \000u 000\253 000\254 000\273 000\274  apply gauss function 14 store top itemset and all support 15 L k c 000\217 C k Top M k of c.count 16  L k  Store MV 17  C k count row Store B+ tree 18 end Figure 2. mBAR Pseudo code 4.4. Incremental Maintenance of Bitmaps For our mBAR algorithm to be viable option for AR one of the main issues to be solved is, when the transactions in base database are inserted, updated or deleted, how to maintain the stored bitmaps consistently The na 000\365 ve solution which rebuilds the bitmaps from the scratch whenever there is changes in transactional database is too time consuming. So, we should devise an incremental maintenance technique for the materialized bitmaps - that is, when transactional data is updated in base database, each bit-string in C 1 also should be appropriately changed and further more the change should be propagated up to the bitmaps for higher level itemset s along the lattice Let us consider each case of insertion, deletion, and update. When a new transactional record is inserted, new corresponding bit is appended in each C 1 itemsets and if the bit is 1, then the support value for the itemset is also increased, and otherwise, the value needs not to be changed. Similarly in case of a record deletion, if the bit to be deleted is 1, then the support value should be decreased by 1. The update in transaction DB can be interpreted as insert-after-delete For the upper level bitmaps L k where which are selectively materialized, if the changes in C 2 k 000t 1 is propagated to them, the support value in each of them also may be changed. So, if some itemset go below the minimum support requirement for materialization, they are removed from the materialized bitmaps. In contrast we need to materialize some itemsets\220s bitmaps because their support values become larger than the minimum support value. In particular, when pattern changes occur in transactional databases, this kind of time consuming de-materialization and materialization of bitmaps are necessary. But except in case of the radical pattern changes, the overhead for upper level bitmap maintenance is, in general, not much burden In the lattice structure, it is less possible for the itemsets in higher upper level to be affected by changes in transactional database. In particular, if any sub-itemset L k-1 of L k is materialized, we can compute the bitmap of L k by bitmap-ANDing L k-1 and an C 1 5. Conclusion and Future Works We believe that the traditional AR algorithms, including the Apriori-based algorithms, have too large disk scan cost and CPU computation cost so that they will not meet the near real-time data mining performance requirements Our BAR technique will eliminate these performance bottleneck in Apriori-based algorithms, because it can avoid the repeated database scan and, further more, can reduce the CPU intensive item containment test operation This enhancement is possible because we transform the original transaction database into bitmap string. After the data is transformed into bitmaps, then the AR problem is intrinsically just bit counting problem. Our next contribution, mBAR, makes it unnecessary to calculate the numerous itemsets repeatedly, at the cast of the space overhead and incremental maintenance. In this respect our mBAR framework is a viable index for AR data mining We plan the following future works. Needless to say we should implement the mBAR algorithm and to evaluate its performance, compared to the existing AR algorithms, against real data set. And, we will adopt the bitmap com materialized bitmap size dramatically. Finally, we investigate on exploiting the GPU\(Graphic Processing Unit\computation power on bitmap operations. Recent works such as [6] will shed light on this issue Bibliography   R. Ramakrishnan, J. Gehrke, "Database Management Sy stems 3rd.", McGRAW-Hill, pp. 889-897, 2003   R Agrawal R. Srikant, "Fast Algorithms for Mining Association rules", Proceedings of the 20th VLDB, pp. 487-499, 1994 3] S. Agarwal, R. Agrawal, P. M. Deshpande, A. Gupta, J. F Naughton, R. Ramakrishnan, S. Sarawagi, "On the Computation Proceedings of the 21st International Conf erence on Data Engineering \(ICDE \22205 1084-4627/05 $20.00 \251 2005  IEEE 


of Multidimensional Aggregates", Proceedings of the 22th VLDB pp. 506-521. 1996  V. Harinaray an, A. Rajaram an, J. D Ullm an Im plem enting Data Cube Efficiently", SIGMOD, pp. 205-216, 1996  T. Johnson \215Performance Measurements of Compressed Bitmap Indices\216, Proceedings of the 25th VLDB, pp. 278-289, 1999   I Buck T. Foley D. Horn et al, \215Brook for GPUs Stream Computing on Graphics Hardware\216, SIGGRAPH, 2004 Proceedings of the 21st International Conf erence on Data Engineering \(ICDE \22205 1084-4627/05 $20.00 \251 2005  IEEE 


the measurements generated by the detected points It is fundamentally different than in standard approaches where it is taken equal to the state predictions VI R ESULTS The multi-target tracking algorithm proposed in this paper is currently being implemented on a optical motion capture system We implemented the algorithm in matlab and we tested it on data previously acquired with the motion capture system Twentytwo markers have been attached to a human subject three on the head two on the shoulders two on each arm ve on the torso and four on each leg A rigid object with six markers on it was held by the subject in his hand during the acquisition of motion The total of 28 markers was tracked by a six 50 Hz camera motion capture system for approximately two minutes i.e for about 6000 frames The rst 2000 frames were used to determine and initialize the estimate of the invariants of motion All the markers attached to the rigid body held by the subject in his hand satisfy the mutual distance invariants and even Kendalls invariant when there are no occlusions The coordination graph clearly exhibits the articulation structure of the human body All the markers on the torso for example belong to the same clique of maximum order equal to ve The markers on the feet all belong to a complete subgraph This is because the subject was asked not to move his feet in order to check adaptability to changes in the coordination and the effect of the forgetting factor As an example the statistics of some invariants are described in the following table  Invariant distance among two targets Mean Std Persistence interval max frames Targets 1 and 3 both on the head 16.8cm 0.12cm All Targets 12 and 13 on the left arm 29.7cm 0.57cm 786 The total number of trajectories segments has been taken as a performance index of the data association algorithm Ideally the number of trajectories should have been equal to the total number of markers i.e 28 An implementation of the JPDA alone generated 112 segments The number of trajectory segments is furthermore highly dependent on the choice of noise covariances in the Kalman lters If the covariances are set too small the measurements do not fall within the validation gates and are associated to clutter If the covariance is set too large the data association becomes very difcult because the number of possible associations increases After a few trials we found a choice that led to the best result of 112 segments The shape integrated JPDA generated 36 segments where most of the wrongly labeled segments were produced because of the incorrect invariants detected between the feet of the subject Tuning the forgetting factor  for the invariants is important to obtain signicant results A small  leads to the creation of invariants which persist in time very briey A large  renders the scheme rigid and not adaptable so that wrong invariants declared as such because of not sufciently exciting dynamics lead to wrong data association VII C ONCLUSIONS This paper continues along the research line presented in  The spirit is to include information due to the statistical dependence among the targets in standard algorithms multi target tracking algorithms that otherwise treat targets as independent This information is of great help in solving the data association problem The proposed schemes should also improve on the techniques proposed in the computer vision literature based on statistical learning methods which do not imply any local coherence in time of the targets trajectories Coordination among targets has been models by the means of motion symmetries or invariants The shape description proposed by Kendall is used as an invariant but since this is not robust w.r.t occlusions it has been integrated with pairwise distances among targets and angles between target velocities The possibility of slow drifts in time of the invariants is dealt with by introducing forgetting factors in the estimate of their statistics In experiments with a motion capture system segmentation of the tracks has been substantially reduced compared to the standard JPDA assuming the possibility of learning the invariants on a sufciently long time interval with persistently exciting dynamics R EFERENCES  Y  Bar Shalom and T  F ortman Tracking and data association  Academic Press 1988  D B Reid An algorithm for tracking multiple targets  IEEE Trans on Automatic Control 25  No 6 pp 843-854 1979  G Gennari A Chiuso F  Cuzzolin and R Frezza Integrating shape and dynamic probabilistic models for data association and tracking  IEEE Conference on Decision and Control 2002  G Gennari A Chiuso F  Cuzzolin and R Frezza Integration of shape constraints in data association lter  IEEE Conference on Decision and Control 2004  I N Goodman and D H Jonson Orthogonal decompositions of multivariate statistical dependence measures  Int Conf on Acoustics Speech and Signal Processing ICASSP 2004 Montreal CA May 2004  I Gordon and D G Lo we Scene modelling recognition and tracking with invariant image features  International Symposium on Mixed and Augmented Reality ISMAR Arlington VA Nov 2004 pp 110-119  D G K endall A survey of the statistical theory of shape with discussion  Statist Sci 4  1989 pp 87-120  M Isard and A Blak e Condensation  conditional density propagation for visual tracking  Int J Computer Vision 1998  K Okuma A T ale ghani N De Freitas J J Little and D G Lo we A Boosted Particle Filter Multitarget Detection and Tracking  European Conference on Computer Vision ECCV Prague May 2004 pp 2839  C Rasmussen and G.D Hager  Joint probabilistic techniques for tracking multi-part objects  Int Conf on Computer Vision and Pattern Recognition 1998  C Rasmussen and G.D Hager  Probabilistic data association methods for tracking complex visual objects  IEEE Transaction on Patter Analysis and Machine Intelligence 23 2001 560576  Y  Song L Gonca v es E Di Bernardo and P  Perona Monocular perception of biological motion detection and labelling  Int Conf on Computer Vision 1999 pp 805812  Y  Song L Gonca v e s and P  Perona Unsupervised learning of human motion  IEEE Transaction on Patter Analysis and Machine Intelligence 25 2003 114  Y  Zhu Efcient Recursive State Estimator for Dynamic Systems without Knowledge of Noise Covariances  IEEE Transaction on Aerospace and Electronic Systems 35 1999 102114 6058 


2          f o r  a l l  c a t e g o r i c a l              let          3          f o r  a l l  c a t e g o r i c a l              let           4   G i v e n  a n  m d i  o f       f o r  a l l  n u m e r i c       let           Given                   a n d                r       f o r  e a c h  c a t e g o r i c a l       a n d        o r       f o r  e a c h  c a t e g o r i c a l       from \(1 3 4 t h e  d e n s e n e s s  b e i n g  a  M I N T  m e a s u r e         f o r  e a c h  n u m e r i c       a n d        f o r  e a c h  n u m e r i c       Accordi n g l y         a n d         a n d  t h e  j o i n            g i v e s the upper bounds of       m a y  n o t  b e  u n i q u e   s i n c e  m u l tiple mdrs  s  c a n  b e  d e r i v e d   A l s o    m a y  n o t  e x i s t   s i n c e      c a n  b e  o b t a i n e d  i n   1    o r  t h e  m d r   can not exist, i.e      i n   4    F i g u r e  1  d e p i c t s  t h e s e  c a s e s   I n Figure 1. Derivation of  by join a  o f  t h e  c o m b i n e d  i t e m s e t    i s  m u l t i p l e  d u e  t o  t h e  l a c k of uniformity of     even if  and  of the original itemsets are unique respectively. In \(b  


nal itemsets are unique respectively. In \(b  does not exist due to the low denseness of       A c c o r d i n g l y    rived via       i s  a  f a m i l y  o f  s e t s  i n  g e n e r a l   T h e n  w e  o b t a i n  t h e  j o i n  o p e r a t i o n         b y  E q   2    F r o m  t h e  a b o v e d i s c u s s i o n                      a n d  t h u s       i l a r l y          T h i s  i n d i c a t e s  t h a t  t h e  j o i n      g i v e s  t h e upper bound of    a n d  a n  u p p e r  s e m i l a t t i c e    Based on this de?nition of join operation on families of sets with denseness and the de?nitions of support and con?dence, the most of the standard algorithms of the Basket Analysis whose complexity is       c a n  b e  a p p l i e d  t o  d e rive generic QARs from data 3.2. Implementation To assess the basic features of QARMINT, we used the standard Apriori-TID algorithm [1], since it is principally an algorithm running on memory, and its computational features are well known. Instead of hash tables, the trie data structure as depicted in Fig. 2 was used under lexicographically ordered itemsets. If any subsets of the joined s e t                a r e  n o t  f r e q u e n t  a c c o r d i n g  t o  a  g i v e n      i s  p r u n e d  b e f o r e  i t s  m d r   is computed. Moreover, after computing the mdr      i s  p r u n e d  i f    i s  n o t frequent. The pruning by these checks are indicated by the slashed itemsets in Fig. 2. A difference from the original Apriori-TID algorithm is that the join of two itemsets    within a family depicted by a solid box is not allowed, and t h e  i t e m s e t s   s  o b t a i n e d  f r o m  a  p a i r  o f  f a m i l i e s    l o n g  t o  a n  i d e n t i c a l  f a m i l y     A n o t h e r  d i f f e r e n c e  i s  t h a t  a join of     c a n  g e n e r a t e  m u l t i p l e  i t e m s e t s   s  a s  d e p i c t e d  i n a dashed box The most expensive process in QARMINT is to derive the mdr  s  o f  j o i n e d  i t e m s e t     W e  i n t r o d u c e  a n  i t e r a t i v e approach to reduce the required computation time. Given                     r s t   a l l  t r a n s a c t i o n s  i n  are sorted for each attribute   i n     T h i s  i s           T h e n  the mdis on the number line of  


are computed from the transactions without taking into account the other attributes When multiple mdis are obtained, one of them is focused and the transactions in the mdi is retained. Next, the identiProceedings of the The 2005 Symposium on Applications and the Internet Workshops \(SAINT-W  05 0-7695-2263-7/05 $20.00  2005 IEEE Figure 2. Trie data structure Figure 3. Time complexity cal process is applied to   and this recursively continues in depth ?rst search \(DFS   is computed, the process continues again from  until the mdi of every              c o n v e r g e s   T h e  m d i s  a l w a y s  c o n v e r g e to these of the mdr  because the denseness is a MINT measure. After the convergence, the search is backtracked to the next mdr  The computation of mdis in each step requires       t i m e  a t  m o s t   I n  t h e  w o r s t  c a s e   o n l y  o n e t r a n s a c t i o n  i s  d r o p p e d  i n  e a c h  s t e p   a n d    s t e p s  r e q u i r e d until the mdis converge. Thus         H o w e v e r   t h i s  d o e s not likely occur. Practically, only a portion of the transact i o n s  a r e  r e t a i n e d  i n  e a c h  s t e p   L e t          b e  a n  e x pected rate of transactions retained in each step the required steps for convergence. The process to search an mdr stops at the latest when the number of retained transa c t i o n s     b e c o m e s  l e s s  t h a n   By solving the equation       w i t h   is        A c cordingly, the expected time complexity of this most expensive process is         4. Performance Evaluation The performance of QARMINT has been evaluated through both arti?cial data and real bench mark data Sets of arti?cial data have been generated under various conditions. The characteristics of the computation time is simlilar to the conventional Basket Analysis except for       T h e  t i m e  m o d e r a t e l y  i n c r e a s e s  w h e n s of all attributes are increased. This is because wider permissible ranges increases the number of mdrs. Figure 3 shows the dependency of the computation time on the n u m b e r  o f  t r a n s a c t i o n      T h e  c u r v e  a l m o s t  f o l l o w s  t h e  r e lation         The real bench mark data  Labor relations Database  in UCI Machine Learning Repository [3] was analyzed by QARMINT. It contains 57 instances, 8 numeric attributes and 8 categorical attributes and many missing values. We ignored the attributes of missing values in each instance and transformed the data into transactions. Though the size 


and transformed the data into transactions. Though the size of this data is quite small, we found many interesting QARs associated with the labor conditions under      and      w h i c h  i s  1 0   o f  t h e  m a x i m u m  a n d  m i n i mum values of each  in the data. The following two are examples                                                                                                                  These rules indicate that the workers having longer durat i o n  c o n t r a c t s  a n d  e v a l u a t i n g  t h e i r  l a b o r  c o n d i t i o n  a s   admit longer working times and less wage increase. These evaluations indicate the suf?cient tractability and the practical applicability of QARMINT 5. Conclusion The mathematical characterization and the extension of the Basket Analysis presented in this paper are expected to provide variety of new approaches of data mining. Their potential has demonstrated by a novel approach called QARMINT for complete mining of generic QARs within a low time complexity. We are implementing QARMINT in a more ef?cient algorithm and evaluating its performance in near future Acknowledgement This research has conducted under the support of JSPS Kiban Kenkyuu \(B 2 References 1] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. Proc. of 20th Int. Conf. on Very Large Data Bases VLDB  499, 1994 2] R. Srikant and R. Agrawal. Mining quantitative association rules in large relational tables. Proc. of 1996 ACM SIGMOD Int. Conf. on Management of Data, pages 1  12, 1996 3] U. C. I. \(UCI http://www.ics.uci.edu/ mlearn/MLRepository.html, 2004 4] J. Wijsen and R. Meersman. On the complexity of mining quantitative association rules. Data Mining and Knowledge Discovery, 2\(3  281, September 1998 Proceedings of the The 2005 Symposium on Applications and the Internet Workshops \(SAINT-W  05 0-7695-2263-7/05 $20.00  2005 IEEE 


0-7695-2263-7/05 $20.00  2005 IEEE pre></body></html 


n M L N n t n t n t n t L M L t L t L tt L t kkkk kkkk kkk kkkk kkkkkkkkP VK VK VK VK PP       kkP t 31 where L  s the error covariance associated with the state estimate t i    kkLX  tt kkk P1  00 0  0                     s s sss s s sss s s sssss N n t n t n 


n t n N n t n t n t n N n t n t n t n t n t n c t L kkkkkk kkkkk kP VKVK VKVK  32 4. Simulations One has run simulations comparing the sequential implementations of MSJPDA algorithm and the new algorithm here. A typical multisensor multitarget tracking environment is assumed in the simulations. According to article [1,3], One known that the performance of sequential MSJPDA is better than the performance of parallel MSJPDA. Therefore, the performance of parallel MSJPDA algorithm will not be compared here There are three sensors, which are fixed in three platforms. Regarding the 2nd sensor as fusion centre situation of the other sensors are: =?-500m?-500m 0m??N =?-500m? 500m?0m??The distance error of each sensor is: =300m, =200m, =100m?The bear error of each sensor is 0.03rad, =0.02rad, =0.01rad?The of sample is T=1s?The nonparametric model of clutter is used in the simulations, and expected number of false measurement is m=1.8 1 sN 3 s 1r 2 2r 3 3r 1 Simulations have been run for racking two targets. The true initialization state of the targets is X1?[-29500m,400m/s,34500m,-400m/s X ?[-26250m,296m/s,34500m,-400m/s]'? 2 The two targets will cross above 31seconds later. To evaluate tracking performance, 50 Monte Carlo runs were performed for three case of the target detection probability Pd=0.97 ? Pd=0.76 ? Pd=0.58. In every run, the total simulation time is 140 steps 


simulation time is 140 steps            Figure 1  RMS position error in case of Pd=0.97          Figure 2  RMS velocity error in case of Pd=0.97       Figure 3  RMS position error in case of Pd=0.76 567 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005         Figure 4  RMS velocity error in case of Pd=0.76         Figure 5  RMS position error in case of Pd=0.58          Figure 6  RMS velocity error in case of Pd=0.58  Table 1 The emanative times comparison for sequential MSJPDA and SD-CMSJPDA algorithm  Pd N A  0.97 0. 76 0.58 Sequential MSJPDA 2 11 17 SD-CMSJPDA 0 3 5 Pd denotes detection probability, N denotes emanative 


Pd denotes detection probability, N denotes emanative times, A denotes the kind of algorithm Table 1 shows the summation of emanative times for sequential MSJPDA and SD-CMSJPDA algorithm in 50 Monte Carlo simulations. From table 1 , it is shown that the stability of SD-CMSJPDA is better than that of sequential MSJPDA as the detection probability varied Figure 1,2 show the RMS errors for position and velocity in case of Pd=0. 97, respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.76 respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.58, respectively. From the figures we can see that the average RMS position error is lower for the SD-CMS JPDA algorithm. We also see that the state estimation precision of sequential MSJPDA get worse as the detection probability decreases The reasons for these simulation results lies:1 state estimation precision will get worse when the detection probability decrease;2 algorithm is to process measurement from each sensor using single sensor JPDA algorithm sequentially. Therefore the estimation error from each sensor will be accumulated Moreover, the sequential MSJPDA algorithm can  t improve the joint detection probability of the multisensor system The estimation error of the SD- CMSJPDA  algorithm will not be accumulated for it processes the measurement from each sensor directly in the mean time .What  s more the new method can greatly improve the joint detection probability of the multisensor system. Therefore, the tracking performance of SD-CMSJPDA algorithm is better than that of sequential MSJPDA. Algorithm All of the simulations are run in the personal computer with a 2.0G CPU and a 256M memory. The average cost time per step is 0.0251 in the sequential implementations of MSJPDA algorithm. And the average cost time per step is 0.0282 in the sequential implementations of MSJPDA algorithm. According to the results we can see that there is few difference in real time between the new method and the sequential   MSJPDA when there is not so many sensors and targets 568 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  5. Conclusion In order to solve the problem of multisensor multi target tracking, a new centralized multisensor  joint probabilistic data association  algorithm is proposed in this paper. The simulation results shows that the tracking performance of the new algorithm is better than that of the sequential MSJPDA algorithm The computational complexity of the new method will increase as the number of sensors and targets grow Therefore, how to improve the real time of SD- CMSJPDA algorithm will be pay attention References 1] He You, Wang Guohong, Lu Dajin, Peng Yingning Multisensor Information Fusion With Application[M Publishion House of Electronics Industry. 2000, Beijing.  [11] B..Zhou and N.K.Bose Multitarget  Tracking in Clutter:Faste Algorithms for Data Association .IEEE Transaction on Aerospace and Electronic Systems 1993,29\(2 2] Bar-shalom,Y\(Ed Applications and Advances,2: Norwood,MA Artech  House, 1992 3] L.Y. Pao, C.W.Frei. A Comparison of Parallel and Sequential Implementation of a Multisensor Multitarget Tracking Algorithm. Proc. 1995 American Control Conf. Seattie, Washington,June 1995 1683~1687 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





