An Approach to Grid Scheduling Optimization Based on Fuzzy Association Rule Mining  Jin Huang,  Hai Jin,  Xia Xie,  Qin Zhang Cluster and Grid Computing Lab Huazhong University of Science and Technology, Wuhan, 430074, China hjin@hust.edu.cn Abstract This paper presents a grid scheduling optimization technique based on knowledge discovery. The main idea is to transform the grid monitoring data into a performance data set, extract the association patterns of performance data through fuzzy association rule mining, then construct optimization logic according to the mining results, and finally optimize the grid 
scheduling. In the process of data mining, a method of association rule mining is proposed based on timewindow and fuzzy set concepts, which can mine data for quantitative attribute value based on the attribute and time dimensions in grid performance data set  1. Introduction Grid computing is not only an aggregation of resources, but also a system with some dynamic characteristics. It has the ability of self-improvement and can optimize itself according to such historical information as configuration optimization, distribution 
and utilization optimization of the resources. The process of optimization could take advantage of the grid monitoring data. In order to get high performance the reasonable decisions for the execution of the applications are needed In a grid system, applications share various resources with others. Therefore, how to make these applications get high performance is the problem of how to deal with grid schedule. Since grid has some unique features such as the resources in the grid are always dynamic, heterogeneous and diverse, schedulers 
have to deal local issues, and the grid scheduling technique is more complex than those conventional scheduling technique. Grid scheduling is one of the major factors that affect the grid performance. If a    This paper is supported by National Science Foundation of China under grant 60273076 and 90412010 scheduler works well in resource selection and task heduling, each task may get the most suitable resource, thus the mean response time of the tasks will 
decrease. This will guarantee each task is completed within the limited period In grid environments, the behaviors of some objects or entities, being involved in the execution of applications, are much the same as those in a stochastic activities of those entities in a grid system follow certain rules. If we study carefully on these activities, an intrinsic relationship between two or more entities will be found, which will consequently enable us to predict future activities of the entities. For example, when a data visualization service program is 
executed, large amounts of input data that might be located in different places are required. Thus, the high network bandwidth utilization of the partial network in a certain period occurs running such service. Therefore we can summarize the rules between some entities according to the grid monitoring data, and predict possible status of entities in the future. Furthermore, we ke preparation for the future activities in advance, and take advantage of free resources and also avoid the excessive requests for the specific resources which improve the performance of the system 
The grid monitoring and accounting tools could record the historical status and activities of grid objects to get large amounts of monitoring and accounting data. These data include the execution information of grid applications, the utilization information of grid services, and the status information of grid system. In order to take good advantage of these data, it is necessary to classify and analyze them in multiple levels. Therefore, we apply data mining technique to grid scheduling optimization system based on the 
historical data to find out important association information from massive data, and optimize the scheduling process The data mining technique enables the system to handle a large amount of data without the evaluation Proceedings of the First International Conferen ce on e-Science and Grid Computing \(e-Science\22205 0-7695-2448-6/05 $20.00 \251 2005 IEEE 


information provided by the user, and enables the system to find out the information that might be ignored by the people. Association rule mining is an ideal approach to solve the above problem. For the data with quantitative values, we introduce the concept of fuzzy set into the mining process. Meanwhile according to the features of grid monitoring data, we apply the mining method based on time-window to perform fuzzy association rule mining The rest of the paper is organized as follows: the related works are discussed in section 2. Section 3 describes the related concepts of association rule and fuzzy set. Section 4 discusses the process of fuzzy association rule mining. Section 5 illustrates the application of association rule in grid scheduling Section 6 concludes the paper 2. Related Works The researches of grid scheduling optimization mainly include three methods [1 p tim iza tio n b a sed  on model analysis and simulation, optimization based and optimization based on online application reconfiguration Dimema rf orm a n c e prediction sim u lator for message passing applications. It reconstructs the time behavior of a parallel application on a target architecture rmance Analysis and Characterization Environment PACE\ modeling toolset for high performance and distributed applications. It includes tools for model definition model creation, evaluation, and performance analysis 4 T h ese m e th o d s  h a v e so m e li m itatio n s a n d can n o t meet the dynamic characteristics in real environment In grid scheduling research, most schedulers get predicted resource parameters from Network Weather Service NWS d iction s NW S is a n a g e n t system deployed on the grid to periodically monitor resource and performance. The goal of the PRAGMA  r o j ect is to realize a  n e xt-g e n eration adaptiv e  runtime infrastructure capable of support selfmanaging, self-adapting and self-optimizing applications on the gr 8 T h es e m e t h ods can predict the performance of the system according to grid monitoring data. However, the prediction methods that have been used right now merely focus on a single element, not considering the role of historical monitoring information played in this regard Autopilot [9 in teg rates ap p licatio n an d s y ste m  instrumentation with resource policies and distributed decision procedures. The resulting closed loop adaptive control system can automatically configure resources based on application request patterns and system performance. The goal of Grid Application Development Software GrADS r am e w ork is to  provide good resource allocation for grid applications and to support adaptive reallocation if performance degrades because of changes in the availability of grid resources. These methods can reconfigure accurately the execution of the applications in time. But application programming needs to be changed for grid scheduling. Consequently, the developers have to be involved in the process of performance optimization 3. Related Concepts of Association Rule and Fuzzy Set 3.1. Association Rule Let I={i 1 i 2 203, i m  be a set of items. Let D be a set of database transactions where each transaction T is a set of items such that T 002 I Let A be a set of items. A transaction T contains A if and only if A 002 T An association rule is an implication of the form A 000\237 B  where A 003 I, B 003 I and A 001 B 001. The rule A 000\237 B holds in the transaction set D with support s where s is the percentage of transactions in D that contain A 004 B This is taken to be the probability P\(A 004 B The rule A 000\237 B has confidence c in the transaction set D  c is the percentage of transaction in D containing A that also contain B This is taken to be the conditional probability P\(B|A Rules that satisfy both a minimum ld min_sup and a minimum confidence threshold min_conf  A set of items is referred as an itemset. An itemset that contains k items is a k itemset. The occurrence frequency of an itemset is the number of transactions that contain the itemset. This is also known, simply, as the frequency, support count, or count of the itemset An itemset satisfies minimum support if the occurrence frequency of the itemset is larger than or equal to the product of min_sup and the total number of transactions in D If an itemset satisfies minimum t of frequent k itemsets is commonly denoted by L k  Given a data set D a min_sup and a min_conf the association rule mining is to find out all the association rules that satisfy min_sup and min_conf specified in the data set D  3.2. Fuzzy Set A fuzzy set expresses the degree to which an belongs to a set. The characteristic function of a fuzzy set is allowed to have values between 0 and 1 which denotes the degree of membership of an element in a given set. If X is a collection of objects denoted Proceedings of the First International Conferen ce on e-Science and Grid Computing \(e-Science\22205 0-7695-2448-6/05 $20.00 \251 2005 IEEE 


 x then a fuzzy set A in X is defined as a set of ordered pairs A={\(x, \265 A x\x 005 X where 265 A x is called the membership function for the fuzzy set A The of X to a en 0 and 1. Let x 1  x 2 203 x n be the elements of the fuzzy set A and 265 1  265 2 203 265 n denoted the membership value of the elements. The fuzzy set A is commonly denoted as follows A=\265 1  x 1 265 2  x 2  203 265 n  x n  Given a fuzzy set A in a finite universe X its cardinality, denoted by Card\(A is defined as Card\(A 002\b 265 A x where x 005 X  Card\(X is referred as the scalar cardinality or the count of A  Union, intersection and complement are the basic operations in classical set. The union of two fuzzy sets A and B is a fuzzy set C written as C=A 004 B whose membership function 265 C x is related to those of A and B by 265 c x\max\(\265 A x\ \265 B  006 x 005 X The intersection of two fuzzy sets A and B is a fuzzy set C written as C=A 001 B whose membership function is related to those of A and B by 265 c x\min\(\265 A x\ \265 B  006 x 005 X  The complement of a fuzzy set A denoted by A\220 is defined by the membership function as 265 A\220 x 1 265 A x 006 x 005 X  4. Process of Fuzzy Association Rule Mining 4.1. Data Preprocessing The primary source of grid monitoring and accounting data mainly includes system logs, user logs service logs, performance records, etc. Generally speaking, the original data cannot be mined and needs to be preprocessed. A fraction of grid monitoring and accounting data is shown in Table 1 When preprocessing the grid monitoring data, the major work includes data cleaning and data joining Data cleaning aims at cutting dirty and useless data and revising the possible mistakes in data set, while data joining aims at combining various data sets that are needed in accord with the purpose of mining actually transforming original semi-structure monitoring data into a new form for mining We extract various performance data from different original monitoring data with analysis tools. The relevant performance data in a certain period is organized and regarded as a performance list with multiple attributes d each attribute is described with a quantitative value, such as the grid service utilization and bandwidth utilization of the partial network. We extract relevant information from monitoring and accounting data in Table 1, and arrange the performance data set in time order. In the new data set, we mine certain association rules [12  4.2. Algorithm of Fuzzy Association Rule Mining The object data contains the categorical and quantitative attributes. For quantitative attribute mining, attribute values are usually first divided into a set of continuous regions, and then these regions are transformed to categorical values. The transformation process may bring the sharp-border problem. So, we apply the concept of fuzzy set to the process of quantitative value mining so as to avoid the deviation caused by the sharp-border problem Grid monitoring and accounting data have their characteristics: \(1\as multiple attributes, and these attribute values may have some potential relationships with each other. \(2\The relationship but also after a certain period. For example, when utilization of a grid service is high, high bandwidth utilization might occur in the partial network connecting with the host running this grid service at that time or after a certain period Concerning the above characteristics, we propose a method of mining based on time-window. When mining the grid monitoring data, we take into account not only the operations on data set but also the influence of field-relevant knowledge on the mining process. For the aforementioned mining situation, the influence of time is obviously important. For the common situations, the interval between two events will decrease the relationship between them, which give much support to  Table 1.  Fraction of grid monitoring and accounting data Time Grid service utilization  Time Network utilization  Time Disk utilization  11:30:00 26%  11:20:00 5 11:30:00 66 11:32:00 20%  11:25:00 11%  11:30:30 64 203 \203  11:30:00 15%  11:31:00 65 11:46:00 44%  11:35:00 13%  \203 203 11:48:00 51%  \203 203 11:45:00 43 11:50:00 59%  12:00:00 42%  11:45:30 59 11:52:00 15%  12:05:00 69%  11:46:00 64 Proceedings of the First International Conferen ce on e-Science and Grid Computing \(e-Science\22205 0-7695-2448-6/05 $20.00 \251 2005 IEEE 


the method based on time-window According to the predefined time-window w the total events space is divided into several sub-spaces that overlap each other. In fact, we divide a large data set into multiple parts by time-window. Each part contains all the attributes in a large data set, and the values of each attribute do not exceed the time-window range. In each time-window, we only consider the association between the first record and others. In addition, the time-windows are specially handled to guarantee that each record in the data set is considered Multiple records in a time-window are organized as a new record which forms an extension data set. Table 2 shows the performance data set and the time-window Next, we describe the mining process by handling data set in Table 2. Considering that the performance data is denoted from A to E In the performance data set, each record contains all the items in a time-window and data item is composed of attribute name and quantitative value. For the transformation from quantitative value to fuzzy set, we provide the membership functions in Figure 1. Therefore, the attribute value is transformed to the corresponding membership value of the fuzzy set. We mine process through the operation of cardinality of fuzzy set 1 on Membership Value 0 Low High Middle 0.1 0.5 0.2 0.9 0.8 1  Figure 1 The membership functions used in this example Assume that minimum support threshold is s  minimum confidence threshold is c and time-window is w the detailed process is described as follows STEP 1: Transform the quantitative values of each data item into fuzzy sets. In this example, the utilization represent is divided into three fuzzy regions M\d High \(H\hus, three fuzzy membership values are produced for each data item according to the predefined membership functions Take the data item \(A, 55%\ an example. The quantity 55% is converted into a fuzzy set 0.83/A.M+0.13/A.H\sing the given membership functions. This step is repeated for the other item data and the results are shown in Table 3. Each term such as A.M is then regarded as an item in the mining process STEP 2: Construct the extended data set according to the predefined time-window w In this example w 3. The extended data set constructed is shown in Table 4 STEP 3: Calculate the scalar cardinality of each item in the records as the count value. The calculation is processed in terms of the data in Table 3. Take the region A.L as an example. Its scalar cardinality is 0.08+0.15+0.93=1.16. This step is repeated for the other regions, and the results that are considered as the candidate 1-itemsets C 1 are shown in Table 5 STEP 4: For each item, check whether its count is larger than or equal to the predefined min_sup s and the attribute regions satisfy min_sup are taken as frequent 1-itemsets L 1  s is set as 3.0 in this example. Since the count values of A.M  A.H  B.H  C.H  D.H and E.H are larger than 3.0, these items are put in L 1  STEP 5: Generate the candidate set C r+1 from L r  C 2 is generated from L 1 A.M A.M, D.H\at the different regions from the same attribute cannot be joined to generate a large itemset such as A.M and A.H In addition, the same itemsets with different sequences are regarded as different STEP 6: Determine the frequent itemset L r+1 from C r+1  Table 2. The performance data set and time-window Record Performance data items     1 E,31  E,55 w  E,94 w 4 E,13  5 E,82  6 E,24  7 E,80  8 E,57  9 E,87  10 E,35  Proceedings of the First International Conferen ce on e-Science and Grid Computing \(e-Science\22205 0-7695-2448-6/05 $20.00 \251 2005 IEEE 


 Table 3. The fuzzy sets transformed from the data in Table 2 Record Fuzzy items of performance data  0.48/E.L+0.37/E.M 0.98/B.H\,\(0.23/C.M+0 1.00/D.H\\(0.83/E.M+0.13/E.H 1.00/E.H 4 \(0.97/A.M+0.03/A.H\,\(0.10/B.L+0.87/B.M 0.30/D.L+0.60/D.M\,\(0.92/E.L 0.80/E.H 1.00/D.M\,\(0.65/E.L+0.13/E.M 0.75/E.H 8 \(0.70/A.M+0.23/A.H\,\(1.00/B.H\,\(0.68/C.L+0.1 0.28/D.L+0.63/D.M\,\(0.77/E.M+0.17/E.H 0.67/B.M+0.25B.H\\(0.55/C.L 0.27/C.M\,\(0.68/D.L+0.10/D.M\,\(0.92/E.H 1.00/D.H\,\(0.38/E.L+0.50/E.M Table 4. The extended performance data set Record Fuzzy items of performance data 1 0.48/E.L+0.37/E.M 1.00/D.H\\(0.83/E.M+0.13/E.H 1.00/E.H 2 1.00/D.H\\(0.83/E.M+0.13/E.H 1.00/E.H 0.30/D.L+0.60/D.M\,\(0.92/E.L 203 \203 8 0.70/A.M+0.23/A.H\,\(1.00/B.H\,\(0.68/C.L+0.10 0.28/D.L+0.63/D.M\,\(0.77/E.M+0.17/E.H 0.67/B.M+0.25B.H\\(0.55/C.L+0.2 7/C.M\,\(0.68/D.L+0.10/D.M\,\(0.92/E.H 1.00/D.H\,\(0.38/E.L+0.50/E.M 9 0.67/B.M+0.25B.H\\(0.55/C.L+0.2 7/C.M\,\(0.68/D.L+0.10/D.M\,\(0.92/E.H 1.00/D.H\,\(0.38/E.L+0.50/E.M 1.00/D.H\,\(0.38/E.L+0.50/E.M Table 5. The support of each itemset et Support A.L 1.16 D.L 2.09 A.M 4.93 D.M 2.90 A.H 3.59 D.H 4.28 B.L 1.63 E.L 2.43 B.M 1.54 E.M 2.60 B.H 6.14 E.H 3.77 C.L 2.94   C.M 1.50   C.H 4.62   a\ Calculate the fuzzy membership value of the candidate itemset in each record. Hence, the minimum operation is used for the intersection Take \(A.M, B.H\ an example. Its membership value for ID 1 is calculated as min\(0.83 0.98\=0.83 Note that the membership value for B.H, A.M\ 0.90. The results for the other records are shown in Table 6. The results for the other 2-itemsets can be derived in similar way b Calculate the count of each candidate 2-itemsets in the records. The results for this example are shown in Table 7 c Check whether these counts are larger than or equal to the predefined min_sup 3.0. The itemsets satisfy min_sup are thus kept in L 2  Table 6. The membership values for \(A.M, B.H Record A.M B.H A.M, B.H 1 0.83 0.98 0.83 2 0.90 0.98 0.90 3 0 1 0 4 0.97 1 0.97 5 0 1 0 6 0.80 1 0.80 7 0 1 0 8 0.70 1 0.70 9 0 0.98 0 10 0.73 0.98 0.73 Table 7. The fuzzy counts of the itemsets in C 2 Support A.M, B.H 4.93 A.H, E.H 3.11 A.M, C.H 3.50 B.H, A.M 5.08 A.M, D.H 3.49 B.H, A.H 5.36 A.M, E.H 3.98 B.H, C.H 3.91 A.H, B.H 3.57 B.H, D.H 4.47 A.H, C.H\4 \(B.H, E.H\4.80 A.H, D.H 2.87 203 203 Two above steps are like the process of generating the candidate itemsets and determining the frequent itemsets in algorithm Apriori [14  STEP 7: If L r+1 is null, then do the next step otherwise, set r=r 1 and repeat STEPs 5 to 7. Since L 2 is not null in this example, set r=2 STEPs 5 to 7 are then repeated to find L 3  C 3 is first generated from L 2  Proceedings of the First International Conferen ce on e-Science and Grid Computing \(e-Science\22205 0-7695-2448-6/05 $20.00 \251 2005 IEEE 


In similar way C 4 is generated from L 3 Since no itemset is put in L 4 it is an empty set. STEP 8 then begins after three iterations STEP 8: Construct the association rules for each large itemset a Form all possible association rules b\ Calculate the confidence factors for the above association rules. Assume the given min_conf c is 0.80. Take the association rule A.H 000\237 E.H as an example. The fuzzy count of A.H, E.H  calculated shown in Table 8  Table 8. The membership values for \(A.H, E.H Record A.H E.H \(A.H, E.H  1 0.13 1 0.13 2 0 1 0 3 1 1 1 4 0.03 0.80 0.03 5 1 0.80 0.80 6 0 0.75 0 7 0 0.92 0 8 0.23 0.92 0.23 9 1 0.92 0.92 10 0.20 0 0 count 3.59 8.11 3.11 The confidence factor for the association rule A.H 000\237 E.H is 87.0 59.3 11.3     10 1 10 1  007  000\246 000\246 HA HEHA HAs HEHAs c e factors of the above association rules are larger than or equal to the predefined min_conf c The rules that satisfy min_conf are output to user. These rules are thus considered as meta-knowledge concerning the given data set In the above process, the conventional algorithm Apriori for transactions database, gradually iterating from low level to high, may be used. But more feasible approach is mining the data set based on the predefined association rule patterns according to the field-relevant knowledge and requirements for the mining. In this rvices as the premise of association patterns. Thus, the output results are shown in Table 9 Considering the facts of grid monitoring and accounting data mining, we apply the time-window method since many association rules may not be included in a record. The main difference between the algorithm based on time-window and the conventional one is that each transaction is used as a unit of scan in the conventional algorithm, and time-window algorithm takes the multiple records divided by timewindow, that is a new record in the extended data set as a scan scope. Due to introducing the concepts of time-window and fuzzy set, we can mine association rule for quantitative data based on the attribute and time dimensions in grid performance data se Table 9. The rules output to user ID Rule Support Confidence 1 A.M 000\237 B.H 4.93 1.00 2 A.M 000\237 E.H 3.98 0.81 3 A.H 000\237 B.H 3.57 0.99 4 A.H 000\237 E.H 3.11 0.87 5 B.H 000\237 A.M 5.08 0.83 6 B.H 000\237 A.H 5.36 0.87 7 A.M, B.H 000\237 D.H 3.16 1.00 5. Application of Association Rule Through the above association rule mining for the grid performance data, the important mete-knowledge behind of mass data can be acquired. To apply these results to optimize grid scheduling, we must further analyze and evaluate the association rules discovered and select the rules that are conducive to optimization In the example above, the association rule A.M B.H 000\237 D.H describes the relationship between the utilization of grid service A and B and the bandwidth utilization of the partial network D Since A.M denotes 215middle utilization of grid service A\216, B.H \215high utilization of grid service B\216 and D.H \215high bandwidth utilization of the partial network D\216, the corresponding optimization logic that describes the association rule above is defined as Figure 2 min_sup s min_conf c time-window w struct object status time_window conclusion premise1 = utilization of A premise2 = utilization of B if \(premise1 == MIDDLE\& \(premise2 == HIGH  conclusion.object = \215D\216 conclusion.status = HIGH conclusion.time_window w  Predictioninfo\(conclusion\; //notify the prediction information  Figure 2 Optimization logic of association rule A.M, B.H 000\237 D.H When the optimization system detects the utilization of grid service A is middle and B is high, it notifies scheduler that the bandwidth utilization of the partial network D may be high within w time-window period Thus, scheduler should decrease further more resource Proceedings of the First International Conferen ce on e-Science and Grid Computing \(e-Science\22205 0-7695-2448-6/05 $20.00 \251 2005 IEEE 


requests for the partial network D within the corresponding period to avoid the bottleneck of resource acquisition Whether the strong association rules discovered can be used to optimize grid scheduling still needs to be analyzed and discussed further. Whether a rule is interesting or not can be judged either subjectively or objectively. The user can judge if a given rule is interesting or not, and this judgment, being subjective may differ from one user to another. However objective interestingness measures, based on the statistical independence and correlation analysis, can be used as one step towards the goal of weeding out uninteresting rules from presentation to the user 6. Conclusion Grid scheduling technique is more complex than the conventional scheduling technique in high performance one of the major factors that affect the grid performance. The monitoring and accounting tools may record the activities and status of various grid objects in the form of large amount of historical performance data. In order to take good advantage of these data, this paper applies an association rule mining technique to the process of grid scheduling, constructs optimization logic according to the mining results, and finally optimizes the grid scheduling. In the process of data mining, a method of association rule mining is proposed based on time-window and fuzzy set, which can mine association rule for quantitative data based on the attribute and time dimensions in grid performance data set Although the proposed method works well in data mining for quantitative data, several limitations remain to be addressed. The above method assumes that the membership functions are known in advance. In the future, we will attempt to dynamically adjust the membership functions in the proposed mining algorithm so as to avoid the bottleneck of membership function acquisition for mining and analyzing the monitoring and accounting data that contain application activities with categorical attributes, we will also discuss and design specific data mining models for grid scheduling optimization References  X  H S u n and M  W u  215G rid Har vest S e rvi ce: a s y ste m  for long-term, application-level task scheduling\216 Proc of the International Parallel and Distributed Processing Symposium Apr. 2003 2  N  H  K a p a d i a  J  A  B  F o r t e s  a n d C  E  B r o d l e y   215Predictive application-performance modeling in a computational Grid environment\216 Proc. of the 8th International Conference on High Performance Distributed Computing Aug. 1999, pp.47-54 3 D i m e m a s P r oje c t ht tp w w w c e p ba upc e s  dim em a s  4 D  A  Ba c i g a lupo S A  J a rv is L. H e a n d G  R N u dd  215An investigation into the application of different performance prediction techniques to e-Commerce applications\216 Proc. of the 18th International Parallel and Distributed Processing Symposium Apr. 2004 5 N W S P r o j e c t htt p nw s  c s uc s b.e du 6 R A G M A  P r oje c t  htt p://w ww pr a g m a g rid.ne t  H Z h u M   P a rash ar J. Yan g Y  Z h an g S  Rao an d S   Hariri, \215Self-adapting, self-optimizing runtime management of Grid applications using PRAGMA\216 Proc. of the International Parallel and Distributed Processing Symposium Apr. 2003 8 A  M. A l k i ndi, D  J  K e rby s on E P a pa e f s t a t hiou, a n d  G. R. Nudd, \215Run-time Optimization Using Dynamic Performance Prediction\216 Proc. of the International Conference on High Performance Computing and Networking 2000, pp.280-289 9 A u topilot P r oje c t htt p://w ww.re nc i.u nc e du Proje c t   Autopilot/AutopilotOverview.htm 10 r A D S P r oje c t  http w ww hipe rsof t.ric e  e du/g ra ds 1 H  P i nto  J Han J  P e i K W a n g  Q C h en  an d U  Dayal, \215Multi-dimensional sequential pattern mining\216 Proc. of the 10th International Conference on and knowledge management Oct. 2001 pp.81-88 12  J  H a n, G  D ong a nd Y  Y i n, \215 E ff ic i e nt m i ning of  partial periodic patterns in time series database\216 Proc of the International Conference on Data Engineering  Mar. 1999, pp.105-115 13  T  P  H o n g  C  S  K u o  a n d S   C  C h i  215 M i n i n g f u z z y  sequential patterns from quantitative data\216 Proc. of the International Conference on Systems, Man, and Cybernetics Oct. 1999, Vol.3, pp.962-966 14 R. A g ra wa l, T  Im ie link s i, a nd A  Sw a m i, \215 M ining  in Large Database\216 Proc. of the ACM SIGMOD Conference  Washington DC, USA, 1993 15  T  P  H ong K  Y  L i n, a nd S  L  W a ng 215 M ining f u zzy  sequential patterns from multiple-item transactions\216 Proc. of IFSA World Congress and the 20th International Conference on NAFIPS July 2001, Vol.3 pp.1317-1321 Proceedings of the First International Conferen ce on e-Science and Grid Computing \(e-Science\22205 0-7695-2448-6/05 $20.00 \251 2005 IEEE 


with respect to different numbers of recommended pages. This  397 experimenr used M i n k n g t h  = 2 and MuxSteps = 5 with different numbers of recommended pages \(from 1 to 10 experimental results are given in Fig. 7. When the number of recommended pages increases, the precision and srrfisfncfion also increase. But, the increase is not significant after the number of recommended pages is more than 5.  Although the precision and sarisfaction can be further improved with more e.g. 10 pagcs will affect the normal browsing activity. As such, we have used 5 as the default number of recommended pages for our system I I 100 90 80 70 60 50 40 30 20 IO 0         0 I 2  3 4 5 6 7 8 9 I O  II I The number of recommmended pages Hg. 7 Scalability vs. Number of recommended pages The last experiment measured the scalability of the sarisfaction of recommendation rules gcneration with respect to different numbers of steps. This experiment used MinLRngrlx  2  and MaxSteps = 5 with different numbers of steps \(from 1 to 10 8. When the number of steps becomes larger, the salisfaclion of recommendation web pages increases. However, the increase becomes insignificant as the number of steps exceeds 5. Therefore, we used a 5-step satisfaction for evaluating the performance of the web recommender system. The experimental results demonstrate that the proposed web recommendation approach is very effective for recommending related pages in the near future \(within 5 steps 102 90 80 70  50 40 30 2046 IO 0 0 1 2 3 4 5 6 7 8 9 1 0 1 1 m-step Rg. 8 Scatability vs m-step satisfaction To summarize, using a support threshold of 10 and recommending only the top 5 pages, the 5-srep satisjich  on achieves a value of 87% and 68% for the corresponding applicability. In plain English, this means that on average, 7 out of I O  visited pages will carry recommendations, and almost 9 out of 10 recommended pages are likely to be visited within the next 5 ciicks IV. CONCLUSION In this paper, we have proposed an intelligent web recommender system known as SWARS based on sequential web access patterns. In the proposed system, the sequential pattern mining algorithm CS-mine is used to mine frequent sequential web access patterns. The mined patterns are stored in the Pattern-tree, which is then uscd for matching and generating web links for online recommendations. The proposed system has achieved good performance with high 


proposed system has achieved good performance with high satisfaction and applicability REFERENCES M. Eirinaki and M.  Vazirgiannis  Web mining for web personalization   ACM Transaction!, on Internet Technology, Vol. 3, No. I ,  2003, pp. 1 27 1. Konstan, B. Miller, D. Mall i  J. Herlocker, L. Gordon, and 1. Riedl  GroupLens: applying collaborative filtering to usenet news   Communications of the ACM, 40\(3 T.W. Yan and H. Garcia-Molina  The SIFT information dissemination system  ACM Transactions on Database Systems, Vol. 24, No. 4, 19W T. loachims, D. Freitag, and T. Mitchel  Webwatcher: a tour guide for the World Wide Web  Proc. of the 5th International Joint Conference on AI. Japan. 1997, pp. 770-775 C. Shahabi, F. Banaei-Kashani, Y. Chen, and 0. McLeod  Yoda: an accurate and scalable web-based recommendation system  Roc. of the 6th International Conference on Cooperative Information Systems CoopIS 2001 B. Mobasher, H. Dai, T. Luo and M. Nakagawa  Effective personalization based on association rule discovery from web usage data  hoc. of the 3rd ACM Workshop on Web Information and Data Management \(WIDMOI W. Lin, S.  A.AIvarez, and C. Ruiz  ollaborative recommendation via adaptive association rule mining  Proc. of the Web Mining for E Commerce Workshop \(WebKDD2000 B. Mobwher  A web personalization engine baed on user transaction clustering  Prcc. of the 9th Workshop on Information Technologies and Systems \(WlTS  99 D.S. Phatak. and R. Mulvaney  Clustering for personalized mobile web usage  Roc. ofthe IEEE FUZZ  OZ, Hawaii. May 2002. pp. 705-710 pp.529-565 IO] R Agnwal, and R. Srikant  Mining Sequential Patterns  Prw. of the 1 lth International Conference on Data Engineering, Taiwan, 1995 I  11 B.Y. Zhou, S.C. Hui, and A.C.M. Fong  CS-mine: an efficient WAP tree mining for web acces  patlems  the 6th Asia Pacific Web Conference \(APWEBW 532 I21 R. Srikant, and R. Agrawal  Mining sequential patterns: generalizations and performance improvements  hac. of the 5th lntematianal Conference on Extending Darabase Technology \(EDBT France, 1996, pp. 3-17 I131 R. Cooky, B.  Mobasher, and J. Srivasrava  Data preparation for mining World Wide Web browsing patterns  Journal of Knowledge and Information Systems, Vol. 1, No. 1. 1999 1141 D. Knuth  The art of computer programming  Vol. 2, 2nd edition Addison-Wesley, October 1998 398 pre></body></html 


in four unknowns: n, ?np, \(?2?j |HT 2?j |HL The Neyman-Pearson decision rule is said to maximize the probability of detecting the track-lost regime, PDET for a given n, subject to the constraint that PFA is less than or equal to some user-de?ned probability. [6]. However, it is convenient in this context to instead ?x both PDET and PFA at some desired values, and use \(41 43 the corresponding minimum \(integer n that must be used in order to meet these goals. Typically there is a range of values of ?np that will achieve them Appropriate values of n and ?np can be determined for each pe element ?j using \(41 43 determinations for each element can then be made using 38 39 for each pe element; if a single overall track-loss metric is desired, Wishart distributed versions of \(33 35 also be used in the development of the Neyman-Pearson decision rule. However, because of the approximate nature of the distribution in \(35 additional covariance information is useful in the matrix likelihood test [5]. Further, a Wishart implementation has the disadvantage of being more computationally intensive than the implementation outlined above D. Application to the PDAF In order to implement the Neyman-Pearson rule, \(15 be rewritten x  k|k  k|k ? 1 k k 44 where, for the PDAF e?\(k  mk i=1 i zi\(k 1? ?0  k|k ? 1   45 This  effective  measurement innovation is then the single lter prediction error for purposes of calculating the sample variances s2?j . The j-th diagonal of the innovations covariance for the tracking regime, ST?sirf , and track-lost regime SL?sirf , can then be used for \(?2?j |HT 2?j |HL respectively Some method of handling the case of no gated measurements must be implemented. As suggested earlier, it is at least mathematically consistent in this case to de?ne the measurement innovations as zero. However, when there is a signi?cant probability of zero measurements gating the assumption that the prediction errors have a Gaussian distribution will be a poor one; in practice, this probability is likely to be signi?cant both when PD &lt; 1 and when in the track-lost regime. So the goals for PDET and PFA will not generally be met unless the innovations from timesteps when no measurements gate are excluded, and it is necessary to use the previous nC timesteps to ?nd n nonzero innovations with which to compute s2?j . The average number of timesteps n  T used when the ?lter is operating in the tracking regime is approximately n/\(PDPG average number of timesteps n  L used to when the ?lter is in the track-lost regime is problem speci?c E. Extensions to Other Data Association Methods This strategy is suitable for other data association methods. For the NN ?lter, the KF measurement innovations are the prediction errors. For MAP and maximum likelihood ML determined by similarly deriving a relationship between the estimated state, predicted state, and Kalman gain IV. EXAMPLE: PDAF TRACK REGIME TEST In this section, an example system using the PDAF is constructed. The SIRF approximation steady-state innovations covariances for the tracking and track-lost regimes are calculated and compared to simulation results. An example 


calculated and compared to simulation results. An example Neyman-Pearson decision rule is determined. Theoretical and simulation results for the probabilities of detection of the track-lost regime, PDET, and of false alarm while in the tracking regime, PFA, are given. An estimate of the number of timesteps required to detect track-loss is also provided 4320 A. Dynamics The kinematic model system \(with timestep x\(k + 1  1 0 1  x\(k  2/2   w\(k 46 y\(k  1 0  x\(k 47 is the standard zero-order hold discrete approximation to a continuous double-integrator system. For ? = 0.1 x\(k + 1  1 0.1 0 1  x\(k  0.005 0.1  w\(k 48 B. Kalman Filter System For Q = Q = 1000, R = R = 0.1 P  kf  0.3000 1.9998 1.9998 19.9965  Skf = 0.4000 are the steady-state covariances C. Clutter and Gating Though the example system is dependent on Q,R and ?, it can be described in the tracking regime using just three independent parameters [12]: the probability of detecting the truth measurement, PD, the normalized target acceleration, \(NTA sity, \(NCD the target is maneuvering, and NCD is a measure of how dif?cult it is to localize the target from the measurements For the example system, NTA = 1. Choosing PD = 1 0.02 results in NCD = 0.002, and an average track-lifetime of approximately 500 timesteps using the PDAF [8], [12 this is a tracking problem of  moderate  dif?culty for the example dynamics Using a four standard deviation gate \(? = 16 PG = 0.99994. For the example system, nz = 1, cnz = 2 and Vk = 8 |S\(k D. Experimental Tracking and Track-lost Regimes The following experimental setup allows for  controlled  track-loss and is used to verify operation of the trackloss detector. Until timestep 1000, the ?lter is tracking in the sense that PD = 1 \(the truth measurement is always 


available to the gating test 2000, PD = 0 \(the truth measurement is never available to the gating test sense that the truth measurement is never gated. PFA can then be calculated using measurement innovations from the rst 1000 timesteps, and PDET from using measurement innovations from the second 1000 timesteps, with roughly 1000 values of s2?j being tested by the decision rule in each regime. Note that PDET is thus calculated using both data points from the transient of  controlled  track-loss as well as from the  steady-state  operation of the ?lter in the tracklost regime E. PDAF SIRF Tracking Approximation For the example system with tracking regime assumptions: PG ? PD = 1, q1 ? PD = 1 [3], and \(25 T\(?Vk cnz 2  mk=1 exp\(??Vk Vk mk ? 1   nz nz/2  I2\(mk 49 Figure 1 shows the resulting function for ?T\(?Vk 0 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Vk   T  L Fig. 1. ?T and ?L as functions of ?Vk for example system ating \(4 6 7 11 24 using \(49 T\(?Vk  T = 0.9602, ?V  k 0.1047, and P  T?sirf  0.3286 2.1122 2.1122 20.5492  ST?sirf = 0.4286 Simulating and then averaging P\(k|k? 1 k example PDAF tracking regime over 1000 timesteps gives P  T?sim  0.3210 2.0831 2.0831 20.3698  ST?sim = 0.4210 Thus ST?sirf has only 1.7% error when compared to the more time-consuming simulation results F. PDAF SIRF Track-lost Approximation Figure 1 shows the function for ?L\(?Vk system with track-lost regime assumptions. Iterating \(4 6 


7 11 30 31  L = 0.3264 V  k = 0.4594, and P  L?sirf  8.1431 15.8765 15.8765 56.2960  SL?sirf = 8.2431 Simulating and then averaging P\(k|k? 1 k example PDAF track-lost regime over 1000 timesteps gives P  L?sim  9.1253 17.3318 17.3318 54.3980  SL?sim = 9.2253 SL?sirf has 10.8% error when compared to the more time-consuming simulation results. In general, it has been observed that the approximation for the tracking regime is more accurate than for the track-lost regime when compared to the simulation results 4321 G. Neyman-Pearson Decision Rule PDET ? 0.99 and PFA ? 0.01 are reasonable targets for the decision rule. The number of effective innovations n necessary to meet these goals can be calculated by incrementing n until, for some value\(s and PFA ? 0.01 \(see Figure 2 the slope of the curve is ?np 0 0.005 0.01 0.015 0.02 0.975 0.98 0.985 0.99 0.995 1 target PD &amp; PFA PFA P D n=6 n=7 n=8 Fig. 2. Neyman-Pearson Threshold Curves As can be seen from the ?gure, the goals require n ? 7 timesteps. A good value for ?np \(one that exceeds the goals for both PDET and PFA 41 43 the means of 25 trials of the example experimental setup and the decision rule \(38 39 ST?sirf = 0.4286, and SL?sirf = 8.2431, the experimental results were PDET = 0.9997 and PFA = 0.0132. Though the actual process of track-loss is dif?cult to quantify, the experimental results n  T = 7.0 and n  L = 24.9 do provide some insight into the number of timesteps necessary for track-loss detection, as the average number of timesteps necessary, n  Loss, should be bounded by n  T ? n  Loss ? n  L V. SIMULATION RESULTS Table 1 provides a comparison of experimental values for PDET, PFA, n  T, and n  F across large ranges of NTA and NCD for PDAF simulation data when PD = 1. For each NTA-NCD combination, n and ?np have been chosen such that, theoretically, PDET ? 0.99 and PFA ? 0.01, with n as small as possible. The mean track-lifetime of all NTA-NCD combinations is approximately 100 timesteps where NCD is labeled  High  1000 timesteps where NCD is labeled  Medium  and 10,000 timesteps where NCD is labeled  Low  12]. The experimental values of PDET, and PFA were calculated using the means of 25 trials of the example experimental setup; trials were selected from realizations where track-loss did not occur until forced at timestep 1001 


where track-loss did not occur until forced at timestep 1001 Having the tracking and track-lost variances of the innovations spaced well apart is the condition for small n As Table 1 shows, this condition is met less often for low values of NTA In the track-lost regime, the assumption that the validated prediction errors are Gaussian distributed is usually quite conservative since the sample variances are generally larger than for a Gaussian distribution. This contributes to generally exceeding the PDET goals. Conversely, the sample variances in the tracking regime, while more Gaussian are still somewhat larger than would be expected from a Gaussian distribution, meaning that the PFA goals may not always be met. However, because n is restricted to integer values, the theoretical values of both PDET and PFA often exceed the desired values signi?cantly for low values of n Unlike the tracking regime, the track-lost regime cannot be described solely in terms of PD , NTA, and NCD. SL is non-linearly dependent on ?, so n, PDET, and PFA vary with ? as well. This can be seen by comparing the Medium NTA results in Table 1 with those in Table 2, where identical values of PD , NTA, and NCD constructed from different values of Q, R, and ? yield different results for n In general, lowering PD increases the tracking regime innovations variance ST?sirf , reducing the separation from SL?sirf and thus having the tendency to raise the required n to meet the goals for PDET and PFA. This can be seen by comparing the Low NTA data in Table 1 \(PD = 1 Table 3 \(PD = 0.9 Over the parameter space explored, the test \(decision rule VI. CONCLUSION AND ONGOING WORK A strategy has been laid out for creating a two-class decision rule to determine the regime of operation for the PDAF in the absence of truth data. Scalar information reduction factors can be used in an iterative scheme to predict the steady-state innovations covariance for both the tracking and track-lost regimes, which results in lower computational burden when compared to Monte Carlo simulation. Then a distribution can be assumed for the sample variances of the prediction errors. Together, these pieces of information constitute a model around which a Neyman-Pearson decision rule can be constructed, where the con?dences in both the probability of track-loss detection and of false alarms are explicitly chosen. Good performance of the test as a trackloss detector was demonstrated for an example system over a large range of tracking dif?culties Ongoing work includes modeling the effective innovations \(45 theoretical distributions for the prediction errors used in the decision rule \(38 desirable to more accurately model the sample distribution of the innovations variance, and it has been shown that the prediction errors of many data association algorithms can be well approximated by a Gaussian mixture [13 REFERENCES 1] Y. Bar-Shalom and T. Fortmann, Tracking and Data Association, Academic Press Inc., 1988 2] T. Fortmann, Y. Bar-Shalom, and Y. Scheffe  Sonar Tracking of Multiple Targets Using Joint Probabilistic Data Association  IEEE J. of Oceanic Engineering, July 1983 4322 TABLE I SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.152e3 1.153e3 1.073e4 5.963e3 11 11.0 336 0.591 0.9900 0.0081 1.0000 0.0099 5e-5 1e-4 0.1 Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





