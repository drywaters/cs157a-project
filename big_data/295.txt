Mining Association Rules from the Star Schema on a Parallel NCR Teradata Database System Soon M. Chung and Murali Mangamuri Department of Computer Science and Engineering Wright State University Dayton, Ohio 45435, USA schung@cs.wright.edu Abstract The Star schema is a popular relational database schema for representing multidimensional data in data warehouses. In this paper, we propose an efficient algorithm, named Star-miner, for mining association rules on the joined result of the tables in the Star schema. The proposed algorithm is designed to be implemented directly on a relational database system by using SQL queries. It is based on the unique properties of the Star schema and does not create the join result of the tables in the 
schema. We implemented and evaluated the Star-miner on a parallel NCR Teradata database system. The performance of Star-miner is more efficient and scalable than other algorithms which mine the association rules from the single joined table 1. Introduction Mining association rules is an important data mining problem and it is formally described in [1 a s f o llow s L e t I   i 1 i 2 i m be a set of items. Let D represent a set of transactions, where each transaction T contains a set of items, such that T  I. Each transaction is associated with a unique identifier, called transaction identifier \(TID\A set of items X is said to be in 
transaction T if X  T. An association rule is an implication of the form X => Y, where X  I, Y  I and X   The rule X => Y holds in the database D with confidence c if c% of the transactions in D that contain X also contain Y. The rule X Y has a support s if s% of the transactions in D contain X U Y The process of mining association rules is divided into two steps. Initially all sets of items \(frequent itemsets\that have support above the user specified minimum support are discovered. Then, association rules that have minimum confidence are easily derived from the frequent itemsets, as 
described in [2  I n this pa pe r  w e c onc e n tr a t e on the  computationally intensive step of finding the frequent itemsets There are many association rule mining algorithms proposed 1  A s s o c i a tion rule s w e re f i rs t introduc e d on m a rk e t ba s k e t  data and later extended to different kinds of data. Relational databases are widely used to manage corporate data. [8  de ta ils  the importance of integrating association rule mining with relational database systems. All these association rule mining algorithms assume that data is stored in a single table. Data stored in data warehouses spans across multiple tables, and one popular multidimensional data model used is the Star schema The Star schema is a relational database schema for 
representing multidimensional data. A Star schema consists of a central fact table \(relationship table\nd multiple dimension tables \(entity tables\he fact table forms the relationships among the dimension tables which contain the attributes of entities. The fact table contains foreign keys to the dimension tables. Typical association rule mining algorithms require the join of the fact table and the dimension tables to be computed first before frequent itemsets are mined. Even though the cost of performing the join is not significant compared to the mining step, the disadvantage of such an approach is that the size of the joined result is large. The number of attributes of the joined result is equal to the sum of the number of attributes of the 
individual tables, and the cost of mining is proportional to the number of attributes. The data stored in the joined result is redundant since the number of times each row of the dimension table appears in the final joined table is equal to the number of occurences of its key or transaction-id in the fact table. Thus the itemsets are counted more number of times even though they are in only few rows in the dimension tables Mining frequent itemsets from the Star schema is divided into two phases. In the first phase, frequent itemsets from each dimension table are mined. Since frequent itemsets are mined with respect to the final joined result, the number of occurrences of the keys of dimension tables in the fact table is 
used to count itemsets. This step is accomplished easily since the cardinality of the dimension tables is low compared to that of the fact table \(property of the Star schema\ next phase frequent itemsets mined in the previous phase are used for the efficient mining of the cross-table frequent itemsets. This is based on the fact that all the subsets of a frequent itemset should be frequent  [2  For e x a m ple c r os s ta ble c a ndida te   itemsets involving items from dimension tables A and B are generated by using the frequent itemsets whose items belong to only table A or table B. We extensively evaluated our algorithm on different datasets and the results prove that it is more 
efficient than mining the joined result 2. Related Work Mining association rules on vertically partitioned tables was first introduced in [4  A n a l g o rithm f o r m i ning  a ssoc ia tion rules on the joined result of the Star schema without performing the join operation was proposed in [6   H o w e v e r   both  algorithms are designed for data stored in file systems and use Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCC05 0-7695-2315-3/05 $ 20.00 IEEE 


specialized data structures. These algorithms require data to be loaded from the data warehouse before mining. On the other hand, our Star-miner algorithm can be implemented directly on a relational database system using SQL qureies 3. Mining Algorithm for the Star Schema In this paper, SQL queries used for the implementation of the proposed Star-miner are expressed in the extended relational algebraic notations listed in Table 1 L e t A  B, a nd C represent the dimension tables of a Star schema and FT be the fact table, without the loss of generality. Let a, b, and c be the transaction-ids \(or keys\of the dimension tables A, B, and C respectively. Then, the frequent itemsets on table T T=FT A B C on FT.a=A.a AND FT.b=B.b AND FT.c=C.c\hould be mined. We also assume that the attributes in the dimension tables are unique. Within each itemset, the items are ordered in the order of the dimension tables, which means items in A are followed by items in B, and items in C Notation Operation R1  R2 UNION  selection condition R\LECT  attribute list R\OJECTION R1 join condition R2 JOIN grouping attributes  function list AGGREGATE FUNCTION  P R RENAME \(table name  b1,b2, ƒ,bn R\ENAME \(attributes Phase 1  Mining  frequent itemsets on dimension tables The following queries are used to mine the frequent itemsets whose items belong to dimension table A, assuming table A contains n attributes excluding the transaction-id or key 1st pass  AI\(a,i1    a,i1 A  a,i2 A    a,in A T  i1,cnt AI a=a  a,cnt  a  COUNT a FT FA 1 i1,cnt  cnt>=:minsup   i1,cnt  i1  SUM cnt T RA 1 a,i1  a,i1 AI i1=i1 FA 1  2nd pass  T 1  a,i1,i2   P.a,P.i1,Q.i1   P RA 1  a=a AND i1<i1  Q RA 1  T 2  a,cnt  a  COUNT a FT FA 2 i1,i2,cnt  cnt>=:minsup   i1,i2,cnt  i1,i2  SUM cnt   i1,i2,cnt T 1 a=a T 2  T  a,i1,i2   P.a,P.i1,Q.i1   P RA 1  a=a AND i1<i1  Q RA 1  RA 2 a,i1,i2  a,i1,i2 T i1=i1 AND i2=i2 FA 2  kth pass  T 1  a,i1,i2, ƒ ,ik   P.a,P.i1,P.i2, ƒ ,P.ik-1,Q.ik-1   P RA k-1  a=a AND i1=i1 AND ƒ ik-2=ik-2 AND ik-1<ik-1  Q RA k-1  T 2  a,cnt  a  COUNT a FT FA k i1,i2, . . . ,ik,cnt  cnt>=:minsup   i1,i2, ƒ ,ik,cnt  i1,i2, ƒ ,ik  SUM cnt   i1,i2 ƒ ,ik,cnt T 1 a=a T 2  T  a,i1,i2, ƒ ,ik   P.a,P.i1,P.i2, ƒ ,P.ik-1,Q.ik-1   P RA k-1  a=a AND i1=i1 AND ƒ ik-2=ik-2 AND ik-1<ik-1  Q RA k-1  RA k a,i1,i2, . . . ,ik  a,i1,i2, ƒ ,ik T i1=i1 AND ƒ ik=ik FA k  In the first pass, frequent 1-itemsets are mined. Table AI is created with only 2 columns \(transaction-id, item\d rows are formed by coupling each non-key attribute of table A with its key or transaction-id. The number of occurrences of each key value of table A in FT is counted, and they are joined to AI to obtain the number of occurrences of each item of table A in the joined result. Frequent 1-itemsets are found by selecting only those items that meet the minimum support constraint. Table RA 1 is created to store all instances of the frequent items in dimension table A \(items together with their transaction identifiers In the second pass, table RA 1 is joined with itself to obtain all instances of the candidate 2-itemsets. The transaction-ids of the resultant table are replaced by their number of occurrences in table FT, and frequent 2-itemsets are found by selecting only  Table 1. Relational algebraic notations A a i1 i2 i3 a0 0 4 7 a1 2 5 6 a2 1 3 6 a3 0 4 8 a4 1 3 7 FT a b c a2 b3 c2 a3 b2 c3 a0 b3 c3 a2 b2 c3 a0 b2 c3 a3 b0 c1 a1 b2 c4 a4 b2 c3 a4 b0 c2 a4 b0 c3 B b i1 i2 i3 b0 9 12 15 b1 10 12 16 b2 11 13 15 b3 11 14 17 b4 10 12 15 C c i1 i2 i3 c0 18 23 25 c1 19 21 25 c2 19 23 24 c3 20 22 25 c4 20 22 26 Fi g ure 1. A Star Schema Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCC05 0-7695-2315-3/05 $ 20.00 IEEE 


those itemsets with at least the minimum support. Frequent kitemsets are found in a similar manner The purpose of creating RA k table, containing all instances of the frequent k-itemsets, is to easily find frequent \(k+1\msets and cross-table itemsets. Usually the dimension tables are smaller in size compared to the fact tables, thus the overhead of creating RA k table is minimal in terms of the storage space and the time required This procedure is repeated on all the other dimension tables. At the end of this step, all the frequent itemsets whose items belong to one dimension table are found \(FA 1 RA 1 FA 2 RA 2  FB 1 RB 1 FB 2 RB 2 FC 1 RC 1 FC 2 RC 2  Phase 2  Mining frequent itemsets across the dimension tables 1\ Mining frequent itemsets across any two dimension tables The following queries are used to find frequent itemsets whose items belong to dimension tables A and B. A m B n denotes the set of candidate itemsets with m A items and n B items. FA m B n denotes the set of frequent itemsets with m A items and n B items 1st pass  T 1  a,b,cnt  a,b  COUNT a,b FT T 2  i1,b,cnt  i1,b  SUM cnt   i1,b,cnt RA 1 a=a T 1  T 3  i1,i2,cnt   P.i1,Q.i1,P.cnt   P T 2  b=b  Q RB 1  FA 1 B 1 i1,i2,cnt  cnt>=:minsup   i1,i2,cnt  i1,i2  SUM cnt T 3  In the first pass, frequent 2-itemsets whose first item belongs to A and second item belongs to B are found. Distinct combinations of the transaction-id of A and the transaction-id of B present in FT \(T 1 are derived and joined to RA 1 on the transaction-id of A \(a\he resultant table consists of the associations between the items of A and the transaction-id of B b\ which are grouped together \(T 2 nd joined to RB 1 on the transaction-id of B \(b\ch tuple of the resultant table \(T 3  consists of an item of A, an item of B and the number of times they are present together in the joined result, from which frequent itemsets are found 2nd pass  T  i1,i2,i3   P.i1,P.i2,Q.i2   P FA 1 B 1  i1=i1 AND i2<i2  Q FA 1 B 1  A 1 B 2 i1,i2,i3  i1,i2,i3 T i2=i1 AND i3=i2 FB 2  T 1  a,b,cnt  a,b  COUNT a,b FT T 2  i1,b,cnt  i1,b  SUM cnt   i1,b,cnt   a,i1 RA 1 i1=i1  i1 A 1 B 2  a=a T 1  T 3  b,i1,i2 RB 2 i1=i2 AND i2=i3  i2,i3 A 1 B 2  T 4  i1,i2,i3,cnt   P.i1,Q.i1,Q.i2,P.cnt   P T 2  b=b  Q T 3  FA 1 B 2 i1,i2,i3,cnt  cnt>=:minsup   i1,i2,i3,cnt  i1,i2,i3  SUM cnt T 4  In the second pass, frequent 3-itemsets whose items belong to both A and B are found \(FA 1 B 2 FA 2 B 1 ndidate itemsets A 1 B 2 or FA 1 B 2 are generated by joining two itemsets with the same prefix in FA 1 B 1 and then pruning with FB 2 To find frequent itemsets, first all instances of items of table A that are present in the candidate itemsets are derived by joining RA 1 with A 1 B 2 on an A item. This table is joined with the derived table containing distinct combinations of the transaction-ids of A and B in FT \(T 1 transaction-id of A \(a\and the result is T 2 All instances of 2-itemsets in B that are present in the candidate itemsets are derived by joining RB 2 with A 1 B 2 on B items, and the result is T 3 These derived tables T 2 and T 3 are joined on the transaction-id of B \(b\nd the result consists of itemsets that has one item of A and two items of B and the number of times they are present together in the final joined result T  i1,i2,i3   P.i1,P.i2,Q.i2   P FA 2  i1=i1 AND i2<i2  Q FA 1 B 1  A 2 B 1 i1,i2,i3  i1,i2,i3 T i2=i1 AND i3=i2 FA 1 B 1  T 1  a,b,cnt  a,b  COUNT a,b FT T 2  a,i1,i2 RA 2 i1=i1 AND i2=i2  i1,i2 A 2 B 1  T 3  i1,a,cnt  i1,a  SUM cnt   i1,a,cnt   b,i1 RB 1 i1=i3  i3 A 2 B 1  b=b T 1  T 4  i1,i2,i3,cnt   P.i1,P.i2,Q.i1,Q.cnt   P T 2  a=a  Q T 3  FA 2 B 1 i1,i2,i3,cnt  cnt>=:minsup   i1,i2,i3,cnt  i1,i2,i3  SUM cnt T 4  Candidate itemsets \(A 2 B 1 or FA 2 B 1 are generated by joining two itemsets with the same prefix in FA 2 and FA 1 B 1 and then pruning with FA 1 B 1 The difference between the generation of FA 1 B 2 and FA 2 B 1 after deriving all instances of A itemsets and B itemsets, is that the derived table with fewer items is joined to FT. This is because the resultant derived table consists of associations between itemsets of one table and the transactionid of the other table, so it can be compressed more if it has fewer items. Therefore, to generate FA 2 B 1 all instances of B items present in the candidate itemsets are joined to FT on the transaction-id of B \(b\This table \(T 3 d to the derived table which consists of all instances of 2-itemsets of table A T 2 transaction-id of A \(a kth pass  In the kth pass, frequent \(k+1\msets whose items belong to both A and B are found \(FA 1 B k FA 2 B k-1 FA k-1 B 2 FA k B 1  Candidate \(k+1\msets are generated by joining two frequent k-itemsets with the same prefix, and then pruning k-1 times using the appropriate tables. Pruning is performed to check if all the k-subsets of the candidate \(k+1\set are frequent \(i.e subset infrequency based pruning\. Assuming that candidate k+1\sets are generated for FA m B n where \(m+n following procedure is used for j := 1 to j := k if j = 1 if n != 1 T 1  i1,i2, ƒ, ik+1   P.i1,P.i2, ƒ ,P.ik,Q.ik   P FA m B n-1  i1=i1 AND AND ik-1=ik-1 AND ik<ik  Q FA m B n-1  else Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCC05 0-7695-2315-3/05 $ 20.00 IEEE 


T 1  i1,i2, ƒ, ik+1   P.i1,P.i2, ƒ ,P.ik,Q.ik   P FA m  i1=i1 AND AND ik-1=ik-1 AND ik<ik  Q FA m-1 B n  else if j  if j = 2                                                     // FA 0 B n FB n T j  i1,i2 ƒ ,ik+1 T j-1 i2=i1 AND ƒ ik=ik-1 AND ik+1=ik FA m-1 B n  else T j  i1,i2 ƒ ,ik+1 T j-1 i1=i1 AND ij-2=ij-2 AND ij=ij-1  ƒ ik=ik-1 AND ik+1=ik FA m-1 B n  else T j  i1,i2 ƒ ,ik+1 T j-1 i1=i1 AND ij-2=ij-2 AND ij=ij-1  ƒ ik=ik-1 AND ik+1=ik FA m B n-1  Frequent \(k+1\msets are found in a similar way as explained in the second pass. All instances of the itemsets of A that are present in the candidate itemsets are derived by joining RA m with A m B n on A items. All instances of the itemsets of B that are present in the candidate itemsets are derived by joining RB n with A m B n on B items. The derived table with less number of items involved in the frequent itemset is joined on its transaction-id to the projection of FT. The resultant derived table is then joined with the other derived table, and the frequent itemsets are found T 1  a,b,cnt  a,b  COUNT a,b FT if m  T 2  i1,..,im,b,cnt  i1,..,im,b  SUM cnt   i1,..,im,b,cnt   a,i1,..,im RA m i1=i1 AND im=im  i1,..,im A m B n  a=a T 1  T 3  b,i1,ƒ,in RB n i1=im+1 AND i2=im+2 ƒ AND in=im+n  im+1,ƒ,im+n A m B n  T 4  i1,ƒ,im+n,cnt   P.i1,P.i2, ƒ ,P.im,Q.i1,Q.i2,ƒ,Q.in,P.cnt   P T 2  b=b  Q T 3  else T 2  a,i1,i2 RA m i1=i1 AND i2=i2 ƒ AND im=im  i1,i2 A m B n  T 3  i1,ƒ,in,a,cnt  i1,ƒ,in,a  SUM cnt   i1,ƒ,in,a,cnt   b,i1,ƒin RB n i1=im+1 AND ƒ in=im+n  im+1,ƒ,im+n A m B n  b=b T 1  T 4  i1,ƒ,im+n,cnt   P.i1,P.i2, ƒ ,P.im,Q.i1,Q.i2, ƒ,Q.in,Q.cnt   P T 2  a=a  Q T 3  FA m B n i1,i2,ƒ,im+n,cnt  cnt>=:minsup   i1,i2,ƒ,im+n,cnt  i1,i2,ƒ,im+n  SUM cnt T 4  The queries used are designed in such a way that the size of the derived tables involved in the join operations are reduced as much as possible. The first optimization step is that only the itemsets present in the candidate itemsets are derived by joining R tables with the candidate itemsets \(\(RA m  i1,..,im A m B n  RB n  im+1,..,im+n A m B n The size of FT is also reduced by selecting only the distinct combinations \(including their counts of the transaction-ids of the corresponding tables a,b  COUNT a,b FT\All instances of itemsets of one table \(with less number of items in the candidate itemset\d to the projection of FT on its transaction-id \(\(RA m A m B n  a=a  a,b FT\he resultant derived table is grouped with respect to the items of one table and the transaction-id of the other table   i1,..,im,b RA m A m B n  a=a  a,b FT\This step reduces the size of the derived table and optimizes the mining of the frequent itemsets, especially in the initial passes. For example in the first pass, after RA 1 is joined to FT \(RA 1 a=a  a,b FT the resultant derived table is grouped by one item of A and the transaction-id of  B \(i.e., maximum number of rows in the derived table = number of items in A × number of transactions in B\This table is joined to RB 1 RA 1 a=a FT b=b RB 1  and frequent 2-itemsets are found \(i.e., maximum number of rows in the final derived table = number of items in A × number of transactions in B × number of non-key attributes in B In the later passes, the size of the derived table produced by grouping itemsets of one table and the transaction-id of the other table is not reduced much, especially when the number of items of the table with less number of items present in the candidate itemsets is more than 1 \(i.e., A 2 B 2 A 2 B 3 A 3 B 3  But the number of candidates generated becomes smaller thereby the size of the derived tables is reduced. Thus, the performance of the above queries used to find frequent itemsets from two dimension tables does not entirely depend on the number of transactions in FT, whereas the performance of the algorithms that mine on a single joined table is sensitive to the number of candidates generated and the number of transactions in FT This procedure is repeated on all other pairs of dimension tables AC, BC\At the end of this step, all the frequent itemsets related to any two dimension tables are found \(FA 1 B 1 FA 1 B 2  FA 2 B 1 FA 1 B 3 FA 2 B 2 FA 3 B 1 FA 1 C 1 FA 1 C 2 FA 2 C 1  FA 1 C 3 FA 2 C 2 FA 3 C 1 FB 1 C 1 FB 1 C 2 FB 2 C 1 FB 1 C 3  FB 2 C 2 FB 3 C 1  2\Mining frequent itemsets across more than two dimensional tables The following queries are used to find frequent itemsets whose items belong to dimension tables A, B and C 1st pass  Candidate itemsets \(A 1 B 1 C 1 or FA 1 B 1 C 1 are generated by joining two itemsets with the same prefix in FA 1 B 1 and FA 1 C 1  and then pruning with FB 1 C 1  Frequent itemsets are found by first deriving all instances of A and B items present in the candidate itemsets by joining tables RA 1 RB 1 A 1 B 1 C 1 candidate itemsets\nd FT \(distinct combinations of a and b\he resultant table is joined with FT combinations of a, b and c\ to obtain the associations between itemsets of A and B, and the transaction-id of C. Frequent itemsets are found by joining with all instances of C items RC 1 sent in the candidate itemsets T  i1,i2,i3   P.i1,P.i2,Q.i2   P FA 1 B 1  i1=i1 AND i2<i2  Q FA 1 C 1  A 1 B 1 C 1 i1,i2,i3  i1,i2,i3 T i2=i1 AND i3=i2 FB 1 C 1  T 1  a,i1,i2 RA 1 i1=i1  i1,i2 A 1 B 1 C 1  T 2  b,i1,a RB 1 b=b  a,b FT T 3  a,i1,i2,b T 1 a=a AND i2=i1 T 2  T 4  i1,i2,c,cnt T 3 a=a AND b=b  a,b,c,cnt  a,b,c  COUNT a,b,c FT Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCC05 0-7695-2315-3/05 $ 20.00 IEEE 


T 5  i1,i2,c,cnt  i1,i2,c  SUM cnt T 4  T 6  i1,i2,i3,cnt   P.i1,P.i2,Q.i1,P.cnt   P T 5  c=c  Q   c,i1 RC 1 i1=i3  i3 A 1 B 1 C 1  FA 1 B 1 C 1 i1,i2,i3,cnt  cnt>=:minsup   i1,i2,i3,cnt  i1,i2,i3  SUM cnt T 6  2nd pass  Frequent 4-itemsets whose items belong to A, B and C are found. Candidate itemsets for FA 2 B 1 C 1 FA 1 B 2 C 1 FA 1 B 1 C 2 are generated by the following queries. Frequent itemsets are found in a similar way as in the first pass using the appropriate tables For example, to generate FA 2 B 1 C 1 RA 2 and A 2 B 1 C 1 are used in place of RA 1 and A 1 B 1 C 1  T  i1,i2,i3,i4   P.i1,P.i2,P.i3,Q.i3   P FA 2 B 1  i1=i1 AND i2=i2 AND i3<i3  Q FA 2 C 1  A 2 B 1 C 1 i1,i2,i3,i4  i1,i2,i3,i4   i1,i2,i3,i4 T i2=i1 AND i3=i2 AND i4=i3 FA 1 B 1 C 1  i1=i1 AND i3=i2 AND i4=i3 FA 1 B 1 C 1  T  i1,i2,i3,i4   P.i1,P.i2,P.i3,Q.i3   P FA 1 B 2  i1=i1 AND i2=i2 AND i3<i3  Q FA 1 B 1 C 1  A 1 B 2 C 1 i1,i2,i3,i4  i1,i2,i3,i4   i1,i2,i3,i4 T i2=i1 AND i3=i2 AND i4=i3 FB 2 C 1  i1=i1 AND i3=i2 AND i4=i3 FA 1 B 1 C 1  T  i1,i2,i3,i4   P.i1,P.i2,P.i3,Q.i3   P FA 1 B 1 C 1  i1=i1 AND i2=i2 AND i3<i3  Q FA 1 B 1 C 1  A 1 B 1 C 2 i1,i2,i3,i4  i1,i2,i3,i4   i1,i2,i3,i4 T i2=i1 AND i3=i2 AND i4=i3 FB 1 C 2  i1=i1 AND i3=i2 AND i4=i3 FA 1 C 2  In the subsequent passes, all combinations of items are generated. At the end of this step, all frequent itemsets related to tables A, B and C are found. If the schema contains four dimension tables, first all itemsets whose items belong to ABC ABD, ACD, BCD are found, and then all itemsets whose items belong to ABCD are found Mining the frequent itemsets across more than two dimension tables is different from mining the frequent itemsets across only two dimension tables. Assume that frequent 3-itemsets are found on tables A, B and C. All instances of 2-itemsets of both A and B that are present in the candidate itemsets are derived by joining tables RA 1 RB 1 A 1 B 1 C 1 and FT RA 1 i1=i1  i1,i2 A 1 B 1 C 1  a=a AND i2=i1 RB 1 b=b  a,b FT The derived table is joined to FT  a,b,c FT\\ and grouped together to obtain the associations between itemsets of A and B and the transaction-id of C \(i.e., maximum number of rows in the derived table = number of distinct 2-itemsets of A and B present in candidate itemsets × number of transactions in C The size of the resultant derived table is not reduced much after grouping, like the case with the initial passes of the frequent itemset mining on two dimension tables. But, in most cases, the number of candidate itemsets generated is smaller, thereby the size of the derived tables to be joined is reduced. Frequent itemsets are obtained by joining the derived table to all occurrences of C items \(i.e., maximum number of rows in the final derived table = number of distinct 2-itemsets of A and B present in the candidate itemsets × number of transactions in C number of non-key attributes in C Thus, the performance of the queries to find frequent 3-itemsets from three dimension tables depends on the number of crosstable frequent 2-itemsets found. In the case of having many dimension tables and a very low minimum support, the number of cross-table frequent 2-itemsets found from all the pairs of dimension tables is very high, thereby the performance of the queries would be degraded In the later passes, the number of candidates generated is much smaller and the frequent itemsets are found easily. The frequent itemsets whose items belong to more dimension tables are found in a similar fashion. As the number of involved dimension tables increases, the number of candidate itemsets generated would be smaller One minor disadvantage of our proposed algorithm is that more tables are generated since a table is created for every combination of items in subsequent passes until no more frequent itemset is found. To minimize this overhead, in the subsequent passes, candidates are not created for those combinations with at least one subset that has no frequent itemset. For example, if the number of frequent itemsets for the combination A p B q C r is 0, then in the subsequent passes, all combinations of A s B t C u where p 012 s, q 012 t and r 012 u are not considered for the generation of candidate itemsets 4. Performance Analysis We implemented our Star-miner algorithm on a NCR Teradata database system. It has two nodes, where each node consists of four Intel 700 MHz Xeon processors, 2 GB shared memory, and 36 GB disk space. The nodes are interconnected by a dual BYNET interconnection network supporting 960 Mbps of data bandwidth for each node. Moreover, nodes are connected to an external disk storage subsystem configured as a level-5 RAID Redundant Array of Inexpensive Disks\ with 288 GB disk space. SQL queries are embedded in C++ programs and submitted to Teradata RDBMS through ODBC. More details of the Teradata database system are described in [5  We used synthetic datasets generated according to the procedure described in [6  Initia lly a ll the dim e nsion ta ble s  a r e  generated using the parameter values in Table 2 Parameter Value Number of dimensions 3 Number of transactions in each dimension table 1K Number of  attributes in each dimension table 10 Largest size of frequent itemsets 4 Largest number of transactions with a common itemset 40 Domain size of each attribute 20  Table 2. Parameters for the generation of dimension tables Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCC05 0-7695-2315-3/05 $ 20.00 IEEE 


The number of transactions in the fact table depends on three parameters: the target frequency of the rules sup the number of maximal potentially frequent itemsets \(|L|\nd the number of noise transactions \(N\o generate cross-table itemsets  transaction-ids of the related dimension tables are grouped together sup times in the fact table. This process is repeated |L times so that |L| maximally potential frequent itemsets would be formed \(i.e., number of transactions in FT sup L|\ + N Noise transactions mean the transactions that do not contain frequent itemsets. In real-life datasets, not all the dimension tables are strongly related [7  F o r exam p l e, if th ere are th ree dimension tables A, B and C, most of the cross-table frequent itemsets may consist of items from A and B, and very few from other combinations of the dimension tables \(AC, BC, ABC\ In Table 3, the number of related tables is the parameter that represents this situation Parameter Value Number of transactions in the Fact table 100K Number of  related tables 2 The performance of our Star-miner is evaluated and compared with PSETM [5 a nd A p riori a l g o rithm s  P S ET M is a n SQL based frequent itemset mining algorithm on a single table, and it is developed on the NCR Teradata database system. For PSETM, the joined result is computed and converted to transaction-id, item\ format. The Apriori algorithm is implemented on a 500 MHz Pentium III machine with 512 MB memory, running Windows 2000. For both PSETM and Apriori, the time taken to compute the joined result \(of the dimension tables with the fact table\ not included in the performance results Star-miner is compared with PSETM and Apriori in terms of performance, even though Star-miner and PSETM are implemented on a parallel database system while Apriori is implemented on a PC. Like other SQL-based association rule mining algorithms, the performance of PSETM deteriorates with lower minimum support and a larger number of transactions [8  so it is not co m p arab le w ith  Star-m in er an d  Apriori for some datasets. To demonstarte that Star-miner is efficient even though it is also SQL-based, its performance was also compared with that of Apriori We measured the performance of the algorithms by varying the minimum support, the number of transactions, and the number of attributes of the dimension tables. Algorithms are evaluated on two types of datasets. In the first dataset, only two of the three dimension tables are strongly related; and in the second dataset, all three dimension tables are strongly related. In Figures 2-4, Star-miner1, PSETM1, and Apriori1 represent their performance for the first dataset; and Star-miner2, PSETM2 and Apriori2 represent their performance for the second dataset The effect of the minimum support on the total execution times of the three algorithms is shown in Figure 2. PSETM performs well for higher minimum support values, but it performance degrades as the minimum support value decreases. The execution time of PSETM increases sharply when minimum support value is decreased from 0.50% to 0.25%. Compared to PSETM, Apriori has steady increase in its execution time PSETM and Apriori have almost the same execution time for both datasets because the number of frequent itemsets is nearly equal. Star-miner is efficient for all the minimum support values on both datasets. Star-miner performs better if the number of related tables is smaller because of the overhead of creating a separate table for each combination of the items from the dimension tables. For example, for second dataset, after finding all frequent itemsets whose items belong to AB, BC and AC, all the frequent itemsets whose items belong to ABC are found i.e., FA 1 B 1 C 1 FA 2 B 1 C 1 FA 1 B 2 C 1 FA 1 B 1 C 2     0 3000 6000 9000 12000 15000 18000 1.00 0.75 0.50 0.25 Minimum Support Time \(sec Star-miner1 PSETM1 Ap r i o r i 1 Star-miner2 PSETM2 Ap r i o r i 2 0 4000 8000 12000 16000 20000 100 200 300 400 500 Number of Transactions \(in 1000s Time \(sec Star-miner1 PSETM1 Ap ri o ri 1 Star-miner2 PSETM2 Ap ri o ri 2 Figure 3 shows how the performance of the three algorithms scales up as the number of transactions in the fact table increases from 100K to 500K. A minimum support value of 0.50% was used. The execution time of both PSETM and Apriori increases almost linearly as the number of transactions increases, but Apriori performs better than PSETM. As expected, the effect of increasing the number of transactions is minimal for Star-miner. The number of frequent itemsets mined for all transaction sizes does not increase much since the minimum support is constant; and the performance of Starminer mainly depends on the number of frequent itemsets Table 3. Parameters for the g eneration of fact table Fi g ure 2. Effect of the Minimum Su pp or t Figure 3. Effect of the Number of Transactions Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCC05 0-7695-2315-3/05 $ 20.00 IEEE 


Figure 4 shows the effect of increasing the number of attributes of each dimension table. A minimum support value of 0.50 was used. When two more attributes are added to each dimension table, six more attributes are added to the joined result. As a result, the number of frequent itemsets mined increases multifold, and the execution time increases sharply for the three algorithms. PSETM ran out of available storage space when the number of attributes becomes 14 for each dimension table. Star-miner performs better in all the cases, but its execution time increases sharply when the number of attributes is increased from 14 to 16.  Interestingly, the number of frequent itemsets mined from the first dataset is more than that from the second dataset when the number of attributes is 14 or 16   0 5000 10000 15000 20000 25000 30000 8 10121416 Number of Attributes Time \(sec Star-miner1 PSETM1 Ap ri o ri 1 Star-miner2 PSETM2 Ap ri o ri 2 5. Conclusion In this paper, we propose an efficient algorithm, named Starminer, for mining frequent itemsets on the joined result of the tables in the Star schema. The proposed Star-miner algorithm can be implemented directly on any relational database system by using SQL queries. We implemented the Star-miner algorithm on a parallel NCR Teradata database system, and compared its performance with that of other algorithms in various cases. The Star-miner algorithm is more efficient than other algorithms which mine the frequent itemsets from a single joined result of the tables in the Star schema. The performance of Star-miner is quite scalable in terms of the number of attributes and the number of transactions in the dataset Acknowledgment This research was supported in part by NCR and AFRL/Wright Brothers Institute \(WBI References 1 R A g ra w a l, T  Im ie linsk i, a nd A  Sw a m i M ining  Association Rules between Sets of Items in Large Databases,Ž Proc of ACM SIGMOD Intl Conf. on Management of Data, 1993, pp. 207-216 2 R. A g ra w a l a nd R. Srik a n t F a s t A l g o rithm s  f o r Mining  Association Rules,Ž Proc. of VLDB Conf., 1994, pp 487-499 3  M H outs m a a nd A  Sw a m i S e t O rie nte d Mining f o r Association Rules in Relational Databases,Ž Proc. of Intl Conf. on Data Engineering, 1995, pp. 25-33 4 V. C  J e ns e n a nd N  Sopa r k a r   F r e que nt I t e m s e t Counting across Multiple Tables,Ž Proc. of Pacific-Asia Conf. on Knowledge Discovery and Data Mining, 2000 pp. 49-61 5 S M C hung a nd M. Ma ng a m uri M ining A s s o c i a tion Rules from Relations on a Parallel NCR Teradata System,Ž Intl Conf. on Information Technology Coding and Computing, ITCC 2004, IEEE CS Press 2004, pp. 465-470 6 E  K K Ng A  W  F u   an d  K  W a n g   M in in g Association Rules from Stars,Ž IEEE Intl Conf. on Data Mining, 2002, pp. 322-329 7 R  A g ra w a l a nd K  Shim  D e v e l oping T i g h tly Couple d Data Mining Applications on a Relational Database System,Ž Proc. of Intl Conf. on Knowledge Discovery and Data Mining, 1996, pp. 287-290 8  S. Sa ra w a g i S. T hom a s a nd R A g ra w a l  I nte g r a ting  Association Rule Mining with Relational Database Systems: Alternatives and Implications,Ž Proc. of ACM SIGMOD Intl Conf. on Management of Data, 1998, pp 343-354 9 Introduction to Teradata RDBMS NCR Teradata Division, 2002  Teradata RDBMS Database Design NCR Teradata Division, 2001  R. Elm a sri a nd S Na v a the  Fundamentals of Database Systems Fourth Edition, Addison-Wesley, 2003 Fi g ure 4 Effect of the Number of Attributes Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCC05 0-7695-2315-3/05 $ 20.00 IEEE 


Meo [14] has proposed a new model to evaluate dependencies in data mining problems. Our study also focuses on dependencies between items. However, we try to discern interestingness of ARs by using relatedness based on relationships between item-pairs. Brijs, et al 19] have introduced a micro-economic integerprogramming model for product selection \(PROFSET Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Our approach considers the simultaneous existence of complementarity and substitutability unlike Brijs, et al 19].  Substitution rules were introduced by Teng and others [20]. According to them, an item-set X is a substitute for Y if X and Y are negatively correlated along with the existence of a negative AR \(X  Y rule set. On the other hand, in our approach, the degree of substitutability \(a component of relatedness pair {x, y} is computed by identifying item-sets that occur in {x, y  s non co-occurring neighbourhood. Other relationships like flexible complementarity and mutual interaction have also been considered in our approach Dong and Li [9] have presented a method of evaluating interestingness of ARs in terms of neighbourhood-based unexpectedness. The neighbourhood of a rule is defined in terms of a distance function on rules.  In our work, we consider both co-occurring and non co-occurring neighbours of items. We also compare our interestingness coefficient \(IC interestingness Conviction \(V Int that IC compares favourably with V and Int in ranking ARs. IC also takes into account aspects of relatedness not accounted for by V and Int, thus making it more intuitively appealing. IC treats the antecedent and consequent of an AR in a symmetric fashion. As a part of our future work, we propose to examine the feasibility of generalizing IC to account for directionality of ARs. In addition, we also propose to apply the suggested framework on real-life datasets  7. References  1] A. A. Freitas. On Rule Interestingness Measures Knowledge-Based Systems, 12, 1999, pp. 309-315 2] A. Silberschatz, and A. Tuzhilin. What makes Patterns Interesting in Knowledge Discovery Systems. IEEE Transactions on Knowledge and Data Engineering, 8\(6 1996, pp. 970-974 3] B. Liu, W. Hsu, L. Mun, and H. Lee. Finding Interesting Patterns Using User Expectations. IEEE Transactions on Knowledge and Data Engineering, 11\(6 832 4] B. Liu, W. Hsu, S. Chen, and Y. Ma. Analyzing the Subjective Interestingness of Association Rules. IEEE Intelligent Systems, 15\(5 5] B. Padmanabhan, and A. Tuzhilin. Unexpectedness as a Measure of Interestingness in Knowledge Discovery Decision Support Systems, 27\(3 6] B. Shekar, and R. Natarajan. A Framework for Evaluating Knowledge-based Interestingness of Association Rules Fuzzy Optimization and Decision Making, 3\(2 pp. 157-185 7] E. R. Omiecinski. Alternative Interest Measures for Mining Associations in Databases. IEEE Transactions on Knowledge and Data Engineering, 15\(1 57-69 8] G. Adomavicius, and A. Tuzhilin. Discovery of Actionable Patterns in Databases: The Action Hierarchy Approach Proceedings of the Third International Conference on Knowledge Discovery and Data Mining\(KDD  1997 AAAI, 1997, pp. 111-114 9] G. Dong, and J. Li. Interestingness of Discovered Association Rules in Terms of Neighborhood-Based Unexpectedness. Proceedings of the Second Pacific-Asia Conference on Knowledge Discovery and Data Mining 1998, pp.72-86 10] J. F. Roddick, and S. Rice. What  s Interesting About Cricket  On Thresholds and Anticipation in Discovered Rules.  SIGKDD Explorations, 3\(1 11] P. Tan, and V. Kumar. Interestingness Measures for Association Patterns: A Perspective. KDD'2000 Workshop on Postprocessing in Machine Learning and Data Mining Boston, MA, August 2000 12] P. Tan, V. Kumar, V. and J. Srivastava. Selecting the Right Interestingness Measure for Association Patterns Information Systems, 29\(4 13] R. J. Hilderman, and H. J. Hamilton. Knowledge Discovery and Interestingness Measures: A Survey, Technical Report Department of Computer Science, University of Regina Canada, 1999 14] R. Meo. Theory of Dependence Values. ACM Transactions on Database Systems, 25\(3 15] S. Brin, R. Motwani, and C. Silverstein. Beyond Market Baskets: Generalizing Association Rules to Correlations Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, May 1997, pp.265276 16] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur. Dynamic Itemset Counting and Implication Rules for Market Basket Data. Proceedings of the ACM SIGMOD International Conference on Management of Data, May 1997, pp. 255264 17] S. Jaroszewicz, and D. A. Simovici.  A General Measure of Rule Interestingness. Proceedings of the 5th European Conference on Principles of Data Mining and Knowledge Discovery \(PKDD 2001 Freiburg, September 2001, pp. 253-266 18] S. Sahar. Interestingness Via What is Not Interesting Proceedings of the 5th ACM, SIGKDD International Conference on Knowledge Discovery and Data Mining 1999, pp. 332-336 19] T. Brijs, G. Swinnen, K. Vanhoof, and G. Wets. Building and Association Rules Framework to Improve Product Assortment Decisions. Data Mining and knowledge Discovery, 8, 2004, pp. 7-23 


Discovery, 8, 2004, pp. 7-23 20] W. Teng, M. Hsieh, and M. Chen. On the Mining of Substitution Rules for Statistically Dependent Items Proceedings of IEEE International Conference on Data Mining \(ICDM  02 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207–216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Int’l Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Int’l Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





