The continuous growth in connection speed al\255 lows huge amounts of data to be transferred through a net\255 work An important issue in this context is network traf\255 fic analysis to profile communications and detect security threats Association rule extraction is a widely used ex\255 ploratory technique which has been exploited in different contexts e.g network traffic characterization However to discover potentially relevant knowledge a very low sup\255 port threshold needs to be enforced hence generating a large number of unmanageable rules To address this issue in net\255 
Abstract 
NEtwork Digest analysis by means of association rules Daniele Apiletti Elena Baralis Tania Cerquitelli Vincenzo D'Elia 
work traffic analysis an efficient technique to reduce traffic volume is needed This paper presents a NEtwork Digest framework which performs network traffic analysis by means of data mining techniques to characterize traffic data and detect anomalies NED exploits continuous queries to efficiently perform real\255 time aggregation of captured network data and supports fil\255 tering operations to further reduce traffic volume focusing on relevant data Furthermore NED provides an efficient algorithm to perform refinement analysis by means of asso\255 ciation rules to discover traffic features Extracted rules al\255 low traffic data characterization in terms of correlation and recurrence of feature patterns Preliminary experimental results performed on different 
network dumps showed the efficiency and effectiveness of the NED framework to char\255 acterize traffic data 
Association rules Continuous queries Net\255 work traffic analysis Stream analysis I INTRODUCTION Due to the continuous growth in network speed ter\255 abytes of data may be transferred through a network every day Thus two major issues hamper network data capture and analysis i 
huge amount of data can be collected in a very short time e.g an Ethernet frame at 10Gbps is received in less than 70ns ii It is hard to identify cor\255 relations and detect anomalies in real-time on such large network traffic traces New efficient techniques able to 
Index Terms 
A 
deal with huge network traffic data need to be devised 
A 
significant effort has been devoted to the application of data mining techniques to network traffic analysis 6 The application domains include studying correlations among data e.g association rule extraction for network traffic characterization 4 11 or for router misconfiguration de\255 tection 14 extracting information for prediction e.g multilevel traffic classification 13  Naive Bayes classifica\255 tion 16 grouping network data with similar properties e.g clustering algorithms for intrusion detection 18 or for classification  9 21 15 While classification al\255 gorithms require previous knowledge of the application do\255 main e.g a labeled traffic trace association rule extrac\255 tion does not Hence the latter is a widely used exploratory 
D Apiletti E Baralis T Cerquitelli 
D'Elia are with the Dipartimento di Automatica e Informatica Politecnico di Torino Corso Duca degli Abruzzi 24 10129 Torino Italy daniele.apiletti elena.baralis tania.cerquitelli vincenzo.delia polito.it technique to highlight hidden knowledge in network flows The extraction process is driven by enforcing a minimum frequency Le support constraint on the mined correla\255 tions However to discover potentially relevant knowledge a very low support constraint has to be enforced hence gen\255 erating a huge number of unmanageable rules 4 To ad\255 dress this issue a network digest representation for traffic data is needed Continuous queries are an efficient tech\255 nique to perform real-time aggregation and 
v 
filtering thus they can be exploited to effectively reduce traffic volume 
A 
new approach jointly taking advantage of both continu\255 ous queries and association rules could efficiently perform network traffic analysis This leads to the proposed framework called NED which performs network traffic analysis by means of data mining techniques to characterize traffic data and detect anomalies NED performs i On-line stream analysis to ag\255 gregate and filter network traffic and ii refinement anal\255 ysis to discover relationships among captured data NED allows on-line stream analysis concurrently with data cap\255 ture by means of user-defined continuous queries This step reduces the amount of network data thus 
obtaining mean\255 ingful network digests for pattern discovery Furthermore NED provides a refinement analysis to discover traffic fea\255 tures from network digests by means of association rule ex\255 traction NED's final output is a set of association rules 12 which are able to characterize network traffic and to show correlation and recurrence of patterns among data Prelim\255 inary experimental results performed on different network dumps showed the efficiency and effectiveness of the NED framework in characterizing traffic data and highlighting meaningful features The paper is organized as follows Section II presents an overview of the NED framework While Section III de\255 scribes the network traffic stream analysis performed by means of 
continuous queries Section IV presents the re\255 finement analysis phase In Section 
V 
Hidden knowl\255 edge extraction 
experiments to val\255 idate the proposed framework are reported Section VI draws conclusions and presents future developments of the proposed approach II OVERVIEW NED NEtwork Digest is a framework to efficiently per\255 form network traffic analysis NED addresses two main issues i to reduce the amount of traffic data and allow a more effective use both in time and space of data analysis techniques ii 
Data stream processing 
from traffic data to characterize network traffic detect anomalies and identify recurrent patterns Data stream processing is performed concurrently with 


Example is expressed in seconds which identifies the current set of data on which rules are applied iii the step length which defines how often the window moves and the output is produced In NED a record produced as output by the continuous query is a flow which summarizes a group of similar and temporally contiguous packets as shown in the following examples Step 2 UOT Figure 2\(b shows the output produced by the continuous query and how the window evolves Some steps are not reported due to lack of space jiltering whereas hid\255 den knowledge is extracted from the stored continuous query results in a refinement analysis step which currently implements an efficient Le filtered and aggregated packet digests can be saved into a permanent data store The storage is required only when different refinement anal\255 ysis sessions need to be performed The refinement analy\255 sis is a two step process i An optional data stream view block selects a suitable user-defined subset of flows to focus the following analysis on ii Association rule extraction is performed either on the data stream view which contains the selected flows or on all the flows in the permanent data store The aim of the refinement analysis is to discover interesting correlations recurrent patterns and anomalies among traffic data Association rule analysis is currently implemented but the framework allows different analysis techniques to be easily integrated To describe NED we will use a running example which will be validated on real datasets in Section V III DATA STREAM PROCESSING The data stream processing block of NED reduces the volume of traffic data by grouping similar packets and dis\255 carding irrelevant ones Network traffic can be considered as a stream of structured data Each packet is a record whose attributes are defined by network protocols In our running example the available attributes are source and destination IP addresses source and destination TCP ports the level 4 protocol e.g TCP UDP and the size of the packet Since packets are captured as an unbounded stream a conventional aggregation process would never ter\255 minate To overcome this issue continuous queries 3 are exploited and CQL Continuous Query Language 2 is used Queries are issued once and then logically run contin\255 uously over a sliding window of the original stream Hence the following parameters need to be defined i Aggrega\255 tion and filtering rules expressed in a subset of SQL in\255 structions ii a sliding window whose length length A Query association rule mining is 6 UOT Units Of Time and the output is produced every 2 UOT continuous queries 1 Le meaningless data for the current analysis is discarded of network traffic Their results can be stored on disk for several refinement analysis sessions Eventually meaning\255 ful knowledge is extracted in the refinement analysis session in the form of association rules e.g rules which represent correlations and implications among network traffic data cont1nuous queries Fig 1 NED's architecture Figure 1 shows NED's main blocks Data stream pro\255 cessing and refinement analysis Captured traffic packets are the input stream of the continuous query block whose objectives are i summarizing the traffic while preserving structural similarities among temporally contiguous pack\255 ets and ii discarding meaningless traffic to reduce traffic volume The output data capture by means of 1 Figure 2\(a reports a toy packet capture to describe how the sample query works The length of win\255 dow Three types of continuous queries are implemented in NED see Section III-A III-B and III-C for more details To improve CQL query readability aggregation queries and filtering queries are decoupled see Figure 3 Aggrega\255 tion is performed concurrently with data capturing while filtering can be executed both on line and off line The packet filtering is performed in the stream analysis block and discards meaningless packets from the aggregation whereas flow filtering is performed in the data stream view block and discards undesired flows for the specific analysis purpose The purpose of Query 1 is to reduce the volume of traffic data while preserving information about TCP flows their participants their size and their fragmentation  Stream analysis flows Le similar records can be summarized by a proper digest and aggregation algorithm Other data mining techniques 12 may be easily integrated in this step Continuous queries perform I Step 


I    I  I ii m+l   Port-Filtering pkt m,pkt f/own  is a set of dataset is available in the NED data store Le the input of the refinement analysis block Association rules identify collections of itemsets Le sets of length also called item is a couple attribute value An attribute models a characteristic of the flow e.g source address destination port Such a be a network traffic dataset whose generic record Each o 0 C   2      t A 1 A A B A B B B A B i 1 2 3 4 5 6 8 6 b Flows in window Fig 2 Packet aggregation  I 7}size B,i A A 2 A Timing Window that are statistically related Le frequent in From Aggregate Aggregate source-IP,source-Port,destination-IP destination-Port Sum size as flow-size Count as packets Packets Range 6 seconds level4  TCP source-IP,source-Port,destination-IP destination-Port Since this query targets all TCP flows in network traffic it does not perform any data filtering but simply aggre\255 gates by IP source address TCP source port IP destina\255 tion address and TCP destination port It also computes the total size and the number of packets of the flow From Port-Filtering Select From Where Select From Where Query 3 has two filtering stages Firstly only flows which do not have well-known ports as source and destination are kept Secondly the longest flows are selected If these two filtering stages are both performed in the continuous query the output flows are significantly reduced but differ\255 ent analysis types become unfeasible To avoid this limita\255 tion filters may be applied in the data stream view block IV REFINEMENT ANALYSIS NED discovers interesting correlations and recurrent pat\255 terns in network traffic data by means of association rule mining which is performed in the refinement analysis phase Let Ti:rnesta:rnp Source A Size This query targets the extraction of the longest IP traffic flows Once packets have been aggregated by source ad\255 dress and destination address flows whose length is lower than a given threshold are discarded The threshold is ex\255 pressed as a percentage of the total traffic of the current window Both filtering and aggregation considerably re\255 duce the dataset size This query targets the recognition of unconventional TCP traffic which is usually exchanged on ports different from the well-known ones Le port number 1024 a A toy packet capture Ti:rne Current flows Output Source Size 2,4,5 Select From Where Group By Select From Where Select From Where Group By Aggregate sourceIP,destinationIP Sum\(size as flow-size Count as packets Packets Range 6 seconds level3  IP sourceIP destinationIP Filter source-IP destination-IP flow-size Aggregate flow-size  ratio  Select Size-Filtering Aggregate sourcePort  1024 And destination-Port 1024 Port-Filtering flow-size  ratio  Select 15 20 Filter sourceIP sourcePort,destinationIP destination-Port flow-size Aggregate Aggregate source-IP,source-Port,destination-IP destination-Port Sum size  as flow-size Count as packets Packets Range 6 seconds level4  TCP sou rce-IP,source-Port destination-IP destination-Port  225 output stream Fig 3 Pipeline of continuous queries From Select From Where Group By Select c Query 3 Aggregate select source_address destination_address sum\(size as length from Packets range 6s group by source_address destination_add ress  I    6 A B B o 5 10 input stream sizeA,o l:::iE{O,l}sizeA i S'lZeB,2 L.:iE{O,1,3,6}sizeA i A,B 1,3,6}sizeA i L.:iE{2,4,5,7}S ize B i L.:iE{3,6}SiZ e A i A,B,C S'lZeC,8 Step Traffic Flow Feature Feature Traffic Feature 7 8 C 7 sizeA,O sizeA,l sizeB,2 sizeA,3 sizeB,4 sizeB,5 sizeA,6 sizeB,7 sizec,8 B Query 2 B 


Rule quality is usually measured by sup\255 port and confidence Support is the percentage of items containing both Yare It describes the strength of the im\255 plication Association rule mining is a two-step process i Frequent itemset extraction and ii association rule generation from frequent itemsets Given a support threshold D Al S{DA2,DPl  It describes the statistical rel\255 evance of a rule Confidence is the conditional probability of finding where szze dest addr dest addr Step x X Y X X Y p p s  I S{DAl,DPl 2 Consider the toy dataset in Fig 2\(a for the itemset mining process With a support threshold greater than 25 the 2-itemsets Select From Where Group By Aggregate source-IP,source-Port,destination-IP destination-Port Sum size as flow-size Packets Range 6 seconds level4  TCP source-IP,source-Port,destination-IP destinationPort  x the underlying dataset An association rule is represented in the form 3 The SYN flooding attack occurs when a vic\255 tim host receives more incomplete connection requests that it can handle To make this attack more difficult to detect the source host randomizes the source IP address of the packets used in the attack An attempt of SYN flood\255 ing 10 can be recognized by mining rules in the form  victim-IP victim-port  size  Feature given X DAl DPl DA2 DPl DAI DPl appears in c of has been set to 2  Due to lack 0 space reported results refer to experiments performed with 100 Mbps link speed and 60 s window length The ratio parameter of Queries 2 and 3 has been set to 0.1 To avoid discarding packets a proper buffer size has to be determined The buffer must be able to store all possi\255 ble flows in a time window whose worst case value is the maximum number of captured packets i.e each packet belongs to a different flow Thus the buffer size has been set to the following number of flows  link speed dest addr is said frequent if it appears in at least s of flows DP1 Flow dest addr Example Example dest Y    s s Feature which contains also A Data stream view   255 minimum size of a packet dest p dest    I A B C  window length  expressed in bytes the following query may be exploited to create a data stream view Filter Select sourceIP source-Port destinationIP destination Port flow-size From Aggregate Where flow-size     s support c confidence states that  dest p Since the complete dataset contains hundreds of flows the support of the SYN-flooding rule may be too low to be relevant To overcome this issue the output of the previous continuous query may be appropriately filtered Since we are interested in flows whose size is lower than a threshold and and an itemset i.e a set of 50  50 The refinement analysis performed on the results of the described data stream view extracts a small number of as\255 sociation rules characterized by high support These rules highlight more effectively any specific traffic behavior V EXPERIMENTAL VALIDATION A set of preliminary experiments have been performed by analyzing NED behavior on real datasets We assessed i the number of extracted association rules and ii the interest of mined rules TABLE I NETWORK TRAFFIC DATASETS are frequent Hence the flows directed to DA2 or DA1 at port DP1 are frequent Once mined frequent itemset association rules 1 12 are used to analyze their correlations Given the itemsets in 1 the following association rule  ID NUlllber of Packets 25969389 24763699 26023835 Size Mbyte 2621 2500 2625 The data stream view block allows to select a subset of the flows obtained as continuous query output The following example focusing on the SYN flooding attack shows its usefulness support c confidence where the left term is the victim fingerprint while the right part is the size of the flow which is supposed to be very small The frequency of this pattern is measured by means of support and confidence c Suppose that to reduce the amount of stored data the network traffic has been aggregated with respect to address and port of both source and destination For each flow the size is computed e.g packets differing in one of these fea\255 tures belong to different flows This step is performed by running the following continuous query on the data stream Three real datasets have been obtained by performing different capture stages using the Analyzer traffic tool 17 on a backbone link of our campus network We will re\255 fer to each dataset using the ID shown in Table I where the number of packets and the size of each dataset is also reported Experiments target the execution of the queries discussed in Section III They have been performed by considering three window lengths 60 s 120 s and 180 s and two link speeds 10 Mbps and 100 Mbps The value of the window window length f step disjoint conjunc\255 tions of  1  


x Example To further investigate the meaning of the rules we consider the following examples Example 0.1 and minimum confidence c 0.3 c 0.3 c com\255 munications Another filtering step is necessary to clearly identify involved hosts This issue has been addressed by Query 3 x.y.z.w  JJ address   6 Consider the following rule Frequent itemset extraction is based on the LCM v.2 algorithm 20 FIMI'04 best implementation algo\255 rithm while association rule generation is performed using Goethal's implementation of the Apriori algorithm 5 Experiments have been performed on a 2800 Mhz Pen\255 tium IV PC with 2 Gb main memory running Linux kernel 2.7.81 All reported execution times are real times includ\255 ing both system and user times They have been obtained using the Linux Figure 4\(a reports the number of extracted rules when varying support and confidence thresholds For Query 1 as reported in Figure 4\(a by enforcing high support and confidence thresholds the number of extracted patterns decreases The decreasing trend is particularly evident for high support thresholds whereas most of the rules have high confidence values for any support threshold Thus in this scenario support is more selective in filtering patterns  s 130.192.c.d Analyses performed on rules extracted from datasets B and C confirm the results obtained on dataset A The traf\255 fic network features inferred from the rules highlight the same NAT routing and servers To identify patterns arising from long flows another step of filtering is required This issue addressed by the second query has been analyzed in the next section s x  s command as in 8 130.192.a.b time A Query 1 address 130.192.a.b destination address D A destination port D P 50 leads to the extraction of a large amount of rules in the following form   port  x.y.z.w peer-to-peer   is frequent regardless of the port number these rules state that the address 130.192.a.b i generates remarkable traffic both as receiver and as transmitter ii it is likely to be a server which provides many services be\255 cause it uses a wide range of ports We can conclude that 130.192.a.b is probably the public IP address of a router implementing NAT An inspection of the network topology confirms such result 5 By setting the minimum support to 0.3 and the minimum confidence to 0.5 some interesting pat\255 terns are extracted NAT rules are still present and other rules become more evident For example  source-address B Query 2 99 98 highlights that Synchronet-rtc service is frequently and mostly used by 4 Considering minimum support JJ sourceport  443  JJ Query 1 Section III-A aggregates packets with respect to source address source port destination address and des\255 tination port Thus it significantly reduces the data car\255 dinality while preserving general traffic features Figure 4\(a reports the number of extracted rules for each dataset considering different support and confidence thresholds Since the three datasets have similar behav\255 ior we focus on dataset A where we observe that some 1-itemsets are highly frequent such as and The actual IP addresses have been masked for privacy reasons identifies 130.192.c.d as an https server It was confirmed to be the student webmail server   destination-port 6101  flow-size 96 JJ source-address The second query Section III-B selects the flows which generate an amount of traffic greater than a certain per\255 centage of the total traffic in a window The aim is to describe more accurately rules extracted by Query 1 Figure 4\(b shows the number of association rules ex\255 tracted from the results of Query 2 applied to the datasets A B and C Rules discovered in this case predominantly have the following form  The third query Section III-C extracts long flows whose source and destination ports are beyond 1024 Figure 4 c shows the number of association rules ex\255 tracted from the result of Query 3 applied to datasets A B and C Because of the additional filtering step the num\255 ber of rules is significantly lower than the ones extracted by Query 1 and Query 2 Furthermore these rules are even more specific than previous ones as shown by the following example 2 2     destination-address 130.192.a.b source  Many extracted rules describe general network features such as NAT routing or main servers Furthermore this analysis assesses the pervasiveness of different services Mined rules highlight the importance of several protocols like netrjs systat and QMTP in the examined network Some rules are worth further investigation Many flows have source and destination ports 1024 This fact may highlight unconventional traffic such as Since port 1 I 1 I c Query 3 source address SA source port SP Example 


Proceedings of ACM CSS Work\255 shop on Data Mining Applied to Security PA November 2001 Series Editor Morgan Kaufmann Publishers The Morgan Kaufmann Series in Data Management Systems Jim Gray August 2000 T Karagiannis K Papagiannaki and M Faloutsos Blinc mul\255 tilevel traffic classification in the dark In  MineNet 06 VI CONCLUSIONS AND FUTURE WORK NED is a framework to efficiently perform network traffic analysis NED provides techniques to perform data stream analysis and refinement analysis The former reduces the amount of traffic data while the latter automatically ex\255 tracts correlation and recurrence of patterns among traffic data Association rule extraction algorithms do not easily de\255 tect patterns nested in rare flows even if they might be rel\255 evant Hence future developments of the NED framework will explore generalized association rules Generalized as\255 sociation rules could represent network traffic at a higher level of abstraction and could become a more powerful tool to efficiently extract hidden knowledge not captured by tra\255 ditional approaches VII ACKNOWLEDGMENT We are grateful to Fulvio Risso for providing the real traffic datasets captured from the campus network REFERENCES 1 R Agrawal and R Srikant Fast Algorithms for Mining Associ\255 ation Rules in Large Databases 130.192.e.f GLOBECOM IEEE Internation Conference on Systems Man and Cybernetics Proceedings of the 20th Inter\255 national Conference on Very Large Data Bases 4 2003 Jiawei Han and Micheline Kamber   7 8 The SANS Institute Port 4662 details Available at http isc.sans.org port html port=4662 T Uno M Kiyomi and H Arimura LCM ver 2 Efficient min\255 ing algorithms for frequent/closed/maximal itemsets In ACM SIGMOD Record SIGCOMM 2004 Q Wang and V Megalooikonomu A clustering algorithm for intrusion detection pages 487-499 1994 2 A Arasu S Babu and J Widom The CQL continuous query language semantic foundations and query execution Data Mining Concepts and Techniques 1998 a Query 1 b Query 2 Fig 4 Number of extracted association rules c Query 3  source-address p MineNet 06 22\(10 1999 M Hossain S Bridges and R Vaughn Jr Adaptive intrusion detection with data mining 247 g J I Computer Communications 1.98 c l 225  225 98.,2,1998 J Erman M Arlitt and A Mahanti Traffic classification using clustering algorithms In Global Telecommunica\255 tions Conference 30\(3 2001 M Baldi E Baralis F Risso and D e Inf Data mining tech\255 niques for effective and scalable traffic analysis 5 Proc SPIE 5812:31-38 2005 SIGMETRICS 05 6 9 10 11 12 13 14 15 16 17 18 19 20 21 S Babu and J Widom Continuous queries over data streams A CM Transactions on Information and System Security TISSEC s The VLDB Journal The International Journal on Very Large Data Bases pages 229-240 2005 F Le S Lee T Wong H S Kim and D Newcomb Miner\255 als using data mining to detect router misconfigurations In 3  0 t1  JJ  destination-port 4662  Proceedings of Canadian Conference on Electrical and Computer Engineering Integrated N et\255 work Management 2005 1M 2005 2005 9th IFIP IEEE Inter\255 national Symposium on  The address 130.192.e.f is identified as having a remark\255 able amount of traffic toward remote hosts on port 4662 Since this is the default port for eDonkey2000 servers 19 of the ED2K peer to peer network we can conclude that i the source host is exchanging data with the ED2K servers and ii its amount of traffic on not well-known ports is mainly related to such peer to peer network pages 105-118 2005 Bart Goethals Frequent Pattern Mining Implementations Available at http://www.adrem.ua.ac.be/goethals/software K Burn-Thornton J Garibaldi and A Mahdi Pro-active net\255 work management using data mining pages 4-7 2003 B Harris and R Hunt TCP IP security threats and attack methods pages 293-298 New York NY USA 2006 ACM Press W Lee and S Stolfo A framework for construct in features and models for intrusion detection systems pages 50\255 60 New York NY USA 2005 ACM Press NetGroup Politecnico di Torino Analyzer 3.0 Available at http://analyzer.polito.it/30alpha  L Portnoy E Eskin and S Stolfo Intrusion detection with un\255 labeled data using clustering z FIMI 8 0  3\(4 2000 A W Moore and D Zuev Internet traffic classification using bayesian analysis techniques In pages 281-286 New York NY USA 2006 ACM Press FIMI http://fimi.cs.helsinki.fi  Y Guan A Ghorbani and N Belacel Y-Means A cluster\255 ing method for intrusion detection 15\(2 2006 c l 5   77 3 4 5 


Used-for references in the LCSH into holonym/meronym relations in our WKB  In the experiments we assume that each topic comes from an individual user We attempt to evaluate our model in an environment that covers great range of topics However it is not realistic to expect a participant to hold such great range of topics in personal interests Thus for the 50 experimental topics we assume each one coming from an individual user and learn her his personalized ontology An LIR is collected through searching the subject catalogue of Queensland University of Technology QUT Library 3 by using the title of a topic Librarians have assigned title table of content summary and a list of subjects to each information item e.g a book stored in QUT library The assigned subjects are treated as the tags in Web documents that cite the knowledge in the WKB  In order to simplify the experiments we only use the librarian summarized information title table of content and summary to represent an instance in an LIR  All these information can be downloaded from QUT's Web site and are available to the public Once the WKB and an LIR are ready an ontology is learned as described in Section 3.3.1 and personalized as in Section 3.3.2 The user con\002dence rates on the subjects are speci\002ed as in Section 3.3.3 A document d i in the training set is then generated by an instance i  and its support value is determined by support  d i   X s 2 021  i  s 2S sup  s Q  14 where s 2 S in O  Q  are as de\002ned in De\002nition 5 As sup  s Q   0 for s 2 S 000 according to Eq 11 the documents with support  d   0 go to D 000  whereas those with support  d   0 go to D   4.4 Performance Measures The performance of the experimental models are measured by three methods the precision averages at eleven standard recall levels 11SPR the mean average precision MAP and the F 1 Measure They are all based on precision and recall the modern IR evaluation methods The 11SPR is reported suitable for information gathering and is used in TREC evaluations as a performance measuring standard An 11SPR v alue is computed by summing the interpolated precisions at the speci\002ed recall cutoff and then dividing by the number of topics P N i 1 precision 025 N  025  f 0  0  0  1  0  2      1  0 g  15 N is the number of topics and 025 are the cutoff points where the precisions are interpolated At each 025 point an aver3 http://library.qut.edu.au Figure 2 Experimental 11SPR Results age precision value over N topics is calculated These average precisions then link to a curve describing the recallprecision performance The MAP is a stable and discriminating choice in information gathering evaluations and is recommended for measuring general-purpose information gathering methods The average precision for each topic is the mean of the precision obtained after each relevant document is retrieved The MAP for the 50 experimental topics is then the mean of the average precision scores of each of the individual topics in the experiments The MAP re\003ects the performance in a non-interpolated recall-precision fashion F 1 Measure is also well accepted by the information gathering community which is calculated by F 1  2 002 precision 002 recall precision  recall  16 Precision and recall are evenly weighted in F 1 Measure For each topic the macro F 1 Measure averages the precision and recall and then calculates F 1 Measure whereas the micro F 1 Measure calculates the F 1 Measure for each returned result and then averages the F 1 Measure values The greater F 1 values indicate the better performance 5 Results and Discussions The experiments attempt to evaluate our proposed model by comparing to an implementation of mental model We expect that the ONTO model can achieve at least the close performance to the TREC model The experimental 11SPR results are illustrated in Fig 2 At recall point 0.3 the TREC model slightly outperformed the ONTO model but at 0.5 and 0.6 the ONTO model achieved better results than the TREC model subtly At all other points their 11SPR results are just the same For the MAP results shown on Table 1 the ONTO model achieved 0.284 which is just 0.006 below the TREC model 2 
512 
516 


TREC ONTO p-value Macro-FM 0.388 0.386 0.862 Micro-FM 0.356 0.355 0.896 MAP 0.290 0.284 0.484 Table 1 Other Experimental results downgrade For the average macroand microF 1 Measures also shown on Table 1 the TREC model only outperformed the ONTO model by 0.002 0.5 in macro F 1 and 0.001 0.2 in micro F 1  The two models achieved almost the same performance The evaluation result is promising The statistical test is also performed on the experimental results in order to analyze the evaluation's reliability As suggested by we use the Student's Paired T-Test for the signi\002cance test The null hypothesis in our T-Test is that no difference exists in two comparing models When two tests produce substantially low p-value usually  0.05 the null hypothesis can be rejected In contrast when two tests produce high p-value usually  0.1 there is not or just little practical difference between two models The T-Test results are also presented on Table 1 The pvalue s show that there is no evidence of signi\002cant difference between two experimental models as the produced pvalue s are quite high  p-value 0.484\(MAP 0.862\(macroFM and 0.896\(micro-FM far greater than 0.1 Thus we can conclude that in terms of statistics our proposed model has the same performance as the golden TREC model and the evaluation result is reliable The advantage of the TREC model is that the experimental topics and the training sets are generated by the same linguists manually They as users perfectly know their information needs and what they are looking for in the training sets Therefore it is reasonable that the TREC model performed better than the ONTO model as we cannot expect that a computational model could outperform a such perfect manual model However the knowledge contained in TREC model's training sets is well formed for human beings to understand but not for computers The contained knowledge is not mathematically formalized and speci\002ed The ONTO model on the other hand formally speci\002es the user background knowledge and the related semantic relations using the world knowledge base and local instance repositories The mathematic formalizations are ideal for computers to understand This leverages the performance of the ONTO model As a result as shown on Fig 2 and Table 1 the ONTO model achieved almost the same performance as that of the TREC model 6 Conclusions In this paper an ontology-based knowledge IR framework is proposed aiming to discover a user's background knowledge to improve IR performance The framework consists of a user's mental model a querying model a computer model and an ontology model A world knowledge base is used by the computer model to construct an ontology to simulate a user's mental model and the ontology is personalized by using the user's local instance repository The semantic relations of hypernym/hyponym holonym/meronym and synonym are speci\002ed in the ontology model The framework is successfully evaluated by comparing to a manual user model The ontology-based framework is a novel contribution to knowledge engineering and Web information retrieval References   C Buckley and E M Voorhees Evaluating evaluation measure stability In Proc of SIGIR 00  pages 33–40 2000   R M Colomb Information Spaces The Architecture of Cyberspace  Springer 2002   D Dou G Frishkoff J Rong R Frank A Malony and D Tucker Development of neuroelectromagnetic ontologies\(NEMO a framework for mining brainwave ontologies In Proc of KDD 07  pages 270–279 2007   S Gauch J Chaffee and A Pretschner Ontology-based personalized search and browsing Web Intelligence and Agent Systems  1\(3-4 2003   X Jiang and A.-H Tan Mining ontological knowledge from domain-speci\002c text documents In Proc of ICDM 05  pages 665–668 2005   J D King Y Li X Tao and R Nayak Mining World Knowledge for Analysis of Search Engine Content Web Intelligence and Agent Systems  5\(3 2007   D D Lewis Y Yang T G Rose and F Li RCV1 A new benchmark collection for text categorization research Journal of Machine Learning Research  5:361–397 2004   Y Li and N Zhong Mining Ontology for Automatically Acquiring Web User Information Needs IEEE Transactions on Knowledge and Data Engineering  18\(4 2006   H Liu and P Singh ConceptNet a practical commonsense reasoning toolkit BT Technology  22\(4 2004   A D Maedche Ontology Learning for the Semantic Web  Kluwer Academic Publisher 2002   S E Robertson and I Soboroff The TREC 2002 002ltering track report In Text REtrieval Conference  2002   M D Smucker J Allan and B Carterette A Comparison of Statistical Signi\002cance Tests for Information Retrieval Evaluation In Proc of CIKM'07  pages 623–632 2007   X Tao Y Li and R Nayak A knowledge retrieval model using ontology mining and user pro\002ling Integrated Computer-Aided Engineering  15\(4 2008   X Tao Y Li N Zhong and R Nayak Ontology mining for personalzied web information gathering In Proc of WI 07  pages 351–358 2007   T Tran P Cimiano S Rudolph and R Studer Ontologybased interpretation of keywords for semantic search In Proc of the 6th ICSW  pages 523–536 2007   Y Y Yao Y Zeng N Zhong and X Huang Knowedge retrieval KR In Proc of WI 07  pages 729–735 2007 
513 
517 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


