Estimating Itemsets of Interest by Sampling Shichao Zhang and Chengqi Zhang School of Computing and Mathematics Deakin University, Geelong Vic 3217 Australia  scz,chengqi}@deakin.edu.au Abstract begin with describing preliminaries in next section In Section 3 we estimate the size of the sample of a Many applications such as marketing and stock in given very large database In Section 4 we present vestment may be time limited and few considera a method to identify the approximating support of tions to accurate itemsets 
For these applications itemsets of interest In Section 5 we evaluate the ef we present a mining model to quickly estimate the fectiveness of the proposed approach experimentally approximate support of frequent itemsets of interest Finally we summarize our contributions in section 6 in large scale databases in this paper An efficient al gorithm is thus designed to reduce the searched space by pruning uninterested frequent itemsets 2 Preliminaries Let I  il iz   i be a set of N distinct literal 1 Introduction called items 
D is a set of variable length transac tions over I Each transaction contains a set of items One of the main challenges in data mining is to iden i,,iz  it E I A transaction has an associated tify frequent itemsets for very large databases that unique identifier called TID An association rule is comprise millions of transactions and items Conse an implication of the form A  B or written as quently, some recent efforts have focused on designing A t B where A 
B c I and A n B  0 A is efficient algorithms 4 51 The main 223limitation\224 of called the antecedent of the rule and B is called the these approaches however is that they require mul consequent of the rule tiple passes over a database For very large databases In general a set of items is called an itemset The that are typically disk resident this requires reading number of items in an itemset A is the length or the database completely for each pass resulting in a the 
size of A sometimes denoted as Al Itemsets of large number of disk I/Os Accordingly many vari some length k are referred to as a k-itemsets For an ants of Apriori algorithm such as sampling 4 and itemset A B if B is an m-itemset then B is called OPUS bascd algorithm 151 have been reported an m-extension of A However many applications such as marketing and Each itemset has an associated measure of statisti stock investment may be time limited 
and little re cal significance called support denoted as supp or p quirement on the accurate of the supports of item For an itemset A c I supp\(A  s or p\(A  s if sets Indeed for a short-time stock investor 223time the fraction of transactions in D containing A equals is money\224 to him So heishe needs approximating s A rule A  B has a measure of its strength frequent itemsets from stock databases quickly The called confidence denoted 
as conf defined as the approximating frequent itemsets are enough to enable ratio p\(A U B A himlher to make a optimal decision on his/her invest The problem of mining association rules is to gen ments so as to obtain a good reward Reversedly if a erate all des A  B that have both support and stock investor pursues accurate supports of frequent confidence greater than or equal to some user speci itemsets in databases, heishe may lose the best in fied minimum support minsupp and minimum con vesting time Hence quickly approximating mining fidence \(minconf 
thresholds respectively models have being expected to be developed For ap There are two main reasons obliging us to ex plications we construct an algorithm to discover ap plore high-speed models for identifying approximat proximating frequent itemsets of interest from very ing frequent itemsets in very large databases One large databases in this paper is that the databases in many applications often in The rest of this paper is organized as follows We volve a million or even over several millions of trans 0-7803-7293-X/01/%17M 0 2001 IEEE 131 2001 IEEE International Fuzzy Systems Conference 


actions and often millions of items Mining such large obtained This seems in some sense to be a good databases is a time consuming procedure For exam estimate of 0 ple the applications such as marketing and stock in In this way a database D can be taken as a trial vestment may be time limited and few requirement For any itemset A it is 1 if the itemset A occurs in a on accurate supports of itemsets mentioned above In transaction T written as T\(A else it is 0 written other wards faster algorithms of digging up approx as 7T\(A Suppose the probability of A occurring imating frequent itemsets in large scale databases is in the database is p and the probability of A not eager to be explored Fortunately some techniques occurring is y  1  p Hence this given database in Statistics are helpful to this problem can be taken as a Bernoulli trial according to the Another one is that some applications which in definition in l In particular we can approximate volve large quantities of data in databases solving the probability p of A by central limit theorem We exact frequent itemsets may often be computation estimate the sample size by a theorem as follows ally expensive and in many cases not even physically tractable they have to be relied on approximate re Theorem 1 Let D be a large database TI Tz   sults when sources are bounded To mine frequent Tm be the tmnsactions in D A be an itemset in D itemsets for these applications, approximation is tac q  0 be the degree of asymptotic to frequent itemsets itly an efficient and viable method E 2 0 be the upperprobability of P[IAve\(X 5 q Recently;'%ome fast model of mining approximate where Ave\(Xn is the auemge Of A occurring in n frequent itemsets by Chernoff bounds have been pro transactions in D suppose records in D am matched posed in 3 41 In this paper we adopt a slightly BertldlI kidS If n random records of D is enough different approach to this problem which can mine for determining the approximate frequent itemsets in approximate frequent itemsets quickly D according to central limct theorem n must be as Our model is required to discover frequent itemsets foiiowS of interest using a 2-step procedure 1 Generate a random subset of a given large database; and 2 Generate all frequent itemsets in the random subset minimum support 1 Z?1+<1/2 n  by pruning having support greater than or equal to 4v2 where zz is a standard normal distribution function n 3 Generating Instance Set We now illustrate the use of this theorem by an example as follows In probability theory if a situation is such that only two outcomes often called success and failure are l 1 suppose a new process is possible it is usually cailed a trial The variable ele doping silicon ment in a trial is described by a probability distribu unknown is the tion on a sample space of two elements 0 representing in this way is defective We assume that the defec failure and 1 success this distribution assigning the tive hips are independent of each many Probability  to 0 and 0 to 1 where 5 2 chips n mast we pmduce and test so that the pmpor Suppose we consider n independent repetitions of a tion of defective chips found Aue\(X does not dif given trial The variable element in these is described fer fmm at least by a probability distribution on a sample space of 2 0.99 That is we want n such that P\(IAve\(X  points the typical point being z  zirzzl zn 0.01  0.99 q  0.01,E  0.995,z0  2.57 we where each z is 0 or 1 and zi represents the result hawe of the ith trial The appropriate probability distri bution is defined by _ for used in electron;c devices that each by more than 0.01 with n  2'572  16513 4  0.012 pe\(z  om\(zl\(1  Q  where m\(z  E z is the number of Is in the results of the n trials, this being so since the trials are independent Given an z in this situation it seems reasonable to estimate 0 by m\(z the proportion of successes considerably smaller than the value n  27000 that is needed by using the approzimating model in Chernof bounds p 41 Based on Theorem 1 the method of generating random database from real database is as 1 gen erating a set X of pseudo-random numbers where 132 


1x1  n and 2 generating the random database RD from D using pseudo-random number set X That is for any zi E X get zi  1 record of D and append it into RD Note that generating random database RD of the given database D doesn't mean to establish a new database RD It only needs to build a view RD over D 4 Identifying Approximate Fre quent Itemsets of Interest In this section we present an efficient algorithm by pruning all uninterested frequent itemsets Piatetsky-Shapiro 2 argued that a rule X t Y is not interesting if support\(X U Y e suppurt\(X x support\(Y Using this argument we establish an al gorithm below for searching itemsets of interest in a database quickly Algorithm 1 FrequentIternsetsbyPmning mininterest minimum interest Input D data set minsupp minimum support Output Frequentset frequent itemsets 1 let frequent itemset set Frequentset t 0 generate the sample RD 2 let L1 t frequent 1-itemsets let Frequentset t Frequentset U LI 3 for k  2 Lk-1  0 k   do begin Generate k-itemsets of interest in RD let ck t the k-itemsets in ED by Lk-1 for any transaction t in RD do begin Check which k-itemsets are included in t let Ct t k-itemsets in t contained by C for any itemset A in Ct do let A.cmnt t A.cuunt  1 end let LI t IC E CK A p\(c  c.count//RDI  minsupp let Frequentset t Frequentset U Lk Prune all uninterested k-itemsets in ck for any itemset i in ck do if an itemset i is uninterested then kt ck ck  i end 4 output frequent itemsets in Frequentset 5 endall The algorithm FrequentItemsetshyPruning gener ates all frequent itemsets of interest in sample RD It is similar to the former algorithm FrequentItem sets So we only elucidate the differences in Step 3 Step 3 generates all sets Lk for k 2 2 by a loop where Lk is the set of all frequent k-itemsets in RD generated in the kth pass of the algorithm and the end-condition of the loop is Lk-1  0 For k  2 we need to prune all uninterested k-itemsets from the set C That is for any itemset i in ck if ip\(X U Y  p\(X Y  mininterest for any ex pressions i  X U Y of i then i is an uninterested frequent itemset and it must be pruned from ck To demonstrate the use of the above algorithm we present the following example Example 2 Let RD a subset of a given transaction database D with 10 transactions in Table 1 is obtained from a grocery store Let A  bread B  coffee C  tea D  sugar E  beer F  butter As sume minsupp  0.3 and mininterest  0.07 The supports of frequent itemsets in Frequentset by Fk quentItemsetsbyPruning are shown below Table 1 Transaction databases in RD I hnsaction ID 1 Items In Table 1 there are six 1-itemsets A B C D E and F in RD For minsupp  0.3 they are all frequent 1-itemsets in Frequentset And L1  A,B,C,D,E,F In Table 1 the set CZ of the 2-itemsets is AB AC AD AE, AF BC BD BE, BF CD CE, CF DE DF and EF Each one of the above 2-itemsets contains at least a subset of LI For minsupp  0.3 LZ  AB AC AD, BC BD BF,CD CF are all frequent 2-itemsets However for minsupp  0.3 and mininterest  0.07 Lz  BC,BD are all frequent 2-itemsets in Frequentset listed in Table 2 below for the al gorithm FrequentItemsetshyPruning Certainly be cause Ip\(A U B  p\(A B  0.05  mininterest Ip\(A U C  p\(A C  0  mininterest Ip\(A U 133 


D  p\(A D  0  mininterest Ip\(B U F  p\(B F  0.05  mininterest Ip\(C U D  p\(C D  0.06  mininterest and Ip\(C U F  p\(C F  0  mininterest so AB AC AD BF CD and CF are not of interest Hence AB AC AD BF CD and CF are pruned from Lz before it is appended into Frequentset Table 2 2-ltemsets in Frequentset I Item 1 Number of I Support I In Table 1 because all 3-itemsets are pruned are pruned from C because all of them are not of in terest And Step 2 ends The frequent itemsets in Frequentset are output As we have seen there are only two frequent 2 itemsets in Frequentset by the algorithm Frequen tItemsetshyPruning after the uninterested frequent itemsets are pruned Totally there are eight frequent itemsets of interest in Frequentset 5 Experiments To study the effectiveness of our model we have performed several experiments Our server is Ora cle 8.0.3 and the algorithm is implemented on Sun SparcServer using Java and JDBC API is used as the interface between the program and Oracle To evaluate our model we have used market transaction databases from the Synthetic Classification Data Sets in Internet http://www.kdnuggets.com We eval uated two methods the sampling approach based on Chernoff bounds 3,4 denoted LRD and our model denoted CRD Below Table 3 summarizes the pa rameters for the data sets Table 3 Synthetic data set characteristics T  row size on average I  size of mazimal frequent sets on auerage 1 Data set name 1 lRI I T I I I lrl I The comparison of our model on CRD with the model on LRD in running time is illustrated in Fig ures 1 and 2 From the figures our model CRD needs lesser time than the model LRD hecause the latter needs larger sample than the former and the former applies more heuristic information to reduce the searched space Figure 1 Running time for minsupp  0.01 7 e Figure 2 Running time for rninsupp  0.015 6 Conclusions Mining frequent itemsets is an expensive process Mining approximate frequent itemsets on a sample of a large database can reduce the computation cost significantly 3 and 4 applied the Chernoff hounds to discover frequent itemsets in large databases For many applications such as marketing and stock in vestment they may be time limited and little re quirement on the accurate of the supports of itemsets For example a short-time stock investor take time as money To satisfy these applications we have pre sented an efficient model for identifying approximat ing frequent itemsets of interest in this paper Our experiments have shown that the proposed approach is effectiveness and efficiency References l R Durrett Probability Theory and Examples Duxbury Press 1996 2 G Piatetsky-Shapiro Discovery analysis and presentation of strong rules In Knowledge dis covery in Databases 1991: 229-248 31 R Srikant and R Agrawal, Mining generalized association rules fiture Generation Computer Systems Vol 13 1997: 161-180 ciation rules In VLDB\22296 1996: 134-145 4 H Toivonen, Sampling large databases for asso SI Geoffrey I Webb Efficient search for association rules In SIGKDD\222OO 2000 99-107 134 


3.4 ExpcriIncntal rcsults For algorithm IDIC-M Sevcral sets of expcrimonts wcrc conducted to coriiparc tlic 1131CM algorithm with thc FUP algorithm and ttiG piirc DIC algorilhtn Note that each data poinl in ttic cxperimen tal rcsul tr is oblained frwm thc average vnlue ovcr 10 wials Thc synthesis data uscd in all the cxperiments arc geiicratcd folluwing ttie tcchniqucs in 43 Table 1 Parameter table Wc use the nota~ion Tx.ty.Drn.dn to deriiitc R dnlabnse cif sixe U  lOOOiri is updated with at1 iiicrcment with size d  1OOl wliilc TI  x and I  y In wir expcIiinctits conipiiring thc IDIC3l nlgoI'ithTl1 iizid the FUP algorithm D and d wcre chosen in siniilitr way iis hi 4 and IT  IO We generated U  d transactions by otic raiidoiii scctl Then thc Brsr D trimsaclions arc slorcrl for Db and the ollrcrs ibr db Notc that in lie sccoiid sct ofcxperimcrits in Scctirm 3.4.2 Di and db wcrc gcncrated scparntcly wirh different rniidom sccds For our IDICN algorithm thc PUP nlgorithm and thc DIC nlgciritliin fhcir exccution time spccdup nttios over prc Apriori algoriitirn arc calculnlcd to cvalualc their perliirtnancc Nntc thiit piirc Apriori al gnrithrii hcrc nicans applying thc Apriori nlgorithtri oi~cc 0vc.r tho updalcd rla~basc WC first cuiiihinc the original dntiihasc nntl tlic iiicreiiietital dntabasc into ii new iiprlatcd dntalmc lien lie Apriori algorillitri is applictlo11 this IICW updatctl database to ohlain thc ncw hrgc iteniscis wc want Spccdup ratio lata poiiir on lie curve ApriorifiUIC-M is ciilciilelcd by dividing ttic cxecution time of the piire Apii ori algorirhin with Ihc execution tiiiic of ttic IDIC-M nip sittiin 111 7 4 IL  2000 illd N  1000 3.4.1 IDIC-M versus PUP for diffcrent supports Thc IDICN algorilli~n and ttic FUP algorithiti werc lcstcd for different 0 ranging from 0.01 to 0.05 with t1pdiWd rlorahasc T10.14.D100.d 1 which is similar to the one iiscd in 4 Tlic intcrval sizes lor counting db niid DU WCIX fixzd nl t 000 and 10000 respcctivcly The rcsiilt is slzown in Figurc 1 The 1DIC-M iilgorithm clearly has a bettcr exccution tinic specdup ratio for iilt sup ports tcstcd Notc that our algoritlitri has n subsiantinl id Minimum Support Figure 1 IDIC-M and FUP for different sup ports vantngc over the FUP algorithm whcn thc niiriirnuni suplmt.1 threshold is sct at at R rclativcly Iiighcr value Tlic spccrfup ratio orthe IDTCN algorithm is morc tiinn I 5 tiiiies highcr than thc PUP algorithiti when 0  0.02 I'hc IDIC-M nl gririthin is inorc an 8 litiics hstcr than the pt1r.e Apriori algfirithtil at this support rhrcshofd luc Wc can sec Lhiit whcn the Support tbrcshold is low tic 1DIC.M ilgoritIim docs riot IIRVC I vcry signiticant nrlvanmgc ovcr tic 1'UP al gorithm Sitice D vcry hrgc nuitihcr ol candidate itetriscts nre cxpec~tl to he gcncmtcil from db in this citsc we can oliscrvc ttint 1131C-M ni;ty not hc vcry clricicnr in dealing with those large oiiiouni CII catididatcs 3.4.2 IDIC-M vcrsiis FUI for diffcrcnt incremcntnl sixes Wc haw donc two scts nT cxpwimculs to camparc tlic pcr formancc of tbc Il>IC_M algarithm with tlic E'UP alp rihrn ror dil'rcrcnt incrcmcntnl six d ranging from IO00 to 500U0 with DU fixcd at 100000 i.c TIO.I4,1~100,dn 8 is lixctl at 0.0 15 For thc firs1 sct orexpcrimcnrs ttic DD and rib arc gencratcd as tisual using thc snmc mndotn sceil Whilc for ttic sccontl sct ofcxpcrimnts the DI3 and I arc gonernlctl scparatcly using dilfcrcnt random sccds The rcsults for lirst sct orexpcrimcnts arc shown in Fig iirc 2 I'he IDIC-M illgoritlim outpcrforins thc FUP nlgn rithtn lor dl incrctiiental sizcs testcd I1 works well cspc cinlly when ilic incrcincntal sixc is smnll I'hc greatest ad vantngc of our algorithm ovcr tic PUP algorithtn was ob tained when d  1000 wlierc the speetliip rrtrio of IDCM is iilrout 7.3 and tllc spdilp ratio uf FU1 is bout 4 WC scc tlrc trend that ils thc iticrcniental size continues to in thc FUP algorithm rlecrcasc I'tiis is diic to the fhct that wlicri thc incrctriciital size is large relalive to the origin;II dutabasc sizc the vaIuc of thc known infrmiation ahout he CFC IS I lie pcrformaiiccs of both the 1131C-M algorithm and 88 


87 I 7 I I ApriorillDlC-M  AprioriiFUP x ApriorillUIC-M I  Apriori/FUP lncremental Slro thousands Figure 2 IDIC-M and FUP for different incre mental sizes with the same random seed origiiial databasc will be lowcr This cxperiincnl caii sim ulatc situiltiuns whcrc the incortiiiig data nrc having similar pnttcrns to the original databasc 3.4.3 IDIC-M versus DIC for diffcrcnt incrcmental sizes Expcrirncnts weic done to cotnpilrc the pcrfoitnancc of thc IUICM algorithni with thc DIC algorithm for different in crcmcntol sixc 11 ranging horn 10000 to 50000 with LIB fixcd at 50000 i.c TlO.I4.D50.tln 0 is fixed at 0.015 Wc are cspccially interestcd in the situation that wbcn the incrcmcntal sizc is relntivc largc comparcd to tlie origi~ial database sizc Note that what we mean 1 DIC here is that we applied the DIC algnrilhtii oncc ovcr tlie updated dalahnsc 5 1 0  I 3 n 3 V U R2 t 0  Aprl&l/lOIC-M AorioritDlc x I I I  I 10 20 30 40 60 lncremental Size thousands Figure 4 IDIC-M and DIC for different incre mental sizes I I I 1 5 10 25 50 lncrernerilal Slzo thousaotla Figure 3 IDIC-M and FUP for different incre mental sizes with different random seeds The resi~lls l'or sccond set of cxpcriinctits arc shown in Figtire 3 Tlic IDIC-M nlgoritlim also ouipcrrorms llic FUI nlgorihm for nil iiicrcniental sizes teslcd Our algorithm has R considcrabte advi>ntage ovcr the FUP nlgoritlirii except wlieii thc incrcrncnral size is iicar to 10000 I'hc greatest ul vantagc of our algorithm ovcr ttic FlJP algorithtn was also ohlaineil at rl  1000 Note that thc curvc of the TDIC-M algoii~li~n convcrgcs to n high valw than thc ciirvc of FLIP docs Also note that nur IDIC-M algorithm is much better chm the PUP algolilhtn in this sihiation wbcre thc origi nal ilatahasc and ttic incrcmcntd datatusc wcrc gcncrntd by diffcrcnt random sccds Thc situation sitnulatcd by this cxperiinciit does cxist in rcal lili when c3ch hatch of iicw incoming dah havc a tlifrcrcnt transaction IxitEcni l'hc rcsults Tor ttic cxpcriincnts arc shuwii in Pigum 4 Although tlic advntitagc is tlecrcasing with ihc increincn tal six niir IDIC-M algorithin outpcrfornis the DIC nl gorirhni for all incrcmentnl sizcs testcd with consider;lblc nmoiin The dccrcasing pcrlbrinnncc of IDICM is due to tlic lcss significanl of known iiiforniation nhout tlic original dalahasr as incrcincnt hccorucs largcr Since DYC is not an iticlcmcntsl inining algorihm it can maintain a rdativcly stcidy advanlagc ovcr Ihc Apriori algorithm Prntn the rc stilts wc can concludc that the advantage of our IDIC-M al gorithm over thc FUP algoriihtn is no1 only corilrihuted by the iisc of tlyiiarnic couiitiiig tcchniquc but also ihe wliolc design of our IDIC-M algorithni 33 A variant algorithm the 1I algorithm We now iiilroducc a variatit algorittim to tlic IDIC-M al gorilhtn callcd llic IDIC_l algorilhm Whilc thci-c niay bc inorc ilian one passcs thrr~iigb Dl3 in Llic IDIC-M algorithm thc IDlCl algorithm giiarniitccs a singlc pnss ovcr the DLI Thc first three slcps of lhc IDIC-I dgorithtn arc cxactly thc sniiie 21s lhc IDIC-M algorittim tie Stcp 4 of thc TDIC_I algoritl~in works as follows In this stcp we clemmine which itcmsets in Lrlb-JJ~jU 89 


Lrl are largc in DR U dtr Instcad of applying a dynamic counting techniquc as iii the IDIC-M algorilhni wc Colin1 all thcsc cmditlate itcrnscls in onc pass over DB After the counting Is finished new large itemscts are idcntiiicd and addctl to LUD with the updakd supports Notc that all the lerntnns in thc previous subsection nlso npplics in thc TDIC-I algorithm which mcitiis that all potcn tial Lun candidates are exrrtnincd by the IDIC-I nlgorilhm 3.6 Experimciital results for algorithm InIC-l Scverat scts of expcrinrcnts were conductcd to coitiparc tlic TDIC-I algorithm with thc FUP algorithm and the pure DIC algorithm The data uscd in this suhscclion for tcst ing the IDIC-I algorithm is sirniler to that uscrl to tcsI the IDICM algorirhm in Section 3.4 3.6.1 IDlC-1 versus FUP for different supports The IDIC-1 algorithm and the FUP algorithm arc tcstctl for dirkretit 8 rangcd from 0.01 to 0.05 with uptfntcd tlalahase 1'10.I4.DIOO.dI which is similar to ibc nnc uscd in 141 lhc intcrval sixcs hr counting rlb ml IIU wcrc lixcd ni 1000 2nd 10000 rcspcciivcly Figure 5 IDIC-1 and FUP for different sup ports Thc rcsult is shown in Figttrc 5 The IDIC-I algorithiri clearly has a bcttcr execution tirnc specitup ratiu for all sup ports testcd Note that our algorithm hns a stcady advan tage ovcr Lhc FUP algorithm at dirfcrctit rniiiiinuin suppnrt tliIcsholds l'lic differcnuc hctwecn tlic speedup rdo of ttic IDIC-I algorilhni ancl that of the FUP algorithln is itiorc tlian 3 for all thc supports lcstcd Connp:ircd to thc 1DIC-M algorithtn wc can say that tile IDIC-1 algorithm is ruiich inore cnicicnt in dcaling with thc largc niirnber o canili date itcmscts gencratcd from dh when thc support threshold is low It ntso has R good pcrfnrinancc whcn tlie support ibrestnold is high 3.6.2 IDIC-1 versus FUP for diffcrent incrcmeiitaf sizes Wc tiavc dnnc two sets 01 experimcnls to comparc the pcr foi4mancc of Ihc 1131CJ algorithm with thc FUP nlgoritlirn l'or different incrcnietital six d ranging froti 1000 to 50000 with UB fixed RI 100000 i.c TIO.I4.D100.dn 8 is iixcd at 0.015 For the first set of cxpcrinicnts he DD and db wc gcnernted as ust[~I using tha satlie riundom seed While for thc secund SCI alcxpcritncnts the UB and rllr arc gcncrated scparnlely using dilfcrent random sccds I ApriotitlDIC-1 1 Apriori/FUP x Incremental S1m thousands Figure 6 IDIC..I and FUP for different incre mental sizes with the same random seed Thc rcstilts for lirst sct ofcxpcritnctits arc shown in FIg urc 6 The II>IC_I algorithin clcarly outpcrt'ortiis thc FUP algorittiin Tor all incrcnicntal siics tcstctt cspccially whcti thc incrcn~ciital six is sitinli Thc grcatesi advantagc or our algorithm ovcr thc PUP algorithm was ol>tainctl whcn d  1000 whcrc llie spccdup ratio of IDIC-I is rn\(~rc than twice Ihc sl~ccrlup ratiu of FUP Wc also scc the trend tha as Ihc iticremcntal sizc contiiiucs to incrcasc tlic pcrrortnnnccs of htrth the IDK-l algoriLhin and the 1:UP afgorillm IC i rcasc  The rosults for sccotid sct of cxpcritiicnis arc shown in Figure 7 Tlic 1DIC-I algorithtn also oulperi'ormt ttic FUP algoritlitn for all incrcmcntiil sizcs tcslcrl Notc that tlic curve ol thc IDICLI nlgiiIitIiin again cutivergcs to A higher valuc than th curvc olFUP does 3.6.3 IDIC-1 VC~SIIS DIC for tliffcrcnt incrctncntal sixcs Expcrimctirs woru done to colnparc the pcrforinnnca of the IL31C-I atgorithm with tlie DIC i~lgoritlun lilt tlii'ibrctit in crcincntal size rl ranging lioin 10000 to 50000 with l?Ll fixed nt 50000 i.c TIO.I4.D50.dti 0 is lixcd at 0.015 The rcsults Car ttic cxperiincrits arc shown in Figure 8 SItdar lo thc resulls lor the IDICM algorithm ttic advan 90 


Figure 7 IDIC-f and FUP for different incre mental sizes with different random seeds 5 4 0 P 23 4 n 10 dz 1 0 I I I I 10 20 30 40 50 Incremental Sire thousands Figure 8 IDtC-t and DIC for different incre mental sizes tage of thc IUIC-I algorithm is decrcasing with the incre mental sizc Our IDIC-I algorithm shows significant ad vantage uvcr the DIC algorithm Wc ciln havc ttic conclu sion similar to the IDIC-M algorithm that dynamic ctiunl iiig tcchnique is not the only coiisc of the efficiency or niir IDIC-I algorithm 4 Conclusion In this paper we propose iiew incrcmcntd updating al gmilhms the IDICM and the IDIC-I 10 handle the prob lem of incrcmcnlnl association rule mining using dynaiiiic counting tcchnique We describe in dclaits our new alp rithins sild illustralcs how dynamic coutiling works effi ciently in this problcm We have fiilly imptctnctitcd our algorithms Gxpcrimcntal results show that oiir ncw dgo rithrns have supcrior pcdormancc in cornparison with an other remit incrcineiilal irprlating algarittini thc PUP nlgo rithin Olhcr than binary or cntcgorical assuciatim rules the incremcntid mining of qm~tttitative iss\(Jciiition rulcs is also wrxth stirdying As new incoiming data miiy afkcl thc data distribution pattern of thc rwiginnl database new rliscrctiza tion may havc to bc done 011 the incrcmcntcd database Hence Ihc updating of tlie discovcred quantitativc nssoci iltioti rules will bc morc complicnted Aiiuther possiblc rc search lopic Is thc incrcmental mining of association rulcs in a distrihutcd cnvironment References I R Agrnwnl antl R Srikanr Fast dgorithrns for mining ns s oci at inn ni I c s 1 n P mceediri~s of hi turriu tion d Col ifcisnce oti Very Luge Diltubascs pages 487-499 Santiago Chi IC 1994 Z S Rrin R Motwani J L1 Ullm;in and S Tsur Dynamic iteinsct coitnting niid jmplicnlirm riiles rw market hnskct ilwln In Procctrdings oJACM SIGMVR Interri&wal Con ference on Morru~uvimr ofDutid pxgcs 255-264 AX 1997 3 M S Chcn antl J Hat Data mining An uvcrvicw I'rorn a database perspe cti vc EEK Trmi sn cl imis wi Ktro whip trnd kiln Eqiiic&ig S\(6 86 G8 X 3 1 9 96 4 D W Chcung J Hnn V T Ng and C Y Wong Mainte nancc of tliscovcrcrl asrociatioti nilcs in lnrgc dnt;ib;ucs An incrccmcn~al iiptliiling tcchniquc In P/-ocrcriiiigs of hrrer ridoiral Corfcrcriw un htd Biginccri/ig pagcs IO&I 14 1996 Cl D W Cheung V T Ng A W Fu ilnd Y Vu Efficicnl inin itig of associalion rider in t1istrit dotabascs KlXTrotis aclionsuii Kriowlcii~c atid Urrtlr Lnngiiieering 8\(6 1996 h J Hnn antl Y Vu Discovery 01 niiiltiplc-level assnciaiion rules froin lnrgc dalnbases In Pmceerlings oj the 21st Itt rertiutional Corlferctrce iw VCI hrge Dotribilscs Zurich Swiscrland 1995 7 C M Kuok A hi and M 11 Wong Mining fwsy nssuci ation riilcs in dnt;ibascs ACM SICMOD Rccurd 27 1 46 1998 E T Shintani and M Kitsiircgawn Pnrtillcl mining dgorithnis for gcneralized assucialion rilles with classification 1iicr.w thy 111 Procccilirrjp of ACM SIGMOU ln!enruticmil Con Jereiicu 011 Mmqyrecrif r$L 1998 9 K Sriktin and R Agrawal Mining gcncridizctl tissociaiion nrles 111 Prot:ccding.v of the 2/31 Oiiet-tr~rrionul Cotferciiue 011 Vcv JA~E h~rrlnbcrses Zurich Swizcrlanrl 1995 IO R Srikmt and li AgIaw;iI Mining quantitative nsmciatiun i-ules in largc rclalionill tables In Proceerlirigs oJACM SIG MO11 Itirerrrafiotiul Confermm uti M~nn~~ir~riit of Ilntu 1996 Ill S Thomas S IWagala K Alsibli an8 S Ranka An efficicnt algorithni Cor tlie incrcmc~i~al uprhlioii of assnci ntion rules in largc rbtabnscs In Procccrhgs id firiernn tionill Confcrwce oti K3lOlVkdgc Discinwry orrd Dcrta Miri ing piigcs 263-266 1997 I21 K Wing S 1.1 W ray and R Liii Inlerestingtiess-bascd intcrvnl mcrgcr for numcric nssucintion ritlcs Iti Prweed itip of hl~rtiU~i\(1iiQ Conjereiicc 011 K~iowlcrl~e Iliscovery aiidDuto Minirag pogcs 121--127 1'198 91 


                 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


