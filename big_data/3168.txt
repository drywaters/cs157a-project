this paper we describe an approach for au\255 tomatically finding the prototypic navigation behavior of web users User access logs are examined in order to extract the most significant user navigation access pattern Such approach gives us an efficient way to better understand the way users are acting and leads us to improve the structure of web sites for improving navigation Another interesting application deals with recommender systems 
Abstract-In 
441 Aid of navigation through recommendation Ali Mroue Laboratory of sciences of infonnations and of systems LSIS UMR 6168 University of Paul Cezanne Aix-Marseille III Av escadrille de Nonnandie Niemen 13397 Marseille Cedex 20 ali.mroue@lsis.org 
INTRODUCTION The World Wide Web provides a rich environment for users to retrieve information Day after day Internet is growing up at 
this vast source of information even with the 
where the system can suggest to users links that could interest them A main issue of navigation pattern discovering deals with the way to decide when two traces are identical or not even when they present some little difference In this paper we describe the overall characteristics of our similarity function and we 
in 
an 
incredible speed Information becomes so huge and so difficult to be found that users can easily be lost 
will I 
introduce some of our results 
assistance of search engine Websites are also growing up more and more and become by themselves a large source of information that is in most cases inaccessible for all users and must be organized Web access logs 
a lot of information that allows one to understand users needs to study and to observe the effectiveness of the structure of the website etc Many approaches have been developed in the web usage mining domain concerning these problems some of them considers that a sequence of pages is only a set of pages and don't refer to the order of pages and others don't take the order of pages in consideration Faced 
their search for infonnation Some of these works are focused 
creating recommender systems 1 that would help people make decisions in a complex 
in 
to this situation we investigated this domain space 
in in 
order to propose an approach that takes into account the problems of other approaches and gives a good solution for users II WEB USAGE MINING Web usage mining also known as web log mining aims to discover interesting and frequent user access patterns from web browsing data stored in the log files of web/proxy servers or browsers Many works of web usage mining have been developed to help users 
where the volume of information available for them is huge A recommender system relies on a prediction model to find out the interests of users and then make recom\255 mendations In general the recommender systems behavior consist of predicting user's next actions based on their previous one Another application of web usage mining is caching and prefetching of web pages Most users perceived that latency is a major problem 
today's World Wide Web Many factors contribute to this problem including transmission latency DNS name server lookups TCP connection establishment and start of session delays at HTTP servers 2 Conventional web caching techniques 3 attempt to address part of this 
in 
problem by temporarily storing recently accessed web content close to the user on the client device or on a proxy server These techniques work well when the content is reused several times potentially by several users However caching may not reduce latency when there is poor locality of reference and  access to dynamic and personalized content A complementary  approach to reducing latency is to effectively predict user access behavior and to use this knowledge to prefetch content close to the user In general prefetching applications consist of predicting the future access of a web user and then prefetch its content in order to resolve latency problems 4 Web usage mining has 
all these applications the goal is the development of 
these huge sources of information The major classes of web usage mining applications are based on the discovery of users navigation patterns The general technique used in order to find navigation patterns are statistical analysis association rules clustering classifi\255 cation and sequential pattern mining In many works these methodologies 
many applications such as improving search engines 5 and for allowing customized browsing in websites 6 7 etc  
effective prediction algorithm The advantage of the prediction algorithm is that it will help web users find what they look for and not to be lost 
In in 
an 
rules Association rules generation 8 can be used to relate pages that are often referenced together 
are combined together in order to find users navigation patterns 225 Statistical Analysis Statistical approaches are the most common methods for extracting knowledge about visitors to a Web site 
in 
analyzing the session file one can perform different kinds of descriptive statistical analy\255 sis frequency mean median etc   Typical extracted statistics include the most frequently accessed pages average page viewing time and average navigation path length 225 
a single server session In the context of Web Usage Mining association rules refer to sets of pages that are 
By 


which the agent reinforces a given link using words encountered in pages downstream of it A third approach to learning uses these two approaches together which give best results User must communicate with Web Watcher in order to tell if user goal is reached or not etc  Letizia 14 is another agent-based approach it is a client side browsing assistant that accompanies the user while browsing It is located on user's single machine No goal is predefined The constitution of groups which are used as support together with the definition of a prototypic course is based targeting advertising aimed at groups of users based on these patterns The various existing approaches in the field of Web Usage Mining have many advantages and disadvantages and in the majority of the cases they are very dependent on application type As an example PageGather 12 is an algorithm which processes the access log file in an in in in in the Web Usage Mining domain there are three kinds of interesting clusters to be discovered User clusters session clusters and page clusters Clus\255 tering users tends to establish groups of user exhibiting similar browsing patterns Clustering sessions tends to establish groups of user sessions in which the users have similar access patterns Clustering pages tends to establish a group of pages according to their content 225 Classification is the task of mapping data items into one of several predefined classes 9 In the web domain Classification is used to develop user profiles that belong to a predefined class or group which is characterized by a set of features or properties 225 Sequential pattern The problem of discovering sequential patterns 10 11 deal with finding inter-transaction pat\255 terns such that the presence of a set of items is followed by another item in the time-stamp ordered transaction set The discovery of sequential patterns basic steps 1 Process the access log into visits 2 Compute the co-occurrence frequencies between pages and create a similarity matrix 3 Create the graph corresponding to the matrix and find maximal cliques or connected components in the graph 4 Rank the clusters found and choose which to output 5 F or each cluster create a web page consisting of links to the documents in the cluster and present it to the webmaster for evaluation Based on the PageGather algorithm index pages are created for easier navigation However PageGather assumes that a visitor can easily navigate a lengthy list of shortcuts and thus provide perhaps dozens of suggested links A draw back for PageGather is that it relies on the human web masters to determine the appropriateness of the generated index pages advance by users Letizia uses the past behaviour of the user to anticipate a rough approximation of its interests It looks for more documents that are related to the users interest or might be relevant to future request when user is not browsing or reading documents By referring to the advantages and disadvantages of the different methods and approaches we conceived our approach based on a similarity function cluster is a collection of objects which are similar between them and are dissimilar to the objects belonging to other clusters Web server access logs allows Web-based organizations to predict user visit patterns and help a final check This will be so difficult to realize accessed together with a support value exceeding some specified threshold These pages may not be directly connected to one another via hyperlinks 225 Clustering is the process of organizing objects into groups whose members are similar some way this paper we present our novel approach for building a similarity function that finds a set of interesting navigation patterns called prototype Those prototypes could be used to build order to differentiate these erros from web surfing sessions which are considerable different A Similarity Function Definition statistical methods in order to find collections of related pages at a web site Then it applies cluster mining to the data and produces candidate index-pages as output Page Gather consists of performing in In In 442 A 5 optimized cartography for the website to find the most popular navigation pattern to predict future page request by users etc  Our similarity function uses a sequential finding algorithm and an n-gram prediction model to construct a list of popular sequence prototypes III EXTRACTING NAVIGATION PATTERNS By referring to many approaches and by studying the needs of different web usage mining applications we drew certain constraints which have guided us in our navigation pattern searching approach Extracting navigation patterns implies a phase of regrouping of user's sessions according to their proximity This regrouping implies the search for a given sequence the more similar sequences to this one One of the characteristics of our work is to assimilate sequences which are not absolutely identical What could be regarded as negligible errors in the course of a web surfing sessions is not taken into account in in the case of sites that have so many pages to be indexed Another appoach named WebWatcher 13 which is a virtual agent that helps user to reach a particular target page or goal it offers suggestions about where to go next The Web Watcher acts as a guide offering suggestions based on its knowledge of each user's interests It learns to suggest appropriate links for users by leaming from previous tours in which user-selected links are identified by adding the keywords chosen by the user at the beginning of tour A second learning strategy involves reinforcement 


order to find relation between pages users and etc  In our case this choice can be explained by our aim for  rebuilding the cognitive process of users while they are searching for information  However in this perspective we consider that each step of information retrieval is important because it could modify knowledge of the user either his i;:j","34 e>.:<.ld   1.87 pages ordered as in seq 1 appear in seq2 in order to consider sequence in lhe containing he groups where Fig 2 Output sample of the similarity function the filtered sequences coming from the access log files of the site The procedure of extraction of the patterns produces as output groups where each one is related to a prototypic representative sequence in which all the members of the group were considered to be similar to it Figure 2  re3uka1 considers the order of appearance of the pages in the session For example the sequence PI P2 P3 P4 will be considered as different from the sequence PI P3 P2 P4 even if the same pages were visited in these two sequences 225 Error tolerance It's the capacity to neglect pages which can be regarded as visited by mistake sc:qucncf i,r--="57  267.;!u!:':::"'12910 12 14 im",\267'S7.23   sequellte 267"230 0<"""1 10 12 1412 r"m"90.47  Sirrllar  scqucnce groupe J lY-;og  a very traditional way using server access log files of a given Web site This similarity function takes into account a certain number of factors in order to produce a value of resemblance Some of these important factors are 225 Order of the pages in the sequence We consider that it is an essential factor for revealing two similar navigation behaviors  The searching phase of the similarity func\255 tion consists of finding and calculating the similarity between all the sequences received at the entry This stage of research uses the following parameters Coverage Error ErrorRate seqMargin 225 Coverage Indicates the minimal number of common elements between the two sequences It is a percentual value which is by default equal to 75 Example Given seql  PI P2 P3 P4 seq2  PI P2 P3 and coverage As same terminal has maximum SeQuences similarity 10 12 14 llu.ni::;:"4 tt example is it necessary to consider that both sequences seq I PI P2 P6 P3 P4 and seq2 Do PI P2 P3 P4 correspond or not to different sequences The answer that we propose is a gradual answer where such a difference will be or not neglected according to a parameter of the function and some other characteristics like the number of pages relatively different from the length of sequences 225 Comparison sequences of different length Even if the sequences present different lengths it can be useful to determine that certain sequences are in fact only sub sequences of others which are larger but whose is iden\255 tical With this intention we used an n-gam prediction model which after having noted the similarity on the first elements will check if the sequences could be identical 1 Searching B Implementation of the function request For this reason each step visited page is important and is represented in our model of information retrieval Another thing can also be considered as novelty in our approach it is the capability of considering some pages as visited by error And this can be done without referring to the time of viewing pages and without having any information about the content of pages etc  1214 151r Groups Bod prototypes Similar sequences having differenllenglh V:<:.:rlfkJll!'in and containing different terminal Conlirrnlng he seauences I simlarity N-Gram Prod_n Model It It Fig 1 Similarity Function Sturcture 443 in in Tl'K'l\\.="S9.32 C<..Jrf="6.2T of sessiOtl S8ssion file every sequences grcupe  1012 1415 dOl"'''90.47   90.47  l!hE""2 90 It is necessary that 3.60 ld="W t'l{.=r::"l 0 12 14 J 5 equ   id=-14 r.eq!J.ence jJ:::"34 I;jt;r  l 10 12 14 15  267"iue:::"2 10 12 14 IS in1","90.47   te  Keep set t consists of a means for determining if it is very probable or not that the shortest sequence if it were to be prolonged its in conformity with the longest sequence For example given seql PI P2 P3 and seq2 PI P2 P3 P4 we will consider that the sequence PI P2 P3 is similar to the sequence PI P2 P3 P4 if according to the other sessions the page P4 is very likely to be found after a sequence PI P2 P3.This model of prediction is based on the n_gram model see 15 The set of these factors mentioned above present the novelty of our approach.For example one of our method's advantages is that it conserves the integrity of the used analysed se\255 quences Prototypes that represent clusters must be necessarily exis\255 tent sequences navigation sessions Statistical approaches or other data mining approaches don't take into account this constraint because it doesn't present an interest for them They search for any type of sequences sub sequences elements of sequences etc   One can divide the procedure carried out by the function of similarity into three phases Searching verification and filtering Figure 1 The similarity function receives as input on a similarity function which provides a rate of similarity for a pair of given sessions The extraction of the sessions and then the courses are done 


PI which means  3  15 22  1    this case we will reject the similarity 14 ou S Fig 3  Output Group number IV RESULTS As we already explained the results of the similarity func\255 tion are prototypes which represent the most frequent courses This list of prototypic courses will be regarded as a mini log file where we can apply any statistical method to obtain for example the most visited pages in the site etc  According to our test on the log files of the site of our laboratory LSIS and on the website of the MIAGE speciality we obtained meanful significant prototypic courses For example in the case of the site of the LSIS laboratory the most frequent course is P3 effect 2 errors for 3 common pages provide an error rate higher than 15 225 SeqMargin This parameter enables us to take account of the difference in length between the sequences to be compared Le The number of elements which appear and which are neither errors nor elements of the sequence This parameter is an integer value which is by default equal to 1 and will be multiplied by the maximum allowed number of errors in relation to the error rate For example ifSeqMargin=1 and ErroFI let us compare the two sequences seql PI P2 P3 and seq2 PIO PI P6 P2 P3 PI2 P13 The sub-sequence PI P2 P3 3 elements is included in seq2 with the page P6 as error The 3 other remaining elements PIO PI2 and P13 are neither the pages visited by error nor the pages of the sequence in common with seq I We calculate the number of elements of the sequence in the margin That is the elements different than the elements in common with seq I and different from the error elements Nb of Element Al\255 lowed in the margin=A  B A SeqMargin B=Maximum number of error allowed  C  D 00 With C  Number of common element found D  ErrorRate let's suppose it's equal to 25 This gives Nb of Element Allowed in the margin  I  B B  3  25  100  0.6 Equipes 444 The results obtained by this procedure are prototypes which represent the most significant courses both in quantity as in quality of the file log Currently we are examining the possibility of developing a recommender system where the prototypes will constitute our behavior database We can use any prediction method based on probabilities and/or on a prediction model for example n-gram for exploiting this database in order to predict the future choices of the uSer and assist him Figure 4 94 roll S Pl5 2 Verification 3 Filtering The verification phase of the similarity function consists of verifying the results obtained from the first phase Searching using the n-gram prediction model Fiche This is consistent within the context ofa laboratory site  since normally the web surfers use this site to search for informations about the members of research teams of the laboratory publications etc Returning to the groups obtained we noted that the number of groups changes between one log and another but preserving certain common prototype groups Figure 3 This number tends to become constant when the number of sessions increases them to be similar And in this case we consider that seq2 is not similar to seq 1 because there exist less than 4 elements of seql in seq2 only 3 elements PI P2 P3 225 Error This parameter indicates the maximum number of pages visited by mistake that appear between two suc\255 cessive pages of the sequence By default this parameter is equal to 1 Example If ErroF I and we compare seq I  PI P2 P3 with seq2 PI P7P2 P3 or seq3 PI P7 P2 P8 P3 we will obtain a positive result in both cases On the other hand a sequence such as seq4 PI P7 P9 P2 P3 will give a negative result 225 ErrorRate This parameter is a rate used to take into account the increase of error with the length of the sequence It is a percentual value that equals by default to 15 This parameter is in direct is related to the number of common elements between the 2 sequences As example given the two sequences Seql PI P2 P3 and Seq2 PI PIO P2 P6 P3 3  15 22  1  In 1 So the number of elements allowed in the margin  I 1  I However there are 3 elements p1O P12 and P13 and thus we consider that seq2 is not similar to seqI Fig 4 Recommender System The filtering phase is the last phase of the similarity function it consists of a filtering of the results after the two first phases have ended searching and verification We tested the effectiveness and stability of our parameters on different log files By varying the values of different parameters we found that The number of result groups decrease with the increase of the value of the coverage Figure 5 which verifies the correct functionality of this parameter as\267 the increase in value of this parameter augments the discriminative nature of the      membres In P22 


The results obtained from our model are represented as a list of sequences Every sequence represents the most interesting user navigation behavior These sequences will be the core of the simulator and will be used in order to predict the navigation of new users The simulator Figure 6 takes as input either one or a sequence of pages and produce as output a list of pages that will be represented as a navigation behavior The sequence 1 26 8 has a presence The probabilistic model is generated from a given list of sequences this model will be used to obtain what page will appear after these pages We can represent this model as a two dimension array Figure 7 Fig 7 Probabilistic model This two dimension array indicate that there is for example a 20 of possibility that users tend to visit P2 after PI 10 tend to visit P3 after PI 40 tend to visit P5 etc  According to these values we dynamically assign a range of numbers for each page This range of numbers will be chosen in relation to their possibility values Finally a random number will extracts the most frequent user behaviour and at the same time it groups user sessions in clusters 1 Functionality 1 3 6\2607 The sequence 1 5 7 5 7 has a presence 1 3 7 The sequence 1 5 7 21 24 has a presence generated and which according to its value the correspondent page will be chosen and sent to the output of the simulator Then the same process will be done but this time for other sequences and another type of page VI RELATED WORK Understanding the behaviour of computer users is an active research area technique allows us to find out how the users are acting under which conditions and what they asking for Many methods have been developed in this research area mainly for extracting the behaviour of web users As a result of our comparisons between our approach and the rest we have found out that Our approach has the following advantages etc  As mentioned before the simulator will take as input a page or a sequence of pages so for example if the simulator receives as input the page pI then all the sequences that start with pI are analyzed and used in order to generate a probabilistic model This probabilistic model will be executed and will give as result the next page that the user will visit after pI and so on can show and help us detect errors in the users activity referring neither to the term of time navigation time etc   nor to the content of the navigation information gives us the possibility to understand what the users are actually doing in terms of errors Fig 6 Simulator Interface 445 Fig 5 Variation Of Coverage Parameter comparison between two sequences Similarly other parameters were checked and tested in the same way V ApPLICATION I   I     225 It 225 It 225 It be This are 2 Pobabilistic Model In In It order to study the navigation behavior of users we built a simulator that reproduces the same navi\255 gation behavior of the users according to the results obtained from the above mentioned approach The simulator doesn't take into account the time of navigation it is only interested about the order of appearance of the pages The aim of the simulation is to study how users are acting in order to provide them better navigation environment Also the simulator can be used or extended in order to provide the best path for attaining a destination page A User navigation simulation can be used to study the behaviour of users running any computer applications apart from web navigation conclusion the core of the simulator is a list of prototypes obtained from the similarity function Every prototype is characterized by a number its presence for example The sequence 1 5 7 21 has a presence 


Uthurusamy Eds Montreal Canada Volume which items or pages where accessed altogether In 16 a method to classify web site users is proposed Each user session is stored U M Fayyad and in in in in finding May 1996 M USENIX Symposium on Internet Technologies and Systems Amsterdam The Netherlands The Netherlands North\255 Holland Publishing Co 2000 pp 377-386 Online Available http://portal.acm.org!citation.cfm?id=346322 5 R Baeza-Yates Web mining 1999 Online Available citeseer.istpsu.edulpitkow99mining.html 7 Proc 5th Int Con Extending Database Technology EDBT 1 1997 pp 770-777 Online Available citeseer.ist.psu.eduljoachims96webwatcher.html 14 H Lieberman Letizia An agent that assists web browsing in Proceedings of the 2004 IEEE Conference on Cybernetics and Intelligent Systems Proceedings of the First International Conference on Knowledge Discovery and Data Mining KDD-95 order to extract the pattern of navigation by clarifying the criteria of clustering and session selection One of the characteristics of our approach is that it offers a certain tolerance to errors in navigation Indeed we make the assumption that a web surfer can look for his way on the site and consequently borrow alternatives which do not call into question its total course So as not to exclude this course from a group tendency it is necessary that the similarity func\255 tion is able to assimilate courses which seem different The obtained results are satisfactory Tested on several sites and starting from various log files they reveal coherent practices of navigation compared to the contents of the site and to statistics The current prospects relate to the development of recommendation tools the production of models of operators of the type Virtual Net surfer for testing of sites and the retro-design of these same sites REFERENCES 1 Z Baoyao C H Sill and C Kuiyu An intelligent recommender system using sequential web access patterns Baoyao C 26 Dunedin New Zealand 2004 pp 3-4 Online Available http://portal.acm.org!citation.cfm?id=979923 6 J E Pitkow and P Peter Mining longest repeating subsequences to predict world wide web surfing in to Srikant and A Rakesh Mining sequential patterns Generalizations and perfonnance improvements in Fayyad P.-S Gregory and S Padhraic From data mining to knowledge discovery Press 1995 Online Available citeseer.ist.psu.edulmannila95discovering.html 11 into account the order of the pages in each sequence There are also other clustering algorithms based on neural network algorithms but the disadvantage they present is that it is not possible to verify the results or to understand clearly the results VII CONCLUSIONS Although there exist many tools for static analysis of Web site visits the most visited page average time of visit etc there is however less tools which are interested in kinemat\255 ics of navigation which are useful for helping web users Moreover existing approaches which are primarily statistical or neuronal are very often dedicated to a given application and do not make it possible to explain the results obtained and thus to include/understand the noted behaviors We proposed a similarity function Sequences in Perkowitz and E dynamic hypertext linking in M databases Siu and C in in AAAI 0 order to cluster them The clusters obtained with this method do not G G Peter M G Apers Mokrane Bouzeghoub Ed vol 1057 Springer-Verlag 25-29 1996 pp 3-17  Online Available citeseer.istpsu.edularticlelsrikant96mining.html 12 vol 11 no 1 pp 95-107 1999  Online  Available citeseer.ist psu.edu/aggarwaI99caching.html 4 Sarukkai and R R Link prediction and path analysis using markov chains in search engines 2005 Online Available http://www.win.tue.nVpersweb/Camera\255 ready/9-Zhou-full.pdf 8 L Tianyi Web-document prediction and presending using association rule sequential classifiers A Thesis submitted in partial fulfilment of the requirements for the degree of Master of Science Simon Fraser University 2001 Online Available citeseer.ist.psu.edulIiO 1 webdocumenthtml 9 K Sch of Comput Eng Nanyang Technol Univ Singapore 2004 pp 1-3 2 E Cohen and Statistical Methods for Speech Recognition Proceedings of the 9th international World Wide Web conference on Computer networks the international journal of computer and telecommunications netowrking H The Fifth International World W"uJe Web Conference WWW5 Towards adaptive web sites Conceptual framework and case study 2000 Online Available citeseer.comp.nus.edu.sg/326006.htmI 13]\267 J Thorsten F Dayne and M Tom M Web watcher A tour guide for the world wide web in Jacobsen H Garcia-Molina and U Dayal From user access patterns INFOCOM Z Proceedings of the 27th Australasian coriference on Computer science Proceedings of the Workshop on Personalization on the Semantic Web PerSWeb 05 446 As a possible disadvantage our approach presents the following 225 The results can only be real complete sequences It cannot detect patterns if they do not appear at least once together as a sequence without errors of other elements Thus it is not the most appropriate method for finding out individual page frequencies Our method can be considered as a sequential pattern algorithm and as a clustering algorithm One of the famous sequential pattern algorithms is AprioriAll This algorithm is highly effective and gives good results because it eventually extracts all the frequent sequences There may exist the possibility that it returns a sequence that does not really exist 2 2000 pp 854-863 Online Available citeseer.istpsu.edularticlelcohenOOprefetching.html 3 C C Aggarwal W Joel L and F Alvis Mining longest repeating subsequences to predict world wide web surfing vol 17 pp 37-54 1996 Online Available citeseer.ist.psu.edu/fayyad96from.html 10 H Mannila T H and in in in Philip S Caching on the world wide web Haim Prefetching the means for document transfer A new approach for reducing web latency MIT Press 1998 16 T W Yan Knowledge and Data Engineering IJCAl a complete sequence This will not be useful for studying the users behaviour in a computer application Nevertheless the result can be useful for a vector that contains the number of visits for each page and an algorithm is used to find similar vectors  C S Mellish Ed Montreal Quebec Canada Morgan Kaufmann publishers Inc San Mateo CA USA 1995 pp 924-929 Online Available citeseer.istpsu.edu/Iieberman9Sletizia.html 15 F Jelinek Proceedings of the Fourteenth International Joint Conference on ArtifiCial Intelligence lJCAl-95 Ai Magazine as take Y V in U M R R A I Discovering Frequent Episodes M 


Used-for references in the LCSH into holonym/meronym relations in our WKB  In the experiments we assume that each topic comes from an individual user We attempt to evaluate our model in an environment that covers great range of topics However it is not realistic to expect a participant to hold such great range of topics in personal interests Thus for the 50 experimental topics we assume each one coming from an individual user and learn her his personalized ontology An LIR is collected through searching the subject catalogue of Queensland University of Technology QUT Library 3 by using the title of a topic Librarians have assigned title table of content summary and a list of subjects to each information item e.g a book stored in QUT library The assigned subjects are treated as the tags in Web documents that cite the knowledge in the WKB  In order to simplify the experiments we only use the librarian summarized information title table of content and summary to represent an instance in an LIR  All these information can be downloaded from QUT's Web site and are available to the public Once the WKB and an LIR are ready an ontology is learned as described in Section 3.3.1 and personalized as in Section 3.3.2 The user con\002dence rates on the subjects are speci\002ed as in Section 3.3.3 A document d i in the training set is then generated by an instance i  and its support value is determined by support  d i   X s 2 021  i  s 2S sup  s Q  14 where s 2 S in O  Q  are as de\002ned in De\002nition 5 As sup  s Q   0 for s 2 S 000 according to Eq 11 the documents with support  d   0 go to D 000  whereas those with support  d   0 go to D   4.4 Performance Measures The performance of the experimental models are measured by three methods the precision averages at eleven standard recall levels 11SPR the mean average precision MAP and the F 1 Measure They are all based on precision and recall the modern IR evaluation methods The 11SPR is reported suitable for information gathering and is used in TREC evaluations as a performance measuring standard An 11SPR v alue is computed by summing the interpolated precisions at the speci\002ed recall cutoff and then dividing by the number of topics P N i 1 precision 025 N  025  f 0  0  0  1  0  2      1  0 g  15 N is the number of topics and 025 are the cutoff points where the precisions are interpolated At each 025 point an aver3 http://library.qut.edu.au Figure 2 Experimental 11SPR Results age precision value over N topics is calculated These average precisions then link to a curve describing the recallprecision performance The MAP is a stable and discriminating choice in information gathering evaluations and is recommended for measuring general-purpose information gathering methods The average precision for each topic is the mean of the precision obtained after each relevant document is retrieved The MAP for the 50 experimental topics is then the mean of the average precision scores of each of the individual topics in the experiments The MAP re\003ects the performance in a non-interpolated recall-precision fashion F 1 Measure is also well accepted by the information gathering community which is calculated by F 1  2 002 precision 002 recall precision  recall  16 Precision and recall are evenly weighted in F 1 Measure For each topic the macro F 1 Measure averages the precision and recall and then calculates F 1 Measure whereas the micro F 1 Measure calculates the F 1 Measure for each returned result and then averages the F 1 Measure values The greater F 1 values indicate the better performance 5 Results and Discussions The experiments attempt to evaluate our proposed model by comparing to an implementation of mental model We expect that the ONTO model can achieve at least the close performance to the TREC model The experimental 11SPR results are illustrated in Fig 2 At recall point 0.3 the TREC model slightly outperformed the ONTO model but at 0.5 and 0.6 the ONTO model achieved better results than the TREC model subtly At all other points their 11SPR results are just the same For the MAP results shown on Table 1 the ONTO model achieved 0.284 which is just 0.006 below the TREC model 2 
512 
516 


TREC ONTO p-value Macro-FM 0.388 0.386 0.862 Micro-FM 0.356 0.355 0.896 MAP 0.290 0.284 0.484 Table 1 Other Experimental results downgrade For the average macroand microF 1 Measures also shown on Table 1 the TREC model only outperformed the ONTO model by 0.002 0.5 in macro F 1 and 0.001 0.2 in micro F 1  The two models achieved almost the same performance The evaluation result is promising The statistical test is also performed on the experimental results in order to analyze the evaluation's reliability As suggested by we use the Student's Paired T-Test for the signi\002cance test The null hypothesis in our T-Test is that no difference exists in two comparing models When two tests produce substantially low p-value usually  0.05 the null hypothesis can be rejected In contrast when two tests produce high p-value usually  0.1 there is not or just little practical difference between two models The T-Test results are also presented on Table 1 The pvalue s show that there is no evidence of signi\002cant difference between two experimental models as the produced pvalue s are quite high  p-value 0.484\(MAP 0.862\(macroFM and 0.896\(micro-FM far greater than 0.1 Thus we can conclude that in terms of statistics our proposed model has the same performance as the golden TREC model and the evaluation result is reliable The advantage of the TREC model is that the experimental topics and the training sets are generated by the same linguists manually They as users perfectly know their information needs and what they are looking for in the training sets Therefore it is reasonable that the TREC model performed better than the ONTO model as we cannot expect that a computational model could outperform a such perfect manual model However the knowledge contained in TREC model's training sets is well formed for human beings to understand but not for computers The contained knowledge is not mathematically formalized and speci\002ed The ONTO model on the other hand formally speci\002es the user background knowledge and the related semantic relations using the world knowledge base and local instance repositories The mathematic formalizations are ideal for computers to understand This leverages the performance of the ONTO model As a result as shown on Fig 2 and Table 1 the ONTO model achieved almost the same performance as that of the TREC model 6 Conclusions In this paper an ontology-based knowledge IR framework is proposed aiming to discover a user's background knowledge to improve IR performance The framework consists of a user's mental model a querying model a computer model and an ontology model A world knowledge base is used by the computer model to construct an ontology to simulate a user's mental model and the ontology is personalized by using the user's local instance repository The semantic relations of hypernym/hyponym holonym/meronym and synonym are speci\002ed in the ontology model The framework is successfully evaluated by comparing to a manual user model The ontology-based framework is a novel contribution to knowledge engineering and Web information retrieval References   C Buckley and E M Voorhees Evaluating evaluation measure stability In Proc of SIGIR 00  pages 33–40 2000   R M Colomb Information Spaces The Architecture of Cyberspace  Springer 2002   D Dou G Frishkoff J Rong R Frank A Malony and D Tucker Development of neuroelectromagnetic ontologies\(NEMO a framework for mining brainwave ontologies In Proc of KDD 07  pages 270–279 2007   S Gauch J Chaffee and A Pretschner Ontology-based personalized search and browsing Web Intelligence and Agent Systems  1\(3-4 2003   X Jiang and A.-H Tan Mining ontological knowledge from domain-speci\002c text documents In Proc of ICDM 05  pages 665–668 2005   J D King Y Li X Tao and R Nayak Mining World Knowledge for Analysis of Search Engine Content Web Intelligence and Agent Systems  5\(3 2007   D D Lewis Y Yang T G Rose and F Li RCV1 A new benchmark collection for text categorization research Journal of Machine Learning Research  5:361–397 2004   Y Li and N Zhong Mining Ontology for Automatically Acquiring Web User Information Needs IEEE Transactions on Knowledge and Data Engineering  18\(4 2006   H Liu and P Singh ConceptNet a practical commonsense reasoning toolkit BT Technology  22\(4 2004   A D Maedche Ontology Learning for the Semantic Web  Kluwer Academic Publisher 2002   S E Robertson and I Soboroff The TREC 2002 002ltering track report In Text REtrieval Conference  2002   M D Smucker J Allan and B Carterette A Comparison of Statistical Signi\002cance Tests for Information Retrieval Evaluation In Proc of CIKM'07  pages 623–632 2007   X Tao Y Li and R Nayak A knowledge retrieval model using ontology mining and user pro\002ling Integrated Computer-Aided Engineering  15\(4 2008   X Tao Y Li N Zhong and R Nayak Ontology mining for personalzied web information gathering In Proc of WI 07  pages 351–358 2007   T Tran P Cimiano S Rudolph and R Studer Ontologybased interpretation of keywords for semantic search In Proc of the 6th ICSW  pages 523–536 2007   Y Y Yao Y Zeng N Zhong and X Huang Knowedge retrieval KR In Proc of WI 07  pages 729–735 2007 
513 
517 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


