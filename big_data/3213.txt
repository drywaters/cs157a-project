An Efficient Data Structure for Mi ning Generalized Association Rules  Chieh-Ming Wu and Yin-Fu Huang Graduate School of Engineering Science and Technology National Yunlin University of Science and Technology 123 University Road, Section 3 Touliu, Yunlin, Taiwan 640, R.O.C Tel: \(+886\xt. 4314 Fax: \(+886\-5-5312063 Email huangyf@el.yuntech.edu.tw     Abstract  The goal of this paper is to use an efficient data structure to improve our earlier research. In the earlier research, we attempted to find the generalized association rules between the items at different levels in the taxonomy tree under the assumption that the original frequent itemsets and association rules were generated in advance In the paper, we proposed an efficient data structure called a frequent closed enumeration table \(FCET\ to store the relevant information using a well-known algorithm. It stores only maximal itemsets, and can be used to derive the information of the subset itemsets in a maximal itemset through a hash function. From experimental results, we found that the combinations of FCET and the hash function not only save spaces, but also speed up producing the generalized association rules  1. Introduction  In the previous research, we have used a VTV table to replace the whole original database. Although a VTV table demonstrated superior speed in operations, it required a large amount of storage spaces, even much larger than the original database, and therefore it is unsatisfactory in real applications. In the paper, we proposed an efficient data structure which substituted for the VTV table and applied to the GMFI and GMAR algorithms in the previous research. The efficient data structure called the frequent closed enumeration table FCET\ could reduce considerable storage spaces, and cooperate with a hash function to speed up execution time Given a taxonomy tree, we aimed to obtain generalized association rules from the original frequent itemsets or the original association rules. Beforehand, the preprocessing was done to convert the original frequent itemsets into an FCET data structure, because the transaction information of those frequent itemsets should be recorded in the FCET. In the taxonomy tree, the leaf nodes not appearing in the original frequent-1 itemsets are called infrequent itemsets. The infrequent itemsets must be identified to learn generalized information. Here, to construct the infrequent itemsets table, we used existing clustering technologies, and then used the same algorithm as constructing the FCET Finding generalized association rules in databases is the fundamental operation behind several common datamining tasks [12  G e ne r a l i z e d a sso c i a t i o n r u l e  fi nd i n g i s  a very important problem in the data mining field with numerous practical applications, including consumer market basket analysis, inferring patterns from web page access logs, and network intrusion detection. Currently data mining is arising to understand, analyze, and use these data. Also, data mining is designed for finding samples of interest in a large database, and thus it is also a central part of knowledge exploration   In data mining, the most well-known method is the Apriori algorithm i n c e th e A p riori alg o rit h m  is costly when finding candidate itemsets, there are many variants of Apriori that differ in how they check candidate itemsets against the database. For example, Park et al  enhanced the Apriori algorithm with a hashing scheme that can identify those candidates that will turn up infrequently when checked against the database. Zaki et  r es en ted th e al g o rithm s Ma x E clat a n d C H AR M  for identifying maximal frequent itemsets. These algorithms are similar to Max-Miner [11  w h ic h also  attempt to look ahead and identify long frequent itemsets early, in order to help prune the spaces of considered candidate itemsets. Zaki et al. also used the diffsets concepts to s a v e s t orag e s p ace w h ile ch ecking against in dense databases. In addition, some researches have made use of transaction reduction techniques including partitioning proposed by Savase  to  generate frequent itemsets quickly. Furthermore, a data structure called FP-Tree was proposed by Han et al. to produce frequent itemsets directly, but not candidate itemsets 
Fifth International Conference on Fuzzy Systems and Knowledge Discovery 978-0-7695-3305-6/08 $25.00 © 2008 IEEE DOI 10.1109/FSKD.2008.609 565 


Up to now, all proposed research can mine all association rules only from scratch. In this paper, our goal is to mine generalized association rules without rescanning the original database. Here, the original frequent itemsets generated in advance can be used to provide users with mine generalized association rules, instead of the original database. We hope to only store the maximal itemsets, rather than all the frequent itemsets; however only storing the maximal itemsets may lead to a loss of information, since all subset frequencies are not available again. Thus, here we propose an efficient data structure called frequent closed enumeration table \(FCET\ to store all information To be followed up, the problem of mining association rules is defined in Section 2. Then, we proposed an efficient data structure FCET and infrequent closed enumeration table \(IFCET\ in Section 3, where it not only saves a large amount of storage spaces, but also speeds up mining time. In Sections 4, we employed an FCET index tree to combine multiple FCETs. In Section 5, several experimental results show the superiority of FCET over other data structures. Finally, we make conclusions in Section 6  2. Problem Descriptions  An association rule is an implication of the form X 000  Y, where X 002 I, Y 002 I, and X 003 Y 004 If c% of the transactions in D support X and Y, we say that X 000 Y holds in the transaction database D with confidence c The definition of Support\(X\ the number of transactions purchasing X, divided by the number of transactions in the database. The definition of Conf\(X 000 Y\ the support of both antecedent and consequent divided by the support of antecedent in the rule Definitely, we can find association rules from scratch given a large transaction database D. However it is unavoidable to scan the original database once more and this is very inefficient. Thus, the efficient data structure proposed here makes use of the original frequent itemsets that have already been generated beforehand, to produce new association rules, rather than re-scanning the original database. In a very large database, rescanning the database to mine specified association rules is very inefficient and unnecessary. Some researches have employed the vertical-TID-vector table \(VTV\tore the database, which uses bit vectors to record the transaction information. Each item is represented by a bit vector where the length of the bit vector is the total number of transactions in the database. If an item appears in the jth transaction, the jth bit of the corresponding bit vector is set to 1; otherwise, the bit is set to 0. Because it uses bitwise operations, it is very efficient when calculating support counts. However, it is not realistic for large-size data sets. For example, a date set with 10,000 items and 1,000,000 transactions will require 10,000,000,000 bits This size is much larger than the original data set. Thus we propose an efficient data structure which only requires a small amount of storage space, instead of re-scanning the original database. In addition, we used a perfect hash function o h e l p u s qu i c k l y f i n d su pport cou n t s    3. Mining Algorithms  3.1 Processing Flow of Mining Algorithms  As shown in Fig. 1, the original VTV table used in our previous studies has been replaced with more efficient data structures; i.e., FCET and IFCET. The components shown inside the dotted box, such as original frequent itemsets, original association rules, FCET, and IFCET are generated beforehand. Rather than scanning the original database, we make use of the FCET and IFCET transformed from the original database and original frequent itemsets to mine generalized association rules    Fig. 1. Processing flow of mining algorithms  3.2 Frequent Closed Enumeration Table \(FCET  Here, we give a property used in the FCET as follows  Property 1: The frequent itemset is called maximal if it is not a subset of any frequent itemsets; any nonempty itemset is a frequent itemset if and only if it is a subset of the maximal itemsets  During frequent itemset generation, we can build a frequent closed enumeration table using generated frequent itemsets, in order to reduce required disk space and speed up the mining operation. According to Property 1, since all subsets of a maximal frequent itemset must also be frequent itemsets, we use an FCET to store all the frequent closed itemsets where an FCET is indexed by a maximal frequent itemset In an FCET, we not only use a bit \(or employ the technique called diffsets\ to reduce the memory footprint of intermediate computations, but also use a perfect hash 
566 


function to rapidly get the support count and the TID set of a frequent itemset. Since we can use the perfect hash function to easily calculate the position of each subset of a maximal itemset, no subsets need to be stored in an FCET. For an FCET, two extra sets are used; i.e., a maximal itemset, and the TID set which is the union of all TID sets for each item in the maximal itemset. For example, for each item in a given maximal itemset ACTW}, if item A appears in transaction 3, 5, 6, 7, item C in transaction 3, 4, 5, 6, 7, 8, item T in transaction 3, 5 7, 8, and item W in transaction 3, 4, 5, 6, 7, then we keep two extra sets for the FCET; i.e., the maximal itemset ACTW} and the TID set {3,4,5,6,7,8} which is the union of {3,5,6,7}, {3,4,5 6,7,8}, {3,5,7,8}, and 3,4,5,6,7 In addition to the two extra sets just mentioned, an FCET still contains other information such as inverted bits, next pointers, and SUB_TID sets. For a subset of the maximal itemset, the inverted bit is used to express whether the transactions containing the subset are the complement of the corresponding SUB_TID set. The next pointer is used to point to the location of the next superset of the current subset while calculating the support count The SUB_TID set stands for the transactions containing the subset or not, based on the inverted bit. For the transaction database and the generated frequent itemsets as shown in Fig. 2\(a\d. 2\(b\e maximal itemsets are ACTW} and {CDW}, and the FCET of {ACTW} can be built as shown in Table 1. As a matter of convenience in dealing with redundancy conditions in FCET, we use a single sentinel to represent NIL. For a FCET, the sentinel is an object with the same fields as an ordinary structure in the FCET. Its inverted_bit field is 0, next_pointer field is NIL and SUB_TID field is 004 All the redundant itemsets are replaced by pointers to the sentinel nil[FCET e d o  n o t s t o r e th e rep eat ite m s et s, f o r  example, in the maximal itemsets {ACTW} and {CDW where [{C},{W},{CW m s et s  bu t w e j u s t  store one copy in FCET of {ACTW} and the repeat itemsets in FCET of {CDW} then replaced by pointers to the sentinel nil[FCET ce, th e repeat ite m s e t s all are  occupy the same memory. The purpose that we do it in this way is mainly to save the storage space and increase the search efficiency    Fig. 2 a\ Transaction database \(b\ Original frequent itemsets with minimum support count 3 \(c\ Vertical-TIDdatabase  Algorithm FCET Set k the maximal length of frequent itemsets Set M the set of maximal frequent itemsets Set TS the set of transactions of all items in the maximal itemset MS Generate_Frequent_Closed_Enumeration_Table k i 1  005 F i  1: j=k+1; M=ÿ; i=0 2: while j 006 0 3:{  j=j-1 4:    do select S 007 F j that S is not subset of M 5:   { MS=S; i=i+1 6 TS=TS 005  k i 1  005 S i TID#_ Where S={s1,s2,s3,Ösk},k=len\(S 7:     M=M 005 MS 8:     create the corresponding FCET i  9:     initialize invert ed_bit=0; next_pointer=NIL SUB_TID 10 11:   else S 007 F j is a subset of k j i 1   005 F i  12:   {if the support of S is equal to the support of F i then 13:      { get the index number k 1 of S within the FCETi using the  perfect hash function 14:         get the index number k 2 of superset of S within the FCETi  where its next pointer=NIL 15:         FCET i k 1  e x t _poi n t er=F C E T i k 2  16 17:       else 18:         { get the TID# of S from Vertical-TID-database 19:            get the index number k 1 of S within FCETi using the  perfect hash function 20 if the length of S.TID# > 1/2 length of TS i then 21:              {   FCET i k 1  n v e rt ed_bi t  1  22:                   FCET i k 1   S U B _ TI D  TS i S.TID 23 24:              else 25:                       {   FCET i k 1  n v e rt ed_bi t  0  26:                            FCET i k 1   SUB _ T I D=S T I D   27 28 29 30 31:         return m i 1  005  FCET i where m is the number of maximal itemsets  Fig. 3 Building FCET from original frequent itemsets  In the FCET1 for the maximal itemset {ACTW shown in Table 1, we create 15 subset entries according to line 8-9 in Fig. 3. N.B., its ms1 and ts1 can also be found according to line 5-6. Here, no frequent itemsets are 
567 


stored, but they can be found using the perfect hash function [9 a s sho w n i n e q ua t i o n  1      1 1 1 1 0 1 1               000 000  C C  C  i  X T i L L i  i  X T i L L i T i HF   1    The hash function used here can be expressed as follows T: the length of a maximal itemset L: the length of a subset itemset  X\(i\e position in a maximal itemset for the ith item of a subset itemset where 1 010 i 010 L and X\(0 HF: the position of a subset item in a maximal itemset  For the maximal itemset {ACTW} shown in Table 1 if we want to find itemset {TW} with X\(0\=0, X\(1\=3 and X\(2\e use the hash function above to obtain HF=10. Because the support count of {TW} is equal to those of its supersets {ATW}, {CTW}, and {ACTW}, it is not a frequent closed ite mset. Therefore, according to line 13-15, we should follow its next pointer and move to the location of its superset to find its support count. For another example to find itemset {T}, we obtain HF=3 using the hash function, and also follow its next point and move to the location of its superset, since {T} is also not a frequent closed itemset. Owing to FCET i n v e rt ed_bi t  1, t h e T I D  of  T s h ou l d be  3,4,5,6,7,8}-{4,6}={3,5,7 8} according to line 20-23   Table 1 FCET1 for the maximal itemset {ACTW}, where NIL  ms={ACTW ts={3,4,5,6,7,8 index 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 inverted _ bit 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 next _ poi nter 12 8 9 12 15 12 15 15 15 15  SUB_TI D  4 6 8 ÿ ÿ 4 8  3 5 7   3.2 InFrequent Closed Enumeration Table IFCET  Here, we also give a property used in the IFCET as follows Property 2: All nonempty supersets of an infrequent itemset must also be infrequent The infrequent closed enumeration table \(IFCET\ has the same skill as the FCET uses, but the itemsets in the IFCET are infrequent itemsets. The reason to store them is that we cannot calculate the support count of generalized itemsets only from the FCET, for a taxonomy tree. For the example as shown in Fig. 4, when we calculate the support count of item T5 \(i.e T5.TIDs=\(M.TIDs 005 N.TIDs\here item M and N are infrequent itemsets, we must construct the IFCET to store the information of infrequent itemsets        Fig. 4 Taxonomy tree We can build up the IFCET based on Property 2. For the example taken from Fig.1, item M, N, X, and Y are all infrequent items, and we can use the existing clustering technique to classify them according to the transactions where they appear. Then, the IFCET can be built up according to the clustering results. In the IFCET, we only store infrequent-1 items and maximal infrequent  itemsets since the information of infrequent-k \(k>1\msets is helpless when counting the  support of non-leaf nodes in the taxonomy tree. Thus, the required memory for the IFCET is not exponentially large. For example, the IFCET for the infrequent itemset {MN} is shown in Table 2 and the generalized association rules generated from Fig.2 and Fig. 4 are shown in Table 3 Table 2 IFCET1 for the infrequent itemset {MN}, where NIL cluster1.ims={MN cluster1.ts={1,2 index 1 2 3 inverted_bit 1 0 0 next_pointer 3    SUB_TID ÿ 1 1 2 
568 


Table 3 The generalized association rules generated from above database and taxonomy tree Generalized association rules confidence Generalized association rules confidence A==>T4 0.75 C,W==>T1 1 A==>T3 0.75 C,T1==>W 1 C==>T1 1 W,T1==>C 1 C==>T3 1 T==>W,T1 0.75 T3==>C 0.8 W==>T,T1 0.75 D==>T4 0.75 T,W==>T1 1 D==>T3 0.75 T,T1==>W 0.75 T==>T1 1 W,T1==>T 0.75 W==>T1 1 C==>T1,T3 1 T4==>T1 1 T3==>C,T1 0.8 T1==>T4 0.83 C,T1==>T3 1 T1==>T3 0.83 C,T3==>T1 1 T3==>T1 1 T1,T3==>C 0.8 A==>C,T3 0.75 C==>T,W,T1 0.75 C==>A,T3 0.75 T==>C,W,T1 0.75 A,C==>T3 1 W==>C,T,T1 0.75 A,T3==>C 1 C,T==>W,T1 1 C,T3==>A 0.75 C,W==>T,T1 0.75 C==>T,T1 0.75 C,T1==>T,W 0.75 C==>T,T1 0.75 T,W==>C,T1 1 T==>C,T1 0.75 T,T1==>C,W 0.75 C,T==>T1 1 W,T1==>C,T 0.75 C,T1==>T 0.75 C,T,W==>T1 1 T,T1==>C 0.75 C,T,T1==>W 1 C==>W,T1 1 C,W,T1==>T 0.75 W==>C,T1 1 T,W,T1==>C 1 C,T1==>T,W 0.75 C,T1==>T,W 0.75  4. FCET Index Tree  In this section, we employ a signature tree called the SG-tree [16 5 t o co m b in e m u ltip le FCET s. It is a  dynamic balanced tree similar to R-tree. Here, we call it an FCET index tree. In the tree, each internal node represents a logical OR with all its children, and contains an entry of the form <sig, ptrs>. The root has a ìNULL signature and contains all the items in maximal itemsets In the leaf node, sig is the signature of an FCET, and ptrs point to the FCETs sharing the signature. In other words the signature of each internal node contains all signatures in the subtree pointed by itself. The index tree can help in finding maximal itemsets very quickly. As shown in Fig 5, if we want to find itemset {BH} with corresponding bit string 0100000100, we start searching from the root. After comparing with the left child, we find \(1111110000 and 0100000100 006 0100000100, so we compares with the right child \(0100001111 and 0100000100\00000100 Then downgrading to the next level, we continue to compare the left child \(0100001100 and 0100000100\00000100, and finally we reach to the FCET of {BGH}. Then, we can use the hash function expressed in equation \(1\o get the support count of BH} in the FCET of {BGH}. The total time complexity is O\(log2n\+O\(1\where n is the number of maximal itemsets              Fig. 5 FCET index tree   5. FCET Partition Tree for a Long Pattern  In this section, we also propose the partition-andmerge method to solve a long pattern problem t h e  length of a maximal itemset is 40, we may build 2 40 1 cells in an FCET; however, this is unrealistic. Instead, 2 40  cells are partitioned into 2 10 2 10 2 10 2 10 where it is feasible to fit 2 10 cells in the memory. Finally, we merge these cells using an intersection operation. For example, if we want to find the support count of {IJKL}, we reach to the node with signature {11111111111111110000000 Because the length of the maximal itemset is too large here we assume the maximal length within a node is 10 the node is designed to point to a partition tree, as the dotted box shown in Fig. 6. Then the SUB_TID of {IJ will be found in the FCET0_1, and the SUB_TID of KL} found in the FCET0_2. Finally, if 1,2,4,7,9 the return result will be {1,7,9} with support count=3            Fig. 6 FCET partition tree   6. Performance Evaluations           
569 


6.1 Simulation Model  In the section, we evaluate the performances of four data structures for algorithm Ap  E clat  C H A R M [10 an d F C ET on a D E L L G X 270 w i t h  Intel Pentium 4 3.2Ghz and 1 GB main memory running Windows XP. All the experimental data are generated from a normal distribution. The relative simulation parameters are shown in Table 4. To make our dataset representative, we generate two types of databases in the experiments; i.e., DENSE databases and SPARSE databases. Each item in the DENSE database is randomly generated from a pool P called potentially frequent itemsets with size 300, whereas each item in the SPARSE database is randomly generated from a pool N i.e., the set of all the items\ with size 1000. Since the items in the DENSE database are more clustered than those in the SPARSE database, larger frequent itemsets will probably be produced in the DENSE database for the same minimum support. In addition, we use the notations T for the average number of items per transaction I for the average number of items in a frequent itemset, and D for the number of transactions. For example, the experiment labeled with T 10 I 3 D 1K represents the simulation environment with 10 items on the average per transaction 3 items on the average in a frequent itemset, and 1000 transactions in total Table 4 Simulation parameters with default values D Number of transactions 100,000 T Number of the items per transaction 5-40 P Number of potentially frequent itemsets 300 I Number of the items in a frequent itemset 2-5 N Number of items 1000 R Number of taxonomy trees 31-75 L Number of levels in a taxonomy tree 3  6.2 Experimental Results  Experiment 1  In this experiment, we explored the required storage spaces of Apriori, CHARM, MaxEclat and FCET in the environment T10.I5.D10K under different minimum supports in SPARSE databases \(Fig 7\. We found that the FCET saves almost 10 times as much space compared with Apriori, especially under smaller minimum supports. The reason is that the FCET only stores maximal frequent itemsets, rather than the frequent itemsets  T10I5D10K 0 2000 4000 6000 8000 10000 12000 0.02 0.01 0.007 0.005 minimum support stora g e s p ace \(b y tes FCET MaxEclat Apriori CHARM Fig. 7 Required storage spaces in SPARSE databases  Experiment 2 In this experiment, we also explored the required storage spaces of Apriori, CHARM, MaxEclat and FCET in the environment T 40 I 20 D 10K under different minimum supports in DENSE databases \(Fig. 8 We found that the FCET saves almost 20 times as much space compared with Apriori, especially under smaller minimum supports. The reason is that the FCET can store the maximal itemsets generated in DENSE databases more efficiently than those in SPARSE databases  T40I20D10K 0 10000 20000 30000 40000 50000 60000 70000 80000 90000 0.02 0.01 0.005 0.003 0.002 minimum support storage space \(bytes  FCET MaxEclat Apriori CHARM Fig. 8 Required storage spaces in DENSE databases   Experiment 3 In this experiment, we explored the execution time of BASIC [17 Cu m u late [1 7  GMFI [9  and GMAR o rithm s  in t h e env i ronm e n t T10.I5.D10K/T40.I20.D10K under different minimum supports, as shown in Fig. 9 and 10, respectively. From Fig. 9 and 10, we found that GMFI/GMAR m pl o y i n g  the FCET perform better than the other ones in either SPARSE databases or DENSE databases   0 2 4 6 8 10 12 14 16 18 3210.75 minimum support execution time \(minutes GMFI GMAR BASIC Cumulate Fig. 9 Mining time in SPARSE databases  
570 


0 10 20 30 40 50 60 3210.75 minimum support execution time \(minutes GMFI GMAR BASIC Cumulate  Fig. 10 Mining time in DENSE databases  7.  Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log2n\ where n is the total number of maximal itemsets. For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results. Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces, especially in dense databases  7.    Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log 2 n\ where n is the total number of maximal itemsets For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces especially in dense databases 8. References  1 g ra w a l, T  Im ieli n s k i an d A  S w a m i M in i n g  association rules between sets of items in large databases,î Proc. ACM International Conference on Management of Data \(1993\, pp. 207-216 2 g ra w a l an d R. Srik an t F a s t alg o rit h m s f o r  mining association rules,î Proc. 20th International Conference on Very Large Data Bases \(1994\ pp.487499  J i aW e i Han  J i an  P e i an d YiW en Yi n   Mi n i n g frequent patterns without candidate generation,î Proc ACM International Conference on Management of Data 2000\p. 1-12 4 J  S   Pa r k  M  S  C h en  an d P S  Y u     A n ef f e c t iv e hash-based algorithm for mining association rules,î Proc ACM International Conference on Management of Data 1995\p.175-186 5 A  Sa va se r e  E  O m i e c i ns ki  a n d S N a va t h e    A n  efficient algorithm for mining association rules in large databases,î  Proc. 21st International Conference on Very Large Data Bases \(1995\ pp.432-443 6 Y i n-F u H u a ng a n d  Chi e h M i ng W u   M i n i n g  generalized association rules using pruning techniques Proc. IEEE International Conference on Data Mining 2002\p.227-234 7 a k i a n d C.J  Hsia o   E ff icie n t al g o rith m s f o r  mining closed itemsets and their lattice structure, î  IEEE Transactions on Knowledge and Data Engineering, vol 17, no. 4, April \(2005\p. 462-478   Bu rdick  M. C a li m l i m  an d J. Geh r k e  M AF I A a maximal frequent itemset algorithm for transactional databases,î Proc. 17th International Conference on Data Engineering, \(2001\p.443-452  a k i S  P a rth a s a rat h y   M. Ogih ara, a n d W. Li   New algorithms for fast discovery of association rules Proc. 3rd ACM International Conference on Knowledge Discovery in Databases and Data Mining, \(1997\pp 283-286 10 M  J  Za ki a n d K  G o ud a  Fa s t ve r t i c a l  mi ni ng usi n g  diffsets,î Proc. 9th ACM International Conference on Knowledge Discovery and Data Mining, Aug. \(2003   Mam o u l i s D  W. C h e u ng an d W. L i a n    Similarity search in sets and categorical data using the signature tree,î Proc. 19th International Conference on Data Engineering, \(2003 12 R. Srik a n t a n d R Ag ra w a l   M i n i n g g e n e ralized  association rules,î Proc. 21st International Conference on Very Large Data Bases, \(1995\.407-419    
571 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


