222A Subjective Distance for Clustering Security Events Jianxin Wang Geng Zhao Weidong Zhang Beijing Electronic Science&Tech Inst Beijing China Beijing China Beijing China School of Information The Key Laboratory Technology-cooperation Office Beijing Forestry University University of Science and Tech Beijing wangjx@bjfu.edu.cn zg@besti .edu.cn zwd@admin.ustb.edu.cn Abstract-It is well known that intrusion detection systems overload their human operators by triggering per day thousands of alarms most of which are false positives A clustering method forwarded by CIaus Julisch is considerably effectual in eliminating the fahe positives and finding the root causes. Eut there may according 
to the variance related to the operators\222 knowledge and experience, exists a gap between the nature of the event clusters and what the operators obtained from the resulted clusters A subjective distance, different from the objective distance defined by Julisch is foharded in this paper to fill the gap by the means of controlling the clustering process with the subjective distance 1 INTRODUCTION Intrusion detection systems IDS as a new and potent approach to protect computer systems are increasingly deployed in response to attacks against enterprise networks 9 However there appears another difficult problem with the use 
of IDS A great number of alarm messages are generally triggered which turn out to be a burden of the human operator It is common for an IDS to trigger thousands of alarms per day up to 99 of which are false positives 6 Association rules between adjacent alarms are mined and then they are used to implement anomaly detection on the IDS alarms 7 According to the work described in that paper alarms that are consistent with these association rules are deemed 223normal\224 and are 
then discarded Clifton and Gengo SI used episode mining to help construct the filtering rules.Claus Julisch and his cooperators using a clustering approach to identify the root cause l 2 51 Their work should be emphasized here with respect to the effect of their clustering method In fact we forwarded a new computer language TTT Language in which their work was assimilated for security log analysis 13 Relatively more detailed discussion about their work will be given in the next sub-section A To cluster the events a distance between two events 
must be defined in advance In practice distance is easy to define for numerical attributes but categorical time or string The distunce defined by Ciaus Julisch Supported by the National High-Tech Research and Development Plan of China under Grant No 2003AA148020 0-7803-901 5-6/05/$20.00 02005 IEEE attributes give rise a problem when being defined the distance of 12 Unfortunately, alarm messages can contain all of these attribute types In Julisch\222s work, similarity or distance can be defined in a uniform manner using taxonomy By way of 
illustration Figure 1 shows how taxonomy of the IP address port and time attribute can be obtained The taxonomy plays a basic role in the clustering method It can be seen in Figure 1 c d that the taxonomy of the attribute time can be constructed in two ways In fact the domain of almost every attribute can be classified this way ANY-IF ANY-PORT ANY-DAY-OF-WEEK ANY.DAY-OF-MONTH J11 J11  4 h0  h23 h0  h23 c Figure 1 Taxonomy of several kinds ofattributes Let 
A be an alarm attribute alarms are defined as tuples over the Cartesian product IT Isi+,dom\(Ai A tree I is called a taxonomy tree if it is the taxonomy on the elements of domain\(Ai\There are 4 examples of taxonomy trees in Figure 1 For two nodes x y i n 5 x is called a parent ofy and y a child of x if there is an edge x  y in E Furthermore x is called a generalization 
of y if T contains a path from x toy in symbols xay If x is a parent of y then x certainiy is the generation of y Finally x~x is trivially satisfied These definitions can be illustrated with the taxonomy tree of Figure l\(a Node 223ANY-IP\224 is a parent of node 223DMZ\224 while 223DMZ\224 is a child of 223ANY-IP\222 And 223ANY-IP\224 is a generalization of 223ANY-IP\224 223DMZ\224 and 223ipl 224 respectively 74 


Based on the definition 223generalization\224 the distance between two nodes in st taxonomy tree can be defined The distance between x and y is the number of edges between x andy ifx is a generation ofy or ify is a generation ofx But the distance between two nodes is undefined if the relationship of generalization does not exist between them A vector every element of which is a node from the corresponding taxonomy tree is also called an alarm It is different from en ordinary alarm for its elements are probably not taken from its attribute domain A cover of a set of alarms b yz  yn is 221an alarm x which is a generalization but as special as possible of each alarm in the set The example is also from Figure l\(a b Given a set of 3 alarms DMZ SO ip3 801 and \(FIREWALL PRIVATE then the cover of the set is DMZ PRIVATE Although the alarm ANY-IF ANY-PORT is the generalization of each alarm in the set it is not the cover of the set because it is not special enough So the distance of a set of alarms is defined as the average distance between its cover alarm and each of its member alarm The distance of the alarm set just mentioned with three members is 1+2+1  413 The clustering problem forwarded by Claus Julisch can be now described as:.finding a set of alarms in all the alarms triggered, satisfying at the same time the requirement that the number of the set members is greater than or equal to a given threshold while the distance of the set is minimal Though Julisch\222s method of clustering with the distance defined by himself is effectual in eliminating.false positives and forming an event overview with high quality there is an inherent deficiency in the distance definition If a generalization alarm is got as a cluster it is then the representative of all the alarms it covers If the proportion among the alarms does not match what it is considered to be then the cluster tortures the nature of among the events with loss of information to some extent when presented to human operators Figure 2 shows a simple example in this case There are altogether 210 primitive events listed in b and the taxonomy trees of source IP address and destination IP address are constructed as shown in a and c respectively According to Julisch\222s clustering problem with a given threshold 100 two results can be independently obtained and the results are shown in d and e respectively But which is a good result and which is a bad one Julish\222s method is not able to answer the question In fact if S1 according to the operator\222s knowledge and experience,\222 is more offensive than S2 then the second result will be better for it will help the operator get more precise information about the alarms in the cluster After getting the cluster S D1 1 10 in the second result he wiIl assume it as a matter of course that S1 attacked Dl much more times than 52 did This is nearly the nature that the triggered alarms suggested But if S1 and 2 are according to the operator, equally offensive then the second result is not a good one So the problem is how to get the good result according to the operator\222s knowledge and experience To quantitatively and systematically solve this problem is the main purpose of this paper SI D2,70 52 DI 30 SI b Primitive alarms d The first clustering result e The second clustering result Figure 2 Two possible results of Julisch\222s method B Other related work To define and use a subjective distance is not new Subjective distance measures are based upon user beliefs or biases regarding relationships in data such as an approach utilizing Bayes Rule to revise prior beliefs 3 and a systematical approach to subjectively measure the interestingness in knowledge discovery 4 But the purpose and method to define and use a subjective distance in this paper are quite different from those of their work 11 SUBJECTIVE DISTANCE The subjective distance of a cluster is defined similar to that of an objective distance described above But an additional concept  the subjective proportion  should be taken into consideration A subjective proportion is the proportion among the child clusters which is in the human operator\222s mind If a subjective proportion of child clusters of All in Figure 3 is 0.4 0.4 0.1 then it means that the operator thinks that A21 and A22 appear in alarms almost equally frequently and that they both appear far more frequently than A23 does Accordingly the objective proportion among its child clusters for instance, should be 10 5 5 namely 0.5 0.25 0.25 A Another approach to get the objective distance Remember that the distance of a cluster \(namely a node with all the alarm elements it covers\within a taxonomy tree is the average number of edges counted from the alarm members to the cover But we will define it another way with its meaning not changed Definition 1 The objective distance of a cluster in a taxonomy tree is the sum of 1 and the frequency-weighted sum of the objective distances of all its child clusters and the objective distance of each leaf cluster is 0 if the proportion among the frequencies with which the child cluster appears is PI pz  p and the objective distances of all its child clusters are dl d  d respectively, then the objective distance of the cluster is 75 


do I+2br-d 1 1=1 The objective distance in the above definition is defined recursively It will be attained level by level from the 0 distance leaf clusters to the root cluster And the constant number 1 implies that the distance between the cluster node and the child cluster node is 1 A simple example shown in Figure 3 will illustrate the definition The nodes A31 A32 A33 A34 A35 are leaves; therefore objective distances of these clusters if they are looked on as clusters according to the definition above, are all 0 And the objective distance of leaf A22 in the second level is 0 as well. But the distance of the cluster A21 is 1  0 X 0.4  0 X0.2  OX 0.4  1 Similarly, the distance ofthe cluster A23 is 1  OX 0.6  0 X 0.4  1 And finally we can get the distance of the root cluster A1 1 as 1  1 X0.5 f OX0.25  1 X0.25  1.75 I All.20\222 I Figure 3 Another objective distance definition Numbers after a coma mean the times repeated Considering the distance definition of Julisch we can work out based on all the primitive alarms the leaf ones thedistanceoftheclusterAl1 2x4 2X2+2X4+2X3  2 X 2  5 X 1  35/20  1.75 This result is the same as the above result worked out using our new definition It can be seen that with the new definition the distance of a cluster is obtained level by level from the leaf to the node that represents the cluster It is this way that the subject distance will be defined in the next sub-section B Subjective distmce The subjective distance of a cluster is defined similar to that of an objective distance defined above The concept of subjective distance is based on the concept of subjective proportion described at the beginning of Section 11 We also take the cluster of All as an example The objective proportion among its child clusters is 10 5 5 namely 0.5 0.25 0.25 distances of all the child clusters are d d2  d respectively, then the subjective distance of the cluster is The definition of the subjective distance of a cluster is quite similar to that of the objective distance in each definition the distance is obtained from the bottom to the top level by level Each step deals with two kinds of distance The first kind is obtained from those contained in the child clusters and these distances are assimilated with a weight into the parent cluster The second kind is the distance generated during the course of using the parent cluster to represent all its child clusters There is difference between the two definitions For the objective one the constant number 2231\224 is simply added for the reason that from the lower level to the upper level next to it the number of edges is always 1 But in the case of the subject distance the difference between the subjective proportion and the objective proportion is measured After the subject proportions are placed into the views of the nodes shown in Figure 3 a taxonomy tree with nodes objective proportions, and subjective proportions can be got as shown in Figure 4 The subjective distance of A21 is 0.5-0.4  0.1-0.2  0.4-0.4  0 X 0.4  0 X 0.2  OX0.4  0.1414 And similarly we can get the subjective distances of A22, A23 as 0 0.5656 respectively. Therefore we are finally able to get the distance of the root cluster A1 1 as 0.2-0.5  0.4-0.25 f 0,4-0.25  0.1414XO.5 f 0 X 0.25  0.5656 X 0.25  0.579 Figure 4 The subjective distance definition the numbers mean the subjective proportion C Synthetic distance After two kinds of distances, objective and subjective ate defined we can combine them together to get an improved distance definition Definition 2 The subjective distance of a cluster is the sum of two parts the first part is the second-order Euclidian distance between the subjective proportion and the objective proportion among its child clusters the second part is the child clusters and the objective distance of each leaf cluster is\2220 If the subjective proportion of a cluster is PI pz  pn Definition 3 The synthetic distance of a cluster in a taxonomy tree is the weighted sum of its objective distance and its subjective distance the sum of whose weight is I And the synthetic distance of a generalization alarm is the objective distance of a cluster is do whose weight is r and fiequency-weighted sum Of the distances Of its sum of the synthetic distance of its elements If the the objective proportion is ql q2  qn and the subjective 76 


subjective distance of the cluster is d then the synthetic distance is d ad I-a 3 1 In this definition we suppose that there is a coefficient Q that we know But how can we get it How to choose the value of this coeficient will be discussed later If the distance in Julisch\222s clustering problem is replaced with the synthetic distance defined above then we will easily solve the problem show in Figure 2 Suppose that the subjective proportion between S1 and 52 is 0.8 0.2 and that between DI and D2 is 0.3 0.7 It can be easily got that the subjective distance of the first resulted cluster SI D 150 is 0 f 0.8-0.3  0.2-0.7  0.707 and thus the synthetic distance is 1.707 Similarly the synthetic distance of the second cluster S Dl 110 is 1.103 So the second cluster is better Supported by this analysis we can retain the second cluster and discard the first one The choice made is satisfactory in that after obtaining the second cluster in Figure 2 the operator will get the information that S attacks D1 for 110 times Based on his knowledge that the proportion between S1 and S2 is 0.8 0.2 he can estimate that 1 attacked D1 for about 88 times while S2 attacked D1 for 22 times or so This is nearly the nature of the alarm cluster in which SI appeared 80 times and S2 appears 30 times Analogously after getting the first cluster S 1 D 1 50 in Figure 2 the operator will guess according to his knowledge that D1 and D2 appeared approximately 45 times and 105 times respectively But this is quite apart from the nature of the cluster in which D1 and D2 appeared 80 and 70 times respectively With this example we know that our subjective distance really works The subjective distance, one may think is of no use if the human operator 223drills down\224 into the cluster to see the alarms as specific as he wants But this is not consistent with the purpose of \223clustering\224 which is applied to decrease the great amount of data to a smaIl number of clusters Besides the number of alarms triggered is generally very large which makes it almost impossible to study the alarms or clusters one by one or in a considerably low level D Getting the biases and belieji of operators It seems that so far we have solved the problem that Julisch encountered But how can we obtain the subjective distance if all the subjective proportions are unknown Our first intuition is to let the operator input the proportions in his mind into the system and edit them whenever he changes his mind about the proportions This seems easy and perfect but it is another burden on the operator After all our purpose is to help the operator not to make more trouble for him Based on the observation that nodes with more descendants generally appear more frequently but it is not always true and in some circumstances it is quite the contrary we can decide the proportions by counting the descendant leaf nodes By this mean we can get the proportion among A3 1 A32 A33 in Figure 4 as 1  1  1 that between A34 and A35 as 1 1 and that among A21 A22 A33 as 3 1 2 But the proportions obtained this way may be quite different from the subjective proportions in the operator\222s mind Therefore we should have a better approach Using the historical data to trace the subjective proportion is another approach After studying the historical data for a considerable period of time the subjective proportion will be close to that the historical data of events presents The effect of an event is closely related to the time when it occurred An approximate relationship between the time and the degree of effect of a historical event on the subjective proportion for instance is shown\222in Figure 5 Figure 5 Different weight of historical data The point to is a time point when the operator investigated the alarms most recently and T is the period of time within which the events are taken into account Pis a parameter to be determined Of course this is only an example and we can draw other kinds of lines with the same trend In order to make our system leam the subjective proportions from the historical data, we would define a 6  function first Let a be an alarm and n be a node in a taxonomy tree define 1 nfa 0 ne a 6 a 4 And then we can according to the relationship shown in Figure 5 get the weight of any node n in the taxonomy trees where t\(a is the time when alarm a occurred With the weight of every node, which is trained from the history data the proportions can be obtained These proportions are used as approximations of the subjective proportions There is much to do about how the values of parameters B and Tare selected and how the shape of the line or curve is determined in Figure 5 Furthermore we can combine these methods for an improved approach For instance historical data can be used to obtain the approximate proportions and these proportions can be adjusted by the human operators whenever they want to But this is out of 77 


the scope of this paper In this paper we suppose that the subjective proportions have been obtained before being used Tree 111 EXPERIMENTS AND CONCLUSIONS We took a set of 19462 alarms to test the effect of the subjective distance on the clustering results Three taxonomy trees have been constructed in advance \(see Table I and the subjective proportions have been manually input, too Data rype Number of Number of layers nodes SourceIP AtlackType DestinationlP Character 6 172 Character 5 1 80 Character 2 13 After the objective distance is replaced with the synthetic one we can get the clusters one by one using Claus Misch\222s method with different values of the parameter U The corresponding results are shown in TabIe 2 U Number of clusters Numberof prim alms Numbermean of layers Objective TABLE 11 USING SYNTHETIC DISTANCE IN CLUSTERING 1.0 0.9 0.S 0.7 0.6 0.5 21 24 25 25 24 24 76 85 8s 9s I05 120 1.842 1.861 1.860 1.92s 2.033 2.107 1.842 1.861 1.860 i 1.925 2.033 2.107 distance mean Subjective distance mean i 1.384 1.015 0.912 0.860 222 0.769 0.732 7 With the definition of subjective distance we have improved the clustering method that Claus Julisch forwarded The new method can solve the problem, indicated in Figure 2 which cannot be solved with the old method A subjective distance will make the human operator believe that, given the cluster what he sees is the nature of the event alarms and thus will help save his time that otherwise he would spend to 223drill down\224 into the cluster to see more detailed information So far as we know other clustering problems will probably be improved with a subjective distance similar to that we define in this paper One aspect of our future work will be to generalize the subjective distance and apply it in more clustering problems How to get the parameters in Figure 5 is another aspect of our future work a Number of clusters Numberofprimalarms Numbermean oflayers Objective distance mean Subjective distance mean REFERENCES I Klaus Julisch Mining Alarm Clusters to Improve Alarm Handling Efficiency 17th Annual Computer Security Applications Conference ACSAC\222OI December 10-14,2001 New York K Julisch and M Dacier Mining Intrusion Detection Alarms for Actionable Knowledge Proc 8th ACM International Conference on Knowledge Discovery and Data Mining Edmonton, July 2032 3 B Padmanabhan and A Tuzhilin A Belief-driven method for discovering unexpected patterns In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining KDD98 pages 94 100 New York August 1998 4 A Silberschatz and A Tuzhilin On subjective measures of interestingness in knowledge discovery In Proceedings of the First International Conference on Knowledge Discovery and Data Mining KDD\22295\page 275-281 Montreal Canada August 1995 K Julisch Dealing with False Positives in Intrusion Detection In 3rd Workshop on Recent Advances in Intrusion Detection 2000 http www.raid-symposium.org/raid2OOO/program html S Manganaris et al A Data Mining Analysis of RTlD Alarms In 2nd Workshop on Recent Advances in Intrusion Detection 1999 http w.ra~d-symposium.org/raid99/index.html A Valdes. Probabilistic Alert Correlation In 4th Workshop on recent Advances in Intrusion Detection, 2001 C Clifton and GGengo Developing Custom lntrusion Detection Filters Using Data Mining In Military Communications Int\222l Symposium MILCOMZODO October 2000 9 R Bace. Intrusion Detection. Macmillan Technical Publishing 2002 IO I Han Towards Efficient Induction Mechanisms in Database Systems. Theoretical Computer Science 133 361-385 October 1994 Ill H Debar M Dacier and A Wespi A revised Taxonomy for Intrusion Detection Systems Annales des Tdecommunications 55\(7 1121 D Fasulo An analysis of Recent Work on Clustering algorithms 1999 http cs.Washington.edu/home/drasulo I31 Jianxin Wang Geng Zhao Wei Wet Peng Ye TIT Language and TIT Security Log Analyzer ACNS2004, June 6  11 Huangshan China 2 SI 6 7 8 8 361-378,2000 0.4 0.3 0.2 0.1 0.0 29 3t 3s 36 39 155 127 144 176 243 2.151 2.200 2.486 2.733 3.157 2.15 I 2.20C 2.486 2.733 3.157 0.727 0.734 0.735 0.708 0.606 78 


Revue Roumaine Math Pures et Appl Lattice theory third ed Proceedings of ICTAI03 Journal of Information Systems Information Processing Letters  April 2002  R W ille Conceptual landscapes of kno wledge a pragmatic paradigm for knowledge processing  R W ille Restructuring Lattice Theory  In I Ri v al editor   24\(1 1999  J Pei J Han and R Mao CLOSET An ef 002cient algorithm for mining frequent closed itemsets In 5 Conclusion Acknowledgements References  Melbourne FL November 2003  M J Zaki and C.-J Hsiao CHARM An ef 002cient algorithm for closed itemset mining Technical Report 99-10 Rensselaer Polytechnic Institute 1999 Journal of Parallel and Distributed Computing Formal Concept Analysis provides a natural effective platform for data analysis and knowledge representation In this paper we propose FCA as a tool of data analysis and representation for digital ecosystem We also propose to use the technologies of data mining to extract knowledge from huge and complex heterogeneous distributed data in the DE Furthermore some issues of the applications of data mining for Digital ecosystem is discussed This work is supported by the project of EU IST Network of Excellence 224OPAALS\224 Proceedings of The 2004 International Conference on Machine Learning and Applications ICMLA04  1540:398\226416 1999  N P asquier  Y  Bastide R T aouil and L Lakhal Ef 002cient mining of association rules using closed itemsets lattices Third IEEE International Conference on Data Mining  chapter Fast discovery of association rules pages 307\226328 AAAI/MIT Press 1996  P  B Bhat C S Ragha v endra and V  K Prasanna Ef 002cient collective communication in distributed heterogeneous systems  American Mathematical Society Providence RI 3rd edition 1967  G Birkhof f  24\(94 1986  M Chein Algorithme de recherche des sous matrices premi 036 eres d'une matrice  13 1969  A Cherv enak I F oster  C K esselman C Salisb ury  and S Tuecke The Data Grid Towards an Architecture For the Distributed Management and Analysis of Large Scienti\002c Datasets 1999  H Fu and E Mephu Nguifo P artitioning lar ge data to scale up lattice-based algorithm In  pages 537\226544 Sacramento CA November 2003 IEEE Computer Press  H Fu and E Mephu Nguifo Mining frequent closed itemsets for large data In  Louisville USA December 2004  B Ganter  T w o basic algorithms in concept analysis T echnical report Darmstadt University 1984  B Ganter and R W ille  pages 105\226137 1995  E Mephu Nguifo M Liquiere and V  Duquenne  XXIII\(2 1978  L Nourine and O Raynaud A f ast algorithm for b uilding lattices  71:199\226204 1999  N P asquier  Y  Bastide R T aouil and L Lakhal Disco vering frequent closed itemsets for association rules  pages 21\22630 2000  F  Pro v ost Distrib uted Data Mining Scaling Up and Beyond In H Kargupta and P Chan editors  MIT/AAAI Press 2000  P  V altche v  R Missaoui and et al Generating frequent itemsets two novel approaches based on galois lattice theory Bull.Math.R.S Advances in Distributed Data Mining Lattice Theory  Taylor and Francis 2002  E Norris An algorithm for computing the maximal rectangles in a binary relation  Springer 1999  R Godin G Mineau and et al M 264 ethodes de classi\002cation conceptuelle bas 264 es sur les treillis de galois et application Formal Concept Analysis Mathematical Foundations Revue d'intelligence arti\002cielle Figure 4 An example of closed itemset lattice ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a 1 2 3 4 5 6 7 8 1 3 4 5 1 2 3 4 6 1 3 4 6 1 2 4 6 1 4 6 1 3 4 1 2 3 1 2 3 7 8 1 2 7 8 1 2 7 1 3 7 8 1 7 8 1 7 1 3 1 4 1 2 1                                                   63\(3 2003  G Birkhof f Lecture Notes in Computer Science  Number XXV in American Mathematical Society Colloquium Publication American Mathematical Society 1973  J Bordat Calcul pratique du treillis de g alois d'une correspondance Math 264 ematiques Informatiques et Sciences Humaines Journal of Experimental and Theoretical Arti\002cial Intelligence JETAI Special Issue on Concept Latticebased theory methods and tools for Knowledge Discovery in Databases  pages 445\226470 University of Calgary Boston 1982  R W olf f and A Schuster  Association Rule Mining in Peer to-Peer Systems  In a Advances in Knowledge discovery and Data Mining  R Agra w al H Mannila R Srikant H T oi v onen and A Verkamo Symposium on Ordered Sets Journal of Experimental and Theoretical Arti\002cial Intelligence JETAI Special Issue on Concept Lattice-based theory methods and tools for Knowledge Discovery in Databases 


Theorem 3.1 Given a Boolean support function  b L 1  conf X  Y os eval b  X  eval b  Y     b L 1  Y   b L 1  X  10 Proof conf X  Y   b L 1  X  Y   b L 1  X   eval b  X   eval b  Y   eval b  X   2 2  cos eval b  X  eval b  Y   eval b  Y   2  eval b  X   2  cos eval b  X  eval b  Y    b L 1  Y   b L 1  X  where eval b  X   eval b  Y  is the dot product of evaluation vectors of X and Y  respectively We have used the the deìnition of the cosine measure cos x  y  x  y   x  2  y  2  as well as the following pair of facts  b L 1  X  Y  eval b  X   eval b  Y  and  b L 1  X   eval b  X   1   eval b  X   2 2  A useful property of the cosine measure is that if two vectors are multiplied by potentially different non-zero constants their cosine measure is unchanged As a result if we normalize each attribute of a binary transaction matrix to have an L 1 norm of 1 then the cosine measure between attributes does not change Thus if we can express   b L 1  Y   b L 1  X  in terms of support based on the Min-Apriori support function  min  L 1  then we will have an alternative deìnition of conìdence that matches the standard deìnition of conìdence for binary transaction data and works for continuous data Theorem 3.2 For traditional binary transaction data whose attributes have been normalized to have an L 1 norm of 1 and an itemset X   i 1 i k  of size k   min  L 1  X  c    X   where c is the mean of the nonzero entries of eval min  X  and   X  is the traditional support of the attributes X for the original unnormalized data Proof First notice that when attribute i j is normalized its non-zero entries become 1   i j   Thus all the non-zero entries of eval min  X  have the value min\(1   i j  Trivially the average of the non-zero values of eval min  X  will be c min\(1   i j   Furthermore eval min  X  will have   X  such entries one for each transaction where all the attributes of X are all non-zero Thus  min  L 1  X  c  X   and the theorem follows Using the previous two results We get a new deìnition of conìdence for continuous data that is normalized to have an an L 1 norm of 1 Deìnition 14 New Deìnition of Conìdence for Continuous Data conf X  Y os eval min  X  eval min  Y    min  L 1  Y  c Y  min L 1  X  c X  where c X  c Y  is the mean of the non-zero values of eval min  X   eval min  Y   We emphasize that this is a mathematical result and we are not claiming that Min-Apriori with this conìdence measure should be preferred to the original Min-Apriori Indeed if cosine similarity were used as a measure of conìdence then it would work for both binary transaction data with traditional support and for continuous data using the Min-Apriori approach Regardless we feel that the ability to derive results such as this indicates the potential value of our framework for generalizing conìdence We provide a numerical illustration of the result that we have just derived using the data in Table 7 Recall that conf CD  AB   5 when we use traditional conìdence Using Table 7\(c we will apply Deìnition 14 with X   C D   and Y   A B   Summing column AB of Table 7\(c we get  min  L 1  AB   6  Summing column CD of Table 7\(c we get  min  L 1  CD   5 From Table 7\(c we see c AB 1  5 and c CD 1  6  Finally cos eval min  CD  eval min  AB   1   3  Thus the conìdence given by Deìnition 14 is conf CD  AB  1  3  3  6  1  6 4  5  1  5  1  3 015 3 4  015 1 4 0  5  4 Related Work While other work in association analysis does not provide a general framework for extending the notion of conìdence some work has considered other measures for evaluating association rules Most notably correlation has been proposed as a substitute for traditional conìdence More common is work that focuses on recognizing new patterns that is proposing new evaluation functions One example is error tolerant itemsets ETIs which were used in this paper As another example the approach to generalized itemsets presented in seeks to nd both sets of attributes and attribute values that are more probable than usual i.e that represent a bump in the probability density function describing the data As a nal e xample some recent work uses a Boolean retrieval model to generalize the notion of an itemset 5 In this approach the patterns of interest are any Boolean formulae that involve binary variables and the logical connectives   and  015  or  and   not  7 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Quantitative association rules 3 13 15 try to predict the values of one set of attributes from another There has also been some work in integrating regression with association analysis Ho we v e r  both these approaches are distinct from our view of association rules and conìdence which involves only the prediction of the strength/presence of one pattern from another Our work is also different from that of constraint-based association analysis as typiìed by or 16 Although our pattern evaluation functions may seem similar to the constraints used in those approaches our pattern evaluation functions can be non-Boolean functions that return continuous values Conceptually this is a shift from evaluating whether a patterns occurs in an object to evaluating the strength of a pattern in a particular object Also most constraint-based association analysis focuses on added efìciency while our focus is on new types of data and patterns 5 Acknowledgments This work was partially supported by NASA grant NCC 2 1231 NSF grant ACI-0325949 and by the Army High Performance Computing Research Center under the auspices of the Department of the Army ARL cooperative agreement number DAAD19-01-2-0014 The content of this work does not necessarily reîect the position or policy of the government and no ofìcial endorsement should be inferred Access to computing facilities was provided by the AHPCRC and the Minnesota Supercomputing Institute 6 Conclusions and Future Work We have described a framework for generalizing the notion of conìdence and have shown that this framework can be used to express conìdence for general Boolean patterns including error tolerant itemsets We have also shown that this framework can be useful for continuous data by presenting an example of the use of a generalized notion of conìdence to derive a conìdence measure for continuous data that agrees with traditional conìdence for binary transaction data There is a need for efìcient algorithms for nding highconìdence association rules for non-traditional patterns and non-binary data sets Also experiments are needed to evaluate the usefulness of such association rules This includes a comparison to results obtained from converting the data into binary transaction data Areas for investigation in the theoretical area include exploring the appropriate deìnitions of conìdence to accompany various support functions that have been deìned It would also be worthwhile to explore using interestingness measures of association patterns  for the functions used to combine the pattern e v a luation vectors References  R Agra w al T  Imielinski and A N Sw ami Mining association rules between sets of items in large databases In SIGMOD 93  pages 207Ö216 Washington D.C May 1993  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB 94  pages 487Ö499 1994  Y  Aumann and Y  Lindell A statistical theory for quantitative association rules In KDD99  pages 261Ö270 1999  A Banerjee S Merugu I S Dhillon and J Ghosh Clustering with bregman divergences In SIAM 2004  pages 234 245 Lake Buena Vista FL April 2004  P  Bollmann-Sdorra A Hafez and V  V  Ragha v a n A theoretical framework for association mining based on the boolean retrieval model In DaWaK 2001 Munich Germany  pages 21Ö30 2001  S Brin R Motw ani and C Silv erstein Be yond mark et baskets generalizing association rules to correlations In SIGMOD 97  pages 265Ö276 1997  J W  Demmel Applied Numerical Linear Algebra SIAM January 1997  J Friedman and N Fisher  B ump-hunting in highdimensional data Statistics and Computing  9\(2 April 1999  E.-H Han G Karypis and V  K umar  T r 97-068 Minapriori An algorithm for nding association rules in data with continuous attributes Technical report Department of Computer Science University of Minnesota Minneapolis MN 1997  T  Hastie R T ibshirani and J Friedman The Elements of Statistical Learning  Springer Verlag August 2001  S Jarose wicz D A Simo vici and I Rosenber g An inclusion-exclusion result for boolean polynomials and its applications in data mining In Proceedings of the Workshop on Discrete Mathematics and Data Mining DM&DM SDM02 Arlington VA  AAAI April 2002  R T  Ng L V  S Lakshmanan J Han and A P ang Exploratory mining and pruning optimizations of constrained associations rules In SIGMOD 98  1998  M Ok onie wski L Gancarz and P  Ga wrysiak Mining multi-dimensional quantitative associations In INAP 2001 October  volume 2543 of LCNS  Springer 2003  A Ozgur  P N T a n and V  K umar  rba An inte grated framework for regression based on association rules In SIAM 2004  April 22-24 2004  R Srikant and R Agra w a l Mining quantitati v e association rules in large relational tables In SIGMOD 96  1996  R Srikant Q V u  a nd R Agra w a l Mining association rules with item constraints In KDD 97  pages 67Ö73 1997  M Steinbach P N T a n H Xiong and V  K umar  Gener alizing the notion of support In KDD 04  pages 689Ö694 New York NY USA 2004 ACM Press  P N T an V  K umar  and J Sri v asta v a  Selecting the right interestingness measure for association patterns In KDD 2002  pages 32Ö41 New York NY 2002 ACM Press  P N T an M Steinbach and V  K umar  Introduction to Data Mining  Pearson Addison-Wesley 2005  G I W e bb  D isco v e ring associations with numeric v ariables In KDD 01  pages 383Ö388 2001  C Y a ng U M F a yyad and P  S Bradle y  Ef cient disco v e ry of error-tolerant frequent itemsets in high dimensions In KDD 2001  pages 194Ö203 2001  M J Zaki and M Ogihara Theoretical foundations of association rules In DMKD 98  pages 7:1Ö7:8 1998 8 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


8] M. Gahegan. Data mining and knowledge discovery in the geographical domain 9] J. R. Holton, J. A. Curry, and J. A. Pyle. Encyclopedia of atmospheric sciences. Academic Press, Boston, 2002 10] Z. Kou, W. W. Cohen, and R. F. Murphy. Extracting information from text and images for location proteomics. In Proceedings of the 3nd ACM SIGKDD Workshop on Data Mining in Bioinformatics, Washington, DC, 2003 11] V. Kumar, M. Steinbach, P. Tan, S. Klooster, C. Potter, and A. Torregrosa. Mining scienti?c data: Discovery of patterns in the global climate system. In Proceedings of the Joint Statistical Meetings, Athens, GA, 2001 12] P. Lynch. Weather forecasting from woolly art to solid science. Meteorology at the Millennium, pages 106ñ119, 2002 13] T. N. Palmer. Predicting uncertainty in numerical weather forecasts. Meteorology at the Millennium, pages 3ñ13, 2002 14] R. Srikant and R. Agrawal. Mining quantitative association rules in large relational tables. In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data, pages 1ñ12, Montreal, Quebec, Canada, 1996 15] A. J. Stevermer. Recent advances and issues in meteorology Oryx Press, London, 2002 16] B. Zupan, E. T. Keravnou, and N. Lavrac. Intelligent Data Analysis in Medicine and Pharmacology. Kluwer Academic Publishers, 1997 Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSACí04 0730-3157/04 $20.00 © 2004 IEEE 


variable names mapped by the where keyword to the relation in DataSchema and pv are variable names that appear in the StructureSchema At instantiation time pv is assigned values of the Structure component and dv is mapped to the relation appearing in Data component. The de?nition for the pattern well-formed formula is now straightforward De?nition 14 A pattern formula is of the form fp\(dv 2 where fp \(formula predicate variablesmapped by thewhere keyword to the relation appearing in Data component From the previous de?nitions the semantics of the where keyword become evident: we impose that the variables of the formula will take values from speci?c relations when the formula predicate is employed in queries Example 2 Let us consider the following formulas 1. f\(x x 2. f\(g\(x x In the ?rst formula variable x is mapped to R using the where keyword, thus the formula is well formed. Keep in mind that the formula predicate by itself is just the part f\(x is not well-formed since y is not mapped via where to any relation, or otherwise range restricted 5. Querying the Pattern Warehouse We de?ne queries to be posed over the pattern warehouse and not individually over its data- or patternbase components. Through this approach, we are able to sustain queries traversing from the pattern to the data space and vice-versa. At the same time, the consistency of the results is guaranteed by the pattern-data mapping De?nition 15 Let PW the set of all possible Pattern Warehouses. A query is a function with signature PW ? PW. Given a query q and a pattern warehouse pw = \(DB,PB D?B, P?B q\(pw DB?, PB   P?B? = ?[D?1, ..., D?m], [P?C1:PT1]?. We assume that tr, tp\(tr ? R1 ? tp ? PC1 tr, tp Note that, similarly to the relational case, the result of a query is always a pattern warehouse containing just one relation and one pattern class. It is also important to point out that, in practice, even if a query always involve both the data and pattern space, operations over patterns are executed in isolation, locally at the PBMS. The reference to the underlying data is activated only on-demand \(whenever the user speci?cally requests so stored intermediate mappings or the formula approximation 5.1. Query operators In this section we introduce query operators that allow basic queries over the the PW . Assuming that DB denotes the set of all possible database instances and PB the set of all possible pattern bases, we consider the following groups of operators  Database operators: they can be applied locally to the DBMS. op : DB ? DB. We denote the set of database operators with OD  Pattern base operators: they can be applied locally to the PBMS. op : PB ? PB. We denote the set of database operators with OP  Cross-over database operators: they involve evaluation on both the DBMS and the PBMS, the result is a database. op : DB  PB ? DB. We denote the set of database operators with OCD  Cross-over pattern base operators: they involve evaluation on both the DBMS and the PBMS, the 


evaluation on both the DBMS and the PBMS, the result is a pattern base. op : DB  PB ? PB. We denote the set of database operators with OCP In the following, we present examples of the last three classes of operators \(database operators coincide with usual relational operators operators, we introduce some examples of predicates de?ned over patterns Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE 5.1.1. Pattern predicates We identify two main classes of atomic predicates: predicates over patterns and predicates over pattern components. From those atomic predicates we can then construct complex predicates. In the following, we denote pattern components by using the dot notation. For example, the measure component of a pattern p is denoted by p.Measure Predicates over pattern components. They check properties of speci?c pattern components. Let p1 and p2 be two patterns, possibly selected by some queries. The general form of a predicate over pattern components is t1?t2, where t1 and t2 are path expressions that must de?ne components of patterns p1 and p2, of compatible type and ? must be an operator, de?ned for the type of t1 and t2. For example, if t1 and t2 are integer expressions, then ? can be a disequality operator e.g. one of &lt;,&gt cases  If t1 and t2 are pattern data for patterns p1 and p2, then ? ? {=,?}. t1 = t2 is true if and only if x x ?? p1 ? x ?? p2 and t1 ? t2 is true if and only if ?x x ?? p1 ? x ?? p2  If t1 and t2 are pattern formulas for patterns p1 a n d  p 2   t h e n             t 1    t 2  i s  t r u e  i f  a n d o n l y  i f  t 1    t 2  a n d  t 1    t 2  i s  t r u e  i f  a n d  o n l y  i f  t 1 logically implies t2 Predicates over patterns. We consider the following set of predicates  Identity if they have the same PID, i.e. p1.P ID = p2.P ID  Shallow equality \(=s are shallow equal if their corresponding components \(except for the PID component i.e. p1.Structure = p2.Structure, p1.Source p2.Source, p1.Measure = p2.Measure, and p1.formula = p2.formula. Note that, to check the equality for each component pair, the basic equality operator for the speci?c component type is used  Deep equality \(=d deep equal if their corresponding data are identical, i.e., ?x x ?? p1 ? x ?? p2   S u b s u m p t i o n       A  p a t t e r n  p 1  s u b s u m e s  a  p a t t e r n  p 2   p 1    p 2   i f  t h e y  h a v e  t h e  s a m e  s t r u c ture but p2 represents a smaller set of raw data i.e. p1.Structure = p2.Structure, p1.Source p 2  S o u r c e  a n d  p 1  f o r m u l a    p 2  f o r m u l a  Complex predicates. They are de?ned by applying usual logical connectives to atomic predicates. Thus, if F1 and F2 are predicates, then F1 ? F2,F1 ? F2  F1 are predicates. We make a closed world assumption, thus the calculation of  F is always ?nite 5.1.2. Pattern base operators OP In this subsection, we introduce several operators de?ned over patterns. Some of them, like set-based operators, renaming and selection are quite close to their relational counterparts; nevertheless, some others, like join and projection have signi?cant di?erences Set-based operators. Since classes are sets, usual operators such as union, di?erence and intersection are de 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


