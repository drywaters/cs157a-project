Abstract 
204Frequent patterns mining is the focused research topic in association rule analysis. Most of the previous studies adopt Apriori-like algorithms or lattice-theoretic approaches which generate-and-test candidates. However, there are extremely invalidated candidate generations in the exponential search space. In this paper, we systematically explore the search space of frequent patterns mining and present a local frequent pruning LFP\ strategy based on local frequent property.  LFP can be used in all Apriori-like algorithms. With a little more memory overhead, proposed pruning strategy can prune invalidated 
Improving Frequent Patterns Mining by LFP XU Yusheng, MA Zhixin, CHEN Xiaoyun, LI Lian School of Information Science and Engineering Lanzhou University Lanzhou, China, 730000 e-mail:{xuyusheng, mazhx, chenxy, lil}@lzu.edu.cn Tharam S. Dillon School of Information System Curtin University Perth, Australia E-mail: tharam.dillon@cbs.curtin.edu.au 
search space and effectively decrease the total number of infrequent candidate generation. For effectiveness testing reason we optimize MAFIA and SPAM and present the improved algorithms, MAFIA+ and SPAM+. A comprehensive performance experiments study shows that LFP can improve performance by a factor of 10 on small datasets and better than 30% to 50% on reasonably large datasets 
Keywords- data mining, association rule analysis, frequent patterns mining, pruning strategy 
I I NTRODUCTION Frequent patterns mining is a task of discovering frequent patterns shared among a large number of objects in a given database [1, 2 I t h a s at t r ac te d  c o n s id e r ab le at te n tio n  f r o m  database practitioners and researches because of its broad applications in many areas such as analysis of market data discovering of Web access patterns in Web-log, extraction of Motifs from DNA sequences, etc. Frequent patterns mining is divided into two related sub-problems, which are frequent 
LFP 
ocal 
F 
So, effective pruning strategy is very important for performance optimization of mining algorithms. Can we find some effective pruning strategies that may help algorithms to generate as fewer infrequent candidates as possible? This is the motivation of this paper In this paper, by systematically explore the search space of frequent patterns mining, we present a pruning strategy 
itemsets mining and frequent sequences mining. Frequent itemsets mining is to find all itemsets that frequently occurred together, while each sequence is an ordered list of itemsets and the order is very important for mining sequential patterns. The search space of frequent patterns mining is quite challenging and its search space is extreme complex and massive. For example, with requent 
L 
 
N k 
different items there are O\(2 N potentially frequent itemsets and O\(N k potentially frequent sequences of length 
LFP LFP 
can prune invalidate search space effectively. Thus, a large number infrequent candidates need not to be generated. In order to test performance improvement, we optimize MAFIA [4 n d SPAM [3  w h ic h  a r e  tw o f as te s t al g o r i t h ms f o r  mi n i n g  frequent patterns. A comprehensive performance study shows that can improve performance of original algorithms by a factor of 10 on small datasets and better than 30% to 50% for reasonably large datasets The rest of the paper is organized as follows. Section 2 introduces the basic concepts related to the frequent patterns mining problem.  Section 3 discusses the related works. The proposed pruning strategy is discussed in section 4. A comprehensive experimental study is presented in Section 5 Finally, conclusions can be found in section 6 II PROBLEM STATEMENT Let 
roperty\. LFP can be used in all Apriori-like algorithms. With a little more memory cost 
P I={i 
X i  
is an itemset. The number of items of an itemset is called its length. An itemset with length of 
k 
where 
is a non-empty collection of items Without loss of generality, we assume that items in an itemset are sorted in lexicographic order and denoted as 
be a set of 
refer to the number of items in itemset 
A sequence 
a sequence with length 
distinct items comprising the alphabet. An itemset 
itemset. Similarly, the number of instances of items in a sequence is called the length of the sequence. Let 
is an ordered list of itemsets, denoted as 
1 i 2 203,i n  n IX i 1 i 2 203i k s X 1 X m X i k X i k 
001 
X 2  
is called a 
X i  
is a 4-sequence. The size of a sequence, denoted as 
is 
is called 
ab 
A sequence is said to be contained in another sequence 
k 
X 1 
001 002 
is a sub-itemset or subpattern of 
then 
is the number of itemsets that it contains. For example, if 
m 
then 
sequence, where 
denoted as 
 
k c X m X a X a X b X a s a s b Y 1 
a X 2  X b X b X 2 X m Y 2 
size\(s 
For example 
s = X 1 size\(s 
 Given two itemsets 
i m 
i m 
im 
001 001 001 002 
i1 i2 
 
such that 
b 
X If s 
is a sub-pattern of s and s 
Y Y Y s  Additionally, pattern 
and X 
if and only if exists is contained in s is a superpattern of s 
 
This relationship is denoted as s 
Y n i 1 i 2  1 1 2 m a b a b b a a P a X 1 X 2 203X m P b 
X then s 
i 1 i 2  n 
is a prefix sub-pattern of 
p=X 1 X 2 203X m 
X 1 X 2 203X m Y m+1 203Y n 1 
 
 A pattern can be extended into each of its super-pattern by adding items. Given a pattern 
X p 
This concatenation is called prefix pattern extensions. It can be itemset extension, denoted T h i s  w o r k  w a s  s up po r t e d  b y  N a t i o n a l  N a t u r a l S c i e n c e  F o un d a ti o n  o f  
978-1-4244-2108-4/08/$25.00  \251 2008 IEEE 
means 
X p X 
concatenates 
and an itemset 
China under grant No. 90612016 and grant No. 60473095 Corresponding author: MA Zhixin is with School of Information Science and Engineering, Lanzhou University, 730000, China 


ab ab min_sup p\220 sup\(p\ = sup\(p\220 min_sup min_sup=3 A Algorithm MAFIA and SPAM is supported by the input pattern. The support count of pattern pruning search space by comparing support of the parent \(current frequent pattern\ with that of a tail \(item selected to extend parent\ If all input patterns that support parent also support the child, this guarantees that any frequent super-pattern MFI For example, with itemset extensions Input database D Input database D or sequence extension, denoted as for frequent pattern mining consists of a collection of input patterns. Each input pattern has a unique identifier called pattern-id and the pattern. Given a pattern if it is a sub-pattern of an input pattern, pattern denoted as The support of is frequent if is frequent and there is no proper super-pattern of has 6 items, which are and of the parent which not has the child has a frequent super-pattern that contains the child. Thus, the space that has the parent but has no the child can be pruned and thus can prune out the sub-space rooted at head and I-step extension candidate items set as input parameters, SPAM generates a new sequence by either S-step or I-step. And this process recursively goes on until no more extension can be performed. When traversing the lexicographic sequence enumeration tree, SPAM uses S-step and I-step pruning strategies to optimize the performance IV LFP PRUNING STRATEGY In this section, we introduce the local frequent property pruning strategy, and demonstrate how to optimize MAFIA and SPAM with this strategy abefg ab a, b, c, d, e a:4, b:6, c:4, d:4, e:5, ab:4, ad:3, ae:4, bc:4, bd:4, be:5 ce:3, de:3, abd:3, abe:3, ade:3, bce:3, bde:3, abde:3 bce:3, abde:3 with the same support, i.e., not exists can extended into  with a specified minimum support threshold p 978-1-4244-2108-4/08/$25.00  \251 2008 IEEE p p 2 LFP MHUT 002 A pattern which is used to mining frequent itemsets. Transaction input database The complete set of all frequent itemset consists of 19 itemsets. All closed frequent itemsets each candidate item is used to extend the node. If a new frequent itemset is gotten, MAFIA recursively goes on this depth first traversal. During the frequent itemsets enumeration process, MAFIA adopts strategy PEP, FHUT and MFUT to prune out invalidate candidates SPAM algorithm is based on the lexicographic sequence tree and can make either depth or width first traversal. Taking current frequent sequent node is maximal if it is not a sub-pattern of any other frequent pattern. The \(closed/maximal frequent patterns mining problem is to find a complete set of closed/maximal\ frequent patterns for an input database And all maximal frequent itemsets that support Suppose minimal threshold X p i X p s ab i d c a ab i s s D p id p p p sup\(p D p p D p p sup\(p p p p p D D i D i f p C s k  S n I n Similarly, with sequence extension path  and the extension path is  The database  For example, Fig.1 shows an input databases such that  is the total number of input patterns in database and a frequent closed pattern. A frequent pattern In lexicographic enumeration space [4  e a c h  n o d e  h a s  a corresponding pattern. A pattern is generated from its parent by Figure 1. Example input pattern database  is greater than or equal to states that any sub-pattern of a frequent pattern is frequent, too. And any super-pattern of an infrequent pattern is also infrequent. This is a basic pruning strategy, which is used in almost all methods S-step extension candidate items set that contain is similar with HUT. It says that if head unions tail is a sub-pattern of a known frequent pattern, any sub-pattern of HUT is also frequent. Thus this sub-space can pruned out All pruning strategies can improve performance effectively Unfortunately, they are complex and couple tightly with their enumerating methods and data structures, and unlike our itemset AP b  c  d 003  1,    a  b  d  e  f 2,    b  c   e 3,    a  b  d  e 4,    a  b  c  e  f 5,    a  b  c  d  e 6 can also be extended into a sequence is the percentage of patterns in denoted as   bc bc PEP FHUT FCI 1,  a i s b c d e 2,  a d 3,  a b e 4,  dc e 5,  b d e MAFIA n d S P A M 3  a r e t w o fa s t e s t  a l g o r i t h m s  t o mine frequent patterns, which are taken as examples for demonstrating the proposed pruning strategy and comparing performance. This section describes them in more details MAFIA was proposed by J. Ayres, et al. It can find closed maximal or frequent itemsets in a given input transaction database. From current node HUT A Pattern  extension item set  FI cd e i f i g a we say that a pattern strategy, they can not be shared by several algorithms as b:6,bc:4 bd:4, be:5, abe:4, bce:3, abde:3 n=\(s 1  we call   states that if a tail extends current frequent prefix and get a new frequent pattern, i.e., head unions tail is frequent we never have to explore any sub-pattern of  III RELATED WORKS Since the frequent patterns mining problem was presented by R. Agrawal, et al [1, 2 man y al g o r ith m s  h a v e b een  proposed, such as APRIORI [1  C H A RM  1 0 F P G r ow t h 5   MAFIA [4 S P A D E 9  S P A M  3 G S P  7  Cl oS p a n 8  e t c   Most of these algorithms adopt an Apriori-like method generates a candidate pattern by extending currently frequent pattern and then test the candidate. During this process, many infrequent patterns are generated. In order to improve the performance, several pruning strategies are proposed, which include Apriori property \(AP\[1  P a r e nt  E qu i v a l e nc e  Pruning \(PEP\ [4  f r e que nt  he a d u ni o n  t a i l  F H U T     head union tail is a sub-pattern of a maximal frequent pattern MHUT  a nd s o o n  T h e y a r e de s c r i b e d a s f o l l o w s   min_sup Without loss of generality, we use the support count for describing the pruning strategy while using the support to present the experimental results in the remaining of this paper. Given a user-specified threshold 


and its candidate extension item set can be pruned out. The reason is that any pattern must be sub-set of in Fig.1 and let 2. Suppose the current node is Without pruning, possible sequence extensions are  c\, \(a abbr. LFP\: Given a frequent pattern of ext ia pattern C.pattern  must not be frequent Proof: Suppose pattern  is frequent. According to Apriori property, any sub-pattern of  must be frequent. Specially, sub-pattern  is infrequent i s 1  p C a p b C b a b p p b p p b p p b a b a b p p p C a p b b a b b p C D s   b a C i C Use LFP to improve MAFIA and SPAM k n s k  S n 220 i S n 220 s 1 s k i s 1 s k i I n 220 C In i I n 220 s 1 s k i i s 1 s k i C C C i i C i C C a C i C Lexicographic enumeration tree describes the search space of Apriori-like methods. If the number of invalidate nodes is decreased, the performance of these methods may be effectively improved. Local frequent property \(LFP\ is such a strategy to prune out infrequent nodes  in i-extension 8\       MAFIA Pseudo-code of MAFIA+ al ure 2 Pseudo-code of LFP strate in Stemp 7\     DFS-SPAM is frequent HUT ia, C Sn s-extension C = C U {i If If If If    i i  For For For For For b b b b  Itmp Itmp b, c, d, e     LFP i 6  5\       IsHUT = whether  3 IsHUT IsHUT Do i i C. freq_candidate p p 3 and is infrequent, sequence 1\ DFS-SPAM+\(node U U LFP\(ia is a leaf and For each If Return Return Ci Then is a candidate. If LFP LFP For each item min_sup each each LFP Fi is infrequent, pattern END End ext pattern to MFI 13 Local property pruning strategy items in i 12 is the last item of pattern in whose has IsHUT and any pattern and and item and its candidate set is   4\       If  10 C and all extensions are frequent 10\           Stop search and go back up subtree 11 of MAFIA 1 LFP 1\ MAFIA  must be infrequent. Additionally, for any pattern orithm Fi 978-1-4244-2108-4/08/$25.00  \251 2008 IEEE is the first item in the tail 6 of DFS-SPAM a ext i pattern candidate candidate C.freq_candidate newNode B Proposed local frequent property strategy \(LFP p p b b\, \(a d b b ure 3 Pseudo-code of MAFIA+ and SPAM i-extension is not in  3 s frequent 5   8  9  s frequent 11  14 b\Pseudo-code of SPAM+ algorithm  9 HUT is in MFI one pattern extension. If a pattern is infrequent, all patterns in its sub-space must be infrequent, too. Thus, only extensions that start from frequent nodes may generate frequent patterns Each frequent pattern has a candidate extension item set, which includes any items that may generate a new frequent pattern by pattern extension. In all Apriori-like algorithms, if some invalidated items in candidate sets can be pruned out, their performance can be effectively improved Given a frequent pattern Suppose item is infrequent. Thus pattern and its pattern-extension candidate set patterns \(k>2\. Frequent 2-patterns are gotten by one step of frequent pattern-extensions and stored in a list. In the second step, MAFIA+ and SPAM+ recursively enumerating candidates by sharing frequent 2-pattern list Note that frequent 2-patterns, which are used by LFP strategy, are not generated in above optimized algorithms. Each 2-pattern can be generated by one pattern-extension. Having generated all frequent 2-patterns, MAFIA+ and SPAM+ start their recursive searching procedures from level two nodes in lexicographic enumeration tree, while MAFIA and SPAM start from root node \(level 0 V PERFORMANCE STUDY To test the performance improvement of strategy, an extensive set of experiments were performed upon an Intel Pentium 4 CPU 1.6GHz PC with 256MB main memory running Microsoft Windows 2003 server. Source codes of MAFIA and SPAM were downloaded from Himalaya Data Mining Tools \(http://sourceforge.net/projects/himalaya-tools and modified with proposed strategy. Same as MAFIA and SPAM, all synthetic datasets are downloaded from FIMI\22004 dataset repository or generated by using the IBM AssocGen program. The experiments include two parts: performance  p\220 LEMMA 1 a a b   C Sn C In is frequent too. This conflicts with that is the last item of is infrequent, item must be infrequent and can be pruned out Fig.2 shows pseudo-code of is the item to prune extension candidate set is the pattern extension type \(itemset extension or sequence extension\. Line 3 ~ 5 delete all candidate items, with which frequent 2-pattern can not be generated, from candidate set In order to test performance improvement of local frequent pruning strategy, we optimize algorithms MAFIA and SPAM Fig.3 describes the optimized algorithms, named MAFIA+ and SPAM+, which also show that strategy LFP can be shared by all Apriori-like methods. Comparing with MAFIA and SPAM there are only three additional lines of code \(line 7 in MAFIA line 2 and 8 in SPAM+\, which call LFP strategy. All other codes are as same as that in MAFIA and SPAM respectively Both MAFIA+ and SPAM+ are all divided into two steps generating frequent 2-patterns and recursively generating frequent greater than Ci Stmp Stmp Stmp Stmp newNode Stmp Itmp Itmp 7\       newNode  4  5  MFI MFI MFI in items in in greater than in in  Item  For example, consider the input database   if each each each item as a prefix, the candidate set  e  Because    2   13\   DFS-SPAMSPIP     2  3  4    12\           Add    a g g   2 6  g gy i strategy. Parameter 


B Performance comparison with SPAM by comparing it with other strategies adopted by MAFIA, which are PEP, FHUT and MFUT. The chosen datasets include T40I10D100K and connect4. T40I10D100K is a sparse dataset. Connect4 is a dense datasets and has much more long maximal patterns. For each test, only one strategy is effective while others are turned off. In this way we can investigate the impact of different strategies on the running time of algorithm MAFIA. Fig.4 shows the experiment results which clearly depicts that on sparse datasets strategy can prune out infrequent pattern-extensions effectively. When datasets are dense, almost all 2-itemsets are frequent ones, only a few of invalidate itemsets can be pruned We next study the impact of In this section we study performance improvement of strategy strategy performs effectively in small databases while not so effectively in large ones VI CONCLUSION With systemically exploring the searching space of frequent pattern mining, a new pruning strategy is proposed, which is local frequent property ies in MAFIA Dataset D10C10T5S8 c\ Dataset D15C15T15S15   is better than FHUT and MHUT, performance increased by a factor of 30%. On dense datasets on mining frequent sequences. Firstly, a set of experiments were performed for studying the performance of the SPAM+ by comparing it with SPAM. The generated datasets include one small database Fig.5 \(a\\ two medium-sized ones \(Fig.5 \(b\ and \(c\ and a large one \(Fig.5 \(d\. The experiments show that SPAM outperforms SPAM by a factor of about 10 on small datasets Although, there is no distinct magnitude difference between SPAM+ and SPAM on reasonably large datasets, the running time of SPAM+ is decreased by 30% to 50%. In small sequence databases, frequent 2-sequences are only a small portion. When SPAM+ traverses the enumeration space, a mass of infrequent extensions are pruned. On the other side, in large database, a majority of 2-sequences are frequent ones. As SPAM+ traverses the enumeration space, only few branches are pruned out. This is the primary reason why A Performance comparison with strategies in MAFIA a a p b  p K g p g b comparison with FHUT, MHUT and PEP upon primary MAFIA algorithm for mining frequent itemsets, and performance comparing with and without LFP strategy upon SPAM for mining frequent sequences strategy can improve performance of SPAM by a factor of 10 on small sparse datasets and better than 30% to 50% on reasonably large datasets R EFERENCES 1 R  Ag r a wa l   T   I m i e l i n sk i  a n d A S wa m i  M i n i n g A s so c i a t i o n  R u l e s  between Sets of Items in Large Databases, in Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, Vol 22\(2\, pp.207-216, 1993 2 R A g r a w a l R   S r ik a n t M in in g S e q u e n tia l  P a tt e r n s  1 1 th I n te r n a ti o n a l  Conference on Data Engineering \(ICDE'95\, Volume 6, pp.3\20514, 1995 3 J   A y r e s  J  G e h r k e  T   Y i u a n d J   F l a n n i c k  Seq uen t i a l PA t t e r n Mi ni ng using A Bitmap Representation, In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 429-435, 2002 4 D  B u r d i c k   M  C a l i ml i m  J Geh r k e  M A F I A   A m a x i m a l  f r e q u e n t  itemset algorithm for transactional databases. In Proc. of 17th Int\220l Conf on Data Engineering, pp. 443-452, 2001 5 J  H a n  J   P e i  a n d Y  Y i n Mi ni ng  f r eq uen t  p a t t e r ns  w i t h o u t  c a nd i d at e generation, in 2000 ACM SIGMOD International Conference on Management of Data \(SIGMOD'2000\, pp.1-12, 2000 6 R  R y m o n   S e ar ch t h r o ugh  s y s t em at i c  s e t  e n u m e r at i o n  I n P r oc  of 3r d Int\220l Conf. on Principles of Knowledge Representation and Reasoning pp. 539-550, 1992 7 R. S r ik a n t R A g r a w a l M i n i n g S e q u e n tia l P a tt e r n s G e n e r a li z a ti o n s a n d  Performance Improvements, In Proc. 5th Int. Conf. Extending Database Technology \(EDBT'96\, Volume 1057, pp.3-17, 1996 8 X  Y a n J  H a n a n d R   A f s ha r  C l o S pa n  Mi ni ng cl o s e d s e qu en t i a l  patterns in large datasets. In SDM'03, pp.166-177, 2003 9 M  J  Z a k i   S P A D E  A n E f f i ci en t A l gor i t hm f or Mi ni ng Fr equ e nt  Sequences. Machine Learning, Volume 0, pp. 1-31, 2000 1 M  J  Z aki and C  H s i a o  E f f i ci en t al g o r i t h m  f o r  m i n i ng c l os ed i t ems e t s  and their lattice structure. IEEE Transactions on Knowledge and Data Engineering, Vol. 17\(4\, pp. 462-478, 2005 is similar as FHUT and MHUT, but not as fast as PEP. The reasons are on sparse datasets, frequent 2-patterns are only a small percent of all 2itemsets, thus and can used in all Apriori-like algorithms and lattice-theoretic approaches. With a little more memory overhead, proposed pruning strategy can prune out invalidated search space and effectively decrease the total number of infrequent candidate generating. For effectiveness testing reason, we optimized MAFIA and SPAM by using proposed pruning strategy and presented the improved algorithms, MAFIA+ and SPAM+. A comprehensive performance experiments study shows that arison with strate arison on connect4 4 ure 4 Performance com 978-1-4244-2108-4/08/$25.00  \251 2008 IEEE Com Fi Figure 5. Performance comparison on different dataset Dataset D7C7T7S7 d\ Dataset D18C18T18S18 LFP LFP LFP LFP LFP LFP LFP LFP Com arison on T40I10D100 


A nd ers o n s l ab el i ng  T he weight ed  average f o r t h e seco nd cl as s i s  43  50  0 53 3  7 50  0 37 5 0 51 0 S i m i l ar l y  we s ee t h at Cl u s t er s 3 a n d 4 y i el d An der s on  s t hi r d c l as s of 50 v ec t or s w i t h  we i g h t ed av er ag e  14 50 0 944 36 50 0 748  0 8 0 3 se e the las t p a ra g ra p h o f S e c tio n 2  Tabl e 2 R es ul t s of R un 2 on t he i ri s dat a C lus te r 1  50 v ec t or s n u mber ed 1 4  7 10  13 16  19 22 25  28 31  34 37  40 43 46  49 52  55 58  61 64  67 70 73  76 79  82 85  88 91 94  97 100  103 106  109 112 115 118  121 124  127 130  133 136  139 142  145 148 C lus te r 2  43 v ec t or s n u mber ed 2  5 8  11 14  17 20 26 29  35 41  44 47  50 56 59  62 65  68 71  74 77  80 83 86  92 98  101 104  107 110 113  116 119  122 125128 134  137 140 143  146 149 C lus te r 3  14 v ec t or s n u mber ed 3  30 45  48 57  63 108 111  123 126  132 135  138 147 C lus te r 4  36 v ec t or s n u mber ed 6  9 12  15 18  21 24 27 33  36 39  42 51  54 60 66  69 72  75 78  81 84  87 90 93  96 99  102 105  114 117 120  129 141  144 150 C lus te r 5  7 v ec t or s n u mber ed 23 32  38 53  89 95  131 Cl u s t er Cen t er  1   0 060833 0 0 Cl u s t er Cen t er  2   0 532946 0 5 Cl u s t er Cen t er  3   0 943452 1 0 Cl u s t er Cen t er  4   0 747685 1 0 Cl u s t er Cen t er  5   0 375000 0 5 N ow we h av e t h e f u zz i f i ed s t an dar di ze d c en t er s of 0 061 ro un d ed  0 51 0  an d 0 80 3 U p o n p l ac i ng F S M F s l i ke t ho se o f F i gu re 5 at t he se va l ue s we ca n use t he 3 rules 45  A 0 061   A 0 0 45  A 0 510   A 0 5 45  A 0 803   A 1 0 5 whe r e Fea t ur e A i s t h e cl as s  1 2 or 3 a s l abe l ed by A nder s on but s t andar di zed h er e I n t he s ame way  Feat ur e 3 is a l s o a cor r ect cl as s i f i er acc ordi ng t o A nder s on s l abel i ng  b ut i t i s m uch bet t er t han F ea t ure 1 an d also b ett er than F ea ture 4  In fact F ea t ure s 3 and 4 are suf f i ci ent t o cl ass i f y t he i ri s d at a acco rding t o t he l abel s gi ven by Anders on H owever   we nee d t o m ent i on t hat t he l abel s t hems el ves wer e us ed i n t he cl as s i f i cat i on an d m ay b i as i t  T hi s i s l i ke t he p rob l em o f t rai ni ng a neural net wo rk o r a sup p o rt ve cto r m ac hine  o n da t a t he y l ea rn th e la b els w h ic h m a y b e inc o rr e c t o r n o isy T o pr ev en t t h e l abe l s f r om i n f l uenc i n g t h e abi l i t y of F ea t ure  3  t o  cluster co rrec t l y acc o rd i ng t o A nd erso n’s l ab els w e clustered the iri s d atase t o n o nly F ea t ure 3 T he se re sult s i n T ab l e 3  co i nc i d ed p erfec t l y wit h A nd erso n’s classe s Tabl e 3 R es ul t s of R un 3 on t he i ri s dat a Cl u s t er 1 50 v ec t or s nu mber ed 1 4 7 10 13 16 19 22  25 28  31 34  37 40  43 46 49  52 55 5 8  6 1 64 67 70  73 76  79 82  85 88  91 94 9 7  1 0 0 103  106  109 112 115 118  121 124  127 130  133 136  139 142 145  148 Cl u s t er 2 39 v ec t or s n u mber ed 2  5 8  14  17  20 21 26  35 41  42 47  5 0  5 6  60 62 66  68 71  72 74  77 80 81 83 84  86 104  107 110  113 117  122 125  134 137 140  141 143 Cl u st er 3 23 v ec t or s n u mber ed 3 9 12 15 24 27 30 39 51 63 75 78  87 90  93 99  105 108  111 114  123 132  135 Cl u s t er 4 14 v ec t or s n u mber ed 6 33 36  45 48  101 102 120  126 129  138 144  147 150 Cl u s t er 5 13 v ec t or s n u mber ed 11  29  38  53  59  65 92 95 98  116 119  128 149 Cl u s t er 6 5 v ec t or s n u mber ed 18  54 57  69 96 Cl u s t er 7 6 v ec t or s n u mber ed 23  32 44  89 131  146 From T abl e 3 w e s ee t hat  C l us t er  1 i s  C l as s 1 as l abel ed b y A nd ers o n but we have no t us ed l ab el s i n t hi s run C l ust ers 2 6 7 provi de t he 50 f eat ur e v ect ors l abel ed as C l ass 2  w hil e C l usters 3  4 an d 5 c o m p ris e A nd erso n’s C l ass 35 3 T h e c o n fid e n c e he re is 1 0 0  tha t  A A   W e a l so 25 mad e r un s f or t he r ul e  A A  b u t with p o o r re su lts 4. Runs on the Wisconsin B reast Cancer Data T h i s da t a ca n  al s o be dow n l oade d f r om t h e U n i v er s i t y of C a lif o rn ia Irv ine  re p o sito ry [1 6  O u r v e rs io n o f t h e fi le is n ame d w isc 9 6 9 9 d ta becau se t her e a r e 699 ve ct ors  E ach vec t or c ont ai ns i n t he f i r s t f i el d an i dent i f i cat i on numb er t hat we don t us e Af t er tha t t her e a r e 9 f eat ur e v al ues f ol l owed b y the lab e l f e a tur e o f 2 f o r beni gn or 4 f o r f o r mal i g n a nt  T h e fea tur e va lue s w e re e n te re d b y hu m a n s d u rin g b io p sy w h e r e a l l f e a t u r e v a l u e s a r e f r om  0 t o 10  a n d d e s c r i be s om e f eat ure o f  t he cel l  s uch as si ze  sy m m et ry   e tc  here appe ar s  t o  b e  a  l o t  o f  n o i s e  i n  t h e  f e a t u r e  v a l u e s u pon e x a m in a ti o n b e c a u se fo r a lab e l o f 4  t he val ues of a g i ven fea tur e va ry fro m lo w to the hig h va lue o f 10  T h er e ar e 241 r ec or ds  l abe l ed as  4 a n d t h e r ema i n i n g 458 are l ab eled as 2  A wort hy goal i s  t o f i nd one or t wo f eat ur es tha t co rr e la te hig h ly with the lab e ls. T o th is e n d w e a sso c ia te Feat ur es 2 and 10 an d che ck t he cl us t er s produced t o s ee ho w the y m atch u p w i t h t he l ab els T ab l e 4 sho w s t he result s 21 0 T a ble 4 R esul ts of A  A  on t he w i s c9699 dat a 2 2 10 10  A  c A  c Cl u s t er Cen t er  1   0 009859 0 000000 Cl u s t er Cen t er  2   0 250000 0 000000 Cl u s t er Cen t er  3   0 777778 0 000000 Cl u s t er Cen t er  4   0 992284 1 000000 Cl u s t er Cen t er  5   0 307256 1 000000 Cl u s t er Cen t er  6   0 671362 1 000000 Cl u s t er Cen t er  7   0 555556 0 000000 548 


21 0 T ab l e 4 s ho ws t hat upo n ass o ci at i ng A w ith A we g et C lus te rs 4 5 a n d 6 tha t ha v e the o u tp u t l a b e l o f 4 s t and ardized to 1   B ecause we used t he l ab el s i n cl ust eri ng t he res ul t s were p erf ect  we checke d each vect o r of C l ust ers 4 5 a n d 6 a n d t h ey w er e t h e ex ac t 241 c l u s t er s l abe l ed 4 2 H ow e v e r  t he A va l ues ar e i ncons i s t ent  ex cept f or Cl us t er 4  who se cent er 0 9 92 i s o ut o f t he ran ge o f t he C l ass 1 C lus te rs  1 2 3 an d 7 w ith c o n fli c ting c e n te rs o f 0 00986 0 2500 0 77 7 8  and 0 5556  O nly C l uster 4 gives a 2 cons i s t ent r es ul t   s o we coul d f orm t he r ul e  A 0  9 9 2   10 2 10  A 1 or  A i s VE RYH I G H    A M ALI G NA NT  T hi s s t udy ha s s hown t he problem wi t h t hi s dat a O t her co m b i na t i o ns also yi eld so m e e rrati c re sult s 5 Ana lys is a nd C onc lusi ons O ur method us es a Q xQ f uzzy m em b ers hi p m a tri x f or med by a k er n el ma ppi n g of t h e Q f ea t u r e vect o r s of di mens i on N   whe r e N  Q   t o c orr el at e vec t ors  of f ea t ures qr to b e c o rr e la te d A n e n try m pr ov i des t h e f u zz y a s s oci at i on o f f ea t ure ve cto rs x a n d x so th a t a v a lue c lo se to un ity q r im p lies th e y b e lo n g to th e sa m e c lus te r \(w ith the fuz z y tru th qr m   B y m e a n s o f a d ju sting  a  q u ite no n se n sitiv e th re sh o ld w e c a n o b ta in a p a rtiti o n ing b y th e m a trix o f t h e ve c to rs into d isjo i nt cl ust ers  or we can no t us e a t hres ho l d and ob t ain f uz zy m em b ersh i p s fro m wh i ch t o p i ck t he asso ciati o ns Af t er w e obt ai n c l u s t er c en t er s we t h en pos i t i on FSM Fs on su c h c e n te rs to us e fo r fuz z y ru le s W e an al y zed t h e i r i s da t a v i a ou r me t h odol og y t o s h ow tha t F e a tur e s 3  a n d  4 c a n b e us e d to g e the r, o r se p a ra te ly to cl ust er t he i ri s d at a i nt o t he cl ass es desi gnat ed by A nd ers o n m at ches And ers o n s l ab el i ng co m p l et el y  W e al so used o ur a lg o rithm to sh o w the inc o n siste n c ie s o f t h e W isc o n sin b reast cancer da t a B ut w e al so showe d that a ve ry hi gh val ue of Fe at u r e al on e 2 i s ef f ec t i v e i n det ec t i n g t h e mal i g n an cy of a c e ll T h e pe r f or ma n c e w a s e x c e l l e n t f or t h e h i g h c on f i de nce ru le s fo r in d ic a ting the iris c la ss o r fo r im p lying m a lign a n c y a j u d ged by t he o ri gi nal l ab el i ng \(of whi ch o ne sho u ld al wa y s  al l ow f or  er r or    Fu t u r e wor k  wi l l  bot h  r ef i ne t he m et ho d and t he C   p ro gram s we wro t e t o i m p l em ent t he algo rit hm  It w i l l also t est t he use o f m ult i p l e a ntec ed en t s 6. Refere nces  R. A g ra w al T Im ie lins k i a nd A Sw am i M ining a ss oci at ion rules betw een sets of item s in large databases Proc. ACM SIG M OD Int C onf M anagem ent D ata 1993 207 216   E. A nd erson, “The irises of the G aspe peninsu la Bull  American Iris Society 59 1935 2 5  J C B ez dek  Patt ern Recogn itio n with Fuzzy Objective Fun ction Algo rithms Ple num Pres s N ew Y ork 1981  4  U  F a yya d  G  P i a t e t s k yS h a p i r o  P  S myt h a n d R  U t h u r u s a my Adv ance s in K nowl edge D is cov er y and D ata M ining  MI T P ress Ca m bridg e MA 1996   Hichem Frigu i  Mem bership Map: d ata transf orma tion based on gra nu lation and fuzz y m em bership ag gre gaton IEE E Tr ans Fuzzy Systems 14 6 2006 885 896  6  A Gy en esei A fuzzy app ro ach for min i n g qu an ti ta ti ve association ru les Acta. Cybern  15 2 2001 305 320  P. Ha je k a nd T H av ra nek  M ec haniz ing H y p othe si s For mat ion Mathe m at i c al F ou nd atio ns for a General Th eory S p ri n gerV erl ag Be rli n, 1987  J H an and M. K am ber   Dat a M ining  Conc ept s and Techni qu es Mor g an Ka uf m ann Sa n Ma te o, C A 2001  Y C H u De te rm ining m em ber shi p f unct ions a nd m inim um f uzz y s u p p o r t i n f i n d i n g f u z z y a s s o c i a t i o n r u l e s f o r c l a s s i f i c a t i on pro blem s Kno wledge-Ba sed Systems 19 2006 57 66  C L K ar r a nd E J. Gen try  Fuz zy c ontrol of pH us ing genetic algorithms  IEEE Tran s. F uzzy Systems  1  1993 46 53  11  C  M  K u ok, A W.-C. Fu and M H. W on g, “Mining f uzzy association ru les in databases ACM SIGMOD Rec 27 1 1998 41 46  12  C G. Loon ey  Interactive clu stering and m erging w ith a new f uzzy expected va lue Patt er n Re cogni tion 35  2002 2413 2423  C G L ooney  Fuz zy c onnec tiv ity c lus te ring w ith ra dia l ba si s k er nel f unct ions  pr epr int t o a ppea r i n Patt er n Re cogni tion   H Ve rli nde M De Coc k a nd R Boute  Fuz zy v er sus quant ita tiv e as soc ia tions rul es a f ai r d at adriv en com par is on IEE E Tr ans SM CB 36 3 2006 679 684  15  L A Zadeh, “Computin g w ith words IEEE Tra ns. Fu zzy Systems 4 1996 103 111  1 6  h t t p    www.c s u ci  ed u   mle ar n  M LR ep o si t o r y.h t ml  U n i v  o f C a l i fo r n i a  I r v i n e M a c h i n e Le a r n i n g R e p o s i t o r y 549 


Used-for references in the LCSH into holonym/meronym relations in our WKB  In the experiments we assume that each topic comes from an individual user We attempt to evaluate our model in an environment that covers great range of topics However it is not realistic to expect a participant to hold such great range of topics in personal interests Thus for the 50 experimental topics we assume each one coming from an individual user and learn her his personalized ontology An LIR is collected through searching the subject catalogue of Queensland University of Technology QUT Library 3 by using the title of a topic Librarians have assigned title table of content summary and a list of subjects to each information item e.g a book stored in QUT library The assigned subjects are treated as the tags in Web documents that cite the knowledge in the WKB  In order to simplify the experiments we only use the librarian summarized information title table of content and summary to represent an instance in an LIR  All these information can be downloaded from QUT's Web site and are available to the public Once the WKB and an LIR are ready an ontology is learned as described in Section 3.3.1 and personalized as in Section 3.3.2 The user con\002dence rates on the subjects are speci\002ed as in Section 3.3.3 A document d i in the training set is then generated by an instance i  and its support value is determined by support  d i   X s 2 021  i  s 2S sup  s Q  14 where s 2 S in O  Q  are as de\002ned in De\002nition 5 As sup  s Q   0 for s 2 S 000 according to Eq 11 the documents with support  d   0 go to D 000  whereas those with support  d   0 go to D   4.4 Performance Measures The performance of the experimental models are measured by three methods the precision averages at eleven standard recall levels 11SPR the mean average precision MAP and the F 1 Measure They are all based on precision and recall the modern IR evaluation methods The 11SPR is reported suitable for information gathering and is used in TREC evaluations as a performance measuring standard An 11SPR v alue is computed by summing the interpolated precisions at the speci\002ed recall cutoff and then dividing by the number of topics P N i 1 precision 025 N  025  f 0  0  0  1  0  2      1  0 g  15 N is the number of topics and 025 are the cutoff points where the precisions are interpolated At each 025 point an aver3 http://library.qut.edu.au Figure 2 Experimental 11SPR Results age precision value over N topics is calculated These average precisions then link to a curve describing the recallprecision performance The MAP is a stable and discriminating choice in information gathering evaluations and is recommended for measuring general-purpose information gathering methods The average precision for each topic is the mean of the precision obtained after each relevant document is retrieved The MAP for the 50 experimental topics is then the mean of the average precision scores of each of the individual topics in the experiments The MAP re\003ects the performance in a non-interpolated recall-precision fashion F 1 Measure is also well accepted by the information gathering community which is calculated by F 1  2 002 precision 002 recall precision  recall  16 Precision and recall are evenly weighted in F 1 Measure For each topic the macro F 1 Measure averages the precision and recall and then calculates F 1 Measure whereas the micro F 1 Measure calculates the F 1 Measure for each returned result and then averages the F 1 Measure values The greater F 1 values indicate the better performance 5 Results and Discussions The experiments attempt to evaluate our proposed model by comparing to an implementation of mental model We expect that the ONTO model can achieve at least the close performance to the TREC model The experimental 11SPR results are illustrated in Fig 2 At recall point 0.3 the TREC model slightly outperformed the ONTO model but at 0.5 and 0.6 the ONTO model achieved better results than the TREC model subtly At all other points their 11SPR results are just the same For the MAP results shown on Table 1 the ONTO model achieved 0.284 which is just 0.006 below the TREC model 2 
512 
516 


TREC ONTO p-value Macro-FM 0.388 0.386 0.862 Micro-FM 0.356 0.355 0.896 MAP 0.290 0.284 0.484 Table 1 Other Experimental results downgrade For the average macroand microF 1 Measures also shown on Table 1 the TREC model only outperformed the ONTO model by 0.002 0.5 in macro F 1 and 0.001 0.2 in micro F 1  The two models achieved almost the same performance The evaluation result is promising The statistical test is also performed on the experimental results in order to analyze the evaluation's reliability As suggested by we use the Student's Paired T-Test for the signi\002cance test The null hypothesis in our T-Test is that no difference exists in two comparing models When two tests produce substantially low p-value usually  0.05 the null hypothesis can be rejected In contrast when two tests produce high p-value usually  0.1 there is not or just little practical difference between two models The T-Test results are also presented on Table 1 The pvalue s show that there is no evidence of signi\002cant difference between two experimental models as the produced pvalue s are quite high  p-value 0.484\(MAP 0.862\(macroFM and 0.896\(micro-FM far greater than 0.1 Thus we can conclude that in terms of statistics our proposed model has the same performance as the golden TREC model and the evaluation result is reliable The advantage of the TREC model is that the experimental topics and the training sets are generated by the same linguists manually They as users perfectly know their information needs and what they are looking for in the training sets Therefore it is reasonable that the TREC model performed better than the ONTO model as we cannot expect that a computational model could outperform a such perfect manual model However the knowledge contained in TREC model's training sets is well formed for human beings to understand but not for computers The contained knowledge is not mathematically formalized and speci\002ed The ONTO model on the other hand formally speci\002es the user background knowledge and the related semantic relations using the world knowledge base and local instance repositories The mathematic formalizations are ideal for computers to understand This leverages the performance of the ONTO model As a result as shown on Fig 2 and Table 1 the ONTO model achieved almost the same performance as that of the TREC model 6 Conclusions In this paper an ontology-based knowledge IR framework is proposed aiming to discover a user's background knowledge to improve IR performance The framework consists of a user's mental model a querying model a computer model and an ontology model A world knowledge base is used by the computer model to construct an ontology to simulate a user's mental model and the ontology is personalized by using the user's local instance repository The semantic relations of hypernym/hyponym holonym/meronym and synonym are speci\002ed in the ontology model The framework is successfully evaluated by comparing to a manual user model The ontology-based framework is a novel contribution to knowledge engineering and Web information retrieval References   C Buckley and E M Voorhees Evaluating evaluation measure stability In Proc of SIGIR 00  pages 33–40 2000   R M Colomb Information Spaces The Architecture of Cyberspace  Springer 2002   D Dou G Frishkoff J Rong R Frank A Malony and D Tucker Development of neuroelectromagnetic ontologies\(NEMO a framework for mining brainwave ontologies In Proc of KDD 07  pages 270–279 2007   S Gauch J Chaffee and A Pretschner Ontology-based personalized search and browsing Web Intelligence and Agent Systems  1\(3-4 2003   X Jiang and A.-H Tan Mining ontological knowledge from domain-speci\002c text documents In Proc of ICDM 05  pages 665–668 2005   J D King Y Li X Tao and R Nayak Mining World Knowledge for Analysis of Search Engine Content Web Intelligence and Agent Systems  5\(3 2007   D D Lewis Y Yang T G Rose and F Li RCV1 A new benchmark collection for text categorization research Journal of Machine Learning Research  5:361–397 2004   Y Li and N Zhong Mining Ontology for Automatically Acquiring Web User Information Needs IEEE Transactions on Knowledge and Data Engineering  18\(4 2006   H Liu and P Singh ConceptNet a practical commonsense reasoning toolkit BT Technology  22\(4 2004   A D Maedche Ontology Learning for the Semantic Web  Kluwer Academic Publisher 2002   S E Robertson and I Soboroff The TREC 2002 002ltering track report In Text REtrieval Conference  2002   M D Smucker J Allan and B Carterette A Comparison of Statistical Signi\002cance Tests for Information Retrieval Evaluation In Proc of CIKM'07  pages 623–632 2007   X Tao Y Li and R Nayak A knowledge retrieval model using ontology mining and user pro\002ling Integrated Computer-Aided Engineering  15\(4 2008   X Tao Y Li N Zhong and R Nayak Ontology mining for personalzied web information gathering In Proc of WI 07  pages 351–358 2007   T Tran P Cimiano S Rudolph and R Studer Ontologybased interpretation of keywords for semantic search In Proc of the 6th ICSW  pages 523–536 2007   Y Y Yao Y Zeng N Zhong and X Huang Knowedge retrieval KR In Proc of WI 07  pages 729–735 2007 
513 
517 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


