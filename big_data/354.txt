OBTAINING BEST PARAMETER VALUES FOR ACCURATE CLASSIFICATION Frans Coenen and Paul Leng Department of Computer Science The University of Liverpool Liverpool L69 3BX frans,phl csc.liv.ac.uk Abstract In this paper we examine the effect that the choice of support and condence thresholds has on the accuracy of classiers obtained by Classication Association Rule Mining We show that accuracy can almost always be improved by a suitable choice of threshold values and we describe a method for nding the best values We present results that demonstrate this approach can obtain higher accuracy 
without the need for coverage analysis of the training data Keywords  Classication Association Rule Mining 1 INTRODUCTION A method of classication that has attracted recent attention is to make use of Association Rule Mining ARM techniques to dene Classication Rules Examples of Classication Association Rule Mining CARM methods include PRM and CPAR 4 C M AR 2  a nd C B A  3   I n general CARM algorithms begin by generating all rules that satisfy threshold values of support the number of instances in the training data for which the rule is found to apply and condence 
the ratio of its support to the total number of instances of the rules antecedent The candidate rules are then pruned and ordered using other techniques CBA 3 generates rules wh ich are prioritised using condence support and rule-length then pruned by coverage analysis  in which each record in the training set is examined to identify a rule that classies it correctly The CMAR algorithm 2 has a s imilar general s t ructure to C B A b u t uses a different coverage procedure that may generate more than one rule for each case The cost of coverage analysis especially when dealing with large data sets with many attributes motivated us to consider whether it is possible to generate an accurate set of 
Classication Rules directly from an ARM process without coverage analysis In we des cribed an algorithm TF P C  of this kind The heuristic applied by TFPC is that once a general rule is found that satises the required thresholds of support and condence no more specic rules rules with the same consequent whose antecedent is a superset will be considered This provides a very efcient method for generating a relatively compact set of CRs Because no coverage analysis is carried out however the choice of appropriate support and condence thresholds is critical in determining the nal rule set In this paper we examine the effect of varying these thresholds on the accuracy of both TFPC and other algo 
rithms We show that classication accuracy can be signicantly improved in most cases by an appropriate choice of values We describe a hill climbing algorithm which aims to nd the best thresholds from examination of the training data We show that this procedure can lead to higher classication accuracy at lower cost than methods of coverage analysis 2 Finding best threshold values To examine the effect that may result from varying threshold values we carried out experiments using test data from the UCI Machine Learning Repository discretized using the LUCS-KDD DN software 1  Here for example the label glass.D48.N214.C7 
refers to the glass data set which includes 214 records in 7 classes with attributes which have been discretised into 48 binary categories Using this data we investigated the classication accuracy that can be achieved using the TFPC algorithm and also from CMAR and CBA across the full range of values for the support and condence thresholds For each support condence pair we obtained a classication accuracy from a division of the full data set into a 90 training set and 10 test set Figure 1 illustrates a selection of results in the form of 3-D plots in which the X and Y axes represent support 1 Available at 
Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


and condence threshold values and the Z axis the corresponding classication accuracy obtained These results demonstrate that the accuracy obtained can be very sensitive to the choice of thresholds The coverage analysis used in CMAR and CBA usually smooths out some of the inuence of this provided sufciently low thresholds are chosen The smoothing is not perfect however ticTacTo e  for example illustrates a case where a low condence threshold leads to a selection of poor rules by CMAR and CBA performs badly for wine if a low support threshold is chosen For CBA the example of ionosphere shows a case where a poor choice of thresholds even values that appear reasonable may lead to a dramatically worse result This is partly because unlike CMAR CBAs coverage analysis may sometimes retain a rule that applies only to a single case This makes the method liable to include spurious rules especially if the data set is small enough for these to reach the required thresholds It is apparent that the accuracy of the classiers obtained using any of these methods may be improved by a careful selection of these thresholds To obtain these values we apply a procedure for identifying the threshold values that lead to the highest classication accuracy from a particular training set The method applies a hill-climbing strategy that makes use of a 3-D playing area measuring  as visualised in the illustrations discussed above The procedure commences with initial support and condence threshold values describing a current location   in the base plane of the playing area Using these values the chosen rule-generation algorithm is applied to the training data and the resulting classier applied to the test data with appropriate cross-validation to obtain a classication accuracy for  The procedure then moves round the playing area with the aim of improving the accuracy value To do this it continuously generates data for a set of eight test locations dened by applying two values and as positive and negative increments of the support and condence threshold values associated with  The rule-generation algorithm is applied to obtain a classication accuracy for each of the test locations which is inside the playing area and for which no accuracy value has previously been calculated The location for which the highest accuracy is obtained is selected as for the next iteration If the current has the best accuracy then the threshold increments are reduced and a further iteration of test locations takes place The process concludes when no improvement in accuracy can be obtained The nal selected will be at worst a local optimum 3 RESULTS Table 1 summarises the results of applying the hillclimbing procedure described above to datasets from the UCI repository for the algorithms TFPC CMAR and CBA For each algorithm the rst two columns in the table show the average accuracy obtained from applying the algorithm to 90 10 divisions of the dataset with ten-fold crossvalidation The rst of the two columns shows the result for a support threshold of 1 and a condence threshold of 50 the values usually chosen in analysis of classication algorithms and the second after applying the hill-climbing procedure to identify the best threshold values For these experiments and were set to and respectively and and to and  In each case the threshold values that produced the best accuracy are also tabulated Table 1 conrms the picture suggested by the illustrations in Figure 1 although note the correspondence is not exact as cross-validation was not used in obtaining the graphical representations In almost all cases an improved accuracy can be obtained from a pair of thresholds different from the default 1 50 choice As would be expected the greatest gain from the hill-climb ing procedure is in the case of TFPC but a better accuracy is also obtained for CMAR in 21 of the 25 sets and for CBA in 20 In a number of cases the improvement is substantial It is apparent that CBA especially can give very poor results with the default threshold values In the cases of ionosphere and wine the illustrations reveal the reason to be that a 1 support threshold leads for these small data sets to the selection of spurious rules This is also the case for zoo and hepatitis and for mushroom  where even a much larger data set includes misleading instances if a small support threshold is chosen In the latter case the hill-climbing procedure has been ineffective in escaping a poor local optimum Notice that here the coverage analysis used in CMAR is much more successful in identifying the best rules although TFPC also does relatively well CMAR is generally less sensitive than CBA to the choice of thresholds but both methods give very poor results when as in the cases of chess and letrecog  the chosen condence threshold is too high and CMAR performs relatively poorly for led7 for the same reason The extreme case is chess  where both CMAR and CBA and TFPC nd no rules at the 50 condence threshold Notice also that for the largest data sets those with more than 5000 cases a support threshold lower than 1 almost always produces better results although the additional candidate rules generated at this level will make coverage analysis more expensive Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


TFPC CMAR CBA a b c Figure 1 3-D Plots a ionosphere.D157.N351.C2 b ticTacToe.D29.N958.C2 and c wine.D68.N178.C3 In general the results show that coverage analysis especially in CMAR is usually although not always effective in minimising any adverse effect from a poor choice of thresholds Although TFPC with the default threshold values produces reasonably high accuracy in most cases the lack of coverage analysis generally leads to somewhat lower accuracy than one or both of the other methods However the results when the hill-climbing procedure is applied to TFPC show that high accuracy can be obtained without coverage analysis if a good choice of thresholds is made In 18 of the 25 cases the accuracy of TFPC after hill-climbing is as good or better than that of CMAR with the default thresholds and in only one case  wine  is it substantially worse the hill-climbing in this case failing to nd the peak of the rather irregular terrain shown in the illustration Conversely the result for penDig demonstrates a case in which the hill-climbing procedure of TFPC works better than the coverage analysis of CMAR in identifying important lowsupport high-condence rules The results also improve on CBA in 14 cases often by a large margin Overall this suggests that a good choice of thresholds can eliminate the need for coverage analysis procedures The signicance of this is that coverage analysis is relatively expensive especially if the data set and/or the numProceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


ber of candidate rules is large as is likely to be the case if a low support threshold is chosen The nal four columns of Table 1 give a comparison of the total execution times to construct a classier with ten-fold cross validation using TFPC with or without the hill-climbing procedure and for CMAR and CBA for the 1 50 thresholds These gures were obtained using our Java 1.4 implementations on a single Celeron 1.2 Ghz CPU with 512 MBytes of RAM TFPC CMAR CBA Execution Time Data set Def best Def best Def best val HC thold val HC thold val HC thold TFPC TFPC CMAR CPAR S C S C S C HC adult.D97.N48842.C2 80.8 81.0 0.2 50.1 80.1 80.9 0.7 50.0 84.2 84.6 0.1 48.4 2.9 20.0 78.0 230.0 anneal.D73.N898.C6 88.3 90.1 0.4 49.1 90.7 91.8 0.4 50.0 94.7 96.5 0.8 46.8 0.5 2.7 2.3 5.8 auto.D137.N205.C7 70.6 75.1 2.0 52.4 79.5 80.0 1.2 50.0 45.5 77.5 2.7 50.8 3.3 61.7 703.9 536.3 breast.D20.N699.C2 90.0 90.0 1.0 50.0 91.2 91.2 1.0 50.0 94.1 94.1 1.0 50.0 0.3 0.3 0.4 0.6 chess.D58.N28056.C18 0.0 38.0 0.1 25.2 0.0 34.6 0.1 11.0 0.0 39.8 0.1 24.0 2.1 46.7 2.0 2.0 cylBds.D124.N540.C2 68.3 74.4 1.2 49.8 75.7 77.8 1.3 49.9 75.7 78.0 1.9 50.0 4.0 163.9 206.9 923.6 are.D39.N1389.C9 84.3 84.3 1.0 50.0 84.3 84.3 1.0 50.0 84.2 84.2 1.0 50.0 0.4 0.5 1.0 2.4 glass.D48.N214.C7 64.5 76.2 2.6 45.6 75.0 75.0 1.0 50.0 68.3 70.7 3.0 51.6 0.4 1.1 0.8 0.8 heart.D52.N303.C5 51.4 56.0 4.2 52.4 54.4 54.8 1.6 50.0 57.3 60.0 4.2 49.2 0.6 2.7 0.9 1.4 hepatitis.D56.N155.C2 81.2 83.8 1.6 51.6 81.0 82.8 2.9 50.0 57.8 83.8 7.1 48.4 0.6 2.4 2.4 10.0 horseCol.D85.D368.C2 79.1 79.9 1.2 50.2 81.1 81.9 2.9 50.0 79.2 83.9 5.5 49.2 0.5 2.1 10.5 66.5 ionsph.D157.N351.C2 85.2 92.9 9.8 50.0 90.6 91.5 2.6 50.0 31.6 89.5 10.0 49.2 2.3 16.3 3066.8 2361.1 iris.D19.N150.C3 95.3 95.3 1.0 50.0 93.3 94.7 2.3 50.0 94.0 94.0 1.0 50.0 0.3 0.4 0.3 0.3 led7.D24.N3200.C10 57.3 62.7 2.2 49.4 62.2 67.4 1.3 40.4 66.6 68.0 1.0 46.0 0.4 1.4 0.6 0.7 letRc.D106.N20K.C26 26.4 47.6 0.1 32.3 25.5 45.5 0.1 31.8 28.6 58.9 0.1 13.7 3.7 196.2 17.2 20.5 mushm.D90.N8124.C2 99.0 99.7 1.8 69.2 100.0 100.0 1.0 50.0 46.7 46.7 1.0 50.0 1.4 30.6 269.0 366.2 nursry.D32.N12960.C5 77.8 89.9 1.0 73.2 88.3 90.1 0.8 62.6 90.1 91.2 1.5 50.0 1.3 21.3 5.8 6.9 pgBlks.D46.N5473.C5 90.0 90.0 1.0 50.0 90.0 90.3 0.2 50.0 90.9 91.0 1.6 50.0 0.3 0.7 0.8 2.2 penD.D89.N10992.C10 81.7 88.5 0.1 62.3 83.5 85.2 0.8 50.0 87.4 91.4 0.1 50.9 3.7 227.8 39.3 43.6 pima.D38.N768.C2 74.4 74.9 2.3 50.0 74.4 74.5 1.6 50.0 75.0 75.7 2.8 50.0 0.3 0.4 0.4 0.7 soyLrg.D118.N683.C19 89.1 91.4 1.1 49.1 90.8 91.8 0.8 51.6 91.0 92.9 0.6 52.2 9.8 644.3 405.6 273.8 ticTacToe.D29.N958.C2 67.1 96.5 1.5 74.2 93.5 94.4 1.6 50.0 100.0 100.0 1.0 50.0 0.4 4.5 1.0 1.6 wavefm.D101.N5K.C3 66.7 76.6 3.2 64.3 76.2 77.2 0.6 50.0 77.6 78.2 2.6 50.0 3.7 210.8 167.3 93.4 wine.D68.N178.C3 72.1 81.9 4.5 51.2 93.1 94.3 2.3 50.0 53.2 65.5 4.8 50.0 0.3 1.0 7.3 11.7 zoo.D42.N101.C7 93.0 94.0 1.0 49.2 94.0 95.0 1.6 50.0 40.4 93.1 7.4 50.0 0.5 2.5 3.1 3.2 Ta ble 2  Accuracy and performance results Default values condence  50 support  1 As would be expected the execution times for TFPC with default threshold values are almost always far lower than for either of the other two methods Less obviously performing hill-climbing with TFPC is in many cases faster than coverage analysis with CMAR or CBA In 13 of the 25 cases this was the fastest procedure to obtain classication rules and it is only markedly worse in cases such as chess and letRecog  where the other methods have failed to identify the rules necessary for good classication accuracy These results suggest that TFPC with hill-climbing is an effective way of generating an accurate classier which is often less costly than other methods 4 CONCLUSIONS In this paper we have shown that the choice of appropriate values for the support and condence thresholds can have a signicant effect on the accuracy of classiers obtained by CARM algorithms The coverage analysis performed by methods such as CMAR and CBA reduces this effect but does not eliminate it CMAR appears to be less sensitive than CBA to the choice of threshold values but for both methods better accuracy can almost always be obtained by a good choice We have also shown that if threshold values are selected well it is possible to obtain good classication rules using a simple and fast algorithm TFPC without the need for coverage analysis We describe a procedure for nding these threshold values that will lead to good classication accuracy Our results demonstrate that this approach can lead to improved classication accuracy at a cost that is comparable to or lower than that of coverage analysis References  C oenen F Leng P  and Zhang L  2005 Threshold Tuning for Improved Classication Association Rule Mining Proc PAKDD 2005 LNCS 3518 Springer pp216-225  Li W   H an J  and P e i  J  2001 CMAR Accurate and Efcient Classication Based on Multiple ClassAssociation Rules Proc ICDM 2001 pp369-376  Li u B  Hs u W  and Ma Y 1998 Integrating Classication and Association Rule Mining Proceedings KDD-98 New York 27-31 August AAAI pp80-86  Y i n X and Han J  2003 CPAR Classication based on Predictive Association Rules Proc SIAM Int Conf on Data Mining SDM03 San Francisco CA pp 331-335 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


  S k-1  KB.base  020  N k-1  minsup    2 X k  minsup  S k-1  KB.base  021  S k-1  KB.base   020   S k-1  KB.base  021  N k-1  minsup   020  N k-1  minsup  021  S k-1  KB.base  020  N k-1  minsup  021  N k-1  minsup  3 X k  minsup  X k  KB.base  020  S k-1  KB.base  021  N k-1  minsup  020  N k-1  minsup  021  N k-1  minsup  due to X k  KB.base  S k-1  KB.base  021  S k-1  KB.base  a nd that N k-1  minsup  021  S k-1  KB.base  e sa m e  set as  S k-1  KB.base  021  N k-1  minsup   4\ce X k  X k  minsup  024  X k  KB.base  X k    S k-1  KB.base  021  N k-1  minsup   020  N k-1  minsup  021  N k-1  minsup               4.3. Concurrent support counting and the placement of variable sized candidates KISP might have only very few new-candidates at a very low minsup value since the information gathered from each mining all contribute to the candidate reduction In each pass, the number of candidates inserted into the hash-tree is far less than that of GSP Thus KISP is enabled to accommodate more candidates in the same hash-tree during the same pass of database scanning Consequently, we may release the restriction of counting only candidate k sequences to count variable sized candidates concurrently in pass k and reduce the total number of database passes  KISP assumes nothing about the main memory size so that the available memory might be insufficient for generating new candidate k sequences. Analogous to GSP  if the set of frequent k-1 sequences cannot fit into the memory, we apply the relational merge-join technique without pruning [14 to g e n e r a te X k and then output X k to disk after counting  On the contrary, the set of X k is possibly small and occupies only a small part of the memory. We maximize memory utilization by continuously generating the candidates of longer size into the hash-tree until the memory space is nearly full KISP can estimate the space required for X k with S k-1  KB.base  d N k-1  minsup   described in the following Considering the number of candidates generated in each pass X 2 has more candidates than any other X k because none in the candidate superset of size two can be pruned For candidates of X k where k 2, some frequent  k-1 sequences are not to be joined if their subsequences do not match. Assume the number of patterns in S 1  KB.base   p and the number of patterns in N 1  minsup  is q The number of new-candidates in pass 2 is 3 p  q  2  p  q  3 p 2 p  pq 3 q 2 q This formula can be applied to estimate roughly the maximum number of candidates in other passes. Whenever there is room for the next set of candidates in a batch KISP continuously generates and inserts them into the same hash-tree  The hash-tree in GSP is designed to store the same sized candidates in the leaves originally. Accommodating variable sized candidates in the same hash-tree might produce the problem of having no item for hashing. For example, inserting a candidate 4 sequence might cause the re-distribution of an overflowed node, while the re-distribution might need to hash on the fourth item of a candidate 3 sequence in the node. We modify the hashing procedure to put the same prefixed candidates, despite their sizes, in the same leaf. In case there is no item for hashing any more, the candidate is stored in one of the descendent leaves \(due to splitting the overflowed leaf We select the leaf having the fewest number of candidates stored to maximize memory utilization. Since candidates of different size are stored in the same hash-tree, we can check the variable sized candidates against a data sequence at the same time. Therefore, the concurrent support counting minimize the number of database scanning required in KISP  Note that a similar technique named pass bundling is described for association mining in o w e v e r  pass bundling statically sets a limit to determine whether the generation should be continued or not, while KISP  dynamically estimates and computes the available memory for maximum utilization  4.4. Manipulation of the knowledge base The knowledge base should provide fast access to the supports of patterns, support quick estimation of candidate storage required, and be able to expand incrementally Figure 2 displays the logical structure of the knowledge base  A knowledge base is composed of a minimal KB.base and one or more KB heads The minimal KB.base is the smallest KB.base among all the KB.bases in the KB heads. We create a KB head to store the newly acquired information only when the \221new pattern\222 mining part of KISP is executed. A KB head comprises \(1\a KB.base indicating the counting base while adding this head \(2\e number of pattern-support heads ps_heads indicating the total number of pattern-support heads in this KB head \(3\e pattern-support heads  summarizing the pattern-support tables, and \(4\he position of next KB head linking the next KB head so that the knowledge base can \221grow\222 incrementally We group all the same sized patterns in the pattern-support tables so that the pattern information of desired size can be directly found through the position of pattern-support in the corresponding ps_head. The summary information the size of the pattern the total number of counted candidates and the total number of non-zero patterns ps_head is used to estimate the number of new-candidates. The pattern-support table is a list of \(support, pattern\pairs. Note that we keep only the Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


 patterns with non-zero support value to minimize the total size of each pattern-support table. The supports of patterns of the same size\tored in support-descending order in the structure to ease the searching of valid patterns on answering an online query. An option to eliminate support sorting is writing the supports in the order of hash-tree traversal. Even when the pattern supports are directly stored without sorting, searching within the knowledge base is still more efficient than re-mining 5. Experimental results Several experiments were conducted to assess the performance of the KISP algorithm, using an 866 MHz Pentium-III PC with 256MB memory running the Windows NT. Like most studies on sequential pattern mining [2, 4, 12, 14, 16 w e g e n e rat e d t h e sy nt h e t i c  datasets for these experiments using the procedure described in Du e to s p ace li m it w e on l y report th e  results on dataset C 20 T 2.5 S 4 I 1.25 having 100,000 sequences 025\006  N 10000 N S 2500, and N I 25000 026\006 We refer readers to [2 o r th e d etails o f th e p ara m eters  5.1. Comparisons of KISP and GSP  The effect of using knowledge base without concurrent counting optimization is studied first. The interactive discovery comprises five consecutive queries, with minsup  values starting from 2.5% down to 0.5 Figure 3 compares the relative performance of KISP  and GSP The total execution time is 435 minutes by KISP  and 799 minutes by GSP  KISP runs faster than GSP for individual mining except for the very first mining. Figure 3 also depicts the ratios of the number of candidates in GSP to those in KISP Take minsup 0.75% for example the execution time ratio of GSP to KISP is 2.1 times. The time saved by KISP resulted from the reduced number of candidates\227 GSP counted 2.3 times the number of candidates. To illustrate the accumulating power of KB  the number of candidates generated by GSP and by KISP  in each pass is enumerated in Table 1 KISP exhibits excellent mining capability for query intensive applications. As we increased the number of queries from 3 to 11, the average execution time \(also the time required for posterior queries\ed from 1763 seconds to 514 seconds In the experiments with concurrent optimization, the number of database scanning reduced by concurrent support counting is 6, and the reduced execution time is 94 seconds for the mining with minsup 0.5%. Most scans were combined in pass three so that the total number of passes and the total execution times were reduced When users need to find the appropriate set of patterns by reducing the number of patterns found in a query, the next specified minsup would be greater than the counting base of KB  KB  base e next experiment, all KB  base s of the KB s were 0.5%, and 100 minsup s ranging from 0.5% to 2.5% were randomly selected. The mining results are all available in very short time with average execution time 4.3 seconds and maximum execution time 22 seconds. For most queries, the execution time of KISP  is several orders of magnitude faster than GSP which always re-mines from scratch. However, one drawback of KISP is that the size of KB in proportion to the number of candidates stored, might be larger than the size of the original database. For example, the size ratio of KB to DB  is 4.9 for minsup 0.5 5.2. Scale-up experiments In the scale-up experiments, the total number of customers was increased from 100K to 1000K, r unning the same series of minsup 2.5% down to 0.5%\ince KISP  retrieves merely S k-1  KB.base  i.e. f r equ e n t   k-1 sequences in KB for generating candidate k sequences, even without large memory KISP may efficiently discover patterns in large databases with KB  Figure 4 shows that the execution time of KISP increases linearly as the database size increases. The execution times are normalized with respect to the time for 100 000 customers 6. Conclusions The knowledge discovering process is iterative and requires many times of mining since no one can predict the best parameters for the desired outcome. Even a change in minimum support value would demand current approaches to execute the time-consuming process again, not to mention the various query operations such as mining constrained patterns [1 pa t t e rn s  w i t h  h i erarchy 14  In this paper, we propose a simple but efficient mining algorithm for interactive discovery of sequential patterns about varying support thresholds. The proposed KISP  algorithm constructs a knowledge base KB in-disk to minimize the response time for iterative mining. No mining is required if the query result is a subset of KB  otherwise, we speed up individual mining through accessing only frequent sequences in KB for direct new-candidate generation. The proposed approach directly generates only the new candidates not being considered before, concurrently counts variable sized candidates in the same database scanning, and incrementally expands the knowledge base. Only the non-zero patterns grouping by size are kept to minimize the size of KB while providing fast access to pattern information. The performed experiments show that KISP enhances GSP by several orders of magnitude for interactive sequence mining, with good linear scalability  However, the disk space could be a problem without Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


minsup  further investigation on minimizing KB for very low thresholds. Future work may include the maintenance of KB for database updating [6 r in teracti v e q u eries o t h er than varying thresholds, though we may answer these queries by reading patterns in KB into an ISL like [11 pattern-lattice, it is desirable to integrate the query requirements into KISP for faster response   KB.base number of ps_heads position of next KB head Minimal KB.base   KB head pattern-support head \(ps_head number of non-zero patterns position of pattern-support size of pattern number of counted candidates number of non-zero patterns position of pattern-support size of pattern number of counted candidates number of non-zero patterns position of pattern-support size of pattern number of counted candidates  Figure 2. Structure of the knowledge base C20-T2.5-S4-I1.25 0 25 50 75 100 125 150 175 200 225 250 2.5 2 1 0.75 0.50 minsup Ratio \(GSP/KISP Execution time Number of candidates  Figure 3. Relative mining performance of 0.5 Pass number Number of candidates  1 2 3 4 5 6 7 8 GSP 10000 7673835 7986 2800 1339 430 63 3 KISP 0 3122860 5941 2387 1259 424 63 3 Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE KISP and Table 1. Number of candidates in each pass GSP 


  Scale-up Performance of KISP 1.0 3.0 5.0 7.0 9.0 11.0 13.0 15.0 100K 250K 500K 750K 1000K Number of customers Execution time ratio 2.50 2 1 0.75 0.50 minsup Figure 4. Linear scalability of the database size Acknowledgements The authors thank the reviewers\222 comments for improving the quality of the paper. This research is supported partially by National Science Council of R.O.C and the LEE and MTI Center for Networking Research at National Chiao Tung Univ., R.O.C  References 1 C  C  A g g a r w a l a nd P   S  Y u 223 O nli n e G e ne r a tion of  Association Rules,\224 Proceedings of the 14th International Conference on Data Engineering Orlando, Florida, USA, Feb 1998, pp. 402-411 2 R A g ra w a l a nd R. Srik a n t, \223 M ining Se q u e n tia l P a tte rns 224  Proceedings of the 11th International Conference on Data Engineering Taipei, Taiwan, 1995, pp. 3-14 3 R A g ra wa l a nd R  Sr ik a n t 223 F a s t A lg o rithm s f o r Mining  Association Rules,\224 Proceedings of the 20th International Conference on Very Large Data Bases Santiago, Chile, Sep 1994, pp. 487-499  J Han J  P e i  B  M o rt azavi Asl  Q C h en  U Day a l an d M  C  Hsu, \223FreeSpan: Frequent pattern-projected sequential pattern mining,\224 Proceedings of the 6th ACM SIGKDD international conference on Knowledge discovery and data mining 2000, pp 355-359 5 C  H i dbe r   Online Association Rule Mining Technical Report UCB/CSD-98-1004, U. C. at Berkeley, 1998  M  Y  L i n an d S  Y  L ee 223Incre m en t al Up d at e on S e q u ent i al  Patterns in Large Databases,\224 Proceedings of 10th IEEE International Conference on Tools with Artificial Intelligence  1998, pp. 24-31  H M ann i l a H  T o i von en and A  I V erkam o  223Di s co v er y o f  Frequent Episodes in Event Sequences,\224 Data Mining and Knowledge Discovery Vol. 1, Issue 3, 1997, pp. 259-289 8  A M. Mue lle r  Fast Sequential and Parallel Algorithm for Association Rule Mining: A Comparison Technical report CS-TR-3515, University of Maryland, 1995 9 B Na g   P  M De shpa n d e a n d D. J De W itt, \223 U sing a  Knowledge Cache for Interactive Discovery of Association Rules,\224 Proceedings of the 1999 SIGKDD Conference San Diego, California, Aug. 1999, pp. 244-253 1 S   P a rt h a sarat h y  S  Dwark ad as an d M  Ogi h ara 223A ct i v e Mining in a Distributed Setting,\224 Proceedings of Workshop on Large-Scale Parallel KDD Systems San Diego, CA, USA, Aug 1999, pp. 65-85  1 S   P art h asarat h y  M  J Z aki   M  Ogi h ara an d S  Dw arkad as  223Incremental and interactive sequence mining,\224 Proceedings of the 8th International Conference on Information and Knowledge Management Kansas, Missouri, USA, Nov. 1999, pp. 251-258 1 J  P e i J Han  H  P i n t o Q Ch en  U Dayal an d M  C Hsu  223PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-projected Pattern Growth,\224 Proceedings of 2001 International Conference on Data Engineering 2001, pp 215-224 13  T Shi n ta ni a n d M. Ki tsure g a w a 223 M ining A l g o rithm s  f o r Sequential Patterns in Parallel: Hash Based Approach,\224 Proceedings of the Second Pacific\226Asia Conference on Knowledge Discovery and Data mining 1998, pp. 283-294 14 R Srik a n t a n d R A g ra wa l, \223 M i n ing  Se que ntia l P a t te r ns  Generalizations and Performance Improvements,\224 Proceedings  of the 5th International Conference on Extending Database Technology Avignon, France, 1996, pp. 3-17  15 K  W a ng 223 D is c o v e r i ng P a t t e r ns f r om  L a r g e a nd D y nam i c  Sequential Data,\224 Journal of Intelligent Information Systems Vol 9, No. 1, 1997, pp. 33-56 16 M J  Za k i 223 E f f ic ie nt En um e r a tion of Fr e q ue nt Se q u e n c e s 224  Proceedings of the 7th International Conference on Information and Knowledge Management Washington, USA, Nov.1998, pp 68-75   Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


21  and denote wjk as the set of weights for Yj where    k j k jw 1 1.  A classifier H is defined as YD ? such that it assigns a weight of the correct class label to an instance as  iWdH where deD, and k j i WW ? . For a set of single-class instances I = &lt; \(x1 y1 x2, y2  xn, yn Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Table 4. Classification accuracy of PART RIPPER, CBA and MMAC Dataset PART RIPPER CBA MMAC Tic-Tac 92.58  97.54  98.60  99.29 Contactlenses 83.33  75.00  66.67  79.69 Led7 73.56 69.34  72.39  73.20 Breastcancer 71.32  70.97  68.18  72.10 Weather 57.14  64.28  85.00  71.66 Heart-c 81.18  79.53  78.54  81.51 Heart-s 78.57  78.23  71.20  82.45 Lymph 76.35  77.70  74.43  82.20 Mushroom 99.81 99.90  98.92  99.78 primarytumor 39.52  36.28  36.49  43.92 Vote 87.81  87.35  87.39  89.21 CRX 84.92  84.92  86.75  86.47 Sick 93.90  93.84  93.88  93.78 Balancescale 77.28  71.68  74.58  86.10 Autos 61.64  56.09  35.79  67.47 Breast-w 93.84  95.42  94.68  97.26 Hypothyroid 92.28 92.28  92.29  92.23 zoo 91.08  85.14  83.18  96.15 kr-vs-kp 71.93 70.24  42.95  68.75 is  m k ii i ydHw m 1  1 ?  , where          yxif yxif yx 0 1  For example, if an item \(A ,a labels  c1    c2  and  c3  7, 5  and 3 times 


labels  c1    c2  and  c3  7, 5  and 3 times respectively, in the training data. Each class label will be assigned a weight, i.e. 7/15, 5/15, and 3/15, respectively for labels  c1    c2  and  c3  This technique assigns the predicted class label weight to the case if the predicted class label matches the case class label. For instance if label  c2  of item \(A, a test data that has  c2  as its class, then the case will be considered a hit, and 5/15 will be assigned to the case 5. Experimental Results We investigated our approach against 19 different datasets from [20] as well as a different datasets for forecasting the behaviour of an optimisation heuristic within a hyperheuristic framework [5, 16]. Stratified tenfold cross-validation was used to derive the classifiers and error rates in the experiments. Cross-validation is a standard evaluation measure for calculating error rate on data in machine learning. Three popular classification techniques a decision tree rule \(PART CBA have been compared to MMAC in terms of classification accuracy, in order to evaluate the predictive power of the proposed method The choice of such learning methods is based on the different strategies they use to generate the rules. Since the chosen techniques are only suitable for traditional classification problems where there is only one class assigned to each training instance, we therefore used classification accuracy derived by only the top-label evaluation measure for fair comparison All experiments were conducted on a Pentium IV 1.6 GH PC.  The experiments of PART and RIPPER were conducted using the Weka software system [20]. Weka stands for Waikato Environment for Knowledge Analysis. It is an open java source code for the machine teaching community that includes implementations of different methods for several different data mining tasks such as classification, clustering, association rule and regression. CBA experiments were conducted using a VC++ implementation version provided by [19]. Finally MMAC was implemented using Java We have evaluated 19 selected datasets from Weka data collection [20], in which, a few of them \(6 reduced by ignoring their integer and/or real attributes Several tests using ten-fold cross-validation have been performed to ensure that the removal of any real/integer attributes from some of the datasets does not significantly affect the classification accuracy. To do so we only considered datasets where the error rate was not more than 6% worse than the error rate obtained on the same dataset before the removal of any real/integer attributes.  Thus, the ignored attributes do not impact on the error rate too significantly Many studies have shown that the support threshold plays a major role in the overall classification accuracy of the set of rules produced by existing associative classification techniques [9, 12]. Moreover, the support value has a larger impact on the number of rules produced in the classifier and the processing time and storage needed during the algorithm rules discovery and generation. From our experiments, we noticed that the support rates that ranged between 2% to 5% usually achieve the best balance between accuracy rates and the size of the resulted classifiers. Moreover, the classifiers derived when the support was set to 2% and 3 achieved high accuracy, and most often better than that of decision trees rule \(PART the MinSupp was set to 3% in the experiments. The confidence threshold, on the other hand, is less complex and does not have a large effect on the behaviour of any associative classification method as support value, and thus it has been set to 30 


Table 4 represents the classification rate of the classifiers generated by PART, RIPPER, CBA and MMAC against 19 benchmark problems from Weka data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 3 5.00 2 5.00 15.00 5.00 5.00 15.00 2 5.00 3 5.00 4 5.00 55.00 6 5.00 75.00 8 5.00 9 5.00 1 2 3 4 5 6 7 8 9 Nine  Scheduling Runs D iff er en ce  in  A cc u ra cy  CB A To p-label A ll-label A ny-label Figure 3a. Difference of accuracy between MMAC evaluation measures and CBA algorithm 35.00 25.00 15.00 5.00 5.00 15.00 25.00 35.00 45.00 55.00 65.00 75.00 85.00 95.00 1 2 3 4 5 6 7 8 9 Nine  Scheduling Runs D iff er en ce in  A cc u ra cy  P A RT To p-label A ll-label A ny-label Figure 3b. Difference of accuracy between MMAC   evaluation measures and PART 


MMAC   evaluation measures and PART algorithm 0 2 4 6 8 10 12 14 16 18 20 22 24 26 Run1 Run2 Run3 Run4 Run5 Run6 Run7 Run8 Run9 Ten Runs  Scheduling Data N um be r o f R ul es To p Label P A RT CB A Figure 4. Classifier sizes of MMAC \(toplabel the scheduling   data collection. The accuracy of MMAC has been derived using the top-label evaluation measure. Our algorithm outperforms the rule learning methods in terms of accuracy rate, and the won-loss-tied records of MMAC against PART, RIPPER and CBA 13-6-0, 15-4-0 and 154-0, respectively The evaluation measures of MMAC have been compared on 9 solution runs produced by the Peckish hyperheuristic [5] with regard to accuracy, and number of rules produced. Figures 3a and 3b represent the relative prediction accuracy that indicates the difference of the classification accuracy of MMAC evaluation measures with respect to those derived by CBA and PART, respectively. In other words, how much better or worse MMAC measures perform with respect to CBA and PART learning methods. The relative prediction accuracy numbers shown in Figures 3a and 3b are conducted using the formula PART PARTMMAC Accuracy AccuracyAccuracy  and CBA CBAMMAC Accuracy AccuracyAccuracy  respectively. After analysing the charts, we found out that there is consistency between the top-label and label-weight measures, since both of them consider only one class in the prediction. The top-label takes into account the topranked class, and the label-weight considers only the weight for the predicted class that matches the test case Thus, both of these evaluation measures are applicable to traditional single-class classification problems. On the other hand, the any-label measure considers any class in the set of the predicted classes as a hit whenever it matches the predicted class regardless of its weight or rank. Is should be noted that, the relative accuracy of MMAC evaluation methods against dataset number 8 in Figure 3a and 3b, is negative since CBA and PART 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





