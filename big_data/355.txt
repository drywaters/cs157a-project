A Pattern Growth Method Based on Memory Indexing for Frequent Patterns Mining Junjie Hou, Chunping Li School of Software, Tsinghua University, P.R.China E-mail houjj03@mails.tsinghua.edu.cn   cli@tsinghua.edu.cn Abstract In this paper, we present an algorithm based on memory indexing for frequent patterns mining \(called MIndexing\, which requires scanning database only one time and does not generate any candidates. The MIndexing algorithm is memory-based and can utilize memory and CPU resources sufficiently to extend the capability in high effectiveness and efficiency. Our experiment results show t hat the MIndexing algorithm performs better than Apriori and FP-growth method for processing sparse data dat asets containing long patterns. Furthermore, with MIndexing algorithm, we adopt a partitioning-based strategy to decompose the mining task into a set of smaller tasks for mining frequent patterns for processing very large datasets 1. Introduction A fundamental problem in data mining is how to find frequent patterns in all kinds of data sets. Finding frequent patterns plays an essential role in association rule mining. Currently, many algorithms for frequent pattern mi  techniques which adopt the candidate generation-and-test method The Apriori algorithm algorithm  Many existing subsequent algorithms are based on it In the process of frequent pattern mining, the Apriori algorithm adopts the candidate-generation-and-test method. The first pass of the algorithm simply counts item occurrences to determine the frequent items. A subsequent pass consists of two phases: \(1 frequent patterns found in current pass are used to generate the candidate item sets; \(2\is scanned to count the occurrence of the candidates to determine the frequent patterns Apriori algorithm requires multiple database scanning. The scanning times depend on the length of the longest patterns. To find long patterns, it may need too many database scanning that is quite timeconsuming. Meantime, while processing data sets that contain long patterns, it generates too many candidates and subsequences of frequent patterns. For example, to find a frequent pattern whose length is 14, it may generate 1 frequent subsequences first which is a huge number. So we can conclude that Apriori is not suited for finding long patterns 14 2 To solve the problem the Frequent-Pattern growth me thod \(FP-growth method which avoids the costly candidate generation and requires only two times database scanning. The first pass finds all frequent items, and the second pass constructs a frequent-pattern tree \(FP-tree\structure using the frequent items, which is used for storing compressed, crucial information about frequent patterns. FP-growth mines the complete set of frequent patterns by pattern fragment growth. The FP-tree is a prefix-tree, which is effective and efficient when there are many same prefixes in the transactions of the data set. So FP-growth method is very effective for dealing   However, the FP-tree is a memory-based structure that may become huge while dealing with large sparse data set, which means th at it will be a low efficient algorithm for this type of data sets. So it is not easy to find frequent patterns while dealing with sparse data sets In our work, we propose a pattern growth method based on memory indexing for frequent pattern mining called MIndexing The method does not generate any candi dates. Through a recursive pattern growth approach, the MIndexing algorithm finds the complete frequent patterns in a database. Our experiment results illustrate th at it is an effective and efficient algorithm for processing usual data sets especially for processing sparse data sets that may contains long patterns Moreover, we provide a prefix-counting way to represent the frequent patterns. It can reduce the number of outputted frequent pa tterns while containing equivalent information with the complete set of frequent patterns. We will give detail information in subsequent sections Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


The paper is organized as follows: Section 2 presents some relevant defi nitions and theoretical background. The MIndexing algorithm is described in Section 3. In section 4 we present the experiments to verify the performance of the proposed algorithm and finally, in Section 5 we give our conclusions 2. Problem Statements In this section, we will describe the problem of frequent pattern mining. Before doing so, we will first give some concepts and terminologies Definition 1 Let I  i 1 i 2 203, i n be a set of literals called items. A subset of I is called an itemset. A k itemset is an itemset of size k For example s  l 1 l 2  203, l n  l i 000\216 I we call s is an k-itemset belonging to I  Definition 2 A transaction T   TID  I t is a tuple where TID is the identifier of the transaction and It is an itemset such that I t 000\216 I We suppose that the items in transaction are ordered alphabetically. If another transaction T 001\215  TID 001\215  I t 001\215 such that I t 001\215 000\216 It we call T 001\215 is a subsequence of T or a sub-pattern of T  Definition 3 Let a transaction database TDB is a set of transactions. The support sup X set X in transaction database TDB is defined as: sup X  T x   TDB where T x is the number of transactions in TDB containing X and TDB is the transaction number of TDB Given a support threshold minsup 0 002\330 minsup 0010 1, an itemset X is frequent provided sup  X  0011 minsup  Definition 4 Given itemset X and Y such as X 000\216 I  Y 000\216 I and X 001 Y  000I The confidence of association rule X 000\306 Y is defined as: confidence = sup X 001 Y sup X  When confidence is greater or equal than minconf that user specified, such as 75%, we think the rule is true. It means that at least 75 percent of the transactions containing X contain Y Usually, itemset X 001 Y is the frequent itemset we should find The main purpose of frequent pattern mining is to find all of the frequent patterns that satisfy the minsup constraint in a given database. Finding frequent patterns is the first step for generating association rules Table 1 The transaction database TDB TID Items in transaction  100 a, b, d 200 a, b, c, d, e 300 a, c, d, f 400 c, d, e, g 500 c e 3 MIndexing Indexing-Based Pattern Growth Algorithm 3.1. Basic Principle Suppose that the database TDB is shown in Table 1  where the database TDB consists of five transactions and a set of items I  a  b  c  d  e  f  g Let the minimum support threshold be min sup 40%, which means that a frequent itemset needs to appear in at least two transactions. By scanning the database once the complete set of frequent items f_list  a 3 b 2 c  4 d 4 e 3} can be found. With Apriori algorithm, we can find the complete set of frequent itemsets as shown in Table 2  The frequent patterns can be classified into five subsets: \(1 a 2 b but not containing a 3 c but not containing a  b 4 d but not containing a  b  c 5 e but not containing a  b  c  d Because the items in transactions are ordered alphabetically, the five subsets in fact are a prefixed patterns b prefixed patterns c prefixed patterns d prefixed patterns and e prefixed patterns respectively. The classified result is shown in Fig. 1 Based on the divide-and-conquer strategy, we can find all frequent patterns prefixed by a and then find patterns prefixed by b  c  d and e respectively. To find a prefixed patterns, we apply the pattern-growth principle i ning process W e  build an a projected database, which is {100, 200, 300}. In the projected database, from the positions item a we can find locally frequent items are b 2 c 2 d 3}. Then we can build ab projected ac projected, and ad projected databases respectively, in which we furthermore can find abd and acd that are 3-length frequent patterns Table 2 Frequent itemsets in TDB Length Frequent k-itemsets 1 a, b, c, d, e 2 ab, ac, ad, bd, cd, ce, de 3 abd, acd, cde a  b c d e ab ac ad bd cd ce de abd acd cde Fig 1 Frequent pattern tree Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


With the b projected database, we can find the set of b prefixed frequent patterns. Notice that item a is not included in the b projected database since all the frequent patterns including item a already is found With all frequent items in the database, we build corresponding projected databases as shown in Table 3 Table 3  Projected databases with frequent items Frequent items Projected databases  a:3 a cd a bcde a bd 000\306 cd bcde bd b:2 a b cde a b d 000\306 cde d c:4 ab c de a c d c de c e 000\306 de d de e d:4 abc d e ab d ac d c d e 000\306 e e e:3 abcd e c e cd e 000\306 null From the projected databases, we can find locally frequent items, which can form a set of 2-length frequent patterns with the found frequent items, i.e ab: 2, ac: 2, ad: 3, bd: 2 cd: 3, ce: 3, de: 2}, in our example. And then we can build recursively corresponding projected databases Accordingly, we can find 3-l ength frequent patterns in 2-itemset projected dat abases, i.e., {abd: 2, acd: 2 cde: 2}. The recursive process will end if we cannot find locally frequent items any more in projected databases 3.2. Algorithm Description We here build the projected databases by a memory indexing method. Given a pattern 000S we treat 000S 000\003 as a prefix pattern if we want to find p prefixed frequent patterns in transactions containing 000S We represent the projected database with respect to 000S 000\003 by 000S ids. In fact p ids in fact is a set of p_tr  pos  p_tr is a pointer to transaction tr that contains pattern 000S and pos is the first occurring position of 000S 000\003 in tr  The MIndexing algorithm finds the frequent patterns by the following three steps Step 1 Read database into memory and find frequent items. And then build up indexing for each frequent item Step 2 Find locally frequent items in the projected database of current prefix pattern. Append frequent items on current prefix pattern to form long prefix patterns, and build indexing for them respectively Step 3 Execute Step 2 recursively. When there are no frequent items in the projected database of current prefix pattern, output the pattern Algorithm MIndexing Input  TDB  minsup Output all frequent patterns begin Scan TDB to find all frequent items. Then read  TDB into main memory to build up indexing for them for each frequent item x do call MinePrefixedItemsets x  x-ids  delete indexing of x-ids   end  Procedure MinePrefixedItemsets x  x-ids  Parameter  x a prefix pattern x-ids indexing for x Output final frequent patterns that cannot grow longer begin for each p_tr  pos  x-ids do traverse items in transaction tr from position pos to the end to increase support for each item  for each p_tr  pos  x-ids do for each item a in tr from pos do if support of a  minsup  TDB  add a pair p_tr  a_pos  formed by x and a  a_pos is the appearing position of a in transaction tr   else delete item a    if\(the set of new prefix patterns is empty output x  else for each new prefix pattern x 001\215 do call MinePrefixedItemsets x 001\215  x 001\215 ids    end  Fig 2 Algorithm MIndexing The basic idea of MIndexing algorithm is to utilize both transactions and indices for the frequent patterns mining. In general, the transactions processing inmemory is highly efficient in comparison with diskbased processing which m ainly need multiple scans MIndexing algorithm scans only one pass over the database and discovers all frequent sequences with larger size recursively by searching the transactions having common sub-sequences. Fig. 2 outlines the MIndexing algorithm 3.3. Representing Final Patterns Specially So far, most of algorithms for frequent pattern mining output the final patterns in one of the three Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


ways: \(1 items    2\aximal frequent items  3 outputting closed frequent items 10  The method of generating the complete set of frequent itemsets and outputting all of them in a simple way may provide enough information for the generating of all association rules. However, the number of all frequent itemsets may be huge in many cases. Furthermore, some of the association rules generated from them may be redundant A frequent itemset is called a maximal frequent itemset if there is no other frequent itemset to be its proper superset. The subsets of a frequent itemset must be frequent, so some algorithms only output the maximal frequent itemsets. The set of maximal frequent itemsets is a subset of all frequent itemsets. It can reduce a lot of storage space if an algorithm only outputs the maximal frequent itemsets. However, it doesn\222t give enough information for the appearing times of the subsets of maximal frequent itemsets. So it may encounter difficulties to generate association rules from them A frequent closed itemset is either a maximal frequent itemset, or a frequent itemset whose support is higher than the supports of all its proper supersets Outputting closed frequent itemset may split the difference between above two methods. The set of frequent closed itemsets is also a subset of all frequent itemsets. To some extent, it can reduce the number of itemsets, and it can provide enough information to generate non-redundant association rules. But in some cases, the total number of closed frequent itemsets may still be huge, especially in sparse dataset, which may be near to the number of all frequent itemsets that are in a simple way. We expect to find some way else to present the final patterns We propose a prefix-counting method, which not only can provide enough information for generating all association rules, but also can further reduce the total number of final outputted itemsets in some cases compared with closed frequent itemsets. It may be the case especially when we mine frequent patterns from sparse datasets. That is because it is a compressive way of representing all frequent itemsets. In the prefixcounting me b[2 d[2  represents a 3 ab: 2, abd: 3}. It can provide enough information to generate some association rules, which may accelerate the process of generating rules from frequent itemsets We usually cannot do this through other methods Suppose minconf = 50%. Given a frequent pattern a[5]b[3] h[2], we can generate an association rule ab 000\306 h, which has a confidence of 66.7 3.4. Scale Up MIndexing algorithm is efficient when the transaction database with a set of memory indexing can fit in main memory. Fo r a large database, it may be partitioned so that MIndexing algorithm can handle each partition in memory. The globally frequent patterns must be frequent at least in one of the partitions. Therefore, we can find all locally frequent patterns as potential globally frequent patterns by running MIndexing algorithm on each of the partitions After we obtain the set of all potential patterns, the globally frequent patterns can be identified through a support counting against all transactions of the large database with an extra database pass Based on above observation, we can extend MIndexing algorithm to deal with large databases as the following steps Step 1 After a complete pass of the whole database, find the complet e set of globally frequent items Step 2 Partition the large database into k parts, so that each part can be handled with MIndexing algorithm in memory Step 3 With each part, find all locally frequent patterns whose items must be a subset of the globally frequent items Scan the whole database, and collect support for locally frequent patterns. Out put the globally frequent patterns 4. Experiments and Evaluation The experiments were performed on a Celeron \(R 2.00 GHz PC with 512 MB main memory, in the environment of Microsoft Windows 2000 server. We implemented MIndexing algorithm in C++ under the platform of the VS .net We chose a set of real and synthetic datasets for testing the performance of MIndexing These datasets have been used as benchmarks for testing previous algorithms of frequent pattern mining. The real datasets we used are BMS-WebView-1, BMSWebView-2 and retail 1 which are described in [13 We generated three synthetic datasets by a data generator 2 T3I2D100K, T4I2D100K and T5I2D100K The characteristics of all databases are shown in Table 4 Fig. 3 shows the performance curves for FP-growth Apriori, and MIndexing algorithms on the six datasets   1 http://fimi.cs.helsinki.fi/data 2 http://www.almaden.ibm.com/cs/quest/syndata.html Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


respectively. Our experiment results show that on these sparse datasets except retail MIndexing may attain a relatively better performance than the others Table 4  Database characteristics Database Item Size #Tran  BMS-WebView-1 497 2.5 59,602 BMS-WebView-2 3,340 5.0 77,512 retail 16,470 13 88,163 T3I2D100K 10,000 3 100,000 T4I2D100K 10,000 4 100,000 T5I2D100K 10,000 5 100,000 On BMS-WebView-1 and BMS-WebView-2 MIndexing algorithm gets a better performance than Apriori is not stable when the minimum support level varies on the two datasets. That is because the number of candidate patterns changes greatly when the support threshold varies. With BMSWebView-1, Apriori is close to MIndexing That is because Apriori does not generate many candidate patterns on the dataset that contains fewer items, which means resulting to a relativ ely less runtime. However with BMS-WebView-2, Apriori is close to FP-growth It generates more candidate patterns on this dataset which contains more items To a certain extent, the runtime of Apriori depends on the number of generated candidate patterns. With these two sparse datasets, the average transaction length is small. It does not cost much main memory and CPU computation charges for MIndexing to find all frequent patterns. So it is the most efficient algorithm on these datasets Comparatively, the average transaction length of retail is much larger than that of other datasets. It is relatively less sparse datasets With this dataset MIndexing need more memory space and CPU computation charges to accommodate the resource costing when MIndexing constructs projected databases and the trie structure to store frequent patterns, so its performance declines On the other three sparse datasets, i.e., T3I2D100K T4I2D100K and T5I2D100K, our experimental results show that MIndexing algorithm performs better than the other two algorithms With these datasets, the average transaction size is 3, 4 and 5 respectively. However, they may contain long frequent patterns whose length may be at 7 or 8 under the support thresholds we specify. A sparse dataset implicates that the long transactions are still few. So are the long frequent patterns. In this case MIndexing does not cost much resource to generate all frequent patterns MIndexing algorithm is very efficient when processing th is type of datasets that contain long but few frequent patterns. Apriori may generate too much candidate patterns when finding the long patterns. To find all frequent patterns, FP-growth method need to construct a large FP-tree. The large resource costing brings on a worse performance To find frequent patterns, FP-growth method needs to construct FP-tree in main memory. The FP-tree becomes very large when processing sparse datasets Therefore, in our experiment, FP-growth method is the most inefficient algorithm Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


  Fig 3  Computational performance MIndexing 5. Conclusions In this paper, we propose an efficient algorithm based the memory indexing technique for finding the complete set of frequent patterns, especially in the case of processing sparse datasets. With the experiment results we show that our algorithm can process sparse datasets efficiently that contain long but few patterns For very large datasets, we adopt a partitioning-based strategy to decompose the mi ning task into a set of smaller tasks for mining frequent patterns References  Agrawal, R., and Srikant, R., Fast Algorithms for Mining Association Rules, Proc. 20 th Int\222l Conf. on Very Large Data Bases \(VLDB\22294   Agrawal R Im ielinski, T., and Swam i, A., Mining Association Rules between Sets of Items in Large Databases Proc. 1993 ACM-SIGMOD Int\222l Conf. on Management of Data \(SIGMOD\22293\ngton, DC, 1993, pp.207-216   R Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. Fast discovery of association rules. Advances in Knowledge Discovery and Data Mining, MIT Press, 1996 pp.307-328  R.J. Bay ardo, Jr. Efficiently  mining long patterns from databases. Proc. 1998 ACM SIGMOD International Conference on Management of Data, volume 27\(2\of SIGMOD Record, ACM Press, 1998, pp.85-93   S Brin R. Motwani, J.D. Ullm an, and S. Tsur. Dy nam ic itemset counting and implication rules for market basket data. Proc. 1997 ACM SIGMOD International Conference on Management of Data, volume 26\(2 ACM Press, 1997, pp.255-264   K Gouda and M. J. Zaki. Efficiently Mining Maximal Frequent Itemsets. Proc. IEEE Int. Conference on Data Mining, San Jose. IEEE Computer Society Press, 2001 pp.163-170  J. Han, J. Pei, and Y Yin. Mining frequent patterns without candidate generation. Proc. 2000 ACM SIGMOD International Conference on Management of Data, volume 29\(2  H. Toivonen. Sampling large databases for association rules. Proc. 22nd International Conference on Very Large Data Bases, Morgan Kaufmann, 1996, pp.134-145  Pei, J., J. Hart, and R. Mao. CLOSET an efficient algorithm for mining frequent closed itemsets. In Tenth International Workshop on Research Issues in Data Engineering: RIDE 2000. IEEE Computer Society Press, Los Alamitos, 2000,  pp.11-20  M.J. Zaki and C.-J. Hsiao. CHARM: An efficient algorithm for closed itemset mining. Proc. Second SIAM International Conference on Data Mining, 2002   MY Lin, Suh-Yin Lee. Fast Discovery of Sequential Patterns by Memory Indexing. Proc. 4th International Conference on Data Warehousing and Knowledge Discovery, 2002, Vol. 2454, pp.150-160   J Pei J Han H. Lu, S. Nishio, S. Tang, and D. Yang H-Mine: Hyper-structure mining of frequent patterns in large databases. Proc. 2001 Int. Conf. Data Mining \(ICDM'01 San Jose, CA, pp.441-448  Z. Zheng, R. Kohavi, and L Mason Real world performance of association rule algorithms. Proc. seventh ACM SIGKDD international conference on Knowledge discovery and data mining, ACM Press, 2001, pp.401-406 Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


was implemented that maintain ed an algorithm instance object in memory thereby preventing the Web Services infrastructure from serialising the obj ect at the completion of each invocation improvement in performance 4.6 Availability The data mining Web Services are hosted at the Welsh eScience Centre Information on how to invoke the services are available at 1 The T r i ana t ool ki t can be do wnl oaded under the GNU Public License To use the toolkit the Triana workow engine needs to be downloaded and installed Once installed the data mining toolkit can then be installed as a folder within Triana and can be downloaded from 3 Installation instructions are also provided A user can also add additional Web Services to the data mining toolbox Access to the UDDI registry for inquiry is available at http://agents-comsc.grid.cf.ac.uk:8334/juddi/inquiry 5 Case Study To demonstrate the use of the toolkit a case study is presented in this section Precisely we use the general classier Web Service to classify a data set The case study also shows how the various data mining tools can be used in conjunction with the classier Web Service for processing data and visualising the results 5.1 Environment Overview The Classier Web Service is deployed on a Windows platform The processor is Pentium 4 memory is 1 GB and network bandwidth is 1 Gb/s We use Tomcat 5.0 with Axis 1.2 for hosting the Web Service The data used in the case study is obtained from the UCI Machine Learning Repository The datas et contains information about a number of different breast cancer cases with patient data being anonymised It includes 201 instances of one class and 85 instances of another class The instances are described by 9 attributes some of which are linear and others nominal Information about the attributes and the instances in the breast cancer dataset is provided in Figure 3 The dataset is already in the ARFF le format Therefore we do not need to use any tool to convert the data format the Classier Web Service requires a dataset in the ARFF format 5.2 Extracting knowledge from the dataset In order to use the Classifer Web Service we rst need to obtain the available classiers that the Web Service supports and the options for the sel ected classication algorithm The supported classiers algorithms are obtained by Num Instances 286 Num Attributes 10 Num Continuous 0 Int 0  Real 0 Num Discrete 10 Missing values 9  0.3 name type enum ints real missing distinct 1 age Enum 100 0 0 0  0 6  2 2 menopause Enum 100 0 0 0  0 3  1 3 tumor-size Enum 100 0 0 0  0 11  4 4 inv-nodes Enum 100 0 0 0  0 7  2 5 node-caps Enum 97 0 0 8  3 2  1 6 deg-malig Enum 100 0 0 0  0 3  1 7 breast Enum 100 0 0 0  0 2  1 8 breast-quad Enum 100 0 0 1  0 5  2 9 irradiat Enum 100 0 0 0  0 2  1 10 Class Enum 100 0 0 0  0 2  1 Figure 3 Information about the Breast cancer data Figure 4 Visualising the C4.5 decision tree for the beast-cancer data set invoking the getClassiers operation For our case study we select the J48 classier The options that the J48 requires are obtained by invoking the getOptions operation Once the classier algorithm and the options have been identied we can invoke the classifyInstance operation 5.3 Results The result of the classication in this instance is viewed in a text viewer Subsequently the decision tree can be plotted using the TreeVisualizer tool Figure 4 shows the results obtained from the Classier Web Service In this instance the attribute node-caps has been chosen to lie at the root of the tree The attribute selection process can also be automated through the use of a genetic search service This example involved the use of four Web Services 1 a Web Service to read the data le from a URL and convert Proceedings of  the 2005 International Conference on Parallel Processing Workshops \(ICPPW05 1530-2016/05 $20.00  2005 IEEE 


this into a format suitable for analysis 2 a Web Service to perform the classication i.e one that implements the C4.5 classier  3 a Web Service to analyse the output generated from the decision tree and 4 a Web Service to visualise the output These Web Services are implemented as standalone service that can be composed using the Triana workow tool 5.4 Conclusion A data mining toolkit is described that enables composition of Web Services from a pre-dened toolbox Web Services have been developed from the Java-based templates provided in the WEKA library of algorithms Using a workow based enactor data analysis can be undertaken on both local and remote data sets A variety of additional services to facilitate the entire data mining process are also supported for data translation visualisation and session management A case study is reported which demonstrate the use of the toolkit on publicly available data sets at the UCI Machine Learning repository Work is underway to include access to relational databases through the OGSA-DAI services available in GridMiner References  Federated Analys is En vironment for Heterogeneous Intelligent Mining Web site at http://users.cs.cf.ac.uk/Ali.Shaikhali/faehim Last viewed March 2005 2 M ath ematica A v ailab le at http://www.wolfram.com/products/mathematica index.html Last viewed March 2005 3 T h e T r ian a P ro b l em So lv in g E n v iro n m en t A v ailab le at http://www.trianacode.org Last viewed March 2005  The W EKA t ool ki t  Uni v ers i t y of W a i k at o Available at http://www.cs.waikato.ac.nz/ml/weka Last viewed November 2004   UC I Machine Learning R e pos itory  S ee W e b s ite at http://www.ics.uci.edu mlearn MLRepository.html  M Ant oni ol et t i and M J ackson OGSA-DAI Product Overview http://www.ogsadai.org.uk/docs/current/OGSADAI-USER-UG-PRODUCT-OVERVIEW.pdf   P  B rezan y  J  Hofer  A  M  Tjoa and A W oehrer  T owards an open service architecture for data mining on the grid Conference on Database and Expert Systems Applications  September 2003  V  C urcin M Ghanem Y  Guo M K ohler  A  R o w e J Syed and P Wendel Discovery Net Towards a Grid of Knowledge Discovery Proceedings of KDD2002 The 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining July 2002  M C  Gomes  O  F  R ana and J  C  C unha P a ttern operators for grid environments Journal of Scientic Programming  IOS Press August 2004  S  Graham S  S i meono v  T  B oubez D Da vis  G Daniels Y Nakamura and R Neyama Building Web Services with Java Making sense of XML SOAP WSDL and UDDI Sams Publishing  2002  R  Khous s a i n o v  X  Z uo and N K u s hmeri ck Gri d enabled Weka A Toolkit for Machine Learning on the Grid ERCIM News  No 59 October 2004  B  Mann S p eci al Int eres t G roup on eScience Data Mining See Web site at http://www.nesc.ac.uk/resources/sigs/esdmsig/index.html Proceedings of  the 2005 International Conference on Parallel Processing Workshops \(ICPPW05 1530-2016/05 $20.00  2005 IEEE 


can test whether the hyperlinks on a page have been placed appropriately by analyzing significant navigational patterns \(association rules In our experiments the percentage of hyperlinks confirmed by rules was calculated by dividing the number of common items in hyperlink sets and whole ranking lists for a given page, by the number of hyperlinks on the page separately for direct, indirect and complex rules \(Fig. 8 Note that the number of hyperlinks was put in the denominator as opposed to calculations in section 7.4 where it was ranking length 48 89%87 0 20 40 60 80 100 direct indirect complex Figure 8. The average percentage of all hyperlinks confirmed by rules Proceedings of the 2005 5th International Conference on Intelligent Systems Design and  Applications \(ISDA  05 0-7695-2286-06/05 $20.00  2005 IEEE The average percentage of hyperlinks confirmed by direct rules amounted to only 48%, probably because there were too few of them. Indirect and complex rules, on the other hand, confirmed many more hyperlinks  87% and 89%, respectively, due to their larger quantity. These relatively great values that we received may have resulted from the enormous differences between the average number of hyperlinks on a page  10 and the average ranking length: 51, 177, 180 for direct, indirect and complex rules respectively. Concluding, indirect and complex rules appear to be better at assessing the usefulness of hyperlinks compared to direct rules Note that in any case at least 11% of hyperlinks were not confirmed by any rule, so they may be recommended to be removed from the content of pages 8. Conclusions and future work Complex rules combining both direct and indirect rules usually increase the length of rankings compared to those based on direct associations. This helps overcome the problem of a multitude of pages with too short rankings Fig. 5 quested ranking length \(Fig. 6 rules substantially change the order of ranking lists \(Fig. 2 and 3 greater extent only confirm hyperlinks existing on web pages compared to lists extracted from complex rules, for short and long ranking lengths \(Fig. 7 of rules, especially indirect and complex ones, can be useful for the assessment of hyperlinks Concluding, far more diverse indirect rules can significantly improve potential value of recommendation. Nevertheless, to confirm the usefulness of indirect rules for end users, some tedious tests with their participation are required Acknowledgements The authors are indebted to thank Marcin Pilarczyk for providing cleansed data about hyperlinks on WUT pages 9. References 1] Agrawal R., Imieli?ski T., Swami A.: Mining association rules between sets of items in large databases. ACM SIGMOD Int. Conference on Management of Data, ACM Press 1993 2] Boley D., Gini, M., Gross, R., Han, E.H., Hastings, K Karypis, G., Kumar, V., Mobasher, B., Moorey, J.: Document Categorization and Query Generation on the World Wide Web Using WebACE. Artificial Intelligence Review 


Wide Web Using WebACE. Artificial Intelligence Review 13 \(5-6 1999 3] Cho Y.H., Kim J.K., Kim S.H.: A personalized recommender system based on web usage mining and decision tree induction. Expert Systems with Applications 23 \(3 2002 4] Chun J., Oh J.-Y., Kwon S., Kim D.: Simulating the Effectiveness of Using Association Rules for Recommendation Systems. AsiaSim 2004. LNCS 3398, Springer Verlag 2005 5] G  ry M., Haddad M.H.: Evaluation of web usage mining approaches for user's next request prediction. WIDM 2003 ACM Press \(2003 6] Ha S.H.: Helping Online Customers Decide through Web Personalization. IEEE Intelligent Systems 17 \(6 2002 43 7] Hamano S., Sato M.: Mining Indirect Association Rules ICDM 2004. LNCS 3275, Springer Verlag \(2004 8] Kazienko P.: IDARM - Mining of Indirect Association Rules. IIS: IIPWM  05. Advances in Soft Computing Springer Verlag \(2005 9] Kazienko P.: Multi-agent Web Recommendation Method Based on Indirect Association Rules. KES  2004. Part II LNAI 3214, Springer Verlag \(2004 10] Kazienko P., Product Recommendation in E-Commerce Using Direct and Indirect Confidence for Historical User Sessions. DS  04. LNAI 3245, Springer Verlag \(2004 269 11] Kazienko P., Kiewra M.: Link Recommendation Method Based on Web Content and Usage Mining. IIS: IIPWM  03 Advances in Soft Computing, Springer Verlag \(2003 534 12] Kazienko P., Kiewra M., Personalized Recommendation of Web Pages. Chapter 10 in: Nguyen T. \(ed Technologies for Inconsistent Knowledge Processing. Advanced Knowledge International, Adelaide, South Australia 2004 13] Kazienko P., Kolodziejski P.: WindOwls - Adaptive System for the Integration of Recommendation Methods in Ecommerce. AWIC  05, LNAI, Springer Verlag \(2005 14] Kazienko P., Matrejek M.: Adjustment of Indirect Association Rules for the Web. SOFSEM 2005. LNCS 3381 Springer Verlag \(2005 15] Kendall, M. G.: Rank correlation methods. London: Charles Griffin &amp; Company, Ltd., London \(1948 16] Lu Z., Yao Y., Zhong N.: Web Log Mining. Chapter 9 in Zhong N., Liu J., Yao Y. \(eds Berlin, New York \(2003 17] Mobasher B., Cooley R., Srivastava J.: Automatic Personalization Based on Web Usage Mining. Communications of the ACM, 43 \(8 2000 18] Tan P.-N., Kumar V.: Mining Indirect Associations in Web Data. WEBKDD 2001. LNCS 2356, Springer Verlag \(2002 145-166 19] Tan P.-N., Kumar V., Srivastava J.: Indirect Association Mining Higher Order Dependencies in Data. PKDD 2000 LNCS 1910, Springer Verlag \(2000 20] Wan Q., An A.: Efficient Mining of Indirect Associations Using HI-Mine. AI 2003. LNCS 2671, Springer Verlag 2003  221 21] Wang D., Bao Y., Yu G., Wang G.: Using Page Classification and Association Rule Mining for Personalized Recommendation in Distance Learning. ICWL `02. LNCS 2436 Springer Verlag \(2002 22] Yang H., Parthasarathy S.: On the Use of Constrained Associations for Web Log Mining. WEBKDD 2002. LNCS 2703 Springer Verlag \(2003  118 Proceedings of the 2005 5th International Conference on Intelligent Systems Design and  Applications \(ISDA  05 0-7695-2286-06/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


lossless frequent patterns representations with borders. In practice, they are also much more concise than representations based on closed itemsets and approximate representations. In this paper, we offered three methods of deriving an upper bound on the length of sets in such representations. We proved that the upper bound on the length of a longest generalized disjunction-free set depends logarithmically on the number of records in the database. The obtained result is of high importance as it guarantees that any generalized disjunction-free set representation for all patterns \(both frequent and infrequent scans, where n is the number of records in the database irrespectively how strong or weak correlations among items in the database are and irrespectively of the lengths of records and number of distinct items The modifications of the basic estimation take into account the support threshold of the representation to be found or, additionally, the information on the length of longest sets of the representation already calculated for a higher support threshold. Though these estimations are more accurate than the basic one, they are still quite rough. Further improvements of the quality of estimating the length of longest itemsets in generalized disjunctionfree representations is subject to further research References 1] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, A.I Verkamo  Fast Discovery of Association Rules   Advances in Knowledge Discovery and Data Mining AAAI, CA, 1996 2] J. Baptiste, J.-F. Boulicaut  Constraint-Based Discovery and Inductive Queries: Application to Association Rule Mining  Pattern Detection and Discovery, Springer London, UK, September 2002, pp. 110-124 3] E. Baralis, S. Chiusano, P. Garza  On Support Thresholds in Associative Classification  SAC, ACM, Taipei Taiwan, March 2004, pp. 553-558 4] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, L. Lakhal  Mining Minimal Non-redundant Association Rules Using Frequent Closed Itemsets  Computational Logic, 2000 pp. 972  986 5] J.-F. Boulicaut, A. Bykowski, C. Rigotti  Approximation of Frequency Queries by Means of Free-Sets  PKDD Springer, Lyon, France, September 2000, pp. 75-85 6] A. Bykowski, C. Rigotti  A Condensed Representation to Find Frequent Patterns  PODS, ACM, Santa Barbara USA, May 2001, pp. 267-273 7] T. Calders, Axiomatization and Deduction Rules for the Frequency of Itemsets, Ph.D. Thesis, University of Antwerp, 2003 8] T. Calders, B. Goethals  Mining All Non-derivable Frequent Itemsets  PKDD, Springer, Helsinki, Finland August 2002, pp. 74?85 9] T. Calders, B. Goethals  Minimal k-free Representations of Frequent Sets  ECML/PKDD, Springer, CavtatDubrovnik, Croatia, September 2003, pp. 71-82 10] J. Han, M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers, 2000 11] S.K. Harms, J. Deogun, J. Saquer, T. Tadesse  Discovering Representative Episodal Association Rules from Event Sequences Using Frequent Closed Episode Sets and Event Constraints  ICDM, IEEE Computer Society, San Jose, California, USA, November-December 2001, pp. 603  606 12] M. Kryszkiewicz  Closed Set Based Discovery of Representative Association Rules  IDA, Springer Cascais, Portugal, September 2001, pp. 350-359 13] M. Kryszkiewicz  Concise Representation of Frequent Patterns based on Disjunction  free Generators  ICDM IEEE Computer Society, San Jose, California, USA 


IEEE Computer Society, San Jose, California, USA November-December 2001, pp. 305  312 14] M. Kryszkiewicz M  Inferring Knowledge from Frequent Patterns  Soft-Ware, Springer, Belfast, Northern Ireland April 2002, pp. 247  262 15] M. Kryszkiewicz  Concise Representations of Association Rules  Pattern Detection and Discovery, Springer London, UK, September 2002, pp. 92-109 16] M. Kryszkiewicz, Concise Representation of Frequent Patterns and Association Rules, Habilitation Thesis Publishing House of Warsaw University of Technology 2002 Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE 17] M. Kryszkiewicz  Reducing Infrequent Borders of Downward Complete Representations of Frequent Patterns  Proc. of The First Symposium on Databases Data Warehousing and Knowledge Discovery, Scientific Publishers OWN, Baden-Baden, Germany, July, 2003, pp 29-42 18] M. Kryszkiewicz  Closed Set Based Discovery of Maximal Covering Rules  International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems Vol. 11. World Scientific Publishing Company, Singapore September 2003, pp. 15-29 19] M. Kryszkiewicz  Reducing Borders of k-Disjunction Free Representations of Frequent Patterns  SAC, ACM Taipei, Taiwan, March 2004, pp. 559  563 20] M. Kryszkiewicz  Reducing Main Components of k-Disjunction Free Representations of Frequent Patterns   Proc. of IPMU  2004 \(in print 21] M. Kryszkiewicz, M. Gajek  Concise Representation of Frequent Patterns based on Generalized Disjunction  Free Generators  PAKDD, Springer, Taipei, Taiwan, May 2002, pp. 159-171 22] M. Kryszkiewicz, M. Gajek  Why to Apply Generalized Disjunction-Free Generators Representation of Frequent Patterns  ISMIS, Springer, Lyon, France, June 2002, pp 383  392 23] M. Kryszkiewicz, H. Rybi?ski, M. Gajek  Dataless Transitions between Concise Representations of Frequent Patterns  Journal of Intelligent Information Systems JIIS Netherlands, 2004, pp. 41-70 24] N. Pasquier, Data mining: Algorithmes d  extraction et de R  duction des R  gles d  association dans les Bases de Donn  es, Th  se de Doctorat, Universit  Blaise Pascal   Clermont  Ferrand II, 2000 25] N. Pasquier, Y. Bastide, R. Taouil, L. Lakhal  Efficient Mining of Association Rules Using Closed Itemset Lattices  Journal of Information Systems, Vol. 24, No. 1 1999, pp. 25  46 26] N. Pasquier, Y. Bastide, R. Taouil, L. Lakhal  Discovering Frequent Closed Itemsets for Association Rules  ICDT, Springer, Jerusalem, Israel, January 1999 pp. 398  416 27] J. Pei, G. Dong, W. Zou, J. Han  On Computing Condensed Frequent Pattern Bases  ICDM, IEEE Computer Society, Maebashi City, Japan, December 2002 pp. 378-385 28] V. Phan-Luong V  Representative Bases of Association Rules  ICDM, IEEE Computer Society, San Jose California, USA, November-December 2001, pp. 639-640 29] A. Savinov  Mining Dependence Rules by Finding Largest Itemset Support Quota  SAC, ACM, Taipei Taiwan, March 2004, pp. 525-529 30] M.J. Zaki, C.J. Hsiao  CHARM: An Efficient Algorithm for Closed Itemset Mining  SIAM, Arlington, 2002 


for Closed Itemset Mining  SIAM, Arlington, 2002 Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





