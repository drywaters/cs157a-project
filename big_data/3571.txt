Compression of Hyperspectral Images with LVQ-SPECK Alessandro J S Dutra William A Pearlman ECSE Department Rensselaer Polytechnic Institute  Troy NY  12180 a.dutra@ieee.org pearlw@ecse.rpi.edu Eduardo A B da Silva PEE/COPPE DEL Universidade Federal do Rio de Janeiro eduardo@lps.ufrj.br Abstract We discuss the use of lattice vector quantizers in conjuncti on with a quadtree-based sorting algorithm for the compression of multidimensional data set s as encountered for example when dealing with hyperspectral imagery An extension of the SPE CK algorithm is presented that deals with vector samples and is used to encode a group of succ essive spectral bands extracted from the hyperspectral image original block We evaluate th e importance of codebook choice by showing that a choice of dictionary that better matches th e characteristics of the source during the sorting pass has as big an in\003uence in performance as the use of a transform in the spectral direction Finally we provide comparison aga inst state-of-the-art encoders both 2D and 3D ones showing the proposed encoding method is very c ompetitive especially at small bit rates We discuss the use of lattice vector quantiz ers in conjunction with a quadtreebased sorting algorithm for the compression of multidimens ional data sets as encountered for example when dealing with hyperspectral imagery An exten sion of the SPECK algorithm is presented that deals with vector samples and is used to encod e a group of successive spectral bands extracted from the hyperspectral image original bloc k We evaluate the importance of codebook choice by showing that a choice of dictionary that b etter matches the characteristics of the source during the sorting pass has as big an in\003uence in performance as the use of a transform in the spectral direction Finally we provide c omparison against state-of-the-art encoders both 2D and 3D ones showing the proposed encoding method is very competitive especially at small bit rates I I NTRODUCTION The compression of hyperspectral images has been given a lot of attention in recent years due not only to the often sensitive nature of the acqui red information but also because of the usually large amount of data needed to represe nt it Methods spanning from direct quantization of spectral values 1 t o t h o s e t h a t employ the discrete wavelet transform 2 a s a d e c o r r e l a t i n g s t e p w e r e d e v e l o p e d  p r o v i ding good compression capabilities along with good quality representation even loss less if desired In 1  M o t t a e t a l  d e 002 n e a p a r t i t i o n o f t h e s p e c t r a l s p a c e w h ose boundaries are optimized by repeated application of a Generalized Lloyd Al gorithm 3  G L A  v a r i a n t  Considering the original data set to have a dimension D  the design of a D dimensional 
Data Compression Conference 1068-0314/08 $25.00 © 2008 IEEE DOI 10.1109/DCC.2008.88 93 


vector quantizer which is usually computationally prohib itive would be required Instead the method chooses to design N vector quantizers each with dimension d i  where P N i 0 d i  D  The resulting Partitioned Vector Quantizer is then the Cart esian product of all the lower dimensional dictionaries In order to remove p art of the remaining source redundancy each resulting vector quantization VQ index is also conditionally entropy encoded based on a causal set of spatially and spectrally adj acent indices As opposed to the previously described method which encode s the spectral band intensity values directly a number of methods that apply a d ecorrelating transform were developed In 2  a 3 D v e r s i o n o f t h e q u a d t r e e b a s e d c o d e c S PECK 4 w a s i n t r o d u c e d  3D-SPECK takes small portions of the hyperspectral block e g 16 spectral bands at a time applies a 3D discrete wavelet transform DWT and exte nds the concept of partitioning sets and rules to the three dimensional case G iven the energy compaction properties of the DWT and SPECK's ef\002ciency in the coding of si gni\002cance information the method achieves very good compression results The compression algorithm herein proposed is a variant of th e original 2D-SPECK tailored to deal with multidimensional data In particular  if we consider each spectral vector as a multidimensional pixel the encoding steps ar e exactly the same the only changes being the de\002nition of vector signi\002cance against a threshold the existence of a lattice-based vector codebook and a threshold scaling fact or 013  Being a successive approximation based method our vector-based extension of SPECK re tains those characteristics that make this class of encoders a very successful one such a s the embeddedness of the bit stream along with its quality/rate scalability This article is organized as follows Section II presents th e basics of successive approximation methods based on lattice vector quantizers Th e encoding algorithm and its differences to the basic scalar method are described in se ction III while the results obtained in the compression of standard AVIRIS hyperspectral i mages appears in section IV Lastly section V presents our conclusions and perspective s of future work II S UCCESSIVE APPROXIMATION C ODING OF V ECTORS A structured method for successive re\002nement of vectors was presented by Mukherjee and Mitra 5   6   i n w h i c h s c a l e d v e r s i o n s o f a g i v e n l a t t i c e are used as quantizers over each step of the approximation process Voronoi region enco ding is the basic operation in this framework and it is performed according to the follo wing concepts 017 Base lattice  003 1  lattice coset from which the codebook is actually derived 017 Shape lattice  003 0  higher scale lattice which determines the shape of the cod ebook The resulting quantizer called Voronoi Lattice Vector Quantizer  is therefore de\002ned as VLVQ\(\003 0  003 1   V 0 003 0   003 1 1 where V 0 003 0  is the zero-centered Voronoi region associated with the lat tice The shape lattice is de\002ned so that it covers the n-dimensional region of support of the data source and in the most common case the base lattice is just a scaled down and possibly translated version of the shape lattice i.e 003 1  003 0 r 000 t  2 t being the translation vector 
94 


Following a different approach da Silva and Craizer 7 s h o w ed that successive approximation of vectors under certain conditions is guara nteed to converge in a 002nite amount of time Formally a vector v is said to be successively approximated by a sequence of codewords u l if the summation v  1 X l 0 013 l u l  3 u l 2 C  f c 0  c 1   c K g converges where C is the codebook c k are the codewords and 013 is a scaling factor to account for the fact that after each interaction the residua l error is bound by a smaller N dimensional hypersphere For every codebook and codewor d dimension there is a choice often empiric of 013 that proves to be optimal i.e that provides the best representation results Since in lossy coding we are interested only in obtaining a cl ose enough approximation of the original data that is with a limited amount of error a 002nite summation is used instead of the in\002nite one resulting in v L  L X l 0 013 l u l  4 In the proposed vector version of the SPECK algorithm the abo ve approximation is done by choosing at each encoding pass the one codeword tha t best represents the residual error between the original data and its current rec onstructed version For the experiments reported herein the codebooks were de\002ned base d on the 1 s t and 2 n d shells of the D 4 lattice with the codewords being properly normalized to un it length Henceforth we will refer to those codebooks as D 4 shell-1 and D 4 shell-2 III LVQ-SPECK The proposed encoding method is based on the SPECK algorithm  4  a n d c o n s i s t s o f a vector extension of its principles to account for the need to work with multi-dimensional samples We will now present a description of the algorithm pointing out the main differences between the vector and scalar case For more det ails about the original scalar method the reader is referred to 4  LVQ-SPECK applies a DWT to each of the scalar bands generating a group of adjacent data sets containing transform coef\002cients We de\002ne a Group Of Images GOI as this set of adjacent transformed spectral bands b i being encoded Figure 1 shows how a vector sample v  x y  is de\002ned for a given GOI of dimension 4 Hence for each spati al coordinate we have v  x y    b n  x y   b n 1  x y   b n 2  x y   b n 3  x y   5 where each component belongs to a distinct spectral band Si nce we are now dealing with vector quantities the signi\002cance measure used will b e de\002ned by comparing the vector's norm against the current encoding threshold T n  that is 000 n  T    1  if max x  y 2T k v x  y k\025 T n 0  otherwise  6 
95 


    b n 2 b n 3 b n b n 1 a Group of Images  spectral bands  b n 2 b n 3 b n b n 1 b 4-dimensional vector sample v  x y  Figure 1 Spectral band Group of Images GOI for encoding As in the scalar case the initial threshold is de\002ned based o n the largest value to be encoded which in this case is the largest norm among all th e transform vectors However the threshold scaling rate is no longer restricted to 1  2  as previously described in Section II The original SPECK algorithm de\002nes two classes of partition ing sets S and I shown on Figure 2 used to convey the signi\002cance information of a group of samples Initially the S set is de\002ned to be the set comprised of the low-low frequency sub-band coef\002cients of the wavelet transform with the I set accounting for all remaining coef\002cients The encoding steps follow those of the original algorithm w ith changes to account for the encoding of vector samples 1 Initialization 017 Partition image transform X into S and I  X 000 S sets 017 The initial threshold T 0 and the threshold scaling factor 013 are transmitted 017 Add S to the LIS and set LSP    2 Sorting pass 017 for each set S 2 LIS  and in increasing order of size jSj  do ProcessS  S   017 if I 6    ProcessI 3 Re\002nement pass 017 for each  x y  in the LSP if the residual norm is larger than the current thr eshold output the index of the codeword that best represents it  This procedure is the equivalent of adding a new term to the summation in Eq 4  Otherwise output the zero-codeword index since there is no re\002nement to take place 4 Quantization step 017 update the encoding threshold i.e set T n  013 002 T n 000 1  and go to step 2 
96 


I S a Initial S and I sets S S S I I b I set partitioning S O  S  S 0 S 2 S 1 S 3 c S-set partitioning Figure 2 Set de\002nition and partitioning for the SPECK algorithm The procedures involved in the encoding/decoding process a re de\002ned as follows 017 ProcessS S  1 output 000 n  S  2 if 000 n  S   1  if S corresponds to a pixel then output its codebook index and ad d S to the LSP  else CodeS S   if S 2 LIS then remove S from LIS 3 else if S  2 LIS then add S to LIS 4 return 017 CodeS S  1 partition S into four equal subsets O  S  2 for each S i 2 O  S   output 000 n  S i   1  if 000 n  S i   1 003 if S i corresponds to a pixel then output its codebook index and ad d S i to the LSP 003 else CodeS S i   else add S i to LIS 3 return 017 ProcessI 1 output 000 n  I  2 if 000 n  I   1  then CodeI 017 CodeI 1 partition I into three sets S i and one I see Fig 2 2 for each S i  do ProcessS  S i  3 ProcessI 4 return Examination of the algorithm shows us that the encoding powe r of LVQ-SPECK stems from the fact that it sorts out those vectors with larger magn itude and immediately starts sending information about their spatial location and orien tation on the n-dimensional hypersphere Subsequent passes provide re\002nement informa tion further reducing the 
97 


reproduction distortion It is also worth noticing that as in the original scalar SPECK codec the generated bit stream is still an embedded one Comparison with 3D transform coding As discussed in 8  t h e p e r f o r m a n c e o f a n i m a g e c o m p r e s s i o n algorithm based on successive approximation of vectors is not only dependent o n how well distributed in space the codewords are but also on how well-matched those a re to the source ndimensional statistics in terms of correlation For each vector to be quantized the better the 002rst approxim ation step the least amount of residue will remain and therefore a smaller number of re\002n ing steps will be needed resulting in better overall performance As such in the pre sent case it is of paramount importance that the quantizer used during the sorting pass p ossesses characteristics that are similar to those of the source being compressed During t he remaining re\002nement steps this requirement becomes less stringent since the re sidue's statistics tend to be more uniformly distributed over the n-dimensional space m aking the packing properties of the lattice more important When one applies a unitary transform A across the spectral dimension thereby inducing an orthonormal basis rotation there generally is a reducti on in the correlation among the components of the source vectors but the euclidean distanc es are maintained due to the norm preserving property Therefore as far as the 002rst appr oximation pass is concerned matching the transformed by A  source vectors to a codebook is equivalent to matching the original source vectors to an appropriately transforme d by A 003 T  codebook In subsequent passes since the residuals tend to have orien tations evenly distributed over an hypersphere what matters is just the relative orien tation of the vectors in the codebooks 8  a n d t h e r e f o r e a l l c o d e b o o k r o t a t i o n s h a v e e q uivalent performance from the second pass on This implies that the codebook of choice s hould be the one whose vectors best match the spectral features of the source In th e next section we validate the above assumption by comparing the performance of LVQ-SP ECK with and without a transform a 4-point DCT in the spectral dimension using di fferent rotated versions of a codebook IV E XPERIMENTAL R ESULTS The LVQ-SPECK algorithm was used to compress scenes of the AVI RIS hyperspectral images Moffet Field and Jasper Ridge obtained from http aviris.jpl.nasa.gov both cropped to 512 002 512 002 224 A pre-processing step was added to remove all the zero-e nergy spectral bands from the hyperspectral block with the indic es of those bands being sent as negligible overhead The spectral bands were then groupe d into 4-dimensional blocks to be encoded The DWT kernel used was the 9/7 wavelet 10  a n d a 5 s t a g e t r a n sform was applied to each spectral band Bit allocation across sub-bands is don e implicitly based on the signi\002cance of each vector being encoded Each signi\002cance test accounts for one bit in the 002nal bit-stream and since both 4-dimensional codebook s used contain 24 vectors in the worst case vector index transmission will demand log 2 24  4  59 bits during the sorting pass and log 2 25  4  64 bits during the re\002nement ones to account for the zero codeword 
98 


Table I A VERAGE SNR  IN D B FOR AVIRIS H YPERSPECTRAL I MAGES   VALUES IN PARENTHESIS INDICATE AVERAGE RMSE Jasper Ridge scene 01 Rate bpppb 0.1 0.2 0.5 1.0 3D-SPIHT[9 19.59 23.59 31.48 38.36 3D-SPECK[9 19.7 23.66 31.75 38.55 JPEG2000 Multi Component[9 18.25 22.17 29.81 36.63 LVQ-SPECK D4-sh1 15.31 244.04 17.13 201.94 20.41 143.66 24.18 96.16 LVQ-SPECK D4-sh1  DCT 15.41 241.98 17.22 200.98 20.63 140.85 24.43 94.33 LVQ-SPECK D4-sh2 17.72 189.65 20.39 144.85 25.65 84.81 31.61 46.27 LVQ-SPECK D4-sh2  DCT 17.71 190.10 20.38 145.16 25.65 85.51 31.39 47.96 2D-SPECK 14.60 262.04 16.23 221.09 19.28 161.13 22.78 111.84 Moffet Field scene 01 Rate bpppb 0.1 0.2 0.5 1.0 3D-SPIHT[9 16.57 21.46 29.88 38.54 3D-SPECK[9 16.67 21.52 29.91 38.60 JPEG2000 Multi Component[9 15.29 19.92 28.19 36.56 LVQ-SPECK D4-sh1 18.46 344.50 20.15 301.50 23.35 232.03 27.18 162.27 LVQ-SPECK D4-sh1  DCT 18.47 343.74 20.32 294.76 23.52 223.05 27.30 153.57 LVQ-SPECK D4-sh2 20.82 283.46 23.39 232.48 28.69 156.40 35.00 97.61 LVQ-SPECK D4-sh2  DCT 20.80 284.96 23.33 235.37 28.58 161.71 34.67 104.33 2D-SPECK 17.76 360.76 19.47 311.04 22.45 239.15 25.86 173.81 Moffet Field scene 03 Rate bpppb 0.1 0.2 0.5 1.0 3D-SPIHT[9 12.92 18.25 27.28 35.62 3D-SPECK[9 12.60 17.98 26.99 35.37 JPEG2000 Multi Component[9 10.79 16.81 25.82 33.44 LVQ-SPECK D4-sh1 15.35 327.59 17.71 278.08 22.57 200.61 27.74 138.41 LVQ-SPECK D4-sh1  DCT 15.38 324.37 18.10 266.58 22.79 191.01 28.02 127.46 LVQ-SPECK D4-sh2 18.72 255.20 22.51 200.99 29.40 135.59 35.16 91.86 LVQ-SPECK D4-sh2  DCT 18.57 260.46 22.30 207.35 28.81 145.90 33.86 103.44 2D-SPECK 14.61 339.25 16.89 283.77 21.34 202.63 26.43 139.21 Table I presents a comparison among the reconstruction resu lts for each of the hyperspectral blocks considered when processed by LVQ-SPECK  the 3D-SPIHT and 3DSPECK algorithms 9  M u l t i Co m p o n e n t f e a t u r e o f J P E G 2 0 0 0  1 1   a n d t h e o r i g i n a l 2D-SPECK codec applied to each of the spectral bands individu ally The 002gure of merit utilized here is the signal-to-quantization noise ratio S NR de\002ned as SNR  10 log 10 P x MSE dB 7 where P x is the power of the original signal and MSE is the reproductio n mean-squared error Table I contains RMSE results for our simulations as we ll Based on Table I we see that the performance attained by the LV Q-SPECK algorithm is quite competitive especially at low bit rates where it o utperforms by a large margin even 3D-based codecs when applied to scenes of the Moffet Fi eld image That is in fact quite impressive considering that in the case of 3D al gorithms the decorrelating transform across the spectral direction has length 16 comp ared to a vector of dimension 
99 


4 in our case It also shows that in the early phases of the app roximation process LVQSPECK does a better job of representing the transformed data As encoding rates go up however 3D-based codecs generally present better resu lts given the high degree of energy compaction provided by the additional transform On e possible way of improving the performance of LVQ-SPECK would be to process a larger numb er of adjacent spectral bands together and use higher dimensional lattices with be tter packing properties such as the 003 24 lattice 12  T h a t i s q u i t e i m p o r t a n t  s i n c e d u r i n g t h e r e 002 n ement steps the residues to be encoded are of a highly uncorrelated nature a s discussed in Section III It is also clear from Table I that simultaneously encoding a g roup of spectral bands using LVQ-SPECK provides much better results than the indivi dual compression of each one of them For instance for a rate of 1.0 bpp there is an app roximate gain of 10dB in SNR for all the images tested It is worth mentioning that for all studied images the codebook based on the D4 shell-2 lattice was the one that prov ided the best performance However we may also say based on the presented results tha t the chosen rotation of the lattice codebook was suitable for the Moffet image but p erhaps not so suitable for Jasper which shows us that this method is quite promising p rovided one uses the proper lattice rotation Figure 3 presents a visual comparison between the original d ata and a reconstructed version at 0.2 bpppb for spectral band 48 of the Moffet Field image Scene 03 with the points from D 4 shell-2 lattice serving as codebook V C ONCLUSIONS We presented a new multidimensional image codec based on the SPECK state-ofthe-art algorithm that makes use of a lattice vector quanti zer codebook suited to the encoding of hyperspectral images We showed that the propos ed algorithm using a 2D DWT kernel applied independently to each spectral band atta ins performance comparable to those methods that employ a 3D transform and clearly outpe rforms the alternative of separately encoding each spectral band Moreover for lo w bit rates LVQ-SPECK produces reproduction results that are in some cases over whelmingly better than those of 3D encoding methods The importance of choosing the right codebook was discussed by comparing equivalent codebooks where one is a rotated version of the other It was veri\002ed that the version of the codebook which more closely matches the source characte ristics over the sorting pass is the one yielding best performance results This properly rotated codebook produced performance comparable to 3D wavelet coders Further improvements are expected with the simultaneous en coding of a larger number of spectral bands and the use of higher-dimensional lattice s e.g E 8  003 16 and 003 24  as the basis for the codebook and methods of fast computation to de termine the best rotation angle to de\002ne the codebook 
100 


a Original Data b Reconstructed at 0.2 bpppb Figure 3 Moffet Field Scene 03 
101 


A CKNOWLEDGMENTS This work was performed at Rensselaer Polytechnic Institute and was supported in part by Fundação CAPES Brazil under Grant No 1535-98/6 R EFERENCES 1 G  M o tta  F  R iz z o  a n d J  A  S to r e r   C o m p r e s s io n o f H y p e r s p e c tral Imagery in Proceedings of the Data Compression Conference  pp 333–342 March 2003 2 X  T a n g  W  A  P e a r lm a n  a n d J  W  M o d e s tin o   H y p e r s p e c tr a l im age compression using three-dimensional image coding in SPIE/IS&T Electronic Imaging 2003  vol 5022 of Proceedings of the SPIE  Jan 2003 3 A  G e r s h o a n d R  M  G r a y  Vector Quantization and Signal Compression  Kluwer Academic Publishers 1991 4 W  A  P e a r lm a n  A  I s la m  N  N a g a r a j a n d A  S a id   E f 002 c ie n t lo w complexity image coding with a setpartitioning embedded block coder Circuits and Systems for Video Technology IEEE Transactions on  vol 14 pp 1219–1235 2004 5 D  M u k h e r je e a n d S  K  M itr a   S u c c e s s i v e r e 002 n e m e n t la ttic e v e c to r quantization IEEE Transactions on Image Processing  vol 11 pp 1337–1348 Dec 2002 6 D  M u k h e r je e a n d S  K  M itr a   V e c to r S P I H T f o r e m b e d d e d w a v e le t video and image coding IEEE Transactions on Circuits and Systems for Video Technology  vol 13 pp 231–246 Mar 2003 7 E  A  B  d a S ilv a a n d M  C r a iz e r   G e n e r a liz e d b itp la n e s f o r e m b e d ded codes in Image Processing 1998 ICIP 98 Proceedings 1998 International Conference on  vol 2 pp 317–321 vol.2 1998 8 L  H  F o n te le s  R  C a e ta n o  a n d E  A  B  d a S ilv a   I m p r o v e d d ic tio n aries for generalized bitplanes-based matching pursuits video coding using ridgelets in Image Processing 2004 ICIP 04 2004 International Conference on  vol 2 pp 1129–1132 Vol.2 2004 9 X  T a n g a n d W  A  P e a r lm a n  Three-Dimensional Wavelet-Based Compression of Hyperspectral I mages  ch Hyperspectral Data Compression Kluwer Academic Publishers 2 006 10 M  A n to n in i M  B a r la u d  P  M a th ie u  a n d I  D a u b e c h ie s   I m a g e c oding using wavelet transform IEEE Transactions on Image Processing  vol 1 pp 205–220 April 1992 11 D  S  T a u b m a n a n d M  W  M a r c e llin  JPEG2000 Image Compression Fundamentals Standards and Practic e  The International Series in Engineering and Computer Science Kluwe r Academic Publishers 2002 12 J  H  C o n w a y a n d N  J  A  S lo a n e  Sphere Packings Lattices and Groups Grundlehren der mathematisch en Wissenschaften  Springer 3rd ed 1998 
102 


Robert Wright is a member of the faculty at the Hawaii Institute of Geophysics and Planetology He has degrees in remote sensing from the University of London and the Open University U.K He specializes in infrared radiometry and is Principal Investigator of several NASA-fundedprojects Philip Kyle is a Professor of Geochemistry at the New Mexico Institute of Mining and Technology Socorro NM He is also the director of the Mount Erebus Volcano Observatory which is supported by the Office of Polar Programs of the National Science Foundation He is currently co-editing with Clive Oppenheimer a special issue on Erebus volcano Antarctica for the Journal of Volcanology and Geothermal Research Jean-Christophe Komorowski is a volcanologist at the Institut de Physique du Globe de Paris IPGP CNRS UMR 7154 in Paris France He has a BA in geologyfrom Boston University USA and a MSc and PhD in volcanology from Arizona State University Tempe USA He has undertaken extensive field studies of volcanoes in the Lesser Antilles Mexico and in Africa Democratic Republic of Congo He was Director and Scientist-in-Charge of the IPGP Soufriere of Guadeloupe Volcano Observatory from 1997-2001 and Chairman of the UNDP-ISDR-UNOCHA Nyiragongo Scientific and Technical Advising Committee 2004-2006 for the United Nations He was one of the principal investigators of the EXPLORIS research project 20022006 funded by the European Commission He is a consultant for the IAEA on safety guidelines for volcanic hazards in site evaluation for nuclear power plants Dan Mandl is the Earth Observing 1 Mission Manager He is also the Principle Investigator on a NASA Earth Science Technology Office ESTO award entitled An Interoperable Sensor Architecture for Sensor Webs in Pursuit of GEOSS and a Co-Investigator on a NASA ESTO award entitled Using Intelligent Agents to Form a Sensor Webfor Autonomous Operations Stuart Frye is senior systems engineer with over 27 years experience on NASA satellite development launch and on-orbit operations Mr Frye has primarily worked supporting Earth science goals in remote sensing observations of land atmospheres and oceans as the lead interface between the science community and the satellite systems He was Secretariat for the International Committee on Earth Observing Satellites CEOS Working Group on Calibration and Validation from January 2004 through November 2005 He was co-winner of the 2005 NASA Software of the Year award for his role in the Autonomous Sciencecraft Experiment on-board the Earth Observing One satellite 11 


  12 volume > 0 SOL Time of Day Filter passes by local \(i.e Mars\e of day False Compare Threshold Percentage difference allowed between predicted and actual values to trigger visual discrepancy \(c.f. \223Comparing Predict vs. Actual\224 40 Remove RS Overhead Remove the Reed-Solomon encoding overhead from the displayed RA Volume values Final value = value * 233 255 True Query Re-query data from database N/A Auto-Query Auto-query the data in view according to the time delay selected from the adjacent pull-down menu \(in seconds  Export Export the data in view to csv format \(c.f. \223Export\224 N/A Report Run a JasperSoft report of the selected pass \(c.f. \223Report by Pass\224 N/A   Export Volumes  As mentioned earlier, a primary purpose of this tool is to provide life-of-mission performance data.  Over the course of the early mission, the export function on the \223Overflight Review\224 was used for this purpose. After approximately two months of data had been collected the review page became unresponsively slow to load all the required data This was largely due to the amount of Javascript on the Review page.  So an additional page was added with the purpose of quick export to csv  General Query  Another important use of the RDE tool is ad-hoc analysis To meet this need, we provided a general-purpose query display.  This includes selection of any set of fields as well any number of SQL \223where\224 clauses.  Only the SQL \223add\224 of where clauses are currently supported, \223or\224 is not    Figure 3 \226 General Query View  Drop Dead Uplink Times  Pass plans include a time value called the \223Drop Dead Uplink Time\224.  These times represents the latest possible time that a User GDS team can submit a forward link product to a Relay GDS team and have it delivered via the indicated link    Figure 4 \226 Drop Dead Uplink View  The provided view is not necessarily a great deal better than the current set of pass spread sheets for those who are used to them. However, new functions such as a \223countdown alarm\224 should improve its utility  Overflight Report  Any tabular record in the Review and General Query displays can be selected and vi ewed as a report.  The report view shows all fields for the overflight record in question Underlying this view is the JasperReport software    Figure 5 \226 Example Report  Other Views  Not all of our initial views made the final cut.  Key remaining views include a \223Latest Pass\224 statistics view \(like the Overflight Report auto-update of recent passes utilized 


  13  Use of Web Technologies The web development effort involved a long learning curve for some of the RDE Upgrade development team HTML We used HyperText Markup Language \(HTML\o layout the web framework and page structures AJAX/JSON  The AJAX \(asynchronous JavaScri pt and XML\tern was introduced to support a responsive web GUI.  An implementation of JSON \(JavaScript Object Notation provides the data service inte rface from the web client to a tomcat-maintained servlet. We have run into very few issues with this technology 10 11 12  CSS Cascading Style Sheets \(CSS\provide a straightforward way to manage web page styles. The language is well supported through the open-source FireBug Edito 14  JavaScript We used JavaScript to provide a rich interactive web interface, however our experien ce with JavaScript was not entirely favorable.   The Java Script \(JS approach to APIs including widgets led to our implementing more JS code than we might otherwise have.  We found the 223typeless\224 language occasionally caused confusion.  We did find that it could be \223made to provide\224 a rich web interface though probably a heavier interface than it needs to be.  One real positive for JS is the huge volume of sample code available on the Internet, saving time when dealing with the many implementation problems that spring up JasperReport  We used a toolkit called JasperReport to build a database report we could execute from our web interface.  A JasperReport is edited using the iReport tool. When the report is opened, the JasperSoft library is executed which updates each field from the database and presents the updated report The next section describes some of the \223analysis views\224 that can be generated from data in the RDE store 5  R ELAY A CCOUNTABILITY A NALYSIS  From the start of the Phoenix mission through the current day the RDE tool has been us ed primarily to extract pass statistics for reporting.  The key primary tracked statistic is the total volume of data delivered by each relay spacecraft Typically the \223Export Volumes\224 web view is used for a 223quick\224 export of these data, though the \223General Query\224 view can also be used for this purpose.  Raw tabular data are exported in Comma Separated Value \(CSV\at and then normally imported into an Excel spreadsheet for charting Figure 6 shows the total volume over time from the start of Phoenix mission \(Sol 0\o Sol 120, about a month past the end of prime mission Cumulative Return Link Data Volume 0 5000 10000 15000 20000 25000 30000 35000 0 30 60 90 120 Sol MRO Cumulative Ret Fill DV MRO Cumulative Ret PHX DV ODY Cumulative Ret Fill DV ODY Cumulative Ret PHX DV   Figure 6 \226 Cumulative Return Link Volume Figure 7 shows the same data values, but charted to show the total data volume transmitted per Sol  Return Link Data Volume by Sol PHX Lander Data + Fill Data 0 50 100 150 200 250 300 350 400 0 30 60 90 120 Sol ODY Ret Data Volume \(PHX Data + Fill bits MRO Ret Data Volume \(PHX Data + Fill bits  Figure 7 \226 Data Volume By Sol  On Figure 6 and Figure 7, the Phoenix Data Volume \(DV shown represents actual telemetry data and science product delivered.  The Fill DV on both figures represents \223fill\224 frames that are transmitted when a relay link is open but no actual lander data is available to transmit.  This chart shows that the Odyssey mission has carried the most part of 


  14 Phoenix relay data over MRO.  The low MRO volumes at the very early end of the chart are due to early MRO communications issues with Phoenix that were not resolved until later in the mission.  The volume discrepancies are clearly visible from both charts Figure 8 shows the data volumes for the 128K overflights as a function of Mars LMST \(Local Mean Solar Time 128 kbps Return Link Data Rate 0 10 20 30 40 50 60 70 80 90 100 0:00:00 6:00:00 12:00:00 18:00:00 0:00:00 Local Mean Solar Time ODY MRO PHX Re q uirement:  30 Mb p ass  Figure 8 \226 Data Volume By Sol  Figure 9 shows a comparison of data volume predicts vs pass performance values  Figure 9 \226 Volume Predicts vs. Performance  6  C ONCLUSIONS  For ASC, the next step is to achieve a level of database independence.  We look to take advantage of emerging best practices technologies and patterns \(e.g. Hibernate  t o  upgrade our core server function to database independence and our Agent Framework to take advantage of a workflow execution function or language such as the Business Process execution Language \(BPEL  RDE is planned for update with the Mars Science Laboratory \(MSL\ission.  This will include any required updates to the planning input s \(MSL will utilize a wider range of relay parameters, including higher data rates multiple frequency channels and possibly new modulation and coding schemes and adaptive data rate functionality and a likely major change in the handling of predicts One significant additional RDE improvement proposed is to develop a web-based tactical interface supporting pass utilization requests and tacti cal updates.  Currently, all tactical relay coordination is accomplished via email and phone.  This process could be greatly improved with the upgrade to of a web-based system.  However, if adopted this upgrade would have a large impact on the current operational tactical process and thus must be carefully engineered across a broad user base  Overall we found we were able to accomplish our primary relay accountability goals using our chosen architecture The ASC server held up as a useful way to manage objectrelational transactions over a distributed network, and our event-driven client mechan isms were assembled in a straightforward manner using the existing ASC Agent Framework as well as the DOM message reactor function Our ability to accomplish this broad integration at a consistently low level of funding tells us that these technologies and patterns are worth pursuing as part of further accountability prototype pilot, and deployment activities A CKNOWLEDGEMENT  The work described in this paper was conducted at the Jet Propulsion Laboratory, Californi a Institute of Technology under a contract with the National Aeronautics and Space Administration  The authors wish to further acknowledge other individuals who made key contributions to research and development including Marti DeMore, Lloyd DeForrest, Derek Kiang Ashley Shamilian, Lori Nakamura, Mark Palm, Priscilla Parrish and Mike Tankenson  


  15 R EFERENCES    http://www.w3.org/XML/Schema   eb Orchestration with BPEL\224 http://www.idealliance.org/pa pers/dx_xml03 papers/0406-01/04-06-01.html  Hi bernat e hom e page www.hibernate.org   Al l a rd, Dan and Hut c herson, Joe, \223C om m uni cat i ons Across Complex Space Networks\224, IEEE Aerospace Conference, March 1-8, 2008  W e b Servi ce Defi ni t i on Language http://www.w3.org/TR/wsdl   B a uer, C h ri st i a n and Ki ng Javi n Java Persi s t e nce for Hibernate, New York: Manning Publications, 2007 7] \223Software Agents An Overview\224 http://www.sce.carleton.ca/netm anage/docs/AgentsOverview ao.html  e thodology.org  http://www.riaspot.com artic les/entry/What-is-Ajax  http://www.json.org 11 h ttp to m cat.ap ach e.o r g   12] http://java.sun com/products/servlet  http://www.w3.org/Sty le/CSS    B IOGRAPHY  Dan Allard has worked as a software engineer at the Jet Propulsion Laboratory for the past 17 years.   He currently leads the development of core JPL accountability systems applications and infrastructure Other recent work includes the development of a message-based ground data system for the Mars Science Laboratory as well as research and development of ontologybased distributed communications     Dr. Charles D \(Chad\ards, Jr received his A.B degree in Physics from Princeton University in 1979 and his Ph.D. in Physics from the Calif ornia Institute of Technology in 1984.  Since then he has worked at NASA\222s Jet Propulsion Laboratory, where he currently serves as Manager of the Mars Network Office and as Chief Telecommunications Engineer for the Mars Exploration Program, leading the development of a dedicated orbiting infrastructure at Mars providing essential telecommunications and navi gation capabilities in support of Mars exploration.  Prior to that he managed the Telecommunications and Mission Operations Technology Office, overseeing a broad program of research and technology development in support of NASA\222s unique capabilities in deep space communications and mission operations.  Earlier in his career, Dr. Edwards worked in the Tracking Systems and Applications section at JPL where he carried out research on novel new radio tracking techniques in support of deep space navigation, planetary science, and radio astronomy  


  16  


Thank you Questions 


 18  Astronautical Congress Valencia, 2006 27  Bu reau  In tern atio n a l d e s Po ids et Mesures. \(2 008  August\SI Base Units. [On http://www.bipm.org/en/si/base_units   B IOGRAPHY  Author, Karl Strauss, has been employed by the Jet Propulsion Laboratory for over 22 years.  He has been in the Avionics Section from day One.  He is considered JPL\222s memory technology expert with projects ranging from hand-woven core memory \(for another employer\o high capacity solid state designs.  He managed the development of NASA\222s first Solid State Recorder, a DRAM-based 2 Gb design currently in use by the Cassini mission to Satu rn and the Chandra X-Ray observatory in Earth Orbit.  Karl was the founder, and seven-time chair of the IEEE NonVolatile Memory Technology Symposium, NVMTS, deciding that the various symposia conducted until then were too focused on one technology.  Karl is a Senior IEEE member and is active in the Nuclear and Plasma Scie nce Society, the Electron Device Society and the Aerospace Electronic Systems Society Karl is also an active member of SAE Karl thanks his wonderful wife of 28 years, Janet, for raising a spectacular family: three sons, Justin, Jeremy Jonathan.  Karl\222s passion is trains and is developing a model railroad based upon a four-day rail journey across Australia\222s Northern Outback   


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


