Detecting Temporally Redundant Association Rules Mirko B\250 ottcher Martin Spott Detlef Nauck Intelligent Systems Research Centre BT Research and Venturing Adastral Park Orion Bldg pp1/12 Ipswich IP5 3RE K mirko.boettcher@bt.com martin.spott@bt.com detlef.nauck@bt.com Abstract Methods for association rule discovery and pruning assume implicitly that the associations hidden in the data are stable over time and thus provide a rather static view on data and their underlying structure This is unrealistic in time-stamped domains which are standard for reallife business data The question 215Which association rules exist?\216 is replaced by 215How do properties of association rules change?\216 In order to cope with the vast number of detectable rule changes pre-processing techniques are required that 223nd those rules which are root cause to interesting rule changes The paper proposes an approach based on statistical tests that 223nds derivative rule change histories and marks the respective rules as redundant The effectiveness in reducing the number of rule histories is demonstrated using real-life survey data 1 Introduction Association rule mining 1 o r ig in ally h a s b een d e v e loped for market basket data analysis where each basket also referred to as a transaction consists of a set of purchased items The goal of association rule mining is to detect all those items which frequently occur together and to form rules which predict the co-occurrence of items However association rule mining is not just bound to this speci\223c purpose It can be applied for example to every relational database Methods for association rule discovery assume implicitly that the associations hidden in the data are stable over time and thus provide a rather static view on data and their underlying structure This is unrealistic in time-stamped domains because the data then captures and re\224ects external in\224uences like management decisions economic and market trends and changes in customer behaviour Such timevarying domains are very common in practice since data is almost always collected over long periods It is crucial for the success of most businesses to detect changes correctly interpret their roots and 223nally to adapt or react to them From this perspective the question Which association rules exist  answered by association rule mining is replaced by How do properties of association rules change Based on these considerations several approaches which aim to 223nd interesting changes in statistical measures like con\223dence and support of association rules have recently been proposed and the term le change g coined 2 7 9 3 In all approaches a time-s tamped data s e t i s divided along the time axis Association ule mining is then applied to each of the subsets to derive a sequence of timedependent rule sets For those ules which are contained in every rule set the time series of their support and con\223dence values 205 called histories 205 are analyzed to detect interesting change patterns Association e mining discovers the exhaustive set of all hidden associations within the data This set is usually vast and hardly presentable the truly interesting rules are hidden within many redundant or obvious ones This rule quantity problem transfers directly to rule change mining a vast number of histories have to be analysed and 223nally far too many change patterns are reported The truly interesting change patterns are mostly hidden within many redundant ones One type of redundancy which is very common in practice is that e changes captured in a istory and consequently also change patterns are simply the owball effect of the changes of other rules For example given our database of surveys the following rule could have been discovered r 1  B ROADBAND 002 H IGH I T U SAGE A change pattern could be that the support history of the rule i.e the fraction of users with broadband internet and high internet usage shows an upward trend However if the fraction of males among all broadband users with high Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


usage is stable over time the history of r 2  M ALE B ROADBAND 002 H IGH I T U SAGE has qualitatively the same shape and hence exhibits the same trend In fact the history of r 2 can be derived from e one of r 1 by multiplying it with a gender related constant factor For this reason the rule r 2 is temporally redundant with respect to its history of support It is reasonable to assume that a user will generally be interested in rules with non-derivative and thus non-redundant histories because they are likely key drivers for changes This is supported by similar statements by many researchers 4 6 As 4 poi nt ed out  215 If t h e s upport of t h e i t e mset changes over time it is not considered interesting if the changes are totally explained by e changes in e support of smaller subsets of items\216 Moreover derivative rules may lead to wrong business decisions In the above example a decision based on the change in rule r 2 would account for the gender as one signi\223cant factor for the observed trend In fact the gender is completely irrelevant We therefore claim that there is a signi\223cant need for approaches to detect rules which are derivative with respect to their histories discard them as redundant and thus provide more clarity about possible key drivers for change In a way this can be seen as a form of pruning As we will show later on standard pruning methods for association rules cannot directly be applied to 223nd derivative rule histories since there is a fundamental difference between detecting the redundancy of a rule at each point in time separately and looking at entire rule histories as a whole The latter has not been investigated to our knowledge In this work we present a formal notion of derivative rule histories and propose a collection of meaningful criteria and testing procedures Finally the effectivity of the proposed framework is tested on real-life data 2 Related Work As already pointed out earlier the number of discovered rules is usually vast and y presentable to a user Therefore pruning methods are broadly used to constrain the set of generated rules Examples are non-redundant rule sets 11 and informative rule sets 5 From the pers pecti v e of rule change mining such pruning methods treat rule sets independently from another However in rule change mining we have many temporally ordered rule sets Thus the rule property utilized for pruning 205 in general a measure based on rule statistics 205 may vary for some rules over time but still match the pruning criterion in each rule set Although these variations may qualify rules as interesting they are discarded by approaches for association rule pruning Consequently conventional pruning approaches should not directly be used in conjunction with rule change mining In the area of rule change mining the discovery of interesting changes in histories for association rules has been studied by several authors In 2 a query l a nguage for shapes of histories is introduced 7 propos e a s t at i s t i cal approach to distinguish trend semi-stable and stable rules with respect to their histories of con\223dence and support They apply conventional association rule pruning to each derived rule set A fuzzy approach to reveal the regularities in how measures for rules change and to predict future changes was presented by A frame w o rk to monitor the changes in association rule measures is described in 9 However none of these publications discusses how to detect and discard rules which are redundant with respect to their history In 6 a m e th o d to d e tect so called fundamental rule changes is presented that can be compared to our idea of 223nding non-derivative rule changes Their approach differs to our approach in the following aspects 223rst their approach can only be applied to histories of two periods length Therefore it cannot be applied to any of the rule change mining approaches above because they require histories signi\223cantly longer than two periods An n to many periods is not straightforward due to the form of the underlying statistical test Second they consider change in a rule as non-fundamental if it can be predicted by the change of each of two more general rules Opposed to our approach and intuition this method would judge the change in rule r 2 of the example in the Introduction as fundamental because t can be predicted by just one more general rule Third the approach of 6 can yi el d c ount eri nt ui t i v e res ul t s due to scaling issues in combination with the tests used assume a rule r whose support changes from 0.08 to 0.06 and two more general rules r 002 and r 002\002 whose support values are constant at 0.1 and change from 0.7 to 0.77 respectively and the wo underlying datasets each having a size of 1000 transactions The approach of 6 t hen cl as s i 223 e s r as nonfundamental at signi\223cance level 0  05  although r decreases by one fourth whereas r 002 is constant and r 002\002 even increases by one tenth In fact neither the change in r 002 nor in r 002\002 explains the change in r  3 Association Rules Formally association rule mining is applied to a set D of transactions T\003D  Every transaction T is a subset of a set of items L  A subset X\004L is called itemset Itissaid that a transaction T supports an itemset X if X\004T  An association rule r is an expression X\002Y where X and Y are itemsets Y  0 and X 005 Y  002  Its meaning is quite intuitive Given a database D of transactions the rule above expresses that whenever X\004T holds Y\004T is likely to hold too We just focus on association rules whose consequent is a 1-itemset since such rules are generally sufProceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


223cient for most business applications A rule is then written as X\002 y  with X\006L and y 003L  If for two rules r  X\002 y and r 002  X 002 002 y  X\006X 002 holds then it is id that r is a generalization of r 002  This is denoted by r 002 007 r  As usual the reliability of a rule r  X\002 y is measured by its con\223dence conf r   which estimates P  y 003T X 006 T   or short P  y X   The statistical signi\223cance of r is measured by its support supp r  which estimates P  X\b  y 004T   or short P  X y   We also use the support of an itemset X denoted by supp X   4 Histories of Association Rule Measures Let D be a time-stamped data t and  t 0 t n  the minimum time span that covers all its tuples The interval  t 0 t n  is divided to n 1 non-overlapping periods T i   t i 212 1 t i   such that the corresponding subsets D  T i  006D each have a size D  T i  t 1 Let 032 T   T 1 T n  be the set of all periods then for each T i 003 032 T association rule mining is applied to the transaction set D  T i  to derive rule sets R  D  T i   Because the measures like con\223dence and support of every rule r  X\002 y are now related to a speci\223c transaction t D  T i  and thus to a certain time period T i we need to extend their notation This is done straightforward and yields conf r T i  n P  y X T i  and supp r T i  n P  X y  T i   Each rule r 003 032 R  D  002 n i 1 R  D  T i  is therefore described by n values for each measure Imposed by the order of time the values form sequences H conf  r conf r T 1   conf r T n  and H supp  r   supp r T 1   supp r T n   Accordingly they are called con\223dence history and support ry of e rule r  These histories are the input to most rule change mining approaches which then detect interesting change patterns 5 Detecting Derivative Rules As laid out in the previous section the aim is to 223nd rules that are non-redundant in the sense that their history is not a derivative of related rules\220 histories In a way the approach is searching and discarding rules that are not the root cause of a change pattern which in turn can be seen as a form of pruning In order to 223nd derivative rules we have to answer the following questions 223rst what is meant by related rules and second what makes a history a derivative of other histories Regarding the 223rst question a natural relation between association rules is generalization  We therefore de\223ne that arule r 002 is related to a rule r iff r 002 is more general than r  i.e r 007 r 002 following Section 4 The following de\223nition for derivative measure histories includes those of itemsets as a generalization from rules Thereby the superset relation is used to de\223ne related itemsets  an itemset Y is related to an itemset X iff X\007Y  X\013Y  As before XY is written for X\bY  De\223nition 1 Let s  s 1 s 2 s p be rules or itemsets with s 007 s i for all i and p 0  In case of rules let the antecedent itemsets of the s i be pairwise disjoint in case of itemsets let the s i be pairwise disjoint Let m be a measure like support or con\223dence m  T  m  s T  and m i  T  m  s i T  its functions over time M   g  R 212\002 R  be the set of real-valued functions over time The history H m  s  regarding the measure m is called derivative iff a function f  M p 212\002 M exists such that for all T 003 032 T m  T  f  m 1 m 2 m p   T  1 For simplicity we call a rule or itemset derivative with respect to a measure m iff its history of m is derivative The temporal redundancy of a rule therefore depends on the measure under consideration e.g a rule can be redundant derivative with respect to its support history but not redundant not derivative with respect to its con\223dence history This in turn is consistent with existing rule change mining approaches because they typically process histories of different measures independently from another The main idea nd the bove de\223nition is that the history of a rule itemset is derivative if it can be constructed as a mapping of the histories of more general s itemsets To compute the value m  s T  the values m  s i T  are thereby considered The de\223nition above does not allow for a pointwise de\223nition of f on just the T 003 032 T butinstead states a general relationship between the measures of the rules independent from e point in time It can therefore be used to predict the value of for example supp s  given future values of the supp s i   A simple example we will see below is m  f  m 1  cm 1  i.e the history of a rule can be obtained by multiplying the history of a more general rule with a constant c  In the following we introduce three criteria for detecting derivative histories which can be used in combination or independently from another The 223rst two criteria deal with itemsets and can therefore be directly applied to the support and antecedent support of rules as well The last criterion is related to histories of rule con\223dences The functions f are quite simple and we make sure that they are intuitive The 223rst criterion checks if the support of an itemset can be explained with the support of exactly one less speci\223c itemset Criterion 1 The term supp XY T   supp Y T  is constant over T 003 032 T given disjoint itemsets X and Y  The meaning of the criterion becomes clear when being rewritten as c  supp XY T   supp Y T  Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


 5 10 15 20 0.15 0.2 0.25 0.3 0.35 0.4 0.45  t supp X 002 z   Xy 002 z Figure 1 Histories of the rule X\002 z and its derivative rule X y 002 z P  XY  T  P  Y T  P  X|Y T  with a constant c The probability of X is required to be constant over time given Y  so the fraction of transactions containing X additionally to Y constantly grows in the same proportion as Y This de\223nition is also closely related to con\223dence and states that the con\223dence of the rule Y\002X should not change For this reason the in\224uence of X in the itemset XY on e support history is not important Due to supp XY T  c 267 supp Y T  2 with c  supp XY T   supp Y T  for any T 003 032 T  XY is obviously a derivative of Y with respect to support history as de\223ned in De\223nition 1 Figures 1 and 2 show an example of a derivative support history of a rule taken from the survey data used for our experiments cf Section 9 For reasons of data protection the underlying rule cannot be revealed For illustration the reader is referred to the example given in the Introduction instead Figure 1 shows the support histories of the less speci\223c rule at the top and the more speci\223c rule underneath over 20 time periods The shape of the two curves is obviously very similar and it turns out that the history of the more speci\223c rule can be approximately reconstructed using the less speci\223c one based on 2 As shown in Figure 2 the reconstruction is not exact due to noise As a result a statistical test is employed in Section 7 to test the validity of the criteria Opposed to the criterion above the following is based on the idea of explaining the support of an itemset with the support values of two subsets Criterion 2 The term supp XY T  supp X T  supp Y T  is constant over T 003 032 T given disjoint itemsets X and Y  5 10 15 20 0.15 0.2 0.25 0.3 0.35 0.4 0.45  t supp orig. hist. Xy 002 z   recon. hist. Xy 002 z  Figure 2 Reconstructed history of X y 002 z using the history of X\002 z supp XY T  measures the probability of the itemset XY in period T which is P  XY  T  Theterm supp XY T  supp X T   supp Y T   P  XY  t  P  X T  P  Y T  is quite extensively used in data mining to measure the degree of dependence of X and Y at time T  Particularly in association rule mining this measure is also known as lift 10 The criterion therefore expresses that e egree of dependence between both itemsets is constant over time The support history of XY can then be constructed using supp XY T  c 267 supp X T  supp Y T  3 with c  supp XY T   supp X T  supp Y T  for any T 003 032 T  that is the individual support values of the less speci\223c itemsets are d corrected with the constant degree of dependence on another According to De\223nition 1 the support history of XY is therefore derivative Overall an itemset is considered derivative with respect to support if more general itemsets can be found such that at least one of the Criteria 1 or 2 holds Finally the last criterion deals with derivative con\223dence histories of rules Criterion 3 The term conf r,T  conf r 002 T  is constant over T 003 032 T given two rules r and r 002 with r 007 r 002  Assuming the rules r  XY 002 z and r 002  Y\002 z with disjoint itemsets X and Y  the criterion translates to P  z XY T  P  z Y T  being constant over time This basically means that the contribution of X in addition to Y to predict z relative to the predictive power of Y remains stable over time and can therefore be neglected The con\223dence history of r is derivative because of the following Be c  conf r T   conf r 002 T  for any T 003 032 T  then for all T 003 032 T conf r T  c 267 conf r 002 T  4 Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


6 Relation to Association Rule Pruning In Section 2 we explained why conventional pruning approaches for association rule pruning should not directly be used for rule change mining However it is possible to extend them naively to rule histories We call the following the naive temporal extension of a rule pruning algorithm a rule is only discarded if it can be discarded from every period\220s rule set and the numeric property of the rule utilized for pruning is constant over time Our criteria have some interesting properties in relation to the naive temporal extensions of two well-known pruning approaches One recent pruning approach for association rules is based on so-called informative rule sets 5 A r ul e r  XY 002 z is not in the informative rule set iff re exists arule r 002  Y\002 z  such that conf r  212 conf r 002  f 0 This is equivalent o conf r   conf r 002  f 1  whose naive temporal extension is obviously a special case of 4 Hence Criterion 3 for detecting derivative rules is consistent with the criterion for the non-informative rule set The set of derivative rules detected by Criterion 3 is a superset of the redundant rules detected by the naive temporal extension of the informative rule set  Another approach in the 223eld of association rule mining to restrict the number of discovered rules is to generate the non-redundant rule set based on closed frequent itemsets 11  A ru le with o n l y o n e item in its co n s eq u e n t is thereby considered redundant iff a more general rule with the same support and con\223dence exists The two conditions of this criterion obviously match 2 and 4 respectively for c 1  Therefore if a rule was considered derivative only if both Criterion 1 and Criterion 3 are satis\223ed with respect to the same more general rule then the set of derivative rules would be a superset of the redundant rules detected by the naive temporal extension of the non-redundant rule set  7 Testing the criteria To check if the history of a rule r or an itemset XY is derivative with respect to support or con\223dence we need to test if the criteria of the previous section hold Due to data usually being noisy we will not check Criteria 1\2053 directly but instead statistically test their validity Also we rewrite the criteria in an equivalent form in order to account for e order of values over time in the histories Our experiments have shown that direct use of the criteria counterintuitively marked some histories as derivative when they were noisy Let 002 i supp X  supp X T i  supp X T i 212 1  be the relative change in support for itemset X and let 002 i conf r  conf r,T i  conf r,T i 212 1  be the relative change in con\223dence for the rule r  both between two periods T i 212 1 and T i  Then the respective Criterion 1,2 or 3 holds iff for any T i 003 032 T  T 1   Criterion 1 002 i supp XY  i supp Y  5 Criterion 2 002 i supp XY  i supp X  i supp Y  6 Criterion 3 002 i conf r 002 i conf r 002  7 This means that if Criterion 1 or 3 holds for an itemset XY  or a rule r  then the relative changes in its history are equal to the temporally related relative changes in the history of a more general itemset X orrule r 002  If Criterion 2 holds then the relative changes in the history of XY are equal to the product of the corresponding relative changes in the histories of X and Y  Obviously 7 are following the same general scheme y i  x i  i 2 n  whereas the quantities y i and x i stand for the left and accordingly right hand side of the equations It is convenient for the following discussion to imagine x i and y i in a plot whereby y i is 205 as implied by De\223nition 1 205 the dependent quantity If y i  x i holds then all points in the plot should be on a straight line with slope 1 and intercept 0  In practice this equality will rarely hold due to noise In fact the underlying relationship will be y i  x i  002 where 002 is a random error with zero mean and unknown but low variance Under the assumption that the dependency of y i from x i can be generally described by y i  ax i  b  002 we\223ta regression line y 032 ax  032 b Wethentestif x i is statistically equal to y i by carrying out the following to two steps 1 Based on the estimates 032 a and 032 b we test the hypothesis that the true parameters of the model are a 1 and b 0 using a standard t-test 8  2 Additionally we test if the variance of 002 is small i.e if e points  x i y i  are suf\223ciently close to the regression line by setting a threshold 037 r for Pearson\220s correlation coef\223cient r  Figure 3 illustrates the testing procedure It shows the scatter plot of the relative changes of the support histories from Figure 1 The egression line is y 1  0107 x 212 0  0103 and the correlation coef\223cient r n 0  97  The above test procedure using a signi\223cance level of 0  05 and 037 r 0  95 shows that the more speci\223c rule is indeed derivative with respect to the history of the ess speci\223c one 8 Implementation Issues The proposed criteria rely on a search over the set of related itemsets or rules respectively Generally this search is exhaustive and thus a potentially exponential number of comparisons is required e.g for every frequent itemset all Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


 0.6 0.8 1 1.2 1.4 0.6 0.8 1 1.2 1.4 rel. change X 002 z rel. change Xy 002 z Figure 3 Scatter plot of the relative changes of the curves shown in Figure 1 subsets have to be enumerated in the worst case The approach\220s apparent complexity may evoke questions about its feasibility Our experiments have been conducted on real business data and computation time was reasonable in particular considering that rule change mining for business data is typically carried out on a weekly r monthly basis Nonetheless in some domains complexity may be an issue In this case the number of comparisons can be considerably reduced by the following simpli\223cation adopted from instead of all related itemsets rules only closely related ones are considered Criterion-wise this means that for an itemset X or rule X\002 z and any y 003X  200 Criterion 1 the itemset X y  is considered 200 Criterion 2 the itemsets X y  and  y  are considered 200 Criterion 3 the rule X y 002 z is considered 9 Evaluation As we already noted in the introductory section the number of histories to be examined by rule change mining approaches and therefore the number of detected change patterns is usually vast Since we consider our approach as a preprocessing step within rule change mining the 223rst question to be answered experimentally is how many rule histories are derivative and thus to which extent our approach is able to reduce their number Algorithms for association rule mining use minimum support and con\223dence thresholds as parameters to control the number of discovered rules Those also in\224uence the number of more general rules discovered for each rule Since our approach utilizes the generalization relationship between rules the second question is therefore how our approach scales for different parameter settings of the mining algorithm and thus for different rule set sizes For our experiments we chose a representative real-life dataset from the CRM Customer Relationship Management domain The dataset contains answers of customers to a survey collected over a period of 40 weeks Each tuple is described by 33 nominal attributes with a domain size between 2 and 39  We transformed the dataset into a transaction set by recoding every attribute attribute value combination as an item Then we split the transaction set into 20 subsets each corresponding to a period of two weeks The subsets contain between 1480 and 2936 transactions To each subset we applied the well-known ri algorithm 1 w ith the s ame parameter s e ttings  F rom the obtained 20 rule sets we created a compound rule set by intersecting them Subsequently we applied the proposed approach to the compound rule set in order to test with iteria 1 and 2 for derivative support s and with Criterion 3 for derivative con\223dence histories To answer the second question we repeated the mining phase described above with different settings for the lower thresholds of support and con\223dence supp min and conf min respectively The experimental results are shown in Table 1 Each row of the table corresponds to one parameter setting shown in the 223rst two columns Column 3 shows the number of rules in the compound rule set and thus the number of histories to which our approach has been applied For each rule we tested if it is derivative w.r.t to its con\223dence history last two columns and its support history Columns 4-7 For each measure we show the number of rules matching each criterion and the percentage of derivative rules among all rules in this order Column 6 also reveals the number of rules matching Criterion 1 or Criterion 2 As it can be seen in the last column between 41  6 and 44  9 of rules are tested as derivative according to their con\223dence history Furthermore this percentage is quite stable for all tested parameter combinations Column 7 shows that between 73  6 and 75  1 of rules are tested as derivative according to their support history Again the percentage of derivative rules is stable for all tested parameter combinations Comparing Columns 4-6 also shows that Criterion 1 223nds more derivative rules than Criterion 2 and that the intersection of the rule sets found by the two criteria is large as well This indicates that the Criteria are consistent Picking up on Section 6 we compared our approach with the naive temporal extension of the approach proposed by 11 On our s i de we onl y d i s carded a rul e i f bot h C ri t e rion 1 and Criterion 3 are satis\223ed with respect to the same more general  From 77401 rules this method discards 24426  which is approximately 31  5  whereby we used the rule mining parameters 0  05  0  2  To contrast this numProceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


supp min conf min histories  C1  C2  C1 b C2  ratio    C3  ratio   0.1 0.6 13586 9286 5249 10035 73.9 6108 44.9 0.05 0.4 49282 32975 22559 36253 73.6 20536 41.6 0.05 0.2 77401 52740 37638 58144 75.1 34142 44.1 Table 1 Number of derivative support histories C1 C2 and con\223dence histories C3 detected by each criterion with different parameter settings of the rule miner ber with the size of the non-redundant rule set suggested by 11 all rules for which a m ore g eneral rule with identical support and con\223dence history exists were discarded From 77401 rules this method discards 3567  which is approximately 4  6  Obviously our pruning technique leads to a drastic increase in the number of pruned rules compared to the naive temporal extension of the approach by 11 This con\223rms experimentally that such a pruning method for static rule sets insuf\223ciently accounts for the temporal change of rules and cannot simply be transfered to temporal sets 10 Conclusion Many businesses collect huge volumes of time-stamped data and are looking to detect change patterns that emerge in rule sets over time Due to the usually vast number of rules under investigation it is advisory to discard redundant rules in a pre-processing step We introduced the notion of a derivative rule history that is a history that can be explained with histories of more general rules and can therefore be discarded We proposed three different criteria for the derivativeness of histories and showed how to implement statistical tests to check if a history meets the criteria The proposed approach thereby overcomes the limitations of existing rule change mining methods for discarding redundant rules as our pruning criteria for the 223rst time take into account entire rule histories We showed that although existing pruning techniques for static rule sets cannot directly be applied our method is consistent with some of them i.e can be seen as a generalization from static to sequences of rule sets Moreover because derivative rules are no key-drivers for change our approach supports the user in identifying root causes and therefore in interpreting change patterns Using real-life data sets taken from surveys we showed the effectiveness of the algorithms Based on con\223dence histories of rules our approach marked between 42 and 45 of the rules as redundant based on support histories even between 74 and 75 Thereby the 223gures seem independent of the choice of parameters support and con\223dence thresholds for the rule miners Furthermore we showed that our approach is far superior to straightforward temporal extensions of pruning methods for static sets of association rules pushing up the relative number of detected derivative histories from 4  6 to 31  5  References 1 R  A g r a w al T  I mielin sk i an d A  N  S w a mi M i n i n g asso ciation rules between sets of ms in rge databases In Proc ACM SIGMOD 1993  pages 207\205216 Washington DC 1993  R Agra w a l and G P sai l a  A ct i v e d at a m i n i ng In Proc KDD 1995  pages 3\2058 Montreal Canada 1995 3 W  H Au and K C han Mi ni ng changes i n associ at i o n r ul es a fuzzy approach Fuzzy Sets and Systems  149\(1\:87\205104 2005 4 S  C hakr abar t i  S  S a r a w a gi  a nd B  Dom Mi ni ng sur pr i s i n g patterns using temporal tion length In Proc VLDB 1998  pages 606\205617 New York City NY 1998 Morgan Kaufmann Publishers 5 J  L i  H S h en and R  T opor  M i n i n g i nf or mat i v e r ul e set for prediction Journal of Intelligent Information Systems  22\(2\:155\205174 2004 6 B  L i u  W  H su and Y  Ma Di sco v e r i ng t h e s et of f undamental rule changes In Proc ACM IGKDD 2001  pages 335\205340 San Francisco CA 2001  B  L i u  Y  M a and R  L ee Anal yzi ng t h e i nt erest i ngness of association rules from the temporal dimension In Proc IEEE ICDM 2001  pages 377\205384 San Jose CA 2001 8 D  M ont gomer y a nd G R unger  Applied Statistics and Probability for neers  Wiley 2002 9 M  S pi l i opoul ou S  B a r on and O  G 250 unther Ef\223cient monitoring of patterns in data mining environments In Proc ADBIS 2003  pages 253\205265 Dresden Germany 2003 Springer  G I W e bb  E f 223 ci ent search for associ at i o n r ul es In Proc ACM SIGKDD 2000  pages 99\205107 Boston MA 2000  M J Z a ki  M i n i n g nonr e dundant associ at i o n r ul es Data Mining and Knowledge Discovery  9\(3\:223\205248 2004 Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


consequently, the results approach more of the presented for the classic vector space model. The several possibilities of values of the parameters were tested however the collections behave in a similar way in its alteration The experiments showed that the proposed model improves the average precision of the answer set for all collections. Besides, the obtained medium precision was not harmed by the recall increase happened when expanding the queries 7. Conclusions In this paper, we presented an extension to the vector space model to contemplate the dependence among the terms of the collection. In the proposed model, the dependence among the terms is represented geometrically in the vector space The proposed model bases on the rotation of the term vectors, in agreement with the dependence among the terms. This rotation is made based on techniques that generate information on the correlation among terms of Proceedings of the International Database Engineering and Applications Symposium \(IDEAS  04 1098-8068/04 $20.00  2004 IEEE the collection. In this work, we used the association rules, however, other techniques can be used The generation of association rules is a known technique of data mining, which allows finding frequent patterns in large databases. In the context of this paper it is used to find sets of terms that appear simultaneously in the collection of documents. This information is useful to modify the term vectors, so that they reflect the semantics of co-occurrence defined for the association rules The extension to the vector space model we presented contemplates the dependence among terms in a clear, flexible and new way. It is clear because the dependence incorporation among the terms is made step by step and the vector space basis reflects the semantics defined for the adopted technique. The proposed model is flexible because it allows the correlation incorporation among the terms of collection obtained in several ways Finally, the proposal is new because in the literature there is not an extension to the vector space model which modifies the vector space basis how it was done in this work We evaluated the effectiveness of the model proposed with four reference collections. There was an increase in the retrieval model effectiveness in comparison with the classic vector space model for all of the reference collections used As future works, the effectiveness of the proposed model will be compared to the effectiveness of the generalized vector space model. Besides, we will research other methods of obtaining correlation among the terms of a collection of documents. These methods will be incorporated in a geometric way to the model proposed in this paper. We also intended to evaluate the model proposed for larger collections formed by Web documents References 1]  Agrawal, R., Imielinski, T., Swami, A  Mining association rules between sets of items in large databases   Proceedings of the ACM SIGMOD Conference,. Washington DC USA, may 1993, pp. 207-216 2] Agrawal, R., Srikant, R  Fast algorithms for mining association rules  Proceedings of the 20th Int  l Conference on Very Large Databases, Santiago Chile, September 1994 3] Baeza-Yates, R., Ribeiro-Neto, B., Modern information retrieval, ACM/Addison-Wesley, 1999 4] Becker, J., Kuropka, D  Topic-based vector space model  Proceedings of the 6th International Conference on Business Information Systems, Colorado Springs, June 2003 


Business Information Systems, Colorado Springs, June 2003 pp. 7-12 5] Bollmann-Sdorra, P., Raghavan, V. V  On the necessity of term dependence in a query space for weighted retrieval   Journal of the American Society of Information Science Volume 49\(13 6] Buckley, C., Salton, G., Allan, J., Singhal, A  Automatic query expansion using SMART : TREC 3  D. K. Harmon editor, NIST Special Publication 500-225: The Third Text Retrieval conference \(TREC 3 7] CAM-Collection. ftp://ftp.cs.cornell.edu/pub/smart/cacm 8] Han, J., Kamber, M. Data mining Concepts and techniques, San Diego: Academic Press, 2001, pp.335-393 9] Harman, D  Overview of the third Text Retrieval Conference  Proceedings of the third Text Retrieval Conference \(TREC-3 10] Mandala, R. , Tokunaga, T., Tanaka, H. M  Combining multiple evidence from different types of thesaurus for query expansion  Proceedings of the 22th annual international ACM SIGIR conference on Research and development in information retrieval, Berkeley, California, United States August 1999, pp. 191-197 11] Nie, J. Y., Jin, F  Integrating logical operators in query expansion in Vector Space Model  Workshop on Mathematical/Formal Methods in Information Retrieval, 25th ACM-SIGIR, Tampere, Finland, August 2002 12] P  ssas, B, Ziviani, N., Meira-Jr, W  Enhancing the setbased model using proximity information  Proceedings of the 9th International Symposium of String Processing and Information Retrieval, Lisbon, Portugal, September 2002, pp 104-116 13] P  ssas, B, Ziviani, N., Meira-Jr, W., Ribeiro-Neto, B  Modelagem vetorial estendida por regras de associa  o  XVI Simp  sio Brasileiro de Banco de Dados, Rio de Janeiro Brasil, 2001 14] P  ssas, B, Ziviani, N., Meira-Jr, W., Ribeiro-Neto, B  Set-based model: A new approach for information retrieval   Proceedings of the 25th International ACM SIGIR Conference on Research and Development in Information Retrieval Tampere, Finland, August 2002 15] Salton, G. \(ed   experiments in automatic document processing. Englewood Cliffs, NJ: Prentice Hall, 1971 16] Salton, G., Lesk, M. E  Computer evaluation of indexing and text processing  Journal of the ACM, 15\(1 36, Janeiro 1968 17] Shaw, W. M., Wood, R. E, Tiboo, H. R  The cystic fibrosis database: Content and research opportunities  Library and Information Science Research,Volume 13, 1991, pp.347366 18] Voorhees E. M  Query expansion using lexicalsemantic relations  Proceedings of the 17th ACM- SIGIR Conference, 1993, pp. 171-180 19] Wong, S. K.M., Ziarko, W., Raghavan, V. V., Wong, P C.N  On modeling of information retrieval concepts in vector spaces  Proceedings of the ACM Transactions on Database Systems, Volume 12, New York USA, June 1987, pp.299  321 20] Wong, S. K. M., Ziarko W., Wong, P. C. N  Generalized vector space model in information retrieval   Proceedings of the 8th ACM-SIGIR Conference on Research and Development in Information Retrieval. New York USA 1985, pp.18-25 Proceedings of the International Database Engineering and Applications Symposium \(IDEAS  04 1098-8068/04 $20.00  2004 IEEE pre></body></html 


 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   100  A   1  000  A   100  A   1  000 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   10  000  A   100  000  A   10  000  A   100  000 Figure 4 DataSet One with c  1 Figure 5 DataSet One with c  2 and therefore these itemsets participate in the implication count The number of tuples created by this step is S  50  c  1   2  4   The rest of the steps create itemsets that should not participate in the count We create three different kind of tuples that break one implication condition The relative weight of each kind is 1  3 Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A created before As in the previous step for each a i create at most c different b j  For each combination  a i  b j  write 50 tuples Then for each a i create eight b  j different than all b j s created before And write the eight tuples  a i  b  j   This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum top-condence level although they satisfy both the minimum support and the maximum multiplicity constraint The number of tuples created by this step is   A  S   3  50  c  1   2  8   Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated and each one appears with u different b j  where c  1  u  c  10 Write 50 such tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the maximum multiplicity condition The number of tuples created by this step is   A  S   3  50  c  5  5  Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated For each pair  a i  b j  write 40 tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum support requirement The number of tuples created by this step is   A  S   3  40 Shufe the output le This step just demonstrates that the operation of the algorithm is independent to the ordering of the tuples Estimate the implication count using algorithm NIPS/CI with a fringe size of four and also without a bounded fringe Perform one hundred such experiments and calculate the mean and the standard deviation of both estimations The total number of tuples for each experiment can be derived by adding the partial number of tuples created in each step For example for  A   10000 S  5000 and c  4 the average number of tuples for the corresponding experiment was 015 3  108  333 A minimum support of 50 tuples for this case corresponds to only 015  001 of the tuples demonstrating that in the implication count contribute even implications that hold for a very small number of tuples Figures 4,5 and 6 show the results for c  1  2  4 for varying cardinalities  A   The x-axis corresponds to the actual implication count of the dataset as that was imposed by the creation process The y-axis denotes the mean relative error as it is calculated by running one hundred experiments We used the following formula to estimate the mean relative error relative error   Actual S  Measured S  Actual S  Graphs Bounded Fringe express the experimental results for the case of a fringe with size F  4 while graphs Unbounded Fringe demonstrate the result for the case of an arbitrarily large fringe The error bars correspond to the statistical deviation of the mean error as that was computed by one hundred such experiments.The deviation is generally negligible which means that the error of the estimated S is always very close to the mean error We also observe that the difference between the estimation using a bounded fringe of size four and a unbounded one is negligible for a very wide range of implication counts and therefore a size of four for the fringe zone is sufcient to provide very accurate results for most applications 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe Figure 6 Dataset One with c  4 6.2 Real-world datasets  Algorithmic comparison We compare our estimates with the results taken using Distinct Sampling DS which has been sho wn pro vide highlyProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


accurate estimates for distinct value queries and event reports This algorithm outperforms other estimators that are based on uniform sampling[8 9 e v e n when using much less sample space We also provide comparison with our Implication Lossy Counting ILC algorithm described in Section 5.1 which is based in the Lossy Counting algorithm introduced in For this series of experiments we used a real dataset of eight dimensions which was given to us by an OLAP company whose name we cannot disclose due to our agreement The cardinalities of the dimensions are presented in Table 3 The parameters of the algorithms are presented in Table 5 For NIPS/CI we used 64 concurrent bitmaps with a fringe size of four thus requiring memory enough to hold  2 4  1   64  K  1920 itemsets We expect that the averaging\([5 o v e r these man y bitmaps will result in an error less than 10 We used the exact same sample space for DS The bound parameter t for DS was set to  1920  50  following the suggestion in F o r ILC we used an approximation parameter   0  01 which increases the memory requirements of ILC relative to those of NIPS/CI or DS On the average ILC used more than twice the memory that NIPS/CI and DS used For example for the experiment in Figure 7\(B it used more than 8,000 entries We evaluate the results of the algorithms with respect to the number of tuples the cardinality of the participating dimensions and the implication conditions To simulate a real data stream scenario we tracked the conditional implication counts of A  B  E  F and the unconditional B  E using the aforementioned algorithms The rst workload corresponds to quite large compound cardinality while the second to very moderate cardinalities Table 4 presents the actual aggregates for various instances of the stream for   5 and  1  60 We believe that most workloads fall somewhere in the middle with respect to the complexity of the wanted implications and the size of the returned counts Figure 7\(A depicts the relative error as the stream evolves for workload A using the algorithms DS NIPS/CI and ILC for different implication parameters In Figure a we show the results for minimum support   5 and  1  60 or  1  80 The different  1 are encoded in the parentheses next to identication of the algorithm in the legend of the graph In Figure b we increased the minimum support to   50 We observe that the behavior of DS varies widely while NIPS/CI remains always below the expected 10 error DS actually keeps a sample of the distinct elements seen so far and tries to scale the implication count that holds for that sample to the whole set of distinct elements In most cases the data in the sample is not representative of the implication The situation for DS is exacerbated when the minimum support increases where quite a lot of samples do not participate in the count making the scaling even more errorprone Algorithm ILC in all cases returned very erroneous results although it used much more space than NIPS/CI and DS since it tries to store not the implication counts but the actual implicated itemsets In these workload the implicated itemsets overwhelm its available memory which is actually larger than the amount given to NIPS/CI and DS In gure 7\(B we present the results of the algorithms for workload B The situation is still in favor of NIPS/CI whose relative error remains always close to the expected 10 unlike DS who returns highly skewed errors even though the domain cardinalities are much smaller and therefore keeps in the sample space much more data As expected from the analysis the error guarantees of NIPS/CI are virtually unaffected by changes in the cardinalities or the number of tuples seen so far in the stream ILC returns very erroneous results although now the cardinalities and the implicated items are much smaller compared to those of workload A The reason is not only because it keeps too much information in memory i.e all the implicated itemsets while both NIPS/CI and DS only hold a mantissa for the count but also because the constraint    rel is broken as the number of tuples increases 7 Related Work There are unique challenges in query processing for the data stream model Most challenges are the result of the streams being potentially unbounded in size Therefore the amount of the storage required in order to get an exact size may also grow out of bounds Another equally important issue is the timely query response required although the volumes of data the need to be processed is continually augmented at a very high rate Essentially the amount of computation per data item received should not add a lot of latency to each item Otherwise any such algorithm wont be able to keep up with the data stream In many cases accessing secondary storage such as disks is not even an option In  there is a discussion of what queries can be answered e xactly using bounded memory and queries that must be approximated unless disk access is allowed Sketching techniques\([14 ha v e been introduced to build summaries of data in order to estimate the number F 0 of distinct elements in a dataset In three algorithms that      approximate the F 0 are described with various space and time requirements Distinct is dri v e n by hashing functions similar to those studied in 14 and provides highly accurate results for distinct value queries compared to those taken by uniform sampling by using only a fraction of their sample size In the algorithms Stick y Sampling and Lossy Counting are introduced that estimate frequency counts with application to association rules and iceberg cubes In a framework for performing set expression on continuously updated streams based on sketching techniques is presented In a general framework over multiple granularities is presented for both range-temporal and spatio-temporal aggregations In a framework for identifying hierarchical heavy hitters i.e hierarchical objects like network addresses whose prexes denes a hierarchy with a frequency above a given threshold is described The effect of impications between columns has been emphasized in the system that identies correlated pairs of columns and soft-dependencies and has been proved very useful in query optimization 8 Conclusions We have presented a generalized and parameterized framework that can accurately and efciently estimate implication counts and can be applied to many scenarios To the best of our knowledge this is the rst practical and truly scalable approach to the problem Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


 Dimension Cardinality A 1557 B 2669 C 2 D 2 E 3363 F 131 G 660 H 693 Table 3 Cardinalities Workload A Workload B Tuples A  B  E  G E  B 134,576 608 50 672,771 12,787 125 1,344,591 34,816 152 2,690,181 84,190 165 4,035,475 132,161 182 5,381,203 187,584 188 Table 4 Impl counts w.r.t tuples NIPS/CI bitmaps 64 NIPS/CI K 2 DS sample size 1920 DS bound t 39 ILC  0.01 Table 5 Algorithm Parameters           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILS \(.8           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6 ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8   a   5 b   50 a   5 b   50 A Workload A B Workload B Figure 7 Relative Error vs stream size of online estimation within small errors of complex implication and non-implication counts between attributes of a data stream under severe memory and processing constraints and even in the presence of noise We prove that the complement problem of estimating non-implication counts can be      approximated when the size of the fringe zone is xed appropriately We demonstrate that existing algorithms for estimating frequent itemsets or sampling cannot be applied to the problem since they lose the cumulative effect of small implications In addition through an extensive set of experiments on both synthetic and real data we have shown that NIPS/CI always remains very close to the actual implication count capturing even very small implications whose total contribution is signicant References  R Agra w al A Evmie vski and R Srikant Information sharing across private databases In ACMSIGMOD  2003  N Alon Y  Matias and M Sze gedy  The space comple xity of approximating the frequency moments Journal of Computer and System Sciences  58:137 147 1999  A Arasu B Babcock S Bab u J McAlister  and J W idom Characterizing memory requirement for queries over continuous data streams In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  B Babcock S Bab u M Datar  R  Motw ani and J W idom Models and Issues in Data Stream Systems In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  Z Bar Y ossef T  Jayram R K umar  D  S i v akumar  and L T r e visan Counting Distinct Elements in a Data Stream In RANDOM  pages 110 2002  A Belussi and C F aloutsos Estimating the selecti vity of spatial queries using the correlation fractal dimension In VLDB95 Proceedings of 21th International Conference on Very Large Data Bases September 11-15 1995 Zurich Switzerland  pages 299310 1995  J Bunge and M Fitzpatrick Estimating the number of species A r e vie w  Journal of American Statistical Association  88:364373 1993  M Charikar  S  Chaudhuri R Motw ani and V  Narasayya T o w ards estimation error guaranties for distinct values In ACMPODS  pages 268279 2000  S Chaudhuri R Motw ani and V  Narasayya Random sampling for histogram construction How much is enough In ACMSIGMOD  pages 436 447 1998  G Cormode F  K orn S Muthukrishnan and D Sri v astana Finding hierar chical heavy hitters in data streams In VLDB  2003  M Datar  A  Gionis P  Indyk and R Motw ani Maintaing stream statistics over sliding windows In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms  2002  A Deshpande M Garof alakis and R Rastogi Independence is Good Dependency-Based Histogram Synopses for High-Dimensional Data In ACM SIGMOD  2001  C Estan S Sa v age and G V a r ghese Automatically inferring patterns of resource consumption in network trafc In ACMSIGCOMM  2003  P  Flajolet and N Martin Probabilistic Counting Algorithms for Data Base Applications Journal of Computer and System Sciences  pages 182209 1985  N Friedman L Getoor  D  K oller  and A Pfef fer  Learning probabilistic relational models In IJCAI  pages 13001309 1999  S Ganguly  M  Garof alakis and R Rastogi Processing set e xpressions o v e r continuous update streams In ACM SIGMOD  pages 265276 2003  P  Gibbons Distinct sampling for highly-accurate answers to distinct v alues queries and event reports In VLDB  pages 541550 2001  P  J Haas J F  Naughton S Seshadri and L Stok es Sampling-based estimation of the number of distinct values of an attribute In VLDB  1995  I Ilyas V Markl P  Haas P  Bro wn and A Aboulnaga CORDS Automatic Discovery of Correlations and Soft Functional Dependencies In ACMSIGMOD  2004  J Jung B Krishnamurthy  and M Rabino vich Flash cro wds and denial of service attacks Characterization and implications for cdns and web sites In ACMWWW  2002  J Ki vinen and H Mannila Approximate dependenc y inference from relations Theoritical Computer Science  149:129149 1995  G Manku and R Motw ani Approximate frequenc y counts o v e r data streams In VLDB  2002  V  Poosala P  J Haas Y  E Ioannidis and E J Shekita Impro v e d histograms for selectivity estimation of range predicates In ACMSIGMOD  pages 294305 1996  S Stolfo W  Lee P  Chan W  F an and E Eskin Data mining-based intrusion detectors An overview of the columbia ids project ACMSIGMOD Record  30\(4 2001  H W ang D Zhang and K G Shin Detecting SYN Flooding Attacks In INFOCOM 2002  2002  K Whang B V ander Zander  and H T aylor  A Linear Time Probabilistic Counting Algorithm for Database Applications ACM Transactions on Database Systems  pages 209229 1990  D Zhang D Gunopulos V  Tsotras and B  See ger  T emporal and spatiotemporal aggregations over data streams using multiple time granularities Information Systems  28\(1-2 2003 The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofcial policies either expressed or implied of the Army Research Laboratory or the U S Government Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





