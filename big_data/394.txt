Blocking Anonymity Threats Raised by Frequent Itemset Mining Maurizio Atzori  Francesco Bonchi   Pisa KDD Laboratory ISTI CNR Area della Ricerca di Pisa Via Giuseppe Moruzzi 1 56124 Pisa Italy Fosca Giannotti  Dino Pedreschi   Pisa KDD Laboratory Computer Science Dep University of Pisa Largo Pontecorvo 3 56127 Pisa Italy Abstract In this paper we study when the disclosure of data mining results represents per se a threat to the anonymity of the individuals recorded in the analyzed database The novelty of our approach is that we focus on an objective denition of privacy compliance of patterns without any reference to a preconceived knowledge of what is sensi 
tive and what is not on the basis of the rather intuitive and realistic constraint that the anonymity of individuals should be guaranteed In particular the problem addressed here arises from the possibility of inferring from the output of frequent itemset mining i.e a set of itemsets with support larger than a threshold   the existence of patterns with very low support smaller than an anonymity threshold k  I n t he fol l owing w e d evelop a simple methodology to block such inference opportunities by introducing distortion on the dangerous patterns 1 Introduction Consider a medical institution where the physicians collect for research purposes data about their patients Having unrestricted access to the data they can perform real mining on all available information using traditional mining tools  not necessarily the privacy 
preserving ones This way they maximize the outcome of the knowledge discovery process without any concern about privacy of the patients which are recorded in the data But the anonymity of patients becomes a hot issue when the physicians want to share their discoveries e.g association rules holding in the data with their scientic community It is generally believed that data mining results do not violate the anonymity of the individuals recorded in the source database In fact data mining models and patterns in order to ensure a required statistical signicance represent a large number of individuals and thus conceal individual identities this is the case of the minimum support threshold in association rule mining We have recently shown that the above belief is ill-founded  Example 1 Consider the following association rule a 
1  a 2  a 3  a 4  sup 80 conf 98  7 where sup and conf are the usual interestingness measures of support and condence as dened in  S inc e the given rule holds for a number of individuals 80 which seems large enough to protect individual privacy one could conclude that the given rule can be safely disclosed But is this all the information contained in such a rule Indeed one can easily derive the support of the premise of the rule sup   a 1 a 2 a 3 
  sup   a 1 a 2 a 3 a 4   conf  80 0  987 81  05 Given that the pattern a 1  a 2  a 3  a 4 holds for 80 individuals and that the pattern a 1  a 2  a 3 holds for 81 individuals we can infer that in our database there is just one individual for which the pattern a 
1  a 2  a 3  a 4 holds In w e s a y that the t w o itemsets  a 1 a 2 a 3  and  a 1 a 2 a 3 a 4  represent an inference channel a simple one for the pattern a 1  a 2  a 3  a 4  By shifting the concept of k anonymity  f rom d ata to patterns we have formally characterized the notion 
of a threat to anonymity in the context of pattern discovery and provided a methodology to eciently and eectively identify such threats that might arise from the disclosure of a set of extracted patterns In this paper we study a methodology to eliminate the threats to anonymity by introducing distortion on the dangerous patterns in a controlled way by measuring the effects of the distortion 2 Problem Denition Denition 1 A binary database D  I  T  consists of a nite set of binary variables I   i 1 i p   also known as items  and a nite multiset T   t 1 t 
n  of p dimensional binary vectors  transactions  recording the values of the items A pattern for the variables in I is a propositional sentence built by AN D    OR    and NOT    logical connectives on variables in I The domain of all possible patterns is denoted P at  I   Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


D abcdefgh t 1 11111111 t 2 11111010 t 3 11111000 t 4 11111110 t 5 11111000 t 6 11111000 t 7 11011000 t 8 10001110 t 9 00111110 t 10 00111000 t 11 00111111 t 12 11000110 Notation patterns sup D  a  f 1 sup D  e    b  d   4 sup D  h  b  Notation itemsets sup D  abc  sup D  abde  sup D  cd  F  D  8    12    a 9    b 8    c 9    d 10    e 11    ab 8    ae 8    cd 9    ce 9    de 10    cde 9  c C l  D  8    12    a 9    e 11    ab 8    ae 8    de 10    cde 9  d MC h 3  C l  D  8  C cde   1   C ab a  1   C ae a  1   C cde e  1   C cde de  1  a b e Figure 1 Example a the binary database D  b dierent notation used for patterns and itemsets the set of frequent   8 c and of closed d itemsets  e the set of maximal inference channels for k 3  Denition 2 Given a database D  a transaction t D and a pattern p  we write p  t  if t makes p true The support of p in D is given by the number of transactions which make p true sup D  p   t D p  t    Denition 3 The set of all itemsets 2 I  is a pattern class consisting of all possible conjunctions of the form i 1  i 2    i m  Given a database D and a support threshold  thesetof  frequent itemsets in D is denoted F  D    X sup D  X   X  2 I  sup D  X      The problem addressed in this paper is given by the possibility of inferring from theoutputoffrequent itemset mining  i.e F  D   the existence of patterns with very low support  that represent a threat for the anonymity of the individuals about which they are true Figure 1\(b shows the dierent notation used for general patterns and for itemsets  Denition 4 Aset S of pairs  X n  where X  2 I and n  N  and a database D are said to be  compatible if S F  D    Given a pattern p we say that S   sup  p  x respectively S   sup  p  x  if for all databases D  compatible with S  we have that sup D  p   x respectively sup D  p  x  In general  t he supp ort o f a pattern p  i 1  i m  a 1  a n can be inferred if we know the support of itemsets I   i 1 i m   J  I 012 a 1 a n   and every itemset X such that I  X  J  sup D  p   I  X  J   1  X  I  sup D  X 1 The right-hand side of Equation 1 is denoted f J I  D  Denition 5 Given a database D and two itemsets I  J  2 I if 0 f J I  D  k  then the set of itemsets  X  I  X  J  is an inference channel We denote such inference channel C J I and we write sup D  C J I  f J I  D   Example 2 Consider D in Figure 1 and suppose k 3  We have that sup D  b  d  e  f bde b  D  sup D  b   sup D  bd   sup D  be  sup D  bde   7  7+7=1  Therefore C bde b is an inference channel of support 1 In w e h a v e s ho wn that s ince a generic pattern p P at  I  can be considered without loss of generality in normal disjunctive form  we can conclude that all possible threats to anonymity are due to inference channels of the form C J I  However two cases can be distinguished  i  I and J are both frequent itemsets  ii  J is not frequent For lack of space we focus only on the rst and most essential form of channels Denition 6 The set of all C J I holding in F  D   together with their supports is denoted C h  k F  D    C J I f J I  D   J sup D  J  F  D     In our previous paper w e h a v e s tudied the p roblem of how to detect the inference channels holding in a set of frequent itemsets that we want to disclose i.e computing C h  k F  D   In this paper we study how to transform sanitize the collection to remove the inference channels Problem 1 Given a collection of frequent itemsets F  D    and the set of all its inference channels C h  k F  D    transform F  D   in a collection of frequent itemsets O k  which can be safely disclosed O k is the output of our problem and it must satisfy the following conditions  i  C h  k O k     ii  015D   O k  F  D     The second condition constraints the output collection of itemsets to be realistic  i.e to be compatible with at least a database This requirement is due to the fact that disclosing an output which is not compatible with any database could represent a threat In fact a malicious adversary could recognize that the set Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


of itemsets disclosed is not real and he could exploit this leak by reconstructing the missing patterns starting from those ones present in the output We call this kind of threat inverse mining attacks  3 Avoiding Redundant Distortion In this Section we show how using a condensed representation of C h  k F  D   we can avoid redundant distortion when blocking inference channels Consider the two inference channels C acd ad  1  and C abcd abd  1  holding in the database in Fig 1\(a one is more specic than the other but they both uniquely identify transaction t 7  It is easy to see that many other families of equivalent and thus redundant inference channels can be found The theory of closed itemsets  c an help us with this problem Denition 7 The set of frequent closed itemsets is C l  D    X sup D  X  F  D     Y  Xs.t  Y sup D  X  F  D     An itemset I C l  D   is said to be maximal i  J  I s.t  J s C l  D    Analogously to what happens for the pattern class of itemsets if we consider the pattern class of conjunctive patterns we can rely on the anti-monotonicity property of frequency  Proposition 1 Given C J I and C L H we say that C J I C L H when I  H and  J  I    L  H   It holds that C J I C L H D f J I  D   f L H  D   Denition 8 An inference channel C J I is said to be maximal w.r.t D and  if  H L such that I  H and  J  I    L  H   f L H 0  The set of maximal inference channels is denoted MC h  k C l  D    Proposition 2 C J I MC h  k C l  D    I C l  D    J is maximal In w e h a v e p ro v e d t hat f rom MC h  k C l  D   we can reconstruct every channel in C h  k F  D   and its support without accessing the database Proposition 3 Given C J I f J I  D  C h  k F  D    let M be any maximal itemset such that M  J Thefollowing equation holds f J I  D   c  X  f M c  X   D 2 where c  I   c  X   M and c  X    J  I    As the set of all closed frequent itemsets C l  D  ontains all the information of F  D   in a more compact representation analogously the set MC h  k C l  D   represents without redundancy all the information in C h  k F  D   Removing the redundancy existing in C h  k F  D   we also implicitly avoid redundant sanitization and thus we dramatically reduce the distortion needed to block all the inference channels In fact to block an inference channel C J I C h  k F  D   we have two main options  making the inference channel anonymous enough i.e forcing f J I  D   k   making the inference channel disappear i.e forcing f J I  D  The following two propositions show that whichever option we choose we can just block the channels in MC h  k C l  D   obtaining to block all the inference channels in C h  k F  D   Proposition 4 Given a database D  consider a database D  s.t C L H f L H  D  MC h  k C l  D   it holds that f L H  D    k  Then from Proposition 1 it follows that C J I f J I  D  C h  k F  D   f J I  D    k  Proposition 5 Given a database D  consider a database D  s.t C L H f L H  D  MC h  k C l  D   it holds that f L H  D    Then from Proposition 3 it follows that C J I f J I  D  C h  k F  D   f J I  D    In the next Section we exploit the properties above to reduce the distortion needed to sanitize our output 4 Suppressive Sanitization The basic idea of Suppressive Sanitization is to hide inference channels pushing their support to 0 this can be done by removing transactions t s.t I  t   J  I   t    Unfortunately we can not simulate such suppression of transactions simply by decreasing the support of the itemset I by f J I for each C J I C h  k F  D   since we would lose databasecompatibility due to the other items appearing in the dangerous transactions Consider for instance a transaction I 012 x y z   removing it we reduce the support of I  but as uncontrolled side eect we also reduce the support of the itemset I 012 x   Therefore in order to maintain database-compatibility we must take into account these other items carefully One na ve way of achieving this is to really access the database suppress the dangerous transactions and reduce the support of all itemsets contained in the suppressed transactions accordingly But this is not enough while introducing distortion to block the real inference channels holding in F  D   transforming it in O k wecouldpossibly create some new fake inference channels not existing in the original database and thus not violating the anonymity of real individuals We do not allow this possibility although fake such inference channels could be the starting point for a backward reasoning of a malicious adversary in other terms could open the door to inverse mining attacks The unique solution here is to perform again the detection algorithm  a nd if necessary  t o b lo c k the n o v el inference Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


 RETAIL Support 0,4 0,6 0,8 1,0 1,2 Number of Transactions 0 200 400 600 800 1000 1200 1400 Suppressive k = 5 Suppressive k = 10 Suppressive k = 25 Suppressive k = 50 RETAIL Support 0,4 0,6 0,8 1,0 1,2 Itemsets distorted 0 20 40 60 80 100 Suppressive k = 5 Suppressive k = 10 Suppressive k = 25 Suppressive k = 50 RETAIL Support 0,4 0,6 0,8 1,0 1,2 Average Distortion 0,0 0,5 1,0 1,5 2,0 2,5 3,0 Suppressive k = 5 Suppressive k = 10 Suppressive k = 25 Suppressive k = 50 Figure 2 Distortion empirical evaluation channels found Obviously this process can make some frequent itemsets become infrequent Algorithm 1 implements the suppressive sanitization which access the database D on the basis of the information in MC h  k C l  D   and adjust C l  D  ith the information found in D  the following pseudo-code outputs a sanitized version of C l  D   but nothing prevents us from disclosing a sanitized version of F  D   Example 3 Consider C l  D  8 and MC h 3  C l  D  8 in Fig.1\(d and e we got 5 maximal channels C cde   1   C ab a  1   C ae a  1   C cde e  1   C cde de  1  due to transactions t 12 t 8 and t 7  The suppression of these 3 transactions reduces the support of some closed itemsets at the end of the suppression phase line 8 of Algorithm 1 we got that C l  D  8    9    a 6    e 9    ab 6    ae 6    de 9    cde 9   Compacting C l  D  8 means to remove from it itemsets which due to the transactions suppression are no longer frequent or no longer closed lines 9  12 i.e C l  D  8   cde 9  At this point Algorithm 1 invokes the detection algorithm see  t o nd out t he maximal c hannels in the new C l  D  8  and if necessary starts a new suppression phase in our example this is not the case The plots in Fig.2 report the following three measures of distortion recorded on the well-known retail dataset f or dieren t v a lues of  and k  1 Absolute number of transactions virtually suppressed 2 The fraction of itemsets in F  D   which have their support changed in O k   I sup D  I  F  D    sup O k  I    sup D  I   F  D    where sup O k  I  s if  I,s O k  0 otherwise 3 The average distortion w.r.t the original support of itemsets 1 F  D     I F  D    sup O k  I   sup D  I   sup D  I  Algorithm 1 Suppressive Sanitization Input C l  D    MC h  k C l  D    D Output O k 1 while MC h  k C l  D      do 2 Scan the database 3 for all t D do 4 if 012C J I f J I MC h  k C l  D   s.t I  t and  J  I   t   then 5 Transaction suppression 6 for all  X sup D  X  C l  D  t X  t do 7 sup D  X  015 sup D  X   1 8 Compact C l  D   9 for all  X s C l  D   do 10 if 012 Y s C l  D  t Y  X or s then 11 C l  D   015C l  D    X s   12 detect MC h  k C l  D   in C l  D   13 O k 015C l  D   Note that the last measure is really tough with itemsets which become infrequent during the sanitization process for these itemsets we count a maximum distortion of 1  sup O k  I   0 However while the number of itemsets distorted is usually very large the average distortion on itemsets is very low this means that quite all itemsets are touched by the sanitization but their supports are changed just a little References 1 http://fimi.cs.helsinki.fi/data   R  Agra w al T  I mielinski a nd A N Sw ami Mining association rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD   M  A tzori F Bonc h i F Giannotti a nd D P e dresc hi k anonymous patterns In Proceedings of the PKDD05   T  C alders and B  G o e thals Mining all n on-deriv able frequent itemsets In Proceedings of the 6th PKDD  2002  N  P asquier Y Bastide R  T aouil and L  L akhal Discovering frequent closed itemsets for association rules In Proc ICDT 99  1999  L  S w eeney  k anonymity a model for protecting privacy International Journal on Uncertainty Fuzziness and Knowledge-based Systems  10\(5 2002 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


hand, is A C granules and the 2 nd tier, the right hand, is A D granules The relationship between the two tiers is described as an association set  sup e association set also can determine a probability distribution on the set of decision granules \(see [1         i Ci cgwdgAcg i wcg dg G sup Pr   sup G  A C    cg i  0.10 cg 1    dg 1 0.8  dg 2 0.2 0.14 cg 2    dg 1 1  0.49 cg 3    dg 1 1  0.22 cg 4    dg 1 1  0.05 cg 5    dg 2 1  Figure 1 A 2-tier structure for rough associations between condition granules and decision granules  Let n be the number of granules in the decision table, it is obvious G  A C   n In addition, to obtain all decision rules in a decision table, the traditional method needs to search in the table to determine the conclusions for each condition.  The 2-tier structure does not need to search any more. Therefore, it is better of using the 2-tier structure than using a decision table in time complexity  4.2 Updating Rough Association Rules  Let Rule  be the set of positive rough association rules. We use the following weight function to determine the relevance of documents weight  term   cgftermRuledgcg fcg    sup  Given a testing document d we use the following equation for using rough association rules to determine its relevance  dtermVterm D termweight drel    4.2 The consequential result of using Equation \(4.2\ is that many irrelevant documents may be marked in relevance. To avoid making many mistakes, we now consider how to update the positive rough association rules based on some interesting negative rough association rules Give a negative rough association rule \223 cg    dg 2  x  224, where termset  cg  t 1  t 2 205 t m We call it an interesting  negative rule if rel  termse t cg   min  rel  d  d D   We use the following procedure to update some positive rough association rules for all interesting negative rules cg in our experiments for i 1 to G  A C  if termset  cg i  termset  cg    subtract half sup from cg i   else if termset  cg i  termset  cg     reshuffe  cg i 222s term frequency distribution by shifting half weights from all terms termset  cg i  termset  cg  to cg i 222s rest terms For example, \223 cd 5   dc 2 1 224 is an interesting negative rule \(see in Figure 1 and Table 4\ and it does not include any condition granules but we have that termset  cd 2  termset  cd 5  termset  cd 3  termset  cd 5  t 3  t 4   Therefore reshuffle operation can be used to update the frequency distributions of cd 2 and cd 3 We first take half weight from both t 3 and t 4 The total weights we can take from cg 2 and cg 3 are \(1/4 + 1/8\ 3/8 and \(1/5 1/10\ = 3/10, respectively. We also distribute the total weight 3/8 to t 6 for cg 2 and 3/10 to both t 5 and t 6  for cg 3   Table 5 Reshuffle operation   Table 5 illustrates the result of reshuffling term frequency distributions for condition granules cd 2 and cd 3 where cd 5 is the negative rule. To compare with the weights in Table 4, we can find that the weights of t 3 and t 4 in granules cd 2 and cd 3 are weakened respectively because t 3 and t 4  are terms of  cd 5   5. Evaluation  We use Reuters Corpus Volume 1, also known as RCV1, to evaluate the proposed method. We also use the first 20 topics that TREC \(Text REtrieval Conference, see http://trec.nist.gov eveloped for filtering track in 2002 We compare rough association rule mining model with two baseline models: closed pattern based association rule model \(see Section 2 for details\, and binary decision rule model \(see Section 3\  A common basic text processing is used for all models, which includes case folding, stemming, stop words removal and 150 term selection that uses tf*idf term frequency times inverse document frequency\ technique In the training phase, the closed pattern based model finds all closed patterns \(where the min_sup is Condition granule t 1 t 2 t 3 t 4 t 5 t 6 t 7  cg 1 4/7 3/7 cg 2  1/4 1/8  5/8  cg 3  1/5 1/10 7/20 7/20  cg 4 1/3 2/9  2/9 2/9 cg 5 1/2 1/2  


the double of the average frequency of terms, and the size of largest patterns is 5\ and calculates their support and confidence values. In the testing phase the relevance of document d is the sum of the multiplications of support and confidence of all closed patterns that occur in d  The binary decision rule model, first obtains a set of granules G in the training phase. It also uses the following equation to evaluate a weight for each term    gtermGg gtermset gstrength termweight      Given a testing document d we also use Equation 4.2\ to determine its relevance in binary decision rule model Table 6 is the experimental results. We use two measures in the table top 25 precision and breakeven points where a breakeven point is a point in the precision and recall curve with the same x coordinate and y coordinate  Table 6 Experimental results Rough association rules Binary decision rules Closed association rules Avg. of breakeven points 0.51 0.48 0.49 Avg. of top25 precision 55.00 50.60% 49.20  Figure 2 shows the differences of rough association mining, binary decision rule mining and closed pattern based association rule mining in top 25 precision for the 20 topics. The positive values \(the bars above the horizontal axis\ mean the rough association mining performed better than others. The negative values \(the bars below the horizontal axis mean others performed better than rough association mining                                        60.00 40.00 20.00 0.00 20.00 40.00 60.00 80.00 1 2 3 4 5 6 7 8 9 1011121314151617181920 Topic Precisio n   Rough-Binary  Rough-Closed   Figure 2 Difference between models  It is no less impressed by the performance of the rough association rule mining since both top 25 precision and breakeven points gain a significant increase. As a result of the experiment we believe that the proposed method is significant since they can improve the effectiveness of the association discovery for Web text mining  6. Related Work  The key issue regarding the effectiveness of WIG is automatic acquiring of knowledge from text documents for describing user profiles  13   It i s also a fundamental issue in Web personalization [3   Traditional information retrieval \(IR\ techniques can be used to provide simple solutions for this problem. We can classify the methods into two categories: single-vector models and multi-vector models. The former models produce one term-weight vector to represent the relevant information for the topic [4  17 T he l a t t e r m o de l s  pr od uc e  m ore  t ha n one vector [1 9  Th e m ain dr awb ack  o f I R b ased  models is that it is hard to interpret the meaning of vectors using user acceptable concepts Text mining tries to derive meaning from documents. Association mining has been used in Web text mining for such purpose for association discovery trends discovery, event discovery, and text classification [6  8 1 9  The association between terms and categories \(e.g a term or a set of terms\can be described as association rules. The trends discovery means the discovery of phrases, a sort of sequence patterns. The event discovery is the identification of stories in continuous news streams   Us ua l l y c l us t e r i ng ba s e d mining techniques can be used for such a purpose. It was also necessary to combine association rule mining with the existing taxonomies in order to determine useful patterns [1    To compare with IR-based models, data miningbased Web text mining models do not use term independent assumption 1 4   A l s o W e b m i n i ng models try to discover some unexpected useful data    T he di s a dv a nt a g e  of a s s oc i a t i on ru l e m i ni n g i s t ha t  the discovered knowledge is very general what makes the performance of text mining systems ineffectively 2 Decision rule mining [16  12   23  c a n be  a  possible solution to specify association rules However, there exists ambiguities whist we use the decision rules for determining other relevance information for specified topics. Rough association 


rule mining can be used to overcome these disadvantages  7. Conclusions  This paper, discusses the application of data mining techniques within Web documents to discover what users want. It introduces the concept of decision patterns in order to interpret decision rules in terms of association mining. It has proved that any decision pattern is a closed pattern. It also presents a new concept of rough association rules to improve of the quality of text mining. To compare with the traditional association mining, the rough association rules include more specific information and can be updated dynamically to produce more effective results The distinct advantage of rough association rules is that they can take away some uncertainties from discovered knowledge through updating supports and weight distributions of association rules. It also demonstrates that the proposed approach gains a better performance on both precision and recall, and it is a considerable alternative of association rule mining This research is significant since it takes one more step further to the development of data mining techniques for Web mining  References   M. L. Antonie and O R. Zaiane, Text document categorization by term association, 2 nd  IEEE International Conference on Data Mining Japan, 2002, 19-26  G Cha n g, M  J  Healey J   A. M  M c Hugh a n d J   T L Wang Mining the World Wide Web: An information search approach Kluwer Academic Publishers, 2001   M   E i rinaki and M. Vazirgiannis  W e b  mining for web personalization ACM Transactions on Internet Technology  2003 3\(1 1-27 4  D A  Evans et a l   CL ARIT  e x pe ri ment s i n  bat c h filtering: term selection and th reshold optimization in IR and SVM Filters TREC02 2002  U Fay y a d G. P i atetsky Shapiro, P  S m y t h a n d R Uthrusamy, eds Advances in knowledge discovery and data mining Menlo Park, California: AAAI Press/ The MIT Press, 1996  R. F e ldman and H. Hirsh Mining as sociation s in text in presence of background knowledge, 2 nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1996, 343-346  J  W  Guan D. A Bell D Y L i u T h e rough s et approach to association rules, 3 rd IEEE International Conference on Data Mining USA, 2003, 529-532 8 J  D Hol t a n d S  M. Chung, M u ltipas s algor ithms  for mining association rules in text databases Knowledge and Information Systems 2001 3 168-183  L i and B. Liu, Learning to classify texts using positive and unlabeled data IJCAI 2003, 587-592  i and N Zhong, W e b mining model and its applications on information gathering Knowledge-Based Systems 2004 17 207-217  N Zhong Ca pturing evolving patterns for ontology-based IEEE/WIC/ACM International Conference on Web Intelligence Beijing, China, 2004, 256-263  Z hong Interp retations of association rules by granular computing, 3 rd  IEEE International Conference on Data Mining USA, 2003, 593-596  i ning ontology fo r automatically  acquiring Web user information needs IEEE Transactions on Knowledge and Data Engineering 2006 18\(4 554-568   Li u Y  Da i X L i  W S. Lee, and P. S. Yu, Building text classifiers using positive and unlabeled examples, 3 rd  IEEE International Conference on Data Mining USA, 2003 179-186  J. Mo staf a  W   Lam  and M. Palakal A m u ltil e v el approach to intelligent information filtering: model, system and evaluation ACM Transactions on Information Systems  1997 15\(4 368-399   Pawlak In purs uit of patterns in data re as oning from data, the rough set way 3 rd International Conference on Rough Sets and Current Trends in Computing USA, 2002 1-9  Sebas tiani M a chin e  learning in autom a te d text categorization ACM Computing Surveys 2002 34\(1 1-47  z vetkov X Yan a nd J. Han, TSP: Mining top-K closed sequential patterns 3 rd IEEE International Conference on Data Mining USA, 2003, 347-354  S.T W u  Y  Li Y X u  B  P h am and P Ch en Automatic pattern taxonomy exatraction for Web mining IEEE/WIC/ACM International Conference on Web Intelligence Beijing, China, 2004, 242-248  H  Y u  J H an, and K  C h ang, P E B L  pos itive exam pl e  based learning for Web page classification using SVM KDD02 2002, 239-248  n Y. Fu M i ning M u ltiple-level Association Rules in Large Databases IEEE Transactions on Knowledge and Data Engineering 1999 11\(5 798-805  G I. Webb and S  Z hang, K-op timal rule discovery  Data Mining and Knowledge Discovery 2004 10 39-79  Y Yao Y Zhao a nd R.B. Maguire, Explanation oriented association mining using rough set theory 9 th  International Conference on Rough Sets, Fuzzy Sets, Data Mining and Granular Computing China, 2003, 165-172 


33 3 xyz3 Itemset SUPPORT xyz3Closed Itemset Equivalence Class abcd abce abcde abde acde abd abe ade 1 11 12 2 2 2 cb  a bdbc de bcd cde d e4 4 4 4 4 4 4 3 3 3 3 6 6 6 acd ace2 2 2 2 Cfreq CM a b Figure 3. Equivalence classes of frequency when intersected by a CAM \(a b  constraint Example 1 In Figure 3\(a itemsets lattice with the frequency equivalence classes and the border of the theory of CAM ? sum\(X.price have that I1 = {?bc, 6?} while on the other hand I2 = {?ac, 3?, ?bc, 6?, ?bd, 4?, ?cd, 4?, ?ce, 4? ?cde, 3 What has happened is that some equivalence classes have been cut by the CAM constraint. With interpretation I1 we mine closed frequent itemsets and then we remove those ones which do not satisfy the CAM constraint: this way we lose the whole information contained in those equivalence classes cut by the CAM constraint. On the other hand, according to interpretation I2, we mine the set of itemsets which satisfy the CAM constraint and then we compute the closure of such itemsets collection: thus, by de?nition, the itemsets bd and cd are solutions because they satisfy CAM and they have not a superset in the result set with the same support and satisfying the constraint Which one of the two di?erent interpretations is the most reasonable? It is straightforward to see that interpretation I1 is not a condensed representation since it loses a lot of information. In extreme cases it could output an empty solutions set even if there are many frequent itemsets which satisfy the given set of user-de?ned constraints. On the other hand, interpretation I2, which corresponds to the de?nition Cl\(FThD\(Cfreq[D,?] ? CAM representation of FThD\(Cfreq[D,?] ? CAM Observe that I2 is a superset of I1: it contains all 


Observe that I2 is a superset of I1: it contains all closed itemsets which are under the CAM border \(as I1 plus those itemsets which arise in equivalence classes which are cut by the CAM border \(such as for instance ce and cde in Figure 3\(a Proposition 3 Cl\(FThD\(Cfreq ? CAM FThD\(Cfreq CAM Let us move to the dual problem. In Figure 3\(b show the usual equivalence classes and how they are cut by CM ? sum\(X.prices are upward closed, we have no problems with classes which are cut: the maximal element of the equivalence class will be in the alive part of the class. In other words when we have a CM constraint, the two interpretations I1 and I2 correspond Proposition 4 Cl\(FThD\(Cfreq ? CM FThD\(Cfreq CM The unique problem that we have with this condensed representation, is that, when reconstructing FThD\(Cfreq[D,?] ? CM care of testing itemsets which are subsets of elements in Cl\(FThD\(Cfreq ? CM not to produce itemsets which are below the monotone border B+\(Th\(CM not need to access the transaction dataset D anymore Since we mine maximal itemsets of the equivalence classes it is impossible to avoid this problem, unless we store, together with our condensed representation the border B+\(Th\(CM closed itemset. This could be an alternative. However since closed itemsets provide a much more meaningful set of association rules, we consider a good tradeo? among performance, conciseness and meaningfulness the use of Cl\(FThD\(Cfreq?CM resentation Finally, if we use free sets instead of closed, we only shift the problem leading to a symmetric situation. Using free sets interpretations I1 and I2 coincide when dealing with anti-monotone constraints because minimal elements are not cut o? by the constraint \(e.g. de in Fig. 3\(a constraints \(e.g. no free solution itemsets in Fig. 3\(b Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 4. Algorithms In this Section we study algorithms for the computation of MP5. We ?rst discuss separately how monotone and anti-monotone constraints can be pushed in the computation, then we show how they can be exploited together by introducing the CCIMiner algorithm 4.1. Pushing Monotone Constraints Pushing CAM constraints deep into the frequent itemset mining algorithm \(attacking the problem FThD\(Cfreq[D,?] ? CAM  since they behave exactly as Cfreq . The case is di?erent for CM constraints, since they behave exactly the opposite of frequency. Indeed, CAM constraints can be used to e?ectively prune the search space to a small downward closed collection, while the upward closed collection of the search space satisfying the CM constraints cannot be exploited at the same time. This tradeo? holding on the search space of the computational problem FThD\(Cfreq[D,?] ? CM extensively studied [18, 9, 4], but all these studies have failed to ?nd the real synergy of these two opposite types of constraints, until the recent proposal of ExAnte [6]. In that work it has been shown that a real synergy of the two opposites exists and can be exploited by reasoning on both the itemset search space and the transactions input database 


set search space and the transactions input database together According to this approach each transaction can be analyzed to understand whether it can support any solution itemset, and if it is not the case, it can be pruned In this way we prune the dataset, and we get the fruitful side e?ect to lower the support of many useless itemsets, that in this way will be pruned because of the frequency constraint, strongly reducing the search space. Such approach is performed with two successive reductions  reduction \(based on monotonicity reduction \(based on anti-monotonicity to  reduction we can delete transactions which do not satisfy CM , in fact no subset of such transactions satis?es CM and therefore such transactions cannot support any solution itemsets. After such reduction, a singleton item may happen to become infrequent in the pruned dataset, an thus it can be deleted by the ?reductions. Of course, these two step can be repeated until a ?xed point is reached, i.e. no more pruning is possible. This simple yet very e?ective idea has been generalized in an Apriori-like breadth-?rst computation in ExAMiner [5], and in a FP-growth [10] based depth-?rst computation in FP-Bonsai [7 Since in general depth-?rst approaches are much more e?cient when mining closed itemsets, and since FP-Bonsai has proven to be more e?cient than ExAMiner, we decide here to use a FP-growth based depth?rst strategy for the mining problem MP5. Thus we combine Closet [16], which is the FP-growth based algorithm for mining closed frequent itemset, with FPBonsai, which is the FP-growth based algorithm for mining frequent itemset with CM constraints 4.2. Pushing Anti-monotone Constraints Anti-monotone constraints CAM can be easily pushed in a Closet computation by using them in the exact same way as the frequency constraint, exploiting the downward closure property of antimonotone constraints. During the computation, as soon as a closed itemset X s.t  CAM \(X ered, we can prune X and all its supersets by halting the depth ?rst visit. But whenever, such closed itemset X s.t  CAM \(X e.g. bcd in Figure 3\(a some itemsets Y ? X belonging to the same equivalence class and satisfying the constraint may exist \(e.g bd and cd in Figure 3\(a ery such X in a separate list, named Edge, and after the mining we can reconstruct such itemsets Y by means of a simple top-down process, named Backward-Mining, described in Algorithm 1 Algorithm 1 Backward-Mining Input: Edge, C, CAM , CM C is the set of frequent closed itemsets CAM is the antimonotone constraint CM is a monotone constraint \(if present Output: MP5 1: MP5 = C split Edge by cardinality 2: k = 0 3: for all c ? Edge s.t. CM \(c 4: E|c| = E|c| ? {c 5: if \(k &lt; |c 6: k=c generate and test subsets 7: for \(i = k; i &gt; 1; i 8: for all c ? E|i| s.t. CM \(c 9: for all \(i? 1 10: if  Y ?MP5 | s ? Y 11: if CAM \(s 12: MP5 = MP5 ? s 13: else 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





