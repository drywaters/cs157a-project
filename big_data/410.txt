A Constructing Algorithm of Concept Lattice with Attribute Generalization Based on Cloud Models Xie Cui-hua   Li Yun   Shen Jie   Cai Jun-jie  Luo Jian-li School of Information and Engineering, Yangzhou University, 225009, Jiangsu, China xiecuihua@etang.com yzliyun@163.com   yzshenjie@hotmail.com Abstract Data mining is to extract the implicit, potential useful information from data. For containing the trivial detail of original concept hierarchies, a large number of rules are directly mined from relational databases and most of such rules are unnecessary. However mining information in high hierarchies will generate some rules that are interesting and useful. In this paper, cloud models is adopted to control the generalization of a set of qualitative attributes, and a new constructing algorithm of concept lattice based on cloud models is presented for mining association rules in large databases, which is integrating attributeoriented generalization and concept lattice. Finally, a specified experiment is conducted to illustrate the superiority of rule knowledge discovery using such lattice with attribute generalization  Keywords concept lattice;  cloud models;  Attributeoriented generalization  1. Introduction Data mining is to extract the implicit, potential useful information from data. For containing the trivial detail of original concept hierarchies, a large number of rules are directly mined from relational databases, and most of such rules are unnecessary. However, mining information in high hierarchies will generate some rules that are interesting and useful. Therefore, it is necessary to abstract the data from low hierarchies to high hierarchies according to the mining goal, which is called data generalization 1 Data generalization can be done in different concept hierarchies, simultaneity, is effective means for compressing data. In the previous studies, many researches have been developed for data generalization. For example, an algorithm of attributeoriented generalization 2 When it is used for numeric attribute in the proceeding of data generalization by the sharp partition, it can not reflect the actual distributing of the data in the background Cloud models 3 e fuzziness and randomness of a linguistic term between quantities and qualities in a unified way. This method can reflect the distribution of data in that domain while keeping the soft boundaries. Therefore, the discovered association rules are easy to understand The hasse diagram of concept lattice 4 reveals the concept hierarchy of the context. It also shows the generalization/specialization relationship between the concepts corresponding to the subset relationship en the property and object sets. Therefore the graph can be used to produce hierarchies and association rules which are consistent with specialization    In this paper, we present a new constructing algorithm of concept lattice using attribute generalization based on cloud models for mining association rules in large databases, which is integrating attribute-oriented generalization and concept lattice The paper is organized as follows. In Section 2, the background and related concept lattice are introduced In Section3, Attribute-oriented generalization based on cloud models is discussed, then we present a new constructing algorithm of concept lattice using attribute generalization based on cloud models. In Section 4 with its application of mining association rules in databases, results from the example of the proposed approach on a large database are presented. The study is concluded in Section 5 2. The basic theory of concept lattices We present the basic definition of a concept Galois\or a binary relation. More details about the underlying theory of concept lattices can be found in \(Barbut & Monjardet, 1970; Davey & Priestley 1992; Wille, 1992\ A \(formal\text is a triple \(G M, I\here G and M are two finite sets and I is a binary relation between G and M, i.e 001\002 G 327 M. The notation gIm is used instead of writing \(g,m  002 004\217 I Given A 002 G and B 002 004\217 M, define A 001\215 m 003 M 004  004\221 g 003 A\ gIm B 001\215 g 003 004\217 G 004  004\221 m 003 B\Im A concept of the context \(G, M, I\ defined as a pair \(A, B\here Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


A 002 G , B 002 M, A 001\215 B and B 001\215 A A partial order on the concepts can be defined as follows:\(A1, B1  005 A2, B2\ A1 002 hich is equivalent to B2 002 B1 The set of all concepts for the context \(G, M, I with partial order 005 004\217 is a complete lattice called the concept \(or Galois\f the context \(Davey Priestley, 1992\d is denoted here by CL\(G, M, I The partial order is used to generate the graph in the following manner: there is an edge \(C1, C2\f C1 C2 and there is no other element C3 in the lattice such that C1 < C3 < C2. The concept C1 is called parent of C2 and C2 is a child of C1. The graph is usually called am, the edge direction is implicit \(here upwards A formal context can be considered as \(the mathematical model of\le, which relates objects and attributes of a \215real situation". The entries in the table indicate by a cross \(in the program ConImp by the letter "x"\ that the object, the name of which precedes the corresponding row, has the attribute, the name of which is at the top of the corresponding column \(of the entry 3 The constructing algorithm of concept lattice with attribute generalization based on cloud models xt, G={1, 2, 3, \203 002\310 30 002\310 M={ province \(PR\, agricultural acreage\(AA agricultural population\(AP\arm investments \(FI\e total value of agricultural output\(TTVAO d M For containing disperse quantitive attributes in formal context, Directly constructing a concept lattice for mining information by carving up numeric attribute averagely, usually generates a large number of useless concept of attributes and association rules We present a new constructing algorithm of concept lattice using attribute generalization based on cloud models for mining association rules in large databases. From our propositions and example, the algorithm is shown valid and can efficiently solve the problem of the quantity of rules 3.1. Attribute-oriented generalization Attribute-oriented generalization\(AOG 2 is a summarization algorithm that integrates the climbing tree and dropping condition methods for generalizing data in a database. Transforming a specific data description into a more general one is called generalization. For example, consider the database shown in Tab.1. Using the CH\(concept hierarchy user-defined parameters to guide the generalization one of the many possible summaries which can be generated is shown in Fig.1, where the province have been selected for generalization, and the actual values for the province attribute in each tuples have been generalized to the level of Northeast, North Northwest, East, middle, South, Southwest Tab.1  The statistics of Agriculture information PR AA AP FI TTVAO heilongjiang 8826.53 20084.6 343 24540 liaoning 3470.40 22724.4 327 27380 jiling 3935.53 14883.2 195 18910 9.6 216 64750 hebei 6560.47 52315.5 279 35760 beijing 414.47 3952.96 132 7020 tianjing 432.27 3830.47 120 5490 henian 6944.40 74522.5 479 50200 7.7 167 12480 9.6 123 17000 2.5 286 10310 niemenggu 4911.53 14877.7 156 15690 ningxia 795.00 3542.94 94 2470 xinjiang 3072.93 10005.9 325 14470 qinghai 572.00 3101.09 72 2450 zhejiang 1731.13 35426.6 173 33680 4562.33 52707.6 192 58050 shanghai 324.00 4188.87 176 6820 anhui 4373.00 48175.6 333 37090 huabei 3486.60 41728.7 395 40220 60 51824 210 39740 jiangxi 2355.47 30530.3 113 25520 guangdong 2524.67 47690.1 468 60070 guangxi 2578.47 36766.4 137 25220 fujian 1238.47 24994.3 117 22870 hainan 433.53 5146.28 212 6870 7.2 340 63710 guizhou 1854.00 28402.1 87 14550 9.8 223 21170  In the proceeding of the numeric attribute generalization, the concept hierarchy tree, which actually is the numerical hierarchy, is usually established according to the distribution characteristics of each attributes for the means that integrate the climbing tree and dropping condition methods for generalizing data in database. There is not an effective ans for calculating the optimal classification problem in constructing histogram 3  In this paper, we use an algorithm of attributeoriented generalization based on cloud models. It carves up the area coverage according to the actual distribution of the data in the background. It has effectively integrated the fuzziness and randomness of Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


a linguistic term between quantities and qualities in an the actual distribution of the data in the background 3.2. The algorithm of Attribute-oriented generalization based on cloud models 3.2.1. Cloud Models The cloud model 3 is a model of the uncertainty transition between a linguistic term of a qualitative concept and its numerical representation. It has three digital characteristics, Expected value \(Ex\tropy En\d Hyper-Entropy \(He\hich well integrates the fuzziness and randomness of spatial concepts in a unified way. In the discourse universe, Ex is the position corresponding to the center of the cloud gravity, whose elements are fully compatible with the spatial linguistic concept; En is a measure of the concept coverage; He is a measure of the uncertainty of the entropy En A piece of cloud is made up of lots of cloud drops visible shape in a whole, but fuzzy in detail, which is similar to the natural cloud in the sky. Any one of the cloud drops is a mapping in the discourse universe from qualitative concept. With the cloud model, the mapping from the discourse universe to the interval  epoin t to m u lti poin t tran s ition   As w e l l  the degree that any cloud drop represents the qualitative concept can be specified. Given three digital characteristics Ex, En and He of qualitative concept Ci the cloud generator can produce as many drops of the cloud as you would like. In the extreme case, {Ex, 0 0}, where both the entropy and hyper entropy equal to zero, denotes the concept of a deterministic datum, and the greater the number of cloud drops, the more deterministic the concept 3.2.2. Cloud Generators Given three digital characteristics Ex, En, and He to represent a linguistic term, a set of cloud drops may be generated by the following algorithm Algorithm 1: Normal Cloud Generation Input: the expected value of cloud Ex, the entropy of cloud En, the hyper entropy of cloud He, the number of drops N Output: a normal cloud with digital characteristics Ex En, and He 1\ Produce a random value x which satisfies with the Input: the expected value of cloud Ex, the entropy of cloud En, the hyper entropy of cloud He, the number the hyper entropy of cloud He, the number of drops N Output: a normal cloud with digital characteristics Ex En, and He 1\ Produce a random value x which satisfies with the normal distribution probability of mean = Ex, and standard error = En 2\ Produce a random value En\220 which satisfies with the normal distribution probability of mean = En, and standard error = He 3\ Calculate   2 2   2  n x E x x E e 212\212 265 4\ Let \(x,\265\(x\\ be a cloud drop in the universe of discourse 5\ Repeat 1-4 until the number of drops required all generated The idea of using only three digital characteristics to generate a cloud is creative. A series of linguistic  hardware and software and are a patented invention in China 3  3.2.3 000\003 the algorithm of carving up concept Based on the definition of cloud transform, the algorithm of carving up concept is given with spiking method, the more details can been seen in 3.3 constructing algorithm of concept lattice with attribute generalization based on cloud models In order to constructing the concept lattice with the boolean values, converting quantitative attributes into qualitative concept with the algorithm of carving up the concept. It carves up the area coverage according to maximal variance method into 3 pieces of linguistic atoms. Such as : \215little\216 001\303 215middle\216 001\303 215big\216; \215many\216 001\303 215middle\216 001\303 215few\216; \215high\216 001\303 215middle\216 001\303 215low\216 and so on Algorithm 2: constructing algorithm of concept lattice with attribute generalization based on cloud models input 002\326 the formal context output 002\326 the hasse diagram of concept lattice with attributes generalization Begin Step 1 002\326 China Northeast North Northwest East middle South Southwest 001\002\001\002\001\002 001\002\001\002 heilongjiang  liaoning   jieling                     sichuang  guizhou  yunnan  xizang Fig.1 the concept hierarchy of the province Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


Suppose : n is the number of attributes in the formal context For 1 000\224 i 000\224 n If Ai is quantitive attribute  then converting it into qualitative concept by using the algorithm of carving up the concept 002\327 Else dealing it with the algorithm of AOG End if End for A new formal context is produced Step 2  002\326 Analyzing the relation between conditional attributes and key attribute by using concept lattice For 1 000\224 j 000\224 n if Aj do not have the strong relation of key attribute then it will be deleted from the new formal  context 002\327 end if End for A succinct formal context is produced Step 3 002\326 constructing concept lattice from succinct formal context with  the incremental constructing algorithm Constructing hasse diagram of concept lattice with the concept explorer 1.2 8  with attribute generalization NO PR AA AP FI TTVA O 1 Northeast big middle big middle 2 Northeast middle middle big middle 3 Northeast middle few middle low 4 North big many middle high 5 North big many middle middle 6 North little few little low 7 North little few little low 8 North big many big high 9 North middle middle middle low 10 Northwest middle middle little low 11 Northwest middle middle middle low 12 Northwest middle few middle low 13 Northwest little few little low 14 Northwest middle few big low 15 Northwest little few little low 16 East little middle middle middle 17 East middle many middle high 18 East little few middle low 19 East middle many big middle le middle middle big middle le middle many middle middle le middle middle little middle 23 South middle many big high 24 South middle middle little middle 25 South little middle little middle 26 South little few middle low 27 Southwest big many big high 28 Southwest little middle little low 29 Southwest middle middle middle middle 30 Southwest little low little low 4.  A case study and analysis 4.1. A case study We use a case to illuminate the feasibility and validity of algorithm. The data of the case comes from the statistics of Agriculture information \(china,1990 The formal context of the case is Tab.1 Step 1 002\326 Using the CH\(concept hierarchy\f the space location to guide the generalization , the actual values for the province attribute in each tuples have been generalized to the level of Northeast, North Northwest, East, middle, South, Southwest In order to construct the concept lattice with the boolean values, converting quantitative attributes into qualitative concept with the algorithm of carving up the concept. It carves up the area coverage according to maximal variance method into 3 pieces of linguistic atoms Step 2 002\326 alyzing the relation between conditional attribute and key attribute by using the Divide-and-Conquer strategy agricultural acreage\(AA\d farm investments \(FI not have the power relation of the total value of agricultural output\(TTVAO\. A succinct formal context\(the part of Tab.2  is not the italic\s produced with AA delected Step 3 constructing concept lattice from succinct formal context with the algorithm of Godin, Fig.2 is the hasse diagram of the succinctness formal context There are many algorithms for mining rule knowledge based on concept lattice. In this paper, we will not discuss the details of these algorithms We use the Northeast node of the hasse diagram to illuminate the process of mining rule knowledge 7 002\304 the minimal support of rule is 2 002\310 the minimal confidence of rule is 100 002\305\002\326 In order to describing the process of mining lue of agricultural population\(AP\which is many by APmany, and so on Firstly, we mine the association rules from the nodes which is above the Northeast flag can find a association rule 002\304 support of rule is 2 002\310 confidence of  rule is 100 002\305  3 >Northeas  FI m i ddle Secondly, mining the branches which have the below node of the Northeast flag. The following association rule set can been mined 1 < 2 > Northeast  APmiddle [100 2 F I big   TTVAOmiddle 2 < 2 > Northeast  FIbig [100 APmiddle  TTVAOmiddle 3 < 3 > Northeast [67 P m i ddle F I big   TTVAOmiddle Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


4 < 7 > APmiddle  FIbig  TTVAOmiddle [29  2 > Northeast The third and fourth association rule can be deleted because confidence is less than 100 4.2.  The analysis of conclusion For containing the trivial detail of original concept hierarchies, a large number of rules are directly mined from relational databases, scantly 17 association rules Fig.2 the Hasse diagram of the succinctness formal context are mining from the database based on concept lattice with attribute generalization using cloud models, the  5 Conclusion The algorithm which we presented has the following advantages 002\326 1\The cloud concept provides a means of both qualitative and quantitative characterization of linguistic terms. This method can reflect the distribution of data in that domain while keeping the soft boundaries 2\nalyzing the relation between conditional the Divide-andConquer strategy, from the hasse diagram, we can find the attribute which do not have the power relation of the key attribute 3 000\003 Using concept lattice, we can overcome the shortcoming of AOG which can not find the relation of attribute 4\We can find the succinctness association rule from the hasse diagram, and can also choose the concept hierarchy to find the association rules If user has the interesting of the relation between part objects, there are two methods to deal with it 002\326 1\ constructing concept lattice from part objects using above algorithm. The shortcoming of this method does not have the complete relation 2\nstructing concept lattice from all objects using above algorithm, choose the relevance concept en attributes. The shortcoming of this method has large compute We can avoid the above shortcoming by using the incremental constructing algorithm of concept lattice ruct concept lattice from part objects using above algorithm for the analysis of the relation between attributes, when the result is not satisfactory user continually add objects to construct concept lattice until the result is satisfactory REFERENCES 1 H a n J i a w e i, Kam be r M. D a ta Mining C onc e p ts a n d Techniques. Morgan Kaufmann publishers,  2001 2 H a n J  F u Y   Ex plora t i on of the  P o w e r of  A ttribute  Advances in Knowledge Discovery and Data Mining AAAI/MIT Press, Portland, Oregon, 1996 pp 399-421 3 L i D e y i. U n c e r ta inty rea s oning ba s e d o n c l o ud m ode ls  in controllers. Journal of Computer science and Mathematics with Application, 1998 pp 99-123 4 W ille R. Re struc t uring la ttic e  the ory  A n a pproa c h  ba se d  on hierarchies of concepts. In: Rivall eds. Ordered Sets Dordrecht:  Reidel, 1982 pp 445- 470 5  X i e Z h i-pe ng L i u Z ong t ia n. C onc e p t la ttic e a n d   association rules discovery, Journal of computer research & development. 2000,  37\(12 pp 1 6 Y u n L i Z ong tia n L i u, L i ng  Che n W e i Che n g  C u ihua   Xie. Extracting Minimal Non-Redundant Associate Rules from QCIL, Proceedings of the 4th International Conf. on Computer and Information Technology CIT\22004\, 2004 pp 986-991 7 Du Yi, L i De y i. Conc e p t  P a r titio n Ba se d on Clo ud a n d Its Application to Mining  Association Rules.  Journal of Software, 2001,12\( 2 pp 196-203  ttp://sou rcef or g e net/projects/conexp Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


CEC-East  04 0-7695-2206-8/04 $ 20.00 IEEE Figure 3. Web Site Structure for the Real Dataset As shown in Table 5, we consider not only forward information but also backward information, and we use web site structure to prune unnecessary candidates Especially, our algorithm allows the noise exist in the transactions.  For example, suppose that a user traverses AXBC and the web site structure has a direct link between A and B.  Although AB is not successive in the access path, AB is counted in the counting procedure.  In other words, X can be seen as a noise, hence we can ignore it 4. Experimental results The following experiments use a real web transaction dataset and several simulated datasets for testing the performance of IPA algorithm.  In the real dataset, it is a networked database used to store information for renting DVD movies.  There are 82 web pages in the web site We collect the user traversing and purchasing behaviors from 02/18/2001 to 03/24/2001 \(seven days There are 428,596 log entries in the original dataset Before mining the web transaction patterns, we need to preprocess these web logs.  The steps are listed below 1. Data Arrangement: A customer's request to view a particular web page often results in several log entries since images are downloaded in addition to the HTML file.  Because we want to get a picture of the customer's behavior, the log entries about images are not important.  Thus, all log entries with filename suffix like .JPG, .GIF, .SME, .CDF are removed After this step, 53,246 log entries are reserved 2. User Identification: Next, users must be identified In this paper, we partition the log entries according to the user's IP address.  That is, we assume that each IP address is used by only one user even if different users may share the same IP address Proceedings of the IEEE International Conference on E-Commerce Technology for Dynamic E-Business  CEC-East  04 0-7695-2206-8/04 $ 20.00 IEEE 3. Session Identification: It is very possible that user visit a web site more than once.  Thus, the goal of the session identification is to identify the different visits for each user.  The method of achieving this is through time out.  That is, if the time between two requests exceed the time limit, we assume that the user start a new session According to these steps, we reorganize the original log entries into 12,157 users  traversing and purchasing sections.  In these sections, the maximum traversing and purchasing length is 27, the minimum length is 1 and the average length is 3.9.  Only 535 \(4.4 records have purchased items.  Based on this real dataset, Table 6 shows the execution time of IPA algorithm in different minimum supports for traversal sequences.  It ranges from 1.251 to 1.071 seconds Table 7 then shows the rule number generated by the IPA algorithm.  It ranges from 44 to 8 rules Table 6. Execution time \(seconds Minimum Support for Traversal Sequence Frequent Itemset 5% 10% 15% 20 L1 0.03 0.02 0.02 0.02 L2 1.091 1.131 1.061 1.041 L3 0.02 0.01 0.01 0.01 L4 0.03 0.02 0.01 N/A L5 0.04 0.02 N/A N/A L6 0.04 N/A N/A N/A Total 1.251 1.201 1.101 1.071 


Total 1.251 1.201 1.101 1.071 Table 7. Rule number in real dataset Minimum Support for Traversal Sequence Frequent Itemset 5% 10% 15% 20 L1 14 4 4 4 L2 13 6 4 3 L3 8 6 2 1 L4 5 4 0 0 L5 3 1 0 0 L6 1 0 0 0 Total 44 21 10 8 In the simulated dataset, we set the number of web pages to 100.  The connection probability for any two web pages is set to 30%.  The purchasing probability for a customer in a web page is set to 30%.  The maximum rule length and navigation length is set to 6 and 20, respectively.  According to these settings, we generate 200,000 users' traversing and purchasing records.  Table 8 shows the execution time of IPA algorithm in different minimum supports for traversal sequence.  It ranges from 48.881 to 39.016 seconds Table 9 shows the rule number generated by the IPA algorithm.  It ranges from 234 to 3 rules Table 8. Execution time \(seconds Minimum Support for Traversal Sequence Frequent Itemset 5% 10% 15% 20 L1 0.401 0.401 0.391 0.391 L2 38.735 38.665 38.715 38.625 L3 0.713 0.130 N/A N/A L4 2.404 N/A N/A N/A L5 2.734 N/A N/A N/A L6 2.894 N/A N/A N/A Total 48.881 39.196 39.106 39.016 Table 9. Rule number in simulated dataset Minimum Support for Traversal Sequence Frequent Itemset 5% 10% 15% 20 L1 97 45 10 3 L2 70 4 0 0 L3 34 1 0 0 L4 22 0 0 0 L5 9 0 0 0 L6 2 0 0 0 Total 234 50 10 3 If we set the minimum support for traversal sequence to 5% and vary the number of transaction records, the experimental results are shown in Figures 4 and 5.  If we set the minimum support for traversal sequence to 5%, the number of web pages to 50, and vary the transaction length, the experimental results are shown in Figures 6 and 7 Proceedings of the IEEE International Conference on E-Commerce Technology for Dynamic E-Business  CEC-East  04 0-7695-2206-8/04 $ 20.00 IEEE Figure 4. Execution time for different transaction records Figure 5. Rule number for different transaction records Figure 6. Execution time for different transaction length Figure 7. Rule number for different transaction length Proceedings of the IEEE International Conference on E-Commerce Technology for Dynamic E-Business  CEC-East  04 0-7695-2206-8/04 $ 20.00 IEEE From the above experiments, we can find that the IPA algorithm outperforms MTS when they generate equal size of rules The main purpose of IPA algorithm is to find useful patterns and to help user  s purchasing and navigation 


patterns and to help user  s purchasing and navigation Thus, it is important to use these patterns in real world situation.  In real dataset experiments, one of the special rules is &lt;children.htm, bogustheclown.htm, children.htm thebuppetsgotolondon.htm: thebuppetsgotolondon&gt;.  According to the web structure in Figure 5, the children.htm is the genre page.  It contains links to all associated product pages.  The bogustheclown.htm and thebuppetsgotolondon.htm are all product pages They introduce the related information about the product.  Because many customers bought the product thebuppetsgotolondon bogustheclown.htm.  Thus, we can establish a dynamic hyperlink from bogustheclown.htm to thebuppetsgotolondon.htm.  This dynamic hyperlink is oriented towards the possible purchasing product according to this user behavior.  If we adopt the MTS algorithm to the same work, this rule cannot be discovered because of their assumptions for backward movements.  Obviously, our algorithm can efficiently capture the whole behaviors of users 4. Conclusions In this paper, we propose an IPA \(Integrating Path traversal patterns and Association rules mining web transactions in the EC \(Electronic Commerce show that our algorithm can correctly find the customer  s traversing and purchasing behaviors and outperforms MTS algorithm when they generate equal size of rules.  In the future, we shall efficiently improve the process of trimming database and pruning candidates to speed up the algorithm Acknowledgement Research on this paper was partially supported by National Science Council grant NSC 92-2213-E-130 007?NSC 92-2622-E-130-002-CC3? NSC 92-2213-E 030-019 References 1] R. Agrawal, et al  Fast Algorithm for Mining Association Rules  Proceedings of the International Conference on Very Large Data Bases, pp. 487-499 1994 2] J. S. Park, M. S. Chen and P. S. Yu  Using a Hash-Based Method with Transaction Trimming for Mining Association Rules  IEEE Transaction on Knowledge and Data Engineering, Vol. 9, No. 5, pp 813-825, 1997 3] J. Han, J. Pei, Y. Yin and R. Mao  Mining Frequent Patterns without Candidate Generation: A Frequent Pattern Tree Approach  Data Mining and Knowledge Discovery, Vol. 8, No. 1, pp. 53-87, 2004 4] M. S. Chen, J. S. Park and P. S. Yu  Efficient Data Mining for Path Traversal Patterns in a Web Environment  IEEE Transaction on Knowledge and Data Engineering, Vol. 10, No. 2, pp. 209-221, 1998 5] S. J. Yen  An Efficient Approach for Analyzing User Behaviors in a Web-Based Training Environment   International Journal of Distance Education Technologies, Vol. 1, No. 4, pp.55-71, 2003 6] M. S. Chen, X. M. Huang and I. Y. Lin  Capturing User Access Patterns in the Web for Data Mining  Proceedings of the IEEE International Conference on Tools with Artificial Intelligence, pp. 345-348, 1999 7] J. Pei, J. Han, B. Mortazavi-Asl and H.Zhu  Mining Access Patterns Efficiently from Web Logs   Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 396-407 2000 8] Y. Xiao and M. H. Dunham  Efficient Mining of Traversal Patterns  IEEE Transaction on Data and 


Traversal Patterns  IEEE Transaction on Data and Knowledge Engineering, Vol. 39, No. 2, pp. 191-214 2001 9] C. H. Yun and M. S. Chen  Using Pattern-Join and Purchase-Combination for Mining Web Transaction Patterns in an Electronic Commerce Environment   Proceedings of the COMPSAC, pp. 99-104, 2000 Proceedings of the IEEE International Conference on E-Commerce Technology for Dynamic E-Business  CEC-East  04 0-7695-2206-8/04 $ 20.00 IEEE pre></body></html 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207–216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Int’l Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Int’l Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





