Transmitting Medical Imagery over 2-Meter Amateur Packet Radio Networks using TCP Reno and UDP 2 Paul D Wiedemeier The University of Louisiana at Monroe 700 University Avenue Monroe LA 79209 318-342-1100 wiedemeier@ulm.edu Abstract The United States Federal Communications Commission permits licensed amateur radio operators to transmit data using the 144.1 MHz 148.0 MHz radio frequencies that comprise the amateur 2-meter band Unfortunately significant time is needed to transmit large files over 2-meter amateur packet radio 
networks because the required Terminal Node Controllers i.e radio modems transmit data at a standard rate of 1200 bps In this paper we evaluate the performance of the Reno variant of the Transmission Control Protocol and the User Datagram Protocol when both are used to transmit medical imagery over ns-2 simulated 2-meter amateur packet radio networks Specifically we obtain file transmission times while varying protocol segment sizes and channel bit error rates and find that smaller segment sizes are 
best for transmitting medical imagery when the bit error rate increases We also discuss image quality measures as they relate to medical imagery transmitted using the User Datagram Protocol TABLE OF CONTENTS 1 INTRODUCTION 1 2 NETWORK PERFORMANCE MODELING 1 3 THE Ns-2 NETWORK SIMULATOR 3 4 Ns-2 SIMULATIONS 3 5 NS-2 SIMULATION RESULTS 4 6 IMAGE QUALITY MEASURES 6 7 CONCLUSIONS 6 8 ACKNOWLEDGEMENTS  
6 REFERENCES 7 BIOGRAPHY 7 1 INTRODUCTION On August 29th 2005 hurricane Katrina rendered unusable a significant percentage of the voice and data communication infrastructure in coastal communities of Louisiana and Mississippi In the days following the hurricane amateur radio operators arrived in the devastated areas provided local emergency communication both voice and data and established communication with other regions of the United States using primarily mobile radio transceivers Tx/Rx 1 1 
1-4244-1488-1/08/$25.00 
C 2008 IEEE 2 IEEEAC paper 1067 Version 5 Updated December 11 2007 and portable power supplies 3 13 This situation was repeated after hurricane Rita decimated the communication facilities within the coastal regions of Louisiana and Texas on September 24th  2005 4 Of the many radio frequencies available to amateur radio operators emergency voice communication is often established using those in the 144.1 Megahertz MHz 148.0 MHz range This frequency range 
commonly referred to as the amateur 2-meter band resides within the Very High Frequency electro-magnetic wave spectrum However the time required to transmit some data using the 2-meter band may be significant While small size files e.g 64 Kilobytes KB are transmitted within a reasonable time e.g  10 minutes large size files such as medical imagery e.g 32 Megabytes MB require significant time e.g several hours or days to transmit due 
to the low data transmission characteristic associated with the Terminal Node Controllers i.e radio modems used In this paper we present and discuss results obtained through ns-2 simulation when various file sizes are transmitted over a 2-meter amateur packet radio network PRN We are specifically interested in understanding how changes in transport layer protocol segment size affect overall file transmission time Two transport layer protocols were simulated using ns-2 The first was the Reno variant of 
the Transmission Control Protocol TCP 12 and the second was the User Datagram Protocol UDP 11 We also discuss measures used to evaluate the quality of medical imagery transmitted using UPD 2 NETWORK PERFORMANCE MODELING Evaluating the performance of transport layer protocols can be accomplished through use of one or more of the following network performance modeling techniques live network tests test-beds hardware emulation mathematical models or software simulation Each is discussed below 1 


Live Network Tests Transmitting data over an actual communication network is the most comprehensive method for testing the performance of data communication protocols as real data is generated However the inability to assess the impact a new protocol has on other network traffic sharing the same communication channel is a significant disadvantage Test-Beds Small isolated test-bed networks can be created and used to test data communication protocols They use actual hardware but without the complexity associated with larger communication networks and since they lack complexity the impact associated with using a new protocol is easier to access Unfortunately test-beds are often limited in their complexity and speed by the equipment on hand and if commercial operating systems are used a researcher may be unable to modify proprietary protocol code Hardware Emulation A network emulator is a hardware based protocol evaluation tool where a test-bed is constructed and a computer emulates the function of a specific piece of the communication network Hardware emulators are beneficial because they test real protocol implementations and the non-emulated routers hubs and switch protocols are easy to modify However hardware emulators often simplify some of the modeled network's real behavior and may not be able to represent complex or changing topologies Mathematical Models The file transmission time FTT channel throughput CT and channel utilization CU associated with a data communication channel are mathematically easy to compute File transmission time specifies the time required by a communication protocol to transmit all bits of a file from source to destination The time required to transmit a file i.e file transmission time across a communication channel is shown in Equation 1 and defined as the sum of the communication channel's transmission delay CTD and the quotient of file size FS and the communication channel's transmission rate CTR 10 Overall a smaller transmission time implies a faster file transmission F S FTT CTD CTR 1 Channel throughput defines the amount of data transmitted in a given period of time e.g bits per second or the channel's effective transmission rate Channel throughput as shown in Equation 2 is defined as the quotient of file size and file transmission time 2 Overall the greater the channel throughput the closer the data transmission protocol is to achieving the channel's defined transmission rate while transmitting the file Channel throughput close to 0.0 is considered poor CT  FS FTT 2 Channel utilization is channel throughput normalized to unity using the channel's defined transmission rate Mathematically channel utilization is defined as the quotient of the channel's throughput and the channel's transmission rate 8 See Equation 3 The best channel utilization is 1.0 which means the channel's throughput matches the channel's defined transmission rate Channel utilization close to 0.0 again is poor _ CT CU CT CTR 3 Hosts that transmit data using connection-oriented transport layer protocols such as TCP retransmit packets when an acknowledgement for said packet fails to arrive within a specified time 12 The main disadvantage associated with using mathematical models to define protocol performance is that they do not account for channel congestion and packets dropped during transmission However mathematically computed file transmission times closely mirror actual data transmissions times generated by connectionless transport layer protocols like UDP Software Simulation A simulator is a software based network evaluation tool that permits researchers to test the performance of data communication protocols Several advantages associated with using software simulation to evaluate data transmission performance are listed below 1 Extensive computer or networking equipment is not required when using a software simulator Software simulators are often installed on personal computers or workstations running the Microsoft Windows operating system or a UNIX/Linux operating system 2 Numerous simulations can be executed in a short time period using software simulators and the data generated can be used to evaluate quickly the performance of data communication protocols 3 Software simulators are able to simulate communication networks that cannot be accessed or do not yet exist due to current limitations e.g physical or monetary 4 Logical topologies representing complex communication networks which cannot be created within a test-bed are easily constructed and evaluated using software simulators 2 


5 Data generated during simulation are maintained in files created by a software simulator This feature permits researchers to identify easily which simulation variables produce significant results 6 Software simulation does not affect the actual traffic within a communication network Software simulators exhibit three disadvantages First some software simulators use abstract data communication protocol implementations rather than using actual data communication protocol code found in real operating systems Second software simulators often do not represent non-network events such as operating system scheduler latency Last software simulators often make assumptions concerning real world events such as competing traffic 3 THE Ns-2 NETWORK SIMULATOR Many network simulators exist and are used to evaluate the performance of data communication protocols 1 For the research presented in this paper we use the discrete event network simulator ns-2 version ns-2.31 6 because it supports the simulation of file transmissions using connection-oriented and connection less transport layer protocols such as TCP and UDP over wired and wireless data communication networks 4 Ns-2 SIMULATIONS As discussed in the Introduction we simulated file transmissions using TCP Reno and UDP Specific ns-2 simulator configurations for TCP Reno and UDP simulations are discussed below TCP Reno ns-2 Simulations over 2-Meter PRAV To evaluate the performance of TCP Reno we simulated the transmission of 64 KB 512 KB 4 MB and 32 MB file sizes over a 2-meter packet radio network that exhibited a transmission rate of 1200 bits per second bps 9 and a 25 microsecond hts transmission delay During simulation we chose to use 64 KB 512 KB 4 MB and 32 MB file sizes for several reasons First a 64 KB file size represents the default size of TCP's maximum window size 12 and is the smallest file size we transmit Second the 64 KB 512 KB 4 MB and 32 MB file sizes are divisible by 23 and provide a good range of small and large files for testing purposes We specifically simulate using 4 MB and 32 MB files sizes because any file size larger than 1 MB is representative of medical imagery 15 As our research focuses on medical imagery transmissions these file sizes are sufficient and the largest we transmit 1200 bps 25 s 1200 bps Ps TxRx T Interference Device Figure 1 A Proposed 3-Node 1-Link 2-Meter Packet Radio Network Physical Topology We chose a 25 pts transmission delay for our simulations because in the future we intend to conduct live tests between transceivers located at the author's home and work The physical distance between these two locales is approximately 4.6 miles which corresponds to a 25 pts transmission delay 14 Figure 1 shows the physical topology of our proposed 2-meter amateur packet radio network while Figure 2 shows the logical topology of our network It is the logical network shown in Figure 2 that we will simulate using ns-2 1200 bps 25 ps Sender Receiver Bit Error Module Figure 2 A Proposed 2-Node 1-Link ns-2 Simulated Logical Topology Notice that the physical topology shown in Figure 1 includes three nodes a sender a receiver and an interference device The logical topology shown in Figure 2 only includes two nodes because the interference device at least for those ns-2 simulations involving TCP Reno will be represented by a bit error module that is attached to the link between the sender and receiver in Figure 2 No error module is required for UDP transmissions due to the connection-less nature of this protocol Our ns-2 simulations set the TCP Reno maximum window size to 64 KB the default and varied the TCP Reno segment size at 64 Byte B 128 B 256 B 512 B 1 KB 2 KB and 4 KB We chose to simulate using these segment sizes because our preliminary ns-2 simulations using sizes  64 KB and sizes  4 KB generated file transmission times greater than those shown in Tables 1-4 We also varied bit error rate BER on the simulated 2-meter packet radio network at 0.0 1.0e-04 1.0e-05 1.0e06 t.Oe-07 and t.Oe-08 These bit error rates provide error free data transmission e.g 0.0 and very poor data transmission e.g l.Oe-04 3 Sender 7 Receiver TNC TxIRx 


UDP ns-2 Simulations over 2-Meter PRAV We evaluated the performance of UDP when used to transmit 64 KB 512 KB 4 MB and 32 MB files over the 2meter amateur packet radio network shown in Figure 2 and exhibiting the same transmission rate and transmission delay mentioned in the previous subsection of this paper Since UDP is an unreliable transport layer protocol bit error rates do not influence file transmission times during ns-2 simulations Thus the bit error module representing an interference device would be absent in Figure 2 5 Ns-2 SIMULATION RESULTS In this paper section we discuss and interpret the data generated by our ns-2 simulated file transmissions using TCP Reno and UDP TCP Reno ns-2 Simulated Performance over 2-Meter PRN When the bit error rate was set to 0.0 the ns-2 simulated data show that TCP Reno transmits a 64 KB file size in a minimal time of 508 seconds or 8.46 minutes using a 512 B segment size When the bit error rate was set to 1.Oe-04 the ns-2 simulated data show TCP Reno transmitted a 64 KB file size in a minimal time of 508 seconds or 8.46 minutes again using a 512 B segment size This data is shown in both Figure 3 and Table 1 Figure 3 and Table 1 also show the transmission times in seconds required by TCP Reno when the bit error rates were 0.0 and 1.Oe-04 for all segment sizes Bandwidth 1200 bps Delay 25 us File Size 64 KB 1100 1000 l 8 900 d 0 600 500 400 X E0 TCP Reno BER 0.0  1.0e-04 Minimal Transmission Times of 508 Seconds I 64 B 128 B 256 B 512 B 1 KB 2 KB Segment Size TCPReno BER 10e04 TCPReno BER UD Table 1 64 KB File Size Transmission Times for various Segment Sizes Segment   Size 64 B 128 B 256 B 1 1 1_ 436 512 B 50 8 08 436 1 KB 436 2 KB 436 4 KB 436 When the bit error rate was set to 0.0 our ns-2 simulation data show that TCP Reno transmits a 32 MB file size in a minimal time of 228,233 seconds or 2.64 days using a 2 KB segment size However a 32 MB file size was transmitted in a minimal time of 279,029 seconds or 3.22 days using a 256 B segment size and a 1.Oe-04 bit error rate See both Figure 4 and Table 2 From our ns-2 simulated data we see that as the bit error rate increases from 0.0 to 1.Oe-0 it is best to use smaller TCP Reno segment sizes when transmitting 512 KB 4 MB and 32 MB file sizes The ns-2 simulation data associated with these file sizes are shown in Table 3 Table 4 and Table 2 respectively Table 1 data show that a 64 KB file size is best transmitted using a 512 B TCP Reno segment size for 0.0 and 1.Oe-04 bit error rates Bandwidth 1200 bps Delay 25 Reno BER 1.0e-04 Minimal Transmission Time 450000 of 279,029 Seconds r I 8 400000 E uJ IUUU i 350000 300000X 250000 200000 64 B 128 B 256 B 4 KB Figure 3 64 KB File Size Transmission Times for various Segment Sizes TCP Reno BER 0.0 I Minimal Transmission Time Reno BER 1.0e-04 TCP Reno BER 0.0  UDP 4 KB Figure 4 32 MB File Size Transmission Times for various Segment Sizes 4 I of 228,233 Seconds I I I I I us File Size 32 MB 500000 TCP 51 B 2K Segment Size TCP 


Table 2 32 MB File Size Transmission Times for various Segment Sizes Segment F!i 64 B _ 128 B 256 B 2 79029 223728 512 B _ I KB 2 KB 228,233 4 KB _223X698 Table 3 512 KB File Size Transmission Times for various Segment Sizes Segment _ Size 64B 128 B 256 B 3,495 512 B 4,240 3,495 1 KB 3,710 3,495 2 KB 3,495 4 KB 3,495 Table 4 4 MB File Size Transmission Times for various Segment Sizes Segment E _ Size 64 B 128 B 256 B 4925 512 B 4 KB 2 KB _ 8,676 27S962 4 KB 27;962 UDP ns-2 Simulated Performance over 2-Meter PRN The ns-2 generated data show UDP transmits a 64 KB file size in a minimal time of 436 seconds or 7.28 minutes for segment sizes 256 B through 4 KB See Figure 3 and Table 1 This time represents a 13.9 decrease in transmission time versus the 508 seconds or 8.46 minutes generated by TCP Reno to transmit a 64 KB file size using a 512 B segment size and when the bit error rate was 1.Oe-04 See Figure 5 Bandwidth 1200 bps Delay 25 us File Size 64 KB 100 00 0 90.00 E 80.00 F 0.00 E2s E 50.00 40.00 M   60.00 s u 20.00  D 10.00 0 00 UDP's 13.9 Decrease in Transmission Time vs TCP Reno BER 1.0e-04 I I I I I II It la _ u,20 00 iL D 10.00 0 00 UDP's 19.8 Decrease in Transmission Time vs TCP Reno BER 1.0e-04 N Ih 9 Ol 64 B 128 B 256 B 512 B 1 KB 2 KB Segment Size UDP vs TCP Reno BER 1 Oe-04 UDP vs TCP Reno BER 0 0 4 KB Figure 6 UDP's Performance Decrease in Transmission Time versus TCP Reno for a 32 MB File Size and for various Segment Sizes The TCP Reno and UPD ns-2 simulated data presented in Tables 1-4 and Figures 3-6 show that UPD transmits the 64 KB 512 KB 4 MB and 32 MB file sizes in less time 5 O _ 64 B 128 B 256 B 512 B 1 KB 2 KB 4 KB Segment Size UDP vs TCP Reno BER 1 Oe-04 UDP vs TCP Reno BER 0.0 0 Figure 5 UDP's Performance Decrease in Transmission Time versus TCP Reno for a 64 KB File Size and for various Segment Sizes Ns-2 simulation shows a 32 MB file size was transmitted in a minimal time of 223,698 seconds or 2.58 days by UDP using a 4 KB segment size See Table 2 However to fairly compare the transmission times of TCP Reno BER 1.Oe-04 and UPD we must consider UPD's transmission time of a 32 MB file size using a 256 B segment size We see that a 32 MB file size was transmitted by UDP through ns-2 simulation in a minimal time of 223,728 seconds or 2.58 days using a 256 B segment size Again see Table 2 but refer also to Figure 4 While not minimal this time represents a 19.8 decrease in transmission time versus the 279,029 seconds or 3.22 days required by TCP Reno to transmit a 32 MB file size using a 256 B segment size and when the bit error rate was 1.Oe-04 See Figure 6 Bandwidth 1200 bps Delay 25 us File Size 32 MB 100 00 E 90\26000 80 00 2 E 70 00 0s 00 2 60.00 E 0"n O0 50.00 0 In o 2 40 00%0 30 00 


compared to TCP Reno for both 0.0 and 1.0e-04 bit error rates As such we argue that connection-less transport layer protocols like UDP should be used to transmit medical imagery over 2-meter amateur packet radio networks However the correctness or quality of the medical imagery seen by the receiver is now at issue and will be discussed in the next paper section 6 IMAGE QUALITY MEASURES Concerning the ns-2 simulations discussed in the prior paper section no actual medical imagery were transmitted Thus image quality was not measured While gray scale color or multi-spectral medical imagery are not corrupted during transmission using connection-oriented transport layer protocols such as TCP Reno image quality is a concern when using connection-less transport layer protocols like UDP because data are susceptible to corruption When data corruption occurs the transmitted image is 1 statistically not equivalent to the original image 2 perceived by a human to be distorted compared to the original image or 3 both 1 and 2 occur Several image quality measures IQM are discussed in literature 5 7 Two general IQM classifications are subjective and objective Subjective IQM SIQM are numerical values based on a user's perception of the transmitted image's correctness compared to the original The primary advantage associated with using SIQM is that since a human is the end user utilizing human perception is acceptable as a quality metric The two main disadvantages associated with using SIQM include 1 the time required to query a user's perception of image quality and 2 the fact that the assessment concerns only the quality of the transmitted image Objective IQM OIQM are numerical values representing the difference that exist between the pixels in the original image and those in the transmitted image The main advantage to using OIQM is that good results are generated when image distortion is due to additive noise The primary disadvantage to using OIQM is that it does not account for the user's observed perception of the transmitted image 7 CONCLUSIONS In this paper we discuss the ns-2 simulated transmission of 64 KB 512 KB 4 MB and 32 MB files sizes using TCP Reno and UDP when segment sizes and bit error rates are varied Our data show that smaller segment sizes minimize file transmission times when the bit error rate increases Based on our results we advocate using connection-less transport layer protocols when transmitting medical imagery over 2-meter amateur packet radio networks We also discuss measures to evaluate the quality of medical imagery transmitted using connection-less transport layer protocols such as UDP Since the ns-2 simulator only simulates the transmission of files across a logical topology image quality measures were not applied to the data discussed in this paper However image quality measures will be used during future planned live tests of medical imagery transmission over actual 2-meter packet radio networks While the research presented in this paper discusses the transmission of medical imagery over Earth-based 2-meter amateur packet radio networks our results can be extended to medical imagery transmitted over packet radio networks on other terrestrial surfaces within our Solar System Specifically Mars and the Earth's moon where communication satellites are currently absent and High Frequency radio data transmissions which are use for voice and data communication on Earth suffer due to a minimal atmosphere or the lack of an atmosphere respectively 16 8 ACKNOWLEDGEMENTS The author graciously thanks the following individuals for their important contributions to this paper First thanks to Heather Nikki Simmons for her input concerning image quality measures Thanks also to Allison M.D Wiedemeier Ph.D for reading several drafts of this paper The author's research is supported through funds provided by the Clarke M Williams Jr Endowed Professorship in Computer Science and the Howard Hughes Undergraduate Biological Sciences Education Program both at The University of Louisiana at Monroe As stated above the ns-2 simulator only simulates the transmission of medical imagery using UDP Thus we are unable to apply IQM to the data presented in this paper However we intend to transmit medical imagery over the physical topology shown in Figure 1 in the near future At that time we will evaluate the quality of transmitted medical imagery using specific IQM which have yet to be identified 6 


REFERENCES 1 Allman Mark  Falk Aaron On the Effective Evaluation of TCP ACM Computer Communication Review Vol 5 No 29 October 1999 2 Alhman M  Glover D  Sanchez L Enhancing TCP Over Satellite Channels using Standard Mechanism Request For Comments 2488 January 1999 3 Amateur Radio Relay League Amateur Radio Volunteers Involved in Katrina Recovery August 31 2005 http://www.arrl.org/news/stories/2005/08/30/1/?nc 1 Accessed October 16 2007 4 Amateur Radio Relay League WIAW General Bulletin ARLB022 Amateur Radio continues Hurricane Rita response September 26 2005 http://www.arrl.org/wlaw/2005-arlbO22.html Accessed October 16 2007 5 Avcybas lysmail  Sankur Bu"lent  Sayood Khalid Statistical Evaluation of Image Quality Measures Journal of Electronic Imaging Vol 11 No 2 Pg 206223 April 2002 6 Breslau  Estrin  Fall  Floyd  Heidemann  Helmy  Huang  McCanne  Varadhan  Xu  Yu Advances in Network Simulation IEEE Computer Vol 33 No 5 Pg 59-67 May 2000 7 Eskicioglu Ahmet M  Fisher Paul S Image Quality Measures and Their Performance IEEE Transactions on Communications Vol 43 No 12 Pg 2959-2965 December 1995 8 Henderson Thomas R  Katz Randy H TCP Performance over Satellite Channels University of California at Berkeley Computer Science Technical Report December 1999 9 Jones Greg Editor Packet Radio What Why How Articles and Information on General Packet Radio Topics Tucson Amateur Packet Radio Corporation Inc ISBN 0-9644707-0-5 1996 12 Postel John Transmission Control Protocol Request For Comments 793 September 1981 13 U.S House of Representatives A Failure of Initiative Final Report of the Select Bipartisan Committee to Investigate the Preparation for and Response to Hurricane Katrina ISBN 0-16-075425-9 Pg 177 February 2006 14 Wiedemeier Paul D Performance Modeling of TCP and UDP over Packet Radio Networks using the ns-2 Network Simulator Proceedings of the 26th ARRL and TAPR Digital Communications Conference Hartford Connecticut ISBN 0-87259-105-0 Pg 141-154 September 28-30 2007 15 Wong S T C  Huang H K Network Multimedia for Medical Imaging IEEE Multimedia Vol 4 No 2 Pg 24-35 April-June 1997 16 Yowell Robert J Investigation of Radio Wave Propagation in the Martian Ionosphere Utilizing HF Sounding Techniques Masters Thesis Air Force Institute of Technology June 1996 BIOGRAPHY Paul D Wiedemeier is an Assistant Professor of Computer Science the Clarke M Williams Jr Endowed Professor in Computer Science and the Principle Investigator of the Digital Communication Research Laboratory all at The University of Louisiana at Monroe Dr Wiedemeier obtained a Ph.D in Computer Engineering and Computer Science from the University of Missouri Columbia a MS in Computer Science from Michigan Technological University and a B.S in Computer Science from Drake University He is a member of the Institute of Electrical and Electronics Engineers the Association for Computing Machinery the Amateur Radio Relay League the Tucson Amateur Packet Radio Corporation the Consortium for Computing Sciences in Colleges and the Louisiana Academy of Sciences Dr Wiedemeier also holds a Technicians Amateur Radio License KE5LKY issued by the United States Federal Communications Commission LO Kruse Hans Performance of Common Data Communication Protocols Over Long Delay Links An Experimental Examination 3rd International Conference on Telecommunication Systems Modeling and Design 1995 11 Postel John User Datagram Protocol Request For Comments 768 August 1980 7 


6.2 Results  Figure 2 shows the overall average results of the BIA maturity assessments of the Belgium and Dutch participants. From this graph, some differences are immediately clear, specifically for skills  governance and scope and architecture Figure 3 shows the results on a deeper level. Based upon these differences, the following analysis can be made  Communications maturity  Average maturity of Belgium participants: 3.0 Average maturity of Dutch participants: 2.9 The scores of the Belgium participants and of the Dutch participants do not show a lot of difference, also on the level of the individual assessment items shown in Figure 3. The expected difference is therefore not confirmed  Value measurement maturity  Average maturity of Belgium participants: 2.7 Average maturity of Dutch participants: 2.9 The scores of the Belgium participants and of the Dutch participants on this variable show some difference. On the more detailed level, a substantial difference is shown on the item Formal assessments reviews The direction of the difference, however, is opposite to the expectation. The expected difference is therefore not confirmed  Governance maturity  Average maturity of Belgium participants: 3.8 Average maturity of Dutch participants: 3.4 On this variable of BIA maturity, the results show the expected difference that the Belgium participants score higher than the Dutch participants. On the more detailed level it becomes clear that especially the prioritization process is scored significantly different The expected difference is therefore confirmed  Partnership maturity  Average maturity of Belgium participants: 3.2 Average maturity of Dutch participants: 3.2 The scores of the Belgium participants and of the Dutch participants on this variable are equal. The expected difference is therefore not confirmed  Scope & Architecture maturity  Average maturity of Belgium participants: 3.3 Average maturity of Dutch participants: 2.8 For this variable the expectation was undecided. The results show a remarkably higher score of the Belgium participants than of the Dutch participants. On the more detailed level this difference shows on all items  Skills maturity Average maturity of Belgium participants: 2.8 Average maturity of Dutch participants: 3.2 For this variable, the results show the expected difference that the Dutch participants score a higher maturity than the Belgium participants. On the more detailed level it becomes clear that this difference shows on all items, but most strongly on Ability to attract and retain and Change readiness The expected difference is confirmed  Table 5 shows the summary of the results of this exploratory study    0,00 0,50 1,00 1,50 2,00 2,50 3,00 3,50 4,00 4,50 5,00 Und e rstand i ng of b u sines s by IT Und ers tand ing of I T b y bu si n e ss Inte r/I ntraorg aniz ati onal lea rn ing K now led ge S ha rin g I T Metr i cs Business Metric s Bal a nc ed M et ric s Formal Assessments/Revie ws Conti n uous Im proveme n Budgetar y Cont r o l IT inves tm ent m a nage m en P riorit iz ation pr oces s Busin e ss per c eption o f IT val ue S hared  goals r isk, re w ards p e nalt i e s Re lat ions hip trus t st yl e Sta nd a rds Ar t iculatio n Ar chit ectu ral inte grat io n D isr upti on from Ch ang es Inn o vation  Entre p reneu rs hip Loc us of Po w er Cha n ge rea d ines s Ab i lity to attract a n d reta i n Netherlands Belgium Communications maturity Value measurement maturity Governance maturity Partnership maturity Scope & Architecture maturity Skills maturity  Figure 3. Detailed BIA maturity scores of the Belgium and Dutch participants Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


Table 5 Summary of expectations and results  Expectation Result Comm M NL Comm M BE Comm M NL Comm M BE Value M NL Value M BE Value M NL Value M BE Gov M NL Gov M BE Gov M NL Gov M BE Par M NL Par M BE Par M NL Par M BE Arch M NL Arch M BE Arch M NL Arch M BE Sk M NL Sk M BE Sk M NL Sk M BE Confirmed Not confirmed Not appliccable no expectation Partnership maturity Scope & Architecture maturity Skills maturity Confirmed Not confirmed Not confirmed Communications maturity Value measurement maturity Governance maturity    7. Conclusions and limitations  The conceptual analysis of the potential influence of national cultures on BIA maturity provides indications that this influence is indeed more than likely and that its influence is consistent over the different dimensions of culture. Our first empirical exploration provided support for the existence of differences in BIA maturity between countries. More specifically, the differences in scores regarding the alignment domains governance maturity and skills maturity could be explained by Hofstede s cultural differences studies, with governance maturity scoring higher in Belgium and skills maturity scoring better in the Netherlands. One a more detailed level, it was expected and confirmed that the portfolio management process received better in Belgium and that Ability to attract and retain and Change readiness clearly received better scores in the Netherlands Our study did not, however, confirm all expected results. The domains communications maturity  value measurement maturity and partnership maturity did not show the difference in scores that were expected. In fact, the scores showed no clear substantial at all. A potential explanation for this result could be the small sample size. Therefore, further study should be made, based on larger samples, to further explore and study the impact of cultural differences on business and IT alignment scores. Such studies can expand the comparison between Belgian and Dutch results, but could also address cultural differences between other nations worldwide  References  1  A VOLIO   B  K AHAI   S    D ODGE   G. \(2000 E-leadership Implications for theory, research, and practice Leadership Quarterly, 11 \(4\, 615-668 2] B ATENBURG R. \(2007 E-procurement adoption by European Firms: A quantative analysis Journal of Purchasing & Supply Management, 13, 182-192 3  B IRGELEN   M VAN   R UYTER   K.D  J ONG   A.D    W TZELS M 2002 Customer evaluations of after-sale service contact modes An empirical analysis of national culture's consequences  International Journal of Research in Marketing, 19, 43-64 4  B OND M.H. \(1984 Hofstede's Culture Dimensions ; An Independent Validation Using Rokeach's Value Survey Journal of Cross-Cultural Psychology, Vol. 15, No. 4, 417-433 5  C HAN   Y.E  H UFF   S.L  B ARCLAY   D.W AND C OPELAND D.G 1997 Business Strategy Orientation, Information Systems Orientation and Strategic Alignment Information Systems Research, Vol. 8, No. 2, pp. 125-150 6  C HAN   Y.E. \(2002 Why Haven t we Mastered Alignment? The Importance of the Informal Organization Structure MIS Quarterly Executive, Vol. 1, No. 2 7  C HAN   Y.E    R EICH   B.H. \(2007 IT alignment: what have we learned Journal of Information Technology advance online publication, 18 September 2007; doi: 10.1057/palgrave.jit.2000109 8  C HIASSON   M.W AND D AVIDSON   E., \(2005\, Taking industry seriously in Information Systems research MIS Quarterly, Vol. 29 Issue 4, p591-605 9  C IBORRA C.U. \(1997 De Profundis? Deconstructing the Concept of Strategic Alignment Scandinavian Journal of Information Systems, 9\(1\, pp. 67-82 10  C UMPS   B  V IAENE   S  D EDENE   G AND V ANDENBULCKE   J 2006 An Empirical Study on Business/ICT Alignment in European Organisations 39th Hawaii International Conference on Systems Science \(HICSS-39\, Waikoloa, Big Island, HI, USA 11  C UMPS   B  M ARTENS   D  D E B ACKER   M  H AESEN   R  V IAENE   S  D EDENE   G  B AESENSD   B AND S NOECKA   M. \(2006 Predicting Business/ICT Alignment with AntMiner KBI0708 Research paper Department of Decision Sciences and Information Management \(KBI\, Catholic University of Leuven 12  D E H AES   S AND V AN G REMBERGEN   W. \(2008 Analysing the Relationship Between IT Governance and Business/IT Alignment  41 st Hawaii International Conference on Systems Science \(HICSS41\, Waikoloa, Big Island, HI, USA 13  E KSTEDT   M  J ONSSON   N  P LAZAOLA   L  M OLINA   E.S    V ARGAS   N  2005 An Organization-Wide Approach for Assessing Strategic Business and IT Alignment PICMET conference 14  G EFEN   D    S TRAUB   D. \(1997 Gender differences in the perception & use of e-mail: Extension to the technology acceptance model MIS Quarterly, 21 \(4\, 389-400 15  H ALL   E  T. \(1976\. Beyond culture. NY: Anchor Press 16  H ENDERSON   J.C    V ENKATRAMAN   N. \(1993 Strategic alignment: Leveraging information technology for transforming organizations IBM Systems Journal, Vol. 32, no. 1 17  H IRSCHHEIM R AND S ABHERWAL R  2001 Detours in the Path toward Strategic Information Systems Alignment: Paradoxical Decisions, Excessive Transformations, and Uncertain Turnarounds  California Management Review, 44\(1\, pp. 87-108 18  H OFSTEDE G. \(1980 Culture's consequences: International differences in work related values Newbury Park, CA: Sage 19  H OFSTEDE G. \(1991 Culture and organizations: Software of the mind London, UK: McGraw Hill Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


20  H OFSTEDE   G. \(2000 The information age across cultures  Proceedings of 5th AIM conference Information Systems and Organizational Change. CD-Rom, 10pp 21 h t t p    www g e e r t h o f s t e d e  c o m  h o f s t e d e _ d i m e n s i o n s  php on February 28th, 2008 22  K AARST B ROWN   M.L AND R OBEY   D. \(1999 More on myth magic and metaphor: Cultural insights into the management of information technology in organizations Information Technology People, Volume: 12, Issue: 2 , Page: 192 218 23  K EARNS   G.S AND L EDERER   A.L. \(2004 The impact of industry contextual factors on IT focus and the use of IT for competitive advantage Information & Management, Vol. 41, pp 899-919 24  L EIDNER D  E  C ARLSSON S  E LAM J    C ORRALES   M. \(1999 Mexican and Swedish managers' perceptions of the impact of EIS on organizational intelligence, decision making, and structure  Decision Sciences, 30 \(3\, 633-658 25  L IVONEN   M  S ONNENWALD   D  H  P ARMA   M    P OOLE K OBER E. \(1998 Analyzing and understanding cultural differences: Experiences from education in library and information studies Proceedings of the 64th IFLA General Conference Amsterdam, Netherlands 26  L UFTMAN   J.N  L EWIS   P.R AND O LDACH   S.H. \(1993 Transforming the Enterprise: The Alignment of Business and Information Technology Strategies IBM Systems Journal, Vol. 32 1 27  L UFTMAN   J.N AND B RIER   T. \(1999 Achieving and Sustaining Business-IT Alignment California Management Review, Vol. 42, 1 28  L UFTMAN   J.N  P APP   R AND B RIER T. \(1999 Enablers and Inhibitors of Business-IT Alignment Communications of the Association for Information Systems, Vol 1, Article 11 29  L UFTMAN   J.N  2000 Assessing Business-IT Alignment Maturity Communications of the Association for Information Systems, Vol 4, Article 14 30  L UFTMAN   J.N  2007 An Update on Business-IT Alignment A Line Has Been Drawn MIS Quarterly, Vol. 6 No. 3, pp. 165 31  M AES   R  R IJSENBRIJ   D  T RUIJENS   O AND G OEDVOLK   H  2000 Redefining Business-IT Alignment through a unified framework http://im www.fee.uva.nl/~maestro/PDF 2000-19.pdf 32  M ILLER   S  B ATENBURG   R.S    W IJNGAERT   L VAN DE 2006 National Culture Influences on European ERP Adoption In J Ljungberg & M. Andersson \(Eds.\, 14th European Conference on Information Systems \(pp. 1-12\. Göteborg 33  M OOIJ   M DE 2000 The future is predictable for international marketers: Converging incomes lead to diverging consumer behavior International Marketing Review, 17 \(2\, 103113 34  P NG   I.P.L  T AN   B.C.Y    W EE   K.L. \(2001 Dimensions of national culture and corporate adoption of IT Infrastructure IEEE Transactions on Engineering Management, 48 \(1\, 36-45 35  P YBURN   P.J. \(1983 Linking the MIS Plan with Corporate Strategy: An Exploratory Study MIS Quarterly, Vol. 7, No. 2 36  R EICH   B.H AND B ENBASAT I. \(1996 Measuring the Linkage between Business and Information Technology Objectives MIS Quarterly, Vol. 20, No. 1, pp. 55-81 37  R OSS   D  N. \(2001 Electronic communications: Do cultural dimensions matter American Business Review, pp. 75-81 38  S ABHERWAL   R AND C HAN Y. E. \(2001 Alignment Between Business and IS Strategies: A Study of Prospectors, Analyzers, and Defenders Information Systems Research, 12\(1\, pp. 11-33 39  S CHWARTZ S.H. \(1994 Beyond individualism-collectivism New cultural dimension of values In U. Kim, H.C. Tirandis, C Kagitcibasi, S-C. Choi, and G. Yoon \(Eds.\, Individualism and collectivism: Theory, method, and applications \(pp85-199\. New York: Reidel 40  S ILVIUS   A.J.G. \(2007 Business & IT Alignment in Theory and Practice 40th Hawaii International International Conference on Systems Science \(HICSS-40\, Waikoloa, Big Island, HI, USA 41  S MACZNY T. \(2001 IS an Alignment between Business and IT the Appropriate Paradigm to Manage IT in Today s Organisation Management Decision, 39 \(10 42  S MITH   P.B AND B OND M.H. \(1998\ Social Psychology across Cultures. Paris: Prentice Hall Europe 43  S OCIETY OF I NFORMATION M ANAGEMENT 2003, 2004, 2005 2006, 2007 Execs provide insight into top management concerns technology developments in new SIM survey  http://www.simnet.org/Content/NavigationMenu/About/Press_Rele ases/PressReleases.htm 44  S RNES   J-O  S TEPHENS   K  S TRE   A.S    B ROWNING   L.D 2004 The Reflexivity between ICTs and Business Culture Applying Hofstede s Theory to Compare Norway and the United States Informing Science Journal, Volume 7, 2004 45  S TRAUB   D  W  1994 The effect of culture on IT diffusion: email and fax in Japan and the US Information Systems Research, 5 1\, 23-47 46  S TRAUB   D  W  K EIL   P    B RENNER A. \(1997 Testing the technology acceptance model across cultures: A three country study Information & Management, 31 \(1\, 1-11 47  T ROCHIM W.,  \(2001\, The Research Methods Knowledge Base, 2e. Atomic Dog Publishing 49  V EIGA   J.F  F LOYD   S    D ECHANT   K. \(2001 Towards modelling the effects of national culture on IT implementation and acceptance Journal of Information Technology, 16 \(3\3, 145-158 50  V ENKATRAMAN   N  1989 The concept of fit in strategy research Academy of Management Review, 14\(3 51  W AARTS   E    E VERDINGEN   Y VAN 2005 The Influence of National Culture on the Adoption Status of Innovations: An Empirical Study of Firms Across Europe European Management Journal, Volume 23, Issue 6, December 2005, Pages 601-610 52  W ALLS   J. \(1993\. Global networking for local development Task focused and relationship focused in crosscultural communication. In L. Harasim, Global networks: Computers and international communication. The MIT Press: Cambridge, Mass 53  W ATSON   R.T  K ELLY   G.G  G ALLIERS   R.D    B RANCHEAU   J.C  1997 Key issues in information systems management: an international perspective Journal of Management Information Systems, Volume 13,  Issue 4, Pages: 91 115 54  W ESTRUP   C  L IU   E  E L S AYED   H    A L J AGHOUB   S  2003  Taking culture seriously: ICTs, cultures and Development In: S Krishna & S. Madon \(Eds.\, ICTs and Development: New Opportunities, Perspectives and Challenges, Ashgate 55  W EILL   P AND B ROADBENT   M. \(1998 Leveraging The New Infrastructure; How Market Leaders Capitalize on Information Technology Harvard Business School Press 56  Y IN 2002, Case Study Research: Design and Methods Third Edition \(Applied Social Research Methods\, Sage Publications Inc Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


Automated Windowed Coefficient Trending Earlier we attempted to find explanatory models whose coefficients changed over time in a predictable way This would then let us predict future coefficient values for these models and apply these trended models towards future estimates The problem was that we were not able to find such stable models at least manually We automated our search for stable models by building a system we refer to as the Automated Coefficient Trend Search ACTS tool A patent is currently pending for the method implemented in this software Against the Far Out data set ACTS is designed to automatically permute fields run chronologically-windowed regressions with the chosen fields establish linear and nonlinear trends for the resulting coefficients and then use these trended coefficients to estimate a future set of data ACTS also automatically tries up to seven different transformations on any given dependant or independent variable The length of calibration windows and the increment between them are automatically varied ACTS establishes most promising coefficient trends and also rates the performance of predictions done using trended-coefficient models With so many parameters transformations windows length and intervals to evaluate ACTS needed to search a huge problem space Despite assembling a cluster of 8 computers to run ACTS if necessary for months the problem still required pruning Each run of ACTS was therefore restricted to producing models with a different number of variables between 3 and 15 We quickly found that the best models contained very few parameters due to over-fitting on wider models permitting us to pare down the problem Neural Netv 1.00 0.75 0.50 0.25 I  M The system produced literally thousands of forecasts for each project If the distribution of project costs and project costs estimates were known a confidence interval could be computed using the cost estimates and a method such as Bayesian model averaging used to integrate them 5 Not having a priori knowledge of this distribution we chose instead to use the inter-quartile range of the estimates that is we looked at whether the estimate fell between the first and third quartile of the estimates The top and bottom 25 were disregarded because there are always outliers and it was presumed the system could work well even when many of the individual estimates were not very good Given the very large number of estimates produced by the system literally thousands or tens of thousands depending on the settings we could afford to squander a few Results were best with only three variables per equation In eight out of 31 projects 26 of the projects the project's actual costs fell within the inter-quartile range of the forecasts of costs for that project produced by the ACTS models For 30 out of 31 projects the inter-quartile range of estimates was made up of over a thousand estimates while for the 3 1st project only 446 estimates were part of the inter-quartile range The significance of these numbers is revealed in the next paragraph When equations with four variables were used only one of the true project costs fell within the inter-quartile range of forecasts produced by ACTS When more than 4 variables were used none of the true project costs fell within the interquartile range From this we concluded that ACTS performed best when only 3 variables were included in each model Using more variables appears to produce over-fitted models Aork MRE By Lauch Month Chronologic 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Launch Month Since 1960 Figure 10 Neural network magnitude of relative error sorted chronologically 11 space 


incapable of accurate forecasts Of course the wider the inter-quartile range of forecasts the more likely it is that the true forecast will fit within that range But how parsimonious is the range As figure 11 shows in about 55\2600 of the inter-quartile ranges created by ACTS the 3rd quartile of estimates is 50%0 larger than the Ist quartile of estimates For about 90\2600 of the ranges created the difference between the I't and 3rd quartile is less than 75\2600 and for 97\2600 of the projects the top of the range is 100 or less than the bottom of the range 45\2600 of the true project costs falling into the new range When the top and bottom of the range were changed by 50\2600 the range estimates encompassed 77\2600 of the true project costs In the end however we felt that the adjusted ranges were too wide for practical use Adaboosting using binomial logit classifiers Short for adaptive boosting the Adaboost algorithm 6 is a type of machine learning algorithm where a set of classifiers are adjusted to favor previous misclassifications These classifiers are actually other estimating algorithms Percentage of Time 3rd Quartile no more than XO/o Greater than 1st Quartile 120 100 80 60 40 20 0 25 50 75 100 Figure 11 Variation in ACTS-produced estimates measured between 3rd and 1st quartile Percentage of time True Cost Falls in Range Produced by ACTS 3 variable ersion 90 8070 60 5040 30 20 10 l 0 Interquartile  or10  or25  or50  or75 Figure 12 The effect of widening the estimating range measured by  of estimates now lying within Of the eight projects that fell within the average percentage difference between the 1st and 3rd quartile was 55 with median of 52 This indicated that the inter-quartile range might be too restrictive We therefore proceeded to widen the range of cost estimates for each project multiplying the ISt quartile estimate by 1 X%0 and the 3rd quartile estimate by 1  X using various Xs In each instance we compared actual project costs to estimates produced and determined how frequently the actual fell into the range of estimates Results are summarized in figure 12 When the top and bottom of the range were changed by 10 32 of the actual estimates fell within the new range Extending each of the range boundaries by 25 resulted in Adaboost works by iteratively weighting observations so that poorly classified ones are given more weight during the next iteration With the algorithm focused on improving estimates for exceptional cases it is somewhat more sensitive to noisy data and outliers though less likely to over fit We chose to test this approach due to the latter property The boosting algorithm seemed structured particularly for binary decisions though we are seeking to estimate a continuous variable cost We therefore staged the method by estimating cost bands so that the estimate for each band reverted to a binary decision The algorithm would separately estimate whether an observation was 12 


our evaluation method tested potential models using either ordinary least squares least absolute above or below X dollars then Y dollars then Z such that instead namely that we were examining a proportion rather X  Y  Z than a binary outcome but this was impractical to use and we judged that the problem was not strongly biased towards The boosting algorithm still required a classifier and model either context For a classifier we implemented binary logit78 a discrete Adaboost MRE By COST Lu was similar to ACTS but with 0 1.00 0.75 0.50 0.25 0.00 0.25 0 50 0 75 1 00 1 25 1 50 1 75 2 00 Cost Figure 13 Adaboost magnitude of relative error sorted by actual cost Adaboost MRE CHRONOLOGICALLY Year Figure 14 Adaboost magnitude of relative error sorted chronologically choice model specifically intended for a binary dependant variable is the cost above or below this point There may have been some support for using the probit model 7 This was done by a statistical programmer with output checked against the statistics program STATA 8 we obtained from this first stage system We also attempted to use OLS as a classifier but our experiments showed no significant effect from adding relatively orthogonal i.e unrelated models Since using more than one model did not make a difference boosting using OLS was little more than a single OLS model with estimates divvied up in bands no dynamic re-weighting of classifiers and thus no boosting effect to speak of To obtain models some changes and improvements and again put were discarded and resolved by the latter 13 Lu 1.00 0.75 0.50 0.25 0.00 0 0.50 0.75 1 00 1.25 1 50 1 75 2 00 error which better discounts outliers and weighted least squares The models which we built another windowed test system that our computing cluster to use trying to find good ones from were then used to bootstrap the binary logit method although coefficients a sea of potential transformations This time 


This exercise required a number of decisions about how many classifiers to use how many bands to estimate and how many variables should be used in the models All these choices were made empirically Models with 3 variables were marginally more accurate than those with 4 More models seemed generally better to a point so we used 99 models We also used 22 cost bands checking whether an estimate was above or below 12M 35M 44M and so on through 519M It is interesting to note that there was no data scarcity-induced limit on the number of bands we could have used since each was estimated using all observations lying above and below and not just between bands As with the neural network training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 The trend in estimate deviation would again permit us to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 13 and chronologically in figure 14 Figure 13 shows that MREs are large and negative for missions at 56M actual mission cost and below Above this point results are mixed although not as poor as at the very low range There seems to be no degradation in predictive accuracy as projects increase in scale The correlation between estimates and actual values is approximately 0.66 Sorted chronologically in figure 14 no trend can be seen as estimates go farther out up to 11 years as was seen with the neural network Again this may have been due to the normalization process in which we mapped each variable to its percentage ranking based on future and past minimum and maximum values basically embedding the effect of time within the factors being input to the model It also may be due to a relatively stable era Weighted combinations of simple models A persistent problem in multivariate estimation methods which has been mentioned is that a model may be produced that over fits the observations used for calibration It would thus not be adequately generalized to estimate out of that sample Techniques such as jack-knifing and outlier removal can mitigate this We chose another approach developing models that were little more than simple rules of thumb and then combining these in a weighted manner so each would have a say in the estimate The weightings were formulated using Excel's Solver to minimize estimation error on a set observations lying 15 to 25 years before the test set We refer to this method as continuous boosting because as with the Adaboost method it too involves a mix of classifiers that are combined according to their performance against a set of test observations Unlike Adaboost which solves a binary problem the algorithms are here estimating on a continuous scale Also unlike Adaboost the models are re-weighted rather than the observations to maximize prediction Before presenting results it is worth noting that we ran a control test where all variables from the simple rules of thumb were combined into a single model and run on the same windows performance was nowhere near as good We chose the models by hand guided by the criteria that they should be relatively orthogonal rely on differing parameters so they could each contribute a unique influence to the combined result Figure 15 shows that MREs are mostly positive though for lower cost missions errors are regularly negative and large Continuous Boost MRE By COST M M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Cost Figure 15 Continuous boosting magnitude of relative error sorted by actual cost 14 


CONCLUSIONS Given that this project was intended to estimate missions lying 10-15 years out we structured it differently than one intended to estimate contemporary projects A variety of conventional techniques were not used as we felt they would over fit training observations and thus not be suitable for prediction We also were not sure which approach would work so we tried many We heavily favored ensemble methods where models are combined because we surmised that any one model could not be guaranteed to have the best view of the future However a single neural network ultimately yielded the most competitive results Another finding was that methods built upon simple models such as with three variables generally did work best not surprisingly because they were less likely to over fit calibration data A graph of the three best methods is shown in figure 18 with estimates for the same missions sorted by cost For the neural network calibration occurred until just before the results shown for continuous boosting calibration occurred no more recently than 15 years prior to the results and for Adaboost calibration also occurred up until the results shown A graph sorted by year is not shown we think it instructive that no results seemed to degrade over time As mentioned but obvious from figure 17 the neural network performs best followed by Adaboost and then continuous boosting It is interesting to note that the three cases in which the neural network goes haywire and predicts too low also correspond to worst performances for the Adaboost method pointing to exceptional data points We will be further investigating these regularly errant results and other outliers which may result in estimating improvements ACKNOWLEDGEMENTS This work was carried out under Small Business Innovation Research contract FS9453-05-C-0023 with the Air Force Research Lab The authors wish to gratefully acknowledge Judy Fennelly and later Ross Wainwright our technical points of contact at AFRL for their continuous support and encouragement Our contract officer Timothy Provencio also provided invaluable assistance Our critical seed stock of data was provided by Joseph Hamaker who previously was Director of NASA Headquarters Cost Analysis Division and now is Senior Cost Analyst with SAIC Ainsley Chong and Dale Martin USAF Ret also lent considerable assistance with data gathering APPENDIX A MISSIONS COLLECTED Active Cavity Radiometer Irradiance Monitor Satellite Active Magnetospheric Particle Tracer Explorer Adeos Advanced Communications Technology Satellite Advanced Composition Explorer Alexis Amos-I AMSC-1 Anik El Anik E2 Applications Technology Satellite-I Applications Technology Satellite-2 Applications Technology Satellite-5 All MREs Lu 1.00 0.75 l 0.50l 0.25 l 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75-2.00oi v 1   Cost o N X  X~~~~~Cl Figure 16 Comparison of estimating error for three best methods 15 


Applications Technology Satellite-6 Aqua Argos Atmospheric Explorer AURA Aurora 2 Calipso Cassini Cassini Spacecraft  Huygens Probe Chandra X Ray Observatory CHIPSat Clark Clementine CloudSat COBE Columbia 5 Contour CRRES DART Dawn DBS-1 Deep Impact Flyby Spacecraft  Impactor Deep Space 1 Deep Space 2 Defense Meteorological Satellite Program-5D Defense Meteorological Satellite Program-5D3 Defense Support Program DSCS 3 FIO DSCS 3 F7 DSCS I DSCS-II DSCS-IIIA DSCS-IIIB Dynamics Explorer-I Dynamics Explorer-2 Earth Observing Satellite 1 Earth Radiation Budget Experiment EchoStar 5 Extreme Ultraviolet Explorer Far Ultraviolet Spectroscopic Explorer FAST FLTSATCOM 6 Galaxy 5 Galaxy 11 Galaxy Evolution Explorer Galileo Orbiter  Probe Gamma Ray Large Area Space Telescope GE 1 GE 5 Genesis GFO 1 Globalstar 8 Glomr GOES 3 GOES 9 GOES N GPS-1 GPS-IIR GPSMYP GRACE Gravity Probe-B GRO/Compton Gamma Ray Observatory GStar4 Hayabusa HEAO-1 HEAO-2 HEAO-3 HESSI-II High Energy Transient Explorer-II HETE HST ICESat Ikonos IMAGE IMP-H Inmarsat 3-F5 Intelsat K INTELSAT-II INTELSAT-IV International Ultraviolet Explorer Iridium James Webb Space Telescope Jason 1 JAWSAT KEPLER KOMPSAT LANDSAT1 LANDSAT-4 LANDSAT-7 Lewis Lunar Orbiter Lunar Prospector Magellan Magsat Mariner-4 Mariner-6 Mariner-8 Mariner1 0 MARISAT Mars Exploration Rover Mars Express/Beagle 2 Mars Global Surveyor Mars Observer Mars Odyssey Mars Surveyor 2001 Orbiter Mars Pathfinder  Sojourner Rovers Mars Polar Lander Mars Reconnaissance Orbiter Mars Telecommunication Orbiter Mars Climate Orbiter Messenger Meteor Mid-course Space Experiment MightySat Milstar 3  Adv EHF Model-35 Morelos NATO III Near Earth Asteroid Rendezvous NEAR Shoemaker New Horizons 16 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


