QuakeSim Efficient Modeling of Sensor Web Data in a Web Services Environment Andrea Donnellan Jay Parker Robert Granat Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 818-354-4737 Andrea.Donnellangjpl.nasa.gov Geoffrey Fox and Marlon Pierce Community Grids Laboratory Indiana University 
501 N Morton Suite 224 Bloomington IN 47404 John Rundle Department of Physics University of California Davis One Shields Avenue Davis CA 95616 Dennis McLeod and Rami Al-Ghanmi University of Southern California Mail Code 0781,3651 Trousdale Parkway Los Angeles CA 90089 Lisa Grant University of California Irvine Irvine 
CA 92697 Walter Brooks NASA Ames Research Center Moffett Field CA 94035 Abstract QuakeSim is a project to develop a modeling environment for studying earthquake processes using a web services environment In order to model interseismic processes multiple data types must be ingested including spaceborne GPS and InSAR data 
geological fault data and seismicity data QuakeSim federates data from these multiple sources and integrates the databases with modeling applications Because the models are complex and compute intensive we are using the Columbia computer located at NASA Ames to integrate and run software programs to improve our 
understanding of the solid Earth and earthquake processes The complementary software programs are used to simulate interacting earthquake fault systems model nucleation and slip on faults and calculate run-up and inundation from tsunamis generated by offshore earthquakes QuakeSim also applies pattern recognition techniques to real and simulated data 
to elucidate subtle features in the processes 1 2 1 1 1-4244-1488-1/08/$25.00 
C 2008 
IEEE 2 IEEEAC paper 1224 Version 5 Updated November 29 2007 TABLE OF CONTENTS 1 INTRODUCTION  1 2 DISTRIBUTED COMPUTING INFRASTRUCTURE  3 3 QUAKETABLES DATABASE  4 4 
APPLICATIONS  6 5 CONCLUSIONS  9 6 ACKNOWLEDGEMENTS  9 REFERENCES  9 BIOGRAPHIES  10 1 INTRODUCTION We are expanding the development of our QuakeSim Web Services environment to integrate both real-time and archival sensor data with high-performance computing applications for data 
mining and assimilation The goal of this work is to substantially improve earthquake forecasts which will ultimately lead to mitigation of damage from this natural hazard We are federating sensor data sources with a focus on InSAR and GPS data for an improved modeling environment for forecasting earthquakes 1 


Figure 1 Example of the QuakeSim portal Users can access the QuakeTables database and ingest the data into various modeling and visualization applications Improved earthquake forecasting is dependent on measurement of surface deformation as well as analysis of geological and seismological data Space-borne technologies in the form of continuous GPS networks and InSAR satellites are the key contributors to measuring surface deformation These disparate measurements form a complex sensor web in which data must be integrated into comprehensive multi-scale models In order to account for the complexity of modeled fault systems investigations must be carried out on high-performance computers The QuakeSim applications GeoFEST and Virtual California are the key applications for modeling fault systems GeoFEST and its supporting software provide a suite of tools for creating and refining finite element meshes than can be used to calculate very detailed seismic deformations associated with individual faults and small fault systems At a large scale Virtual California has been developed to perform simulations of large interacting fault systems i.e the entire western U.S over hundreds of years Both applications have been ported to and are being optimized for running in parallel on Columbia Both applications are key interpreting surface deformation data from planned InSAR missions for understanding underlying fault properties and interaction Work here will lay the groundwork for NASA's planned Dynamics Ecosystem Structure and Dynamics of Ice DESDynI mission with a target launch date of 2014 The mission will produce a minimum of 650 GB of data per day If data downlink bandwidth limitations are overcome raw data production will be greater than 1 TB/day The sheer volumes of data will require routine automated data processing on supercomputers Data and products must be transported to and from the supercomputing resources and distributed for further processing and analysis QuakeSim is intended to establish infrastructure for the upcoming DESDynI mission as well as other potential missions We are building upon our Grid of Grids approach which includes the development of extensive Geographical Information System based Data Grid services We are extending our earlier approach of integrating the Data Grid components with improved Execution Grid services to interact with high-end computing resources Our first targets for deploying these services are the Columbia computer at NASA Ames and the Cosmos computer cluster at JPL One of the key issues in this project is the utilization and representation of streaming InSAR and GPS data A federated ontology is being developed to semantically represent GPS data and manage its availability via real-time streaming data services As part of the development process multiple GPS data representations and data access services are currently being examined An Application Programming Interface API is used to interface users with the data represented by the ontology This API is part of the QuakeSim portal Scientists are able to perform queries at different levels of abstraction and run simulations of the data obtained from QuakeSim The communication between QuakeTables and the federated ontology is managed through a middleware API 2 


Figure 2 Examples from the portal version 2.0 showing improved richness of interfaces including the available QuakeSim applications Google Maps output of surface deformation from the Disloc application and an example mesh with embedded fault generated from the portlet 2 DISTRIBUTED COMPUTING INFRASTRUCTURE QuakeSim s current distributed computing infrastructure consists of Web services interacting with a clients in a component-based Web portal Figure 1 The Web services provide access to data particularly fault models and application codes through well-defined programming interfaces expressed in WSDL The QuakeSim portal is a graphical user interface that provides the following capabilities 1 Allows the user to couple databases with simulation codes This is typically done in the input file creation process in which users select desired fault models that will be used in the simulation from our fault database GPS data sources are also supported 2 Assists users with setting up the complicated input files used by the codes 3 Allows the user to track the progress of running jobs 4 Allows the user to do simple plotting to inspect results such as finite element meshes and calculated surface stresses 5 Allows the user to create and manage archives of jobs by storing metadata all parameters used times submitted simple text descriptions generated by the user's interactions with the portal This allows the user to know exactly how a particular results was obtained and to quickly modify and resubmit it if desired 6 Allows the user to download output files created by a particular run We recently released version 2.0 of the QuakeSim portal We use the JSR 168 portlet-complian GridSphere container which is a popular product in the science gateway community GridSphere enables developers to quickly develop and package third-party portlet web applications that can be run and administered within the GridSphere portlet container It is used by the Open Grid Computing Environment Project the Scripps GPS Explorer portal and many TeraGrid Science Gateways All portlets are developed using Java Server Faces JSF QuakeSim 2.0 has improved the richness of interfaces Figure 2 We make use of Google Maps YUI JavaScript Libraries and BFO Plotting libraries for meshes The service architecture we have adopted is useful for running relatively small simulation problems but will need major enhancements to interact securely with the batch schedulers used by Columbia and other high-end supercomputers Rather than developing this from scratch we are integrating our approach with the Globus toolkit and services The classic grid is Globus used by the National Science Foundation TerraGrid and Open Science Grid http://www.globus.org The Globus Toolkit is an open source software toolkit for building Grid systems and applications The Grid allows people to share computing power applications and databases across boundaries without sacrificing local autonomy Globus provides the following relevant to QuakeSim 3 


Figure 3 Example output on a search from the QuakeTables database  A secure remote execution and job management service GRAM that has bindings to several queuing systems PBS LSF LoadLeveler etc  Remote file management and file transfer GridFTP  Information services MDS  A single sign-on security environment GSI that enables limited delegation useful for example in GridFTP third-party file transfers and  A client programming API the Java COG Kit for its services The Java COG has been used by the QuakeSim portal and related projects to provide access to the NSF TeraGrid Adopting Globus will provide several important features missing from the current command-line based system The COG provides a rich client development environment that allows us to build graphical user interfaces as well as command line tools The COG also supports the creation of graph-based workflows for chaining together several operations The GRAM service supports multiple scheduling/queuing systems and provides an API for programmatically creating batch scripts that is independent of the queuing system GridFTP supports third-party transfers in addition to uploads and downloads This allows us to directly transfer files between two backend computers from a portal server Globus provides optional information services MDS that can be used to access machine information that can be displayed to the user or used internally to assist with job submission decisions Globus services can be used to set up cross-realm authentication For example services running at NASA JPL can be configured to accept user credentials signed by the NASA ARC Certificate Authority We can take advantage of numerous external projects such as Kepler and Condor-G for workflow composition and high throughput computing 3 QUAKETABLES DATABASE The QuakeTables database is part of the QuakeSim environment Figure 3 Currently QuakeTables houses paleoseismic and fault data that can be ingested into QuakeSim applications We are expanding the database to include GPS velocities and interferograms processed from Synthetic Aperture Radar data It is a challenge to convert data particulary those collected and reported by a variety of means into standard data for modeling applications In QuakeSim applications we model fault activity such as rate of strain accumulation or offset related to earthquakes over a finite fault segment Therefore the modeler is interested in the general fault characteristics such as geometry and average rate of slip with an associated uncertainty Paleoseismic data and results are typically reported in scientific publications and there is no standard format or method for this reporting Typically a geologist digs a trench across a fault and looks for disrupted layers and carbon samples within these disrupted layers The samples are carbon dated and ultimately the geologist publishes a paper with information on a particular earthquake rupture or sequence of ruptures for a single point on a fault Alternatively there may be measurements or models to estimate fault parameters as a result of the occurrence of an earthquake In order to ingest this information into a model then judgment must be exercised as to how to extrapolate this information along the length of a fault segment We have expended considerable effort in combing through the literature and other existing databases online or off to include as much information as possible about the faults in California in the QuakeTables database 4 I I I a I I 


1 N 1 1   Figure 4 Extended Entity Relationship EER for the QuakeTables fault database Consistency in the data is maintained and uncertainties are allowed The representation includes data items and types and geophysical definitions e.g width is in kin 5 


For many faults there are multiple interpretations The purpose of QuakeTables is to standardize data for modelers and allow the modeler to further refine interpretations about faults As such then QuakeTables does not house one single self-consistent fault model for California Rather it houses the many different interpretations which can be many even for a single earthquake It is therefore important for the user to be able to access a self-consistent set of faults for their model and to be able to trace the fault segment recorded in the database back to the original reference Another issue is that different applications may use parameters that are reported in different ways For example slip on a fault can be reported in Cartesian or polar coordinates As a result we have also created mathematical relationships between fault data items to ensure the consistency and semantic integrity of the data The QuakeTables fault database also includes entries for including uncertainties on the data Figure 4 The current design of QuakeTables allows for rectangular faults which is consistent with the modeling applications One important requirement for the new QuakeTables design is its capability to store data from different data sources and keep it in its original format along with any calculated or derived datasets based on this original set This feature was implemented using two different dataset representations within QuakeTables The first is DataSet which are the original datasets by authors in their own format These sets are stored in dynamic tables to preserve their original format This type of dataset could also be snapshots of specific data that people want to preserve in a specific format For example we find that in carrying out pattern recognition of seismicity the seismic catalog is occasionally updated and earthquakes are inserted removed or their magnitude or location is changed The previous catalogue is no longer available and these changes can impact our results Hence we want to store all versions of the standard seismic catalogue The other data representation is QTSet which is a dataset that is derived from DataSet and conforms to the QuakeTables format that is used by simulation programs Each QTSet is linked to its original DataSet and a DataSet could have multiple QTSets Since DataSets are originally public domain QTSets could be set to public or private to users or groups of users 4 APPLICATIONS Our QuakeSim applications include traditional high performance software as well as data analysis and assimilation codes The high-performance modeling applications include GeoFEST 1 a finite element model that simulates stresses associated with earthquake faults Virtual California 2 which simulates large interacting fault systems and PARK 3 which simulates complete earthquake cycles and earthquake interaction The portal also contains Disloc which models surface deformation from faults within an elastic half-space and Simplex which is an inversion application which finds the optical dislocation model of fault slip from GPS and InSAR deformation data 4 Analysis methods include Pattern Informatics 5 which examines seismic archives to forecast geographic regions of future high probability for intense earthquakes and RDAHMM 6 a time series analysis application that can be used to determine state changes in instrument signals such as generated by Global Positioning System arrays The portal also has a mesh generation tool and tool to filter GPS time series data We expand on some of the applications here Virtual California Virtual California VC is a numerical simulation program for studying the system-level dynamics of the vertical strikeslip fault configuration in California 7,8 The majority of plate boundary deformation in California is accommodated by slip i.e earthquakes on the strike-slip faults included in the Virtual California models Figure 5 Virtual California uses topologically realistic networks of independent fault segments that are mediated by elastic interactions Virtual California is a backslip model inasmuch as the plate tectonic stress increases are produced by means of applying a negative backslip velocity to each segment whose magnitude is that of the long-term rate of slip on the segment Since positive slip reduces the stress on a fault segment negative slip due to the backslip increases the stress On each time step all faults are checked to determine whether the shear stress has reached the failure threshold Once at least one segment reaches the threshold the long time steps stop and short failure time steps a.k.a Monte Carlo Sweeps or mcs begin An mcs begins with a check of each site to determine whether it has failed followed by a parallel updating of each segment An update of a segment consists of increasing the sudden seismic slip on each segment so that the stress of the segment considered in isolation drops to a residual value plus or minus a random overshoot/undershoot The elastic stress on all segments is then recalculated and another mcs is carried out This iterative process repeats until all segments are below the failure threshold at which time the mcs time steps cease and the long plate tectonic time steps begin again Virtual California also includes a stress-dependent precursory slip or stress leakage of the type that has been observed in laboratory experiments by 9 and 10 The physics of this process is that as the stress on a segment increases a small amount of stable sliding occurs that is proportional to the level of the stress above the residual Lab experiments and field data suggest that the frictional parameter alpha 8 is of the order of a few percent Alpha is defined as the fraction of aseismic slip relative to total slip Therefore it may be possible to detect precursory signals before earthquakes using InSAR data from missions such as DESDynI Virtual California simulations enable testing for precursory signals Hence a focus is to analyze the magnitude and spatial distribution any precursory slip in the simulations 6 


Figure 5 Example fault model used by Virtual California left panel which is also included in the QuakeTables database Output converted to InSAR fringes for a given time step right panel The fringes represent surface deformation for an earthquake In this time step two earthquakes have occurred GeoFEST GeoFEST uses stress-displacement finite elements to model stress and flow in a realistic model of the Earth's crust and upper mantle in complex regions such as southern California including the Los Angeles Basin The model includes stress and strain due to the elastic response to an earthquake event in the region of the slipping fault the timedependent viscoelastic relaxation and the net effects from a series of earthquakes The physical domain may be twoor three-dimensional and may contain heterogeneous materials and an arbitrary network of faults Finite element modeling in three dimensions allows faithful modeling of complex faulting geometry inhomogeneous materials realistic viscous flow and a wide variety of fault slip models and boundary conditions Because finite elements conform to nearly any surface geometry and support wide variations in mesh density solutions may be made arbitrarily accurate with high computational efficiency GeoFEST runs in the high-performance domain of messagepassing parallel computer systems 11 including the Columbia system at NASA Ames and the COSMOS system at JPL among others In includes the functions of the PYRAMID parallel adaptive mesh refinement library 12 Source code is available with a no-fee license from Open Channel and it runs within the QuakeSim web-based problem-solving environment 13 All documentation and links to Open Channel and the portal can be found at http://quakesim.org also computed as a necessary byproduct The computational domain represents a region of the earth's crust and possibly underlying mantle It is typically a square or rectangular domain in map view with a flat upper free surface and constant depth but the domain may deviate from this The only requirement is that it be a bounded 3D domain with appropriate surface boundary conditions to render the problem well defined These boundary conditions may be specified as surface tractions and/or displacements which are usually specified on all surfaces and at times on interior surfaces such as faults Free surfaces have zero surface traction by definition Faults are interior surfaces and may have associated dislocation increments at set times The solid domain may contain layers or other distributions of material with associated rheological properties Currently supported materials are isotropic Newtonian elastic Newtonian viscoelastic and non-Newtonian powerlaw viscosity Elastostatic solutions are supported such as computing the displacements and stresses immediately caused by a specified slip distribution on a fault or finding the interior displacement and stress distribution due to a surface traction or displacement These solutions are not time-dependent Viscoelastic solutions which are time dependent are also supported in which the material flows and relaxes in response to imposed stress such as an earthquake event One may compute the viscoelastic response to a single event or to multiple events in a sequence The sequence may be user-specified Locationspecific body forces are supported The primary quantity computed by GeoFEST is the displacement at each point in a domain The stress tensor is 7 


lick on a 5tatiorn 5ymboi fr more jnf'rmation Figure 6 Google maps interface through the QuakeSim portal showing classified GPS time series data using the RDAHMM services Boundary conditions and solutions apply to a finite-element discretized approximation to this domain The domain is defined internally as a mesh of space-filling tetrahedral or hexahedral elements with three components of displacement at each mesh node constituting the solution Stress is computed for each element and is element-wise constant for the current linear tetrahedral element type Surface nodes carry special boundary conditions such as tractions or specified displacements Nodes on faults are special split-nodes that define screw or tensile dislocation on the fault without perturbing the mesh geometry Temporal evolution is by discrete time steps using an implicit solution technique allowing large time steps without numerical instability RDAHMM RDAHMM or Regularized Deterministic Hidden Markov Model carries out time series analysis and mode detection in GPS and other signals Examples of signals that RDAHMM can detect are ground subsidence from withdrawal of water from aquifers and earthquake coseismic and post-seismic signals We have integrated the processing of GPS position time series data into the QuakeSim portal By wrapping the RDAHMM time series analysis software as a web service filter it is seamlessly integrated into work and data processing flows Raw GPS data lHz are converted to RYO real-time format and made available through a data server Then data are passed through a series of filters that perform format conversion and station separation Message passing is handled through NaradaBrokering Finally data are passed to the RDAHMM analysis application We have implemented an interface through which the RDAHMM software can be applied to archived daily GPS solutions to perform time series segmentation Segmentation results are provided both graphically and 8 


through numerical descriptions of segmentations and fitted models which are available for download In addition we have implemented a proof-of-concept Google maps interface to RDAHMM analysis of real-time streaming GPS data The segmentation analysis is performed on the last ten minutes of real-time data and then displayed graphically upon mouse-over in the Google maps interface Figure 5 5 CONCLUSIONS carried out at the Jet Propulsion Laboratory California Institute of Technology under contract with NASA from the Earth Science and Technology Office and at University of Southern California Indiana University NASA Ames University of California Davis and University of California Irvine and Brown University under subcontract with the Jet Propulsion Laboratory REFERENCES Through the QuakeSim project we are developing tools for carrying out comprehensive simulations of earthquake fault interactions Our focus to date has been on California but the tools are applicable to any region in the world and through collaborations with the University of Queensland Australia we will be extending the database and models to cover that regions of the world We have recently released the QuakeSim 2.0 portal based on GridSphere the popular science gateway product We are working to extend our distributed computing environment by interfacing with the Columbia computer at NASA Ames and the Cosmos computer at JPL Our QuakeTables fault database is designed to allow modelers to make use of paleoseismic data and the associated uncertainties We have expanded QuakeSim to access GPS time series data and are developing the ontologies to access GPS velocity data and synthetic aperture radar interferograms InSAR data Current earthquake risk estimation is based on static models inferred from past earthquake activity as determined through paleoseismology and historical earthquakes Earthquake fault systems are continuously changing state based on deformation of the Earth's crust and mantle as well as strain release and transfer from earthquakes It is therefore important to develop time-dependent models for earthquake forecasting Current earthquake hazard maps have an outlook of decades 14,15 Our goal is to improve earthquake forecasting by in effect migrating from static hazard maps to dynamically changing earthquake forecasts based on the current state of the system 6 ACKNOWLEDGEMENTS We'd like to thank the many other QuakeSim contributors These include JPL staff members Charles Norton for the adaptive mesh refinement and porting and optimization of software to the Columbia and Cosmos systems and Margaret Glasscoe for testing the portal using science applications and developing the web pages and Harout Nazerian for developing the web pages and testing the portal Galip Aydin John Youl Choi and Zhigang Qi at Indiana University have also contributed to development of the QuakeSim portal Gleb Morein at UC Davis has worked on the development of Virtual California including porting to the Columbia computer system Lorena Medina corrected and validated the QuakeTables database at UC Irvine Terry Tullis at Brown University and Nick Beeler from the USGS develop and maintain the PARK application This work was 1 Parker Jay Andrea Donnellan Gregory Lyzenga John B Rundle Terry Tullis Performance Modeling Codes for the QuakeSim Problem Solving Environment International Conference on Computational Science 855862 2003 2 Rundle J.B K.F Tiampo W Klein and J.S.S Martins Self-organization in leaky threshold systems The influence of near mean field dynamics and its implications for earthquakes neurobiology and forecasting Proc Nat Acad Sci USA 99 Supplement 1 2514-2521 2002 3 Tullis T E Earthquake models using rate and state friction and fast multipoles Geophysical Research Abstracts European Geophysical Society 5 13211 2003 4 Lyzenga G.A W.R Panero A Donnellan The Influence of Anelastic Surface Layers on Postseismic Thrust Fault Deformation J Geophys Res 105 31513157 2000 5]Tiampo K.F Rundle J.B McGinnis S and Klein W Pattern dynamics and forecast methods in seismically active regions Pure and Applied Geophysics 159 2002 6 Granat R A Regularized Deterministic Annealing EM for Hidden Markov Models Ph.D Thesis University of California Los Angeles 2004 7 Rundle JB Klein W Tiampo K et al Linear pattern dynamics in nonlinear threshold systems Phys Rev E 61 2418-2431 2000 8 Rundle P.B J.B Rundle K.F Tiampo et al Nonlinear network dynamics on earthquake fault systems Phys Rev Lett 87 Art No 148501 2001 9 Tullis T.E Rock friction and its implications for earthquake prediction examined via models of Parkfield earthquakes Proc Nat Acad Sci 93 3803-3810 1996 10 Karner SL Marone C Frictional restrengthening in simulated fault gouge Effect of shear load perturbations J Geophys Res 106 19319-19337 2001 9 


11 Parker J Donnellan A Lyzenga G et al Performance modeling codes for the QuakeSim problem solving environment Lect Notes In Computer Sci 2659 855862 2003 12 Norton C G Lyzenga J Parker E Tisdale Developing Parallel Active Tectonics Simulations Using GeoFEST and the PYRAMID Adaptive Mesh Refinement Library Eos Trans AGU 85 Fall Meet Suppl Abstract SF43A-0785,2004 13 Pierce M Youn C Fox G Interacting data services for distributed earthquake modeling Lect Notes In Computer Sci 2659 863-872 2003 14 Blanpied M Working Group on California Earthquake Probabilities Earthquake Probabilities in the San Francisco Bay Region 2002-2031 US Geol Survey Open File Report 30-214 2003 15 Petersen M.D W.A Bryant C.H Cramer T Cao M Reichle A.D Frankel J.J Lienkaemper P.A McCrory and D.P Schwartz Probabilistic Seismic Hazard Assessment of for the State of California US Geol Survey Open File Report 96-706 1996 revised 2003 BIOGRAPHIES Andrea Donnellan is the DESDynl Science Lead and QuakeSim principal investigator at NASA's Jet Propulsion Laboratory and is a research professor at the University of Southern California Donnellan uses GPS and InSAR satellite technology coupled with high performance computer models to study earthquakes plate tectonics and the corresponding movements of the earth's crust She has been a geophysicist at JPL since 1993 She received a B.S from the Ohio State University in 1986 with a geology major and mathematics minor She received her M.S and Ph.D in geophysics from Caltech's Seismological Laboratory in 1988 and 1991 respectively Donnellan received an M.S in Computer Science from the University of Southern California in 2003 She held a National Research Council Postdoctoral Fellowship at NASA Goddard Space Flight Center Donnellan was a Visiting Associate at the Seismological Laboratory at Caltech from 1995 to 1996 In 1996 Donnellan received the Presidential Early Career Award for Scientists and Engineers in 2003 the Women in Aerospace Award for Outstanding Achievement and in 2006 she was the MUSES of the California Science Center Foundation Woman of the Year Jay Parker is a Senior Scientist in the Geodynamics and Space Geodesy group of the Jet Propulsion Laboratory a NASA center administered by the California Institute of Technology His graduate research used computer simulations to explain mesospheric ionization response to so ar ares an dynamic instabilities Dr Parker's research subjects at the Jet Propulsion Laboratory include a variety of topics in remote sensing analysis and modeling These include supercomputing algorithms for electromagnetic scattering and radiation satellite geodesy and finite element simulation for earthquake-related deformation and ocean sensing through GPS signal reflection He is currently the software engineer and coinvestigator for the QuakeSim project which has developed a solid Earth science framework including a variety of simulation and analysis tools He also develops the SEASCRAPE software system for high-fidelity simulation and parametric retrieval of atmospheric infrared spectrometry at Remote Sensing Analysis Systems Inc of Altadena CA Dr Parker is a member of the American Geophysical Union and co-chair of the Data Understanding and Assimilation working group of the APEC Cooperation for Earthquake Simulation Robert Granat is a senior member of the technical staff in the Data Understanding Systems Group at NASA's Jet Propulsion Laboratory He received a B.S in engineering and applied science from Caltech in 1996 an M.S in electrical engineering in 1998 from University of California Los Angeles and a Ph.D in electrical engineering with an emphasis in signal processing from UCLA He works on the application of hidden Markov models to the study earthquakes and spacecraft systems Geoffrey Fox received a Ph.D in Theoretical Physics from Cambridge University and is now professor of Computer Science Informatics and Physics at Indiana University where he is director of the Community Grids Laboratory He previously held positions at Caltech Syracuse University and Florida State University He has published over 550 papers in physics and computer science and currently works in applying computer science to Defense Earthquake and Ice-sheet Science and Chemical Informatics He is involved in several projects to enhance the capabilities of Minority Serving Institutions 10 


Marlon Pierce is assistant director of the Community Grids Laboratory and leads the portal and services development for the QuakeSim project He has a Ph D in computational condensed matter physics from Florida State University 1998 His research interest is the application of distributed computing techniques and technologies to problems in physical sciences John Rundle is the Director of the Computational Science and Engineering Center at the University of California at Davis and is Director of the California Institute for Hazard Research of the University of California He was educated at Princeton University BSE 1972 and the University of CaliforniaLos Angeles MS 1973 PhD 1976 His research is focused on understanding the dynamics of earthquakes through numerical simulations pattern analysis of complex systems dynamics of driven nonlinear Earth systems and adaptation in general complex systems He has published over 200 papers in the peer-reviewed literature Dennis Mcbeod is currently Professor of Computer Science at the University of Southern California and Director of the Semantic Information Research Laboratory He received his Ph.D M.S and B.S degrees in Computer Science and Electrical Engineering from MIT Dr McLeod has published widely in the areas of data and knowledge base systems federated databases database models and design and ontologies His current research focuses on dynamic ontologies user-customized information access database semantic heterogeneity resolution and interoperation personalized information management environments information management environments for geoscience and homeland security information crisis management decision support systems and privacy and trust in information systems Rami Al-Ghanmi is a PhD student in the Computer Science Department at the University of Southern California He received his MS in Computer Science from the USC 2006 and BS in Computer Engineering from King Fahd University of Petroleum  Minerals Dhahran Saudi Arabia 2002 He is currently working with Professor Dennis McLeod at the Semantic Information Research Group at USC His research interests are Semantic Web Services and Ontology-based Federation of scientific data Lisa Grant is an associate professor of public health at University of California Irvine She studies environmental problems from a geologic perspective with emphasis on natural hazards The primary objective of her research program is to identify active faults and to quantify their potential for generating large earthquakes by discovering their history of earthquake production over geologic time intervals The record of previous large earthquakes on active faults is one of the best indicators of future earthquake activity The results of her research on earthquake occurrence patterns are applied for earthquake forecasting land-use planning building design risk assessment disaster preparedness planning and public education about the earthquake threat Grant received a B.S in environmental Earth science from Stanford in 1985 an M.S in both environmental engineering and science and geology in 1989 and 1990 respectively and a Ph.D in geology and geophysics from Caltech in 1993 She serves on the board of directors of the Southern California Earthquake Center and is an associate director of the California Hazards Research Institute Walter Brooks has worked at NASA Ames Research center since 1977 As chief of the NASA Advanced Supercomputing NAS Division he oversaw the full range of high-performance computing efforts within the division Brooks has led groups that simulated and designed space science missions such as Infrared Astronomical Satellite IRAS Space InfraRed Telescope Facility SIRTF and Stratospheric Observatory for Infrared Astronomy SOFIA In 1993 he was selected to lead the Space Station redesign management team Awarded the NASA Outstanding Leadership medal for his work in the area of high-end computing Dr Brooks has played an important role in the high-end computing aspects of the Return-to-Flight effort He was a member of the High End Computing Revitalization Task Force that produced an important report in 2003 that outlines a set of the organization's key findings and recommendations to advance the state of high-end computing in the U.S He is past president of the SGI user group a Sloan Fellow an AIAA Associate Fellow and author of more than 50 publications Brooks received a doctorate in physics from Stevens Institute of Technology performing research for his thesis at Brookhaven National Laboratory He earned a master's degree from the Stanford Graduate School of Business in 1991 11 


 12 7  C ONCLUSIONS AND F UTURE W ORK  This research task started with an all-FORTRAN implementation of the FTIR spectrometry algorithm converted it to C code, and developed a number of H W/SW systems on the V4FX60 hybrid-FPGA The execution ti me of the all software C implementation of the FTIR spectrometry algorithm was recorded and used for comparison as a base case Two software-based optimizations were applied that reduced the executi on time by more than 4.5x These included modifying the cod e to use all single-precision math library functions no n-ANSI when dealing with single-precision data and utilizi ng the IBM Performance Libraries  Perflib  to improve the speed of all single and double-precision arithmetic The idea of using a DP-only Perflib was introduced and then used in conjunction with the single-precision APU-FPU to fu rther improve system performance The bulk of the research dealt with looking into ha rdwarebased improvements to the FTIR spectrometry system  These included the Xilinx APU-FPU, and a single-pre cision dot-product co-processor The APU-FPU delivered significant speedup for all single-precision floati ng-point operations The dot-product co-processor although ineffective in the FTIR spectrometry system due to poor spatial locality of the data, showed nearly a 2x im provement over the APU-FPU when working with smaller, sequent ially accessed data sets Furthermore it was implemented  as a load/store-based APU-connected FCM thus establishin g a reference for creating similar APU co-processors T he design of a non-system-bused CPU-coupled co-process or is frequently overlooked in design guides yet it is a  very effective way to offload software routines to hardw are implementation The ML410 development board on which all of this w ork was conducted, hosts the V4FX60 hybrid-FPGA contain ing two PPC405 processors This research task focused o n optimizing the performance of the FTIR spectrometry  algorithm on a single PPC405 core, however, the des ign can be extended to utilize both available cores. Figure 16 on the following page shows a dual-core design that can be  implemented on the ML410 board The two PPC405 processors each have dedicated PLB interfaces but s hare a common OPB. On the common OPB, the processors need to negotiate access to the RS232 UART and SystemACE CF  controller. This negotiation can be done through du al-ported shared BRAM accessible by each processor from thei r respective PLB. The ML410 board has two external me mory interfaces that are both utilized in this concept PPC405 CPU0 uses the DDR2 DIMM 256 Mbytes while PPC405 CPU1 utilizes the DDR on-board component memory 64  Mbytes\. Each of the processors has some dedicated on-chip memory OCM connected through OCM interface The instruction side OCM is particularly necessary so e ach processor can store its own boot code in its own on chip memory as booting both processors from external mem ory may not be possible Both processors have their own FPUs connected on dedicated FCB interfaces Since the processing of individual interferograms is a comple tely independent task an up to 2x reduction in executio n time may be possible with a dual-core system However o ne bottleneck that may limit the speedup is negotiatin g access to the shared CF card controller Additional improvement to the overall performance o f the FTIR spectrometry system may be possible by rewriti ng the software in C. The automatic conversion from FORTRA N to C using f2c most likely does not produce optimal code, and it is certainly not appealing to read Some functio ns may also need to be rewritten with an optimized pattern  of data access This can help in cases such as the dot-prod uct coprocessor Further performance improvement may be achieved by trying a different compiler one that is specifical ly targeted for the embedded PPC405 processor A V2P performanc e study done at the NASA Goddard Space Flight Center concluded that using the WindRiver Diab DCC 5.2 com piler provides a 38 performance increase over the GNU-GC C 3.4 compiler The comparison was based on running a  Dhrystone benchmark application on a 400 MHz PPC405 design The GNU-GCC compiler achieved 458 DMIPS while the WindRiver Diab DCC achieved 628 DMIPS as  reported by Xilinx\ [10   Implementing additional hardware co-processors may result in the further reduction of execution time Using t he dotproduct design as a reference the FFT function fo r example can be implemented in the hardware This w ill help in the spectrum computation component of the s oftware processing. It may be necessary to re-arrange the d ata access pattern for optimal co-processor performance to av oid the pitfall seen when deploying the dot-product core Finally, no embedded processing system is complete without an OS. Linux is a good choice and is supported by X ilinx in EDK It is important to first finalize the hardware  design prior to deploying the OS Support for the APU may be lacking in Linux and getting the OS to recognize th e hardware FPU may be a project in itself For the FTIR spectrometry algorithm this research task started the process of moving from an all software system to a mixed HW/SW implementation on the V4FX60 hybridFPGA In the best case a more than 8x speedup was achieved compared to the FTIR base system This implementation, although nearly 2x faster the V2P s ystem at NASA JPL still lags behind the current state-of-th e-art space processor  the BAE RAD750 However the marg in between the two was narrowed down significantly and with further research as suggested above will most lik ely be eliminated altogether Directly benefiting from the  work presented in this paper is a 3-year JPL technology 


 13 development task that will support the MATMOS on-bo ard processing implementation for a future flight i nstrument   Fig. 16 Dual-core concept targeting ML410 development boar d 


 14 R EFERENCES  1  D L Bekker Hardware and Software Optimization of Fourier Transform Infrared Spectrometry on HybridFPGAs  MS Thesis Rochester Institute of Technology Rochester NY August 2007 Available http://hdl.handle.net/1850/4805  2  P J Pingree J.-F L Blavier G C Toon and D  L Bekker An FPGA/SoC Approach to On-Board Data Processing  Enabling New Mars Science with Smart Payloads in IEEE Aerospace Conference 2007  Big Sky MT March 2007 Available http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?isnu mber=4 144550&arnumber=4161501  3  S I Feldman D M Gay M W Maimone and N L Schryer, “A Fortran-to-C Converter,” Computing Scie nce Technical Report 149, AT&T Bell Laboratories, Murra y Hill NJ March 22 1995 Available http://www.netlib.org/f2c/f2c.pdf  4  IBM PowerPC Embedded Processor Performance Libraries tech rep IBM Microelectronics Divisi on Hopewell Junction, NY, December 12 2003 5  Virtex-4 Data Sheet: DC and Switching Characterist ics Datasheet DS302 Xilinx Inc San Jose CA March 27 2007 Available http://www.xilinx.com/bvdocs/publications/ds302.pdf  6  ML410 Embedded Development Platform  Xilinx Inc San Jose CA March 6 2007 Available http://www.xilinx.com/bvdocs/userguides/ug085.pdf  7  G Toon J.-F Blavier M McAuley and A Kiely Advanced On-Board Science Data Processing System for a Mars-orbiting FTIR Spectrometer R&TD Task 01STCR  R.05.023.048 NASA Jet Propulsion Laboratory Pasadena, CA, 2005 8  APU Floating-Point Unit v3.0 product specificati on Xilinx Inc San Jose CA January 26 2007 Availa ble http://www.xilinx.com/bvdocs/ipcenter/data_sheet/ap u_fp u.pdf  9  PowerPC 405 Processor Block Reference Guide Xilinx Inc San Jose CA July 20 2005 Available www.xilinx.com/bvdocs/userguides/ug018.pdf  10  D. Petrick, “Analyzing the Xilinx Virtex-II Pro Pow erPC with the Dhrystone Benchmark Applications,” tech. r ep NASA Goddard Space Flight Center Greenbelt Maryland Available http://klabs.org/DEI/Processor/PowerPC/v2pro_ppc_pe rf ormance_petrick.doc  B IOGRAPHY  Dmitriy Bekker has just completed his Masters Degree in Computer Engineering at the Rochester Institute of Technology in Rochester NY His areas of interest include FPGAs embedded systems digital signal processing and system architecture. He has coop  internship experience working at Brookhaven National Laboratory Syracuse Research Corporation NASA Dryden Flight Research Center, and the Jet Propulsion Laboratory. He recen tly won in the 2006 IEEE Student Design Contest for his pro ject in autonomous vehicle navigation. He is a member of IE EE Dr Lukowiak is an assistant professor in the Computer Engineering Department at Rochester Institute of Technology in Rochester NY His research interests are concentrated in the area of multidisciplinary projects that require modeling and hardware implementations FPGA and ASIC of data processing systems Dr Marcin Lukowi ak obtained his Ph.D in Technical Sciences from the P oznan University of Technology in October 2001 Muhammad Shaaban is an associate professor of computer engineering at the Rochester Institute of Technology His research interests include high performance computing processor microarchitecture heterogeneous and reconfigurable computing Shaaban has a PhD in Computer Engineering from the University of Souther n California. He is a senior member of the IEEE Dr. Blavier first joined the JPLMkIV Team in August 1985 as a contractor from Ball Aerospace He participated in the MkIV campaigns in McMurdo Antarctica groundbased and from Punta Arenas Chile NASA DC-8 In late 1987, he started graduate work with Profs Delbouille and Dubois at the University  of Liège Belgium his research tasks included install ing the 


 15 Fourier transform spectrometers at the Internationa l Scientific Station of the Jungfraujoch Switzerland  for atmospheric measurements and at the Institute of Astrophysics in Liège for laboratory measurements He was hired by JPL in August 1990 as MkIV cognizant engin eer and participated in all the MkIV campaigns since th en \(one DC-8 campaign 19 balloon campaigns Dr J.-F Bla vier obtained his Ph.D in Physics from the University o f Liège in July 1998 Paula Pingree is a Senior Engineer in the Instruments and Science Data Systems Division at JPL She has been involved in the design integration test and operation of several JPL flight projects most recently Deep Impact DI She has worked on the Tunable Laser Spectrometer development for the 2009 Mars Rover and is presently the Electronics CogE for the Juno Mission s Microwave Radiometer She also enjoys research and technology development for Smart Payloads in her s pare time Paula has a Bachelor of Engineering degree i n Electrical Engineering from Stevens Institute of Te chnology in Hoboken, NJ, and an MSEE degree from California State University Northridge.  She is a member of IEEE 


  16  Figure 15. AIRS-AMSRE differences as a function of AIRS error estimate over one day  AIRS has an error estimate of the total water vapor value that it calculates. The diffe rences between AIRS and AMSR-E are shown as a function of this estimate in figure 15 and very little correlation is found 11  R ELEVANT W ORK  Merged A-Train Level 2 Data A merged product that preserves the relationship of observed atmospheric water properties facilitates the hydrological studies by enabling scientists to get directly at the model data without worrying about the logistics of finding, collecting, and coordinating the measured quantities from different instruments. Previously there did not exist a capability to discover and access data from the A-Train\222s multiple instruments as merged multi-parameter data sets Enabling Orchestratable Service Workflows Our distributed service-oriented approach of loosely coupled services also enable s a higher level of reusability and orchestration with other services. Increasing numbers of workflow engines are already supporting Web Services as components/operators, which can then be orchestrated together into higher-level meta/virtual services SciFlo, a Scientific Dataflow Execution Environment, is a workflow engine that already supports assembling reusable SOAP Services, native execu tables, local command-line scripts, and codes into a distributed computing flow \(a graph of operators\8 SciFlo can u tilize o u r g en eric SOAP services as part of a larger coordinated data flow The Taverna Workbench is a free software tool for designing and executing workflows. Like SciFlo, it can orchestrate SOAP-based Web Services as components within a workflow. Taverna provides a visual editor to construct and edit the sequence of services in the workflow We have found that Taverna can dynamically introspect a given WSDL and construct the workflow component interface representing it Giovanni Giovanni, an acronym for the Goddard Earth Sciences Data and Information Services Cent er, or GES DISC, Interactive Online Visualization and Analys is Infrastructure, is a webbased tool to help visualize Earth science data  It  provides a simple and intuitive way to visualize, analyze and access vast amounts of Eart h science remote sensing data without having to download the data. Similar to the services developed here, it addresses the difficulties of traditional data acquisition and analysis methods by moving the complexity to the server-side Giovanni provides multiple in terface instances based on instrument and measurement ty pes. For example, the \223ATrain Along CloudSat Track Inst ance\224 can provide plots of vertical profiles of clouds, temperature, humidity, cloud and aerosol classification across the multiple instruments of the A-Train A distinction between Givanni\222s A-Train data and the data set in this paper is that we are using a formal merged product of the A-Train. We leverage the NEWS effort that is based on error- and resolution-weighted mean of the input data sets, with associated uncertainty estimates. This provides a formal model of the collective A-Train observations rather than the collection of the individual instrument measurements Each of Giovanni\222s multiple interface instances provides a very simple and easy to use web interface. However, we recognized that sometimes scientists want more than the simple interfaces. Some scien tists may want to process Level 3 products using their own trusted code, or may want to perform variations of their own plots. With Giovanni, the individual scientist wanting more custom advanced capabilities must depend on the Giovanni development team Giovanni is based on the web portal paradigm where users visit a web page and use web tools to find and visualize data. Similar to Giovanni, our client APIs also make data acquisition more seamless. However, our services are based on the different paradigm were the power and flexibility of data analysis and processing are shifted back into the scientists own familiar computing environments. We realize that scientists generally want to perform \223exploratory computing\224 where they can sere ndipitously analyze the data using their own familiar and trusted code 


  17 Giovanni 2 was inherently synchronous where processing was bounded to a single http session. Long service running times still require the user to hold the same http session Similar to our asynchronous Web Service we discussed, the upcoming Giovanni 3 will be supporting asynchronous sessions. They will be using a RSS feed to monitor the service request. Version 3 will also be based on a servicesoriented architecture, wher e Giovanni services can be offered as a standard SOAP Web Services. This is similar to our approach, as well as SciFlo\222s services 12  C ONCLUSIONS  To achieve the science research goal of investigating longterm and global-scale trends in climate, water and energy cycle, and weather variability, we enhanced and improved on existing algorithms to work with distributed and heterogeneous data and information systems infrastructure By developing a service-oriented architecture for discovering, accessing, and mani pulating of NEWS merged A-Train data sets, we can strengthen the interconnectedness and reusability of these services across broader range of Earth science investigations The merged NEWS Level 2 data is a formal model containing the voluminous data from the AIRS, AMSR-E MLS, MODIS, and CloudSat instruments. Previously scientists wanting to perform long-term and global-scale studies encompassing simultaneous measured quantities would quickly face a data acce ss hurdle of first finding the data, then manually downloading them, and finally merging the data into a cohesive model\227before starting their analysis. Additionally the voluminous nature of the data particularly because of the MODIS data\each scientist potentially downloading the same data resulting in redundancy of reprocessing on the client sides. Our paradigm pushes more of the commonly repeated processing onto the server side. Moreover, this avoids repeated downloading of the same data among the science users. We can deliver customi zed averaged, subsetted, and summarized data of the merged A-Train observations to the scientists for them to immediately begin their analysis work We recognized that scientists also often want to perform 223exploratory computing\224 where they can freely explore the aspects of the data and run serendipitous exploration in their own familiar environment. We developed client-side distributed APIs in popular analysis environments such as Matlab, IDL, and Python. Our APIs hide the complexity of Web Services and allow the service capabilities to be embedded in the scientists own computing environments By purposely avoiding the \223web portal\224 paradigm and providing the suite of platform specific APIs in each of these language platforms, we enable the scientists to remain within their own familiar environments to select, process and download the data seamlessly into their environment for their own further analysis. Alternative methods involving web portals force the scientists to leave the environment and manually interact with the web portal to search and download the data We can examine not only long-term changes in amplitude of a single variable but also those among multiple variables Our L3Q clustering method was specifically designed to preserve information about the covariability of multiple observations, such as those from the A-Train.  Weather and climate variability is characterized by changes among atmospheric observables, but those changes have been limited by a lack of observations and analytical techniques We are not aware of any multi-parameter analyses to date The full potential of the A-Train climate record will not be realized until the multi-parameter climatology is understood. The work presented is one method of approaching this difficult problem Our service tool addresses several objectives of the NASA Earth science data community including 1\mprove interoperability to facilitate the transparent access and manipulation of heterogeneous and distributed data by science users, 2\ransition and deploy existing Earth science research analysis tools and software using a 223Service Oriented Architecture\224 \(SOA\ to enhance their reuse potential for other science domains and improve overall awareness and access of these tools by a broad community, 3\ increase users\222 ability to customize their discovery, access, deliv ery, manipulation, and preferred format of data and information 12  F UTURE W ORK  On-demand Level 3T Summaries from Level 3Q We plan to develop services for creating custom summaries of the L3Q data into more refined Level 3T summaries L3T\create their own custom Level 3 products on demand from L3Q. The custom Level 3 products are the transformation of L3Q data based on user-specific objectives such as regression and correlation analyses. The cust om production will generate not only the transformed data but also the statistical estimation of the accuracy of the summarized data based on the distribution of L3Q and the quality of L3Q Delegating the Temporal-Spatial Data Querying Currently, our processing layer utilizes existing and legacy processing code that was developed in IDL, Matlab, and C++. Though the original intent was to be able to adapt existing code and wrap as a service, this meant maintaining its original form of accessing the source data for processing Small modifications were made to enable these codes to quickly access the data based on file path and file naming schemes. However, we want to decouple the file accessibility and processing roles 


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


