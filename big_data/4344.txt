Abstract 
 
Hsiao-Fei Liu, Chung-Tsai Su CoreTech Trend Micro, Inc Taipei Taiwan ken_liu, chungtsai_su}@trend.com.tw An-Chiang Chu Computer Science and Information Engineering National Taiwan University Taipei Taiwan anchiang@gmail.com 
Quasi-biclique mining for bipartite graphs has found important applications in providing security services However, the standard MapReduce algorithm for mining quasibicliques does not scale well due to the need of shuffling and reducing a huge number of map outputs. To cope with web-scale graphs, we propose a scalable algorithm with the use of Giraph which is a new rising large-scale graph processing platform following the bulk synchronous parallel \(BSP\ model 
Fast Quasi-Biclique Mining with Giraph 
Experimental results on real world domain-IP graphs demonstrate that our proposed solution is able to reduce CPU time by 80% and disk I/O by 95%, compared with the standard MapReduce algorithm 
Keywords Giraph; Bulk Synchronous Parallel; Graph Partitioning; Bipartite Graph; Quasi-Clique 
 
I I NTRODUCTION Node clustering for bipartite graphs plays important roles in many real applications [1, 16 a n d es pe ci ally th e d ete cti on of  network incidents [4, 7, 8, 15, 20 o ng t he m o st re nown  clustering algorithms for graph data, quasi-biclique mining is an approach which focuses on finding dense subgraphs in a bipartite graph Quasi-bicliques g ained popularities [11, 14, 15 21 in m ode lin g  cluste r s in  bi pa rti t e g r a p h s f o r th eir  res i sta n ce  
against noise and missing information, which are unavoidable in real datasets A few applications of quasi-biclique mining in security services are given below 
Given a bipartite domain-IP graph each quasi-biclique corresponds to a set of closely related domains and IPs Thus when a domain is reported to contain malicious materials we can quickly react to inspect all related IPs and domains to provide better protection Given a bipartite website-client graphs, quasi-biclique mining can find the set of websites sharing similar clients. Consider a website which is reported to be a command and control \(C&C\ server. Hackers used to setup multiple C&C servers for high availability and these C&C servers usually share the same bots. Thus finding websites sharing similar clients with the 
002 002 
reported C&C server can help to identify remaining C&C servers To the best of our knowledge, the only quasi-biclique mining algorithm which targets at web-scale data was due to Su et al. [1  T h ei r m o tiv e i s t o  dete ct n etw ork  in ci de n t s  by  analyzing how quasi-bicliques in domain-IP graphs evolve over time. Su et al 
s algorithm is based on MapReduce [3, 6, 9 19  T h e st and ar d Ma pRe d u ce algo r i th m  f o r m in in g  qu asibicliques suffers from the need of shuffling and reducing a huge number of map outputs and does not scale very well. To achieve better scalability, Su et al. have adopted a heuristic strategy at the expense of solution quality In this paper, we propose a scalable quasi-biclique mining algorithm with the use of Giraph 
 
2  w h ich is a n e w risin g  large-scale graph processing platform and very suitable for implementing iterative algorithms Our approach consists of 3 phases. The first phase is to divide the graph into smaller partitions with an iterative algorithm implemented by using Giraph. The partitioning algorithm is carefully designed so only nodes believed to be closely related would be assigned to the same partition. Next, a MapReduce job is run to augment each partition with its adjacent inter-partition edges so that we do not lose any information. Finally, a MapReduce job is run to compute quasi-bicliques for each augmented partition in parallel Experimental results on the domain-IP graphs constructed from web browsing logs demonstrate that our proposed algorithm achieves significant im 
provement over the standard MapReduce algorithm II P RELIMINARY The formal definitions for bipartite graphs and quasibicliques are given below   and The elements in   
003 004\005 006 007 010 
Definition 1 
A Problem Definition We say G X, Y, E is a bipartite graph if and only if X Y E X Y X Y are called the vertices, and the element s in E are called edges. A vertex u is a neighbor of a vertex v if we have either u, v 
011    
G 
    0 1   quasi-biclique   
011 011 012 011 012 012 
Definition 2 
E or v, u E. The degree of a ver tex is defined to be the number of its neighbors Given a bipartite graph G X, Y, E a vertex x X and a threshold a tuple X Y is said to be the quasi-biclique for x, denoted by x if Y 
         
 
011 011 011 012\013 011 
        
 y Y x, y E and X x X there are at least Y of vertices y in Y satisfying x y E  
2013 IEEE International Congress on Big Data 978-0-7695-5006-0/13 $26.00 © 2013 IEEE DOI 10.1109/BigData.Congress.2013.53 347 


quasix x x X Given a bipartite graph G X, Y, E a vertex x X and a threshold the quasi-biclique mining problem is to compute the set S x x X  B Giraph Compute Combiners Aggregators A Partitioning XY Given a bipartite graph G X, Y, E an induced subgraph C X c Y c E c is said to be a partition of G if and only if Y c is the set of neighbors of X c in G. A set of partitions P X Y E  X Y E X k Y k E k is said to be a partitioning result of G if and only if X X X k X and X i X j for all i 002 j x X x X y Y y y y Y X Y v v v 
012 012 012 011 012 011 012 011 010 010 013\013\013 010 003 005 
Definition 3 Definition 4 
In other words, the biclique for a vertex is a tuple consisting of \(1\ertices connecting to at least of s neighbors and \(2 s neighbors. An example is shown in Figure 1 Figure 1. quasi-bicliques of a bipartite graph The quasi-biclique mining problem is to compute the set of quasi bicliques for vertices in part    0 1  quasi biclique    Giraph is a large-scale graph processing platform which follows the design of Google s Pregel system [13 and cou l d be seen as a variant of the Bulk Synchronous Parallel \(BSP model [18  Giraph takes a directed graph and a user defined function e input. The execution consists of a sequence of iterations, called supersteps During a superstep the user defined function is involked fo r each vertex in parallel The user defined function can read messages sent to the vertex in the previous superstep, send messages to other vertices that will be received in the next s uperstep, set/get the values of the vertex and its outgoing edges make changes to the topology that will take effect in the ne xt superstep and vote to halt Initially each vertex is in the active state. A vertex deactives itself by voting to halt and keeps inactive until it receives a message. The execution stops if all vertices are inactive and no messages are in transit In addition, users are allowed to define some utility functions, called and for purpose of message reduction and global communication Giraph is great for implementing iterative algorithms for it would not incur unnecessary I/O workload. Unlike chained MapReduce, there are no extra I/O workloads caused by the shuffling phases in the MapReduce jobs and reading/saving of the outputs of intermediate iterations However, there are two limitations in using Giraph by our experience. First, Giraph requires the whole graph to be loaded into memory before execution, so you must have a lot of memory to analyse large graphs. Second, Giraph requires careful control over the message complexity of each superstep to avoid out-of-memory errors. If your algorithm has superlinear message complexity, chained MapReduce should be a better choice III A LGORITHM Our algorithm consists of three phases: \(1\ partitioning, \(2 augmenting and \(3\ining. The ke y idea is to first divide the input graph into partitions through an iterative Giraph algorithm. The resulting partitions could be considered as relaxed quasi-bicliques or communities.  We then augment each partition with its adjacent inter-partition edges so that we have all the required information for computing the complete set of quasi-bicliques. Finally, we compute quasi-cliques for each augmented partitions in parallel by a MapReduce job The goal of partitioning is to divide the bipartite graph into communities of vertices with dense connections internally and sparser connections between communities. The formal definition of graph partitioning is given below. Note that by the definition two different partitions would have disjoint part vertices but may have common part vertices         1 2  Our algorithm proceeds as foll owing. In the first iteration we set the community ID for each vertex in part to the MD5 checksum of its neighb ors. Then each vertex in part sends its community ID and degree to its neighbors. The intuition behind this step is quite simple: vertices with the same neighbors should belong to the same community In the second iteration, each vertex in part would receive community IDs and degrees of its neighbors sent in the previous iteration. We set the community ID of to the community ID of its highest-degree neighbor. And then informs all of s neighbors about its community ID. The intention for selecting the community ID sent from the highest-degree neighbor as s community ID is to increase the probability that most part vertices in the same community would have the same community ID. It is based on the assumption that the underlying communities have structures similar to bicliqu es so that the highest-degree part vertex within a community would have connections to most of the part vertices of the same community From the third iteration, we apply the majority rule to adjusting the communities iteratively until the convergence criteria are met. The majority rule says that a vertex could not change its community ID unless more than half of s neighbors have a common community ID which is not the same as s community ID. The process iterates until all 
         
G 
1 1 1  2 2 2  1 2 
 
348 


sets its value to the hash of its neighbours at superstep 0 Figure 3. Each vertex in part sets its value to the value of its highestdegree neighbour at superstep 1 Figure 4 A vertex in part changes its value if the majority value of its neighbours exists and is different from its current value at superstep 2 Figure 5 A vertex in part changes its value if the majority value of its neighbours exists and is different from its current value at superstep 3 Figure 6 All vertices in part stop to change values so the convergence criteria are met at superstep 4 
X Y X Y  
vertices stop to change their community IDs. A partition is defined to be the vertices in part with the same community ID and their adjacent vertices in part  A running example is demonstrated through Figure 2-6 where vertices in part are shown on the left side and vertices in part are shown on the right side The value of a vertex is its community ID Figure 2. Each vertex in part 
X Y X Y X 
349 


The number of iterations in the execution of Algorithm 1 is at most E Proof S S i S i S S E S i S i i v i S i v v S iv v v M M M M M M M v v B Agumenting X Given a bipartite graph G X, Y, E and a partition C X c Y c E c the augmentation A C  A X c A Y c  A E c of C is an induced subgraph such that A Y c Y C and A X c is the set of neighbors of Y c in G. The set X c is called the core of A C  x X x x x  x y y x X C v  v X v x x X C C v X 
Theorem 1 Algorithm 1 Definition 5 Algorithm 2 
 
Theorem 1 guarantees the convergence of our partitioning Algorithm   000 Let be the edges whose endpoints are assigned with the same community IDs. Denote by the set just after the th iteration. Define the potential of our algorithm to be the size of  Since 0    it suffices to prove that the potential strictly increases after each iteration, i.e  2 Let be a vertex which changes its community ID in the th iteration. It follows that the number of edges in associated with is greater than degree and the number of edges in is less than degree Therefore changing of s community ID strictly increases the potential by at least one   002 We implement the algorithm by using Giraph. The pseudocode of our Compute\(\nction is shown in Algorithm 1. The checking of convergence criteria could be easily done by using the aggregator utility provided by Giraph. We are not going into the implementation details here Partitioning Compute\(messages if getSuperstep\(\PartX setValue\(MD5Checksum \(getNeighbors msg := \(getValue\(\etDegree elif getSuperstep maxDegree := -1 for msg in messages id, degree\sg if degree > maxDegree setValue\( id msg := getValue else empty map for msg in messages if msg is not in keys  msg  else  msg msg   1 for id in keys cnt   if id > getDegree\(\d id != getValue setValue\(id msg := getValue for in getNeighbors sendMessage msg voteToHalt After partitioning, there would be some missing information due to inter-partition edges. The goal of augmenting is to extend each pa rtition with the information of its adjacent inter-partition ed ges so that each augmented partition is self-contained for computing all quasi-bicliques for all of its part vertices. Note that if the partitioning algorithm really divides the graph into communities, there would not have too many edges between partitions and the partitions should not expand too much after augmenting. The formal definition of an augmented partition is given below         1 2   An example of an augmented partition is shown in Figure 7 The augmenting algorithm is implemented by using MapReduce, and details are shown in Algorithm 2 Augmenting Map\(keyIn, valueIn  keyIn: community ID of a vertex in part valueIn  neighbors of    neighbors  valueIn communityId := keyIn keyOut valueOut := \(communityId, neighbors output \(keyOut, valueOut for in neighbors keyOut valueOut := \(communityId  output \(keyOut, valueOut Reduce\(keyIn, valueIn empty set empty set  keyIn if is in part  communityId, neighbors\ := valueIn output \(communityId  neighbors else for \(communityId valueIn add to add communityId to for communityId in  keyOut := communityId valueOut   output \(keyOut valueOut 
     
1  for all 1 associated with 
014 014 
350 


Theorem 2 Algorithm 3 Algorithm 4 Input Output 
C Refining Given a partitioning result P X Y E  X  Y E X k Y k E k of a bipartite graph G X Y E let C i  X i Y i E i and S i v v X i for all i k We have that S S S k  v v X Proof X X X k X v v v X i i k v X i C i Y i v A Y C Y i A X C X v v A X C A Y C A C i v A X C A Y C v v v v v X Y E A  v v X v X y v y E y Y v Y x x v E v A G X A Y E B x x X B G X Y E S X x x S C x S X Y  x  Y C X Y C M y C x y x M M x M x M x x M M x C x C C C A Experiment Setting 
 
    
The refining step is to compute quasi-bicliques for the core vertices of each augmented partition This is achieved by first assigning each augmented partiti on to a reducer. And then each reducer runs a sequential algorithm to compute quasibicliques for augmented partitions assigned to it.  The following theorem ensures the correctness                quasi-biclique or all  1    quasi-biclique  or all  000 By definition we have so it suffices to prove that quasi-biclique  quasibiclique   for all in  1, 2   Let be a vertex in Since  by definition contains all the neighbors of Then by the definition of augmentation and which share at least one neighbor with Thus we have that quasi-biclique     induced graph and quasi-biclique   quasi-biclique  must be equal to quasi-biclique     002 The pseudo-code is shown in Algorithm 3. Sequential quasi-biclique mining algorithms have been extensively studied  and th e on e u s ed in ou r experi m e nt s is s h o w n in  Algorithm 4 for completeness Refining Map\(key, value  key: community ID of a vertex value neighbors of   output \(key,  value Reduce\(keyIn, valueIn empty set empty set empty set empty set for neighbors\ valueIn if is in part  add to for in neighbors add   add to else add to for in neighbors add   add to     compute quasi-biclique   using a sequential algorithm output Sequential Quasi-Biclique Mining  1\bipartite graph     2\real number 0, 1  3  quasi-bicliques    empty map Ans := empty set for in   empty set   neighbors of key := MD5 checksum of  key    for key in keys  empty map for in key s e con d   for in s neighbors if is in keys      1  else    for in keys if    key  s e con d    add to key f i rst for key in keys add key  output Ans IV E XPERIMENTS Our experiments are run on a Hadoop-0.20 cluster composed of 14 machines, each of which is equipped with one QuadCore Xeon E5520 CPU, 8GB RAM, six 300GB disks 
Figure 7. An example of an augmented partition 
012 011 010 010 013\013\013 010 012 011 010 010\013\013\013\010 012 012 012 006 010 012 006 010 012 012 010 012 011 012 011 006 012 011 012 013 
A C i G A i G i i G i i G i i A i G G G 
1 1 1  2 2 2      1 2 1 2 C        contains all the vertices in      Since     it follows that C   
 2                    
351 


X B Scalability Evaluation C Convergence Rate Evaluation D Graph Decomposition Evaluation 
   
012 012 
and the CentOS-5.3 OS. The Giraph algorithms are implemented with Giraph release 0.2.0 For partitioning, 15 workers \(mappers\sed to run Giraph. For augmenting and clu stering, 15 mappers and 15 reducers are used Datasets are extracted from web browsing logs provided by TrendMicro Research Lab Each log record contains the domain and IP of a visited webs ite. There would be an edge between a domain and an IP in the constructed bipartite domain-IP graph if and only if the domain and IP ever appear in the same log record Through all of the experiments, the following parameter settings are used. When running the partitioning algorithm, a vertex s community ID is changed only if greater than 66% of its neighbors have a community ID different than its current one and the convergence criterion is reached if more than 99.9999% of part vertices stop to change community IDs The threshold is set to 0.8 when computing quasibicliques The scalability of our algorithm is verified by gradually increasing the number of input vertices and observing the growth curves of CPU time and I/O workload Totally two hours of web browsing logs from 2013/01/15 00:00AM to 2013/01/15 2:00AM ar e used in this experiment By the results shown in Figure 8 and Figure 9, the CPU time and I/O workload of our algorithm grow linearly as the input vertices increase Figure 8. CPU time vs. number of vertices Figure 9. I/O workload vs. number of vertices Theoretically the partitioning algorithm may take a long time to reach the convergence cr iterion in the worst case when the input graph is large. However, in practice if the input graph has its underlying communi ties similar to bicliques the convergence rate is usually quite fast. Thus we would like to evaluate the convergence rate of our partitioning algorithm on real world domain-IP graphs In this experiment, we gradually increase the number of input vertices to our partitioning algorithm and observe how the convergence rate changes Totally two hours of web browsing logs from 2013/01/15 00:00AM to 2013/01/15 2:00AM are used in this experiment. By the results shown in Figure 10 it takes at most twelve iterations to reach the convergence criterion and the number of required iterations is independent of the input size Figure 11 shows that the time to reach the convergence criterion rises as we gradually en large the input graph. It is caused by the increase of messages produced in each iteration The message complexity can be reduced by some tricks if necessary. For example, each vertex can maintain the last values sent from its neighbors so that vertices whose values do not change don t have to send their valu es to neighbors again We didn t apply the trick to optimizing the message complexity because it usually takes a few minutes to converge and cannot be the performance bottleneck of our system Figure 10. number of required it erations vs. number of vertices Figure 11. converge time vs. number of vertices We decompose the graph into augmented partitions so that there are no dependencies between any two augmented partitions in successive mining of bi-cliques. Therefore, the 
352 


Algorithm 5 Algorithm 6 
E Performance Comparison y Y y x x X Y M y y Y x x M M x M x M x x M Y x X X Y x X x 
012 013 012 
                 
parallelism is dominated by the max part resulting from decomposition One of the typical graph decomposition methods is to compute connected components [7, 20 To ev alu ate ou r decomposition method, we compare the maximum augmented partition with the maximum connected component in the bipartite graph constructed by using web browsing logs from 2013/01/15 00:00AM to 2013/01/15 1:00AM. The result is shown in Figure 11, where the maximum augmented partition is smaller than one-tenth of the maximum connected components. It proves that our decomposition method is more effective and can lead to better parallelism Figure 11: size of the max part after decomposing 3.4 million of input vertices We compare the performance of our proposed algorithm GMR\ith the standard MapR educe algorithm \(SMR\MR consists of two MapRed uce jobs: Enumeration and Deduplication. The Enumeration job is to enumerate all quasibicliques, and the Deduplication job is to eliminate duplicate quasi-bicliques from the output of the Enumeration job. The details are described in Algorithm 5 and 6 Enumeration Map\(keyIn, valueIn  keyIn: a vertex in part valueIn: neighbors of  for in valueIn keyOut valueOut := \(keyIn, valueIn output \(keyOut, valueOut Reduce\(keyIn, valueIn empty set empty set empty map for neighobrs\ valueIn add to for in neighbors if is in keys      else    for cnt  if cnt   add to keyOut := keyIn vaueOut   output \(keyOut valueOut Deduplication Map\(keyIn, valueIn  keyIn: a vertex in part valueIn: the quasi-biclique for  keyOut := MD5 checksum of valueIn valueOut := valueIn output \(keyOut, valueOut Reduce\(keyIn, valueIn output valueIn The bipartite domain-IP graph is constructed by using web browsing logs from 2013/01/15 00:00AM to 2013/01/15 1:00AM. The resulting graph contains 3.4 millions of vertices among which 1.3 millions are domains and 2.1 millions are IPs The results are shown in Figure 12 and Figure 13 where GMR is able to reduce CPU time by 80% and I/O workload by 95%, compared with SMR Figure 12. CPU time for 3.4 millions of input vertices Figure 13. I/O workload for 3.4 millions of input vertices 
353 


Giraph: large-scale graph processing infrastructure on Hadoop in 4th Annual Hadoop Summit, 2011 3 J e f f r ey D e an a n d  S a nj ay G h e m a w at   MapReduce: simplified data processing on large clusters Communications of the ACM, vol. 51, no 1, pp. 107-113, 2008 4 Q i D i ng  N a tal l i a K a te n k a   P a ul Ba r f o r d  Er ic D  K o l aczy k and  Mar k  Crovella Intrusion as \(anti\social communication: characterization and detection in 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 886-894, 2012 5 L iY ung H o  J a nJ a n W u a n d P a ng f e ng L i u   graph database for large-scale social computing in 5th IEEE International Conference on Cloud Computing, pp. 455-462, 2012 6 L iY ung H o  J a nJ a n W u a n d P a ng f e ng  L i u  Optimal Algorithms for Cross-Rack Communication Optimization in MapReduce Framework in 4th IEEE International Conference on Cloud Computing pp. 420-427, 2011 7 L inh V u H o ng D N S T r af f ic A n al y s is f o r N e t w o r k bas e d Mal w ar e Detection. Stockholm: KTH Information and Communication Technology, 2012  N a n J i an g J i n Ca o Yu J i n  E r ra n L  L i a n d Z hi L i Z h a n g  Identifying suspicious activities through DNS failure graph analysis in 18th IEEE International Conference on Network Protocols pp. 144-153, 2010 9 H y e o n g S ik K i m   Pad m ash r e e Rav i n d ra an d  K e mafo r A n y a n w u   ScanSharing for Optimizing RDF Graph Pattern Matching on MapReduce in 5th IEEE International Conference on Cloud Computing, pp. 139-146 2012  J i n y a n L i  K e l v i n Si m G u i m e i L i u  a n d L i m s o on W o n gx  Maximal quasi-bicliques with balanced noise tolerance: concepts and coclustering applications in 8th SIAM International conference on Data Mining pp. 72-83, 2008 11 X i a o w n Li u  J i n y a n Li a n d Lu s h e n g W a n g   Quasi-bicliques complexity and binding pairs in 14th Annual International Computing and Combinatorics Conference pp. 255-264, 2008 12 Y u c h e n g L o w  J o s e ph G o nz al ez A a po K y r o l a, D a nny Bi c k s o n C a r l o s  Guestrin and Joseph M. Hellerstein Distributed GraphLab: a framework for machine learning and data mining in the cloud Proceedings of the VLDB Endowment, vol. 5, no. 8, pp. 716-727, 2012 13 Gr z e g o r z M a l e wi c z  M a t t h e w H  Au s t e r n  Aa r t J  C  B i k  Ja m e s  C  Dehnert, Ilan Horn, Naty Leiser and Grzegorz Czajkowski Pregel: a system for large-scale graph processing in 2010 ACM SIGMOD International Conference on Management of data, pp. 135-146, 2010 14 N ina  Mis h r a  D a na Ro n a n d Ram  S w am inat ha n  A new conceptual clustering framework Machine Learning vol. 56, no. 1-3, pp. 115-151 2004  C hun g-T s a i Su W en Kw a n g T s a o  W ei R on g Chu  a nd Mi n g Ra y  L i a o   Mining web browsing log by using relaxed biclique enumeration algorithm in mapreduce in 2012 International Workshop on Behavior Informatics, 2012 16 Jime n g S u n  H u im i n g Q u  D e e p ay an  Ch ak ra b a rti a n d  C h risto s F a l o u t so s Neighborhood formation and anomaly detection in bipartite graphs in 5 th IEEE International Conference on Data Mining pp. 418-425, 2005  Fen ggu a n g T i an an d K e k e C h en   Towards Optimal Resource Provisioning for Running MapReduce Programs in Public Clouds in  4th IEEE International Conference on Cloud Computing, pp. 155-162 2011 18 L e s l ie G   V a l ian t A bridging model for parallel computation Communications of the ACM, vol 33, issue 8, pp. 103-111, 1990  C a i r o n g Yan   X i n Yan g   Z e Yu  M i n L i and  Xia o li n L i  IncMR Incremental Data Processing Based on MapReduce 5th IEEE International Conference on Cloud Computing, pp. 534-541, 2012 20 San d e e p Y a d a v  A s h w ath K u mar K r ish n a  Re d d y  A   L  N a rasimh a  Reddy and Supranamaya Ranjan Detecting algorithmically generated malicious domain names in 10th annual conference on Internet measurement pp. 48-61, 2010 21 Y u n Z h an g  El issa J C h e s l e r M i ch ae l A   L a n g sto n   On Finding Bicliques in Bipartite Graphs: a Novel Algorithm with Application to the Integration of Diverse Biological Data Types in 41st Annual Hawaii International Conference on System Sciences pp. 473, 2008 
I C ONCLUSIONS AND F UTRUE W ORK Quasi-biclique mining plays an important role in providing security services. In this work, we propose a fast algorithm for quasi-biclique mining in web-scale graphs. The main idea is to partition the graph into communities by using an iterative Giraph program so that each community is smaller enough to be processed by a single reducer in parallel. We also prove the convergence of our partitioning algorithm Our ongoing research includes extending the partitioning algorithm for more general bipartite graphs. Our partitioning algorithm is based on the assumption that the input graph is composed of biclique-like communities. The assumption is generally true for domain-IP graphs but may fail to hold for other kinds of graphs like website-client graphs, machine-file graphs and customer-product graphs. Another interesting research direction is to inves tigate the possibility of using GraphLab [9 stribu ted  g r ap h datab ase s [5 to fu rt her  improve the performance A CKNOWLEDGMENT We would like to thank the support from Trend Micro SPN Team and many valuable feedbacks from Yun-Chian Cheng Jon Oliver, Chris Huang and Eugene Koontz R EFERENCES  C ha ru C  A gga r w a l a n d Hai xun W a n g  M a n a g i n g a nd Min i n g G r a p h Data. New York: Springer, 2010 2 A ve ry Ching   
      Distributed                           in     
354 


i.e space-ìlling curves Z-order Hilbert cf cf cf i.e cf 
002\003\004 005\006\004 007 010\003\007\003  011\012\013 014\012 002\003\004 005\006\004 007 010\003\007\003 011\012\013 014\012 
p p  p p p p k 
002 
partitions This partitioning function should yield equally-sized partitions and preserve at the same time data locality   points that are close in the spatial domain should be assigned to the same partition In order to satisfy these constraints the partitioning function has to map multidimensional datapoints into an ordered sequence of unidimensional values In practice this transformation is performed with the help of that precisely map multidimensional data to one dimension while preserving data locality In our R-Tree construction we implemented and tested two types of space-ìlling curves the curve and the curve The MapReduce design for this phase consists of several mappers and one reducer Each mapper Algorithm 6 Appendix samples a predeìned number of objects from its data chunk and outputs the corresponding single-dimensional values obtained after applying the space-ìlling curve Afterwards the reducer Algorithm 7 Appendix collects a set of single dimensional values from all mappers orders this set and then determines partitioning points in the sequence partitioning points delimit the boundaries of each partition The second phase concurrently builds individual R-Trees by indexing the partitions outputted by the rst phase The mappers Algorithm 8 Appendix split the dataset into partitions using the space-ìlling curve computed during the rst phase Each mapper processes its chunk and assigns each object it reads to a partition identiìer The intermediate key is represented by the partition identiìer such that all datapoints sharing the same key   belonging to the same partition will be collected by the same reducer Then each reducer Algorithm 9 Appendix constructs the R-Tree associated with its partition leading to a total of reducers building small R-Trees Finally the last phase merges the small R-Trees into a global one indexing all datapoints of the initial dataset VIII C ONCLUSION In this paper we have proposed to adopt the MapReduce paradigm in order to be able to perform a privacy analysis on large scale geolocated datasets composed of millions of mobility traces More precisely we have developed a complete MapReduce-based approach to GEPETO for GEoPrivacyEnhancing TOolkit a softw are that can be used to design tune experiment and evaluate various sanitization algorithms and inference attacks on location data as well as to visualize the resulting data Most of the algorithms used to conduct an inference attack represent good candidates to be abstracted in the MapReduce formalism For instance we have designed MapReduced versions of sampling as well as the means and the DJ-Cluster clustering algorithms and integrate them within the framework of GEPETO These algorithms have been implemented with Hadoop and evaluated on a real dataset Preliminary results show that the MapReduced versions of the algorithms can efìciently handle millions of mobility traces Currently the clustering algorithms that we have implemented can be used primarily to extract the POIs of an individual from his trail of mobility traces which correspond only to one possible type of inference attack In the future we aim at integrating other inference techniques within the MapReduced framework of GEPETO In particular we want to develop algorithms for learning a mobility model out of the mobility traces of an individual such as Mobility Markov Chains MMCs In a nutshell a MMC represents in a compact way the mobility behavior of an individual and can be used to predict his future locations or even to perform deanonymization attacks thus extending the range of inference attacks available within GEPETO We also want to design MapReduced versions of geo-sanitization mechanisms such as geographical masks that modify the spatial coordinate of a mobility trace by adding some random noise or aggregate several mobility traces into a single spatial coordinate More sophisticated geo-sanitization methods will also be integrated at a later stage such as spatial cloaking techniques and mix zones A CKNOWLEDGMENT This work was funded by the location privacy activity of EIT ICT labs Experiments presented in this paper were carried out using the Gridê5000 experimental testbed being developed under the INRIA ALADDIN development action with support from CNRS RENATER and several Universities as well as other funding bodies see https://www.grid5000.fr R EFERENCES  The Apache Hadoop Project http://www hadoop.or g  The Apache Mahout Frame w ork http://mahout.apache.or g  The Hadoop MapReduce Frame w ork http://hadoop.apache.or g mapreduce  L O Alv ares V  Bogorn y  B K uijpers J A F  de Mac 032 edo B Moelans and A A Vaisman A model for enriching trajectories with semantic geographical information In 
002\003\004\005\006\007\010\007\011\003\012\013\003\014\013\005\010\015\007\011\007\011\003\012\011\012\016\013\014\006\012\017\007\011\003\012\013 013 002\003\012\020\007\015\006\017\007\011\003\012\013\003\014\013\020\004\010\021\021\013\022\023\024\015\025\025\020 013 
002\003\004\005\006\007\010 011 002\003\004\005\006\007\012 002\003\004\005\006\007\013    011 005\014\004\015\015\007\016\017\020\021\006\006\005 005\014\004\015\015\007\016\017\020\021\006\006\005 022\015\023\024\004\015\007\016\017\020\021\006\006  
Proceedings of the 15th ACM International Symposium on Geographic Information Systems Proceedings of the Workshops of the 2nd IEEE Conference on Pervasive Computing and Communications Proceedings of the 6th IEEE/ACM International Workshop on Grid Computing 
010 010 010 010 
 1 
002 
002\003\004\005\006\007\010\005\011\002\003\012\002\013\012 005\014\015\012\016\017\002\020\021\017 022\023\024\006\015\015\012 
Fig 6 Building an R-Tree with MapReduce of the  page 22 2007  A R Beresford and F  Stajano Mix zones User pri v ac y in locationaware services In  pages 127 131 2004  F  Cappello E Caron M Dayde F  Desprez E Jeannot Y  Je gou S Lanteri J Leduc N Melab G Mornet R Namyst P Primet and O Richard Gridê5000 A large scale reconìgurable controlable and monitorable grid platform In  pages 99Ö106 Seattle Washington USA November 2005 
1945 


size 
sample scalar sample const scalar values step total samples R count i values i step count values i count count partitions scalar value i R partitions i  scalar  partitions i partitionId i partitionId value partitionId key tree values tree treeRoot 
      0 mod 0    1   0      1       
 A Cary  Z Sun V  Hristidis and N Rishe Experiences on processing spatial data with mapreduce In  pages 302Ö319 Berlin Heidelberg 2009 Springer-Verlag  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters  51\(1 2008  R M Este v es R P ais and C Rong K-means clustering in the cloud  a mahout test In  WAINA 11 pages 514Ö519 Washington DC USA 2011 IEEE Computer Society  S Gambs M.-O Killijian and M N del Prado GEPET O a GEoPri v ac y Enhancing Toolkit In  pages 1071Ö1076 April 2010  S Gambs M.-O Killijian and M N del Prado Cortez Sho w me ho w you move and I will tell you who you are  4\(2 2011  P  Golle and K P artridge On the anon ymity of home/w ork location pairs  pages 390Ö397 May 2009  M C Gonzalez C A Hidalgo and A.-L Barabasi Understanding individual human mobility patterns  453\(7196 June 2008  M Gruteser and D Grunw ald Anon ymous usage of location-based services through spatial and temporal cloaking In  2003  A Guttman R-trees A dynamic inde x structure for spatial searching In B Yormark editor  pages 47Ö57 ACM Press 1984  L Jedrzejczyk B A Price A K Bandara and B Nuseibeh I kno w what you did last summer Risks of location data leakage in mobile and social computing  November 2009  Y  J  egou S Lant  eri J Leduc M N G Mornet R Namyst P Primet B Quetier O Richard E Talbi and T Ir  ea Gridê5000 a large scale and highly reconìgurable experimental grid testbed  20\(4 November 2006  J H Kang B Ste w arta G Borriello and W  W elbourne Extracting places from traces of locations In  pages 110Ö118 2004  J Krumm Inference attacks on location tracks  pages 127Ö143 2007  J K La wder and P  J H King Using space-ìlling curv es for multidimensional indexing In  pages 20Ö35 London UK 2000 SpringerVerlag  J B MacQueen Some methods for classiìcation and analysis of multivariate observations In L M L Cam and J Neyman editors  volume 1 pages 281Ö297 University of California Press 1967  K Shv achk o H K uang S Radia and R Chansler  The Hadoop Distributed File System In  2010  R W  Sinnott V irtues of the ha v ersine In  volume 68 page 159 1984  C Song Z Qu N Blumm and A.-L Barabasi Limits of predictability in human mobility  327\(5968 2010  S Spaccapietra C P arent M L Damiani J A F  de Mac 032 edo F Porto and C Vangenot A conceptual view on trajectories  65\(1 2008  W  Zhao H Ma and Q He P arallel k-means clustering based on mapreduce In  CloudCom 09 pages 674Ö679 Berlin Heidelberg 2009 Springer-Verlag  Y  Zheng Q Li Y  Chen X Xie and W Y  Ma Understanding mobility based on gps data In  UbiComp 08 pages 312Ö321 New York NY USA 2008 ACM  Y  Zheng X Xie and W Y  Ma Geolife A collaborati v e social networking service among user location and trajectory  33\(2 2010  Y  Zheng L Zhang X Xie and W Y  Ma Mining interesting locations and travel sequences from gps trajectories In  WWW 09 pages 791 800 New York NY USA 2009 ACM  C Zhou D Frank o wski P  Ludford S Shekhar  and L T erv een Discovering personal gazetteers An interactive clustering approach In  pages 266Ö273 ACM Press 2004 A PPENDIX 
space lling curve order space lling curve build RTree 
Algorithm 6 Algorithm 7 Algorithm 8 break Algorithm 9 
R-Tree First Phase Mapper randomly sample objects in chunk R-Tree First Phase Reducer R-Tree Second Phase Mapper load ouput of rst phase R-Tree Second Phase Reducer 
map\(K key V value emitIntermediate reduce\(K key v alues for if emit setup\(Conìguration conf map\(K key V value for if emitIntermediate reduce\(K key v alues emit 
002 002 002  002 003 002 002 002 003 002 002 002 
Proceedings of the 21st International Conference on Scientiìc and Statistical Database Management Communications of the ACM Proceedings of the 2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications Proceedings of the International Workshop on Advances in Mobile Computing and Applications Security Privacy and Trust held in conjunction with the 24th IEEE AINA conference Perth Australia Transactions on Data Privacy Proceedings of the 7th International Conference on Pervasive Computing Nature Proceedings of the 1st International Conference on Mobile Systems Applications and Services Proceedings of Annual Meeting of ACM SIGMOD Department of Computing Faculty of Mathematics Computing and Technology The Open University International Journal of High Performance Computing Applications Proceedings of the 2nd ACM international workshop on Wireless mobile applications and services on WLAN hotspots Pervasive Computing Proceedings of the 17th British National Conference on Databases Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies Sky and Telescope Science Data Knowl Eng Proceedings of the 1st International Conference on Cloud Computing Proceedings of the 10th International Conference on Ubiquitous Computing IIEEE Data\(base Engineering Bulletin Proceedings of the 18th International Conference on World Wide Web Proceedings of the 12th ACM International Workshop on Geographic Information Systems 
1946 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





