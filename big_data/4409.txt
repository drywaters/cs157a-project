Measures of Class Membership in Association Rule based Classi\002cation Viet Phan-Luong Laboratoire d'Informatique Fondamentale de Marseille UMR CNRS 6166 CMI de Universit 264 e Aix-Marseille 39 rue F Joliot Curie 13453 Marseille France viet.phanluong@lif.univ-mrs.fr Abstract In this work we focus on the measures of class membership de\002ned for classi\002ers based on associative classi\002cation rules We revisit the 037 2 test for de\002ning an effective measure For comparison we adapt the weight of evidence 050Wang and Wong TKDE 2003\051 for a system based on the notions of support and con\002dence Some variants of those measures are also de\002ned The effect of the de\002ned measures is veri\002ed through the experimentation on categorical UCI datasets 1 Introduction The association rule based classi\002cation has received extensive attention in data mining and machine learning research 2 3 4 5 6 7 The approach consists of two main steps 050i\051 Extraction of a set of class-association rules called a classi\002er from the learning data and 050ii\051 Selecting signi\002cant rules in this set for classifying unseen data In general the high quality rules are de\002ned on the notions of support and con\002dence The support of a rule shows how frequent the rule classi\002es correctly the learning data and the con\002dence represents the precision of the rule Further heuristics to enhance the quality of selected rules based on statistical tools such as the chi-square test the correlation coef\002cient ha v e been proposed Instead of basing on support and con\002dence another approach to select interesting rules is to base on residual analysis in statistics In this approach a pattern is considered signi\002cant if its frequency of occurrence is signi\002cantly different from its expected occurrences using a test based on the notion of standardized residual For classifying an unseen object the effect of each class label on the object is considered in a similar manner using a measure called the weight of evidence This is a competitive measure in classi\002cation In this work we focus in the measures to quantify the effect of a class label on an unseen object using the rules in the classi\002er built on small size key itemsets We revisit the 037 2 test to de\002ne an effective measure We adapt the weight of evidence in the context of classi\002cation based on the notions of support and con\002dence To improve those measures some variants are de\002ned The effect of those measures and of the variants is veri\002ed and compared with the classical measure de\002ned on the notion of con\002dence through the experimentation on categorical UCI datasets 2 Preliminaries 2.1 Dataset and Association Rule A dataset is a set of objects 050or transactions\051 Each object is represented by an identi\002er and a list of valued attributes each valued attribute is called an item Let I be the set of all items of a dataset D  A subset of I is called an itemset An itemset consisting of k items is called a k itemset  Class labels are special items for de\002ning the class of objects The support of an itemset I with respect to a dataset D  denoted by sup 050 I 051  is the number of the objects in D that have all the items of I  In practice one is interested in itemsets with support larger than some threshold minsup  These itemsets are called frequent itemsets An association rule 050AR\051 is an expression of the form X  Y  where X and Y are itemsets and X 134 Y    2.2 Class-association Rule A class-association rule 050CAR\051 is an expression of the form X  C  where X is an itemset and C a class label Let r be a CAR X  C  An object O is covered by r 050or O satis\002es r 051 if O has all the items of X  An object O is correctly classi\002ed by r if O satis\002es r and C is actually the class label of O  The support of r with respect to a dataset D  denoted by sup 050 r 051  is the number of the objects in D that are correctly classi\002ed by r  The con\002dence of r is de\002ned by conf 050 r 051  sup 050 r 051 sup 050 X 051  A rule with con\002dence 1 is called an exact rule  Example 1 For the training dataset represented in Table 1 a  C is a CAR with support 2 and con\002dence 0  67  because sup 050 aC 051  2 and sup 050 a 051  3  Over CARs the precedence order  is a partial order de\002ned as follows Given two CARs r and r 0  r 026 r 0 050 r precedes r 0 051 if 226 conf 050 r 0 051  conf 050 r 051  or 
2009 International Conference on Advanced Information Networking and Applications Workshops 978-0-7695-3639-2/09 $25.00 © 2009 IEEE DOI 10.1109/WAINA.2009.81 685 


Table 1 A training dataset   Oid  Itemset  Class label    1  acd  C   2  abe  C   3  abd  C   226 conf 050 r 051  conf 050 r 0 051 and sup 050 r 0 051  sup 050 r 051  or 226 conf 050 r 051  conf 050 r 0 051 and sup 050 r 051  sup 050 r 0 051 and the number of items in LHS 050 r 051 050the left-hand side of r 051 is less than the number of items in LHS 050 r 0 051  An itemset I is called a key itemset 050or a minimal generator 051 if 8 I 0 022 I sup 050 I 0 051  sup 050 I 051 implies I  I 0  An association rule X  Y  with X and Y being key itemsets represents an equivalent class of rules with respect to the con\002dence and support Based on this property classi\002ers can be built with only key itemsets 3 Related Work Using an Apriori-like algorithm CBA  e xtracts class association rules 050CARs\051 and sorts them in the precedence order In this order CARs are selected for building the classi\002er A CAR is selected if it classi\002es correctly at least a training object Once a CAR is selected all the objects that are covered by the rule are discarded from the selection process As a consequence each training object is covered by at most one CAR of the built classi\002er CMAR is de v eloped on the ideas of CB A b ut it uses FP-growth for computing CARs In contrast to CBA 050i\051 CMAR selects only positively correlated CARs for the classi\002er using the 037 2 test and 050ii\051 CMAR allows each training object to be covered by several CARs HARMONY uses the same strate gy as FP-gro wth for computing CARs An important difference from CBA and CMAR is that during the CAR mining process HARMONY maintains for each training object a topk list of the highest con\002dence rules mined so far 050 k 025 1 051 that classify correctly the object At the end of the process those rules are grouped by class label to form the classi\002er For classifying an object O  CBA searches the classi\002er in the precedence order for the 002rst rule that covers the object to predict the class label If such a rule does not exist CBA predicts the majority class label CMAR and HARMONY follow the vote model For each class label HARMONY computes the score of the topk highest con\002dence rules that cover O  It predicts the class label with the highest score The sum of the con\002dences is a score CMAR de\002nes a measure called the weighted 037 2 as the score In HPWR signi\002cant patterns 050itemsets\051 is de\002ned on the deviation between the observed and expected frequency of occurrences For building classi\002ers classi\002cation rules are measured in a similar manner For a rule r  P  C  using the conditional probability HPWR de\002nes the weight of evidence of P in favor of C by W 050 C  C j P 051  log P r 050 C j P 051  P r 050 C 051 000 log P r 050  C j P 051  P r 050  C 051 0501\051  log P r 050 P j C 051  P r 050 P j C 051 0502\051 The weight of evidence is positive if P provides positive evidence supporting C  otherwise it is negative or zero For classifying an object O  the rules that cover O are grouped by class label The sum of the weights of evidence is computed for each group The object is classi\002ed with the class label that has the largest sum 4 Class Membership Measures 4.1 Con\002dence The con\002dence of rules is a natural measure of class membership Each rule that covers an object represents a conditional probability space the object can go into Using the con\002dence measure there are two main strategies for classifying an unseen object CBA looks for the 002rst rule of the classi\002er in the precedence order that covers the object to predict the class of the object If such a rule does not exist CBA predicts the majority class With such a simple strategy of classi\002cation CBA is a very good classi\002er HARMONY is also a very competitive classi\002er with the classi\002cation based on the con\002dence of rules For classifying an object the best strategy of HARMONY is to compute for each class label the sum of con\002dences of all the rules of the classi\002er that cover the object 050 006 conf 050 r i 051 051 The class label that has the largest sum is the predicted label 4.2 C-coef\002cient In CMAR only positively correlated rules are selected for building classi\002ers Those rules are determined by the 037 2 test Given a rule r  P  C  let 017 T be the number of the objects in the dataset 017 p c and pc be respectively the number of the objects in the dataset that are covered by r  the number of the objects in the dataset that have the class label C  and the number of the objects in the dataset that are correctly classi\002ed by r  The 037 2 value of r is de\002ned as follows Let o 1  pc o 2  p 000 o 1  o 3  c 000 o 1  o 4  T 000 c 000 o 2 0503\051 n 1  p 002 c  T  n 2  p 002 050 T 000 c 051  T  0504\051 n 3  050 T 000 p 051 002 c  T  n 4  050 T 000 p 051 002 050 T 000 c 051  T 0505\051 
686 


037 2  050 o 1 000 n 1 051 2  n 1  050 o 2 000 n 2 051 2  n 2  050 o 3 000 n 3 051 2  n 3  050 o 4 000 n 4 051 2  n 4 0506\051 The authors of remark ed that in the v ote model the application of Equation 6 to de\002ne the effect of a class label does not yield a good result because Equation 6 may be favorable to minority classes They proposed another sophisticated measure that integrates both information of correlation and popularity called the weighted 037 2  This measure yields the best results in the experimentation of CMAR However the authors also remarked that it is hard to verify theoretically the soundness or effect of this measure We have another observation Through the expressions in Equations 3 4 5 we can show that 050 o 1 000 n 1 051 2  050 o 2 000 n 2 051 2  050 o 3 000 n 3 051 2  050 o 4 000 n 4 051 2 0507\051 for any rule built from the dataset and o 1  o 2  o 3  o 4  n 1  n 2  n 3  n 4  T 0508\051 That is for any rule P  C  the numerators of the fractions in Equation 6 are identical and the sum of the denominators is constant and equal to T  Hence 037 2  050 o 1 000 n 1 051 2 050 1  n 1  1  n 2  1  n 3  1  n 4 051 0509\051 If o 1  n 1  then from equation 7 o 1  n 1  o 2  n 2  o 3  n 3 and o 4  n 4  and 037 2 is minimal 037 2  0  We have the same result if o 2  n 2 or o 3  n 3 or o 4  n 4  Conversely if 037 2 is minimal i.e 037 2  0  then o 1  n 1  o 2  n 2  o 3  n 3 and o 4  n 4  Otherwise if the classi\002cation by r  P  C is well distinct from the classi\002cation by the equi-probable distribution either much better or much worse then 037 2 tends to the maximal value In the context of this work where the classi\002er is built with the rules that classify correctly the training objects with the highest con\002dence it is the case where the classi\002cation by P  C is much better than the classi\002cation by the equi-probable distribution Hence 037 2 has the similar property as the information gain in sense of the weight of evidence of HPWR However the experimentation shows that 037 2 is not competitive measure In addition by Equation 9 for a non-zero value of o 1 000 n 1  050i.e o 1 6  n 1 051 if one among n i  i  1  4  tends to zero then 037 2 tends to the maximal value This is the case of minority classes To improve it 002rstly we delete the last two terms of 037 2  because they concern only the objects that are not covered by r  Let 037 0 2  050 o 1 000 n 1 051 2  n 1  050 o 2 000 n 2 051 2  n 2 05010\051 037 0 2 preserves the above property of 037 2  Secondly in order to deal appropriately with minority classes we shift Equation 10 to the scale of the dataset to de\002ne a measure called C-favor  to estimate the favor of P to a class C  We denote o 0 1  conf 050 r 051 002 T o 0 2  T 000 o 0 1  05011\051 037 r  r  050 o 0 1 000 c 051 2  c  050 o 0 2 000 050 T 000 c 051\051 2  T 000 c 05012\051 037 r still has the similar property as the information gain and can be used to measure the class membership Let O be an object represented by an itemset X  Let the rules with class label C that cover O be r i  P i  C  i  1   k  We de\002ne the C favor of X by F C 050 X 051  006 i 1 k 037 r i 05013\051 To classify O  we compute F C 050 X 051 for each class label C  and O is predicted in the class that has the largest F C 050 X 051  As the con\002dence is an important measure we think that the combination of con\002dence and F C can be a better measure The combined measure denoted by cF C 050 X 051  is de\002ned as follows For each class label C  cF C 050 X 051  F C 050 X 051 002 006 i 1 k conf 050 r i 051 05014\051 The class label that corresponds to the largest value cF C 050 X 051 is used to predict O  To verify the effect of F C and cF C  we shall compare them with a measure that we get by adapting the weight of evidence 050HPWR in the context of the systems based on the notions of support and con\002dence 4.3 Weight of evidence For a rule r  P  C  HPWR de\002nes a measure of evidence provided by P in favor of C as the difference in gain of information when predicting C on P instead of predicting some other class label Remind that in Section 3 this measures is de\002ned by Equation 2 HPWR applies this measure in the context of signi\002cant association patterns de\002ned by standardized residual In the context of frequent itemsets de\002ned on the notions of support and con\002dence the con\002dence of r  conf 050 r 051  sup 050 r 051 sup 050 P 051  corresponds to the conditional probability P r 050 C j P 051  Moreover we have P r 050 P j C 051  P r 050 C j P 051 002 P r 050 P 051  P r 050 C 051 05015\051 Hence P r 050 P j C 051  P 050 P j C 051  conf 050 r 051 002 0501 000 rsup 050 C 051\051  0501 000 conf 050 r 051\051 002 rsup 050 C 051 05016\051 where rsup 050 C 051  sup 050 C 051 T  the relative support of C with respect to the number of the objects of the dataset Let r i  X i  C  i  1   k be the rules that cover an object O represented by an itemset X  Then the adapted weight of evidence of X in favor of C is E C 050 X 051  log conf 050 r 1 051 002 0501 000 rsup 050 C 051\051  0501 000 conf 050 r 1 051\051 002 rsup 050 C 051  
687 


  log conf 050 r k 051 002 0501 000 rsup 050 C 051\051  0501 000 conf 050 r k 051\051 002 rsup 050 C 051 05017\051 In this work we omit the condition X i 134 X j    8 i 6  j 1 024 i j 024 k  required in because checking this condition is a cost operation and also because when the omission is applied to all class labels each one can share almost the same effect For classifying O  the adapted weight of evidence of X is computed in favor of each class label The object O is predicted in the class that corresponds to the highest weight of evidence 4.4 Variants It is easy to see that when conf 050 r 051  1 Equation 16 becomes in\002nite as well as W 050 C  C j P 051  In such a case from Equation 17 the class label of any exact rule can be selected for the prediction without consideration of the information gain Under this observation we de\002ne a variant of the weight of evidence as follows The expression log conf 050 r 051 002 0501 000 rsup 050 C 051\051  0501 000 conf 050 r 051\051 002 rsup 050 C 051 05018\051 is modi\002ed into log 0501  T:conf 050 r 051  c 051 000 log 0501  T 0501 000 conf 050 r 051\051  1 000 c 051 05019\051 Let vE C 050 X 051 denote the variant of the weight of evidence as de\002ned in Equation 17 but use Equation 19 in the place of Equation 18 This variant preserves the property of the weight of evidence and allows to compare the gain of information of exact rules Finally we consider the combination of the measures E C 050 X 051 and vE C 050 X 051 with the measure by the sum of con\002dences to try to improve them These combined measures are denoted respectively cE C 050 X 051 and cvE C 050 X 051  cE C 050 X 051  E C 050 X 051 002 006 i 1 k conf 050 r i 051 05020\051 cvE C 050 X 051  vE C 050 X 051 002 006 i 1 k conf 050 r i 051 05021\051 5 Implementation We apply those measures of class membership to a method for building classi\002ers to classify unseen objects This method builds classi\002ers on small key itemsets using a pre\002x tree structure for extracting class association rules It has the following particularities 226 It can adapt the level-wise computation of Apriori but it is different from Apriori by enumerating the itemsets and counting their occurrences in the pre\002x tree it combines the two phases generating candidates and computing their support into one phase 226 It builds the classi\002ers with the highest con\002dence rules that classify correctly each training object as in HARMONY In contrast to HARMONY it postpones the search for those rules until the end of the class association rule extraction process 226 Another contrast to HARMONY is that we do not consider the itemsets of all sizes but we limit the maximal size of key itemsets to 5 Under this limit we can explore key itemsets with low supports In fact we apply the support constraint using minsup  only to i itemsets with i 024 2  This constraint is not applied to itemsets with size  2  excepting those with support 1  Though itemsets with very small supports can be considered only rules with maximal con\002dences and supports that classify correctly each training object are added to the classi\002er 6 Experimental Results Table 2 The 23 UCI datasets   13 small datasets    Dataset  obj  item  class    anneal  798  106  5   auto  205  142  7   breast  699  48  2   glass  214  52  7   heart  303  53  5   hepatitis  155  58  2   horseColic  368  94  2   ionosphere  351  104  2   iris  150  23  3   pimaIndians  768  42  2   ticTacToe  958  29  2   wine  178  68  3   zoo  101  43  7    10 large datasets    adult  48482  131  2   chess  28056  66  18   connect  67557  66  3   led7  3200  24  10   letRecog  20000  106  26   mushroom  8124  127  2   nursery  12960  32  5   pageBlocks  5473  55  5   penDigits  10992  90  10   waveform  5000  100  3   The program for verifying the effect of the class membership measures is implemented in C and experimented on a laptop with a Pentium 4 1.7 GHz mobile processor and 768 MB memory running Linux 9.1 Let us call SIM the implemented program For comparison we evaluate the measures on the same 23 UCI categorical datasets 05013 small and 10 large datasets\051 obtained from the author of using the 10-fold cross validation Table 2 represents the characteristics of these 23 UCI datasets To have an idea on the effect of the measures we use the experimental results of HARMONY as the reference The reasons are 
688 


Table 3 Experimentation on 13 small datasets     HAR    SIM    Dataset   006 conf   006 conf   F C  cF C  E C  cE C  vE C  cvE C   Sz    anneal   94.04   94.27   91.35  92.58  95.17  94.94  92.02  93.37   4   auto   72.00   76.50   78.50  78.00  77.00  77.00  78.50  78.00   3   breast   91.18   90.14   92.03  92.03  93.19  91.88  92.17  93.19   3   glass   68.58   71.90   70.48  71.90  71.43  71.90  71.43  71.43   5   heart   56.33   59.00   56.67  58.67  58.67  58.67  58.00  58.67   5   hepatitis   82.01   82.67   81.33  82.00  84.67  85.33  82.67  80.67   3   horseColic   81.68   83.89   83.61  83.89  83.61  83.89  83.89  83.89   4   ionosphere   89.15   90.00   90.86  90.29  90.00  90.00  90.29  90.29   4   iris   93.98   94.00   94.00  94.00  94.00  94.00  94.00  94.00   3   pimaIndians   69.22   69.08   73.42  73.95  73.29  73.82  73.42  73.42   3   ticTacToe   96.42   97.89   98.11  98.21  97.89  97.89  98.21  98.63   4   wine   90.57   91.76   91.18  91.18  91.76  91.76  91.18  91.18   4   zoo   89.00   92.00   93.00  93.00  92.00  92.00  93.00  92.00   3    Total/Avg   82.63   84.08   84.20  84.59  84.82  84.85  84.52  84.52     Table 4 Experimentation on 10 large datasets     HAR    SIM    Dataset   006 conf   006 conf   F C  cF C  E C  cE C  vE C  cvE C   Sz    adult   83.40   84.23   82.02  83.73  83.41  84.29  82.61  83.78   3   chess   44.93   60.57   61.58  61.79  60.80  60.56  61.70  61.76   5   connect   77.30   77.67   78.24  78.88  79.72  79.21  78.88  79.45   4   led7   74.35   74.37   74.19  74.22  74.22  74.31  74.42  74.44   5   letRecog   70.82   71.29   71.54  71.47  70.82  71.25  69.74  72.34   4   mushroom   100   100   100  100  100  100  100  100   3   nursery   92.94   98.33   96.71  97.85  98.77  98.66  97.21  97.85   5   pageBlocks   91.17   90.93   91.52  92.08  91.74  92.08  91.52  91.90   4   penDigits   96.03   97.05   97.02  97.02  97.02  97.04  97.02  97.02   4   waveform   77.92   79.86   79.78  79.82  75.72  76.24  79.78  79.78   3    Total/Avg   80.89   83.45   83.26  83.69  83.22  83.36  83.27  83.83     050i\051 HARMONY is a good classi\002cation system its performance is well compared with the important systems such as FOIL CP AR 13 and SVM 14 050ii\051 SIM shares with HARMONY on many points in the method for building classi\002ers and is also different from HARMONY on many other points 050iii\051 Thanks to the authors of HARMONY we get the runnable program of HARMONY and can use it to experiment HARMONY and SIM on the same UCI datasets and on the same computer For the experiments conducted in this work the parameter setting for HARMONY is done as described in for the 13 small datasets minsup  10  and for the 10 large datasets minsup  50  items are sorted in the correlation coef\002cient ascending order 050the order with which HARMONY gets the best results in general\051 In particular for the dense datasets connect and ionosphere  only the items with supports no greater than 20  000 and 190 respectively are considered to generate class association rules The same consideration is applied to SIM Tables 3 and 4 represent the experimental results of the classi\002cation accuracy of HARMONY 050abbreviated by HAR.\051 and SIM using the measures 006 conf  F C  cF C  the adapted weight of evidences 050 E C 051 and its variants 050 cE C  vE C  cvE C 051 In these tables the column Sz represents the maximal size of itemsets extracted by SIM  7 Discussions and Conclusion Notice that the results represented in column HAR 050HARMONY\051 of Tables 3 and 4 are different and better than what was reported in The dif ferences can be due to the method for dividing data randomly in the 10-fold cross validation On the results of the experiments conducted in this work Tables 3 and 4 show that 226 For the same measure 006 conf  on average the classi\002cation by SIM is about 1  45 and 2  56 more accurate than that by HARMONY for the 13 small and 10 large datasets respectively The good accuracy of SIM can be explained by the selection of CARs having the maximal con\002dence and support among those having small supports These rules may be very speci\002c However as they are built on the small size key itemsets they are in general not speci\002c 226 F C 050the measure based on 037 2 revisited\051 and E C 050the adapted weight of evidence\051 are comparable to 006 conf  
689 


Moreover we can say that F C is comparable to E C and the modi\002ed version of E C i.e vE C  is correct and effective  The combinations of F C  E C  and vE C with 006 conf improve slightly F C  E C  vE C  and 006 conf  We can say that the sum of con\002dences is an important measure and these combined measures are useful in particular in the context where the accuracies of the classi\002cation by the sum of con\002dence by F C  and by E C are almost very good For conclusions 002rstly the adapted weight of evidence is a good class membership measure built on the gain of information F C  a measure built on the revisited 037 2 test provides another view of information gain It is comparable to the adapted weight of evidence The sum of con\002dence is a simple and natural measure with the good performance The combinations of the sum of con\002dence with the previous measures are interesting and useful to improve their performance Next based on the average accuracy values of different measures we recommend to use the combined measures because the average accuracy values of the combined measures are in general better than that of the non-combined measures Finally through the results on each dataset between the combined measures based on 037 2 and the weight of evidence we suggest the following propositions 017 For the 13 small datasets though the average accuracy value of cE C is slightly better than that of cF C  we can observe that cF C is much often wins cE C  Indeed cE C wins cF C on only 3 datasets while cF C wins cE C on 6 datasets Hence we can recommend cF C for the small datasets 017 For the 10 large datasets the average accuracy value of cvE C is slightly better than that of cF C  and cvE C wins cF C on 4 datasets and cF C wins cvE C 3 datasets Hence we can recommend cvE C for large datasets References  B Lent A Sw ami and J W idom Clustering association rules Proc Intl Conf on Data Engineering ICDE'97 IEEE Computer Society 1997 pp 220-231  W  Li J Han and J Pei CMAR Accurate and Ef 002cient Classi\002cation based on multiple class-association rules Proc IEEE Intl Conf on Data Mining ICDM'01 San Jose CA IEEE Computer Society 2001 pp 369-376  B Liu W  Hsu and Y  Ma Inte grating Classi\002cation and Association Rule Mining Proc 4th Intl Conf on Knowledge Discovery and Data Mining KDD'98 AAAI Press 1998 pp 80-86  Y  Sun Y  W ang and A.K.C W ong Boosting an Association Classi\002er in IEEE Transactions on Knowledge and Data Engineering vol 18 no 7 IEEE Computer Society 2006 pp 988-992  J W ang and G Karypis HARMONY  Ef 002ciently Mining the Best Rules for Classi\002cation Proc SIAM Intl Conf on Data Mining SDM'05 2005 pp 205-216  J W ang and G Karypis On Mining Instance-Centric Classi\002cation Rules in IEEE Transactions on Knowledge and Data Engineering vol 18 no 11 2006 pp 1497-1511  Y  W ang and A.K.C W ong From Association to Classi\002cation Inference using Weight of Evidence in IEEE Transactions on Knowledge and Data Engineering vol 15 no 3 2003 pp 764-767  F  Coenen The LUCS-KDD Implementations of the FOIL PRM and CPAR algorithms http://www.csc.liv.ac.uk frans/KDD/Software/FOIL PRM CPAR/foilPrmCpar.html Computer Science Department University of Liverpool UK 2004  Y  Basti de R T aouil N P asquier  G Stumme and L Lakhal Mining Frequent Patterns with Counting Inferences in ACM SIGMOD Explorations vol 2 no 2 2000 pp 66-75  R Agra w al and R Srikant F ast algorithms for mining association rules Proc 20th Intl Conf on Very Large Databases VLDB'94 Santiago Chile 1994 pp 487-499  V  Phan-Luong and R Messouci Building Classi\002ers with Association Rules based on Small Key Itemsets Proc 2nd IEEE International Conf on Digital Information Management ICDIM'07 France 2007 pp 200-205  J Quinlan and R Cameron-Jones FOIL A Midterm Report Proc European Conf on Machine Learning ECML'93 1993 pp 3-20  X Y in and J Han CP AR Classi\002cation based on Predicti v e Association Rules Proc 3rd SIAM Intl Conf on Data Mining SDM'03 San Francisco CA SIAM 2003 pp 369-376  C Cortes and V  V apnik Support-V ector Netw orks  in Machine Learning vol 20 no 3 1995 pp 273-297 
690 


Figure 3 Precision-recall curve in ROI-250 image dataset 5.3 Experiment 3 Mammograms-1080 image dataset This experiment employed a dataset composed by 1080 mammograms images collected in the Clinical Hospital of University of Sao Paulo at Ribeiro Preto The dataset was previously classi\002ed into 4 levels of breast tissue density 0501\051 mostly fatty 050362 images\051 0502\051 partly fatty 050446 images\051 0503\051 partly dense 050200 images\051 and 0504\051 mostly dense 05072 images\051 Figure 4 Precision-recall curve in Mammography-1080 image dataset Breast density is an important risk factor in the development of breast cancer In this experiment the images are represented by the feature set proposed in b uilding a vector of 85 features including shape and size of the breast the conditions of the breast contour nipple position and the distribution of 002broglandular tissue This dataset was divided in training set and test set The training set is composed of 720 images and test set is composed of 360 images Figure 4 shows the P&R curves over test dataset and also the number of features selected in each method Again the proposed methods reached the highest values of precision and select the smallest number of features 050a\051 050b\051 050c\051 050d\051 Figure 5 Queries in Mammography dataset 050a\051 226 is the query image 050b\051 using the features selected through 002tness function F cB  050c\051 using the features selected through classi\002cation error of C4.5 and 050d\051 using the all features extracted Results for the retrieval of the 5 most similar images from a query image are also provided in this experiment as illustrated in Figure 5 The image 5.\050a\051 is the query image taken from the mostly fatty image class Images shown in 5.\050b\051 are the 5 most similar images retrieved for the proposed 002tness function F cB  The row 050c\051 shows the results for C 4  5 classi\002er whereas 050d\051 illustrate the images resulting from all features 050no selection applied\051 In Figure 5 the images surrounded by dashed lines are false positives 050not relevant images\051 For this query the proposed method achieved the highest precision 050100%\051 when compared the results of C4.5 and the original feature vector 050precision of 40%\051 


6 Conclusions This work proposed a novel genetic feature selection framework for CBIRs It employs a wrapper strategy that searches for the best reduced feature set while optimizing 050or preserving\051 the quality of the solution From a ranking evaluation function three new 002tness functions namely F cA  F cB and F c have been proposed and evaluated in three experiments The proposed genetic feature selection approach which encompasses F cA  F cB and F c  has been compared with 050a\051 traditional methods found in the literature 050b\051 the StARMiner feature selector and 050c\051 the whole feature vector and signi\002cantly outperformed them The proposed approach has been able to optimize the accuracy of similarity queries while selecting a signi\002catively reduced number of features Additionally the proposal of combining the quality of the query results with the criterion of minimizing the number of selected features F cA and F cB  led to high accurate query answers while reducing the number of features more than the 002tness function F c  Therefore the 002nal processing cost of the queries is also reduced References  P  M d Aze v edo-Marques N A Rosa A J M T raina C Traina-Jr S K Kinoshita and R M Rangayyan Reducing the semantic gap in content-based image retrieval in mammography with relevance feedback and inclusion of expert knowledge International Journal of Computer Assisted Radiology and Surgery  3\0501-2\051:123\226130 June 2008  R Baeza-Y ates and B Ribeiro-Neto Modern Information Retrieval  Addison-Wesley Essex UK 1999  B Bartell G Cottrell and R Bele w  Optimizing similar ity using multi-query relevance Journal of the American Society for Information Science  49:742\226761 1998  O Cord 264 on E Herrera-Viedma C L 264 opez-Puljalte M Luque and C Zarco A review on the application of evolutionary computation to information retrieval International Journal of Approximate Reasoning  34:241\226264 July 2003  J G Dy  C E Brodle y  A Kak L S Broderick and A M Aisen Unsupervised feature selection applied to content-based retrieval of lung images IEEE Transactions on Pattern Analysis and Machine Intelligence  25\0503\051:373\226 378 March 2003  W  F an E A F ox P  P athak and H W u The ef fects of 002tness functions on genetic programming-based ranking discovery for web search Journal of the American Society for Information Science and Technology  55\0507\051:628\226636 2004  W  F an P  P athak and M Zhou Genet ic-based approaches in ranking function discovery and optimization in information retrieval a framework Decision Support Systems  2009  D E Golber g Genetic algorithms in search optimization and machine learning  Addison Wesley 1989  R L Haupt and S E Haupt Practical Genetic Algorithms  John Wiley  Sons New Jersey United States second edition edition 2004  J Horng and C Y eh Applying genetic algorit hms to query optimization in document retrieval Information Processing  Management  36:737\226759 2000  S K Kinoshita P  M d Aze v edo-Ma rques R R PereiraJr J A H Rodrigues and R M Rangayyan Contentbased retrieval of mammograms using visual features related to breast density patterns Journal of Digital Imaging  20\0502\051:172\226190 June 2007  F  K orn B P agel and C F aloutsos On the  dimensionality curse and the self-similarity blessing IEEE Trans on Knowledge and Data Engineering  13\0501\051:96\226111 2001  H Liu and L Y u T o w ard inte grating feature select ion algorithms for classi\002cation and clustering IEEE Transactions on Knowledge and Data Enginnering  17\0504\051:491\226502 April 2005  M X Ribeiro A J M T raina C T raina-Jr  and P  M Azevedo-Marques An association rule-based method to support medical image diagnosis with ef\002ciency IEEE Transactions on Multimedia  10\0502\051:277\226285 2008  U S Cancer Statistics W orking Group United states cancer statistics 1999-2005 incidence and mortality webbased report atlanta 050ga\051 Department of health and human services centers for disease control and prevention and national cancer institute 2009 Available in http://apps.nccd.cdc.gov/uscs   L T amine C C and M Boughanem Multiple query evaluation based on an enhanced geneticnext term algorithm Information Processing  Management  39\0502\051:215\226 231 2003  R S T orres A X  F alc 230 ao M A Gonc\270alves J P Papa Z B W Fan and E A Fox A genetic programming framework for content-based image retrieval Journal of the American Society for Information Science and Technology  42\0502\051:283\226292 2009  A Tsymbal P  Cunningham M P echenizkiy  and S Puuronen Search strategies for ensemble feature selection in medical diagnostics In Proceedings of the 16th IEEE Symposium on Computer-Based Medical Systems  pages 124\226 129 June 2003  A Tsymbal M Pechenizkiy  and P  Cunningham Sequential genetic search for ensemble feature selection In Proceedings of the International Joint Conferences on Arti\002cial Intelligence  pages 877\226882 August 2005  C.-M W ang a and Y F  Huang Ev olutionary-based feature selection approaches with new criteria for data mining A case study of credit approval data Expert Systems with Applications  36\0503 Part 2\051:5900\2265908 2009  H Y an J Zheng Y  Jiang C Peng and S Xiao Selecting critical clinical features for heart diseases diagnosis with a real-coded genetic algorithm Applied Soft Computing  8:1105\2261111 2008  T  Zhao J Lu Y  Zhang and Q Xiao Feature selection based on genetic algorithm for cbir In IEEE Congress on Image and Signal Processing  volume 2 pages 495\226499 2008 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnêt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnêt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnêt create sound clips  A5.1 We donêt need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIês   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





