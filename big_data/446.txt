New Parallel Algorithms for Frequent Itemset Mining in Very Large Databases Adriano Veloso and Wagner Meira Jr Computer Science Dept Universidade Federal de Minas Gerais  adrianov meira  dcc.ufmg.br Srinivasan Parthasarathy Computer and Information Science Dept The Ohio-State University srini@cis.ohio-state.edu Abstract Frequent itemset mining is a classic problem in data mining It is a non-supervised process which concerns in nding frequent patterns or itemsets hidden in large volumes of data in order to produce compact summaries or models of the database These models are typically used to generate association rules but recently they have also 
been used in far reaching domains like e-commerce and bio-informatics Because databases are increasing in terms of both dimension number of attributes and size number of records one of the main issues in a frequent itemset mining algorithm is the ability to analyze very large databases Sequential algorithms do not have this ability especially in terms of run-time performance for such very large databases Therefore we must rely on high performance parallel and distributed computing We present new parallel algorithms for frequent itemset mining Their ef“ciency is proven through a series of experiments on different parallel environments that range from shared-memory mul 
tiprocessors machines to a set of SMP clusters connected together through a high speed network We also brie”y discuss an application of our algorithms to the analysis of large databases collected by a Brazilian web portal 1 Introduction Our ability to collect and store data is fair outpacing our ability to analyze it This phenomenon is mainly explained by the gap between advances in computing power and storage capacity technologies Moores law states that computing power doubles approximately every 18 months while the corresponding law for advances in storage capacity technology is even more impressive  storage capacity doubles approximately every 9 months 5 As a c ons e 
quence the value of data is no longer in how much of it you have or neither in how fast can you gather it but in how quickly and how effectively can the data be reduced and explored Thus implementation of non-trivial data mining ideas in high performance parallel and distributed computing environments is becoming crucial Clearly such parallel computing environments greatly increase our ability to analyze data since the computing power is increased However these environments also pose several challenges to data mining such as high communication and synchronization overhead data skewness and workload balancing In this paper we present new parallel algorithms for a key data mining task the discovery of frequent itemsets 
This task has a simple problem statement to nd the set of all subsets of items or attributes that frequently occur together in database transactions or records Although the frequent itemset mining task has a simple statement it is both CPU and I/O intensive mostly because of the high dimension and large size of the databases involved in the process Several ef“cient sequential algorithms were proposed in the literature 2 12 9 Th ere a re also so me p a rallel alg o rithms 1 7 6 13 10 Ho we v e r  th ese alg o r ith m s n eed se v eral rounds of communication incurring in serious synchronization and I/O overhead Our new algorithms need only one round of communication while searching for the fre 
quent itemsets and one reduction operation to generate the correct model Further they can make use of both sharedand distributed-memory advantages and deal with both high dimension and large size problems We evaluated our algorithms through a broad series of experiments using different parallel environments 2 De“nitions and Preliminaries D EFINITION 1  I TEMSETS  For any set   its size is the number of elements in  Let  denote the set of  natural numbers 
 1 2      Each   is called an item A non-empty subset of  is called an itemset The power set of   denoted by      is the set of all possible subsets of   An itemset of size                 is called a  itemset for convenience we drop set notation and denote 
 as        For        we say that  contains  if    A set of itemsets      is called an itemset collection and  is a    collection if no Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


itemset in it contains another      and     implies     D EFINITION 2  T RANSACTIONS  A transaction   is an itemset where  is a natural number called the transaction identi“er or   A transaction database               is a nite set of transactions with size      The absolute support of an itemset  in  is the number of transactions in  that contain   given as              012     The relative support of an itemset  in  is the fraction of transactions in  that contain   given as                 D EFINITION 3  F REQUENT AND M AXIMAL F REQUENT I TEMSETS  An itemset  is frequent if           where   is a user-speci“ed minimum-support threshold with 0    1 A collection of frequent itemsets is denoted as 015        A frequent itemset  015       is 012 if it has no frequent superset A collection of maximal frequent itemsets is denoted as 015        015       is a  015  collection on   L EMMA 1 2 A n y s ubs et of a frequent i t e ms et i s frequent    015       and  012  implies   015        Thus by de“nition a frequent itemset must be subset of at least one maximal frequent itemset  L EMMA 2 12 015       is the smallest collection of itemsets from which 015       can be inferred  P ROBLEM 1  M INING F REQUENT I TEMSETS  Given   and a transaction database   the problem of mining frequent itemsets is to nd 015        E XAMPLE 1 Let us consider the example in Figure 1 where    1,2,3,4,5  and the gure shows  and      Suppose    0.4 40 015       is composed by the shaded and bold itemsets while 015       is composed only by the bold itemsets Note that 015        is much smaller than 015         A naive approach to nd 015       is to rst compute       for each       and then return only those that        0.4 This approach is inappropriate because  If   the dimension is high then      is huge i.e      If   the size is large then computing       for all      is infeasible By applying lemma 1 we can greatly improve the search for 015        In Figure 1 once we know that the itemset  34  is not frequent we do not need to generate any of its supersets since they must also be not frequent This simple pruning trick was rst used in 2 a nd i t great l y reduces the number of candidate itemsets generated Several different searches can be applied by using this pruning trick Figure 1 Frequent Itemset Mining Example Basic Algorithm Our algorithm employs a backtrack search to nd 015       and 015       Asolution is represented as an itemset    012  012     Each item 012  is chosen from a nite possible set    Initially  is empty it is extended one item at a time as the search proceeds The size of  is the same as the depth of the corresponding itemset in the search tree Given a  candidate itemset     012  012  012      the possible values for the next item 012  comes from a subset   012  called the combine set  Each iteration tries extending   with every item 012 in    An extension is valid if the resulting itemset    is frequent If    is frequent and it is no subset of any already known maximal frequent itemset then    is a maximal frequent itemset The next step is to extract the new possible set of extensions     which consists only of elements in   that follow 012  The new combine set    consists of those elements in the possible set that produce a frequent itemset when used to extend     Any item not in the combine set refers to a pruned subtree The algorithm performs a depth-“rst traversal of the search space When the search nishes 015       and 015       were found The computation of       isbasedontheassociativity of subsets Let      be the  of  in  the set of  in  in which  has occurred and thus                According to 12       can be obtained by intersecting the tidsets of two subsets of   For example in Figure 1      1 5 9 10  and   012   1 4 5 8 9 10   Consequently   012       012   1 5 9 10     012    012     4 and thus  012    0.4 E XAMPLE 2 Figure 2 shows how the algorithm works We used sequence numbers above each itemset to facilitate the understanding of the backtrack search First the algorithm process the items and then it starts a depth-“rst search for frequent itemsets For example for sequence number   13 the itemset being processed is  124   Therefore the depth is  3,and        5  When   15 the algorithm visits the itemset  124  again but now     and Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


consequently  124          Although an itemset  can be visited more than once       is computed only in the rst visit Figure 2 Basic Algorithm 3 New Parallel Algorithms In this section we propose several different parallel algorithms First we present the design of our parallel approaches and then we show how to ef“ciently implement these approaches using different parallel environments In our parallel and distributed scenario  is divided into  partitions        Each partition   is assigned to a node    A given node   is composed by one or more processor units one memory unit and one database   A set of nodes is called a cluster  Our scenario is composed by a set of inter-connected clusters 3.1 Algorithm Design 3.1.1 The Data Distribution Approach In this approach all nodes generate the same se t of candidate itemsets and each node   can thus independently get       for all candidates   A naive method to get       would be to perform a communication round and synchronization between all nodes for each itemset  generated Then we could easily check if           This method is obviously inappropriate since it performs several rounds of communication Another method would be each node   generating all    012  within its partition   andthen only one sum-reduction operation between all nodes is necessary to get          012   This approach is also inappropriate since processing all    012  is infeasible We overcame all these problems and developed an approach which needs only one round of communication and prunes the search space for frequent itemsets i.e our approach processes much fewer itemsets than   012    We rst present the basic theoretical foundation of our method L EMMA 3 For a given itemset  if          then 015              P ROOF If                 then         since              L EMMA 4            is an upper bound for         and therefore it determines all itemsets  such that           P ROOF If          then  must be frequent in at least one partition of  If  is frequent in some partition          then it can be inferred by         and consequently by              The Data Distribution approach has four distinct phases P HASE 1  F IRST A SYNCHRONOUS P HASE  Each node   reads its local partition    and constructs the tidsets for all items in    Then each node performs our basic algorithm on its partition At the end of the search each node   will know both        and         P HASE 2  C OMMUNICATION P HASE  Each node   has to exchange        with every other node so that at the end of the communication operation each node can compute             Since only                 is exchanged the communication overhead is minimized note that by lemma 2        is the smallest frequent itemset collection from which        can be inferred P HASE 3  S ECOND A SYNCHRONOUS P HASE  Now that all nodes know             by lemma 4 they can nd         without further synchronization Now each node performs a top-down enumeration of itemsets as follows Each itemset present in            is broken into  subsets of size      The support of this itemset can be computed by intersecting the tidsets of each item present in this itemset This process iterates generating smaller subsets and computing their supports until there are no more subsets to be checked At the end of this phase each node will have the same set of candidate itemsets since the enumeration is based on the same upper bound P HASE 4  R EDUCTION P HASE  After all nodes nish the top-down enumeration a sum-reduction operation is performed to nd       for each candidate itemset  i.e                   Nextstepistoremovethose  with         andthen        was found E XAMPLE 3 Figure 3 shows an example of the Data Distribution approach The database  in Figure 1 was divided into two partitions   and    In the rst phase   and   independently apply the basic algorithm on   and    respectively After both nodes nish the rst phase   will know        and         while   will know        and         In the second phase they exchange their maximal frequent collections so that both nodes will know             Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


 1235 1245   In the third phase each node performs the top-down enumeration   process itemsets  1245 124 145 245   while   does not process any further itemset since                     In the fourth phase   and   exchange       and       of each itemset   so that they know all  with           For instance      1      3,and         0.4 Figure 3 Data Distribution Example 3.1.2 The Candidate Distribution Approach The basic idea of this approach is to separate the candidates in disjoint sets Each set is assigned to a node so that it can asynchronously process its tasks There are four distinct phases P HASE 1  F IRST A SYNCHRONOUS P HASE  Each node   reads its local partition    and constructs the tidsets for all items in    P HASE 2  C OMMUNICATION AND A SSIGNMENT P HASE  Each node has to exchange the local tidsets with every other node After each node knows the tidsets of the items in all partitions we start the assignment of the tasks to the nodes Note that each item corresponds to a different backtrack tree which corresponds to a disjoint set of candidate itemsets The way the backtrack trees are assigned to different nodes depends on the implementation we will discuss more about this in the next section After the assignment there is no dependence among the nodes P HASE 3  S ECOND A SYNCHRONOUS P HASE  Each node proceeds the search for frequent itemsets within the backtrack trees assigned to it using our basic algorithm Since the dependence among the processors was decoupled in the previous phase there is no need for further synchronization and the costly communication of tidsets in the previous phase is amortized in this phase P HASE 4  R EDUCTION P HASE  Finally a join-reduction operation is performed and        was found E XAMPLE 4 Figure 4 shows an example of the Candidate Distribution approach In the rst phase   reads   and constructs                    and     At the same time   reads   and constructs                    and      In the second phase both nodes exchange its local tidsets and then the backtrack tree for item  1  is assigned to    while backtrack trees for items  2,3,4  are asigned to    In the third phase each node applies the basic algorithm on the backtrack trees assigned to it i.e   has processed the white and bold itemsets while   has processed the shaded itemsets Figure 4 Candidate Distribution Example 3.2 Algorithm Implementation Two dominant paradigms for using multiple processors have emerged distributedand shared-memory The performance-optimization objectives for distributedmemory implementations are different from those of shared-memory implementations In the distributedmemory paradigm synchronization is implicit in communication so the goal becomes communication optimization In the shared-memory paradigm synchronization stems from locks and barriers and the goal is to minimize these points A third very popular paradigm combines the best of these two paradigms Clusters of SMPs are part of this third paradigm The physical memory is distributed among the nodes but each processor within each node has free access to the entire memory of the node Clusters of SMPs necessitate a hierarchical parallelism implementation with shared-memory primitives used in a node and message passing used among the nodes A fourth paradigm has also emerged which allows a number of clusters to be connected together through a high speed network to work like a single super-computer 3.2.1 The Shared-Memory Implementation Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


Cache Locality False Sharing and Synchronization Our algorithms avoid the use of complex structures which may have poor locality In fact they use simple intersection operations to determine the frequent itemsets This feature enables the algorithms to have good cache locality Also a problem unique to shared-memory systems is false sharing which occurs when two different shared variables are located in the same cache block causing the block to be exchanged between the processors even though the processors are accessing different variables A simple solution would be to place unrelated data that might be accessed simultaneously on separate cache lines While this simple approach will eliminate false sharing it will result in unacceptable memory space overhead and most importantly a signi“cant loss in cache locality 8 An o t h e r t ech n i q u e fo r e limin atin g false sharing is called Privatization 3 It in v o lv es making a private copy of the data that will be used locally so that operations on that data do not cause false sharing This technique allows us to keep a local array of frequent itemsets per processor Each processor can use its local array during the search for frequent itemsets Privatization eliminates false sharing completely and there is no need of any kind of synchronization among the processors We implemented the Candidate Distribution approach using this technique As explained earlier the main idea is to assign distinct backtrack trees to distinct processors Each backtrack tree corresponds to a disjoint set of itemsets and by using the privatization techni que there is no dependence among the processors The backtrack trees are assigned to the processors in a bag of tasks approach that is given a bag of backtrack trees to be processed each processor takes one tree and as soon as it nishes the search for frequent itemsets on this tree it takes another tree from the bag When the bag is empty we have found         We observed that the amount of work associated with a given backtrack tree is generally proportional to the support of the generating item or root We sorted the backtrack trees in the bag in descending order of support of their roots so that bigger tasks will be processed earlier 3.2.2 The Distributed-Memory Implementation We implemented both Data and Candidate Distribution approaches using the distributed-memory paradigm While data distribution deals with the large size problem candidate distribution deals with the high dimension problem The distributed-memory implementation of the Candidate Distribution approach is similar to the shared-memory implementation that is different backtrack trees are assigned to different nodes Next we will describe how the Data Distribution approach was implemented Reducing the Large Size Problem One step in the Data Distribution approach deserves special attention the topdown enumeration It is very important to implement this step in an ef“cient way otherwise one node can replicate work computing       for the same itemset  more than once As mentioned before we can do this by simply intersecting the tidsets of all items that compose  However this approach is not ef“cient because it would need    intersections if the size of  is   To solve this problem we store the intermediate tidsets in a hash-table whose key is the itemset For example suppose we must nd       We rst perform the operation               and then              and            After     is processed it is stored in the hash-table so that we do not need to process     again 3.2.3 The Hierarchical Implementation Using both Sharedand Distributed-Memory We implemented a hierarchical version of the Candidate Distribution approach where different backtrack trees are assigned to different nodes and inside each node its backtrack trees are assigned to its processors This approach greatly reduces the amount of communication performed We also implemented a hybrid approach which combines data distribution among the nodes and candidate distribution inside each node This hybrid approach reduces data skewness since there are fewer partitions although bigger to be processed 3.2.4 The Massively Parallel Implementation Balancing Communication and Data Skewness Averyimportant issue is how to reduce communication and data skewness Unfortunately we cannot reduce both of them at the same time if communication is reduced data skewness is increased and vice-versa Therefore we must rely on how to balance these two metrics This is especially important in massively parallel environments because if we perform only candidate distribution then the amount of communication will be too large Otherwise if we perform only data distribution then there will be a large number of partitions and data skewness will be too high What is needed is a way to use both Data and Candidate Distribution approaches We implemented this hybrid approach in such a way that data is distributed among the clusters and candidates are distributed among the nodes within each cluster This choice reduces the communication among the clusters Note that Data Distribution approach needs much less communication than Candidate Distribution approach since only            is transfered among the clusters Further the number of partitions will be always the same as the number of clusters involved in the mining process which is much smaller than the number of processors reducing data skewness 4 Experimental Evaluation Our experimental evaluation was carried out on two 8 node P ENTIUM processor clusters In one of the clusters all Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


nodes have two processors Each node has 1GB of main memory and 120GB of disk space Writes have a latency of 1.2  secs with transfer bandwidth of 100MB/sec All nodes inside a cluster are interconnected through a highspeed network the   The clusters are interconnected through an optic-“ber We have implemented the parallel algorithms using the MPI message-passing library MPICH and POSIX P THREADS  We used a real database for evaluating the performance of our algorithms The WP ORTAL database is generated from the click-stream log of a large Brazilian web portal We scanned the log and produced a transaction le i.e   where each transaction is a session of access to the site by a client Each item in the transaction is a web request but not all web requests were turned into items to become an item the request must have three properties 1 the request method is GET 2 the request status is OK and 3 the le type is HTML A session starts with a request that satis“es the above properties and ends when there has been no click from the client for 30 minutes WP ORTAL has 3,183 items comprised in 7,786,137 transactions In each experiment WP ORTAL was divided into  partitions where  depends on the number of processors or clusters employed Our evaluation is based on two basic parameters number of processors and minimum support Thus for each minimum support employed we performed multiple executions of the algorithms where each execution employs a different number of processors Further we employed four different metrics in our evaluation Execution Time It is the total elapsed time spent for mining   Timings are based on wall clock time Communication It is the total amount of bytes transfered among the nodes during the mining operation Parallel Ef“ciency It is given by          where  is the execution time obtained when using  processors and   is the execution time obtained when using    processors For instance if   100 secs and   50 secs then the parallel ef“ciency is 1 100 Data Skewness This metric quanti“es how even or uneven the frequent itemsets are distributed among the partitions Intuitivelly if the frequent itemsets are evenly distributed among the partitions then         012   i.e the upper bound is close to     012    We used the well established notion of entropy 4 t o de v e l o p a s u i t a bl e m eas ure for data skewness For a random variable the entropy is a measure of the non-uniformity of its probability distribution Suppose              In this case the value                   can be regarded as the probability of occurrence of  in    We rst de“ne the skewness of an itemset  as              where              015        Then the data skewness metric can be de“ned as a weighted sum of the skewness of all itemsets that is                 where                          Please refer to 4 for fur ther explanations and insights about this metric In the rst series of experiments we used the 8 node dual processor cluster In all graphs Data Distribution Distributed refers to the distributed-memory implementation of Data Distribution approach   is divided into  partitions where  is the number of processors Candidate Distribution Distributed refers to the distributed-memory implementation of Candidate Distribution approach   partitions Data Distribution Hierarchical refers to the hierarchical implementation of the Data Distribution approach    partitions since there are 2 processors per node and Candidate Distribution Hierarchical refers to the hierarchical implementation of the Candidate Distribution approach    partitions The rst experiment we conducted was to empirically verify the advantages of our parallel algorithms in terms of execution times We varied the number of processors 1 to 16 and the minimum support the number of frequent itemsets generated varied from 91,874 for        to 283,186 for        and compared the execution times for each parallel algorithm Figure 5 shows the execution times obtained for different parallel con“gurations In all cases the distributed implementations are slightly better when the number of processors is less than 8 If more processors are used the hierarchical implementations become better since communication in the Candidate Distribution approach and data skewness in the Data Distribution approach become too high in the pure distributed implementations Further the ef“ciency increases when we reduce the minimum support since the asynchronous phase becomes more relevant 10  The Data Distribution approaches seem to be the best ones Figure 6 shows the results obtained in a similar experiment but now the appraised metric is the amount of communication performed by each algorithm The Candidate Distribution approaches require approximatelly 3-4 orders of magnitude more communication than the Data Distribution approaches Further the hierarchical implementations save communication requirements by a factor of 5 In the next experiment we investigated how data skewness increases as we vary the number of processors and consequently partitions using the Data Distribution approach note that the Candidate Distribution approaches do not generate data skewness si nce the data is communicated among the processors As showed in Figure 7 data skewness is higher for smaller minimum supports Also as expected data skewness is higher for the pure distributed implementations since there are two times more partitions in this case In the last experiment of this series we invesProceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


tigated how data skewness affects parallel ef“ciency Figure 8 shows that the effect of data skewness are worse for the pure distributed implementations Although data skewness increases as we reduce the minimum support parallel ef“ciency increases for lower minimum support values since the asynchronous phase becomes more relevant  0  50  100  150  200  250  300  2  4  6  8  10  12  14  16   Execution time \(secs  Number of processors  WPortal - 0.01   Data Distribution \(Distributed   Candidate Distribution \(Distributed    Data Distribution \(Hierarchical    Candidate Distribution \(Hierarchical               0  50  100  150  200  250  300  350  400  450  500  2  4  6  8  10  12  14  16   Execution time \(secs  Number of processors  WPortal - 0.005   Data Distribution \(Distributed   Candidate Distribution \(Distributed    Data Distribution \(Hierarchical    Candidate Distribution \(Hierarchical              Figure 5 Total Execution Times as a function of number of processors SMP cluster The other series of experiments employed the two clusters but to make equal the computational power of both clusters we used only 4 dual nodes of the rst cluster Our objective is to evaluate our algorithms in terms of execution times and speedup We varied the number of processors and the minimum support value and in each parallel con“guration employed each cluster used the same number of processors i.e 2 processors mean that cluster 1 and cluster 2 used 1 processor Figure 9 shows the execution times obtained For    0.01 we observed a speedup number of 11/16 while for    0.005 the speedup number reached almost 14/16 These numbers shows that combining both Data and Distribution approaches in the inter-cluster parallel environment can be an ef“cient approach since it reduces communication among the clusters and reduces data skewness since there are only two logical partitions  1000  10000  100000  1e+06  1e+07  1e+08  1e+09  2  4  6  8  10  12  14  16   Communication \(bytes  Number of processors  WPortal - 0.01   Data Distribution \(Distributed   Candidate Distribution \(Distributed    Data Distribution \(Hierarchical    Candidate Distribution \(Hierarchical          Figure 6 Communication as a function of number of processors  0  0.05  0.1  0.15  0.2  0.25  0.3  2  4  6  8  10  12  14  16   Data Skewness  Number of processors  WPortal - Data Distribution   Distributed \(0.01   Hierarchical \(0.01    Distributed \(0.005    Hierarchical \(0.005            Figure 7 Data Skewness as a function of number of processors 5 Related Work Several algorithms for mining frequent itemsets were proposed in the literature 2 12 11 A PRIORI 2 w as the rst ef“cient algorithm and it forms the core of almost all current algorithms During the rst pass over  the support of each item is counted The frequent items are used to generated candidate 2-itemsets  is scanned again to obtain the support of all candidate 2-itemsets and the frequent ones are selected to generate candidate 3-itemsets This iterative process is repeated for   3,4 until there are no more frequent  itemsets to be found Clearly A PRI ORI needs  passes over   incurring in high I/O overhead In the parallel case A PRIORI based algorithms do a reduction operation at the end of each pass thus incurring also in high synchronization cost Three different parallel implementations of the A PRIORI algorithm on IBMSP2 a distributed-memory machine were presented in 1 In 10 t he aut hors s h o w ed t h e i mpact i n s ynchroni zat i o n Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


 0.75  0.8  0.85  0.9  0.95  1  0  0.05  0.1  0.15  0.2  0.25   Parallel Efficiency  Data Skewness  WPortal - Data Distribution   Distributed \(0.01   Hierarchical \(0.01    Distributed \(0.005    Hierarchical \(0.005            Figure 8 Parallel Ef“ciency as a function of Data Skewness  0  50  100  150  200  250  300  350  400  450  500  2  4  6  8  10  12  14  16   Execution time \(secs  Number of processors  WPortal   0.01   0.005   Figure 9 Total Execution Times as a function of number of processors 2 clusters overhead due to ne-grained parallel approaches The parallel algorithms NPA SPA HPA and HPA-ELD proposed in 7 a r e sim ilar to th o s e i n  1   H P A E L D is th e b est among NPA SPA and HPA because it reduces communication by replicating candidates with high support on all processors E CLAT  an algorithm presented in 12 n eed s only two passes over  and decomposes the search-space for frequent itemsets in disjoint sub-spaces In 13 s e v eral E CLAT based parallel algorithms were presented Our basic algorithm need only one access to   In the parallel case our algorithms need only one communication round Also complementar experiments show that our basic algorithm generates a smaller number of candidates than both A PRIORI andE CLAT based algorithms 6 Conclusions The huge size of the available databases and their high dimension make data mining applications very computationally demanding to an extent that high performance parallel computing is fast becoming an essential component of the solution In fact data mining applications are poised to become the dominant consumers of supercomputing and high performance systems in the near future There is a practical necessity to develop effective and ef“cient parallel algorithms for data mining In this paper we presented several parallel algorithms for frequent itemset mining a fundamental data mining task Key issues such as load balancing communication reduction attention to data skewness improving cache locality and reducing false sharing were all addressed Our parallel algorithms need only one access to the disk-resident database and are based on a novel and ef“cient backtrack algorithm which was also presented in this paper Also our algorithms are the rst ones able to deal with both large size and high dimension problems The algorithms were evaluated on different parallel environments and showed to be very ef“cient We also presented a possible application mining large web logs to better understand web patterns We intend to continue our work by distributing the databases in a widely distributed scenario and developing proper algorithms to deal with the challenges imposed by this scenario References 1 R  A g r a w al an d J  S h a fer  P a rallel m in in g o f a sso ciatio n rules Transactions on Knowledge and Data Engineering  8\(6\:962…969 1996  R  A gra w al and R  S r i kant  F ast al gori t h ms for m i n i n g a ssociation rules In Proc 20  Int Conf Very Large Databases VLDB  pages 487…499 Morgan Kaufmann Dec 1994  R Bi anchi ni and T  L eBl anc S oft w are cachi ng on cachecoherent multiprocessors In Proc Symp on Parallel and Distributed Processing SPDP  pages 521…526 IEEE May 1992 4 D  W  L  C h eung and Y  Xi ao E f f ect of dat a d i s t r i b ut i o n in parallel mining of associations Data Mining and Knowledge Discovery  3\(3\:291…314 1999 5 J  G r a y a nd P  S h er no y  R u l e s o f t humb i n dat a engi neer i ng In Proc Int Conf on Data Engineering ICDE  pages 3…12 IEEE Ma y 2000  E  H Han G Karypi s and V  K umar  S cal abl e paral l e l d at a mining for association rules Transactions on Knowledge and Data Engineering  12\(3\:728…737 2000  M  J oshi  E  H Han G Karypi s and V  K umar  E f  ci ent par allel algorithms for mining associations Parallel and Distributed Systems  1759:418…429 2000  S  Part hasarat hy  M  Z aki  M  Ogi hara and W  L i  Paral lel data mining for association rules on shared-memory systems Knowledge and Information Systems  3\(1\:1…29 2001  A V e l oso W  Mei ra and M Bunt e Real w o rl d a ssoci at i o n mining Advances in Databases LNCS  2405:73…77 2002 Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


 A V e l o so W  Mei r a M B unt e S  Par t hasar at hy  and M Zaki Parallel incrementa l and interactive frequent itemset mining In Proc Int Work on High Performance Data Mining HPDM  pages 81…90 SIAM May 2003  M Z a ki and S  Part hasarat hy  A l o cal i zed al gori t h m f or par allel association mining In Proc Symp on Parallel Algorithms and Applications SPAA  pages 120…128 ACM Aug 1997  M Z a ki  S  P ar t h asar at hy  M  O gi har a and W  L i  Ne w a l gorithms for mining association rules In Proc 2  Int Conf on Knowledge Discovery and Data Mining SIGKDD  pages 14…24 ACM Aug 1997  M Z a ki  S  P ar t h asar at hy  M  O gi har a and W  L i  Ne w par allel algorithms for fast discovery of association rules Data Mining and Knowledge Discovery An International Journal  1\(4\:343…373 1997 Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing \(SBAC-PAD03 0-7695-2046-4/03 $17.00 © 2003 IEEE 


0 Figure 15 ROC map of associative keywords derived from a category 2 keyword 223perlor marrce\224 6 Conclusion The tcchniqiic OF data mining such as association algo rithm becnmcs to tie uscd widcly in variniis liclds H\(Iw cver it is tun h;ird to spccify tlic thrwhold for dcrivitig roles in nrdcr to rlcrivc mcaningful rulcs In Ibis paper wc tricd to cvaluatc thc perl\221ottunnce of ruics ilcrivcd by onc or mining icchnolugies brtscd 011 ihc cliar;icters riTROC prnplis cnnsirlchg the ROC coiivcx hull irieiliod and proposed the strelcgy to detcrruinc tho thrcsh old valucs in thc basic nlguritl~tns fot cxlcntlcd association rulcs Morcovcr wc pt-op~isctl thc visualization nielliotl hy the ROC map aid shnw how to fincl uid tlic rdntionship hc twccii n qiicry and ihc derived kcyworils In tbc fiiturc to ilerivc inorc optimal and iucnninghil rulcs wc have lo much inorc cxtciid basic and ROC nfgo rilhtns which arc prrjposed in this paper Ackaowledgrnent A par1 of this work is supportcd by hc grant of ScIeii tilic Research 10780259 OK244 103 from thc Ministry of Bducaticin Scicncc Sprk antl Culturc of Japan Wc arc gratehil to Nissho lwai Intocoin Co Lttl for thc sourcc prograins oftlic lull text seaixh sysicm 223OpetiTcxt\224 Referenccs 111 C Harbcr D Dobkin and 11 Hulidatipaii 221I\222hc quickhull algririlhin I\222or convex liull 221l\222cchnicnl Rcpurt CiCGSl Uni vcrsity 01\221 Miiiiicsotn 1903 z C 0 0,25 Figure 16 ROC map enlarged by the area of 0 5 FP  0.26 and 0 I Y\222P I 0.25 in Figure 13  2 LJ M I.\221;iyyed G Piatctsky-Sliapiro P Smytli and R Uthir rusamy Ailvirrices in Kmwludge Disucrveiy nd Dfim Miii hg AAAIIMIT l\222rcss 1996 131 J Han S Nishio H Kawano and W Wci Gcncralization hasccl datii mining in nhject-oricnied databascs wing in otrjcct cube mntlel 13a1u iind Knowledge EiiRirweririg 141 M Knwahrirn H Kawriio antl T Masegawn Evnluiilion 01\221 index had clustcring in spatial riawbasc Proceedings of Advcrmsrl I3rrrihcrsc S.ytnpo.viwn 22296 pngcs 79-X6 1 986 IS M Knwihara H Kawanu and T Hnsegawa Dah mining teclmologics fur hihliugraphic navigation systcm Tmiisrrc Iiott.r O~IIIE U\222S 39\(4 1998 161 M Knwnh;ire M Kawano atitl T 1 lmegawa lniplemcnta tion of bihliogfiiphic nnvigation system wiih tcxi data iniri ing Syskws Science 3\(24 101-1 IS 19N 71 H Kiiwano Moncluu Web search cnginc with tcxiual dm inini ng  Pro lP;Kk Puc$c Hiin C\221orference on Cotmtiirii whwi C~t~pirler rrrid Sigrid I\222ruuessing l>iips 402-405 1997 8 H Kiiwnno M Kawithsra and H T The strricturc cif text mining navigator Cor bibliographic sc;ircli Sy\222nrposriori OH fi$r,luRy IN I~XI~IES\221P pages 121-128 1998 191 E I\222rovtist and T Pawcclt Anidysis antl visualization ofclas silicr pcrfurmnncc Compririson iindcr imprccisc class and cost distributions In frocedirtgs rf3rd Irit\222I ConJerenctr MI Kiiowlerl,qe IXscnvwy mil Uri Mining KLID-97 pages 4348 1997 I IO R.Mldman Practical kxt mining In Sscortd Synqmviiri/1 on Principles of L1 Minirig arid Kti~~l~tlgc ljisorvrry PKDD-97 Nrintes Pmnce 1998 f I I R Srikant and K Agmwal Miniiig gcncralizcd association rulcs Plvceetlings oJ!h 21sr VLUA pagcs 407419 1995 25 199X 83 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


