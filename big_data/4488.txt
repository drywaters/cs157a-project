A New Association Rules Mining Algorithm Based on Vector  Xin Zhang, Pin Liao College of Science and Technology Nanchang University Nanchang, Jiangxi, China zhangxin77@gmail.com Huiyong Wang School of Mathematics and Computation Science Guilin University of Electronic Technology Guilin, Guangxi, China why608@guet.edu.cn   Abstract As a classical algorithm of association rules mining Apriori algorithm has two bottlenecks: the large number of candidate itemsets and the poor efficiency of counting support A new association rules mining algorithm based on vector is proposed, which can reduce the number of candidate frequent itemsets, improve efficiency of pruning operation and count support quickly using vector inner product operation and vector addition operation between transaction vector and 
itemset vector. According to the results of the experiments, the proposed algorithm can quickly discover frequent itemsets and is more efficient than Apriori algorithm Keywords-data mining; association rules; vector I   I NTRODUCTION  Association rules mining is an active research topic in the data mining field, which is the key step in the knowledge discovery process  A sso c i a t i o n r ul e s m i ni ng m a y discover interesting associations or dependency relations between item sets among mass data, helping people make better decisions Mining association rules have two problems: frequent itemsets discovery and association rules generation. Since the overall performance of mining is greatly determined by the first problem, this paper focuses on the process of discovering frequent itemsets Apriori algorithm [1 as  a cl as s i c a l alg o r i t h m of  
association rules mining, adopts an iterative method to discover frequent itemsets. It consists of two steps: the join step and the prune step. In the join step, a candidate k itemsets is generated by joining two frequent k 1\-itemsets Then in the prune step, all itemsets whose k 1\-subset is not a frequent k itemsets are removed from the candidate k itemsets. Then, the database is scanned to compute the support of the candidate k itemsets. This process is repeated until no new candidate k itemsets is generated. A huge calculation and a complicated transaction process are required during the algorithm. Therefore, the mining efficiency of Apriori algorithm will be poor if the transaction database is large 
Recently, many methods have been proposed to improve Apriori algorithm’s efficiency. AprioriTid [2 re du c e s th e  overhead of I/O by scanning the database only once in the first iteration. DHP [3  a h as h b as e d  alg o r i t h m is es pec i al ly  effective for the generation of frequent 2-itemsets. Some algorithms avoid generating candidate itemsets, such as DLG 4 d  ED M [5    M a x Min e r alg o ri th m  10  ef f i ci en tly  identifies long frequent itemsets, which, in turn, can be used to generate other frequent itemsets In order to avoid the deficiency of Apriori algorithm, a new algorithm is proposed in this paper. The transaction database is scanned only once and every transaction is transformed into vector at the same time. Each itemset is expressed as a vector. Lastly, with vector inner product operation and vector addition operation between the transaction vectors and the itemset vectors, the new algorithm reduces the number of candidate frequent itemsets 
improves the efficiency of counting support and generates frequent itemsets quickly II  B ASIC CONCEPTION AND PROPERTIES  Let I  i 1  i 2  i m be a universal set of items. Let D  T 1  T 2  T n be a set of transactions. Each transaction T  is a subset of I viz T 002  I All items in transaction and itemset are arranged in dictionary order Definition 1 Transaction Vector TV  A transaction is expressed as a m-dimensional vector Transaction vector of transaction T is expressed as 
TV   x 1  x 2  x m  x k 001\031 0,1  k 1,2,…,m i k 001\031 T and then x k 1, otherwise x k 0, and the dimension of TV is equal to the number of items in I  Example 1: Let I A,B,C,D,E} be a universal set. If a transaction is expressed as T i A,B,E}, then TV i 1,1,0,0,1 Definition 2  k Itemset Vector IV k  A k itemset is expressed as a n dimensional vector k 
itemset vector of k itemset c is expressed as IV k   y 1  y 2  y m  y k 001\031 0   k 1,2,…,m i k 001\031 c and then y k 1 otherwise y k 0. The dimension of IV k is equal to the number of items in I The value of k is equal to the number of digital 1” in IV k  Example 2: Let I A,B,C,D,E} be a universal set.  If a 2itemset is expressed as c A,B}, then IV 2 
1,1,0,0,0 Definition 3 Cumulative k Itemset Vector k CIV  C is a set of k itemsets. In C, the total number of all the k itemsets having a equal last item as i j is r j The k itemset vectors of these k itemsets are expressed as s k i i j x IV     1 003 s 003 r j And the cumulative k itemset vector of all k itemsets ended with item i j in C is expressed as 
2009 Third International Conference on Genetic and Evolutionary Computing 978-0-7695-3899-0/09 $29.00 © 2009 IEEE DOI 10.1109/WGEC.2009.64 429 


000   j j x j r s s k i i k i IV CIV 1    1 Example 3: Let I A,B,C,D,E} be a set of items. If a frequent 3-itemsets is expressed as L 3 A,B,C}, {A,B,E B,C,E} }, then 3 C CIV 1,1,1,0,0 3 E CIV 1,2,1,0,2 Property 1 All nonempty subsets of a frequent itemset must also be frequent [1 Property 2 Let X be a k itemset. If X has a k 1\-subset which is not a frequent k 1\itemset, then X is not a frequent k itemset [1   Inference 1 Let L k-1 be a frequent k itemsets, let l   i p1  i p2  i p\(k-1 be a frequent k 1\itemset, and l 001\031 L k-1 let i q  be a item contained in L k-1 and q  p k-1 A k itemset X  which is generated by joining l with i q is expressed as X  i p1  i p2  i p\(k-1  i q A vector S is expressed as 1 1     k i k l q CIV IV S 2 Suppose there exists a number r such that for each r 001\031  p 1  p 2  p k-1  q e have S  r  k 1, then X is not a frequent k itemset S  r   p r es en ts th e v a lu e  o f th e  r th element in S  Proof Suppose X is a frequent k itemset, then all k 1\subsets of X are contained in L k-1 According to definition 2 each value of the p 1 th p 2 th p k-1  th element in 1  k l IV is equal to 1 There may be some itemsets ended with item i q in L k-1  which is not k 1\subset of X Then according to definition 3, the value of the p 1 th p 2 th p k-1 th element in 1  k i q CIV  is at least equal to k 2, and the value of the q th element is at least equal to k 1. Therefore for each number r 001\031  p 1  p 2  p k-1  q here is S  r  004 k 1, which contradicts with the conditions. So X is not a frequent k itemset III  A NEW ALGORITHM BASED ON VECTOR  The proposed algorithm is similar to Apriori algorithm which can be divided into two steps. First, the algorithm finds out all frequent itemsets. Then it generates all association rules from frequent itemsets. Obviously, the second step is easier than first step, so we only describe the algorithm of discovering frequent itemsets At first, we generate all transaction vector which only entries either 0 or 1 by scanning the database once. Every transaction T is expressed as transaction vector TV r 1 003 r 003 n  At the same time, we count the support number of every item i j in T which is expressed as sup i j If the support number of i j is beyond the user specified threshold MinSupNumber viz. sup i j  004 MinSupNumber then i j  001\031 L 1  Secondly, the set of candidate 2-itemsets C 2 is generated by joining L 1 with itself. Each 2-itemset in C 2 has the form  i p  i q  p  q Then each i p  i q in C 2 is expressed as a 2itemset vector 2  q p i i IV and the support number of the set  i p  i q is   2 1  int   sup 2  1 q p i i r n r q p IV TV i i   000  3 where  is the inner product operator, int s  t h e  ro und function that transforms a real number to an integer by truncating the fractional part. For example, int[1   1 a n d int[0   0 I f  sup  i p  i q  004  MinSupNumber then i p  i q  001\031 L 2 After the frequent 2-itemsets L 2 is obtained, the frequent k itemsets L k-1 can be iteratively used to generate candidate k itemests C k According to the sort rule, a k itemset  i p  i q  i j can be generated by joining a arbitrary k 1 itemset i p  i q in L k-1 with a item i j in L k-1 which is bigger than i q  Then, we generate cumulative k 1\itemset vectors of k 1\-itemests in L k-1 which is 1  k i j CIV  k 003 j 003 m Then with plusing every 1  k i j CIV  j  q 1 q 2 m with the 1    k i i q p IV 000 of every frequent k 1\intemset i p  i q in L k-1 we can get S  1 1      k i k i i j q p SV IV 000 Based on vector S and Inference 1, it can be determined i p  i q  i j  005 C k or i p  i q  i j  001\031 C k  At last, we compute the support number of each candidate k itemset i p1  i p2  i pk in C k according to the following formula 000    n r k i i i r pk p p pk p p IV TV k i i i 1    2 1   1 int    sup 2 1 000 4 If the support number is beyond the user specified threshold MinSupNumber then i p1  i p2  i pk  001\031 L k  Repeat the above process with successively increasing number k until either C k or L k is empty. At the end of procedure, we can get the all frequent itemsets A detailed description of the proposed algorithm is given in Figure 1 IV  E XPERIMENTAL RESULTS  To assess the performance of the new algorithm, we performed several simulation experiments. We compare the performance of the new algorithm with the Apriori algorithm. The new algorithm and the Apriori algorithm is programmed in Matlab, all experiments are performed on a PC with 2.5GHz Intel Dual-Core E5200, 2GB memory. We use synthetic data to evaluate two algorithms, which is generated by the QUEST data generator of IBM Almaden Lab. The way of generating synthetic data is similar to that of [2  T h e pa ram e t e rs u s e d  i n ou r ex pe ri m e n t s ar e def i n e d as: N is the number of items; |D| is the number of transactions; |T| is the average size of transactions; |I| is the average size of the maximal frequent itemsets. We generated 
430 


four datasets as: T10I2, T10I4, T20I2, T20I4, by setting N=500 and |D|=1000 Input: a transaction database D the minimum threshold of support minSupNum  Output: the set of frequent itemsets L  1\  for all transaction t 001\031 D do 2\ generate TV  3 L 1 frequent 1-itemsets 4 C 2  L 1 006 L 1  5 L 2  c 001\031 C 2  sup  c  004 MinSupNum  sup  c e result of the formula \(4 6\or k 3 L k-1 000   k begin 7\ for j  k  j 003 m  j  8\     generate 1  k i j CIV   C k candidate_gen L k-1  9 L k  c 001\031 C k  sup  c  004 MinSupNum  10\d 11\eturn L  001 L k  candidate_gen\( frequent itemsets L k-1  1\  for all k 1\itemset l 001\031 L k-1 do 2\ for all i j 001\031 L k-1 do 3 S is the result of the formula \(2 if for every r 1 003 r 003 k   such that S  r  004 k 1 then 4\         add l 001  i j to C k  Figure 1. The proposed algorithm The following four figures show the comparison of the execution time of the Apriori algorithm and the proposed algorithm, along with the minimum support increasing The results illustrate that the execution time of two algorithms decrease along with the minimum support increases, but the performance of the new algorithm is much better than that of the Apriori. Moreover, the smaller the minimum support is, the more efficient the performance of the new algorithm is. This is because that as minimum support reduces, Apriori produces more candidate itemsets and spends more time making pruning operation and counting support operation. However, by inner product operation and addition operation, the new algorithm can reduce candidate itemsets and simplify pruning operation and counting support operation  Figure 2. The execution time in T10I2D1K  Figure 3. The execution time in T10I4D1K  Figure 4. The execution time in T20I2D1K  Figure 5. The execution time in T20I4D1K V  C ONCLUSION  In this paper, a new association rules mining algorithm based on vector has been presented in order to easily generate candidate frequent itemsets and quickly compute support of itemsets. The new algorithm only scans the database once, stores all transaction data into transaction vectors. And, for a more convenient calculation, express 
431 


itemsets as the form of vector. In the joint step, candidate frequent k-itemsets can be generated by enlarging the last item of frequent k 1\itemstes only. This step can significantly reduce the times of comparison among items. In the prune step, the k times comparison of numerical value is much more efficient than the circular comparison in the Apriori algorithm. Then, inner product operations between transaction vector and itemset vector are performed to count the support of items. Therefore, frequent itemsets is generated quickly. The experiment results indicate that the proposed algorithm is much more efficient than Apriori algorithm for mining association rules  R EFERENCES   1  R. Agrawal, T. Imielinski, and A. Swami, “Mining association rules between sets of items in large databases,” Proceedings of the ACM SIGMOD International Conference on Management of Data Washington DC, pp. 207-216, May 1993 2  R. Agrawal and R. Srikant, “Fast algorithms for mining association rules in large databases,” Proceedings of the 20th International Conference on Very Large Data Bases, September 1994 3  J. S. park, M.-S. Chen, and P. S. Yu, “An effective hash based algorithm for mining association rules,” ACM SIGMOD International Conference on Management of Data, pp. 175-186, May 1995 4  S.-J. Yen and A. Chen, “An efficient approach to discovering knowledge from large databases,” Proceedings of the International Conference on Parallel and Distributed Information Systems, pp. 818, 1996 5  S.-J. Yen and A. Chen, “An efficient data mining technique for discovering interesting association rules,” Proceedings of the International Conference and Workshop on Database and Expert System Applications, pp. 664-669, 1997 6  S.M. Pauray and Chien-Ming Chen, “Mining interesting association rules from customer databases and transaction databases Information Systems, vol. 29\(3\, pp. 685-696, 2004 7  F. Berzal, J.C. Cubero, N. Marrin and J.M. Serrano, “TBAR: An efficient method for association rules mining in relational databases Data and Knowledge Engineering, vol. 37, pp. 47-64, 2001 8  D. Holt John and Soon M. Chung, “Mining association rules using inverted hashing and pruning,” Information Processing Letters, vol 83, pp. 211-220, 2002 9  D. K. Sotiris Kotsiantis, “Association Rules Mining: A Recent Overview,” GESTS International Transactions on Computer Science and Engineering, vol. 32\(1\. 71-82, 2006   B. J. Roberto, “Efficiently mining long patterns from databases ACM SIGMOD International Conference on Management of Data pp. 85-93, 1998    
432 


TABLE V  E XAMPLES OF ASSOCIATION RULES         The municipality performance is not proper in handling the complaints which refer to 'asphalt settlement', 'asphalt layer', 'garbage collection' and 'installation of garbage ban subjects The probability of being satisfied is 70% in the last month of winter The unit 82 which refers to the section 2 of region 1 has the best performance in the third month of winter in comparison with the other months in handling the 'garbage collection subject The performance of municipality in handling the complaints of 'installation of garbage ban' subject in the first month of winter is very perfect and citizens feel satisfied with the probability of 100 The unit 83 which refers to the geographical section 3 of this region has the best performance and can be seen as a benchmark for the other sections Citizens feel dissatisfied with the unit 87 which refers to the section 5 of this region with the probability of 95%. This unit has a good performance just in handling the complaints which refer to the 'asphalt settlement' subject The subjects of 'asphalt layer' and 'garbage collection cause a high number of complaints in the first month of winter while the municipality performance is worse  V  C ONCLUSTION  Governments have to consider convenient channels and services to connect between the governmental managers and citizens.  Furthermore, data mining tools are needed to manage citizens' requirements and process which collect analyze, reflect, and evaluate their needs In this research, we have used clustering and association rules on the data of the urban service management system in Iran to find the subjects that cause complaint and the factors that affect the rate of satisfaction. The results show that the municipality should give top priority to the 'snow', 'garbage and waste', 'cleaning' and 'asphalt' requirements and specially to the 'asphalt settlement', 'asphalt layer', 'Garbage collection and 'Installation of garbage ban' subjects during the winter in geographical region1 Analyzing the rules, make it possible to understand the impact of factors such as time and responsible units on the rate of satisfaction. Besides, units with a perfect or worse performance in providing services and handling complaints are identified The results of the research are very beneficial in providing improved urban services and the development of citizens' satisfaction.  This study could be notable as one of the first studies on using data mining tools in CiRM R EFERENCES  1  Ahn. J. And Young, S., "Customer pattern search for after-sales service in manufacturing". Journal of Expert Systems with Applications, vol.36, pp.5371–5375, 2009 2  Cheng, Ch. and Chen, Y.,  "Classifying the segmentation of customer value via RFM model and RS theory", journal of Expert Systems with Applications, vol.36, pp.4176–4184, 2009 3  Cock, M. D., Cornelis, C. and Kerre, E. E., "Elicitation of fuzzy association rules from positive and negative examples. Journal of Fuzzy Sets and Systems", vol.149, pp.73–85, 2005 4  Han, J. and Kamber, M., Data Mining: Concepts and Techniques Second ed., Morgan Kaufman Publisher, 2006, pp. 383-407 5  Jukic, N. And Nestorov, S., "Comprehensive data warehouse exploration with qualified association-rule mining", Journal of Decision Support Systems, vol.42, pp.859–87, 2006 6  Ngai, .E.W.T., Xiu, L. and Chau, D.C.K., "Application of data mining techniques in customer relationship management: A literature review and classification", journal of Expert Systems with Applications, vol 36, pp. 2592–2602, 2008 7  Pan,S., Tan, C., Lim, E., "Customer relationship management \(CRM in e-government: a relational perspective", Decision Support Systems vol. 42, pp. 237– 250, 2006 8  Sasaki,T., A.Watanabe,Y. and Minamino,K, "An Empirical Study on Citizen Relationship Management in Japan", Proc. PICMET conference of IEEE Xplore., Aug. 2007, pp. 2820-2823 9  Tan, P., Steinbach, M. And Kumar, V, Introduction to Data Mining Addison Wesley, ISBN: 0-321-32 136-7, 2006, pp. 171-180             Lift Confidence Support Consequent Antecedent ID 1.089 75.214 28.947 satisfied month = 12 1 3.234 95 0.987 dissatisfied unit = 87 2 1.079 75.51 12.5 satisfied subject = 379 3 3.234 100 0.987 dissatisfied month = 10 and subject = 413 4 3.234 100 0.329 dissatisfied month = 10 and unit = 83 5 1.448 100 0.658 satisfied unit = 83 and subject = 524 6 3.234 100 0.329 dissatisfied month = 10 and subject = 380 and  unit = 250 7 1.184 81.818 7.237 satisfied subject = 380 and month = 12 and unit = 250 8 
281 


  4öÕ«Wâââ›6m²}}}Ñd7oÞìççw  2ª+ ´··SbÂHGo}~ýùo€± 5§î~9tóËÓ÷¿p 3Täéû_7žý¸óê$ÆL!nMiN%Q6M 9àó×Ÿ~»ûú×µ§ß¾þøÏÂÌH¤Ff  zðà×Ó'×®fbe%RË¿ß¿‰QÆÄÉÅ*%d x…xX¹˜¥Y¹Ø™å„ÙX   svv>~ü¸……Å«W¯ÄÅÅgÍš•ššŠ¬†‘‘HöôôãQQ@‰c yöôçƒ@öß×¯~¿zùÌÆ˜XYI0ÿ÷ok ƒ‘‡‹Q<¼lØu2    R¯F H<@`	¨!GÀ€#Ap <‹CƒDÁ	ØŸ4išÍvõŠ>1™™ÎüÎEAI’„h4ó<#zž':]×EQ„äyÄº®cªò+|Æ÷ý¦i,Ë:Žš|Ë0¡°ï;^ïû®ª*Ïst®ë’@š¦a"AgGhÃ€•mÛÇA¿ï{×u¡#-ÎË²Œ+¯ëjÛö4MAˆ;eGq·m‹ò<OÓ4‘,ËB H§Ói°,‹sŽs.„!@Ek­çy@÷}—R¶®¯œç9ÏsUL 001 5\006\010\031\030\020\001 1+<2\001 002\020\033\031\007\033\003\007\017\001 027\003\017\017\020\030\007\016\001 030\020\033\031\037\017\006\013\007\001 034\006\017\011\001 037\032\013\016\020 033\001 006\017\020\014\016\020\017\016\001'\013\030\001 \033\031\032\017\001\033\003\017\003\016\020\017+\001 001 b¼ŒfV-°0‚W¯^‰‹‹Ã ÓXDD1ÎÞ·o b¼ŒfV-°0‚W¯^‰‹‹Ã ÓXDD1ÎÞ·o O¯ùÙû‹°,‡a (*II’Öâ4MeY.Ë’ã8(«ª:¨1’$4M£ª*tC×uÃœN'žçasžç¢(<ÏÓ4ÍqdA@Ê,ËAX¿» GšMËN|<Ë›šÇ·dY¢”°Ë˜W RÌÈÈØÓÓS  wï^'''4Ó ŒÿÿÿãWT@‰O FV  DFFT dÀÅq©A$´··SbÂ(XAÙŠ'íPx~Â@;$;{Ú	£`¸³gÏFFFT edd a € ’=í*ƒAÝ#3®Ïž=IP a € ’=í*ƒAÝ#3®Ïž=IP 3fÌurrÚ·oŸ³³óÃ‡åääÚ¥# ³‰¼¼<0v€‘ráÂ}}}4·nÝRWW¯©©iiir©Y£ñ>œÀŠ+–-[¶yóf`"inn&¨’äöîÝ,ˆ´k‚add bO• eYAð-®ª*Ãu]eY~nÅ·þ"`Áƒëºî<Ó4™¦Ç1‚©(Š˜\–EUUœRŽ]×¹®Áyž°Ä¶m]×9tª»ü[T tz¢#QHn`_2ÙÉÛ»±‰jã+^~ÏŸ72ÿš$	!$Cr   Xkqï»uÎI 001 5\006\010\031\030\020\001 1+12\001 020\030\037\020\007\017\001 0203\020\037\031\017\006\013\007\001 017\006\014\020\001 006\014\027\030\013\023\020\014\020\007\017\001 034\006\017\011 001 037\032\013\016\020\033\001\006\017\020\014\016\020\017\016\001 001 5\006\010\031\030\020\0011+1\001\016\011\013\034\016\001\017\011\020\001\0203\020\037\031\017\006\013\007\001\017\006\014\020\001'\013\030\001 &\036!"!\001\003\007\033\001 036!"!%&\032\013\016\020\033\001 013\030\001 003\032\032\001 017\011\030\020\020\001 033\003\017\003\016\020\017\016\001 034\006\017\011\001 003\032\032 037\013\007'\006\033\020\007\037\020\001 017\011\030\020\016\011\013\032\033\001 023\003\032\031\020\001 0200\031\003\032\001 017\013\001 N+N=+\001 012\011\020\001 030\020\016\031\032 017\016\001 016\011\013\034\007\001 006\007\001 017\011\006\016\001 006\010\031\030\020\001 037\032\020\003\030\032\022\001 006\032\032\031\016\017\030\003\017\020\001 017\011\003\017\001 036  032\013\016\020\033\001\013\031\017\027\020\030'\013\030\014\020\033\001 &\036!"!\001\034\006\017\011\001\030\020\016\027\020\037\017\001\017\013\001\003\032\032\001\017\011\030\020 020\001 033\003\017\003\016\020\017\016+\001 001 033\010\027\014\006\004\012\034\021\003\014\006\010 001 007\001\013\031\030\001\027\030\020\023\006\013\031\016\001\034\013\030,\001\034\020\001\016\011\013\034\001\034\006\017\011\001\017\011\020\001\011\020\032\027\001\013'\001\0203\027\020\030 006 014\020\007\017\003\032\001\030\020\016\031\032\017\016\001\017\011\003\017\001\034\011\020\007\001\033\003\017\003\016\020\017\001\006\016\001\033\020\007\016\020\035\001\017\011\020\001\007\031\014\020\030\001\013'\001 030\0200\031\020\007\017\001 006\017\020\014\016\020\017\016\001 010\020\007\020\030\003\017\020\033\001 006\016\001 023\020\030\022\001 032\003\030\010\020+\001 012\011\006\016\001 003 020\037\017\016\001 017\011\020\001 027\020\030'\013\030\014\003\007\037\020\001 013'\001 017\011\020\001 006\007\017\020\030\020\016\017\006\007\010\001 027\003\017\017\020\030\007\016\001 033\006\016\037\013\023\020 030\022\001 003\032\010\013\030\006\017\011\014\016+\001\026\020\001\006\014\027\030\013\023\020\033\001\017\011\020\001\027\020\030'\013\030\014\003\007\037\020\001-\022\001\031\016\006\007\010\001\003\032 032 037\013\007'\006\033\020\007\037\020\001\014\020\003\016\031\030\020+\001W\023\020\007\001\017\011\013\031\010\011\001\003\032\032%\037\013\007'\006\033\020\007\037\020\001-\003\016\020 033\001 003\032\010\013\030\006\017\011\014\001 013\031\017\027\020\030'\013\030\014\020\033\001 017\011\020\001 016\031\027\027\013\030\017\001 003\016\020\033\001 003\032\010\013\030\006\017\011\014\001 034\020\001\033\006\016\037\013\023\020\030\001\017\011\003\017\001\017\011\020\030\020\001\006\016\001\016\017\006\032\032\001\007\020\020\033\001'\013\030\001'\031\030\017\011\020\030\001\006\014 027\030\013\023\020 014\020\007\017+\001 007\001 017\011\006\016\001 027\003\027\020\030\001 034\020\001 016\011\013\034\001 017\011\003\017\001 017\011\020\001\027\020\030'\013\030\014\003\007\037\020\001 037\003 007\001 020\001\006\014\027\030\013\023\020\033\001\034\006\017\011\001\017\011\020\001\011\020\032\027\001\013'\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\016\001-\022\001\030\020\033 031\037 006\007\010\001 030\020\033\031\007\033\003\007\017\001 006\017\020\014\016\020\017\016+\001 026\020\001 027\030\013\027\013\016\020\001 037\011\003\007\010\020\016\001 017\013\001 036 001\003\032\010\013\030\006\017\011\014\001-\022\001\006\007\017\030\013\033\031\037\006\007\010\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\001\010\020\007\020\030\003\017 006\013\007\001 003\007\033\001 034\020\001 030\020'\020\030\001 006\017\001 036!"!%&\032\013\016\020\033+\001 W3\027\020\030\006\014\020\007\017\003\032\001 030\020\016\031\032\017\016 001 016\011\013\034\001 017\011\003\017\001 034\020\001 003\030\020\001 003-\032\020\001 017\013\001 006\014\027\030\013\023\020\001 017\011\020\001 027\020\030'\013\030\014\003\007\037\020\001 013 001 036!"!\001\034\006\017\011\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\016+\001 &\036!"!%&\032\013\016\020\033\001\013\031\017\027\020\030 013\030\014\016\001 036!"!\001 006\007\001 017\020\030\014\016\001 013'\001 030\020\033\031\007\033\003\007\017\001 006\017\020\014\016\020\017\001 030\020\033\031\037 017\006\013\007\035\001 030\020\033\031\007\033\003\007\017\001 027\003\017\017\020\030\007\016\001 030\020\033\031\037\017\006\013\007\035\001 003\007\033\001 006\014\027\030\013\023\020\033\001 0203  020\037\031\017\006\013\007\001\017\006\014\020+\001 001 033\010\036\005\002\005\015\005\006\004\005\021\010 001 6 001 010\030\003\034\003\032\035\001 002\035\001 003\007\033\001 005\030\006,\003\007\017\035\001\002+\001\\5\003\016\017\001 \032\010\013\030\006\017\011\014\016\001'\013\030\0017\006 007 006\007\010\001 016\016\013\037\006\003\017\006\013\007\001 002\031\032\020\016 001 012\006 7\022\004\002\011\011$\010\012\021\027\006 004\007\006 5 030\003 006 012\0306\005\006 014\004\012\007\017\006\004\012\006O\011\022-\006\032\031\022\021\011\006/\031\030\031\(\031\027\011\027 035\0016//<+\001\001  001 P\003\016\017\006\033\020\035\001 024\035\001 003\0160\031\006\020\030\035\001 035\001 012\003\013\031\006\032\035\001 002\035\001 005\017\031\014\014\020\035\001 U\035\001 003\007\033\001 G\003,\011\003\032\035\001 G+\001 7\006\007\006\007\010\001 7\006\007\006\014\003\032\001 013\007%\002\020\033\031\007\033\003\007\017\001 016\016\013\037\006\003 017\006\013\007\001 002\031\032\020\016\001 4\016\006\007\010\001 5\030\0200\031\020\007\017\001 032\013\016\020\033\001 017\020\014\016\020\017\016 001 012\006 7\022\004 002\011\011$\010\012\021\027\006 004\007\006 3 027\030 006 012\0306\005\006 014\004\012\007\017\006 004\012\006 014\004\015\016\026\030\031\030\010\004\012\031\005\006 032\004\021\010\002 035\001 NNN+\001  001 P\003\022\003\030\033\020\013\035\001 002\035\001 010\030\003\034\003\032\035\001 002\035\001 003\007\033\001 U\031\007\013\027\031\032\013\016\035\001 036+\001 013\007 016\017\030\003\006\007\017%P\003\016\020\033\001\002\031\032\020\0017\006\007\006\007\010\001\006\007\001G\003\030\010\020\035\001\036\020\007\016\020\001\036\003\017\003-\003\016\020\016  001 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\0063 030\003 006%\012\0306\005\006\014\004\012\007\017\006\004\012\006/\031\030\031\006\020\012\021\010\012\011\011\022\010\012\021 035\001 6///+\001  001 R\031\016\016\003\006\007\035\0015+\035\001G\006\031\035\001R+\035\001\005\0319\031,\006\035\001W+\035\001\003\007\033\001G\031\035\001R+\001\\W3\037\020\027 017\006\013\007\001 002\031\032\020\001 7\006\007\006\007\010\001 034\006\017\011\001 003\001 002\020\032\003\017\006\023\020\001 007\017\020\030\020\016\017\006\007\010\007\020\016\016\001 7\020\003\016\031\030\020  017\006 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\0067'0 035\001:NNN+\001 1 001 014\006\020\037\006\007\016,\006\001 W+\001 032\017\020\030\007\003\017\006\023\020\001 006\007\017\020\030\020\016\017\001 014\020\003\016\031\030\020\016\001 013\030\001 014\006\007\006\007\010\001\003\016\016\013\037\006\003\017\006\013\007\016 001 020\020\020\006\035\022\031\012\027\017\0060\012\0041\005\011$\021\011\006\031\012$\006 031\030\031\006\020\012\021\010\012\011\011\022\010\012\021 035\001\023\013\032+61\035\001\007\013+\0016\035\001\027\027+\0011\\001%\001=/\035\001:NN 001  001 003\0160\031\006\020\030\035\001>\035\001P\003\016\017\006\033\020\035\001\024\035\001\012\003\013\031\006\032\035\001\002\035\001\003\007\033\001G\003,\011\003\032\035\001G+\001 032\013\016\020\033\001\005\020\017\001P\003\016\020\033\001\036\006\016\037\013\023\020\030\022\001\013'\001\005\014\003\032\032\001&\013\023\020\030\016\001'\013\030\001 016\016\013\037\006\003\017\006\013\007\001 002\031\032\020\016 001 C\011\0301\004\022\033\010\012\021\006 031\012$\006 012\007\004\022\015\031\030\010\004\012\006 001-\027\030\011\015\027\006P\004\026\022\012\031\005 035\001\023\013\032+\001;\035\001\007\013+\001:\035\001:NNN 001  001 005\006\007\010\011\035\001 002\035\001 015\013\011\007\016\017\020\007\035\001 012\035\001 002\003\010\011\003\023\003\007\035\001 021\035\001 003\007\033\001 025\006\020\035\001 024+\001 007 001 W''\006\037\006\020\007\017\001 \032\010\013\030\006\017\011\014\001'\013\030\001\036\006\016\037\013\023\020\030\006\007\010\001!\013\016\006\017\006\023\020\001\003\007\033\001>\020 010\003 017\006\023\020\001 003\017\017\020\030\007\016 001 012\006 7\022\004\002\011\011$\010\012\021\027\006 004\007\006 020\020\020\006 012\030\017\006 014\004\012\007\017\006 004\012\006 2\022\031\012\026\005\031\022\006\014\004\015\016\026\030\010\012\021 035\001>\003\007\037\011\003\007\010\035\001&\011\006\007\003\035\001:NN/+\001 S 001 005\0319\031,\006\035\001 W+\001 023\012$\010\022\011\002\030\011$\006 010\027\002\004\024\011\022-\006 004\007\006 012\030\011\022\011\027\030\010\012\021 001 0204\002\011\016 030\010\004\012\006 026\005\011\027\017\006 007\017+\001 015\013\031\030\007\003\032\001 013'\001 003\017\017\020\030\007\001 002\020\037\013\010\007\006\017\006\013\007\001 003\007\033\001 030 017\006'\006\037\006\003\032\001"\007\017\032+6=26N=1%6NS=\035\001:NN:+\001  001 025\006\020\035\001 024\035\001 015\013\011\007\016\017\020\007\035\001 012\035\001 002\003\010\011\003\023\003\007\035\001 021+\021\035\001 003\007\033\001 002\003\014\003\037\011\003\007 033\030\003\007\035\001_+\001\\^\007\001\036\006\016\037\013\023\020\030\006\007\010\001\\!\013\017\020\007\017\006\003\032\032\022\0014\016\020'\031\032 001  003 017 017 020 030\007\016\001 030\013\014\001\036\003\017\003-\003\016\020\016 001 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\006%\020\020\020\006%\012\030\017\006\014\004\012\007\017\006\004\012\006 2\022\031\012\026\005\031\022\006\014\004\015\016\026\030\010\012\021\034\0065 001 6N 001 011\013\007\010\035\001 035\001 024\003\013\035\001 024+\024+\035\001 011\016\011\006\014\003\035\001 7+\001 7\011\002\026\005\010\031\022\010\030\001 Q\022\010\011\012\030\011$\006 026\005\030\010$\031\030\031\(\031\027\011\006 010\012\010\012\021\017\006 WWW\001 012\030\003\007\016+\001 007\001 _\007\013\034\032\020\033\010\020\001\003\007\033\001\036\003\017\003\001W\007\010\006\007\020\020\030\006\007\010\035\001612/1:%/=N\035\001:NN;+\001 
419 


only, and concepts with words The corpus of the PubMed abstracts that used in the experim ents is consists of 10000 biomedical abstracts with keyword search breast cancer treatments and side effects   All experim e nts are applied on the 10000 docum ents after divided them into six documentsets 50, 100, 500, 1000 5000, and all 10000 documents. The systems are implemented by using VS .Net 2005 \(C#\a nd the experiments were performed on Intel Core2 Duo, 1.8 GHz system with Windows XP and 2 Giga of RAM A large number of association rules can be extracted by sel ecting the values of minimum support and confidence in the mining process.  The D-EART system gives the best results by using low support and high confidence values Moreover, the number of concepts that entered to the mining process is fewer by using the fuzzy weighting schema. Table V shows the experiments that are applied on various documentsets by different threshol d values. It noticed that the number of extracted association rules in D-EART system is useful and always less than that in Apriori-concept system The reason returns to the strong effect of using the fuzzy weighting schema in D-EART system Fig. 9 and Fig. 10 show that the execution time of Aprioriconcep t system is increased regular ly when the documentsets are increased compared to D-EART system. The mining process in Apriori-word system takes more time for less number of concepts in the documents. The reason is that the mining process in Apriori algor ithm depends on the size of documents rather than the number of concepts. The results show that the execution time of Apriori-concept system is about seventh fold of D-EART system. The D-EART system scans the documents only one time as the number of documents increased. Therefore the size of documents does not influence in the mining process. Finally, the results reveal that the execution time for D-EART system is much better than that of the Apriori-concept system in all cases  TABLE  V   T HE  N UMBER  OF  A SSOCIATION  R ULES  FOR  A PRIORIC ONCEPT  AND  D EART  S YSTEMS Minimum Support s  Minimum Confidence c  No. of Documents Systems s 1 c 50 3 50 7 60 10 50 500 Apriori-concept D-EART 183 71 76 31 17 5 10 2 1000 Apriori-concept D-EART 227 86 91 34 11 4 8 3 5000 Apriori-concept D-EART 239 92  75 27 20 4 15 2 10000 Apriori-concept D-EART 345 135 102 39 37 10 30 7   D5000 0,00 5,00 10,00 15,00 20,00 25,00 30,00 Apriori-concept D-EART Time in minutes s=1, c=50 s=3, c=50 s=7, c=60 s=10, c=50 Fig. 9 Execution time of Apriori-concept and D-EART systems at D=5000 D10000 0,00 5,00 10,00 15,00 20,00 25,00 30,00 35,00 40,00 Apriori-concept D-EART Time in minutes s=1, c=50 s=3, c=50 s=7, c=60 s=10, c=50 Fig. 10 Execution time of Apriori-concept and D-EART systems at D=10000 V  C ONCLUSIONS  AND  F UTURE  W ORK This paper presented a new text mining system for extracting ass o ciation rules based on concepts representation from online textual documents. This system overcame some of the problems in the prev ious EART system and the drawbacks of the Apriori algorithm by using the data structure hash table in the mining process. The results of comparing DEART and Apriori-concept syst ems reveal that the number of extracted association rules in D-EART system is always less than that in Apriori-concept system. Moreover, the execution time for D-EART system is mu ch better than that of Aprioriconcept system in all cases. So concept technique would be suitable to apply to any large corpus of medical text such as portions of the web. The future work will apply D-EART on PDF full text document with figures and images instead of using only the abstract part R EFERENCES  Fast algorithms for mining association rules,” In Jorge B Bocca, Matthias Jarke, and Carlo Zaniolo, editors Proc. 20 th Int. conf. of very Large Data Bases, VLDB Santigo, Chile 1994, pp. 487-499  T. I m ielinski, and A. Swa m i, “Mining association rules between Sets of items in large databases,” In Buneman, Peter and 


Jajodia, Sushil \(Eds Proc. of the ACMSIGMOD Int. Conf. on Management of Data, Washington D.C., 1993, pp. 207–216  e m ettinen, and A. Verka m o, “Applying data mining technique for descriptive phrase extraction in digital document collections,” in Proc. of IEEE Forum on Research and technology Advances in Digital Libraries Santa Barbra CA, 1998  m adzadeh, M. Rahgozar and A. Zarnani, “A new model for discoveri ng XML association rules from XML documents,” in Proc. 3 rd Int. Conf. on Knowledge Mining, ICKM Prague, Czech Republic, 2006 Aug. 25-27, pp. 365-369  i r, Y. Aum a nn, R Feldman, and M. Fresko Maximal association rules: A tool for mining associations in text Journal of Intelligent Information Systems 25:3, pp. 333-345, 2005  A  Ca m p i   M. Kl e m ettinen, and P  L   Lanzi M i n ing association rules fro m XML data,” in Proc. of the 4 th Int. Conf.  on Data Warehousing and Knowledge Discovery Aixen-Provence, France September 4-6, 2002  a m p i, S. Ceri, M. Kl emettinen, and P. L. Lanzi, “A tool for extracting XML as sociation rules,” in Proc. of the 14 th IEEE Int. Conf. on Tools  with Artificial Intelligence \(ICTAI’02 2002, pp. 57–64  and E. Meglio A Text M ining Strategy based on local contexts of words JADT 2004: 7 th Journées internationales d’Analyse statistique des Données Textuelles, 2004  r own Della Piet ra V J deSouza, and P V. Lai, “Class-based ngram models of natural language Computational Linguistics vol. 18 pp. 467–479, 1992  A. Napoli  and Y. T oussaint, “Towards a text mining methodology using association rule extraction,” Published online: 31 May 2005 © Springer-Verlag 2005  i cords and J. Lumpkin, “Der iving general association rules from XML data in Proc. of Fourth ACIS Int. Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel Distributed Computing SNPD'03\Lübeck, Germany, October 16-18 2003  m a n and I. Dagan, “Knowledge discover y in textual databases KDT\ in Proc. 1 st Int. Conf. on Knowledge Discovery and Data Mining 1995  R. Feld m a n, and H. Hir s h, “Mini ng associations in text in the presence of backgr ound knowledge,” in Proc. 2 nd Int. Conf. on Knowledge Discovery and Data Mining Portland, USA, 1996  m a n and I. Dagan and H Hirs h, “Mining text using keyword distributions Journal of Intelligent Systems 10, pp. 281-300, 1998  H. Zhang Q Qiu, and Z. Wang, “PCAR an ef ficient approach for mining association rules 5 th Int. Conf. on Fuzzy Systems and Knowledge Discovery, IEEE 2008  Fürnkranz, “A study using n-gram features for text categorization Austrian Research Institute for Artificial Intelligence Technical Report  OEFAI-TR-98-30 Schottengasse 3 A-1010 Wien, Austria, 1998  Bauer, J Mostafa M. Palakal, and S. Mukhopadhyay C oncept extraction and association from cancer literature WIDM’02  Mclean, Virginia, USA, November 8, 2002  J. Han, J. Pei, and Y Yin, “Mining frequent patt erns without candidate generation,” In W. Chen, J. Naughton, and P. A. Bernstein, editors, 2000 ACM SIGMOD Intl. Conf. on Management of Data ACM Press, 05 2000, pp. 1-12  W  Jin, R. K. Sr ihar i, and X Wu, “Mining concept associations for knowledge discov ery through concept chain queries,” Z.-H. Zhou, H. Li and Q. Yang \(Eds.\2007 LNAI 4426, pp. 555–562 2007.Springer-Verlag Berlin Heidelberg 2007 20  R. Joshi, X. Li , S. Ramachandaran and T. Leong \(2004\. “Automatic Model Structuring from Text using BioMedical Ontology Available http://www.aaai.org/Papers/Workshops/2004/WS-0401/WS04-01-013.pdf   Agrawal, and R. Sr ikant, “Discovering Trends in Text Databases,” in Proc of KDD, Int. Conf. on Knowledge Discovery  NewPort Beach, CA, , August 14-17, 1997, pp. 227-230  A. Dasigi, R. Dingledine, and B Ciliax, “T ext analysis of Medline for discovering functional relationships among genes: evaluation of keyword extraction weighting schemes Int J. Data Mining and Bioinformatics Vol. 1, No 1, 2006  i ve s, and J. Oliveira Concept-based knowledge discovery in texts extracted from the web ACM SIGKDD pp.29-39, July 2000  u b and D. R s n er, “Mining as sociation rules fro m  unstructured documents,” in Proc. 3 rd Int. Conf. on Knowledge Mining ICKM Prague, Czech Republic, Aug. 25-27, 2006, pp. 167-172  D. Rösner, N Is m a il, and F. Torkey  A text m i ning  technique using a ssociation rules extraction Int. J. of Computational Intelligence WASET, Vol. 4, Nr.1, 2007  a ju m d er, M  M i tra, and B. Chaudhuri, “N-gram: a language independent appr oach to IR and NLP Int. Conf. on Universal Knowledge and Language  ICUKL India November 2002  K. Ober m a y e r \(2 009\of concept based keyw ord extraction for tag recomm Available http://www.kde.cs.unikassel.de/ws/dc09/papers/paper_17.pdf   2009 a l library of Medi cine website [Online Available http://www.nlm.nih.gov   a k, “Discovering know le dge from XML documents,” In Wong John, Eds. Encyclopedia of Data Warehousing and Mining. Idea Group Publications 2005  onstrained association rules to predict heart disease,” in Pr oc. IEEE Int. Conf. on Data Mining, ICDM 2001, San Jose, CA, USA , 2001, pp. 433–440  Yong Youn, and U Kim, “A new method for mining association rules from a collection of XML documents ICCSA 2005 LNCS 3481, pp. 936–945, 2005 Springer-Verlag Berlin Heidelberg 2005  I W itten, S  Cunningha m  and G. Buchanan S calable browsing f or large collections: a case study 5 th Conf. digital Libraries  Texas, pp.215-218, 2000   M. Roche J´erom e Az´e, O. Matte-Tailliez, and Y. Kodratoff  Mining texts by association rules discovery in a technical corpus  Intelligent Information Processing and Web Mining Proc. of the Int. IIS: IIPWM'04  Conf held in Zakopane, Poland, May 17-20, 2004      M ining association rules fro m a collection of XML documents using cross filtering algorithm Int. Conf. on Hybrid Information Technology \(ICHIT'06 IEEE, 2006    W   W a n, and G. Dobbie Extr acting association rules from XML documents using XQuery,” in Proc. of the 5th ACM Int. Workshop on Web Information and Data Management \(WIDM’03 2003, pp.94–97  e iss, N Indurkhya, T. Zhang and F. Damerau TEXT MIN ING Predictive Methods for Analyzing Unstructured Information Springer Science-business Media, Inc. 2005  Li a nd T. Leong, “Automated kno wledge extraction for decision model construction: A data mining approach AMIA  Annual  Symposium Proc pp. 758-762, 2003  2009 bMed website [Online]. Available http://www.ncbi.nlm.nih.gov/pubmed  


To resolve this problem, we proposed a new KDD model. It consists of two steps: the first organizes the database records in homogeneous clusters having common properties which permit to deduce the data’s semantic. This step consists of TAH’s and MTAH generation of relieving attributes. The second permits to Discovering Knowledge. It consists to deduce the Fuzzy  Cluster Lattice corresponding to MTAH lattice generated in the first step, then traverse this lattice to extract the Meta Knowledge \( Set of fuzzy associations meta-rules on the clusters \, and in end deduce the rules modeling the Knowledge \(Set of fuzzy associations rules on the attributes\While basing on the hierarchical structure offered by the lattices, we proceed to discover the Knowledge in a hierarchical way. Thus, according to the degree of detail required by the user, this approach proposes a level of knowledge and different views of this knowledge Moreover, this solution is extensible; the user is able to choose the fuzzy method of classification according to the domain of his data and his needs This solution reduced considerably the number of generated rules, offered a better interpretation of the data and optimized both the space memory and the execution time As futures perspectives of this work, we mention 1\o test our approach on several the large data set, and 2\ to define a new intelligent method of evaluation of requests which takes into account the Meta knowledge and/or the knowledge base generated by our KDD model XI  R EFERENCES  1  P. Berkhin, “Survey of clustering data mining techniques“, Technical report, Accrue Software, 2002 2  M. Zaki, “Mining Non-Redundant Association Rules”, Data Mining and Knowledge Discovery, No 9, 2004, p. 223–248 3  G. Stumme, R.Taouil, Y. Bastide, N. Pasquier, and     L. Lakhal Intelligent structuring and reducing of association rules with formal concept analysis”, Proceedings of KI’2001 Conference, Vienna Austria, Lecture Notes in Artificial Intelligence 2174, SpringerVerlag, September 2001, p. 335–350 4  N. Pasquier “Data Mining : Algorithmes d'Extraction et de Réduction des Règles d'Association dans les Bases de Données”, Thèse Département d’Informatique et Statistique, Faculté des Sciences Economiques et de Gestion, Lyon, 2000 5  R. Agrawal, T. Imielinski, and Swami A., “Mining Association Rules between sets of items in large Databases”, Proceedings of the ACM SIGMOD Intl. Conference on Management of Data, Washington USA, June 1993, p. 207-216 6  R. Agrawal, and R. Skirant. “Fast algoritms for mining association rules”. In Proceedings of the 20th Int'l Conference on Very Large Databases, pages 478-499, June 1994 7  N. Pasquier, Y. Bastide, R.Taouil, and L. Lakhal,          “ Efficient Mining of Association Rules Using Closed Itemset Lattices Information Systems Journal, vol. 24, no 1, 1999, p. 25-46 8  M. J. Zaki, and C. J. Hsiao, “ CHARM : An Efficient Algorithm for Closed Itemset Mining ”, Proceedings of the 2nd SIAM International Conference on Data Mining, Arlington, April 2002, p. 34-43 9  G. Stumme, R. Taouil, Bastide Y., Pasquier N., and L. Lakhal, “Fast Computation of Concept Lattices Using Data Mining Techniques BOUZEGHOUB M., KLUSCH M., NUTT W., SATTLER U., Eds Proceedings of 7th Intl. Workshop on Knowledge Representation Meets Databases \(KRDB’00\Berlin, Germany, 2000, p. 129-139 10  G. Stumme, R.Taouil, Y. Bastide, N. Pasquier, and     L. Lakhal Computing Iceberg Concept Lattices with TITANIC”, J. on Knowledge and Data Engineering \(KDE\ vol. 2, no 42, 2002, p. 189222 11  S. Ben Tekaya, S. Ben Yahia, and Y. Slimani. “Algorithme de construction d`un treillis des concepts formels et de détermination des générateurs minimaux”, ARIMA journal, Novembre 2005, Numéro spécial CARI'04, pages: 171-193, 2005 12  T. Hamrouni, S. Ben Yahia, and Y. Slimani. “Prince : Extraction optimisée des bases génériques  de règles sans calcul de fermetures In Proceedings of the Intl. INFORSID Conference, Editions Inforsid Grenoble, France, pages : 353--368, 24-27 May 2005 13  B. Ganter, and R. Wille, Formal Concept Analysis: mathematical foundations. \(translated from the German by Cornelia Franzke Springer-Verlag, Berlin-Hei delberg 1999 14  T.Thanh, H.Siu Cheung, and C. Tru Hoang, “A Fuzzy FCA-based Approach to Conceptual Clustering for Automatic Generation of Concept Hierarchy on Uncertainty Data.” ,CLA 2004, pp. 1–12 ISBN 80-248-0597-9 15  L. Zadeh. Fuzzy sets. Inform ation and Control, \(69\338-353, June 1965 16  M. Sassi, M., A. Grissa Touzi, and H. Ounelli, “ “Interpretting Fuzzy Clustering Results based on Fuzzy Formal Concept Analyis”, IEEE International Conference on Fuzzy Systems. Imperial College London, UK, 2007 17  A. Grissa Touzi, M. Sassi, and H. Ounelli,  “Using Formal Concept Analysis for Flexible Querying Optimization”, 23nd International Conference on Computers and Their  Applications, \(CATA’08 Mexico, Avril 2008 18  A. Grissa Touzi, M. Sassi, and H. Ounelli, “An innovative contribution to flexible query through the fusion of conceptual clustering, fuzzy logic, and formal concept analysis”, International Journal of Computers and Their Applications. Volume. 16, N°. 4, pp 220-233, December, 2009 19  M. Sassi, A. Grissa Touzi, and H. Ounelli, “A Fuzzy Linguistic Database Summarization Approach”, Fuzzy Systems Conference IEEE International Conference on Fuzzy Systems.   Hong Kong, Juin 2008 20  J.C,  Bezdeck,  R.Ehrlich,  and  W.Full,  "FCM: The Fuzzy  C-Means Clustering Algorithm", Computers and Geoscience, vol. 10, no. 2-3 pp. 191–203, 1984 21  N. Pasquier, Y. Bastide, R.Tou il, and L.Lakhal, “Pruning closed itemset lattices for association rules”, Proceedings of 14th International Conference Bases de Données Avancées, Hammamet Tunisia, 26–30 October 1998, p. 177–196 22  M. J. Zaki, “Generating Non-Redundant Association Rules Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Boston, MA, August 2000, p 34-43 23  Y. Bastide, R.Taouil, N. Pasquier, G. Stumme, and L.Lakhal Mining frequent patterns with counting inference”, SIGKDD Explorations, vol. 2, no 2, 2000, p. 66-75 24  B. Ganter, “Two basics algorithms in concept analysis”, Technical report, Darmstadt, 1984 
134 


   


                        





