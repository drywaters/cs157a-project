A Novel Intelligent Fuzzy Ag ent Based on Input-Output Association  Hui Li 1, 2 and Qinfan Zhang 1  Peiyong Duan 1  School of Control Science and Engineering 1 School of Information and Electrical Engineering  Shandong University  Shandong Jianzhu University 2 School of Thermal Energy Engineering Ji\222nan 250101Shandong, P. R. China Shandong Jianzhu University  duanpeiyong@sdjzu.edu.cn  Ji\222nan 250014, Shandong, P. R. China  lhh@sdjzu.edu.cn qfzhang@sdu.edu.cn    Abstract  
T he inhabited environments are MIMO uncertainly, and nonlinear complex systems. This paper presents a novel intelligent fuzzy agent\(IFA\sed on input-output associational algorithm for intelligent inhabited environments An input-output dynamic associational algorithm based on Hebb learning is proposed, which can divide a complex system into multiple simple systems and eliminate the irrelevant data from the learning data to improve the learning rate. The IFA learns the user\222s preferences according to the manually operation of the user and proactively controls the environments. Initially the IFA extracts the membership functions and fuzzy rules from the data captured. After that the IFA learns the membership functions 
and optimizes the fuzzy rules online when the user\222s preferences change. The experience results show that the proposed system is effective 
Index Terms \226Intelligent inhabited environments, Hebb rule Fuzzy, Learning  I  I NTRODUCTION  The inhabited environments are constituted by building objects \(such as lighting, HVAC and other equipments\e intelligent inhabited environments are living spaces equipped with embedded intelligent technology that would gather information from sensors \(and other computers\ and make decisions according to intellig ent methods. The intelligent inhabited environments have the characteristics of ubiquity transparency and intelligence[1 It is h a rd to  b u ild th e 
  
mechanistic models for inhabited environments due to the fact that the inhabited environments are MIMO, uncertainly, and nonlinear complex systems. So it is vital to build the mathematical models according to the input-output data captured from the systems. This paper proposes a novel fuzzy control system which is build according to the input-output data and embedded in the intelligent agent. The system can learn the user\222s preferences online according to the users\222 manually operation and proactively control the intelligent inhabited environments. However, if the dimensions of inputs and outputs are great, the complex of model will increase greatly.  In literature [2 e f u zz y a g en t learn ed t h e fu z z y  rules according to all the inputs and outputs, which leaded to larger fuzzy rules. To a really MIMO complex system 
generally not all the inputs associate with the outputs. For example, the temperature information indoor is useless for light control in intelligent inhabited environments. Hence finding the most relevant associations between inputs and outputs is very vital that will eliminate the unnecessary and redundant associations. A dynamic associational algorithm for MIMO based on Hebb[3 s  prop os ed, w h i c h can di v i de t h e  complex environment system into multiple simple subsystems and eliminate the irrelevant data from the learning dataset to improve the learning rate. The presented intelligent fuzzy agent can learn the user\222s preferences online according to the user\222s manually operation and proactively control the intelligent inhabited environments. The presented methods can 
maximize the environment\222s comfort and reduce the human machine interaction, while reducing the computational and communication complexity The rest of the paper is organized as follows. Section 2 presents the adapting intelligent fuzzy agent. Section 3 presents the dynamic associational algorithm of input-output The experiments and results will be present in Section 4 Finally, section 5 will summarize the work   I NTELLIGENT F UZZY A GENT  Fig. 1 is the control block diagram of the intelligent fuzzy agent \(IFA\. When the user manually operates the corresponding actuators that modulate the environment state variables like temperature, humidity, and intensity of 
Fig. 1 The control block diagram of the IFA XY  Fuzzification Defuzzification Inference mechanism Manual controller Knowledge base Input-output association Actuator Environment 
This work is supported by Natural Science Foundation of Shandong Province \(Y2008G07\, Science and Technology Program of Shandon g Province 2009GG10001029\, and Key Program of Natural Science Foundation of Shandong Province \(ZR2009GZ004 


265 265  3           ji  002 003 m is the fuzzy index, let 2 m 265 of every input variable for each membership function j k q   2  1 265 is the membership function value from  j x to i w and 1 1   Let 12   max        qk j iii i iiii AAA A ux uxux ux 212  212  1  The multidimensional space is simplified into multiple single dimensional spaces for the sake of simplifying algorithms. The clustering number of every input and output variables general is five or seven. The input-output data are clustered based on FCM fuzzy clustering method, and finally the clustering centres are acquired. The mathematical algorithms of the FCM are       2 11  cN m mjiji ij J xw illumination for desired values, the IFA records the current state information, learns the user\222s preference according to the operation, and then modifies or adds fuzzy rules. The IFA includes four phases: \(1\apturing and associating inputoutput data. \(2\tracting fuzzy rules. \(3\gent control 4\ .Modifying fuzzy rules online  A   Capturing and Associating Input-Output Data The input and output vectors of the IFA can be defined as 12     in Xxx x x 265  4  Where m J is the clustering criterion function  j x is the normalized data space 1 1 2  j N 004 for each Gaussian membership function can be calculated as follows     1 2ln ij ij ij cc  The states of the devices used to control the environmental comfort initially are off. When the user overrides the actuators, the agent records the data of the sensors and actuators and acquires an item learning information. The associational weights of inputoutput are calculated at the same time. After a period of time the IFA has captured enough input-output data  c i w x w x ji m i j m i j 1 1 1 1 1 2 1 1 2  Due to the fact that the multiinput multi-output fuzzy rules can be divided into multiple multi-input single-output fuzzy rules, ultimately, the learning data can be divided into m learning subsets based on inputoutput associational algorithm  B  Extracting Fuzzy Rules Multiple fuzzy rule sub-bases can be extracted according to the multiple data subsets. Supposing the first data subset is  212 212 212 212   112 1     in Xxxx x  into multiple groups. One group includes the fuzzy rules that  and 12    j m Yyy y y   respectively 1 2  in   1 2  j m   1,2 kkk Z XY k N   111 1 1,2 kkk Z XY k N   11 Yy  which means there are 1 n inputs associated with the 1 y The extraction of one fuzzy rule sub-base is as follow 1\ Membership Function Construction The learning of membership functions adopts fuzzy clustering method. The FC s a g ood fu zz y cl u s t e ri ng  m e t h od f o r t h e cent ral  value learning. Firstly confirming the domain of discourse  x i min x imax  f th e i n p u t o u tp u t  v a riab les, th e n  m a k i n g  normalization   min max min ii i ii xx x x x 265  212  2    004 005  212  212  5  Where 005 is the maximum overlap between two adjacent fuzzy sets 2\ Rule Construction The fuzzy rules adopt Mamdani model[5 in th e f o llo w i n g f o r m   R i If 1 x is 1 i A and 2 x  is 2 i A and\205and 1 n x is 1 i n A  then 1 y is 1 i B   Where 1 2  il   i is the index of rules and l is the number of rules The extracting method of fuzzy rules is based on MendalWang[6   To one input-output data pair   11  XY calculating the membership function value  q i i A x  one fuzzy rule is generated as follows If 1 x  is  1 q A and\205and 1 n x is 1 q n A  then y is y  If the number of data pairs is 1 N then the 1 N fuzzy rules will be generated. However, some of them are redundant, and some of them are contrary. Here the fuzzy rules are divided         265    1   After the FCM clustering, the clustering centres of every dimension are acquired. Gaussian membership functions are used to describe the fuzzy sets, so the clustering centres are the central values of the membership functions. The standard deviation i      N j m ji N j j m ji j x w 1 1       i w is the clustering centre c i   2  1  c i ji 


 Here X is the input vector Y is the output vector On the basis of the Hebb network, the intelligent associational weight matrix algorithm is as follows Step1: Initiate the associational weight matrix w set 11 12 1 21 22 2 12 0 m m nn nm ww w ww w w ww w  Then   1 1 1 1 p p N jj j p N j j wy y w 006     6  1 p y is the output of the p group. Let 12 1111 max   q k j pppp BBB B uy uyuy uy have the same antecedent. We assume that there are l groups and group P has p N rules in the following form If 1 x  is  1 q A and\205and 1 n x is 1 q n A  then 1 y is 1 1 y  If 1 x  is  1 q A and\205and 1 n x is 1 q n A  then 1 y is 2 1 y   If 1 x  is  1 q A and\205and 1 n x is 1 q n A  then 1 y is 1 p N y  The weights of the rules are computed as 1  1  q i n jj i A i wux  the rule is fired. If the user\222s manual operation is y let 12  max     qkj BB BB uy uyuy uy   and     2 1 m y y y Y 006  7  Where  p y is the maximal value of the corresponding fuzzy membership function  D  Online Adaptive Learning Algorithm   The user\222s preferences usually change over time, so the IFA needs to continually learn the changes of the user\222s preferences. If the IFA does not meet the user\222s desires, the user will manually operate the corresponding devices. At the moment, the agent records the states    y x of sensors and actuators  For the sake of avoiding the impulsion or the wrong action, the agent begins the learning process when the same records are twice. The weight of every rule is calculated based on the membership function values of the input linguistic variables. If 0 i w   n x x x X 007 007 007 007 initiate the output association flag vector 0     2 1  005  Step3: In the occasion of generating one event, updating the flag bits of inputs changed from 0 to 1. For example, if i x and j x changed while the others remain consistently, then   006  1 2  p j N    q B is the selected Linguistic variable. Then the p group fuzzy rules can be combined into a single fuzzy rule If 1 x  is  1 q A and\205and 1 n x is 1 q n A  then 1 y is  q B  Ultimately the 1 N fuzzy rules can be combined into l fuzzy rules  C  Agent Fuzzy Control As mentioned above the IFA has learned the user\222s preference and extracted the fuzzy rules according to the input-output data pairs captured. Next the IFA can proactively control the environment based on the user\222s desires. The fuzzy control adopts singleton fuzzification, max-product composition, product implication, and height defuzzification The relation of input-output is as follow   1  1 1 1  1 1    n l pq ii p i n l q ii p i yAx yx Ax        The online adaptive learning algorithm is as follows 1  If manual operation = 1, record the input-output data P and fuzzificate them 2  For each fired rule R, compared with the rule P, if the rule R = rule P, then increase the degree of R. If the antecedents of rule are the same as the consequents of rule are different, then decrease the degree of the rule If the degree of R is zero, then remove the rule R 3  If all the fired rules are different with the rule P, then add a new rule. Calculating the maximal membership function values   q i i A ux and   q j j B uy of the inputoutput variables, then the new fuzzy rule is as follow If 1 x  is  1 q A and 2 x is  2 q A and\205and 1 n x is 1 q n A   then 1 y is  1 q B  If the degree of one rule decreases to zero, the rule has no effect on the system and is removed. This keeps the rule base small and simple  III  D YNAMIC A SSOCIATIONAL A LGORITHM  Here, taking the environment system as a black box, the inputs of the system are the states of the environment. The outputs of the system are the inhabitants\222 actions. Conforming all the input variables and output variables in intelligent inhabited environments, let     2 1 n x x x X    Step2: Initiate the input association flag vector 0     2 1   n y y y Y 007 007 007 007 and initiate the learning rate 1  0 


007  0 0.5  1.5  2.5  3.5  4.5  0.1   xi 007  1  is the forgetting factor. If the value of TI        xj 007 and the other bits remain 0. At the same time, updating the flag bits of outputs changed from 0 to 1 Step 4: Calculate the new associational weight values, the formula is as follow Y T X k k w w 007 005\007 b  212  212 1  1  8 Here 1     Fig 2 Association curves of  the different inputs to AC set 1  Y t n then the input i x is associated with the j y On the other hand if the  ij w t  then the input i x is irrelevant or redundant with the j y and would become a candidate to be removed IV  E XPERIMENT A ND R ESULT  In the experiment room, there are one desk and one bed, and there are a light in the ceiling, a lamp on the desk, and a lamp beside the bed. Moreover, there are air-condition, auto window, and auto curtain. Wireless sensor networks[7 a r e distributed in the room. The inputs include temperature sensor indoor\(TI\emperature sensor outdoor\(TO\, luminance sensor indoor\(LI\inance sensor outdoor\(LO\air pressure sensor\(PC\d bed pressure sensor\(PB\There are six output control devices. Besides the three lights\( respectively, ceiling light L1, table lamp L2, bed lamp L3\here are an air condition\(AC\n auto curtain\(ACur\d an auto window\(AW\. The three lights are all adjustable. The air condition is variable frequency. Auto curtain and auto window are on-off control. Under different environment conditions, the experimenter operates the corresponding devices according to his preferences. The experiment was executed five successive days and acquired 280 leaning data. The input vector is     X LI LO PC PB TI TO 0  1  2  3  4  5  212 k w is the associational weight matrix of last time 01 b b is great, it is easy to forget the old information and remember the new information. Adding forgetting factor in algorithm can prevent the associational weights calculated to increase endlessly When an association are strong at the beginning and decrease over time, the corresponding associational weight value might decrease over time Step 5: Reset 0   t if the  ij w  and the output vector is   1,2,3 YACAWACurLLL   The experiment included six inputs and six outputs. The associational weight matrix can be derived according to the above algorithms. Calculating the associational weight matrix w and normalizing it 0.1027 0.0772 0.2527 0.2072 0.1288 0.1948 0.1109 0.0868 0.3209 0.1945 0.2128 0.0657 0.1027 0.0858 0.0743 0.1952 0.3856 0.0812 0.0514 0.0644 0.1486 0.2027 0.0789 0.5681 0.3677 0.2879 0.0887 0.1075 0.0627 0.0484 0.2646 0.3979 0.1149 w t  According to the control devices and the associational matrix, ultimately the system was divided into multiple groups    TI TO AC 013     TI TO AW 013     L ILO ACur 013     1 L ILOPCPB L 013     2 L ILOPC L 013    3 L IPB L 013  Fig. 2 is the association curves of the different inputs to AC over the time. The x-coordinate is time and the unit is day From the fig. 2, the weights of TI, TO are improved quickly over time and are greater than the weights of the other inputs                   007 return to step 3 and wait for one new event The frequencies of events are different. It is obvious that the frequency of light adjusting is higher than the frequency of the air-condition adjusting. The lower the frequencies of events are, the less the associational weights are. To overcome the defects, the associational weight matrix is normalized according to horizontal direction Step1: calculate the base vectors of associational weights matrix  T 12   n Qqq q  nm n n m m w w w w w w w w w w              2 1 2 22 21 1 12 11    i ij ij q w w  10 Here, the sum of weights in every line of the matrix is 1 and the influence of different frequencies to matrix is eliminated. The weight of matrix really means the influence degree of one input to one output. Set a threshold 0  0.2  0.3  0.4  0.5  0.6  0.7       TO PB LI LO PC   m j ij i w q 1 9 Here i q is the weights sum of input j x to all outputs Step2: Normalize the associational weights matrix             X  0.0928 0.0813 0.0418  Let threshold 0.15 


3 0.1643 4 0.1843 4 0.1425 6 0.1822 5 0.1312 8 0.1678 6 0.1247 10 0.1616 7 0.1121 15 0.1741 8 0.1305 20 0.1689 9 0.1528 30 0.1777 The AC learning data subset has 167 data, which includes six inputs and one output under no association  1 V   Cal l ag h a n  G  Cl ar ke  M   C o l l ey H  H ag r as, J   Ch in F  D o ct o r  223Inhabited intelligent environments\224 BT Technology Journal vol. 22, no 3 , 2004, pp. 233\226247 2  A cam po r a a nd V  L o ia. \223 U s i ng f u z z y te chno l o gy in am b i e n t intelligence environments\224 The 2005 IEEE International Conference on Fuzzy Systems 2005, pp. 465-470 3 C  Mar z ba n R V i s w ana t ha n 223 Stochastic neural networks and the weighted Hebb rule\224, Neural Networks, 1993 IJCNN  apos; 93-Nagoya Proceedings of 1993 International Joint Conference on Vol. 3, 25-29 Oct. 1993, pp. 2658 \226 2661 4 J B ezd ek   Pattern recognition with fuzzy objective function algorithms  1981, New York: Plenum  H   M a m d ani  223A p p l i c a t i on s of algori t h m s for c ont rol of s i m p le d y n a m i c plant\224 Proceeding of the Institution of Electrical Engineers 1974 vol.121, no. 12,  pp. 1585-1588 6  W a ng 223 T he W M m e tho d co m p l e te d a f l e x ibl e f u z z y s y s te m appr o a ch  to data mining\224 IEEE Transactions on Fuzzy systems 2003, vol. 11 no.6,  pp. 768 - 782 7 P  D u a n  H  L i 223 Z ig be e  w i r e le s s s e ns o r ne tw o r k bas e d m u l t iag e n t  architecture in intelligent inhabited environments\224 The 4th International Conference on Intelligent Environments, University of Washington Seattle, USA 21-22, July 2008,  pp. 1-6  TABLE I RMS FOR IFA AND MLP IFA MLP Num of fuzzy sets RMS Num of hidden nodes RMS        L ILOPCPBTITO AC 013 According to the fuzzy rules extracting methods, 56 fuzzy rules are derived. The learning error RMS \(Root Mean Square\s 0.1214. After association, the AC learning data decrease to 86, which only include two inputs \(TI, TO\  12 fuzzy rules are derived. The learning error RMS is 0.1147. Moreover, the total rule numbers are cut down and the leaning efficiency is improved tremendously Now, adopting the same 280 learning data, the novel IFA method compared with the multilayer perception \(MLP neural network without input-output association. We tested our IFA learning method with different numbers of fuzzy sets which generates MIMO Mamdani FLCs that represent the rules in a more descriptive human readable form. The MLP back-propagation neural network was tested with different numbers of hidden nodes in a single hidden layer. Table 1 illustrates the RMS for the two methods over the sample data The results show that the optimum number of fuzzy sets for IFA is 7. From the table 1, the MLP method gives a higher scaled RMS than the IFA methods. It is particular importance that the MLP method can not be adapted online as it needs to repeat its time-consuming learning cycles if a new behavior is to be added. Our method does not need to calculate iteratively which can then be adapted online in a life-long mode in a non intrusive manner 2 0.2014 2 0.2312 10 0.1849 40 0.1790 V  C ONCLUSION  This paper presents a novel IFA system based on intelligent dynamical association algorithm which can be used to learn the user\222s preferences and proactively control the environment. The weights between the inputs and outputs are continuously computed and updated in the occurrence of an event. The weights will increase when both the corresponding inputs and outputs simultaneously change. In contrast the weights will decrease over time when the corresponding inputs and outputs do not simultaneously changed and become irrelevant. The adaptive fuzzy system of IFA includes four phases, respectively, capturing and associating input-output data, extracting fuzzy rules, agent control, and online modifying fuzzy rules. The proposed fuzzy learning method can lean the use\222s behaviours in a life-long mode and proactively control the inhabited environments. The proposed intelligent dynamical associational algorithm can result in decreasing the rule base which can lead to fewer memory requirements and faster processing, and reducing the computational and processing complexity while not decreasing the quality of the system R EFERENCES  


N 005 002 005 200 400 600 800 1000 2 0.22 0.44 0.67 0.88 1.11 3 0.33 0.66 0.97 1.32 1.65 5 0.55 1.08 1.64 2.14 2.75 10 1.2 2.2 3.2 4.3 5.5 20 2.1 4.3 6.5 8.7 10.8 TABLE I T IME S ECONDS  USED BY ALL PARTIES FOR DATA ENCRYPTION 003 200 400 600 800 1000 Time 0.15 0.32 0.48 0.63 0.8 TABLE II T IME S ECONDS  USED BY EACH MODERATOR that these computational costs do not include the overhead of key generation and computing two parameters 032 and 033  However generating these parameters belongs to the preparation period of the mining process Therefore it can be implemented before the protocol is executed without affecting the computation time of the protocol For evaluating the efìciency of the protocol in practice we build an experiment on the privacy preserving frequency mining in C environment which runs on a laptop with CPU Pentium M 1.8 GHz and 1GB memory The used cryptographic functions are derived from Open SSL Library To measure the computation cost of the frequency mining protocol in worst case we assume that all parties involve in the protocol except the miner are the moderators We measure the computation cost of the frequency mining protocol for 10 parties Before executing the protocol we generate a pair of keys for each party with the size of public keys set at 512 bits Table 1 illustrates our measurements of all partiesês computation time in the submission phase it is in regard to 010 and 010 017 010  for a typical scenario where 010 003 004\005\005\005  010 017 010 003\002\005 The computation time of all parties is about 10.8 seconds Table 2 illustrates our measurements of a moderatorês computation time it is linear in 010 and does not depend on  and 010 017 010 For a typical scenario where 010 003 004\005\005\005  the computation time of a moderator is about 0.8 seconds The minerês computation time it is linear in 010  010 017 010  and   However it is very small it only is 021\020 0126 when 010 003 004\005\005\005\005  010 017 010 003\002\005  and  003\004\005  V P RIVACY PRESERVING FOR CLASSIFICATION RULES LEARNING IN TWO DIMENSION DISTRIBUTED SETTING A Privacy preserving association rules mining 1 Association rules and frequent itemset The association rules mining problem can be formally stated in Let  003  002 006 005 006 007\007\007\006  002 003 be the set of all items Let 002\003 a transaction database where each transaction 020 is a set of items such that 020 012   Associated with each transaction is a unique identiìer denoted by 020&\002  We say that a transaction 020 contains  a set of some items in  if  012 020  The problem is to nd the association rules that have an implication of the form  016  014 6\006  015  where  012    012   and  017  003 7  The support 6 and the conìdence  of the rule  016  are deìned as 6 003 014 006  020  007\003 020 006  020  007 010 002\003 010  003 014 006  010  007\003 020 006  020  007 020 006  007 Where 020 006  007 stands for the number of transactions containing the set  in 002\003 and 010 002\003 010 denotes the total number of transactions in 002\003  The strong association rules are required to meet a minimum support  6 021\003\014  and a minimum conìdence   021\003\014  deìned by the miner A set of items is referred as an itemset An itemset that contains 016 items is a 016 itemset The support count of an itemset is the number of transactions containing the itemset The minimum support count is deìned as 6 021\003\014 010 002\003 010 An itemset is frequent if its support count is not less than the minimum support count Association rule mining is a twostep process 1 Finding all frequent itemsets 2 Generating strong association rules from the frequent itemsets Agrawal et al 2 presented the Apriori algorithm to efìciently identify frequent itemsets for boolean association rules The name of the algorithm is based on the fact that the algorithm uses the Apriori property i.e all nonempty subsets of a frequent itemset must also be frequent 2 Finding a frequent itemset Assume that the transactions set 002\003 is two-dimension distributed into 011\012 parties as in Section 002 007\005  Each party 014 003\004  015 003\004 006 007\007\011\006 016 003\004 006 007\007\006 012 wns 002\003 003\004 that contains information about certain attribute set  003 004 004 and certain records Given a candidate set  of the 016 items the parties wish to cooperatively nd whether or not the candidate set is frequent from the joint transaction set  002\003  without disclosing each partyês individual transactions and even the local frequent itemsets Assume that  is partitioned into parts  004  where 016 011 017  S 012\002 004 006 007\007\006 012 003  Each  004 consists of items 011  003 004 004  Note that if  is frequent in 002\003  it is frequent in at least one horizontal partition 002\003 003  where 002\003 003 003 002\003 003 002 020 007\007\007 020 002\003 003\021  In addition if  004 is frequent in 002\003 003 every  004  016 011 017  is frequent in 002\003 003\004  Considering a map from each 002\003 003\004 to a binary number that is done by each 014 003\004 as follows 8 003\004 003 002 004 006 if  004 is frequent in 002\003 003\004  005 006 otherwise Thus 002\003 is mapped to the binary matrix 011 004 012    Hence  is frequent in at least one horizontal partition 0029 003 as long as at least a row 015 in  with all elements are 004  As the result 8 003 003 003 004 004 010 8 003\004 013\010 017 010 003\005  Clearly using frequency mining can allow the miner to nd a random permutation of  031 036 002   031 036 012  Therefore the miner can identify whether  is frequent or not without knowing  be frequent in which 002\003 003  3 Finding all frequent itemsets and their support counts In the classic Apriori algorithm The k e y issue is computing the support of an itemset To nd out if a particular itemset is frequent we count the number of records where the values for all the attributes in the itemset are 1 Thus the 
101 


problem is to compute the frequency of values tuples that all values in the tuple are 004  The privacy preserving protocol for nding frequent itemsets and support counts follows Apriori algorithm as below 1 002 Finding an item 004 Itemsets is frequent 003 2 022 002 003  3 The miner sets  002 003 021 4 for each  011 022 002 do 5 The miner uses the frequency mining protocol to identify whether or not  is frequent 6 if  is frequent then 7 The miner does  002 003  002 020  8 Let  011  003 004 004  the miner broadcasts the requirement for computing 017\024++;\036 006  007 to all 014 003\004  015 011\002 004 006 007\007\007\006 011 003  9 All parties involve in frequency mining protocol to compute 017\024++;\036 006  007 10 end if 11 end for 12 for 002  003\002   037 005 002 006 003 021  016\016 003 do 13 The miner does 022 037 Approri-gen  037 005 002  14 for each  011 022 037 do 15 The miner uses the frequency mining protocol to identify whether or not  is frequent 16 if  is frequent then 17 The miner does  037 003  037 020  18 Let  consists of items partitioned into the sets  004  where 016 011 017  017 007\002 004 006 007\007\007\006 012 003  the miner broadcasts the requirement for computing 017\024++;\036 006  007 to all 014 003\004  016 011 017  19 The miner and the parties involve in frequency mining protocol to compute 017\024++;\036 006  007 20 end if 21 end for 22 end for 4 Analysis of protocol Statement 1 Correctness If all participants follow the protocol then the minerês result is the frequent itemsets and the support count of each frequent itemset Proof Candidate itemsets are generated by the Apriorigen procedure The correctness of that procedure has proved  The 022 037 sets are generated correctly as long as the input to the procedure is correct We show by induction that the  037 sets are generated correctly At steps 004 013 022 with  003\004   002 is correctly generated by the frequency mining protocol Assume that  037 005 002 has been correctly generated then 022 037 is correctly generated by Apriorigen procedure Since frequency ming protocol is correct the support count of each  011 022 004 005 002 is computed correctly Hence  037 is generated correctly from  037 005 002  The entire frequent itemsets and the support counts gives correct results Statement 2 Privacy The protocol preserves the privacy of the honest users against the miner and up to 011\012 013 002 corrupted parties as long as there is at least an honest be the moderator Proof Since all support count computations and frequent itemsets identiìcation are done independently using frequency mining This statement follows immediately from Theorem 2 5 Evaluation of complexity The communication analysis critically depends on the number of frequency computations called We incur the cost of privacy preserving frequency mining for each call Let 036 be the maximum size of a frequent itemset and let 022 003  015 003\004 006 007\007\007\006 036  and  003  015 003\004 006 007\007\007\006 036  represent the number of candidate itemsets and the found number of frequent itemsets at each round the total communication consists of cost of nding frequent and the cost of the support counts computation that is 022 003 003 012 003 006\002 006\020  016\002 022 003 007 0114 016\006\020  016\002  003 007 0104 bits Similarity the computational complexity is 5 006 003 012 003 006\002 006\006  016 022 003 007 011 016\006  016  003 007 010 007 modular exponentiations B Privacy preserving learning of ID3 tree in two-dimension distributed data Using the primitive of proposed privacy-preserving frequency mining we can learn ID3 trees in two-dimension distributed data without loss of accuracy The minerês algorithm has the same complexity as the original ID3 tree algorithm except for an additional linear overhead factor Which has a value determined by the number of times frequency mining protocol using to compute gain 1 ID3 decision tree learning we rstly present a brief review of ID3 decision trees An ID3 tree is a rooted tree containing nodes and edges Each internal node is a test node and corresponds to an attribute The edges going out of a node correspond to the possible values of that attribute The ID3 algorithm works as follows The tree is constructed top-down in a recursive fashion At the root each attribute is tested to determine how well it alone classiìes the samples The best attribute is then chosen and the samples are partitioned according to this attribute The ID3 algorithm is then recursively called for each child of this node using the corresponding subset of data Thus major problem of the algorithm is choosing the best attribute that can achieve the maximum information gain at each node Clearly the problem of choosing the best attribute can be reduced to computing entropies that require computation of the frequency of tuples of va 2 Protocol of privacy-preserving ID3 tree learning Let 002\003 be a data set that has the set of non-class attributes 005 003 002 005 002 006 007\007\006 005  003 and  the class attribute Without loss of generality we assume that all attributes have the same domain size d 002 037 002 006 007\007\007\006 037 002 003  002\003 is two-dimension distributed into 011\012 parties as in Section 002 007\005  Each party 014 003\004  015 003 004 006 007\007\011\006 016 003\004 006 007\007\006 012 wns 002\003 003\004  There are 011 parties 014 003\021  015 003\004 006 007\007\011 007 holding the classiìcation attribute   The parties wish to cooperatively build the 002 023 decision tree classiìer from the joint data set of all parties without disclosing each partyês individual transactions and even the number of the local records Assume that parties have set a system as the computation model described in Section 023  In this section we use frequency protocol as the primitive to design the privacy protocol for building decision tree following the ID3 
102 


Algorithm PrivacyPreservingID3 006 005\006 006 002\003 007 1 If 005 is empty return a leaf-node with the class value assigned to the most of all transactions in 002\003  2 Use the privacy preserving method to count the number of records with each class label If 002\003 consists of records which have the same class label 8  return a leaf node with 8  3 Otherwise Determine the best attribute 005 003 for 002\003 using the privacy-preserving method For 005 003 003 002 037 002 006 007\007\007\006 037 002 003 let 002\003 006 037 002 007  002\003 006 037 002 007 be a partition of 002\003 that every record in 002\003 006 037 005 007 has attribute value 037 005  Return a tree whose root is labeled 005 003  the root has outgoing edges labeled 037 002  037 002 s.t each edge 037 005 goes to the tree 014 036\0158\037"\033\014 036\6\\0368\015\011\031&\002 023\006 005 013 005 003 006\(\006\002\003 006 037 005 007\007 007 3 Analysis of protocol The communication/computation depends on the number of records number of vertically partition number of attributes number of attribute values per attribute number of classes and complexity of the tree For a rough analysis the cost of computation involves in terms of the time number of frequency mining protocol called to build the tree Assume that there are 036 nodes in nal classiìcation tree In total each node needs  006\004 016 004 007 the calls of frequency mining protocol All node of the tree need 036 006\004 016 004 007 the frequency computation Therefore in total the entire classiìcation process will require O 036\004<\010 006  016 010 017 010 007  encryptions and O 036\004<\010 006  016 010 017 010 007 4  bits communication VI C ONCLUSION In this paper we proposed a method for privacy-preserving classiìcation learning in two-dimension distributed data which has not been investigated previously Basically the proposed method is based on the ElGamal encryption scheme and it ensures strong privacy without loss of accuracy We illustrated the applicability of the method by applying it to design the privacy preserving protocol for some learning methods such as association rules mining decision tree learning We conducted experiments to evaluate the complexity of the protocols The experimental results showed that the protocols are efìcient and practical R EFERENCES  A Evmie vski R Srikant R Agra w al and J Gehrk e Pri v ac y preserving mining of association rules In Proc of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ACM Press pp 217-228 2002  C C Aggarw al P S Y u Eds Pri v ac y-Preserving Data Mining Models and Algorithms Series Advances in Database Systems Springer Vol 34 2008  D Agra w al and C Aggarw al On the design and quantiìcation of pri v ac y preserving data mining algorithms In Proc ACM SIGMOD pp 247-255 2001  D Boneh The decision Dif fe-Hellman problem In ANTS-III V o l 1423 of LNCS pp 48-63 1998  F  W u J Liu and S Zhong An ef cient protocol for pri v ate and accurate mining of support counts Pattern Recognition Letters Vol 30 Issue 1 1 pp 80-86 2009  O Goldreich F oundations of Cryptography  B asic T ools V o l 1 Cambridge University Press 2001  H Martin and S Kazue Ef cient receipt-free v oting based on homomor phic encryption In Proc of Advances in Cryptology-Eurocrypt 2000  J Benaloh and D T uinstra Receipt-free secretballot elections e xtended abstract In Proc of the 26th Annual ACM Symposium on Theory of Computing ACM Press pp 544-553 1994  J V aidya and C Clifton Pri v ac y preserving nai v e Bayes classiìer for vertically partitioned data In Proc of the 2004 SIAM Conference on Data Mining 2004  J V aidya and C Clifton Pri v ac y preserving association rule mining in vertically partitioned data In Proc of the eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining pp 639-644 2002  Luong T D Ho T B 2010 Pri v ac y Preserving Frequenc y M ining in 2-Part Fully Distributed Setting IEICE Trans Information Systems to appear  M Kantarcoglu and J V aidya Pri v ac y preserving nai v e Bayes classiìer for horizontally partitioned data In IEEE ICDM Workshop on Privacy Preserving Data Mining pp 3-9 2003  M.Freedman K.Nissim and B.Pinkas Ef cient pri v ate matching and set intersection In Proc of Eurocrypt Vol 3027 of LNCS Springer-Verlag pp 1-19 2004  R Agra w a l a nd R Srikant Pri v ac y preserving data mining In Proc of ACM SIGMOD Conference on Management of Data pp 439-450 2000  R Agra w al R Srikant and D Thomas Pri v ac y preserving OLAP  I n Proc of the 2005 ACM SIGMOD International Conference on Management of Data SIGMOD 05 ACM pp 251-262 2005  R Agra w a l a nd R Srikant Pri v ac y-preserving data mining In Proc of the ACM SIGMOD Conference on Management of Data ACM Press pp 439-450 2000  R.Agra w al T  Imielinski and A.Sw ami Mining association rules between sets of items in large databases In Proc of the 1993 ACM SIGMOD international Conference on Management of Data 207-216 1993  S Zhong Z Y a ng and T  Chen k-Anon ymous data collection Journal of Information Sciences Vol 179 Issue 17 pp 2948-2963 2009  V S V e rykios E Bertino I.N F o vino L.P  Pro v e nza Y  Saygin and Y Theodoridis State-of-the-art in privacy preserving data mining ACM SIGMOD Record Vol 3 No 1 pp 50-57 2004  Y  Lindell B Pinkas Pri v ac y preserving data mining In Adv ances in Cryptology Crypto2000 Vol 1880 of LNCS Springer-Verlag pp 36-53 2000  Y  Tsiounis and M Y ung On the security of ElGamal-based encryption In Public Key Cryptographyê98 Vol 1431 of LNCS pp 117-134 1998  Z Y a ng S Zhong R.N Wright Pri v ac y-preserving classiìcation of customer data without loss of accuracy In Proc of the 2005 SIAM International Conference on Data Mining SDM pp 21-23 2005  W  Du and Z Zhan Using randomized response techniques for pri v ac y preserving data mining In Proc of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ACM Press pp 505510 2003  W  Du and Z Zhan Building decision tree classiìer on pri v ate data In Proc of IEEE International Confonference on Privacy Security and Data Mining pp 1-8 2002 
103 


              


   


                        





