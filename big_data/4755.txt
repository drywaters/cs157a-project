I-FAC Ef\002cient Fuzzy Associative Classi\002er for Object Classes in Images Ashish Mangalampalli 1 1 IIIT Hyderabad India ashish m@research.iiit.ac.in Vineet Chaoji 2 Subhajit Sanyal 2 2 Yahoo Labs Bangalore India f chaojv subhajit g yahoo-inc.com Abstract We present I-FAC a novel fuzzy associative classi\002cation algorithm for object class detection in images using interest points In object class detection the negative cl ass C N is generally vague  C N  U 000 C P  where U and C P are the universal and positive classes respectively But image classi\002cation necessarily requires both posi 
tive and negative classes for training I-FAC is a singleclass image classi\002er that relies only on the positive class for training Because of its fuzzy nature I-FAC also handles polysemy and synonymy common problems in most crisp non-fuzzy image classi\002ers very well As associative classi\002cation leverages frequent patterns mined from a given dataset its performance as adjudged from its false-positive-rate\(FPR\-versus-recall curve is very go od especially at lower FPRs when its recall is even better IFAC has the added advantage that the rules used for classi\002cation have clear semantics and can be comprehended easily unlike other classi\002ers such as SVM which act as black-boxes From an empirical perspective on standard 
public datasets the performance of I-FAC is much better especially at lower FPRs than that of either bag-of-words BOW or SVM both using interest points 1 Introduction Association Rule Mining ARM enables the extraction of latent frequent patterns which are based on their respective frequencies and thus represent the dominant trends in the given dataset A new classi\002cation approach called associative classi\002cation 12   1 3    1 4  h a s g a i n e d p o p ularity of late because of its accuracy which can be attributed to its ability to mine huge amounts of data in order to build a classi\002er based on frequent patterns in a dataset The advantages of associative classi\002ers are that frequent 
itemsets capture all dominant relationships between items in a dataset and that they deal only with statistically signi\002cant associations Thus the classi\002cation framework is robust because low-frequency patterns noise are eliminated during the ARM stage But associative classi\002cation like many other classi\002ers cannot be used directly on datasets and domains which make heavy use of numerical attributes like image classi\002cation as it expects categ orical/binary attributes One method to circumvent this problem is to use binning or clustering to convert numerical attributes to categorical attributes But using crisp binn ing or crisp clustering introduces uncertainty especially at the 
boundaries of bins or clusters leading to loss of information  Small changes in the selection of number of bins or clusters may lead to polysemy one bin or cluster containing features with different meanings and synonymy two features with same meaning mapped into different bins or clusters thus generating misleading results A more effective way to solve this problem is having features belong to clusters with some membership value in the interval 0 1  i n s t e a d o f b e l o n g i n g e n t i r e l y t o a p a r t i c u l a r cluster Thus fuzzy features replace categorical ones In this paper we present our algorithm I-FAC which adapts fuzzy associative classi\002cation to 002t the image clas si\002cation perspective by leveraging Speeded-Up Robust 
Features SURF that can be extracted from images 1  SURF is a fast scale and rotation-invariant interest point detector and descriptor for images These interest points which can vary in number from image to image can be used for further processing like clustering and classi\002ca tion Generally obtaining the negative class set C N is an issue in image classi\002cation due to its ill-de\002ned nature as compared to the positive class C P  Effectively the negative class set C N  U 000 C P  where U is the universal set of all images But conventional classi\002ers need both positive and negative classes for training Because C N is not well-de\002ned classi\002ers so trained on a subset of the neg 
ative class may not perform well on disparate test images The advantage of I-FAC is that only positive class samples are required to train the classi\002er with no reliance on negative class samples for training and without the need for unlabelled examples or outliers In the literature one-cl ass classi\002ers which rely on unlabelled examples 6 o r t r e a t outliers and noise as negative examples for training 7  have been proposed In the bag-of-words BOW approach each SURF point belongs only to one of the clusters in the codebook which is created by applying crisp clustering like k means on a sizeable set of images Using fewer number of clusters would avoid synonymy but would at the same time give rise to polysemy Thus in BOW deciding 
upon the number of clusters that should be used is an important but dif\002cult task because of which 031 1000-3000 are generally used But I-FAC relies on fuzzy c-means FCM clustering 5 a n d c r e a t e s f a r l e s s n u m b e r o f c l u s ters  031 100 using only the positive class training images as compared to the number of clusters used for the codebook in BOW thus avoiding synonymy Due to the fuzzy nature of clusters it is able to address polysemy as well The main contributions of this work are  a use of fuzzy sets and logic in object class classi\002cation in im 
2010 International Conference on Pattern Recognition 1051-4651/10 $26.00 © 2010 IEEE DOI 10.1109/ICPR.2010.1067 4372 
2010 International Conference on Pattern Recognition 1051-4651/10 $26.00 © 2010 IEEE DOI 10.1109/ICPR.2010.1067 4396 
2010 International Conference on Pattern Recognition 1051-4651/10 $26.00 © 2010 IEEE DOI 10.1109/ICPR.2010.1067 4388 
2010 International Conference on Pattern Recognition 1051-4651/10 $26.00 © 2010 IEEE DOI 10.1109/ICPR.2010.1067 4388 
2010 International Conference on Pattern Recognition 1051-4651/10 $26.00 © 2010 IEEE DOI 10.1109/ICPR.2010.1067 4388 


c 1  026 1  1  c 2  026 1  2      c k  026 1 k  positive class label    c 1  026 n 1  c 2  026 n 2      c k  026 n;k  positive class label Figure 1 Fuzzy­cluster based representation ages By doing so we can deal with polysemy and synonymy better as compared to crisp sets This is re\003ected in the experiments section Section 4 where usage of fuzzy sets yields better results than obtained using crisp sets b  I-FAC is an associative classi\002er which relies on frequent itemsets Frequent itemsets capture all dominant relation ships between items in a dataset This helps in making the algorithm more resilient to noise 2 Related Work 9 a n d  1 0  d e s c r i b e t h e a p p l i c a t i o n o f A R M i n d e tecting features in images and videos respectively with the help of spatial con\002gurations and local neighborhoods The video mining method proposed in 11 b u i l d s o n l o c a l neighborhoods of quantized local features 3 i s b a s e d o n the bag-of-words approach for generic visual categorization 3 I-FAC and Image Classi\002cation This section describes key components of I-FAC for image classi\002cation SURF points extracted from C P are clustered using FCM clustering followed by fuzzy ARM The fuzzy association rules are then transformed into fuzzy classi\002cation rules during training For actual classi\002cation membership values for the SURF points extracted from a test image are interpolated using cosinesimilarity with the centers of the fuzzy clusters generated previously in the training phase The classi\002cation is then done by calculating the cumulative fuzzy information gain in conjunction with a threshold 016  3.1 SURF Point Generation The 002rst step in I-FAC extracts SURF points from images in the positive class training dataset with no negative class images being used Assuming we need k fuzzy clusters we run FCM cosine distance metric is used on all the n SURF points extracted from the training images From the k fuzzy clusters for each SURF point we have its membership value  026  in each of the k fuzzy clusters The k membership values for each SURF point are then used to transform SURF-point-based representation of the images into a fuzzy-cluster-based representation Each SURF point is represented as a separate record with each record consisting of k cluster attribute ids and corresponding 026 pairs cluster id 026  followed by the positive class label as shown in Figure 1 3.2 Fuzzy Association Rule Mining Subsequently we use the fuzzy ARM algorithm with appropriate minimum support described in 8 t o e x t r a c t latent patterns in the form of fuzzy association rules from the fuzzy-cluster-based representation of the SURF points  as shown in Fig 1 This algorithm is optimized to extract rules from very large datasets having many attributes high dimensions which is common in the image domain The support supp  I  of an itemset I  in the crisp domain is de\002ned as the proportion of transactions in the dataset which contain I  During fuzzy ARM each of the k dimensions corresponding to k clusters is taken as an attribute The membership values of a SURF point in each of the k clusters provide the values for these k attributes Fig 1 Moreover support as de\002ned for crisp association rules has been generalized in a suitable way for the fuzzy environment 2   4   A t n o r m T  given by Eq 1 satis\002es the condition T  x 1  x 8 x 2 0  1  with fuzzy sets A and B in a 002nite universe D  lying in the range 0 1  T h e cardinality of a fuzzy set in D is de\002ned by Eq 2 Using Equations 1 and 2 we get fuzzy support de\002ned in Eq 3 T M min t-norm the most popular t norm has been used in I-FAC to derive the rule-set R with m 0 rules from the fuzzy-cluster-based representation of SURF points A  x   T B  x   T  A  x   B  x  1 j A j  X x 2 D A  x  2 sup  A  B   1 j X j X x 2 D  A  T B  x  3 H  Y   000 z X i 0 p i log p i 4 H  Y j X   X P rob  X  H  Y j X  5 IG  Y j X   H  Y  000 H  Y j X  6 3.3 Fuzzy Associative Classi\002er Training Entropy and information gain are calculated for each rule in the rule set R  Given a rule of the form X  Y i  where X is an itemset composed of varying number of attributes a 1  a 2      a l  the entropy of X is given by Eq 4 where z is the number of classes being considered Y i is the class label pertaining to the rule In the crisp case the fraction of records in the dataset where Y i occurs is denoted by p i  But in the fuzzy case p i of Y i is calculated by taking the maximum membership value among all attributes clusters in each record in which Y i exists The average conditional entropy H  Y j X  for Y  with respect to X  is given by Eq 5 In the fuzzy case H  Y j X  is calculated using a t-norm  T M t-norm in this case The frequency for each record involving Y is a function of the 
4373 
4397 
4389 
4389 
4389 


minimum membership value of all attributes fuzzy clusters a 1  a 2      a l that are involved in X  The information gain IG  Y j X  is given by Eq 6 ARM generates a large number of rules most of which are redundant and are pruned by I-FAC The information gain of each rule and rule length i.e number of attributes in each rule is used for the pruning process Each rule r q is compared to all r q 1 to r 0 m rules A given rule r q with information gain IG q and rule length rl q  is pruned  R  R 000 r q  if there exists another rule r s with information gain IG s and rule length rl s  which is a superset of r q  and rl q  rl s and IG q  IG s  After pruning the size of R reduces from m 0 to m 00  3.4 Image Classi\002cation The actual classi\002cation stage is relatively straightforward and is dependent upon the rule set R derived in the training stage But before that we identify all the n 0 SURF points in the image being classi\002ed The centers of the k clusters generated during the training phase are used to calculate the fuzzy membership of each of the n 0 SURF points in each of the k clusters For each SURF point s  cosine similarity values are calculated between s and each of the k cluster centers The normalized similarity between each cluster center c and s is denoted as fuzzy membership value 026 sc  Similar procedure is followed for all the n 0 SURF points at the end of which we have a record in fuzzy cluster format representation for each of the n 0 SURF points The membership value 026 ij for each cluster c i in each of the n 0 records is aggregated to get one record cr with cumulative membership values in each of the k clusters of the whole image Eq 7 Then each rule r in the rule set R with m 00 rules is applied to this cumulative record cr  When r is applied we identify each of the t attributes clusters that are a part of the precedent right hand side of the rule of r  The cumulative fuzzy membership value  c\026  for each of these t clusters is extracted from cr  The product Eq 8 of the arithmetic mean of these cumulative fuzzy membership values and the information gain  IG  associated with r is used to come up with a derived metric we call fuzzy information gain  F IG  The cumulative fuzzy information gain is calculated Eq 9 as each rule is applied on cr  If at the end cumulative F IG 025 threshold 016  then the image in question belongs to the positive class or else it belongs to the negative class 026 i  n X j 1 026 ij where i  1 to k 7 F IG  Y  IG  000 P t i 1 c\026 i t 001 8 cumulative F IG  m 00 X f 1 F IG f 9 4 Performance Study and Results We have compared I-FAC with minimum support supp between 0.005 and 0.05 depending on dataset fuzzi\002cation factor m  1.5 and 100 fuzzy clusters to two baseline approaches namely BOW and SVM both based on SURF points The support value relies on how dense or sparse the dataset is the number of items singletons involved in the dataset and the average length of transactions in the dataset  15   I n B O W  w e c o u n t h o w m a n y times each visual word in the code-book occurs in an image A feature vector consisting of weighted frequency of each word from the bag of words is used for training and testing The results for BOW have been taken from the baseline of 10  w h i c h u s e s 3 0 0 0 c l u s t e r s t o c r e a t e the code-book To generate a single feature vector per image for SVM  libSV M implementation using RBF kernel classi\002cation SURF points from an image are combined using Latent Semantic Hashing FPR-versus-recall for SVM was calculated using a threshold for the probability of positive class CALTECH Cars Rear background dataset was used as negative training set for BOW and SVM I-FAC does not expect any negative class training set The other datasets used are CALTECH Cars Rear The positive class training positive class test and negative class test datasets respectiv ely are cars markus cars brad and the 002rst 200 images from CALTECH-101 background class TUD Motorbikes CALTECH-4 motorbikes TUD motorbikes and 200 random images from CALTECH-256 clutter class were used for positive class training positi ve class testing and negative class testing respectively ETHZ Giraffes Training was done on 93 images of giraffes downloaded from Google Images The positive class test and negative class test datasets were 87 giraffe images and the rest 168 images respectively from the ETHZ Shape Classes dataset GRAZ Bikes The positive class training and positive class test sets respectively are randomly picked 25 and 38 images from the GRAZ bikes dataset The 002rst 200 images from CALTECH-101 background class dataset were used as negative class test set CALTECH Faces 52 randomly picked images from the CALTECH Human Faces Front dataset were used for each of the positive class training and test sets The 002rst 200 images from CALTECH-101 background class dataset were used as negative class test set I-FAC consistently performs well on the basis of FPRversus-recall when compared to either BOW by high margins on all 002ve datasets or SVM by high margins on three datasets Cars Faces and Giraffes and by reasonable margins on the remaining two datasets Fig 2 It especially performs very well at low FPRs  024 0  3  which is highly desirable for an image classi\002er The performance of I-FAC can be attributed to two broad reasons First its fuzzy nature helps avoid polysemy and 
4374 
4398 
4390 
4390 
4390 


synonymy which are common problems with BOW Second SVM has to deal with a lot of noise in the training images which hampers the creation of a clear hyperplane affecting the assignment of probability with which the positive class occurs in a given image This problem does not occur in I-FAC which uses only the positive class for training and makes a classi\002cation decision based on cumulative F IG in conjunction with a threshold 016  For each dataset 016 has been determined by cross-validation on the respective positive and negative classes test sets The variation in 016 in\003uences the variation of FPR-versusrecall curve for I-FAC in each dataset Fig 3 shows FPRversus-recall variation as number of clusters and m of FCM is varied for the Cars dataset with best results achieved when 100 clusters and m  1  5 were used At m  1  001 m 031 1  FCM reduces to k means i.e crisp clustering Higher values of m  e.g m  2  gave worse results than those shown in Fig 3 References 1 H  B a y  A  E s s  T  T u y te la a r s  a n d L  J  V  G o o l S p e e d e d up robust features SURF Computer Vision and Image Understanding  110\(3\:346–359 2008 2 M  D  C o c k  C  C o r n e lis  a n d E  E  K e r r e  E lic ita tio n o f fuzzy association rules from positive and negative examples Fuzzy Sets and Systems  149\(1\:73–85 2005 3 C  D a n c e  J  W illa m o w s k i L  F a n  C  B r a y  a n d G  C s u r k a  Visual categorization with bags of keypoints In ECCV International Workshop on Statistical Learning in Computer Vision  2004 4 D  D u b o is  E  H  ullermeier and H Prade A systematic approach to the assessment of fuzzy association rules Data Mining Knowledge Discovery  13\(2\:167–192 2006 5 J  C  D u n n  A f u z z y r e la ti v e o f th e is o d a ta p r o c e s s a n d its use in detecting compact well-separated clusters Journal of Cybernetics  3:32–57 1973 6 G  P  C  F u n g  J  X  Y u  H  L u  a n d P  S  Y u  T e x t c la s s i\002 c a tion without labeled negative documents In ICDE  pages 594–605 2005 7 L  M  M a n e v itz a n d M  Y o u s e f  O n e c la s s s v m s f o r d o c ument classi\002cation Journal of Machine Learning Research  2:139–154 2001 8 A  M a n g a la m p a lli a n d V  P u d i F u z z y a s s o c ia tio n r u le m in ing algorithm for fast and ef\002cient performance on very large datasets In FUZZ-IEEE  pages 1163–1168 2009 9 T  Q u a c k  V  F e r r a r i a n d L  J  V  G o o l V id e o m in in g w ith frequent itemset con\002gurations In CIVR  pages 360–369 2006 10 T  Q u a c k  V  F e r r a r i B  L e ib e  a n d L  J  V  G o o l E f 002 c ie n t mining of frequent and distinctive feature con\002gurations In ICCV  pages 1–8 2007 11 J  S i v ic a n d A  Z is s e r m a n  V id e o d a ta m in in g u s in g c o n 002gurations of viewpoint invariant regions In CVPR  pages 488–495 2004 12 F  A  T h a b ta h  A r e v ie w o f a s s o c ia ti v e c la s s i\002 c a tio n m in ing Knowledge Engineering Review  22\(1\:37–65 2007 13 A  V e lo s o  W  M  J r   a n d M  J  Z a k i L a z y a s s o c ia ti v e classi\002cation In ICDM  pages 645–654 2006 14 X  Y in a n d J  H a n  C P A R  C la s s i\002 c a tio n b a s e d o n p r e d ic tive association rules In SDM  2003 15 Z  Z h e n g  R  K o h a v i a n d L  M a s o n  R e a l w o r ld p e r f o r mance of association rule algorithms In KDD  pages 401 406 2001 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 Recall False Positive Rate I-FAC BOW  SVM  a CARS Dataset 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 Recall False Positive Rate I-FAC BOW  SVM  b TUD Bikes Dataset 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 Recall False Positive Rate I-FAC BOW  SVM  c Giraffe Dataset 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 Recall False Positive Rate I-FAC BOW  SVM  d GRAZ Bikes Dataset 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 Recall False Positive Rate I-FAC SVM  e Faces Dataset Figure 2 Results on Standard Datasets 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 Recall False Positive Rate cl=100,m=1.001 cl=100,m=1.1  cl=100,m=1.5  cl=50,m=1.1  cl=50,m=1.5  cl=25,m=1.1  cl=25,m=1.5  cl=10,m=1.1  cl=10,m=1.5  Figure 3 FPR versus Recall for varying clusters and m 
4375 
4399 
4391 
4391 
4391 


  40  The second step is started by making logical and   between each pair of frequent 1-itemsets, as we mentioned earlier in this paper, and by assigning 30% as a new value to the minimum support threshold, we found that the frequent 2itemsets will be: {{1, 3}, {1, 4}, {3, 5}}, and the graph is constructed by drawing an edge between each pair of frequent items, as in Figure 1   Figure 1: a simple directed graph to display frequent k-itemsets k 2  To determine frequent 3-itemsets, we traverse the graph as if there is a path among three nodes {i , j} and {j, k then the set {i, j, k} will be frequent 3-itemset. Here, in this example, {{1, 3, 5}} is the only frequent 3-itemsets. As there are no extra edges, the algorithm terminates In the standard situation, as the database contains hundreds of thousands of transactions and different items constructing only one graph is not practical, and so we suggested to construct different graphs for each cluster and find from this graph all frequent itemsets, then combine the subsets of frequent itemsets together to get the whole set of frequent itemsets, and this technique is scalable with all transactions databases of different sizes 6  Experimental Results To assess the efficiency of the proposed technique, we have implemented the CGAR, along with Apriori algorithm using Java programming language on a Pentium IV 1700 MHz PC with 512MB of available physical memory. The test databases are the standard datasets available to evaluate rule mining algorithms, they are: T10I4D100K and T40I10D100K We execute both algorithms, Apriori and CGAR, at various values of minimum support thresholds, as the number of frequent itemsets generated inversely proportional with the value of the minimum support. Figure 2 displays the average execution time in seconds to generate all frequent itemsets using CGAR and Apriori algorithm        Figure 2: a comparison between Apriori and CGAR The experimental results in Figure 2 show that the CGAR algorithm has better performance than Apriori in terms of the execution time. When there is an increase in the number and size of frequent itemsets discovered, i.e. reduction in the minimum support threshold, the performance gap between these algorithms is displayed in greater clearance 7  Conclusion In this paper we propose a new framework, which is scalable and efficient.  The entire database is divided into 1 3 5 4 Time Seconds  Minimum Support 


  41  partitions of variable sizes, each partition will be called a cluster.  Each cluster is consid ered one at a time by loading the first cluster into memory and calculating large itemsets and the corresponding support counts. Then the second cluster is considered similarly and the cumulative support count is calculated for the cumulative large itemsets. This process is continued for the entire set of clusters and finally we have the whole large itemsets and the corresponding cumulative support counts.  This approach reduces main memory requirement since it considers only a small cluster at a time and hence it is scalable for any large size of the database Experiments using two of the standard transaction databases available on the Internet, T10I4D100K and T40I10D100K, show that CGAR outperforms Apriori, a familiar and widely used association rule mining algorithm When there is a reduction in the value of the minimum support threshold, the performance gap between the algorithms becomes more evident References 1 uhJ i u a n T s ay J i un nY a n n Ch ia ng C B A R  an e f f i cie n t m e t h o d f o r  mining association rules, Knowledge-Based Systems 18 \(2005\105 2 R A g r a w a l  T   I m il ie ns ki, A  S w am i Mi ni ng as s o cia t io n r u l e s be tw e e n s e ts  of items in large databases, Proceedings of the ACM SIGMOD International Conference on Management of Data, Washington, DC, 1993 pp. 207–216 3  A y s e Oz el and H  A l tay  G venir  A n A l g o r ithm f o r Mi ni ng A s s o ciati o n  Rules Using Perfect Hashing and Database Pruning, \(2000  4  A g ra w a l R  S r ik ant   M i ni n g s e q u e nt ia l pa tt ern s  P r oc eed in gs of th e 11 th  International Conference on Data Engineering \(ICDE\, 1995 5 F  B e rz a l  J C  C u b e ro N M a rin   J M  Se rra n o  T B A R a n e ffi c i e n t me th od for association rule mining in relational databases, Elserier Data and Knowledge, Engineering 37 \(2001\ 47–64 6   Br i n R  Mo tw an i C. S i l v e r s t e i n, Be y o nd m a r k e t  bas k e t s  g e ne r a l i z i ng  association rules to correlations, ACM SIGMOD Conference on Management of Data, Tuscon, Arizona, 1997 pp. 265–276 7 s ho k S a v a s e r e E d w a r d O m ie ci ns k i a n d S h am ka n t N a v a t h e  A n E f f i cie n t  Algorithm for Mining Association Rules in large databases. 1995 8 a n, J P e i, J Y i n  Y  Min i ng f r e que nt P a tte r ns w i tho u t Ca nd i d a t e  Generation. In: ACM-SIGMOD, Dallas \(2000 9 h o w J ane Y e n an d A r be e  L  P  Che n A G r aphB a s e d A ppr o a c h f o r  Discovering Various Types of Association Rules, IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 13, NO. 5 SEPTEMBER/OCTOBER 2001 10 D  W  C h e u ng J. H a n  V  T   N g A  W  F u Y  F u A f a st dis t r i bu te d  algorithm for mining association rules, Proceedings of International Conference on PDIS’96, Miami Beach, Florida, USA, 1996 11  Be n F r an kl in G e ne al og ical D a t a  Mi ni ng  2 0 0 6    


 F  To check the validity of association rules  E quatio n  1  is use d  as  it is done i n  th e  last column of  Figur e  7   G  Select one  of th e  rules which have Improvement  value more than   1     H  In case if there is another job asking to get the files  and the s e files are available in the same sites then  choose another rule to serve the new request  Otherwise  apply  Aprior i  algorithm for recent STTs  of  new replicas sites    VI I   I NTERPRETING THE  R ESULT S    This section means to explain how the association  rul e s work better than the traditional and random methods  As it is shown in  Figure 7 after applying  Aprior i  algorithm  we get  602  different rules which can be used to select the  best combination of replica sites   Let us explain Figure 7  in details    Rule #1: if Site\(s S 4  S 7  are selected then this implies  that site\(s S 3  can also be selected at the same time. This  rule has 100% confidence    In other words, it means if site  S 4   and  S 7  are selected to  work together to transfer the requested files, t h en this  implies site\(s  S 3  can also be selected to share the work at  the same time. This rule has confidence  100  This  particular rule has  confidence  of  100  meaning that  S 4  S 7   and  S 3  can be selected as a best set of replicas by  Replica  Manage r  to ge t  requested files. To compute the correlation  of this rule and see how far it is better than choosing the site  randomly, we use an Improvement equation     indicates that it has support of  26  transactions, meaning that in transaction  Single Trip  Time  Table  there are  2 6  concurrent uncongested trips of  S 4  S 7   i.e. these sites have similar network conditions in particular  time      indicates the total number of transactions  involving uncongested trips of  S 3   in  Rule 1  is equal to  174    This is a piece of a side information; it is not involved  in calculating the confidence or support for the rule itself       is the number of transactions where   S 4   S 7 as well as   S 3 has uncongested trips. In  Rule 1  it is equal  to  2 6       or   indicates how much  more likely we are to encounter  S 4  and  S 6  transaction if we  consider just those transactions where  S 3  S 5 and  S 8  have  uncongested trips. As compared to the entire population of  the transactions, it's t h e  confidenc e  divided by  support \(c   where the latter is expressed as a percentage   For  Rule 1 the  confidenc e is  100   support \(c  in  percentage   174/194\*100 = 89.6 9 So, th e    Lift ratio = 100/89.69.1 = 1.1   As it is clearly shown in  Figure 7  some r u les with  an improvement value less than one means this is an  unreliable rule. Whereas the rule with a value more than one  means this rule is better than random replica selection with  number of time equal to improvement value as it is shown in  Figure 8                   When improvement value is more than 1 it is better to use  EST to select replica sites, because it selects the sites able to  work simultaneously    I n  Figure  9  we sho w  the  comparison between EST and  traditional model using highest bandwidth a s  a criterion to  select the best replica. As we can observe our technique has  a better performance most of the times because it selects the  sites which have the stable links. In traditional method the  site which has the highest bandwidth does not always me a n  to be the best because sometimes this highest bandwidth  link can be congeste d   Let us declare more by the following  scenario  o f  Figure 1 0 suppose   S 0   be the computing site  and let   S 1  S 3   S 1 4   be replica sites  Red stars referring to  congested router s   Using traditional selection method the  file will be got from S14 since it has less number of Hops  routers\ and highest and also has highest bandwidth link         Figure 8. Improvement ratio for different rule s  Figure 9. Traditional selection strategy and ES T    
193 


               Using  ES T the replica   S 3    is selected as a best replica  because the link b etween  C S  and  R S  is uncongested     VII I   C ONCLUSIO N  In this paper we presented a dynamic replica  selection strategy that aims to adapt at ru n time its criteria to  flexible QoS binding contracts specified by the service  provider and/or the client. The adapta b ility feature  addressed by our replica selection strategy is inferred from  the observation that the basic metrics, which influence the  QoS that the user perceives when accessing a replica  depend directly on the application being replicated and on  the cli e nts\222 preferences. To reach this objective that, we  used   the concept of association rules of data mining  approach to the most stable links sites in order to reduce the  searching space the response time and network resources  consumed    A CKNOWLEDGEMENT S  Au t hors wish to express their sincere thanks to  Prof. Arun Agarwal, from GridLabs Department of  Computer and Information Sciences, University of  Hyderabad, India for providing all the infrastructural and  computational support required to carry out this work  His  academic suggestions to improve the quality of the work are  also highly appreciated and acknowledged   R EFERENCE S     M  Rashedur Rahma n   Ken Barke r   Reda Alhaj j    Replica  selection in grid environment:a dat a mining approac h    Distributed systems and grid computing \(DSGC\,pp: 695  226  700  2005    J. Gwertzman and M. Seltzer    The case for geographical  push  cashing  In Proceeding of the 5th Workshop on Hot ZTopic in  Operating Systems, 1995     R. Kavitha, I. Foster   Design and evaluation of replication  strategies for a high performance data gri d  in, Proceedings of  Computing and High Energy and, Nuclear P h ysics, 2001   S. Vazhkudai, J. Schopf, I. Foster   Predicting the performance of  wid e area data transfer s  in: 16th International PDPS, 2002   S. Vazhkudai, J. Schopf   Using regression techniques to predict  large data transfer s  in: Computing: Infrastru c ture and  Applications, The International Journal of High Performance  Computing Applications, IJHPCA , August, 2003   A. Abbas, Grid Computing    A Practical Guide to Technology  and  A PPLICATION S    2006   http://goc.pragm a grid.net/wiki/index.php/UoHy d   S. Vazhkudai, S Tuecke, I. Foster   Replica selection in the  globus data gri d  in: First IEEE/ACM International Conference  on Cluster Computing and the Grid, CCGrid 2001   J. Guyton and M. Schwartz   L o cating nearby copies of replicated  internet server s    In Proceeding of ACM SIGCOM M 222  95, 1995   A. Tirumala, J. Ferguson, Iperf 1.2   The TCP/UDP Bandwidth  Measurement Tool, 2002   R. Wolski, Dynamically forecasting network performance using  the Network Weat h er Service, Cluster Computing \(1998   Yunhong Gu, Robert L. Grossman   UDT: UD P based data  transfer for hig h speed wide area network s  Computer  Networks, Volume 51, Issue 7, 16 May  2007, Pages 1777 1799  Elsevier   R.M. Rahman, K. Barker, R. Alhajj   Predicting the performance  of GridFTP transfer s  in: Proceedings of IEEE Symposium of  Parallel and Distributed Systems, 2004, New Mexico, USA, p  238a   J. F. Kurose, K.W. Ross   Compute r  Networking A To p Down  Approach Featuring the Interne t 3rd edition   S. Venugopal, . R. Buyya,"The Gridbus Toolkit for Service  Oriented Grid and Utility Computing: An Overview and Status  Report"2004   R   Agrawal  T  Imielinski  A.Swami    Mining associatio n  rules  between sets of items in large database s  In: Proc. ACM  SIGMOD Intl. Conf. Management Data, 199 3  R  M Rahman, K Barker and R Alhajj   Replica selection  strategies in data gri d    Jou r nal of Parallel and Distributed  Computin g   Volume 68, Issue 1 2 Pages 156 1 1574, December  2008   A. Jaradat, R. Salleh and A. Abid   Imitating K Means to  Enhance Data Selectio n  Journal of Applied Sciences 9 \(19  356 9 3574, 2009, ISSN 181 2 5654, Asian Ne t work for Scientific  Informatio n 2009   S. Venugopal, . R. Buyya, K. Ramamohanarao, "A taxonomy of  Data Grids for distributed data sharing, management, and  processing". ACM Comput. Surv. 38, 1 \(Jun. 2006  AC M   New  York, NY, US A  http://www.resample.com/xlminer/help/Index.ht m  A   K Pujar i    Data mining technique s    Hyderabad : Universities  Press, 2002   G. Williams, M. Hegland and S. Roberts   A Data Mining  Tutoria l  IASTED International Conference on Parallel and  Distributed Computing and Networks P DCN\22298\ 14 December  199 8   T  Ceryen, and M. Kevin, 2005   Performance characterization of  decentralized algorithms for replica selection in dstributed object  system s  Proceedngs of 5th International Workshop on Software  and Performance, July 11  14, Palm a de Mallorca, Spain, pp  25 7 262    F  Corina, and M. Mesaac, 2003  A scalable replica selection  strategy based on flexible contract s  Proceedings of the 3rd  IEEE Workshop on Internet Applications, June 2 3 24, IEEE  Computer Society Washington, DC, USA p p: 9 5 99   R. M. almuttari, R. Wankar, A. Negi, C.R. Rao   Intelligent  Replica Selection Strategy for Data Gri d    In proceeding of the  1 0 t h  International conference on Parallel an d  Distributed  Proceedin g  Techniques and Applications  IEEE Computer  Society W a shington, DC  WorldComp2010, GCA2010   LasVega s   USA  Volume3  pp: 9 5 100  July 1 2 1 5 201 0   Cisco Distributed Director  http://www.cisco.com/warp/public/cc/pd/cxsr/dd/index.shtm l  M   Sayal, Y. Breitbart, P. Scheuermann, R  Vingralek   Selection  algorithms for replicated web server s  In Proceeding of the  Workshop on Internet Server Performance,1998   E. Zegura, M. Ammar, Z. Fei, and S. Bhattacharjee   Applicatio n layer anycasting: a se r ver selection architecture and  use in a replicated web servic e  IEEE/ACM Transactions on  Networking, vol. 8, no. 4, pp. 45 5 226 466, Aug. 2000     Figur e  10   Data Grid and their associated network geometr y   
194 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





