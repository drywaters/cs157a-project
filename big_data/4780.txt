Implementing Legacy-C Algorithms in FPGA Co-Processors for Performance Accelerated Smart Payloads Paula J Pingree Lucas J Scharenbroich Thomas A Werne Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena California 91109-8099 818-354-0587 818-354-5322 818-354-3008 Paula.J.Pingreegjpl.nasa.gov LucasJ.Scharenbroichgjpl.nasa.gov ThomasA.Wernegjpl.nasa.gov Abstract Accurate on-board classification of instrument data is used to increase science return by autonomously 1 INTROD identifying 
regions of interest for priority transmission or 1.1 SDI generating summary products to conserve transmission 1 2 SN bandwidth Due to on-board processing constraints such classification has been limited to using the simplest 2 SYM D functions on a small subset of the full instrument data 2 1 
MD FPGA co-processor designs for SVM classifiers will lead 2.1 VA to significant improvement in on-board classification 
2.3 IM capability and accuracy 3 
EU i 3 SEU MI We implemented a SWIL2 classifier developed for the 4 CONCLI Hyperion instrument on the EO-1 spacecraft on the Xilinx 5 ACKNON Virtex-4FX60 FPGA as a baseline challenge We have taken 6 REFERE advantage of Impulse CTM the commercially available C-to7 BIOGRA HDL tool by Impulse Accelerated Technologies 
which supports the development of highly parallel co-designed hardware algorithms from software and applications This paper describes our approach for implementing the Hyperion linear SVM on the Virtex-4FX FPGA as well as additional experiments with increased numbers of data 1.1 Smart i bands and a more sophisticated SVM kernel to show the potential for better on-board classification achieved with On board c 
embedded FPGAs over current in-flight capabilities.34 advanced scie Currently avai consumption boards and al Recently deve FPGAs suc versatility of embedded pri advantage of i same chip hardware/softand lower cost SBCs 2 a radiation-hard l Smart Payload SVM Support 
Vector Machine SWIL Sea Water 
Ice  Land 1-4244-1488-1/08/$25.00 
C 2008 IEEE IEEEAC paper#1230 Version 
2 Updated 2007:12:13 Christine Hartzell Georgia Institute of Technology Atlanta GA 30332 Gtg733w\(mail.gatech.edu TABLE OF CONTENTS iUCTION 1 WART PAYLOAD MOTIVATION 1 VMS FOR HYPERSPECTRAL CLASSIFICATION                                                                       2 EVELOPMENT FOR V4FX FPGA 3 ALIDATION  
4 PLEMENTATION  5 XTENSIONS  6 ITIGATION  7 TfbON  7 WLEDGEMENTS  7 CNCES 7 LPHY 8 1 INTRODUCTION Payload Motivation omputation has become a bottleneck for nce instrument and engineering capabilities ilable spacecraft processors have high power are expensive require additional interface re limited in their computational capabilities loped hybrid field-programmable 
gate arrays h as the Xilinx Virtex-4FX 1 offer the running diverse software applications on ocessors while at the same time taking reconfigurable hardware resources all on the package These tightly coupled ware co-designed systems are lower power t than general-purpose single-board computers ind promise breakthrough performance over lened SBCs leading to a new architecture for I development Table I 


Computational Platform Performance DMIPS RAD750 SBC 240 Xilinx Virtex-I1 Pro 450 Xilinx Virtex-4 680 TABLE I PERFORMANCE SBC vs EMBEDDED FPGAs Designs based on embedded FPGA processors also benefit from the following advantages over SBCs  Higher level of reuse  Reduced risk of obsolescence  Simplified modification and update  Increased implementation options through modularization We have selected the Xilinx ML4 10 evaluation platform Figure 1 for development and demonstration of selected Smart Payload concepts including the SVM implementation on the Virtex-4FX FPGA Figure 1 A good candidate for the development of a future instrument computer for space The Xilinx ML4 10 evaluation board comes with the V4FX6O FPGA that features two embedded PowerPC405 processors 3 1.2 SVMs for Hyperspectral Classification Support Vector Machines 4 have found broad application in general machine learning and classification tasks as well as onboard remote sensing 5 A SVM is a maximum margin classifier that finds a separating hyperplane between two labeled classes such that the distance to the nearest datum in each class is maximized Figure 2 By selecting such a maximum margin hyperplane the SVM classifier can exhibit better generalization to new data than other linear classification methods The goal of training a support vector machine is to learn a set of weights such that the sign of a weighted sum of dot products between the training data xi and a test vector t will correctly predict the class of the new data vector y  sgn wiKxi,t y e 1,+1 SVMs also incorporate the kernel trick 6 which allows them to be extended from purely linear to non-linear classifiers This trick is accomplished by formulating the training and testing algorithms in terms of dot products x,y and then replacing the dot products with a kernel function K\(x,y  E\(x y that represents a dot product after passing the arguments through some non-linear function X By cleverly constructing the kernel function the high-dimensional dot product can be computed efficiently Figure 2 Maximum margin separating hyperplane between two data classes The circled data points are the support vectors that lie on the margin SVMs are well suited to onboard autonomy applications They represent a state-of-the-practice method in machine learning and have a history of reasonable performance across many domains The property that makes SVMs particularly applicable is the asymmetry of computational effort in the training and testing stages of the algorithm Classifying new data points requires orders of magnitude less computation than training because the process of training a SVM requires solving a quadratic optimization problem This naively requires on the order of 0\(n 2 


operations where n is the number of training examples Faster algorithms that exploit the specific structure of the SVM optimization problem have been developed 7 but the training remains the primary computational bottleneck After a SVM is trained many of the weights wi will be equal to zero This means that these terms can be ignored in the classification formula The input vectors that have a corresponding non-zero weight are called support vectors Even more computational savings can be realized in the case of using a linear kernel function The weighted sum over the kernel function is associative so all the support vectors can be collapsed into a single vector with a single weight Reducing the number of support vectors is key to successfully deploying a SVM classifier onboard a spacecraft where there are severe constraints on the amount of CPU resources available Previously deployed classifiers 5 have used such reduced-set methods but were still constrained to operate on only a subset of the available classification features Removing such bottlenecks is critical to realizing the full potential of SVMs as an onboard autonomy tool Original/legacy main loop r\(c Lass_ index c lass_index-lLNUMJCLASSES c lass-index  PIXEL output  svm bicuses[cIass index  ref l8  svm-coefs[cLass index][0   ref l21  svm-coefs[cLass index   ref l31  svm-coefs[cLass index][2   ref 134  svm-coefs[cLass_index][3   ref 141  svm-coefs[cLass_index][4   ref 151  svm-coefs[cLass_index   ref 185  svm-coefs[cIass index][6   ref 111  svm-coefs[cLass_index][7   ref 1156  svm-coefs[cLass_index][8   ref 1210  svm-coefs[cLass index][9   ref 1213  svmcoefs[cLass_index][1  if c Iss index==O 11 output  best-vaLue  best-class  cIass index best-value  output Figure 3 FPGA Co-design for the SVM Algorithm 2 SVM DEVELOPMENT FOR V4FX FPGA JPL has developed SVM classification algorithms that can be used onboard spacecraft to identify high priority data for downlink to Earth and to provide onboard data analysis to enable rapid reaction to dynamic events To meet NASA's science objectives these classifiers detect flooding volcanic eruptions and sea ice break-up Current pixel-based machine learning and instrument autonomy algorithms that have successfully detected and identified various natural phenomena are flying on computational technologies such as the RAD6000 and Mongoose V processors that have limited computing power extremely limited active storage capabilities and are no longer considered state-of-the-art To date such on-board classification has been limited to using the simplest function a linear kernel on only a subset of the full instrument data 11 of 242 bands for Hyperion on EO1 We have implemented on the Virtex-4FX60 a linear SVM classification algorithm This migration to a low power high-speed FPGA computing platform adds flexibility and scalability to the system For the FPGA-based development of the SVM the previously software-only legacy algorithm is implemented in the FPGA hardware fabric to take advantage of high-speed parallel processing capabilities while the image file input and classification file output is managed within the embedded PowerPC processor Figure 3 illustrates the partitioned system The Producer is coded in a file called sw.c It reads an input image file containing 857,856 pixels and streams data to the SVM The Consumer also coded in sw.c streams data from the SVM and writes pixel classifications e.g snow water ice land cloud or unclassified to an output file The original legacy SVM code is put in a file called hw.c The SVM algorithm in hw.c was transformed from C-toHDL using the Impulse C tool set by Impulse 8 and simulated to validate execution of the co-designed system The sw.c program will execute on the V4FX embedded PowerPC processor and communicate with the hardwareaccelerated algorithm in the FPGA fabric Next the converted HDL code was synthesized for the Virtex-4FX FPGA using the Xilinx ISE  EDK development environment Synthesis determined the V4FX resource output for the SVM algorithm to be 0 0 0 0 0 0 1 Adders/Subtractors 6 bit 3 Adder/Subtractor 32 bit 1 Multipliers 32 bit 5 Comparators 32 bit 2 Floating Point Adders/Subtractors 32 bit 1 Floating Point Multipliers 32 bit This translates to the following resource utilization for the V4FX6O device on the ML4 10 development platform Table II 3 


Impulse C implementation The two implementations FPGA RESOURCES V4FX6O on ML41O produce identical classifications on a pixel-by-pixel basis  Number of SI T151 out of 2 O4 The combination of the good agreement of our results with Number of Slice Flip Flops 1290 out of 50560 2 the ASE results as well as the independence of the results Number of 4 input LUTt 1838 out o50560 3 from the software platform leads us to believe that our Number of F IFORAMB1 6s 2 out of 232 11 Numbe of DP48s 4 ou of 128 3 mpementatnsvad TABLE II IMPULSE C RESOURCE REPORT FOR SVM The color key is blue  water cyan ice dark purple snow lavender unclassified The results of the simulation effort were presented at the 2007 NASA Space Technology Conference 9 as a preliminary report of this on-going task 2.1 Validation The output of the Producer-SVM-Consumer path is a file composed of a column of integers indicating the resulting class of each pixel in the image This output file is then reformatted in Matlab'TM to the original pixel-wise dimensions of the image Additionally each class is assigned an arbitrary color and the number of pixels belonging to each class is tabulated We can then easily calculate the percentage of pixels belonging to each class and visualize the resulting file of classified pixels Validation was required in two facets of this project It was necessary to validate both the pixel classification results from the SVM and the Impulse C implementation of the SVM We began the classification process by comparing the pixel classification percentage results to those achieved on the SVM used in the ASE on the Earth Observingi Satellite The classification percentages show good agreement particularly for the snow and water classes Table III It is possible however for the raw percentage results to look reasonable while the pixel classification visualization shows no resemblance to the physical features in the image The visualizations were integral in our validation efforts Our resulting visualizations show excellent agreement with the results from the ASE SVM Figure 4 In addition to the qualitative comparison of the images we also conducted a pixel-by-pixel comparison of the ASE results and our classifications This comparison was made less accurate due to our lack of a raw classification data file for the ASE image The pixel-by-pixel classification comparison showed that 76.8 of the pixel classifications in our results matched those of the ASE results Figure 5  Table III We believe the discrepancies to be due to the differences in the training datasets of the SVMs A In order to dismiss the possibility of errors being introduced by the Impulse C implementation of the SVM we also Figure 4 A comparison of the results from a the Impulse C wrote a conceptually identical version of the code in C and SVM implementation b the ASE SVM and c the original compared the resulting output to that achieved by the hyperspectral image 4 


T______ _P ercent of Pixels classified ASE Run 0716 Agreement Snow 30.6 31.9 82.1 Water 31.0 31.1 81.7 Ice 3.0 7.3 28.8 Land 0.0 0.7 0.0 Cloud/unclassified 35.3 29.0 79.3 TABLE III A COMPARISON OF THE PERCENTAGES OF PIXELS CLASSIFIED IN EACH CLASS BETWEEN THE ASE SVM AND OUR IMPULSE C SVM IMPLEMENTATION THE AGREEMENT PERCENTAGES INDICATE THE PERCENT OF THE PIXELS CLASSIFIED IN EACH CLASS BY THE IMPULSE C SVM THAT WERE ASSIGNED TO THE SAME CLASS BY THE ASE SVM 2.2 Implementation In order to implement the SVM on the ML4 10 board a minor modification to the producer/consumer model was required The model requires the producer and consumer modules run concurrently so the producer-hardwareconsumer data flow necessitates a multi-threaded processing environment This design allowed for simultaneous bidirectional communication between software and hardware which permitted us to use small-depth buffers between hardware and software In lieu of running a multithreaded operating system on the PPC we combined the producer and consumer functions into a single function that alternated hardware read and write operations The communication between the software module and the hardware core was then implemented as two separate buffers see Figure 6 SWMod'Ul a NW ModuIJ 8 _elemnt 32bit FIFOs Figure 6 Hardware HW SW Modules After this small algorithmic change we used Impulse C to generate the hardware module Following a few trivial changes to the software an endian-swap using optimized printf functions etc the design was ready to be put onto the ML410 board For this project we used the following board resources a single PowerPC-405 PPC processor running at 100MHz a Processor Local Bus PLB a 256MB DDR2 DIMM a System ACE Compact Flash interface an On-Chip Peripheral Bus OPB a PLB-to-OPB bridge and a UART see Figure 7 In addition the hardware portion of the project was instantiated in the FPGA fabric PLB I_I A B j C Figure 5 The black pixels in image b indicate the indices where the classifications of pixels a and c were not identical Figure 7 FPGA Hierarchy 5 OPB 


The PPC ran the software portion of the task which sends data to and collects data from the SVM hardware module We chose to use the PPC instead of a Microblaze processor because the PPC can operate at triple the clock frequency of the Microblaze and the Microblaze would be instantiated in valuable FPGA fabric whereas the PPC exists external to the fabric Since the 256MB DIMM is the largest source of memory on the board we used it as main memory for the program The PLB is a high-speed bus compared to the OPB that allows for fast data transfer to/from the memory and SVM core peripherals The 16GB Compact Flash card was used to hold the input and output data files which are too large to fit on the DIMM The UART was used to for debugging output The OPB is a low-speed bus that is the default interface between the processor and the System Ace controller and UART peripherals We synthesized the design and ran it on the ML410 board The classification output is shown in Figure 8a 2.3 Extensions Having successfully implemented the legacy SVM designed for Hyperion we considered two extensions to the algorithm using a larger number of bands with the same linear kernel SVM and creating a new SVM with a nonlinear kernel For the expanded linear kernel SVM we arbitrarily selected 30 of the available 242 bands in the image For the nonlinear kernel SVM we used the same 11 bands as the legacy SVM with the kernel K\(x,y x,y  1 where x,y is the dot product of x and y Because training data was not available for the original legacy SVM we could not generate new SVMs that would be comparable to it so we used new training data to generate the two new SVMs then also generated a new 11-band linear-kernel SVM for comparison to the legacy SVM See Table IV for FPGA fabric utilization percentages for each of these SVMs Table V shows a runtime comparison of each SVM in a software-only implementation PPC+FPU and in the Impulse C co-designed implementation PPC  HW TABLE IV PERCENT FABRIC UTILIZATION FOR SVMs  in..ar II Ihangd5 Lanear 3\(2 band3 1 Poomial Lices Stke Flip Flops 4iput LUIs FIF016/RAM.B16S IDSP4$s 41 3 8 8 6 1 9 la TABLE V SVM RuN-TIME COMPARISON IN MIN SEC Speedup itssirer P PP  24 5.8 10.2 C Figure 8 A comparison of the results from a the 11-band linear SVM hardware implementation b the 30-band linear SVM hardware implementation c the polynomial SVM hardware implementation to d the original hyperspectral image Color differences between image b and the other 2 images a  c are due to the different bands qty 11 vs 30 selected for each classification 6 45 3:11 A 112 3 F=28 29:13 Spe r IPHW ll1band Liear I MynomLal 2.04 5.,3 32:24 0:52 1 


The hardware implementation of these SVMs produced results that agree very well with the software simulations of the algorithms Figure 8b shows the 30-band linear SVM classification output Figure 8c shows the polynomial SVM classification output See Table VI for a summary of classification disagreement for each of the three SVMs These disagreements may be due to floating-point hardware implementation differences between the FPGA hardware and the processor that ran the software simulations SW/HW Implementation SVM Classifier Difference 1 11-band Linear 0.34 30-band Linear 0*19  Polynomial 1.23 TABLE VI CLASSIFICATION DISAGREEMENT PERCENTAGE BEWTEEN SOFTWARE SIMULATION  PHYSICAL IMPLEMENTATION 3 SEU MITIGATION Space-flight qualified FPGAs are susceptible to radiation single event upsets SEUs therefore this issue must be addressed for the SVM V4FX design to be flight-ready The expected SEU rates of Rad-Hard flight processors such as the RAD750 in a GEO environment is on the order of 1 error every 5-10 years Expected SEU rates for the Xilinx Virtex FPGA are approximately one error per week Recent data from similar FPGAs flown on JPL's Mars Exploration Rovers validate these predictions 10 It should be noted that next generation Xilinx parts such as the Virtex-4 are expected to be produced on CMOS SOI process lines providing an order of magnitude improvement in SEU rate as well as other speed/power and radiation tolerance improvements In order to achieve parity with Rad-Hard processors we must reduce the SEU error rates by approximately two orders of magnitude and do this in a way that is relatively transparent to the application Future work toward this goal could use the Xilinx Triple Modular Redundancy TMR Tool 11 to triplicate logic as there are sufficient remaining resources as well as run the dual-core processors in lock-step The simplest approach may be to include only SEU detection in the design and when detection occurs re-load the FPGA configuration file This is a viable strategy for non-critical applications that can withstand occasional interruption for re-configuration Partial reconfiguration is another possible solution albeit more complex to implement where only the effected portion of the FPGA needs to be configured 4 CONCLUSION FPGAs with embedded processing capabilities are demonstrating breakthrough performance previously impossible with traditional processors This paper presented results from the synthesis of a legacy software SVM classification algorithm to the Xilinx V4FX60 FPGA platform as well as two extensions to demonstrate the increased capabilities of this implementation Using commercially available C-to-HDL translation tools this work was made possible under very limited funding Hardware acceleration of legacy software algorithms such as the described SVMs promises to provide needed capability for more advanced on-board data processing in future science missions While the current method is to implement only those software classification algorithms that will fit within very constrained on-board processing resources with embedded FPGAs such as the V4FX60 increasingly advanced SVMs may be implemented with room to grow in on-board resources Our results demonstrate that our most advanced extension the 2,1 polynomial kernel is achieved with only 9\2600 utilization of the FPGAs DSPs Imagine the possibilities 5 ACKNOWLEDGEMENTS Abdullah Aljabri their vision and development JPL and Charles D Norton JPL for support of Smart Payload technology 6 REFERENCES 1 Virtex-4 Family Overview DS112 v1.5 Xilinx Inc San Jose CA 2006 Available 2 MCP750 CompactPCI Host Slot Processor Motorola Computer Group Temple AZ 2001 Available 3 Xilinx Development Boards Xilinx Inc San Jose CA 2006 Available 4 C Cortez and V Vapnik Support vector networks Machine Learning 20 1995 273 279 5 Rebecca Castano Ngia Tang Thomas Dogget Steve Chien Dominic Mazzoni Ron Greely Ben Cichy and Ashley Davis Onboard classifiers for science event detection on a remote sensing spacecraft Proceedings of the 12th ACM SIGKDD International conference of 7 


Knowledge Discovery and Data Mining ACM Press 2006 845 851 6 M Aizerman E Braverman and L Rozonoer Theoretical foundations of the potential function method in pattern recognition learning Automation and Remote Control 25 1964 821 837 7 J Platt Sequential Minimal Optimization A Fast Algorithm for Training Support Vector Machines Microsoft Research Technical Report MSR-TR-98-14 1998 8 David Pellerin and Scott Thibault Practical FPGA Programming in C Prentice Hall 2005 9 P Pingree C Norton Smart Payload Development for High Data Rate Instrument Systems Proceedings of the NASA Space Technology Conference 2007 10 J George R Koga G Swift G Allen C Carmichael and C W Tseng Single Event Upsets in Xilinx Virtex-4 FPGA Devices pre-publication paper 2006 11 Xilinx TMRTool Fact Sheet The research described in this paper was carried out by the Jet Propulsion Laboratory California Institute of Technology under a contract with the National Aeronautics and Space Administration 7 BIOGRAPHY Paula Pingree is a Senior Engineer in the Instruments and Science Data Systems Division at JPL She has been involved in the design integration test and operation of several JPL flight projects the most recent being Deep Impact DI where she was Test Bench Manager pre-launch and Co-Lead of the Impactor Comet Encounter activity postlaunch She is presently the Electronics CogE for the Microwave Radiometer instrument on the Juno spacecraft planned for a 2011 launch to Jupiter She also enjoys research and technology development for Smart Payloads such as this paper presents Paula has a Bachelor of Engineering degree in Electrical Engineering from Stevens Institute of Technology in Hoboken NJ and a Master of Science in Electrical Engineering from California State University Northridge She is a member of IEEE Lucas Scharenbroich is a Researcher in the Machine Learning and Instrument Autonomy group at JPL He has been involved with the design implementation and deployment of machine learning algorithms to enable autonomous science capabilities primarily through the Autonomous Sciencecraft Experiment ASE He is presently involved in the application of machine learning to automated code generation crop yield prediction and object tracking in remote sensing data Lucas has Bachelor of Science degrees in Electrical Engineering and Compute Science from the University of Minnesota Duluth and a Master of Science in Information and Computer Science from the University of California Irvine Thomas Werne is an Associate Engineer in the Model Based Systems Engineering and Architectures group at JPL He is currently working on implementing FPGA-based technology for Smart Payload applications Thomas has Bachelor of Science degrees in Electrical Engineering and Mathematics and a Master of Engineering in Electrical and Computer Engineering from Rose-Hulman Institute of Technology He is a member of IEEE Christine Hartzell is an undergraduate Aerospace Engineering student at the Georgia Institute of Technology GIT She will be graduating in May 2008 and then pursuing a PhD in Aerospace Engineering with a focus on Space System Design Her research has focused on bioastronautics instrument design methods heatshield design and trajectory selection in labs at GIT and at the Jet Propulsion Laboratory JPL 8 


Communities and Commercial Success: Individual and community-level theory grounded in the atypical case of TimeZone.Com," Journal of Management, 27, 2001, pp 297-312 55  T  R Gr a e ff  U sin g P r omo tio na l M e ssa ge s to M a n a g e  the Effect of Brand Self Image on Brand Evalution Journal of Consumer Marketing, 1996, 13, pp. 4-18  T  W   Bri g n a l l  a n d T  L  V  V a l e y   A n ol i n e co m m uni t y  as a new tribalism: The World of Warcraft," Proceedings of the 40 th Hawaii International Conference on System Science, Big Island, Hawaii, 2007   J. Y i an d S  A   L a  B ran d Pers on alit y  Bran d  Identification - Brand Equity Model : An Exploratory Study on the Difference Between Users vs. Non … Users Marketing Research, 17\(3\02, pp. 1-33  W i k e pedia M M ORP G \(h ttp://e n  w i k e pedia org/wiki/MMORPG   K. L ee, an d J.I K w o n   T h e Inf l u en ce of Appropriation and on Performance in Online Game Focusing on MMORPG The Journal of MIS Research Vol. 16, No. 4, 2006  W i k e pedia, th e Free En c y clopedia, MMOR P G   available in http:// en.wikipedia.org/ siki/MMORPG#Aca  demic_attention\, 2006  e De v e lopm e n t  Prom otio n Ins tit u t e   2006 The Rise of Korean Games 2006  w w  m m o rpg c h art.co m  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


 10  Early Career Award for Scientists and  Engineers PECASE in 2000 He has been co investigator on the NASA/ ESA SOHO spacecraft and the CNES Picard mission Michael is a member of the IEEE Computer Society the Institute for Mathematical Statistics and AAAS  Jay Parker is a Senior Scientist in the Satellite Geodesy and Geodynamics group of the Jet Propulsion Laboratory California Ins titute of Technology His graduate research used computer simulations to explain mesospheric ionization response to solar flares and dynamic instabilities Dr Parker's research subjects at the Jet Propulsion Laboratory include a variety of topics in remote  sensing analysis and modeling  These include supercomputing algorithms for electromagnetic scattering and radiation satellite geodesy and finite element simulation for earthquake related deformation and ocean sensing through GPS signal reflection He i s currently the software engineer and co investigator for the QuakeSim project which has developed a solid Earth science framework including a variety of simulation and analysis tools He also develops the SEASCRAPE software system for high fidelity simul ation and parametric retrieval of atmospheric infrared spectrometry at Remote Sensing Analysis Systems Inc of Altadena CA Dr Parker is a member of the American Geophysical Union and co chair of the Data Understanding and Assimilation working group of t he APEC Cooperation for Earthquake Simulation   


 11 This model specifies that covariates act multiplicatively on time t r than on the hazar d function.  That is, we assume a baseline hazard function to exist and that the effects of the covariates are to alter the rate at which an individual proceeds along the time axis.  In other words, the covariates z accelerates or decelerates the time to failure Kalbfleisch and Prentice 2002, Lawless 2003  It should be pointed out that the distribution-based regression models for exponential and Weibull distributions in the previous section are th e special cases of both PHM and AFT.  This correspondence is not necessarily true for models based on other distribu tions. Indeed, two-parameter Weibull distribution has the uniq ue property that it is closed under both multiplication of fa ilure time and multiplication of the hazard function by an arbitrary nonzero constant Lawless 2003, Kalbfleisch & Prentice 2002, Klein Moeschberger 2003  2.6. Counting Process and Survival Analysis   In the previous sections, we introduced censoring and survival analysis models th at can handle the censored information; however, we did not discuss how the censored information is processed.  Accommodating and maximally utilizing the partial information from the censored observations is the most challenging and also the most rewarding task in survival anal ysis.  This also establishes survival analysis as a unique fiel d in mathematical statistics Early statistical inferences for censored data in survival analysis were dependent on asymptotic likelihood theory Severini 2000\ Cox \(1972, 1975\ proposed partial likelihood as an extension to classical maximum likelihood estimation in the context of hi s proportional hazards model as a major contribution. Asymptotic likelihood has been and still is the dominant theory for developing survival analysis inference and hypothesis testing methods \(Klein and Moeschberger 2003, Severini 2000\. There are many monographs and textbooks of survival analysis containing sufficient details for applying survival analysis \(Cox and Oakes 1984, Kalbfleisch and Prentice 1980, 2002, Lawless 1982, 2003, Klein and Moeschberger, 2003\. A problem with traditional asymptotic lik elihood theory is that the resulting procedures can become very complicated when handling more complex censoring mechanisms \(Klein Moeschberger 2003\. A more elegant but requiring rigorous measure-theoretic probability theo ry is the approach with counting stochastic processes and the Martingale central limit theorems.  Indeed, this approach was used by Aalen 1975\ to set the rigorous mathematical foundation for survival analysis, and later further developed and summarized by Fleming and Harrington \(1991\, Andersen et al. \(1993\several research papers.  In reliability theory Aven and Jensen \(1999\ dem onstrated such an approach by developing a general failure model, which we briefly introduced in Section 1.2. However, the counting process and Martingale approach require measure theoretic treatments of probability and st ochastic processes, which is often not used in engineering or applied statistics.  A detailed introduction of the topic is obviously beyond the scope of this paper, and we only present a brief sketch of the most important concepts involved.  Readers are referred to the excellent monographs by Andersen et al. \(1993 Fleming and Harrington \(1991\ Aven and Jensen \(1999\ for comprehensive details, and Kal bfleisch and Prentice \(2002 Klein and Moeschberger \(2003\, Lawless \(2003\ for more application–oriented treatments The following discussion on this topic is drawn from Klein and Moeschberger \(2003  A counting stochastic process N  t  t 0 possesses the properties that N  0 ro and N  t   with probability one. The sample paths of N  t ht continuous and piecewise constant with jumps of size 1  step function In a right-censored sample, \(we assume only right censoring in this section N i  t  I  T i t  i   which keep the value 0 until individual i fails and then jump to 1  are counting processes. The accumulation of N i  t ocess     1 t N t N n i i is again a counting process, which counts the number of failures in the sample at or before time t   The counting process keeps track of the information on the occurrences of events,   for instance, the history information such as which individual was censored prior to time t and which individual died at or prior to time t as well as the covariates information This accumulated history information of the counting process at time t is termed filtration at time t denoted by F t For a given problem F t  rests on the observer of the counting process.  Thus, two observers with different recordings at different times will get different filtrations.  This is what Aven and Jensen 1999\ referred to as different information levels or the amount of actual available information about the state of a system may vary  If the failure times X i and censoring times C i  are independent,  then the probability of an event occurs at time t given the history just prior to t  F t\n be expressed as  t T if dt t h t C t X dt t C dt t X t P F dt t T t P i i i i i i r t i i r          1     t T if F dt t T t P i t i i r 0   1    51  Let dN  t be the change in the process N  t over a short time interval    t t t Ignoring the neglig ible chance of ties 1   t dN if a failure occurred and 0   t dN otherwise  Let Y  t denote the number of individuals with an observation time T i t Then the conditional expectation of dN  t   dt t h t Y F dt t C dt t X t with ns observatio of number E F t dN E t i i i t              52 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es — the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


