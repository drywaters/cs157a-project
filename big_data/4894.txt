On Finding Similar Items in a Stream of Transactions Andrea Campagna and Rasmus Pagh IT University of Copenhagen Denmark Email f acam,pagh g itu.dk Abstract While there has been a lot of work on 002nding frequent itemsets in transaction data streams none of these solve the problem of 002nding similar pairs according to standard similarity measures This paper is a 002rst attempt at dealing with this arguably more important problem We start out with a negative result that also explains the lack of theoretical upper bounds on the space usage of data mining algorithms for 002nding frequent itemsets Any algorithm that even only approximately and with a chance of error 002nds the most frequent k itemset must use space 012\(min 
f mb n k   mb  k g  bits where mb is the number of items in the stream so far n is the number of distinct items and  is a support threshold To achieve any non-trivial space upper bound we must thus abandon a worst-case assumption on the data stream We work under the model that the transactions come in random order and show that surprisingly not only is small-space similarity mining possible for the most common similarity measures but the mining accuracy improves with the length of the stream for any 002xed support threshold Keywords 
algorithms streaming sampling data mining association rules I I NTRODUCTION Imagine that we have a set of m sets transactions each a subset of f 1      n g  and that we want to 002nd interesting associations among items in these transactions This problem is often framed in a market basket model where we are interested in 002nding those pairs of items that are frequently bought together Whether a pattern is really interesting or not is a problem dependent question and for this reason various similarity measures other than number of co-occurrences have been introduced Some of the most common measures are Jaccard  cosine  and 
all con\002dence  3 Besides these measures we are also interested in association rules which are intimately related to the overlap coef\002cient similarity measure See 4 Chapter  for background and discussion of similarity measures We initiate the study of this problem in the streaming model where transactions arrive one by one and we are allowed limited time per transaction and very small space The latter constraint implies we cannot hope to store much information regarding pairs that are not similar and moreover we cannot store the input In particular classical frequent item set algorithms such as Apriori and FPgrowth that w ork i n se v eral passes o v er the data cannot be used The survey of Jiang and Gruenwald gi v es a good overview of the challenges in data stream association mining 
Pre vious w orks on transaction data streams ha v e focused on 002nding frequent itemsets and can be classi\002ed in the following way Landmark model The frequent itemsets are searched for in the whole stream so that itemsets that appeared in the far past have the same importance as recent ones Damped model This model is also called Time-Fading  Recent transactions have a higher weight than the older ones so nearer itemsets are considered more interesting than the further Sliding window Only a part of the stream is considered at a given time in this model the one falling in the sliding window This implies storing information concerning the transactions falling within the window since whenever a transaction gets out of the window span it has to be removed from the counts of the itemsets The last two models make the problem of achieving low 
space usage simpler since most of the information in the stream has little or no effect on the mining result The challenge is instead to handle the real-time requirements of data stream settings All these approaches look for frequent items and do not try to compute any similarity relying on the tacit assumption that whatever is frequent is automatically interesting This assumption is not always true Example Suppose we have item 1 appearing in 20 of transactions item 2 appearing in 20 of transactions and the pair f 1  2 g appears in 10 of transactions Suppose moreover that the pair f 
3  4 g appears in only 5 of transactions and that these transactions are the only ones in which 3 and 4 appear The set f 1  2 g has a frequency that is two times the one of f 3  4 g  But looking at the similarity function cosine  we can easily realize that s 1  2  10  20  0  5 while s 
3  4  5  5  1  If we base the idea of similarity only on frequencies we are likely to miss the pair f 3  4 g which holds a much higher similarity than the more frequent pair f 1  2 g  Notice also that f 3  4 g holds a higher similarity for all the measures we are addressing so the example shows how frequencies alone do not suf\002ce to infer similarity properties of pairs 016 
2010 IEEE International Conference on Data Mining Workshops 978-0-7695-4257-7/10 $26.00 © 2010 IEEE DOI 10.1109/ICDMW.2010.152 121 
2010 IEEE International Conference on Data Mining Workshops 978-0-7695-4257-7/10 $26.00 © 2010 IEEE DOI 10.1109/ICDMW.2010.152 121 


Our contributions In this paper we address the problem of 002nding similar pairs in a stream of transactions We 002rst show a negative result which is that a worst-case stream does not allow solutions with non-trivial space usage To approximate even the simplest similarity measure one essentially needs space that would be suf\002cient to store either the number of occurrences of all pairs or the contents of the stream itself Imposing a minimum support  for the items we are interested in alleviates the problem only when  is close to the number of transactions Theorem 1 Given a constant k  0  and integers m  n    consider inputs of m transactions of total size mk with n distinct items Let s max denote the highest support among k itemsets where each item has support  or more Any algorithm that makes a single pass over the transactions and estimates s max within a factor 013  2 with error probability 016  1  2 must use space 012\(min m n k   m  k  bits in expectation on a worst-case input distribution 016 This lower bound extends and strengthens a lower bound for single-item streams presented in Of course many data streams may not exhibit worst-case behavior Several papers have considered models of data streams where the items are supposed to be independently chosen from some distribution or presented in random order  W e present an upper bound that w orks for a worst-case set of transactions under the condition that it is presented in random order which is suf\002cient to bypass the lower bound Our method is general in the sense that it can evaluate the similarity of pairs according to several well-established measure functions We note that outside the streaming domain distributed sorting algorithms such as the one built into MapReduce can be used to permute transactions in random order by using random values as keys It seems likely that our approach can also be used in a 1-pass MapReduce implementation Theorem 2 Let 016  0 be constant and s  M  1 be integers We consider a data stream of transactions subsets of f 1     n g  of maximum size M  where in each pre\002x the set of transactions appears in random order For all the similarity measures in 002gure 1 there is a streaming algorithm depending on s and M  that maintains a  1 006 016 approximation of the s most similar high-support pairs in the stream as follows Within the m transactions seen so far let 001 be the s th highest similarity among pairs f i j g where both i and j appear at least  times where  can be any function of m  There exists L  O log mn  such that if 001  L  max 032 q mbM s  M 033  then the pairs maintained all have similarity at least 1 000 016  with high probability and all such pairs with similarity 1  016  or more are reported To process a pre\002x of mb items the algorithm uses time O  mb log nm   with high probability and space O  n  s   016 It is worth noticing that s can be chosen as O  n   which yields a space usage linear in the number of distinct items Conversely choosing s smaller does not improve the space usage so we may assume s 025 n  In absence of a known bound on the maximum transaction size one can use M  n  Then the algorithm guarantees to detect pairs with similarity at least L  max n p mb n o  Using s 025 n and ignoring the logarithmic factor L this means that up to input size mb  n 2 we can detect similarity n  and after this point we can detect similarity p mb  Assuming that  is chosen as a linear function of m relative support threshold we see that the accuracy improves with the length of the stream A Previous work Denote by m the number of transactions seen up to the moment in which we want to report the similar pairs Let n indicate the number of distinct items that can appear in transactions Without loss of generality we can assume these items are in the set f 1      n g  Parameter b is the average length of transactions such that mb is the size of the data set seen so far Most of the algorithms we describe actually consider the problem of 002nding frequent objects in a stream of items so they do not focus on itemsets like we do But given a stream of transactions we can of course generate the stream of all pairs occurring in these transactions and feed them to a frequent item algorithm We do not consider here that this might not be possible for large transactions in settings where real-time constraints are important In the following we let M 2 denote the length of the derived stream of pairs Landmark model Many papers have addressed the problem of frequent items in a stream Starting from the seminal paper streaming algorithms ha v e started to 003o w in recent years Many important contributions to the problem of frequent items and indirectly frequent itemsets have thus been presented In several independent papers 15 16 algorithms have been presented that can 002nd all pairs with support at least k using space M 2   k 000 1 and constant time per pair in the stream These algorithms may generate false positives i.e it is only known that the output will contain the frequent pairs Cormode and Muthukrishnan consider the problem of reporting hot items in a fully dynamic database scenario The space usage is similar to the schemes above but the error probability can be reduced arbitrarily at the cost of space Also in is a lo wer bound on the number of bits of memory necessary in order to answer queries that concern reporting the items with frequencies over a certain threshold This lower bound is extended and generalized by our lower bound in theorem 1 
122 
122 


In the C OUNT S KETCH algorithm tackles the problem of reporting the k most frequent itemsets For worstcase distributions their algorithm has similar performance to those mentioned above but for skewed distributions they are able to detect itemsets with smaller frequencies in the same amount of space A false negative approach Yu et al present algorithms directly addressing the problem of 002nding frequent itemsets in a transaction stream The algorithm does not 002nd itemsets that are similar by means of measure functions other than support Under the assumption that items occur independently which is arguably quite strong since we are assuming that there may be dependencies resulting in frequent sets the authors show upper bounds on space usage similar to those of The performance is tested on arti\002cial data sets where the independence assumption holds For itemsets of size two or more the paper lacks a theoretical analysis of the proposed algorithm but claims an empirical space usage bounded by m 3 k 3  Sampling according to the similarity Our algorithms builds on top of an idea presented in 19 The sampling technique used in that algorithm is such that pairs are sampled a number of times that is proportional to their similarity A more technical explanation can be found in section III-A where we improve the sampling procedure to make it suitable for a streaming environment The algorithms presented in 19 ha v e near opt imal running time when no information on the distribution of similarities are given As a matter of fact the running time is linear in the size of the input and output when there are many pairs of roughly the same similarity The methods presented are highly general and apply to many measure functions that are linear in the number of occurrences of a pair However the method does not directly apply to a streaming setting since it needs two passes over the data II L OWER BOUND There are two na  021ve approaches to handling k itemset support counting in a data stream setting One consists in storing all the transactions seen possibly trying to compress the representation and the other one maintains support counts for all k itemsets seen so far Theorem 1 says that it is not possible to beat the best of these approaches in the worst case with support threshold   1  The proof is a reduction from communication complexity Proof The inputs considered for the lower bound have m transactions of size k  Let n 0  min n b mk 2   c  000 1 be the largest possible number of items that can appear  times in m 2 transactions minus 1  We pick an arbitrary set F of n 0 items and will form an input stream that consists of two parts 017 In the 002rst m 2 transactions we ensure that each item in F appears  times or more while no k subset of F appears This can be done by putting one item not in F in each transaction 017 In the last m 2 transactions we encode information that will require many bits to store as detailed below Consider the 002rst s  min m 2  000 n 0 k 001  transactions in the second part Since s 024 000 n 0 k 001 we can map the numbers f 1      s g to unique k itemsets in F  In particular any bit string x 2 f 0  1 g s can be mapped to the unique set of transactions corresponding to the positions of 1 s in x  In this data set each k itemset from F appears at most once Suppose we have an algorithm that can determine the support of the most frequent itemset within a factor 013  2 with probability 1 000 016  This implies that on inputs where no itemset appears more than twice the algorithm can distinguish with probability 1 000 016  the cases where the most frequent itemset appears once and twice Given x 2 f 0  1 g s we consider the memory con\002guration after the algorithm has seen the set of transactions that correspond to x  This can be seen as a message that encodes suf\002cient information on x that allows us to determine if one of the itemsets we have seen appears later in the stream Lower bounds from communication complexity see 20 Example  tell us that e v en when we allo w error probability 016  1  2 the amount of communication to determine whether x y 2 f 0  1 g s have a 1 in the same position corresponding to the same k itemset appearing twice is 012 s  bits in expectation This means that the memory representation even if it is compressed must use 012 s  bits Using the estimate 000 n 0 k 001 025 min 000 n k 001  000 mk 3   k 001   012\(min n k   m  k  we get the lower bound stated in the theorem Corollary 3 Any deterministic algorithm that determines the highest support in a transaction data stream must after having processed transactions of total size mb  use space 012\(min mb n k  bits on a worst-case input 016 III O UR ALGORITHM We present a new algorithm for extracting similar pairs from a set of transactions using only one pass over the data The algorithm is approximate so false negatives and false positives occur Most of our discussion will concern space usage but we are also aiming for very low per-item time complexity of the algorithm In particular we will not allow anything like iterating through all pairs in a transaction The measures we will address are reported in Figure 1 and are all symmetric This means that we are interested only in looking at pairs  i j  where i  j  For this reason we will use set notation for the pairs so instead of  i j  we will write f i j g  Parameters of the algorithm We recall that  is the item support threshold and M is the maximal transaction size Increasing  will decrease the minimum similarity the algorithm will be able to spot M is a characteristic of the transactions supplied as a parameter to the algorithm In absence of a known bound on M  one can set M  n  The 
123 
123 


Measure s  i j  f  j S i j  j S j j  Cosine j S i  S j j p j S i jj S j j 1  p j S i j 001 j S j j Dice j S i  S j j j S i j  j S j j 1   j S i j  j S j j  All con\002dence j S i  S j j max j S i j  j S j j  1  max j S i j  j S j j  Overlap coef j S i  S j j min j S i j  j S j j  1  min j S i j  j S j j  Figure 1 Measures that we cover with our algorithm and the corresponding functions The overlap coef\002cient measure has the property that 002nding pairs having similarity over a certain threshold implies 002nding all association rules with con\002dence over that the same threshold As argued in 19 Jaccard similarity can be handled via dice similarity Pre\002x T 1  T 2      T m Pair sampling T  m 2\+1      T m     new counts previous counts S AMPLE C OUNT f i j g f i p g f p q g    f i j g Similar Pairs Figure 2 Overview of the algorithm with all its components parameter s determines the space usage of the algorithm which is O  n  s  words Notation In the streaming framework the total number of transactions is not known In order to address this issue we consider sets of transactions pre\002xes  of the stream of increasing size Suppose that so far we have seen m transactions T 1      T m 022 f 1      n g  The current pre\002x has length 2 t  t 2 N f 0 g when m falls in the interval 2 t  2 t 1   Our algorithm maintains counts of all items and store copies of the counts every time the current pre\002x changes that is every time the number of transactions seen is two times the length of the current pre\002x Each time the current pre\002x changes we update our estimate of the most similar pairs and use this estimate until the next change of current pre\002x The algorithm is based on two pipelined stages a stream of pairs generation phase and a store and count phase We will describe the two phases separately since the output of the former phase will constitute the input of the latter Figure 2 gives an overview of the algorithm The pre\002xes of the stream are fed to a pair sampling stage that uses the stored counts from the previous pre\002x to compute sampling probabilities Given the current pre\002x the counts relative to that pre\002x will be used in order to sample pairs in the stream until a new set of counts is stored for the pre\002x of length 2 t 1 The idea is that since transactions come in random order the sampling probabilities should be approximately the same as for the B I S AM sampling procedure which bases the sampling probabilities on exact item frequencies In section IV we show how this technique samples with high probability the pairs having a high enough similarity In fact we show that a stronger property holds with high probability Even when we split the stream into 024 chunks each with the same number of transactions we will sample these pairs suf\002ciently often in each chunk to reliably estimate their similarity A Pair sampling We base our technique on the sampling method of the B I S AM algorithm 19 F or each transaction the pairs are sampled according to their support such that the pair f i j g is sampled with probability 034 f  j S i j  j S j j   where f is a function that depends on the similarity measure considered and 034 is a parameter that is used to control the sampling rate We 002x 034  4  M  where the number of chunks 024 is given by equation 6 BiSam idea The idea is that after both i and j have appeared  times the expected number of times f i j g is sampled is proportional to s  i j   Also the number of samples follows a highly concentrated binomial distribution so the true similarity can be estimated reliably for pairs that are sampled suf\002ciently often For any f that is non-increasing in both parameters the B I S AM algorithm performs the sampling in time that is expected linear in the transaction size plus the number of samples However the time to process a transaction may be quadratic with nonnegligible probability which is problematic for application in a streaming context We refer to 19 for details Streaming adaptation Two things allow us to arrive at a version suitable for streaming 017 While B I S AM produces dependent samples in the sense that the number of times two different itemsets is sampled is not independent we show how to make the samples produced independent This will ensure that the number of samples from each transaction is highly concentrated around its expectation 017 The requirement of minimum support  will ensure that processing of a single transaction takes linear time with high probability More precisely Any set of consecutive transactions with a total of log m items will require linear time with high probability To achieve independence we will change the sampling probabilities by rounding them down to the nearest negative power of 2 This means that the expected number of times f i j g is sampled is no longer exactly proportional to s  i j   but is changed by a factor 015 i;j 2 1    However since the 
124 
124 


sampling probability is known which means that 015 i;j will be constant for any given f i j g  we can still use the sample counts to reliably estimate similarity Details For a transaction T t we can visualize the pairs in T t 002 T t as a 2-dimensional table with rows and columns sorted by support where we are interested in the pairs below the diagonal index i  j  Since f is non-increasing the sampling probabilities are decreasing in each row and column This means that for any k  0  in time O  j T t j  we can determine what interval in each row of the table is to be sampled with probability 2 000 k  To produce the part of the sample for one such interval we describe a method for producing a random sample of S  f 1      036 g  for a given integer 036  where each number is sampled with the same probability p  Since p\036 may be much smaller than 036  we want the time to depend on the number of samples rather than on 036  This can be achieved using a simple recursive procedure similar to the one used in ef\002cient implementations of reservoir sampling With probability 1 000 p  036 we return an empty sample Otherwise we choose one random element x from S  and recursively take a sample of the set S nf x g with sampling probability p  The set S can be maintained in an array where sampled numbers are marked In case more than half of the numbers are marked we construct a new array containing only unmarked numbers the amortized cost of this is constant per marking To select a random unmarked number we sample until one is found which takes expected O 1 time because no more than half of the numbers are marked In summary for each sampling probability 2 000 k we can compute the corresponding part of the sample in expected time O  j T i j  z k   where z k is the number of samples This is done for k  1  2      2 log nm   Sampling probabilities smaller than  nm  000 2 are ignored since the probability that any such pair would be sampled in any transaction is less than 1 m  That is with high probability ignoring such pairs does not in\003uence the sample To state our result let 2 000 N denote the set of negative integer powers of 2 Lemma 4 Let  f  N 002 N  2 000 N be non-increasing in both parameters Given a transaction T t and support counts j S i j for its items in expected time O  j T t j log nm   z  we can produce a random sample of z 2-subsets of T t such that 017 f i j g is sampled with probability  f  j S i j  j S j j  if  f  j S i j  j S j j    nm  000 2  and otherwise with probability 0  and 017 the samples are independent 016 For all similarity measures in 002gure 1 and any feasible value of 034  the minimum support requirement will ensure that the expected number of samples in a transaction is at most j T t j  This means that for each transaction T t  the time spent is O  j T t j log nm  with high probability B SampleCount This phase sees the stream of pairs generated by the pair sampling and has to 002lter out as many low similarity pairs as possible while successfully identifying high similarity pairs By the properties of pair sampling this is essentially the task of identifying frequent pairs in the stream of samples We aim for space usage that is smaller than that of standard algorithms for frequent item mining in a data stream In order to accomplish this we use a modi\002cation of an algorithm by Demaine et al Their algorithm 002nds frequent items in a randomly permuted stream of items and so does not directly apply to our setting where only the transactions are assumed to come in random order Demaine et al are able to sample random elements by simply taking the 002rst elements from the stream This would not work in our setting where all these elements might be pairs coming from the same transaction Reservoir sampling Instead we use a reservoir sampling method W e sk etch the mechanism here and we refer to the original paper for a complete description Suppose we have a sequence of d items and we want to sample a random subset of the sequence We 002rst of all put in the sample the 002rst s elements that we see For each subsequent element in position t  s  we will put it in the sample with probability s=t  When a new element has to be included in the sample another one that is already part of the sample has to be evicted Each element of the set of samples will be chosen as the victim with probability 1 s  This technique ensures we will end up with a set of samples that is a true random sample of size s  SampleCount We consider the stream of pairs divided into 024 chunks The pair sampling generates these chunks such that each chunk corresponds to some set of transactions i.e all the pairs sampled from each transaction end up in the same chunk We run reservoir sampling on every other chunk to produce a truly random sample of size s 2  We then proceed to count the occurrences of the elements of the sample in the next chunk Assume in the following that we number chunks by  024   such that reservoir sampling is done on evennumbered chunks indexed by  024 even   When doing the above whenever we see a pair f i j g whose count must be updated we weigh the sample by the factor 015 i;j that got lost during the pair sampling phase so as to consider an expected number of samples exactly proportional to s  i j   At the end of a counting chunk we estimate the similarities of all pairs sampled and keep the s 2 largest similarities seen so far At the end of the stream the similarity estimates found are returned to supersede the previous estimates Pseudocode for the SampleCount algorithm is shown in 002gure 1 
125 
125 


Algorithm 1 Pseudocode for the S AMPLE C OUNT phase 1 procedure S AMPLE C OUNT  P s size   P is a stream of pairs each of which has associated a similarity value The length of P is known 2 S out   3 while There are elements in P do 4 S 0   5 S   6 t  0 7 S  the 002rst s 2 elements in P 8 while  t  size 2 000 s 2 do 9 i  the next element in P 10 Choose uniformly at random a number r 2 0   11 if r 024 s  s  2 t  2 then 12 Choose uniformly at random a victim from S and substitute it with i 13 end if 14 t  t  1 15 end while 16 initialize  S 0  S   S 0 is an associative array indexed on the distinct items present in S  initializing it means putting all its entries to 0 17 while  t  size  do 18 i  the next element in P 19 if i 2 S then 20 S 0  i   S 0  i   015 i 21 end if 22 t  t  1 23 end while 24 Choose the s topmost distinct items between S 0 out and S 0  and assign them to S 0 out 25 end while 26 Return S 0 out 27 end procedure IV A NALYSIS Let S i denote the set of transactions containing the element i  This means that S i  S j is the set of transactions containing the pair f i j g  Let S 1 i denote the set of transactions containing i in the current pre\002x of the stream Similarly S k i will denote the set of transactions containing i in C k  the chunk k of the suf\002x of the stream up to the point in which a new current pre\002x changes the counts of items occurrences So S k i  S i  C k De\002nition 5 Given x y 2 R we say that x  016 L  approximates y  written x 016;L  y  if and only if x 025 L implies x 2 1 000 016  y  1  016  y   016 The notation extends in the natural way to approximate inequalities In what follows we will use  016 L  approximations where L  C log mn  for a suitably large constant C depending on the accuracy 016 in Theorem 2 The task is to analyze the accuracy of the new approximation computed when the current pre\002x changes We introduce two random events G OOD P ERMUTATION GP and G OOD B ISAM S AM PLE GBS and bound the probability that they do not happen A permutation of the transactions is called good for f i j g  denoted GP i;j  if and only if the following conditions hold for the current pre\002x 1 j S 1 i j 016;L  j S i j  2 and j S 1 j j 016;L  j S j j  2  2 8 k j S k i  S k j j 016;L  j S i  S j j  2 k  Essentially goodness means that the frequencies of individual items are close in the 002rst and second half of the current pre\002x and the frequency of the pair is evenly spread over the chunks in the second part of the current pre\002x Lemma 6 Given 016 2 0 022 R  we have Pr[GP i;j  025 1 000 6 001 e 000j S i j 016 2 6 Proof An interesting property of the random variables j S 1 i j and j S k i  S k j j is that they are negatively dependent First of all we bound the probability that j S 1 i j is far from j S i  2 j  Using Chernoff bounds we can write Pr j S 1 i j 000 j S i j  2 j 024 016 j S i j   024 2 001 e 000 j S i j 016 2 6 1 
126 
126 


Looking at j S k i  S k j j we can write Pr j S k i  S k j j\000j S i  S j j  2 024 j 024 016 j S i  S j j  2 024  024 2 001 e 000 j S i  S j j 016 2 6 024 2 We use the fact that Chernoff bounds also holds for negatively dependent random variables Since the last bound is the weakest of the three the lemma follows We want GP i;j to hold with probability 1 000 o 1 n 2  whenever items i and j both have support   From Lemma 6 we get that this holds if j S i  S j j  C\024 log n  for some constant C depending on 016  If s  i j   2 024Lf     025 024L then j S i  S j j 025 2 024L  Hence a suf\002cient condition for the similarity is s  i j   024L  3 It remains to understand what is the probability that given a good permutation the pair sampler will take a number of samples for a given pair in each chunk k that leads to a 1 006 016  approximation of s  i j   We denote the latter event by GBS i;j;k  and want to bound the quantity Pr[GBS i;j;k j GP i;j   For this purpose consider the random variable X i;j;k de\002ned as the number of times we sample the pair f i j g in chunk k  Assuming GP i;j we have that over the randomness in the pair sampling algorithm E  X i;j;k  016;L   f  j S 1 i j  j S 1 j j  034 j S i  S j j  2 024  Since the occurrences of f i j g are independently sampled we can apply a Chernoff bound to conclude X i;j;k 016;L  E  X i;j;k   This leads to the conclusion Lemma 7 X i;j;k 016;L   f  j S 1 i j  j S 1 j j  034 j S i  S j j  2 024 016 Suppose that X i;j;k is close to its expectation Then we can use it with 1 006 016  approximations of j S i j and j S j j  to compute a 1 006 O  016  approximation of s  i j   This follows by analysis of the concrete functions f of the measures in Figure 1 A suf\002cient condition on the similarity needed for a 1 006 016  approximation of X i;j;k can be inferred from lemma 7 If s  i j  025 4 024L=\034 then E  X i;j;k  025 s  i j  034  4 024 025 L  So it suf\002ces to enforce s  i j  025 4 024L=\034  4 In order to have O  mb  pairs produced by the pair sampling phase we will choose 034  4 M  The expected number of pair samples from T t is less than j T t j 2 034 f      using that f is decreasing For all measures we consider f     024 1   so j T t j 2 034 f     024 j T t j 2 M 024 j T t j  It remains to understand which is the probability that a pair of items each with support at least   is not sampled by SampleCount Let the random variable X k represent the total number of samples taken in chunk k  The probability that a f i j g is sampled in chunk k is X i;j;k X k  so the probability that it does not get sampled in any evennumbered chunk is Q k 2  024 even  1 000 X i;j;k X k  s  We have seen before that X i;j;k 016;L 025 s  i j  034  4 024  For what concerns X k using a Chernoff bound we can get X k 016;L  E  X k  024 mb=\024  using the linear upper bound on the number of samples So we can compute Y k 2  024 even  1 000 X i;j;k X k  s 024 022 1 000 s  i j  034 024 2 024\015 i;j mb 023 s\024 2 024 022 1 000 s  i j  034 4 mb 023 s\024 2 024 C exp 024 000 s  i j  034 s\024 8 mb 025 In order for this probability to be small enough  O 1 m 2   we need to bound the similarity to s  i j  025 8 mbL s\024\034 5 To choose the best value of 024 we balance constraints 3 and 5 getting 024L   mbL s\024\034  024  r mbM s 6 From which we can deduce s  i j   L  max  r mbM s  M   7 V D ATASET CHARACTERISTICS We have computed for a selection of the datasets hosted on the FIMI web page 1  the ratios between the number of occurrences of single items and pairs in the 002rst half of the transactions and the total number of occurrences of the same items or pairs The values of some of this ratios the most representative are plotted 002gure 3 on the x axis items or pairs are spread evenly after they have been sorted according to their associated ratio The y axis represents the value of the ratios We have taken into account only items and pairs whose support is over 20 occurrences in the whole dataset in order to avoid the noise that could be generated by very rare elements As we can see the number of occurrences and co-occurrences are not so far from what would be expected under a random permutation of the transactions The synthetic data set behaves exactly like we would expect under a random permutation with the ratio being very close to 1  2 for almost all items/pairs This means that even for real data sets where the order of transactions is not random the sampling probabilities used in the pair sampling are reasonably close to the ones that would be obtained under the random permutation assumption VI C ONCLUSIONS We presented the 002rst study concerning the problem of mining similar pairs from a stream of transactions that does rely on the similarity of items and not only on the frequency of pairs A thorough experimental study of carefully engineered versions of the presented algorithm remains to be carried out 1 http://fimi.cs.helsinki.fi 
127 
127 


Figure 3 Plots of the ratios j S 1 i j  j S i j and j S 1 i  S 1 j j  j S i  S j j  R EFERENCES  E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J D Ullman and C Yang Finding interesting associations without support pruning IEEE Trans Knowl Data Eng  vol 13 no 1 pp 64–78 2001  Y.-K Lee W.-Y Kim Y D Cai and J Han Comine Ef\002cient mining of correlated patterns in Proc IEEE International Conference on Data Mining ICDM 2003  IEEE Computer Society 2003 pp 581–584  E Omiecinski Alternative interest measures for mining associations in databases IEEE Trans Knowl Data Eng  vol 15 no 1 pp 57–69 2003  J Han and M Kamber Data Mining Concepts and Techniques 2nd edition  Morgan Kaufmann 2006  R Agrawal and R Srikant Fast algorithms for mining association rules in large databases in Proc International Conference On Very Large Data Bases VLDB 1994  Morgan Kaufmann Publishers Inc Sep 1994 pp 487–499  J Han J Pei Y Yin and R Mao Mining frequent patterns without candidate generation A frequent-pattern tree approach Data Min Knowl Discov  vol 8 no 1 pp 53 87 2004  N Jiang and L Gruenwald Research issues in data stream association rule mining SIGMOD Record  vol 35 no 1 pp 14–19 2006  Y Zhu and D Shasha Statstream Statistical monitoring of thousands of data streams in real time Morgan Kaufmann 2002 pp 358–369  G Cormode and S Muthukrishnan What's hot and what's not tracking most frequent items dynamically ACM Trans Database Syst  vol 30 no 1 pp 249–278 2005  E D Demaine A L  opez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space in Proc 10th Annual European Symposium Algorithms ESA 2002  2002 pp 348–360  J X Yu Z Chong H Lu Z Zhang and A Zhou A false negative approach to mining frequent itemsets from high speed transactional data streams Inf Sci  vol 176 no 14 pp 1986–2015 2006  A Chakrabarti G Cormode and A McGregor Robust lower bounds for communication and stream computation in STOC  C Dwork Ed ACM 2008 pp 641–650  S Guha and A McGregor Stream order and order statistics Quantile estimation in random-order streams SIAM Journal on Computing  vol 38 no 5 pp 2044–2059  N Alon Y Matias and M Szegedy The space complexity of approximating the frequency moments J Comput Syst Sci  vol 58 no 1 pp 137–147 1999  J Misra and D Gries Finding repeated elements Sci Comput Program  vol 2 no 2 pp 143–152 1982  R M Karp S Shenker and C H Papadimitriou A simple algorithm for 002nding frequent elements in streams and bags ACM Trans Database Syst  vol 28 pp 51–55 2003  M Charikar K Chen and M Farach-Colton Finding frequent items in data streams Theor Comput Sci  vol 312 no 1 pp 3–15 2004  A Campagna and R Pagh Finding associations and computing similarity via biased pair sampling in Proc 9th IEEE International Conference on Data Mining ICDM 2009    Finding associations and computing similarity via biased pair sampling Invited for publication in Knowledge an Information Systems  2010  E Kushilevitz and N Nisan Communication complexity  New York Cambridge University Press 1997  J S Vitter Random sampling with a reservoir ACM Trans Math Softw  vol 11 no 1 pp 37–57 1985  D Dubhashi and D Ranjan Balls and bins a study in negative dependence Random Struct Algorithms  vol 13 no 2 pp 99–124 1998 
128 
128 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





