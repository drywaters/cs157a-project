An efficient for discovery of Frequent Itemsets Venkateswari S  Suresh R.M   Department of Software Engineering  Department of Computer Science  Noorul Islam University, Kumaracoil, Thuckalay, K.K.Dist, India  RMD Engineering College, Kavaraipettai, Chennai, India 1 venki.vasan@gmail.com Abstract Association rule mining is a well researched method for discovering interesting relations between variables in large databases. The first phase of association rule mining is the discovery of frequent itemsets which is a critical step and plays important role in many data mining tasks. The existing algorithms for finding frequent itemsets suffer from many 
problems related to memory and computational cost. In this paper, we propose a new algorithm ILLT \(Indexed Limited Level Tree\ which gets frequent itemsets efficiently in the given database without doing multiple scans and extensive computation. ILLT algorithm works in two phases. First, the transactional data is converted into three level compact tree structures. Then, these trees are scanned to discover the frequent itemsets. Experimental status shows the effectiveness of the algorithm in mining frequent itemsets Keywords  Data Mining, e-commerce, frequent itemsets ILLTree I I NTRODUCTION Large amount of data have been collected routinely in the course of day-to-day management, in business, administration 
banking, e-commerce, the delivery of social and health services, environmental protection, security and in politics With the tremendous growth of data, users are expecting more relevant and sophisticated information which may be lying hidden in the data. Existing analysis and evaluating techniques do not match with this tremendous growth. Data mining is often described as a discipline to find hidden information in databases.   It involves different techniques and algorithms to discover useful knowledge lying hidden in the data [1   Association rule mining has been one of the most popular data mining subjects which can be simply defined as finding interesting rules from collection of data [2  T h e first ste p  in association rule mining is finding frequent itemsets. It is a very resource consuming task and for that reason it has been 
one of the most popular research fields in data mining II P ROBLEM S TATMENT Following is the formal statement of association rule mining for transactional databases [2,3  Let I = {i1, i2,im} be a set of literals, called items.  Let D be a set of transactions, where each transaction T is a set of items such that T  I. A unique identifier TID is given to each transaction. A transaction T is said to contain X, set of items in I, if X  T. An association rule is an implication of the form X  Y", where X  I, Y  I, and X  Y = . An item set X is said to be large or frequent if its support s is greater or equal than a given minimum support threshold  The rule X  Y has a support s in the transaction set D if s% of the 
transactions in D contain X  Y. In other words, the support of the rule is the probability that X and Y hold together among all the possible presented cases. It is said that the rule X  Y holds in the transaction set D with confidence c if c% of transactions in D that contain X also contain Y . In other words, the confidence of the rule is the conditional probability that the consequent Y is true under the condition of the antecedent X Association mining task can be broken into two steps:  The first step is to find each set of items, called itemsets, such that the co-occurrence rate of these items is above the minimum support  and this itemsets are called frequent itemsets. The size of an itemset represents the number of items in that set 
Second step of generation of association rules is straight forward III R ELATED W ORK Several algorithms are devised for finding frequent itemsets Apriori is the foremost and popular algorithm for finding frequent itemsets [3  T h e A pri o r i i s a lev e l wi se  s e a r ch algorithm for mining frequent itemsets for Boolean association rules. Apriori uses the property stating that for a k itemset to be frequent , all its k-1 itemsets have to be frequent The drawback of  Apriori is that of repeated database scanning and high computational cost Another approach of discovering frequent itemsets is the FPgrowth \(Frequent Pattern growth\rithm [4   T h is algorithm uses compact data structure called FP-Tree. FPgrowth algorithm requires only two scans of database. First 
scan get all frequent 1-itemset and the next scan of database constructs the tree structure. Mining takes place directly in the tree structure. The shortcoming of the algorithm is the generation of thousands of FP tree. This memory based data structure consumes lot of space and time and this becomes a serious bottleneck for cases with large databases. Several methods do single scan of databases for finding the itemsets 7  Th e TI M V a l g o r i t h m  8  use s t h r e e d i men si on al i t e m se t  matrix vectors. Unlike Apriori this algorithm needs one pass to scan the database and gains all frequent itemsets without generating candidate itemsets. The Diffset algorithm [9  us es tree data structure and mines the frequent itemset by traversing the search tree thus saving a lot of I/O In this paper, we propose  a new algorithm named ILLT 
Limited Level Tree algorithm  that uses candidates but it has an advantage of  single database scan and finding the frequent itemsets  quickly for any given support threshold The remainder of the paper is organized as follows:  The proposed algorithm ILLT is described in section 2 Experimental results are discussed in section 3.  Conclusion is presented in section 4 IV T HE ILLT A LGORITHM The main objective of developing this Indexed Limited Level Tree algorithm is to reduce the repeated database scans and to reduce the cost of computation required for generating 531 978-1-4244-8594-9/10/$26.00 c  2010 IEEE 


frequent itemsets. Proposed ILLT algorithm is a three level tree structure algorithm composed of two steps. First step is construction of ILLTree structures. Second step is mining the tree structures for finding frequent itemsets. Frequent itemsets for any given support levels can be discovered quickly from the ILLTree structures. The mining process might take in some cases less than one full scan of the data structure for discovering frequent itemsets V C ONSTRUCTING ILLT REE STRUCTURES First step of ILLT algorithm is construction of tree data structures. The levels of the tree are limited to three with an index node so it is named as Index Limited Level Tree \(ILLT The compact trees constructed in the first step is done  by doing only one scan of given transactional database. From the resultant ILLTree it is easy to find frequent itemsets for different support levels. Scanni ng the database again is not needed at any stage. The tree structures store the contents of the transactions in their nodes ILLT algorithm scans the database first and generates candidate itemsets for every transaction in the database. These generated candidate itemsets are of different lengths. An unique tree is constructed for each k-length itemsets. Level 1 of the trees is the header node This header node with the label H indicates which k- itemset that tree stores. If the label of the header node is 3, then this tree stores all 3-itemsets. The header node has n children at level two. n is the total number of items that occur in the transactions. Each node in the level two indicates an individual item. Third level of the tree stores the candidate itemsets as its nodes. An index is associated with each tree. The Indexed Limited Level Tree structure with three levels is shown in the figure 1 Fig. 1: ILLTree structure When candidate itemsets are generated, an unique identification number is assigned to each of them. This candidate itemset with its identification number is stored in the index first. Then this itemset is stored in the third level of the tree in the following syntax Node Id number><Number of occurrence Then a new candidate itemsets has to be inserted in the tree, it is first checked with the index. If it already exists in the index only the occurrence number is incremented in the respective nodes. If it does not exist, then a new identification number is assigned and the itemset is stored in the tree. Depending on the length of the candidate itemset the appropriate tree is chosen for insertion.  For illustration we use the example in table 1 TID Items   T1 ABC   T2 AB   Table 1 T1 and T2 are Transaction Ids. Transaction T1contains 3 items and T2 contains 2 items The candidate itemsets generated for the transactions in the table are T1: {a}{b}{c}{ab}{ac}{bc}{abc T2: {a} {b} {ab Candidate itemsets of transaction T1 are 1-Itemsets a}, {b}, {c 2-Itemsets ab}, {ac}, {bc 3-Itemsets : {abc Fig 2:  Tree structures for T1 The respective tree structures after the insertion of candidate itemsets of transaction T1 is shown in figure 2 Candidate itemsets for transaction T2 is as follows 1-Itemsets : {a}, {b 2-Itemsets : {AB The resultant 1-item and 2-item tree structures af ter inserting the candidate itemsets of given two transactions is in figure 3 Fig 3:   Tree structures after T1 and T2 In the figure 3, trees show the difference in the occurrence value for the nodes of repeated candidate itemsets. The second part of the node value shows the number of occurrences. Since 532 2010 International Conference on Signal and Image Processing 


2010 International Conference on Signal and Image Processing 533 


     IV  A NONYM OUS  A LGORITHM BASED ON ANATOMY TECHNIQUE  In this section, we show an effective anonymous algorithm based on anatomy technique that meets the privacy requirement of l diversity. Our algorithm consists of two phases: item partition and record partition A  Item partition Our algorithm first split item set so that highly-correlated item set are in the same column. It is helpful to data utility and privacy. For data utility, grouping highly-correlated item set together helps protect the relationship between item set. For data privacy, the association of uncorrelated item set presents high identification risks because its values is much less frequent and thus more vulnerable to attack 1  Correlation measure Mean-square contingency coefficient s used f o r measuring correlations between two categorical attributes. We choose to use this measure because the transaction data can be viewed as categorical attributes containing only two values Given two attributes I 1 and I 2 with value domains {u 1 u 2 u d1  and {v 1 v 2 v d2 respectively. Their domain sizes are d1 and d2. The mean-square contingency coefficient between I 1 and I 2  is defined as            12 2 2 1 11 1   min 1 2 1 2  d i d j f f f f f d d j i j i ij I I  3 Here, f i and f j are the fraction of occurrences of u i and v j in the data, respectively. f ij is the fraction of occurrences of u i and v j in the data. For transaction data, the dimension is very large but only contains two value 0 and 1, 0 indicates there is no value in this item and 1 indicates there is a value in this item So \(3\ can be rewritten as    j i j i ij f f f f f I I     2 2 1 2   4 Here, f i and f j are the fraction of occurrences of u i 1 and v j 1in the data, respectively. f ij is the fraction of occurrences of u i 1  and v j 1 in the data. Because value 0 is meaningless, so we only calculate the value 1. It is easy to know that  1  0 2 1 2   I I   2  Item clustering Having computed the relationship between each pair of attributes, we use clustering to divide attributes into columns In our algorithm, each item is a point in the clustering space The distance between two items is defined as d\(I 1 I 2 1 2 1 2  I I  When two attributes are highly-correlated, the distance between them is smaller We use the k-medoid methods to achieve item clustering We use the well-known k-medioid algorithm PAM \(Partition Around Medoids\ algorithm [14  P A M  a l g o ri t h m fi r s t  randomly select k objects as the initial medoids, the remaining points will be assigned to the nearest medoid. Then, PAM chooses one medoid point and one non-medoid point and swaps them as long as the cost of clutering decreases. Here, the clustering cost is measured as the sum of the cost of each cluster, which is measured as the sum of the distance from each non-medioid point to the medoid point in the same cluster When no points can be swapped, program terminates. PAM's time complexity is O\(\(m-k 2 where m is the size of attributes and k is the number of cluster. PAM is suitable only for small amounts of data, however, the cluster space here is the number of attributes, therefore, PAM will not cost too much B  Record partition In the record partition phase, records are partitioned into buckets. Here we use a simple linear-time algorithm to handle record partition. Our algorithm is described in Figure 2 Algorithm Multi-Anatomy\(T l C,p  1. Multi-B  Bcnt = 0 2. Initialize p hash buckets Hi\(1 i p 3. For each record t in T 4. For each column C i in t, hash its value in hash buckets H i  5. if the most frequent value in each Hi satisfy c\(H i H i  1 l  6.     B Bcnt all record t in hash buckets H i  7.     Multi-B=Multi-B B Bcnt  8.     Bcnt=Bcnt+1 9.     Clear all hash buckets 10.For each non-empty hash bucket 11.    t=the record in hash bucket 12.   assign t to a random bucket in B i if the most frequent value in each column C j of B i satisfy c\(v ij B i  1 l when add t 13.Retrun Multi-B Figure 2  Anonymous algorithm At first, we initialize p hash buckets where p is the number of columns \(line 1-2\. Then we scan each record in the transaction table T, for each column we hash its value in the corresponding hash bucket. If the table after partitioning satisfies l diversity \(line 3-5\, we add the records in hash bucket to Multi-B where Multi-B is the union of each buckets Then we clear all hash buckets \(line 6-9\. Finally, we randomly assign the left records to bucket if it satisfies l diversity \(line 10-12\. After that, the function returns the result Multi-B We now analyze the time complexity of the record partition We use p hash bucket and the complexity of hash is O\(1\. And we only scan each record once which takes O\(n\ time where n is the number of records. Totally, the time complexity of our algorithm is O\(n\\(m\, for m<<n, so the total time complexity is therefore O\(n V  E XPERIMENTS  In this section, we evaluated experimentally the proposed anonymization techniques. We evaluated three aspects: privacy leakage, data utility and execution time A  Experimental setup The experiments were performed on a 2.66 GHz Intel IV processor machine with RAM of 1 GB. The operating system was Windows XP Professional Edition, and the implementation was built and run in Visual C++ 6.0. We used two different datasets: Connect and T40I10D100K, all can be publicly 176 WeP2.5 


     available from FIMI Repository \(http://fimi.cs.helsinki.fi/data Connect is a real dataset containing game state information T40I10D100K is a synthetic dataset. T40I10D100K have a very large item set that is extremely larger than a relation table Table II provides a brief description of these datasets TABLE II  D ATASET S TATISTICS Dataset  Transactions|D  Average Length Items Data size\(K bytes  Connect 67,557 43 129 9,225 T40I10D100K 100,000 39.6 942 15,478  In our algorithm, attributes are partitioned into two or more columns. For a bucket that contains k records and p columns we generate k records as follows. We randomly permutated the values in each column because many data mining algorithms cannot handle anatomy table well currently B  Privacy leakages By Theorem 2, through l diversity multi-dimension anatomy a table is partition into p columns. The number of faked records would be very large Figure 3 is the number of faked records of T40I10D100K As expected, the number of faked records is so large that we have to use function ln. As shown in the figure 3, when we increase the value of l where l is the l diverse, the number of fake records becomes larger. In particular, when we increase the value of p where p is the number of columns, the number of fake records becomes exponentially larger Figure 3  Number of faked records of T40I10D100K C  Data utility In data utility, we use Apriori algorithm to mining association rules of anonymous data Because Connect is very dense, so we limit the length of association rules where the minimum length is 2 and maximum length is 4. Figure 4 shows the number of association rules of the anonymous data Connect when p=10 l 5,6,8,10. It shows that the different between anonymous data and original data is much small indicating that our algorithm for dense date set introduces little information Figure 5 shows the number of association rules of the anonymous data T40I10D100K when p=20 l 6,8,10,12. It shows that when the confident is low, such as confident=10 the difference between anonymous data and original data is very small. When confident increases, the gap increases slowly but the difference usually maintain at 40% -60 Figure 4  Number of association rules of Connect   Figure 5  Number of association rules of T40I10D100K  Figure 6  Accuracy radio of T40I10D100K 177 WeP2.5 


     The number of association rules is part of data utility, the correct number of association rules is also a key part of data utility So we introduce the following equation AR \(accuracy radio M / N where N is the number of association rules for original data and M is the number of common association rules for anonymous data Figure 6 is the accuracy radio of T40I10D100K. It shows that, when confident decreases, the accuracy radio increases Because our algorithm losses the information of unusual association rules while maintains the information of usual association rules D  Execution time Figure 7 is the time cost of T40I10D100K. It shows that our algorithm is very efficient, when the l value increases, the time cost will slightly increases because it costs a little more time to meet l diversity Figure 7  Time Cost of T40I10D100K VI  C ONCLUSION AND FUTURE WORK  In this paper, we study the privacy issues in transaction data publication. Based on anatomy technique, we propose a new privacy model called multi-dimension anatomy that meets the privacy requirement of l diversity and suits to transaction data We introduce a simple linear-time anonymous algorithm Experiments show that our algorithm can safely and effectively protect privacy in transaction data There are still a number of promising areas for future work First, the linear-time algorithm proposed in this paper seems a little simple, some new heuristic algorithm needs to further research. Second, while an anatomized table has been generated, it remains an open problem on how to use the anonymized data A CKNOWLEDGMENT  The authors gratefully acknowledge financial support for this research by the NFS of Fujian Porvince under grants No 2009J01295, the NFS of Fujian Province under grants No 2010J01330 and scientific project of Fujian Provincial Department of Education under grants No. JA09004 R EFERENCES  1  K. Hafner. Researchers Yearn to Use AOL Logs, but They Hesitate New York Times, August 23, 2006 2  M. Barbaro, T. Zeller and S. Hansell. A Face Is Exposed for AOL Searcher No. 4417749. New York Times, Aug 9, 2006 3  R. Agrawal, T. Imielinski, and A. N. Swami. Mining  Association Rules between Sets of Items in Large Databases. SIGMOD 1993 4  P. Samarati and L. Sweeney. Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression. Technical report, CMU, SRI, 1998 5  L. Sweeney. Achieving k-anonymity privacy protection using generalization and suppression. International journal on uncertainty Fuzziness and knowldege based systems, 10\(5\:571  588, 2002 6  L. Sweeney. k-anonymity: a model for protecting privacy. International journal on uncertainty, Fuzziness and knowldege based systems 10\(5\:557  570, 2002 7  A. Machanavajjhala, J. Gehrke, and D. Kifer. l-diversity: privacy beyond k-anonymity. In To appear in the 22st International Conference on Data Engineering \(ICDE06\, 2006 8  N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond kanonymity and l-diversity. In Proc. of the 21st IEEE International Conference on Data Engineering \(ICDE\, Istanbul, Turkey, April 2007 9  C. C. Aggarwal. On k-Anonymity and the Curse of Dimensionality. In VLDB, pages 901909, 2005   Y. Xu, K. Wang, A. W. C. Fu, and P. S. Yu. Anonymizing transaction databases for publication. In Proc. of the 14th ACM SIGKDD, August 2008   M. Terrovitis, N. Mamoulis and P. Kalnis. Anonymity in unstructured data. Technical Report, Hong Kong University, 2008   X.Xiao and Y.Tao. Anatomy:simple and effective privacy preservation In VLDB,2006,pages139150   H.Cramter. Mathematical Methods of Statistics. Princeton,1948   L.Kaufmanand P.Rousueeuw. Finding Groups in Data: an Introduction to Cluster Analysis.John Wiley &Sons,1990                    178 WeP2.5 


Communication Review Vol 31 No.15 pp.35793585 2008  M C h e n T K w o n e t a l   M o b i l e A g e n t B a s e d W i r e l e s s Sensor Networks Journal of Computers Vol 5 No.2 pp 1421 2006  G a n e s a n D e t a l  C o p i n g w i t h i r r e gul a r s p a t i o t e m po r a l sampling in sensor networks ACM SIGCOMM Computer Communication Review Vol 34 No.1 pp.125-130 2004  Co h e n L  A v r a h a m i B a ki s h G e t a l  R e a l t i m e d a t a mining of non-stationary data streams from sensor networks Information Fusion Elsevier Vol 9 No.3 pp 344-353 2008  X W a n g J M a  S W a ng D B i  T i m e S e ri e s F o r e c a s t i n g Energy-efficient Organization of Wireless Sensor Networks IEEE Sensors Journal Vol 7 No.1 pp 1766-1792 2007  Y X u J W i n t e r W C Lee Dual prediction-based reporting for object tracking sensor networks. 1 st Annual International Conference on Mobile and Ubiquitous Systems Networking and Services USA pp.154-163 2004  X D a i  F X i a  Z W a ng Y S u n  A n E n e r g y E f f i c i e n t In-Network Aggregation Query Algorithm for Wireless Sensor Networks Proc of the 1 st International Conference on Innovative Computing Information and Control China pp 255-258 2006 85 Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010 


        


9 Appendix Fig 6: Forest Cover Types of the U.S. \(Source. USGS National Atlas of US Summary of Forest Cover Type Data Type Multivariate Abstract The forest cover type for 30 x 30 meter cells obtai ned from US Forest Service \(USFS\ Region 2 Resource Information System RIS\ data Data Characteristics The actual forest cover type for a given observatio n \(30 x 30 meter cell\ was determined from US Fores t Service \(USFS\ Region 2 Resource Information System RIS data Independe nt variables were derived from data originally obta ined from US Geological Survey \(USGS\ and USFS data. Data is in raw form \(not scaled\ and contains binary \(0 or 1 columns of data for qualitative independent variables \(wilderness areas and soil types Summary Statistics Number of instances observations 581012 Number of Attributes 54 Attribute breakdown 12 measures, but 54 columns of data \(10 quantitativ e variables, 4 binary wilderness areas and 40 binary soil type variables Missing Attribute Values None 43 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





