Visualisation and Prediction of Conversation Interest through Mined Social Signals Dumebi Okwechime Eng-Jon Ong Andrew Gilbert and Richard Bowden CVSSP University of Surrey Guildford Surrey GU17XH UK  d.okwechime e.ong a.gilbert r.bowden  surrey.ac.uk Abstract  This paper introduces a novel approach to social behaviour recognition governed by the exchange of non-verbal cues between people We conduct experiments to try and deduce distinct rules that dictate the social dynamics of people in a conversation and utilise semi-supervised computer vision techniques to extract their social signals such as laughing and nodding  Data mining is used to deduce frequently occurring patterns of social trends between a speaker and listener in both interested and not interested social scenarios The con“dence values from rules are utilised to build a Social Dynamic Model SDM that can then be used for classi“cation and visualisation By visualising the rules generated in the SDM we can analyse distinct social trends between an interested and not interested listener in a conversation Results show that these distinctions can be applied generally and used to accurately predict conversational interest I INTRODUCTION As naturally social entities humans can easily extract social information from non-verbal communication without the need of understanding what is being said Psychologists believe this skill is hard-wired in the human brain Gesture vocal signal and body language triggers unconscious analysis of socially relevant information Since non-v erbal communication plays such an important role in our social interaction a method of modelling it would prove valuable in understanding relationships identifying context/intent or generating synthetic responses in an Arti“cial Intelligent AI context Our aim is to devise a model for non-verbal communication It will allow classi“cation and visualisation of multimodal exchanges in social signals between a speaker and listener in a conversation Unlike other social models that rely on intangible psychological observations we propose the use of tangible rules governed by the data to discern distinct trends and characteristics We achieve this by using data mining to ef ciently identify social trends between a group of three people in a conversation The con“dence values are used to build a Social Dynamics Model SDM  SDM allows for ef“cient visualisation of multimodal social trends to enable visual distinctions between the two scenarios Using these distinctions the modal accurately predicts conversational interest within a window of less than ve minutes This work was supported by the EPSRC project LILiR EP/E027946/1 and the European Communitys Seventh Framework Programme FP7/20072013 under grant agreement number 231135 DictaSign This paper is divided into the following sections Section II brie”y details a background in different approaches for understanding social interaction Section III presents our dataset and methods for conversational analysis Section IV describes the visualisation of the SDM and the remainder of the paper presents an evaluation and conclusions II BACKGROUND Traditional social interaction research can be grouped into two main categories emotion based on cognitive psychology  and linguistics based on dialogue understanding 5][6 Although emotion understanding is of vital importance in how people socially interact emotion recognition in a natural conversation is a very complex problem and would require extensive data and research in deducing social trends Also structured dialogue can not be easily interpreted to observe generalised behaviour Methods that utilise machine learning models such as HMMs or Dynamic Bayesian Netw orks  are applied to generic features in audio signals and pix el intensities to discern social behaviour However in this paper we derive rules of social behaviour by focusing mainly on non-verbal social cues Psychological studies have proven that observing nonlinguistic/non-verbal unconscious social signals can provide effective information in social interaction understanding  A number of researchers ha v e used machine analysis of non-verbal social signals to interpret social behaviour The idea of Social Signal Processing  originally introduced by Pentland is to use visual and v ocal analysis to understand social behaviour and predict outcomes of dyadic interactions to enable a Human-Centred computing paradigm This is achieved using textures i.e speaker energy and amount of movement from multimodal social signals Similarly Curhan et al use these texture features to predict outcomes of negotiations based on thin slices of employment negotiation data Although these methods perform well in predictions they rely on psychological observations to derive prior assumptions of what is positive or negative social behaviour This may not be accurate in all social contexts Also their approach is unable to discern cooccurence of social signals of multiple modes as these more complex dependencies are dif“cult to identify However in this work we introduce the SDM which utilises data mining to derive tangible rules for visualising multimodal social interaction and for accurately predicting social context This 951 


Fig 1 a Image showing full-body view of recorded video data of three people having a conversation b Image showing close-up face view c Diagram showing the con“guration of cameras microphones mic and conversers We refer to the three people in the conversation as person 1 2 and 3 is achievable independent of prior psychological evaluations relying solely on the trends in the data Eagle et al introduce reality mining which uses mobile devices like smart badges and cellular phones to extract proximity and vocal information to derive social networks The main difference to our method is we use multimodal social signals as features for apriori association rule mining with the aim of deriving speci“c association rules that govern conversational interest III CONVERSATION ANALYSIS A Dataset The dataset 1 is composed of approximately 30 minutes of audio and video recordings 43000 frames of the fullbody frontal view 516  340 25 frames per second 48kHz and the close-up face view 720  576 25 frames per second 48kHz of 3 individuals having a conversation with each other An image of the full-body recording for each person is shown in Figure 1\(a and the face recording in Figure 1\(b We refer to the 3 individuals as person 1 2 and 3 Each person remained in a stationary position relative to the cameras as shown in Figure 1\(c Prior to capture each person was given a questionnaire and asked to score from 1-3 their interest where 1 is low interest and 3 is high interest on a given set of book genres  lm genres and music genres  They were also given speci“c questions like favourite sports  language\(s spoken uently  favourite music concerts  favourite theatre shows etc Their questionnaires were analysed to choose topics for conversation that would lead to the following 4 generic scenarios  All interested in topic  Two people interested in topic one person is not  One person interested in topic two people are not  None are interested in topic These 4 generic scenarios where derived from 8 topics of conversation as detailed in Table I The sixth column of Table 1 The dataset along with annotation can be made available upon request Please email  d.okwechime@surrey.ac.uk   I shows the limited duration of each topic chosen to suite the scenario A projector displayed the topic of conversation for discussion and a quiet bell would ring to make the subjects aware of the change in topic The subjects were unaware of the nature of the experiment and were simply asked to discuss the topic displayed on the screen The aim of this experiment was to observe the social dynamics between the three people in scenarios when interested or not interested in the topics The next step is to quantise their social signals from the data in a form that is suitable for data mining in order to obtain rules governing social behaviour B Social Signals Pentland proposed measuring non-linguistic social signals using four main observations activity level  engagement  emphasis and mirroring  Using this as our base we chose to observe 7 social signals in the conversation Voiced  Talking  Laughing  Head Shake  Head Nod  Activity Measure  and Gaze Direction  We use a variety of techniques to derive each of these labels 1 V The audio stream is represented using 12 MFCCs Mel-Frequency Cepstral Coef“cients and a single energy feature of the standard HTK setup For each person a few voiced segments were labelled and a Mahalanobis distance measure was used to derive a correlation between the voiced and non-voiced regions 2 T With the voiced segments labelled it was a simple process of labelling the voiced segments which were talking This was done by hand 3 Laughing[L The Viola-Jones face detector w a s used to segment the face region in each frame The lip region was localised by cropping the lower-centre region of the face An AdaBoost classi“er was then trained for laughing and used to label the remaining data 4 Head Shak The Viola-Jones face detector was used to determine the movement of the face A Fast 952 


 Scenario P1 P2 P3 Topic Period 1 3 3 3 Classical Music 5 min 2 3 3 1 Adventure Novels 5 min 3 3 1 3 Philosophy Novels 5 min 4 1 3 3 Rock Music 5 min 5 3 1 1 Sailing Spoken in French 2.5 min 6 1 3 1 Triathlon/Les Miserables Spoken in Afrikaans 2.5 min 7 1 1 3 Radio Head Concert 2.5 min 8 1 1 1 Horror Novels 1.5 min TABLE I T ABLE SHOWING 8 DIFFERENT SOCIAL SCENARIOS DICTATED BY THE TOPIC OF CONVERSATION T HE THREE PEOPLE ARE REFERRED TO AS P1 FOR PERSON 1 P2 FOR PERSON 2 AND P3 FOR PERSON 3 T HE NUMBERS INDICATE THEIR INTEREST IN THE TOPICS WHERE 3 IS A HIGH INTEREST AND 1 IS A LOW INTEREST Fourier Transform FFT was used to identify high frequency movement along the x-axis 5 Nod[N Similar to head shakes an FFT was used to identify high frequency movement along the y-axis 6 Activity Measur The torso region of the full body video was segmented using colour and the meanscaled standard deviation of velocity was measured The leg and head regions are ignored because there was minimal leg movement subjects remained stationary and since we are more interested in gesture activity changes in head posture/gaze would bias the activity measure 7 Gaze Dir The eye pupils and the corners of the eyes were tracked using a Linear Predictor tracker  The corners of the e yes were normalised to 0 and 1 and the position of the eye pupil within this region was used to determine if the person was gazing left  right GR or centre GC This produces N T sets of social signal labels where N T is the total number of frames of 27 dimensions where 1  9 is for person 1 10  18 for person 2 and 19  27 for person 3 We de“ne 2 complete sets of social signal vectors for interested and not interested scenarios as   Int   and   NotInt  such that    f i  N T i  1 where f i is a 27 dimensional binary vector C Mining for Frequent and Distinctive Social Trends This experiment is driven by the speaker At any given time there is only one speaker and one listener We are interested in the combination of social signals a listener performs when interested and not interested in the conversation Manually observing all combinations of listener and speaker behaviours in such a large data set would be virtually impossible A solution would be to make some common sense prior assumptions of expected trends i.e an interested listener would gaze more at the speaker than when they are not interested and focus primarily on these assumptions However there is no way of proving or disproving such assumptions and there is a large list to chose from We wish to employ a data driven approach to learn such rules We propose a novel approach to deriving social dynamics and trends between the subjects based on data mining Data mining allo ws for lar ge data sets to be processed to identify the reoccurring patterns within the data in an ef“cient manner In this work Apriori Association rule   mining is used F ormally de v eloped for supermark ets to analyse millions of customers shopping trends we aim to nd association rules between a speaker and listener that indicate interested and not interested from the multitude of possible rules that could exist An association rule is a relationship of the form  R A i  R C i where R A i is a set of social signals of the speaker and R C i a social signal of the listener R A i   r A i  1   r A i   R A i   is the antecedent where r A i denotes a speakers social signal and R C i   r C i  1  r C i   R C i   the consequence where r C i is a listeners social signal An example would be if R A 1    N    L    and R C 1    L   as de“ned in Section III-B then  R A 1  R C 1 would imply when the speaker nods and laughs the listener is very likely to also laugh The belief of each rule is measured by a support and con“dence value The support measures the statistical signi“cance of a rule it is the probability that a transaction contains itemset R A i  sup   R A i  R C i  sup   R A i  R C i  1 The con“dence is the number of occurrences in which the rule is correct relative to the number of cases in which it is applicable con f  sup   R A i  R C i  sup  R A i   100 2 Apriori Association mining is applied to the social signal labels for both interested listener and not interested listener scenarios to derive frequently occurring association rules Traditionally data mining looks for a combination of symbols that occur simultaneously However a listeners social behaviour is always a response to the speakers social signals hence co-articuation is not possible To account for this temporal bagging within a set temporal window is used to enforce a temporal coherence between features Given a speakers social signal we observe the listeners social behaviour s  10 frames in the future approx 1 2 a second IV VISUALISING AND INTERPRETING SDM The SDM allows visualisation of multimodal trends in social interaction between a speaker and listener in a conversation Using the mined con“dence values the conditional probability of the listeners social responses given the 953 


Fig 2 a The skeleton of SDM Consists of 7 black lines that are attached to a central intersection Each line represents a different speakers socia l signal b 9 pentagon rings where each ring represents a listeners social response The individual rings are coded by colour and size c SDM is made up of the rings superimposed on the skeleton The points where the rings intersect the skeleton are known as nodes and infer a listeners social respons e given a speakers social signal A few are indicated by the three red arrows arrows 1 2 3 Arrow 1 is pointing at node   arro w 2 at node L   and arro w a t node S   The idea is that the nodes will v ary in size re”ecting the respecti v e mined con“dence v alue speakers social signals can be observed ef“ciently to distinguish social trends without needing to rely on psychological observation To avoid over complicating the diagram with the numerous combinations of association rules to visualise the SDM only association rules with single antecedents i.e  R A i   1 are used whereby the likelihood of a listeners social response is derived by a single speakers social signal The more complex rules are still kept in the model however we use the simpler rule for visualisation to discern prominent trends A Interpretation of an SDM The SDM is made up of two components a skeleton and a set of pentagon rings  The skeleton consists of 7 black lines that collectively meet at a central intersect Each line represents a different speakers social signal and are con“gured as shown in Figure 2\(a the labels are as detailed in Section III-B With regards to the speaker voiced  and talking  social signals are ignored since the speaker is guaranteed to be voiced and talking To add clarity to the gaze labels instead of  and GC we use G-S G-OL G-N representing gazing at speaker  gazing at another listener and gazing at no one  respectively This allows us to know who they are gazing at The second component is a set of 9 pentagon rings Each ring represents a different listeners social signal As presented in Figure 2\(b the individual rings are coded by colour and size Shown in Figure 2\(c the superimposed skeleton and pentagon rings make up the SDM The points where the rings intersect the skeleton infer the occurrence of a listeners social response given a speakers social signal We refer to these points as nodes three of which are indicated by the red arrows arrows 1 2 and 3 in Figure 2\(c Arrow 1 is pointing at node   denoting that when the speaker laughs the listener gazes at no one Arrow 2 is pointing at node   when the speak er laughs the listener nods and arrow 3 at node   when the speak er shak es their head the listener gazes at another listener These nodes can vary in diameter re”ecting the size of the mined con“dence value given the rule A set of example node sizes are presented in the right corner of Figure 2\(c Using this structure we can ef“ciently visualise prominent rules when comparing social scenarios simplifying a potentially complex set of social behavioural information V EVALUATION A IDENTIFY DISTINCT TRENDS To identify trends in social behaviour between a listener and speaker in an interested and not interested scenario we performed data mining separately on our interested and not interested datasets of social signal labels as detailed in Section III 357 rules in total were extracted from the mining in the interested scenario 63 of which had single antecedents i.e  R A i   1 153 with two antecedent 133 with three antecedents and 8 with four antecedents Mining in the not interested scenario extracted 396 rules consisting of 63 rules with single antecedents 162 with two antecedents 162 with three antecedents and 9 with four antecedents Such complex rules up to 4 dimensions/antecedents would be impossible to derive any other way than analytically Using the con“dence values derived from these rules two SDMs were built as shown in Figure 3\(a and 3\(b Figure 3\(a is the SDM of a speaker given an interested listener and Figure 3\(b is the SDM of a speaker given a not interested listener By observing both diagrams the similarities they share are instantly noticeable All nodes on the third pentagon from the top third biggest ring representing the listeners social response gazing at speak er These are prominent in all instances of the speakers social signals in both diagrams A similar trend exists with minor variations in nodes on the third pentagon from the bottom third smallest ring representing the listeners social response laughing From this observation we can see that contrary to some social interaction studies neither a constant gaze at speaker nor long periods of laugher can distinguish between an interested or not interested listener in a conversation The clearest distinction between the two diagrams are the nodes on the smallest pentagon colour coded light blue representing the listeners social response v oiced Voiced regions imply an exchange of short single words like 954 


Fig 3 a SDM generated from the mined interested listeners con“dence values b SDM generated from the mined not interested listeners con“dence values uh-huh or yea Voiced is a v ocal form of backchannel response 21][22 Backchannel responses are used by the listener to give feedback to the speaker expressing acknowledgement understanding and presence in the conversation  Here we see that a majority of these V nodes are bigger in the interested scenario when compared to the not interested scenario especially in the rule   when the speaker is gazing directly at the listener The next discernible trend belongs to the nodding nodes  on the fth pentagon from the centre colour coded dark blue Similar to voiced nodding N is a visual form of backchannel response  used to con“rm engagement in the conversation Although the listener in the not interested scenario mirrors the speaker well in comparison to the interested scenario i.e   the listener in the not interested scenario barely nods in response to any other social signals In this case mirroring is not a discerning social behaviour between an interested and not interested listener However from the diagram we can see that the interested listener nods more consistently in response to the other social signals especially when the speaker gazes directly at the listener i.e  N The nal discernible trend is the talking social response  second pentagon from the centre colour coded red While only a mild occurrence in the interested scenario it rarely occurs at all in the not interested scenario Talking suggests turn-taking  whereby the listener attempts to participate in the conversation whilst the speaker is speaking To validate these ndings using these con“dence values we take the ratio of interested and not interested results for matching rules We obtain results of greater than 1 when the rule occurs more frequently in the interested scenario and less than 1 when they occur more in the not interested scenario The results are shown in Table II Rows 1 2 and 5 relating to the listeners social responses T and respecti v ely  are the most prominent distinct trends between the two scenarios with an average of greater than 1  5 as shown on the last column of Table II Also rows 3 and 4 relating to the listeners social responses and  produce an a v erage of 1 v arying equally in both the interested and not interested social scenarios This is analogous to our earlier observations This proves that a clear distinction between an interested and not interested listener can be determined using the SDM The next step is to use these distinct social signals for conversation interest prediction B CONVERSATION INTEREST PREDICTION Using the discerning conversation social signals we attempt to use the SDM to accurately predict conversation interest To perform this test we eliminate one persons social activity from our dataset then perform mining using only the other two peoples social interaction with each other This is done for the combined dataset of the interested and not interested scenarios resulting in an SDM This SDM becomes the trained classi“er We then observe all the eliminated persons social responses when in the role of a listener in both social scenarios Using the SDM classi“er we attempt to predict conversation interest based on the generalisation of rules across the subjects the model was trained on The predictions are done on the entire dataset using different time frame windows ranging from 100 frames 4 seconds to 7000 frames approx 4 1  2 minutes with 1 frame increments There are three people in our dataset so we are able to perform this test three times once for each person alternating the eliminated listener The results are shown in Figure 4 As shown in Figure 4 with only 4 seconds of observation we obtain predictions better than random However as more evidence accumulates the performance increases to 90 for a time window of approximately 4 1 2 minutes Theses results prove the SDM can derive distinct social trends between the two scenarios which can generalise well for accurate predictions VI CONCLUSION SDMs can accurately predict conversational interest Unlike other methods we show that by using data mined con“dence values discerning trends in exchanges in social signals is straight forward without the need for psychological observations This approach is not context dependent 955 


 Speaker L S N A G-L G-OL G-N Aver Listener V 1.4 1.7 1.5 1.4 1.5 1.1 1.7 1.5 T 2.2 3.9 14 2.2 2.7 2.5 4.4 4.6 L 0.9 1.2 1 1 0.8 0.9 1 1 S 0.5 0.7 5 0.7 0.7 0.3 0.6 1.2 N 1.8 2 1.2 1.6 1.9 1.6 1.9 1.7 A 0.8 1.2 1.3 1.6 1.1 0.9 1.1 1.1 G-S 0.9 1 1 1 0.9 1 1 1 G-OL 0.8 0.9 0.7 0.7 1 0.7 1 0.9 G-N 1.2 1.1 1.3 1.1 1.5 1 0.9 1.1 TABLE II R ATIO OF interested TO not interested CONFIDENCE VALUES FOR MATCHING RULES A N AVERAGE IS CALCULATED IN THE LAST COLUMN  Fig 4 Prediction percentage scores using varying frame windows for each person and future work will explore larger datasets of other social scenarios such as debates arguments and social signal exchanges between partners A means of real-time predictions would be a valuable addition which only requires a tool for extracting social signals of multiple people from a live video stream One of the main bene“ts of this model is it has vast applications It can be used for generating social behaviour in computer generated avatars used in the movie and gaming industries and in Human Computer Interaction HCI It can also be used to assist social scientists and medical psychologists in diagnosing certain social related conditions with just a short period of observation R EFERENCES  M Knapp Non v erbal communication in human interaction  i n Harcourt Brace College Publishers  1972  M Ar gyle The psychology of interpersonal beha viour   in Penguin  1967  A Agra w al T  Imielinski and A Sw ami Mining association rules between sets of items in large databases in In Proc of the ACM SIGMOD Int Conf on Management of Data SIGMOD  1993  P  Ekman and W  Friesen F acial action coding system  i n Consulting Psychologists Press Palo Alto CA  1977  M Ar gyle Bodily communication  i n Methuen  1987  A K endon R Harris and M K e y  Or ganisation of beha vior in f ace to face interaction in The Hogue Netherlands Mouton  1975  D Gatica-Perez I McCo w an D Zhang and S Bengio Detecting group interest-level in meetings in Proc IEEE Int Conf on Acoustics Speech and Signal Processing  2005  A Dielmann and S Renals Automatic meeting se gmentation using dynamic bayesian networks IEEE Trans on Multimedia  2007  N Ambady  F  Bernieri and J Richeson T o w ard a histology of social behavior Judgmental accuracy from thin slices of the behavioral stream Advances in experimental social psychology  2000  A V inciarelli M P antic and H Bourlard Social signal processing Survey of an emerging domain Image and Vision Computing  vol 27 no 12 pp 1743…1759 2009  A V inciarelli M P antic H Bourlard and A Pentland Social signal processing state-of-the-art and future perspectives of an emerging domain in Proceeding of the 16th ACM Int Conf on Multimedia  ACM 2008 pp 1061…1070  A Pentland Social signal processing in IEEE Signal Processing Magazine  2007 pp 108…111    A computational model of social signaling  i n 18th Int Conf on Pattern Recognition ICPR  2006  J Curhan and A Pentland Thin slices of ne gotiation Predicting outcomes from conversational dynamics within the rst 5 minutes Journal of Applied Psychology  vol 92 no 3 pp 802…811 2007  N Ambady and R Rosenthal Thin slices of e xpressi v e beha vior as predictors of interpersonal consequences A meta-analysis Psychological Bulletin  vol 111 no 2 pp 256…274 1992  N Eagle and A Pentland Reality mining sensing comple x social systems Personal and Ubiquitous Computing  2006  A Mertins and J Rademacher  Frequenc y-w arping in v ariant features for automatic speech recognition in 2006 IEEE Int Conf on Acoustics Speech and Signal Processing 2006  vol 5 2006  P  V iola and M Jones Rapid Object Detection using a Boosted Cascade of Simple Features in Proc IEEE CVPR  2001  E J Ong Y  Lan B J Theobald R Harv e y  and R Bo wden Robust facial feature tracking using selected multi-resolution linear predictors in Int Conf Computer Vision  2009  R Agra w a l and R Srikant F ast algorithms for mining association rules in large databases in In VLDB94 Proc of 20th Int Conf on Very Large Data Bases  1994 pp 487…499  V  Yngv e On getting a w ord in edge wise  i n Papers from the sixth regional meeting of the chicago linguistic society  1970 pp 567…577  A Mulac K Erlandson W  F arrar  J  Hallett J Mollo y  and M Prescott uh-huh whats that all about differing interpretations of conversational backchannels and questions as sources of miscommunication across gender boundaries in Communication Research  1998  M Schr  oder D Heylen and I Poggi Preception of non-verbal emotional listener feedback in Proc of Speech Prosody  2006  E Gof fman Replies and responses  i n Language in Society  1976 956 


pnewitemarray pnewitemarray=ptempitem>productid ptempitem=ptempitem->next cofnewitemarray=cofnewitemarray+1  else  newdbitemset=new dbitemset item item  customer_id customer_id customer_id  item item    item item  item item  item item   item item   itemsets for each y? { Tk-1 Tk-1} do  // Tk-1 is all frequent k-1-itemsets in t. Tk-1  Tk-1 is a natural join of Tk-1 and Tk-1 on the first k-2 items 


if? z| z=k-1 subset of y ? ?Hk-1.hassupport\(z then Hk.add\(y itemsets=itemsets? y end Dk= Dk? t   //such that t contains itemsets only in the set itemsets End Hk.prune\(min_sup IV. TEST CONTRAST The test data are sale data from a real company. There are 5581 affairs and 1559 different fields, in Fig.1 the data structure of improved PHP algorithm, candidates k-itemset Hash are in database Dk Fig.1 the structure of database Dk of improved PHP algorithm  The structure of the node customer_id is struct tid  short customerid struct tid *next struct dbitemset* pdbitemset  The structure of item struct dbitemset  short *pitem struct dbitemset *next  In the structure of dbitemset, pitem is point the field or item of fixed size array. For put out the itemset from array, should know the length k of the itemset , and take out k items combinations from array to gain a itemset. The size of tha array should be proper to fill the itemset, not too much nodes there, to save the memory size. The main program of candidate k-itemset ot database Dk is as follow newdbitemset=new dbitemset pnewitemarray=new short[SIZEOFPITEMARRAY ptempitem=pkitemset; // pkitemset point to the list of candidate k-itemset pnewitemarray=ptempitem->productid;     //product ID is the item of candidate k-itemset newdbitemset->pitem=pnewitemarray 


newdbitemset->next=NULL thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next while\(ptempitem!=NULL  if\(cofnewitemarray<SIZEOFP ITEMARRAY  pnewitemarray pnewitemarray=ptempitem>productid ptempitem=ptempitem->next cofnewitemarray=cofnewitemarray+1  else  newdbitemset=new dbitemset item item  customer_id customer_id customer_id  item item    item item  item item  item item   item item 


  pnewitemarray=new short[SIZEOFPITEMARRAY pnewitemarray=ptempitem>productid newdbitemset->pitem=pnewitemarray newdbitemset->next=NULL thisnewdbitemset->next=newdbitemset thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next   The test result is as Tab TABLE 1 ITEMSETS IN DIFFERENT MINIMUM SUPPORT COUNT The minimu m support count F1 | F| 2 | F| 3 F  4  F 5 Frequent set  7 1559 406 0 0 0 1965 6 1559 1960 0 0 0 3519 5 1559 8605 8 0 0 10172 4 1559 34172 129 1 4 2 35876 It can get the same frequent activities that PHP algorithm and improved PHP algorithm, in Tab.2 we can find that there is inverse ratio between frequent activity and minimum support count, when the minimum support counts decrease 


frequent activities increase, just as the real world. The cost of CPU time compared as Fig.2 of these two algorithms 213 124 89 73 57 53 52 65 0 40 80 120 160 200 240 4 5 6 7 Minimum Support Count T i m e  s e c o n d s  PHP Improved Fig.2 the compare CPU time cost of PHP and improved PHP From Fig.2, as support count decreasing, the CPU running time of these two algorithms gets increasing, but the PHP is much speeder than the other, especially when count in 6. so wen know the more frequent itemsets and their items, the more efficient of the algorithm V. THE APPLICATION After we got the frequent itemset by the improved PHP algorithm, we can generate the strong associate rule that satisfied minimum support and minimum confidence. The 


confidence can be computed by Confidence\(A ? B A | B A  B support_count\(A Support_count\(A  B support_count\(A And then we can use confidence threshold strengthen the association, we use improved PHP algorithm dig X company 2002 year sale data, minimum support is 5, 3-itemset, they are 83,549,915}, {485,558,1290}, {454,1097,1546 83,549,982}, {631,980,1490}, {454,1103,1546 810,1026,1469}, {360,830,1036}, and their none null subset as for frequent 3-item {83,549,915}, we can get 83 ? 549? 915,      confidence = 5/9 = 56 83 ? 915? 549,      confidence = 5/5 = 100 915 ? 549? 83,      confidence = 5/5 = 100 83? 915 ? 549,      confidence = 5/50 = 10 549? 83 ? 915,      confidence = 5/78 = 6 915? 83 ? 549,      confidence = 5/73= 7 If the minimum confidence threshold is 80%, only if 83 ? 915 ? 549and 915 ? 549 ? 83 can generate strong associate rules, after these rules, we can not only arrange the shelf of related goods in pairs or groups, but also combine those goods that have most consumers, promote the consumer the pretermission goods. For example, the stronger associate rule 83 ? 915? 549, from the database field product, we know product_id corresponding product_name, this stronger rule is pillow ? pillow clothe ? bedsheet , if there is someone bought pillow and pillow clothe, then promote good bedsheet is an efficient way of sale VI. CONCLUSION We put Hash candidates k-itemset into affair of Dk, that is itemset include in affair, so CPU running time decreased improved PHP algorithm is more efficiency, we can use this improved PHP algorithm in commercial database digging and other wide usages REFERENCES 1] YANG Xuejun. The application of CRM[J].computer application,2002\(5 2] CHEN Shuangqiu, LIU Dinghong, LI Hongxing, the costumer analysis and design based on data warehouse[J].computer engineering and application,2001\(4 3] Jawei Han and Micheline Kamber,Data Mining:Comcepts and 


Techniques \(2001 4] J.D.Holt,and S.M.Chung, Efficient Mining of Association Rules in Text Databases CIKM99,  Kansas City, USA, pp.234-242, \(Nov.1999 5] J.S.Park, M.S.Chen and P.S.Yu, Using a Hash-Based Method with Transaction Trimming for Mining Association Rules, IEEE Transactions on Knowledge and Data Engineering, Vol.9, No.5 Sept/Oct, 1997 6] S. A. zel and H. A. Gvenir, An Algorithm for Mining Association Rules Using Perfect Hashing and Database Pruning, in: Proceedings of  pnewitemarray=new short[SIZEOFPITEMARRAY pnewitemarray=ptempitem>productid newdbitemset->pitem=pnewitemarray newdbitemset->next=NULL thisnewdbitemset->next=newdbitemset thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next   The test result is as Tab TABLE 1 ITEMSETS IN DIFFERENT MINIMUM SUPPORT COUNT The minimu m support count F1 | F| 2 | F| 3 F  4  F 5 Frequent set  7 1559 406 0 0 0 1965 6 1559 1960 0 0 0 3519 


5 1559 8605 8 0 0 10172 4 1559 34172 129 1 4 2 35876 It can get the same frequent activities that PHP algorithm and improved PHP algorithm, in Tab.2 we can find that there is inverse ratio between frequent activity and minimum support count, when the minimum support counts decrease frequent activities increase, just as the real world. The cost of CPU time compared as Fig.2 of these two algorithms 213 124 89 73 57 53 52 65 0 40 80 120 160 200 240 4 5 6 7 Minimum Support Count T i m e  s e c o n d s  PHP Improved Fig.2 the compare CPU time cost of PHP and improved PHP From Fig.2, as support count decreasing, the CPU running 


time of these two algorithms gets increasing, but the PHP is much speeder than the other, especially when count in 6. so wen know the more frequent itemsets and their items, the more efficient of the algorithm V. THE APPLICATION After we got the frequent itemset by the improved PHP algorithm, we can generate the strong associate rule that satisfied minimum support and minimum confidence. The confidence can be computed by Confidence\(A ? B A | B A  B support_count\(A Support_count\(A  B support_count\(A And then we can use confidence threshold strengthen the association, we use improved PHP algorithm dig X company 2002 year sale data, minimum support is 5, 3-itemset, they are 83,549,915}, {485,558,1290}, {454,1097,1546 83,549,982}, {631,980,1490}, {454,1103,1546 810,1026,1469}, {360,830,1036}, and their none null subset as for frequent 3-item {83,549,915}, we can get 83 ? 549? 915,      confidence = 5/9 = 56 83 ? 915? 549,      confidence = 5/5 = 100 915 ? 549? 83,      confidence = 5/5 = 100 83? 915 ? 549,      confidence = 5/50 = 10 549? 83 ? 915,      confidence = 5/78 = 6 915? 83 ? 549,      confidence = 5/73= 7 If the minimum confidence threshold is 80%, only if 83 ? 915 ? 549and 915 ? 549 ? 83 can generate strong associate rules, after these rules, we can not only arrange the shelf of related goods in pairs or groups, but also combine those goods that have most consumers, promote the consumer the pretermission goods. For example, the stronger associate rule 83 ? 915? 549, from the database field product, we know product_id corresponding product_name, this stronger rule is pillow ? pillow clothe ? bedsheet , if there is someone bought pillow and pillow clothe, then promote good bedsheet is an efficient way of sale VI. CONCLUSION We put Hash candidates k-itemset into affair of Dk, that is itemset include in affair, so CPU running time decreased improved PHP algorithm is more efficiency, we can use this improved PHP algorithm in commercial database digging and 


other wide usages REFERENCES 1] YANG Xuejun. The application of CRM[J].computer application,2002\(5 2] CHEN Shuangqiu, LIU Dinghong, LI Hongxing, the costumer analysis and design based on data warehouse[J].computer engineering and application,2001\(4 3] Jawei Han and Micheline Kamber,Data Mining:Comcepts and Techniques \(2001 4] J.D.Holt,and S.M.Chung, Efficient Mining of Association Rules in Text Databases CIKM99,  Kansas City, USA, pp.234-242, \(Nov.1999 5] J.S.Park, M.S.Chen and P.S.Yu, Using a Hash-Based Method with Transaction Trimming for Mining Association Rules, IEEE Transactions on Knowledge and Data Engineering, Vol.9, No.5 Sept/Oct, 1997 6] S. A. zel and H. A. Gvenir, An Algorithm for Mining Association Rules Using Perfect Hashing and Database Pruning, in: Proceedings of  the Tenth Turkish Symposium on Artificial Intelligence and Neural Networks \(TAINN'2001 Eds Gazimagusa, T.R.N.C. \(June 2001 7] SUN Zhengxi, The theory and technology of Intelligent control M].Beijing Qinghua press, 1997 8] Mitani, Koji, CRM pursues economies of depth [J]. Japanese Journal of Diamond Harvard Business, 1999, \(617  the Tenth Turkish Symposium on Artificial Intelligence and Neural Networks \(TAINN'2001 Eds Gazimagusa, T.R.N.C. \(June 2001 7] SUN Zhengxi, The theory and technology of Intelligent control M].Beijing Qinghua press, 1997 8] Mitani, Koji, CRM pursues economies of depth [J]. Japanese Journal of Diamond Harvard Business, 1999, \(617  


algorithm could not extract the correct hierarchy with 30 assigning five labels incorrectly to the root label. None of the HE algorithms could extract the correct hierarchy in the absence of 40% multi-labels. With 40% and Voting, the number of labels falsely assigned to the root was 13, while with GT it was only three. For BoosTexter, Voting assigned two labels wrongly to the root label in the experiment with TABLE I 20 NEWSGROUPS ALL, -20%, -30% AND -40% RESULTS Measure all 20% 30% 40%ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT A 0.635 0.638 0.429 0.549 0.613 0.633 0.383 0.456 0.596 0.619 0.322 0.412 0.563 0.591 0.255 0.387 F1 0.694 0.696 0.565 0.677 0.675 0.688 0.528 0.604 0.662 0.677 0.469 0.566 0.640 0.657 0.392 0.542 F 0.691 0.692 0.480 0.605 0.671 0.688 0.429 0.507 0.658 0.676 0.364 0.465 0.630 0.652 0.296 0.441 OE 0.221 0.220 0.336 0.222 0.259 0.236 0.387 0.275 0.273 0.259 0.415 0.301 0.301 0.291 0.434 0.316 RL 0.100 0.098 0.124 0.073 0.108 0.110 0.128 0.077 0.098 0.103 0.132 0.079 0.101 0.106 0.135 0.082 C 4.188 4.168 6.080 4.164 4.397 4.446 6.184 4.286 4.246 4.340 6.326 4.328 4.334 4.463 6.397 4.379 AP 0.789 0.790 0.677 0.778 0.774 0.782 0.657 0.758 0.769 0.774 0.645 0.747 0.759 0.764 0.638 0.740 AUPRC 0.775 0.772 0.618 0.749 0.743 0.727 0.581 0.691 0.733 0.722 0.555 0.671 0.715 0.708 0.535 0.660 H-loss 0.103 0.123 0.121 0.094 0.102 0.098 0.124 0.108 0.106 0.103 0.132 0.117 0.115 0.111 0.145 0.122 Wins 1 5 0 3 1 6 0 2 2 6 0 1 2 6 0 1 LCAPD 0 0 0 0 0 0 0 0 0 0 0.12 0 0.05 0 0 0 0.51 0.26 0.17 0 CTED 0 0 0 0 0 0 0 0 0 0 0.14 0 0.07 0 0 0 0.50 0.21 0.21 0 TO* 0 0 0 0 0 0 0 0 0 0 0.11 0 0.05 0 0 0 0.39 0.18 0.15 0 30% removed labels and and six labels in the experiment with 40% removed labels. GT resulted in zero distances in the both cases. Assigning more labels to the root creates more shallow and wider hierarchies \(trivial case as stated before The good hierarchy extraction with ART networks demonstrates the system robustness  even with strongly damaged data the system can rebuild the original hierarchy C. RCV1-v2 Dataset The next experiment was based on the tokenized version of 


the RCV1-v2 dataset introduced in [21]. Only the topics label set consisting of 103 labels arranged in a hierarchy of depth four is examined here. Documents of the original training set of 23,149 were converted to TF-IDF weights and normalized Afterwards the set was splitted in 15,000 randomly selected documents as training and the remaining as test samples In this case, the Voting variant of HE applied to the TTML resulted in the LCAPD, CTED and TO* values 0.12, 0.15 and 0.13, respectively. The corresponding values of the GT variant are 0.05, 0.07 and 0.05. The poor performance of the Voting method is due to the fact that for the TTML only very high threshold values succeed in removing enough noise The Voting results are thus dominated by bad hierarchies extracted for all but the highest thresholds The classification and HE results for this dataset are shown in Table II. ML-ARAM has better performance results on this data set in all points than ML-FAM except for RL being the best of all classifiers in terms of the multi-label performance measures. BoosTexter is the best in terms of all ranking measures For both HE algorithms the distances of BoosTexter are the best, those of ML-FAM second, followed ML-ARAM and ML-kNN. All three distance measures correlate. Interesting is also that for ML-kNN the distance values obtained by both HE methods are almost the same The hierarchy extracted by GT from the TTML has much lower distances values as compared with the hierarchies extracted by both methods from predicted multi-labels. This reflects a specific problem of HE, since only a small fraction of the incorrectly classified multi-labels can prevent building of a proper hierarchy. For example, 16.5% of misassigned labels in the extracted hierarchy are responsible for about 80% of LCAPD calculated from the predictions of MLARAM. This large part of the HE error is caused by only 4% of the test data. Under these circumstances the other distances behave analogically. Most labels were not assigned making them trivial edges, but six labels were assigned to a false branch. This can happen when labels have a strong correlation and in the step Hierarchy Construction of the basic algorithm the parent is not unique in the confidence matrix. BoosTexters results suffer less from this problem because it generally sets more labels for each test sample 


Both HE algorithms behaved similarly on the predictions of the ART networks. They constructed a deeper hierarchy than the original one and wrongly assigned the same 11 labels to the root node. The higher distances come from Voting assigning more labels to the wrong branch. For MLkNN both algorithms again create very similar hierarchy trees, both misassigned 28 labels to the root label. For BoosTexter it was seven with Voting and eight with GT Voting produced a deeper hierarchy here D. WIPO-alpha Dataset The WIPO-alpha dataset1 comprises patent documents collected by the World Intellectual Property Organization WIPO ments. Preprocessing was performed as follows: From each document, the title, abstract and claims texts were extracted stop words were removed using the list from [20] and words were stemmed using the Snowball stemmer [22]. All but the 1%-most-frequent stems were removed, the remaining stems were converted to TF-IDF weights and these were normalized to the range of [0, 1]. Again, TF-IDF conversion and normalization were done independently for the training and the test set. The original hierarchy consists, from top to bottom, of 8 sections, 120 classes, 630 subclasses and about 69,000 groups. In our experiment, only records from the sections A \(5802 training and 5169 test samples H \(5703 training and 5926 test samples 1http://www.wipo.int/classifications/ipc/en/ITsupport/Categorization dataset/wipo-alpha-readme.html August 2009 TABLE II RCV1-V2 RESULTS Measure ARAM FAM kNN BoosT A 0.748 0.731 0.651 0.695 F1 0.795 0.777 0.735 0.769 F 0.805 0.787 0.719 0.771 OE 0.077 0.089 0.104 0.063 RL 0.087 0.086 0.026 0.015 C 11.598 11.692 8.563 5.977 AP 0.868 0.860 0.839 0.873 AUPRC 0.830 0.794 0.807 0.838 H-loss 0.068 0.077 0.097 0.081 Wins 4 0 0 5 LCAPD 0.29 0.22 0.25 0.20 0.34 0.34 0.21 0.18 


CTED 0.32 0.23 0.28 0.22 0.38 0.37 0.24 0.20 TO* 0.27 0.18 0.22 0.17 0.31 0.30 0.21 0.17 document in the collection has one so-called main code and any number of secondary codes, where each code describes a group the document belongs to. Both main and secondary codes were used in the experiment, although codes pointing to groups outside of sections A and H were ignored. We also removed groups that did not contain at least 30 training and 30 test records \(and any documents that only belonged to such small groups 7,364 test records with 924 attributes each and a label set of size 131 In this case, the Voting variant of the HE algorithm applied to the TTML resulted in the LCAPD, CTED and TO* values of 0.13, 0.12 and 0, respectively. GT showed the same values. Remarkable are the TO* distances, which are equal to 0. This is due to the fact that the WIPO-alpha hierarchy contains 16 single-child labels that are not partitioned by the true multi-labels: whenever a single-child label j is contained in a multi-label, so is its child, and vice versa. It is therefore theoretically impossible to deduce from the multilabels which of them is the parent of the other. As a result the HE algorithms often choose the wrong parent, resulting in higher LCAPD and CTED values. TO*, as described above is invariant under such choices The results obtained on the WIPO-alpha dataset are shown in Table III. The classification performance of the ART-based networks on this dataset is slightly worse than that of BoosTexter. Mostly in the terms of OE, RL, C, AP, AUPRC, and H-loss measures BoosTexter is better because its rankings are better and it assigned more labels to each sample. But the ART networks have better HE results because their predicted labels are more consistent with the original hierarchy. MLkNN has the worst classification results and distance values again. The reason for the high relative difference between LCAPD as well as CTED and TO* obtained for the ART networks or BoosTexter as compared to the results of the other datasets is because most of the labels were assigned in the right branch but not exactly where they belong Both HE algorithms extracted the same hierarchy from the predictions of ML-ARAM and a very similar hierarchy for ML-FAM. About 5% labels were assigned wrongly to the 


root label in the hierarchies of the ART networks. For MLTABLE III WIPO-ALPHA\(AH Measure ARAM FAM kNN BoosT A 0.588 0.590 0.478 0.564 F1 0.694 0.691 0.614 0.693 F 0.682 0.682 0.593 0.679 OE 0.052 0.057 0.110 0.042 RL 0.135 0.136 0.056 0.025 C 25.135 25.269 22.380 11.742 AP 0.790 0.785 0.724 0.802 AUPRC 0.720 0.684 0.688 0.762 H-loss 0.090 0.093 0.149 0.079 Wins 1 2 0 6 LCAPD 0.16 0.16 0.17 0.17 0.32 0.38 0.21 0.21 CTED 0.18 0.18 0.19 0.19 0.38 0.53 0.27 0.27 TO* 0.05 0.05 0.07 0.07 0.24 0.32 0.08 0.08 kNN both HE methods wrongly assigned about the half of the labels and about 20% of total labels were assgined to the root label. Here, GT extracted a much worse hierarchy as shown by CTED being 0.15 higher for GT than for Voting For BoosTexter both HE methods built the same hierarchy and no label was wrongly assigned to the root. All extracted hierarchies were one level deeper than the original one Although Voting produced worse hierarchies than GT on two previous datasets, this time its distance values were comparable or even better. In comparison to Voting, GT has higher values for all distances on the multi-labels of MLkNN. Voting has the advantage of being a much simpler method and of being more dataset independent. Still the tree distances have the same ranking order for all classifiers for both HE methods VI. CONCLUSION In this paper we studied Hierarchical Multi-label Classification \(HMC tive was to derive hierarchical relationships between output classes from predicted multi-labels automatically. We have developed a data-mining-system based on two recently proposed multi-label extensions of the FAM and ARAM neural networks: ML-FAM and ML-ARAM as well as on a Hierarchy Extraction \(HE algorithm builds association rules from label co-occurrences 


and has two modifications. The presented approach is general enough to be used with any other multi-label classifier or HE algorithm. We have also developed a new tree distance measure for quantitative comparison of hierarchies In extensive experiments made on three text-mining realworld datasets, ML-FAM and ML-ARAM were compared against two state-of-the-art multi-label classifiers: ML-kNN and BoosTexter. The experimental results confirm that the proposed approach is suitable for extracting middle and large-scale class hierarchies from predicted multi-labels. In future work we intend to examine approaches for measuring the quality of hierarchical multi-label classifications REFERENCES 1] M. Ruiz and P. Srinivasan, Hierarchical text categorization using neural networks, Information Retrieval, vol. 5, no. 1, pp. 87118 2002 2] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni, Incremental algorithms for hierarchical classification, The Journal of Machine Learning Research, vol. 7, pp. 3154, 2006 3] , Hierarchical classification: combining Bayes with SVM, in Proceedings of the 23rd international conference on Machine learning ACM New York, NY, USA, 2006, pp. 177184 4] F. Wu, J. Zhang, and V. Honavar, Learning classifiers using hierarchically structured class taxonomies, in Proceedings of the 6th International Symposium on Abstraction, Reformulation And Approximation Springer, 2005, p. 313 5] L. Cai and T. Hofmann, Hierarchical document categorization with support vector machines, in Proceedings of the thirteenth ACM international conference on Information and knowledge management ACM New York, NY, USA, 2004, pp. 7887 6] C. Vens, J. Struyf, L. Schietgat, S. Dz?eroski, and H. Blockeel Decision trees for hierarchical multi-label classification, Machine Learning, vol. 73, no. 2, pp. 185214, 2008 7] E. P. Sapozhnikova, Art-based neural networks for multi-label classification, in IDA, ser. Lecture Notes in Computer Science, N. M Adams, C. Robardet, A. Siebes, and J.-F. Boulicaut, Eds., vol. 5772 Springer, 2009, pp. 167177 8] M. Zhang and Z. Zhou, ML-kNN: A lazy learning approach to multilabel learning, Pattern Recognition, vol. 40, no. 7, pp. 20382048 2007 9] R. Schapire and Y. Singer, BoosTexter: A boosting-based system for text categorization, Machine learning, vol. 39, no. 2, pp. 135168 


2000 10] K. Zhang, A constrained edit distance between unordered labeled trees, Algorithmica, vol. 15, no. 3, pp. 205222, 1996 11] A. Maedche and S. Staab, Measuring similarity between ontologies Lecture notes in computer science, pp. 251263, 2002 12] G. Carpenter, S. Martens, and O. Ogas, Self-organizing information fusion and hierarchical knowledge discovery: a new framework using ARTMAP neural networks, Neural Networks, vol. 18, no. 3, pp. 287 295, 2005 13] A.-H. Tan and H. Pan, Predictive neural networks for gene expression data analysis, Neural Networks, vol. 18, pp. 297306, April 2005 14] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps, IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 698713, 1992 15] Y. Freund and R. Schapire, A decision-theoretic generalization of online learning and an application to boosting, Journal of computer and system sciences, vol. 55, no. 1, pp. 119139, 1997 16] K. Zhang and T. Jiang, Some MAX SNP-hard results concerning unordered labeled trees, Information Processing Letters, vol. 49 no. 5, pp. 249254, 1994 17] G. Tsoumakas and I. Vlahavas, Random k-labelsets: An ensemble method for multilabel classification, Lecture Notes in Computer Science, vol. 4701, p. 406, 2007 18] K. Punera, S. Rajan, and J. Ghosh, Automatic construction of nary tree based taxonomies, in Proceedings of IEEE International Conference on Data Mining-Workshops. IEEE Computer Society 2006, pp. 7579 19] T. Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization, in Proceedings of the Fourteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1997, pp. 143151 20] A. McCallum, Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering, 1996 http://www.cs.cmu.edu/ mccallum/bow 21] D. Lewis, Y. Yang, T. Rose, and F. Li, RCV1: A new benchmark collection for text categorization research, The Journal of Machine Learning Research, vol. 5, pp. 361397, 2004 22] M. Porter, Snowball: A language for stemming algorithms, 2001 http://snowball.tartarus.org/texts/introduction.html 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


