A Brief Review on Freq uent Pattern Mining Xun Zhu 1 Hongtao Deng 2  School of Mathematics & Computer Science Jianghan University Wuhan, China iammodern@qq.com 1 hongtaodeng@126.com 2  Zheng Chen 3  School of Arts & Science Jianghan University Wuhan, China 420552383@qq.com   Abstract 227frequent pattern mining plays an essential role in association rule mining, which has been a focused theme in data mining research for over the past 15 years. Since the pioneering work of Agrawal, abundant literature has been dedicated to this research. In this article, we provide a brief overview of the 
current status of frequent pattern mining and discuss a few promising research directions Keywords-frequent pattern mining; association rules application; review I   INTRODUCTION Frequent patterns are itemsets that appear in a data set with frequency no less than a user-specified threshold. Finding frequent patterns plays an essential role in mining associations correlations, and many other interesting relationships among data. Moreover, it helps in data indexing, classi 336 cation clustering, and other data mining tasks as well. Thus, frequent pattern mining has become an important data mining task and a focused theme in data mining research Frequent pattern mining was first proposed by Agrawal et al.[1,2 f o r m ark et  bas k et an a l y s is in th e f o rm of as s o ci ati o n  
rule mining, which is useful for discovering interesting relationships hidden in large data sets. For example, we may find a strong relationship, which can be represented in the form of association rules or sets of frequent items, exists between the sale of diapers and beer because many customers who buy diapers also buy beer. Retailers can use this relationship to help them identify new opportunities for cross-selling their products to the customers. Besides market basket data, association rule mining is also application to other application domains such as bioinformatics, multimedia data mining, Web mining, and scientific data analysis Since the first proposal of this new data mining task and its associated efficient mining algorithms, there have been hundreds of follow-up research publications. In this article, we 
perform a brief overview of frequent pattern mining methods representation and applications II  BASIC  CONCEPT This section reviews the basic terminology used in association analysis and presents a formal description of the task Itermset and Support Count Let 12     d I ii i 
 are the set of all items in a market basket data and 12     N Ttt t  are the transactions. Each transaction i t  contains a subset of items chosen from I In association analysis, a collection of zero or more items is termed an itemset 
If an itemset contains k items, it is called a k-itemset A transaction i t is said to contain an itemset X if X is a subset of i t Mathematically, the support count  X 
002 for an itemset X can be stated as     iii XtXttT 
002 003\004 1 The symbol 200 denotes the number of elements in a set Association Rule An association rule is an implication 
expression of the form XY 
005 where X and Y are disjoint itemsets, i.e XY 
006=\007 The strength of an association rule can be measured in terms of its support  and confidence  Support determines how often a rule as applicable to a given data set, while confidence determines how frequently items in Y appear in transactions that contain X The formal definitions of these metrics are     XY Support s X Y N 
002 002 b 005 3 Frequent Itemset A k-itemset X is frequent if X in a transaction database T no lower than T 
t times, where 
002 b 005 2      XY Confidence c X Y X t is a user-specified minimum support threshold \(called min_sup  and T is the total number of transactions in T 
 Association Rule Mining The association rule mining problem can be formally stated as follows: Given a set of transactions T find all the rules having support 978-1-4244-9857-4/11/$26.00 \2512011 IEEE 
n min_sup and confidence n min_conf, where min_sup and min_conf are the user-specified support and confidence thresholds A common strategy adopted by many association rule mining algorithms is to decompose into two major subtasks Frequent Itemset Generation whose objective is to find all the itemsets that satisfy the min_sup threshold; and Rule 


Generation whose objective is to extract all the highconfidence rules from the frequent itemsets found in the frequent itemset generation step The computational requirements for frequent itemset generation are generally more expensive than those of rule generation. Efficient techniques for generating frequent itemsets are reviewed in Section 001\013  III  E FFICIENT ALGORITHMS FOR MINING FREQUENT PATTRENS  A  Apriori algorithm and its extensions Since there are usually a large number of distinct single items in a typical transaction database, and their combinations may form a very huge number of itemsets, it is challenging to develop callable methods for mining frequent itemsets in a large transaction database. Agrawal and Srikant [2 obs er v e d  an interesting downward closure property, called Apriori among frequent k-itemsets: A k-itemset is frequently if all of its sub-itemsets are frequent. This implies that frequent itemsets can be mined by first scanning the database to find the frequent 1-itemsets, then using the frequent 1-itemsets to generate candidate frequent 2-itemsets, and check against the database to obtain the frequent 2-itemsets. This process iterates until no more frequent k-itemsets can be generated for some k This is the essence of the Apriori algorithm [2 d  it s  alternative [3  Since the Apriori algorithm was proposed, there have been extensive studies on the improvements or extensions of Apriori, e.g., Park et al h a sh t e c hni q u e t o  c ount  support, instead of comparing each itemset in the transaction with every candidate itemset. Toivonen [5 pr o p o s e d a  sampling-based frequent itemset generation algorithm. Geerts et al. [6 ri v e d a t i g h t  u p p e r bou n d of  t h e n u m ber of  candidate patterns that can be generated in the level-wise mining approach. This result is effective at reducing the number of database scans B  FP-growth algorithm and its extensions In many cases, the Apriori algorithm significantly reduces the size of candidate sets using the Apriori principle. However it can suffer from two-nontrivial costs: \(1\nerating a huge number of candidate sets, and \(2\epeatedly scanning the database and checking the candidates by pattern matching. Han et al. [7 e v i s e d an  F P g ro w t h  m e th o d  th a t  m i n e s th e c o m p le te set of frequent itemsets without candidate generation FP-growth works in a divide-and-conquer way. The first scan of the database derives a list of frequent items in which items are ordered by frequency-descending order. According to the frequency-descending list, the database is compressed into a frequent-pattern tree \(FP-tree\, which retains the itemset association information. The FP-tree is mined by starting from each frequent length-1 pattern \(as an initial suffix pattern constructing conditional pattern base, then constructing its conditional FP-tree, and performing mining recursively on such a tree. The pattern growth is achieved by the concatenation of the suffix pattern with the frequent patterns generated from a conditional FP-tree The FP-growth algorithm transforms the problem of finding long frequent patterns to searching for shorter ones recursively and then concatenating the suffix. It uses the least frequent items as a suffix, offering good selectivity. Performance studies demonstrate that the method substantially reduces search time There are many alternatives and extensions to the FPgrowth approach, including depth-first generation of frequent itemsets by Agarwal et al.[8   HMin e  by P e i et al  9  w h ich  explores a hyper-structure mining of frequent patterns; building alternative trees; exploring top-down and bottom-up traversal of such trees in pattern-growth mining by Liu et al  a nd a n  array-based implementation of prefix-tree-structure for efficient pattern growth mining by Grahne and Zhu [1 C  Eclat algorithm Both the Apriori and FP-growth methods mine frequent patterns from a set of transaction horizontal data format \(i.e TID: itemset}\, where TID is a transaction-id and itemset is the set of items bought in transaction TID. Alternatively mining can also be performed with data presented in vertical data format \(i.e. {item: TID_set Zaki [12 prop o s ed  E q ui val e nc e CLA SS T r a n sfor m a t i o n  Eclat\ algorithm by exploring the vertical data format. The first scan of the database builds the TID_set of each single item. Starting with a single item \(k = 1\ the frequent \(k+1\itemsets grown from a previous k-itemset can be generated according to the Apriori property, with a depth-first computation order similar to FP-growth [7  T h e co m p ut a t i o n  is done by inter section of the TID_sets of the frequent kitemsets to compute the TID_sets of the corresponding \(k+1 itemsets. This process repeats, until no frequent itemsets or no candidate itemsets can be found Besides taking advantage of the Apriori property in the generation of candidate \(k+1\itemset from frequent k-itemsets another merit of this method is that there is no need to scan the database to find the support of \(k+1\itemsets \(for k 0011 1\ This is because the TID_set of each k-itemset carries the complete information required for counting such support Another related work which mines the frequent itemsets with the vertical data format is [13  T h is w o rk d e m o n s tr at ed  that, though impressive results have been achieved for some data mining problems using highly specialized and clever data structures, one could also explore the potential of solving data mining problems using the general purpose database management systems IV  C OMPACT REPRESENTATION OF FREQUENT ITEMSETS  A major challenge in mining frequent patterns from a large dataset is the fact that such mining often generates a huge number of patterns satisfying the min_sup threshold, especially when min_sup is set low. This is because if a pattern is frequent, each of its sub-patterns is frequent as well. A large pattern will contain an exponential number of smaller, frequent sub-patterns. To overcome this problem, closed frequent pattern mining and maximal frequent pattern mining were proposed [14, 17   


A pattern 001 is a closed frequent pattern in a dataset D if 001 is frequent in D and there exists no proper super-pattern 002\025 such that 002\025 has the same support as 001 in D. A pattern 001 is a maximal frequent pattern \(or max-pattern\ in set D if 001 is frequent, and there exists no super-pattern 002\025 such that 001  000\000  002\025 and 002\025 is frequent in D. For the same min_sup threshold, the set of closed frequent patterns contains the complete information regarding to its corresponding frequent patterns; whereas the set of max-patterns, though more compact, usually does not contain the complete support information regarding to its corresponding frequent patterns The mining of frequent closed itemsets was proposed by Pasquier et al. [1  w h ere a n A p ri o r ib as e d alg o rith m cal le d  A-Close for such mining was presented. Other closed pattern mining algorithms include CHARM [1  a nd F P Cl o s e  16 The main challenge in closed frequent pattern mining is to check whether a pattern is closed. There are two strategies to approach this issue: \(1\to keep track of the TID list of a pattern and index the pattern by hashing its TID values. This method is used by CHARM which maintains a compact TID list; and \(2 to maintain the discovered patterns in a pattern-tree similar to FP-tree. This method is exploited by FPClose. Mining closed itemsets provides an interesting and important alternative to mining frequent itemsets since it inherits the same analytical power but generates a much smaller set of results. Better scalability and interpretability is achieved with closed itemset mining Mining max-patterns was first studied by Bayardo [1 where MaxMiner, an Apriori-based, level-wise, breadth-first search method was proposed to find max-itemset by performing superset frequency pruning and subset in frequency pruning for search space reduction. Another efficient method MAFIA, proposed by Burdick et al. [18 use s ver t i c a l  b i t m a p s  to compress the transaction id list, thus improving the counting efficiency. Yang [19  p r ov i d e d  th e o r e ti cal an a l y s is of  th e  complexity of mining max-patterns. The complexity of enumerating maximal itemsets is shown to be NP-hard V  APPLICATIONS  Frequent patterns mining has been applied to a variety of application domains, such as indexing and similarity search of complex structured data, multimedia mining, and web mining A  Indexing and similarity search of complex  data Complex objects such as transaction sequence, event logs proteins and images are widely used in many fields. Efficient search of these objects becomes a critical problem for many applications. Due to the large volume of data, it is inefficient to perform a sequential can on the whole database and examine objects one by one. High performance indexing mechanisms thus are in heavy demand in filtering objects that obviously violate the query requirement gIndex pro p o s e s a d i s c r i m i na t i v e  f r eq ue nt p a t t er nbased approach to index structures and graphs. A fundamental problem arises: if only frequent patterns are indexed, how to find those queries which only have infrequent patterns? gIndex solved this problem by replacing the uniform support constraint with a size-increasing support function, which has very low support for small patterns but high support for large patterns SeqIndex is one example using frequent pattern-based approach to index sequences. Taking frequent patterns as features, new strategies to perform structural similarity search were developed such as Gra  l a nd PI S [2 2 B  multimedia datamining A multimedia database system stores and manages a large collection of multimedia data, such as audio, video, image graphics, speech, text, document, and hyper text data Multimedia data mining is finding patterns and knowledge from multimedia data Frequent pattern analysis in multimedia data plays a similar important role in multimedia data mining. To mine frequent patterns in multimedia data, each image object can be treated as a transaction and frequently occurring patterns among different images can be discovered. Zaïane et al. [2  d e v e l o ped a progressive algorithm for mining multimedia associations Chengcui Zhang et al  pr opo sed  a n a u t o m a t i c  spatiotemporal mining system of rolling and adherent leukocytes for intravital videos C  Web mining  Web mining is the application of data mining techniques to discover patterns and knowledge from the We  T h er e ar e three different types of web mining: web content mining, web structure mining, and web usage mining. Web content mining is a knowledge discovery task of finding information within web pages, while web structure mining aims to discover knowledge hidden in the structures linking web pages. Web usage mining is focused on the analysis of users’ activities when they browse and navigate through the Web Association rules discovered for pages that are often visited together can reveal user groups n d c l ust e r w e b p a g e s Web access patterns via association rule mining in weblogs were proposed by [2  S e qu en t i a l  pa t t e rn m i n i n g i n w e bl og s  could find browse and navigation orders \(i.e. pages that are accessed immediately after another\, which might be used to refine cache design and web site design VI  R ESEARCH DIRECTIONS  Although abundant literature published in research into frequent pattern mining, however, there are still several critical research problems that need to be solved before frequent pattern mining can become a cornerstone approach in data mining applications First, the set of frequent patterns derived by most of the current pattern mining methods is too huge for effective usage There are proposals on reduction of such a huge set, including closed patterns, maximal patterns, approximate patterns representative patterns, clustered patterns, and discriminative frequent patterns. However, it is still not clear what kind of patterns will give us satisfactory pattern sets in both compactness and representative quality for a particular application, and whether we can mine such patterns directly and efficiently. Much research is still needed to substantially 


reduce the size of derived pattern sets and enhance the quality of retained patterns Second, we need mechanisms for deep understanding and interpretation of patterns, and contextual analysis of frequent patterns. The main research work on pattern analysis has been focused on pattern composition and frequency. The semantic of a frequent pattern includes deeper information: what is the meaning of the pattern; what are the synonym patterns; and what are the typical transactions that this pattern resides? In many cases, frequent patterns are mined from certain datasets which also contain structural information. We believe the deep understanding of frequent patterns is essential to improve the interpretability and the usability of frequent patterns. One initial study in this direction is done by Mei et al. [28   Finally, applications often raise new research issues and bring deep in sight on the strength and weakness of an existing solution. This is also true for frequent pattern mining. Much work is needed to explore new applications of frequent pattern mining. For example, bioinformatics has raised a lot of challenging problems, and we believe frequent pattern mining may contribute a good deal to it with further research VII  C ONCLUSIONS  In this article, we present a brief overview of the current status and future directions of frequent pattern mining. With over the past 15 years of extensive research, there have been hundreds of research publications and tremendous research development and application activities in this domain. It is impossible for us to give a complete coverage on this topic with limited space and our limited knowledge. Hopefully, this short overview may provide a rough outline of the recent work and give people a general view of the field. In general, we feel that as a young research field in data mining, frequent pattern mining has an achieved tremendous progress and claimed a good set of applications R EFERENCES  1  Agrawal R, Imielinski T, Swami A. Mining association rules between sets of items in large databases. In Proceedings of the 1993 ACMSIGMOD international conference on management of data\(SIGMOD’93\, pages207–216.1993 2  Agrawal R, Srikant R. Fast algorithms for mining association rules. In Proceedings of the 1994 international conference on very large databases\(VLDB’94\, pages487–499.1994 3  Mannila H, Toivonen H, Verkamo AI\(1994\ Ef  cient algorithms for discovering association rules. In Proceeding of the AAAI’94 workshop knowledge discovery in databases\(KDD’94\,pages181–192 .1994 4  Park JS, Chen MS, Yu PS. An effective hash-based algorithm for mining association rules. In Proceeding of the 1995 ACM SIGMOD international conference on management of data\(SIGMOD’95 pages175–186. 1995 5  Toivonen H. Sampling large databases for association rules. In Proceeding of the1996 international conference on very large databases\(VLDB’96\, pages134–145. 1996 6  Geerts F, Goethals B, Bussche J. A tight upper bound on the number of candidate patterns. In Proceeding of the 2001 international conference on data mining\(ICDM’01\,pages155–162. 2001 7  Han J, Pei J, Yin Y. Mining frequent patterns without candidate generation. In Proceeding of the 2000 ACM-SIGMOD international conference on management of data\(SIGMOD’00\,pages1–12.2000 8  Agarwal R, Aggarwal CC, Prasad VVV. A tree projection algorithm for generation of frequent itemsets. J Parallel Distribut Comput 61 pages350–371.2001 9  Pei J, Han J, Lakshmanan LVS. Mining frequent itemsets with convertible constraints. In Proceeding of the 2001 international conference on data engineering\(ICDE’01\,pages433–332. 2001   Liu J, Pan Y, Wang K, Han J. Mining frequent itemsets by opportunistic projection. In Proceeding of the 2002 ACM SIGKDD international conference on knowledge discovery in databases\(KDD’02\, pages239–248. 2002   Grahne G, Zhu J. Ef  ciently using pre  x-trees in mining frequent itemsets .In Proceeding of the ICDM’03 international workshop on frequent itemset mining implementations\(FIMI’03\, pages123 132.2003   Zaki MJ. Scalable algorithms for association mining. IEEE Trans Knowl Data Eng 12. pages372–390. 2000   Holsheimer M, Kersten M, Mannila H, Toivonen H. Aperspective on databases and datamining. In Proceeding of the 1995 international conference on knowledge discovery and data mining\(KDD’95 pages150–155. 1995   Pasquier N, Bastide Y, Taouil R, Lakhal L. Discovering frequent closed itemsets for association rules. In Proceeding of the 7 th international conference on database theory\(ICDT’99\. pages398–416. 1999   Zaki M J,Hsiao C J. CHARM: an ef  cient algorithm for closed itemset mining. In Proceeding of the 2002 SIAM international conference on data mining\(SDM’02\, pages457–473. 2002   Grahne G, Zhu J. Ef  ciently using pre  x-trees in mining frequent itemsets .In Proceeding of the ICDM’03 international workshop on frequent itemset mining implementations\(FIMI’03\, pages123 132.2003   Bayardo RJ. Ef  ciently mining long patterns from databases. In Proceeding of the 1998 ACM SIGMOD international conference on management of data\(SIGMOD’98\, pages85–93. 1998   Burdick D, Calimlim M, GehrkeJ. MAFIA: a maximal frequent itemset algorithm for transactional databases. In Proceeding of the 2001 international conference on data engineering\(ICDE’01\, pages443–452 2001   Yang G. The complexity of mining maximal frequent itemsets and maximal frequent patterns. In Proceeding of the 2004 ACM SIGKDD international conference on kowledge discovery in databases\(KDD’04 pages344–353. 2004   Yan X, Yu PS, Han J. Graph indexing: a frequent structure-based approach. In Proceeding of the 2004 ACM SIGMOD international conference on management of data\(SIGMOD’04\, pages335–346. 2004   Yan X, Yu PS, Han J. Substructure similarity search in graph databases In Proceeding of the 2005 ACM SIGMOD international conference on management of data\(SIGMOD’05\, pages766–777. 2005   Yan X, Zhu F, Han J, Yu PS. Searching substructures with superimposed distance. In Proceeding of the 2006 international conference on data engineering\(ICDE’06\,pages88. 2006   Zaïane OR, Han J, Zhu H. Mining recurrent items in multimedia with progressive resolution re  nement. In Proceeding of the 2000 international conference on data engineering\(ICDE’00\, pages461–470 2000   Chengcui Zhang, Wei-Bang Chen, Lin Yang, Xin Chen, and John K Johnstone. Automatic in vivo Microscopy Video Mining for Leukocytes. .SIGKDDExplor 9:30–37 2007   Srivastava J, Cooley R, Deshpande M, Tan PN. Web usage mining discovery and applications of usage patterns from web data.SIGKDDExplor 1:12–23 2000   Eirinaki M, Vazirgiannis M. Web mining for web personalization ACM Trans Inter Tech 3:1–27 2003   Kuramochi M, Karypis G. Frequent subgraph discovery. In Proceeding of the 2001 international conference on data mining ICDM’01\,pages313–320. 2001   Mei Q, Xin D, Cheng H, Han J, Zhai C. Generating semantic annotations for frequent patterns with context analysis. In Proceeding of the 2006 ACM SIGKDD international conference on knowledge discovery in databases\(KDD’06\, pages337–346.2006  


composed of condition attribute set C and decision attribute set D. Let B ? C, if the range of B is VB = \(b1, b2, b3, ..., bK the number of range B is | VB | = k, and | U / IND {B}| = k then B can be decomposed into k binary information granules by quotient set|U/IND {B}|The decision attribute set D can be decomposed into | U / IND \(D by quotient set |U/IND {D For example: The values in corresponding to the objects of the item of TID in Table 1 were {chestnut, apple, wild grape watermelon, pears, walnuts}, can be decomposed into six binary information granules as follows:a1={100100 a2={100100}?a3={010010}?a4={011100}?a5={001000 a6={000001 Definition 4 The number of 1 in the binary information granules P is defined as granularity of the binary information granules, which is denoted by | P For example: The granularity of the binary information a1 100100 000001 Definition 5 For a given information system S = \(U, A = C D in Ci, then we have the granularity associated with computing is ci?cj??ck \(where ? on behalf of binary Boolean "and operator For example: Two binary information granules are a1 101000011001 100101001101 operation is as follows: 101000011001 ? 100101001101 100000001001 Based on the above definition, we give the definitions of support and confidence based on binary information granules as follows Definition 6 For any item set X ? I, Y ? I, X ? Y =?, the support of X is sup \(X  of collection of objects; [X] = [x1] ? [x2] ? ... ? [xn], xi is one of the project in X; Y is the same like X The support of the rule X?Y sup\(X ? Y   The confidence of the rule X?Y confidence\(X?Y sup\(X?Y sup\(X X Y X For example ? The support between the two binary information granules a1 and b2 is sup\(a1 565 


b2 confidence\(a1?b2 a1?b2 a1 C. The computing of multiple minimum support based on granular computing When we use multiple minimum support in the multi-level association rules, we can get the rules of high frequency of data entry rules by higher minimum support, as well as low frequency data entry rules by lower minimum support. In this paper, for setting multiple minimum support of each item, we adopt the multiple minimum support proposed by Lin[5] to remove some redundant rules as much as possible, and to reduce the generation search space of frequent itemsets Definition 7 Let I={a1?a2??an},and sup?ai??sup ai+1?,1?i?n-1, Then minimum support can be specified as follows       niasup 1ni1asupasup  a i 1ii i  Definition 8 Let A ? B be a frequent itemset, A ?  B and without loss of generality, let a? A , and a be the one with the smallest minimum support, i.e. MIS\(a ai the minimum support of a is set as MIS \(a a minconf, then the association rule A?B is strong Summarize the above two different definitions of the minimum support, so that the overwhelming majority of rules generated are effective. To prune the spurious frequent itemsets so as to make most of the generated rules become interesting we combine these two specifications as shown below, which we call the definition 9 Definition 9 Let I={a1?a2??an},and sup?ai?? sup ai+1?,1 ? i? n-1, Then minimum support can be specified as follows 


      niasup 1ni1}asupminconf{maxasup  a i 1ii i  For example: we can see from Table 2, the support of [1 is 4/6, and the support of [1 **] is the greatest in the all items, so its MIS value is its own support 2/3. From this calculation, we can draw the support of various items and items among layers in Table 3 IV. APPLICATION EXAMPLE We use the form of a hierarchical structure to encode the user purchase information in table 1, express the value in the corresponding with the various items hierarchical encoded of the TID by information granulation, and calculate the support corresponding to each item encoded by the hierarchical structure, which are shown in table 2. Through the definition of multiple minimum supports based on Granular Computing and the minimum support derived form the various items calculated in Table 2,we can calculate minimum supports corresponding to each item and level-items and show  in Table 3 The entire mining process of multi-level association rules is start at the search of is high-level frequent itemsets. Each item has a minimum support by definition 2. Search the transaction data in table 2 to find the frequent itemsets which meet the minimum support in same layer and multiple layers, while record them in table 4. For example, we calculate the multiple minimum support of [1 **] and [2 **] is1 / 3, sup \([1 **] [2  111100 ? 100101 which meet the minimum support 1 / 3. therefore, [1 **] and [2 are combined as frequent itemset. To extract the frequent itemsets repeatedly, and filter the candidate itemsets which does not meet the minimum support. Finally all the frequent 2 


itemsets are shown in table 4. While no further increase in levels, the entire search process will end Then we find frequent 3 - itemsets from all the frequent 2 itemsets in table 4 which meet the multiple minimum support of definition 2, and filter out the candidate itemsets which does not meet. Repeatly extract the frequent 3  itemsets, until the entire search process is completed, the results is shown in table 5 The generation process of frequent itemsets of multi-level mining is as follows TABLE II.  THE EXPRESSION ANDSUPPORT OF BINARY INFORMATION GRANULE The Name of Information Granule The Expression of Binary Information Granule Support 21*] 000001 1/6 112] 001000 1/6 3**] 010010 2/6 22*] 100100 2/6 111] 100100 2/6 11*] 101100 3/6 12*] 011100 3/6 121] 011100 3/6 2**] 100101 3/6 1**] 111100 4/6 TABLE III.  MULTIPLE MINIMUM SUPPORTS Items MIS Items MIS 1**] 2/3 [21*] 1/12 2**] 1/3 [22*] 1/6 3**] 1/6 [111] 1/6 11*] 1/4 [112] 1/12 12*] 1/4 [121] 1/4 TABLE IV.  TABLE TYPE STYLES The Combination of Items The Expression of Binary Information Granules Support 1**]and[2**] 100100 2/6 1**]and[22*] 100100 2/6 1**]and[3**] 010000 1/6 11*]and[12*] 001100 2/6 


11*]and[22*] 100100 2/6 11*]and[2**] 100100 2/6 11*]and[121] 001100 2/6 12*]and[111] 000100 1/6 12*]and[112] 001000 1/6 12*]and[22*] 000100 1/6 12*]and[3**]  010000 1/6 111]and[2**]  100100 2/6 566 111]and[22*] 100100 2/6 111]and[121] 000100 1/6 112]and[121] 001000 1/6 121]and[22*] 000100 1/6 121]and[3**] 010000 1/6 TABLE V.  TABLE TYPE STYLES The Combination of Items The Expression of Binary Information Granules Support 11*]and[12*]and[22*] 000100 1/6 11*]and[121]and[22*] 000100 1/6 12*]and[111]and[22*]  000100 1/6 12*]and[111]and[2**] 000100 1/6 121]and[111]and[22*] 000100 1/6 121]and[111]and[2**] 000100 1/6 The multiple minimum support of each item is compared with the support assembled by various items shown in the table 4, then we can get the sets of the table 5. From the results, we can get the association rules of the same layer, such as [11 and [12 *] and [22 *], and they all satisfy the minimum support MIS \([11 *] and [12 *] and [ 22  12 22 rules of different layers, such as [12 *] and [111] and [2 they also satisfy the minimum support MIS \([12 *] and [111 and [2  111 2 6, so that we achieve the multi-level association rules Different rules support of different data items produced need to meet different multiple minimum support in order to find these rules, such as the support of [12 *] and [2 **] is sup 12 *] and [2 011100 ? 100101 multiple minimum supports MIS \([12 *] and [2 


MIS \([MIS \([12 2 12 *] and 2 frequency down-generating search space of frequent itemsets V. CONCLUSIONS This paper presents a multi-level association rule mining method based on binary information granules operations and multiple minimum support constraint, with hierarchical encoding and binary granular computing of information granules operation to acquire frequent itemsets at intra level as well as inter level. We give a new definition of support and confidence based on binary information granules. And combined with the definition of multiple minimum supports we effectively restrained the generation search space of frequent itemsets, and found new rules implied in the scarce data items, achieved association rule mining of multi-layer including cross-layer. We got more meaningful rules, and avoided the generated useless rules from the high frequency data items. At last, the method is applied to mining agricultural information association rules, which has been proven to be effective and practical REFERENCES  1] Agrawal R, Imielinski T, Swami A. Mining association rules between sets of items in large databases[C] // Proceedings of the 1993 ACM SIGMOD. Washington:ACM SIGMOD, 1993: 207-216 2] Lin Q.Granular Language and Its Deductive Reasoning.[J Communications of ACM,2002,5\(2 3] Xu Jianfeng, Liu Lan, Qiu Taorong, Hu Ran. On Data Ming Algorithms Based on Binary Numeral Granular Computing. [J].Compter Science 2008,35\(3 4] Liu Qing,Jiang S L.Reasoning about Information Granules Based on Rough Logic.In:RSCTC 2002,L NA I 2475,2002.139?143 5] Ming-Cheng Tseng,Wen-Yang Lin.Efficient mining of generalized association rules with non-uniform minimum support.[J].Data knowledge engineering,62\(2007  567 


of the proposed approach are described. A simulation dataset with 64 items and 10000 transactions were used in the experiments. The dataset followed the exponential distribution. The initial population size P is set at 50, the archive size is set at 30, the crossover rate pc is set at 0.8, and the mutation rate pm is set at 0.001. The parameter d of the crossover operator is set at 0.35 according to Herrera et al.s paper [14] and the set of minimum support values is {3 4%, , 13%}. The experiments were first made for demonstrating the evolution of the Pareto fronts by the proposed approach. The evolution of the Pareto fronts of chromosomes in the archive along with different generations by the proposed approach is shown in Fig. 1 From Fig. 1, we can observe that the solutions were distributed on the Pareto fronts and the final solutions after 500 generations were better than those in different generations. Besides, we can also found that the derived solutions on a Pareto front are trade-offs between the two objectives. It thus depends on the user preference to decide which solutions on a Pareto front are desired. The experiment was then made for comparing the final Pareto front of chromosomes in the archive of the proposed approach with the previous approach [2], and is shown in Fig. 2  250 300 350 400 450 500 550 600 60 70 80 90 100 110 120 130 140 Suitability To tal N um be r o f L 1 


Generation = 0 Generation = 100 Generation = 200 Generation = 300 Generation = 400 Generation = 500  Fig. 1. The Pareto fronts derived by the proposed approach with different generations 450 500 550 600 65 70 75 80 85 90 95 Suitability To tal N um be r o f L 1 The Proposed Approach The Previous Approach  Fig. 2. Comparison results of final Pareto fronts between the proposed approach and the previous approach  From Fig. 2, it is easily to know that the Pareto front derived by using the proposed approach is better than the previous one.  From the experimental results, we thus can conclude that the proposed approach is not only effective in finding an appropriate set of solutions, but also can provide different options to users for further analysis VI. CONCLUSIONS AND FUTURE WORKS The SPEA2 adopted a fine-grained fitness assignment strategy, a density estimation technique, and an enhanced archive truncation method to derive better Pareto solutions 25]. In this paper, we have utilized it to propose a more sophisticated multi-objective approach to find the appropriate sets of membership functions for fuzzy data mining. Two objective functions are used to find the Pareto front. They are minimizing the suitability of membership functions and maximizing the total number of large 1-itemsets respectively Experiments on a simulation dataset were also made to 


show the effectiveness of the proposed approach. The results show that the proposed approach is effective in finding an appropriate set of solutions. Further, the experiments also show that the proposed approach can derive better Pareto front than the previous one [2]. In the future, we will continuously enhance the multi-objective genetic-fuzzy approach for more complex problems REFERENCES 1] C. C. Chan and W. H. Au, Mining fuzzy association rules, The Conference on Information and Knowledge Management, Las Vegas pp. 209-215, 1997 2] C. H. Chen, T. P. Hong, Vincent S. Tseng and L. C. Chen, A multi-objective genetic-fuzzy mining algorithm, The 2008 IEEE International Conference on Granular Computing, 2008 3] C. H. Chen, T. P. Hong, Vincent S. Tseng and C. S. Lee, A genetic-fuzzy mining approach for items with multiple minimum supports, Soft Computing, Vol. 13, No. 5, pp. 521-533, 2009 4] C. H. Chen, Vincent S. Tseng and T. P. Hong, Cluster-based evaluation in fuzzy-genetic data mining, IEEE Transactions on Fuzzy Systems, Vol. 16, No. 1, pp. 249-262, 2008 5] O. Cordn, F. Herrera, and P. Villar, Generating the knowledge base of a fuzzy rule-based system by the genetic learning of the data base IEEE Transactions on Fuzzy Systems, Vol. 9, No. 4, pp. 667674, 2001 6] C. A. Coello, D. A. Van Veldhuizen and G. B. Lamont, Evolutionary Algorithms for Solving Multi-objective Problems, Kluwer Academic Publishers, 2002    7] K. Deb, Multi-objective Optimization Using Evolutionary Algorithms John Wiley & Sons, 2001 8] K. Deb, S. Agrawal, A. Pratab and T. Meyarivan, A fast and elitist multiobjective genetic algorithm: NSGA-II, IEEE Transactions on Evolutionary Computation, Vol. 6, No. 2, pp. 681-695 9] C. M. Fonseca and P. J. Fleming, "Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization," The International Confidence on Genetic Algorithms pp. 416-423, 1993 10] T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, " Genetic-Fuzzy Data Mining with Divide-and-Conquer Strategy", IEEE Transactions on Evolutionary Computation, Vol. 12, No. 2, pp. 252-265, 2008 11] T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, "A GA-based fuzzy 


mining approach to achieve a trade-off between number of rules and suitability of membership functions", Soft Computing, Vol. 10, No. 11 pp. 1091-1101. 2006 12] T. P. Hong, C. S. Kuo and S. C. Chi, "Mining association rules from quantitative data," Intelligent Data Analysis, Vol. 3, No. 5, pp 363-376, 1999 13] T. P. Hong, C. S. Kuo and S. C. Chi, "Trade-off between time complexity and number of rules for fuzzy mining from quantitative data," International Journal of Uncertainty, Fuzziness and Knowledge-based Systems, Vol. 9, No. 5, pp. 587-604, 2001 14] F. Herrera, M. Lozano and J. L. Verdegay, Fuzzy connectives based crossover operators to model genetic algorithms population diversity Fuzzy Sets and Systems, Vol. 92, No. 1, pp. 2130, 1997 15] M. Kaya and R. Alhajj, A clustering algorithm with genetically optimized membership functions for fuzzy association rules mining The IEEE International Conference on Fuzzy Systems, pp. 881-886 2003 16] M. Kaya and R. Alhaji, Utilizing genetic algorithms to optimize membership functions for fuzzy weighted association rules mining Applied Intelligence, Vol. 24 ,  No 1, pp. 7-15, 2006 17] M. Kaya and R. Alhajj, Integrating multi-objective genetic algorithms into clustering for fuzzy association rules mining, The IEEE International Conference on Data Mining, pp. 431-434, 2004 18] M. Kaya, Multi-objective genetic algorithm based approaches for mining optimized fuzzy association rules, Soft computing, Vol. 10 pp. 578-586, 2006 19] C. Kuok, A. Fu and M. Wong, Mining fuzzy association rules in databases, SIGMOD Record, Vol. 27, No. 1, pp. 41-46, 1998 20] Y. C. Lee, T. P. Hong and W. Y. Lin, Mining fuzzy association rules with multiple minimum supports using maximum constraints, Lecture Notes in Computer Science, Vol. 3214, pp. 1283-1290, 2004 21] H. Roubos and M. Setnes, Compact and transparent fuzzy models and classifiers through iterative complexity reduction, IEEE Transactions on Fuzzy Systems, Vol. 9, No. 4, pp. 516-524, 2001 22] J. D. Schaffer, Multiple objective optimization with vector evaluated genetic algorithms, The International Conference on Genetic Algorithms, pp. 93-100, 1985 23] C. H. Wang, T. P. Hong and S. S. Tseng, Integrating membership functions and fuzzy rule sets from multiple knowledge sources, Fuzzy Sets and Systems, Vol. 112, pp. 141-154, 2000 24] S. Yue, E. Tsang, D. Yeung and D. Shi, Mining fuzzy association rules with weighted items, The IEEE International Conference on Systems 


Man and Cybernetics, pp. 1906-1911, 2000 25] E. Zitzler, M. Laumanns and L. Thiele, "SPEA2: Improving the strength Pareto evolutionary algorithm for multiobjective optimization," Proc. Evolutionary Methods for Design, Optimization and Control with App. to Industrial Problems \(Barcelona, Spain, 2001 pp. 95-100 


9] Y. Gong, S. Mabu, C. Chen, Y. Wang, and K. Hirasawa, "Intrusion detection system combining misuse detection and anomaly detection using genetic network programming," in Proc. of the SICE-ICASE International Joint Conference, 2009, pp. 3463-3467 10] "Kddcupl999 data. " [Online]. Available: kdd.ics.uci.eduldatabases kddcup99/kddcup99.htrn1 11] R. P. Lippmann, D. J. Fried, I. Graf, J. Haines, K. P. Kendall, D. Mc Clung, D. Weber, S. Webster, D. Wyschogrod, R. K. Cunningham and M. A. Zissman, "Evaluating intrusion detection systems: The 1998 darpa offline intrusion detection evaluation," in Proc. of DARPA Information Survivability Conference and Exposition 2000, vol. 2 IEEE Computer Society Press, 2000 12] K. Shimada, K. Hirasawa, and J. Hu, "Class association rule mining with chi-squared test using genetic network programming," in Proc. of the IEEE International Conference on Systems, Man and Cybernetics 2006, pp. 5338-5344 


n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 


L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 


Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size 


of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data 


Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L 


Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set 


Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different 


members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube 


Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 


B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 


4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


