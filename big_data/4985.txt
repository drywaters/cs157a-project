A Comparative Study of Enterprise and Open Source Big Data Analytical Tools  1 Udaigiri Chandrasekhar 2 Amareswar Reddy 3 Rohan Rath  S chool of Information Technology and Engineering VIT University Vellore, India 1 u.chandrasekhar@vit.ac.in 2 amareshnlr@gmail.com 3 rohanrath1105@gmail.com   Abstract In this paper we bring forward a comparative study between the revolutionary enterprise big data analytical tools and the open source tools for the same The Transaction Processing Council TPC has established a few benchmarks for measuring the potential of software and its use. We use similar benchmarks to study the tools under discussion. We try to cover as many different platforms for big data analytics and compare them based on computing environment amount of data that can be processed decision making capabilities ease of use energy and time consumed, and the pricing  Keywords Big data enterprise open source analytical tools Hadoop business intelligence metadata MapReduce SQL security, reliability  1  INTRODUCTION We are in a flood of data today. Statistics show that 90% of the world!s data was generated in the last two years itself, and it is growing exponentially To tackle such data and process it we need to leave the traditional batch processing behind and adopt the new big data analytical tools The data generated everyday exceeds 2.5 quintillion bytes which is a mind-boggling figure The growth of data has affected all fields whether it is the business sector or the world of science. To process such huge amounts of data various new tools are being introduced by companies like Oracle and IBM while on the other handle Open Source Developers continue their work in the same field 1.1  What is Big data Big Data is the massive amounts of data that collect with time and are difficult to analyze using the traditional database system tools. Big Data includes business transactions, photos surveillance videos and activity logs Scientific data from sensors can reach massive proportions over time and Big Data also includes unstructured text posted on the Web, such as blogs and social media 1.2  Managing and Analyzing Big Data For the past two decades most business statistics have been created using structured data produced from functional techniques and combined into a information factory or data warehouse. Big data significantly improves both the number of information resources and the variety and number of information that is useful for research. A significant number of this data is often described as multi-structured to differentiate it from the arranged functional data used to fill a data warehouse. In most companies, multi-structured data is growing quicker than structured data Two important information management styles for handling big data are relational DBMS products enhanced for systematic workloads \(often known as analytic RDBMSs, or ADBMSs\ and non-relational techniques \(sometimes known as NoSQL systems\ for handling multi-structured data. A nonrelational system can be used to produce statistics from big data, or to preprocess big information before it is combined into a data warehouse 1.3  Need for Big Data Analysis When a business can make use of all the information available with large data rather than just a part of its details then it has a highly effective benefit over the market opponents Big Data can help to gain ideas and make better choices Big Data provides an opportunity to create unmatched company benefits and better service distribution. It also needs new facilities and a new way of thinking about the way company and IT market works. The idea of Big Data is going to change the way we do things today The International Data Corporation \(IDC\ research forecasts that overall details will develop by 50 times by 2020 motivated mainly by more included systems such as receptors in outfits, medical gadgets and components like structures and connects. The research also identified that unstructured details such as data files, email and video - will account for 90% of Proceedings of 2013 IEEE Conference on Information and Communication Technologies \(ICT 2013 978-1-4673-5758-6 13/$31.00 \251 2013 IEEE 372 


all details designed over the next several years But the number of IT experts available to handle all that details will only develop by 1.5 times the present levels The electronic galaxy is 1.8 billion gigabytes in dimension and saved in 500 quadrillion data files. And its dimension gets more than dual in every two years period of time If we evaluate the electronic universe with our actual universe then it's nearly as many pieces of details in the electronic universe as stars in our actual universe 1.4  Characteristics of Big Data A Big Data platform should give a solution which is designed specifically with the needs of the enterprise in the mind The following are the basic features of a Big Data offering  Comprehensive  It should offer a wide foundation and address all three size of the Big Data task Volume Variety and Velocity  Enterprise-ready  It should include the performance security, usability and reliability features  Incorporated  It should easily simplify and speeds up the release of Big Technological innovation to business Open Source based - It should be start resource technology with the enterprise-class overall performance and incorporation Low latency flows and updates Solid and fault-tolerant Scalability Extensible Allows adhoc queries Little maintenance 2  ENTERPRISE BIG DATA ANALYTICAL TOOLS VERSUS OPEN SOURCE BIG DATA ANALYTICAL TOOLS Ever since the research on big data started companies like IBM Google and Oracle have been leading the race of enterprise analytical tools and have been occupying the major section of the market Little is known about the new age analytical tools that have been developed recently and are spreading faster than expected. In this paper, we bring to light the working characteristics of many such tools for big data Keeping aside all the expensive and closed source applications we also strive to explain the working and advantages of open source analytical tools, which are as good as their counterparts in the enterprise world We explain where these can be used and what are the issues involved in the same 2.1  Enterprise Analytical Tool 2.1.1  Pentaho Pentaho is a very useful tool to visualize high volumes of data and analyze it to draw conclusions It supports all the right set of tools for the entire processing lifecycle of big data It provides exploration and visualization features for the business sectors; also performs predictive analysis It provides a 15 times boost to scripting and coding It is very useful for interactive reports time series forecasting statistical learning evaluation and visualization of predictive models. It supports Predictive Modeling Markup Language It provides visual tools that define instant access to data. It is built on a modern high performing, lightweight platform. This platform fully works on a modern 64 bit multi-core processor and harnesses the power of new-age hardware Pentaho is unique in leveraging external data grid technologies such as Infinispan and Memcached to load vast amounts of data into memory The Instaview feature on Pentaho enables us to instantly view the reports generated by careful analysis of the data in a multi-dimensional and interactive format   Fig1. Instaview feature on Pentaho  The problem is with the pricing During the survey it was found that a pricing request needs to be sent to Pentaho Big data Analytics in order to view the quotation But reviews show that it is priced at a much lesser price as compared to other commercial BI tools 2.1.2  TerraEchos TerraEchos a world leader in innovative security alternatives utilizing Big Data in Movement statistics was known as the champion of a 2012 as it received the IBM Beacon Award for Outstanding Information Management Innovation TerraEchos a next-generation big-data statistics company has implemented the first foundation to blend and narrow large volumes of live complicated structured and unstructured data on the fly # and at the same time to draw out, evaluate and act upon the data, in the moment Unlike database-centric techniques to high-speed highvolume data research, the TerraEchosKairos foundation is not restricted by the amount type or rate of the data It consistently examines data while it is still moving without requiring storing it Based on a streaming operating system and with statistics and creation segments updated to specific Proceedings of 2013 IEEE Conference on Information and Communication Technologies \(ICT 2013 978-1-4673-5758-6/13/$31.00 \251 2013 IEEE 373 


p rograms the TerraEchosKairos foundat i protection intellect facilities and growin g other marketplaces where understanding a n now is crucial  Fig2. Annual Lobbying by Terra E 2.1.3  Cognos Cognos is known for business inte l p erformance and strategy management as w to work for analytics applications. IBM's C known to provide an organization with wh a become top-performing and analytics dri v the individual workgroup division mid s large business Cognos application has b ensure that companies make better choices development This tool is for those who w company's intellect or performance Cognos solutions are designed to help decisions and manage performance for o results Combine your financial and oper a single seamless source of information in t your choice for transparent and timely rep o action IBM offers a full range of customer su p certification and services for Cognos a n Analytics customers  Fig3. Query Analyser for Co g  An investment of over a lakh in Indian cu r for the effective use of Cognos. It seems le g to invest in it, but for personal usage it may the general public  i on is perfect for g  professional and n d performing right  E chosInc  l ligence financial w ell. It also extends C ognos Software is a tever they need to v en.With items for s ize company and b een developed to for their future and w ish to grow their you make smarter o ptimized business a tional data into a t he environment of o rting, analysis and p port training and n d other Business  g nos r rency is needed g it for a company not be possible for 2.1.4  Attivio The AIE or Active Intelligen c huge impact on the busines s customers information assets the situation and help them in w helps to fulfil a strategic goal w h AIE has the capabilities to p latforms and brings together b o data content for an efficient a n p ower to incorporate and rel a content silos  without any requirement It has a very a search capability for BI whic h analytical tools It offers both intuitive searc h p owers, making structured and in ways never been thought of of big data and can be useful f varied technical skills and prior i 2.1.5  Google BigQuery Google BigQuery allows y against very large datasets wi t This can be your own data o shared for you BigQuery wor k of very large datasets, typicall y large, append-only tables It!s a high speed tool which c seconds It can handle trilli o terabytes of data It!s simplici t close reference to SQL It ha groups and user based Google secure SSL access method It through BigQuery browser th e Script It has a very distinguished pr i storage, interactive queries and per GB/month 0.035 per G processed\ respectively They all have default limi t example is a fine example of a and optimized for processing analytics  Fig4. Workplace fo r c e Engine of Attivio has had a s  through the availability of It helps in a quick analysis of w hatever ways they need it. It h ich has been established analyse anything on many o th structured and unstructured n d agile BI. It is equipped with a te all the available data and advance modelling of data a dvance intuitive Google like h  incorporates all the needed h  capabilities along with SQL unstructured data more useful It has great insight in the field f or all kinds of users and their i ties  y ou to run SQL-like queries t h potentially billions of rows o r data that someone else has k s best for interactive analysis y using a small number of very c an analyse billions of rows in o ns of records pertaining to t y stands out as it works with s a powerful sharing through Accounts It works on a very has multiple access methods e  REST API or Google Apps i cing r ange which varies as per batch queries which are $0.12 G B processed 0.02 per GB t s assigned too.This Netezza system that has been designed a specific workload business  r Google BigQuery Proceedings of 2013 IEEE Conference on Information and Communication Technologies \(ICT 2013 978-1-4673-5758-6/13/$31.00 \251 2013 IEEE 374 


2.1.6  Netezza A revolutionary data analytical tool was e and was named Netezza. It is an advance d warehouse engine that incorporates a da t storage components into a single environm e run predictive analysis business intell i applications needed in every field It is based on IBM blade architectur e p rocessors an Asymmetric Massively P AMPP approach to process workloads a field programmable gate arrays # special i a means to filter data before it is proces s used to speed the process of queries and p which preprocesses In this computat i p rocessed by FPGAs as opposed to makin g the work The Netezza design has led to an expon e field of big data analytics and is very u commodity or specialty environment It is not only ideal for processing com p queries such as those found when performi n It preprocesses data and then feeds the C fashion Further it is not burdened b y structures and online transaction proce s resulting in a simple code path for faster pe r 2.2  Open Source Analytical Tools  2.2.1  Apache Hadoop Licensed under Apache v2 Apache H a source system framework that not only r p latform using data intensive processing. F it supports running on large clusters. Supp o and provides security and reliability for dat a its own Hadoop computation paradigm c where in the work is divided into vario processed on a clustered system or a grid It has a capacity to handle petabytes of d files within seconds and provides a very h computational processing Cloudera is the leader in Apache Had o and services and offers a powerful new enables enterprises and organizations to lo structured as well as unstructured Hadoop is written in the Java programmi n a top-level Apache project being built an d community of contributors Hadoop stablished by IBM d high performance t abase server and e nt. This is used to i gence and many e  which uses x86 P arallel Processing a nd it uses FPGAs i zed processors\ as s ed This FPGA is p erform a function i onal kernels are g the CPU do all of e ntial growth in the u seful as a hybrid p lex scanning type n g deep analytics C PU in a balanced y  legacy database s sing features  r formance a doop is an open r uns on distributed or commodity use o rting data mobility a processing. It has c alled MapReduce us units and then d ata in millions of h igh bandwidth of o op-based software data platform that ok at all their data n g language and is d  used by a global Fig5. Work Flow o 2.2.2  Zettaset One of the most flexible o p works on any Apache Hadoop d availability and security of the d system and is very cost effe c second name Zettaset has created Orchest r an enterprise management to o issues of Hadoop deployment w are sophisticated tools It has the capacity to autom a installation of Hadoop on a clu s is ready for enterprise usage Orchestrator is that it is not ba s is a very secure open system 2.2.3  HPCC Systems Abbreviated as HPCC H i Cluster, as the name suggests i s It was developed by LexisNexi s versions to this tool paid as w structured and unstructured c p erforming scalability from p rocessing makes it even stron g It is commercially available a tool so easily available to the m selecting this tool is a platfor m includes a highly integrate d capabilities from raw data pr queries and data analysis using Working as an optimized cl u high performing system resulti n of Ownership along with sec u with a very good processing s p centric programming language p rogrammer productivity for d this platform It has a good tolerance for p rocessing in case of system f a warehouses and high volume o security analysis of massive a m  o f Apache Hadoop p en source analytical tools it d istribution. Its features include d ata. It is easy to deploy on any c tive Simplicity is Zettaset!s r ator software solution that is o l that addresses the common w ith easy-to-use interfaces that a te simplify and accelerate the s ter management system which  The outstanding feature of s ed on Hadoop distribution but i gh Performance Computing s a clustered computing system s Risk Solutions. There are two w ell as free and both w ork on c ontent data It has a high 1-1000s of nodes Parallel g er a tool nd offers a lot of features for a m asses A major advantage of m  for data-intensive computing d  system environment with ocessing to high-performance a common language u ster it is a very low costing n g a very low TCO \(Total Cost u rity scalability and reliability p eed. It has an innovative dataincorporated which increases evelopment of applications on faults and capabilities for rea ilures. It can also manage data o nline applications to network m ounts of log information  Proceedings of 2013 IEEE Conference on Information and Communication Technologies \(ICT 2013 978-1-4673-5758-6/13/$31.00 \251 2013 IEEE 375 


2.2.4  Dremel An interactive ad-hoc query system it was developed by Google to offer analysis of nested readable data Scalable to extremes up to 1000s of computer systems and petabytes of data; it has the power to process trillions of rows together in just a matter of seconds by multi-level execution of trees and columns. It is not meant for replacing the old system, MR and is often used for analysis of crawled web documents, tracking install data for applications on Android Market and also for crash reporting for Google products Other important uses are spam analysis debugging of map tiles on Google Maps, tablet migrations in managed Bigtable instances  2.2.5  Greenplum HD Greenplum HD allows customers to start with big data statistics without the need to develop an entire new venture. It is provided as application or can be used in a pre-configured Data Handling Equipment Component Greenplum HD is a 100 percent open-source qualified and reinforced edition of the Apache Hadoop collection that contains HDFS MapReduce, Hive, Pig, Hbase and Zookeeper. IT prevails of a finish information research foundation and it brings together Hadoop and Greenplum data resource into only one Data Handling Equipment  Available as application or in a preconfigured Data Handling Equipment Component, Greenplum HD provides a finish foundation such as set up training international support and value-add beyond simple appearance of the Apache Hadoop submission Greenplum HD makes Hadoop quicker, more reliable, and easier to use Greenplum HD facilitates Isilon!sOneFS Scale-Out NAS Storage space for Hadoop EMC Isilon scale-out NAS is the first and only Business NAS remedy that can natively include with the Hadoop Allocated Data file System \(HDFS\ part. By dealing with HDFS as an over the cable method you can quickly set up a extensive big data statistics remedy that brings together Greenplum HD with Isilon scale-out NAS storage systems to provide a very effective extremely effective and versatile information storage and statistics environment The Greenplum HD DCA Component easily combines the Greenplum HD application into a product offering an enhanced setting designed for performance and stability. The Greenplum Data Handling Equipment marries the unstructured batch-processing power of Hadoop with the Greenplum Database and the cutting-edge Extremely Similar Handling \(MPP\ structure. This allows businesses to draw out value from both arranged and unstructured data under only one, smooth foundation 2.2.6  HortonWorks Hortonworks is an authentic and free Hadoop Distribution system. It is developed on top of Hadoop and it allows clients to capture process and perform together at any broad variety and in any framework in a simple and cost-effective way Apache Hadoop is a key of the Hortonworks framework.  It is ideal for organizations that want to combine the power and cost-effectiveness of Apache Hadoop with the amazing alternatives and balance required for organization deployments Hortonworks is the latest organization of Hadoop system and expert support but it's an old element when it comes to working with the platform. The organization is a 2011 spinoff of Search engines, which remains one of the greatest clients of Hadoop Actually Hadoop was usually developed at Search engines and Hortonworks managed an extensive broad variety of nearly 50 of its very first and most well-known associates to Hadoop There!s differentiator between Hortonworks and the other suppliers. Hortonworks products are 100% Begin Source and are free contrary to some of and Cloudera!s Company amazing and/or value-adding Hadoop products, which are not 2.2.7  ParAccel The open source tool, ParAccel data analytics platform has been used by organizations for its interactive capabilities to analyze big data in an enhanced fashion It offers a high storage along with compression of adaptive capabilities Inmemory processing and compilation on the fly is also important, making it easy to work with and adapt to   Fig6. ParAccel Query Analyzer 2.2.8  GridGrain An enterprise open source system GridGain as the name suggests is for grid computing This was specially made for Java and is compatible with Hadoop DFS and offers an alternative for Hadoop's MapReduce It offers a distributed in-memory and scalable data grid which is the link between data sources and different applications An open source version is available on Github or a commercial version can be downloaded from their homepage Proceedings of 2013 IEEE Conference on Information and Communication Technologies \(ICT 2013 978-1-4673-5758-6/13/$31.00 \251 2013 IEEE 376 


3  COMPARISON OF SECURITY BETWEEN OPEN SOURCE AND ENTERPRISE TOOLS Although you can take an open source project compare it against a closed source project and say that one is more secure than the other based on some number of observations or measurements this determination will probably be based on factors other than the nature of the project's open or closed source code Secure design source code auditing quality developers, design process, and other factors, all play into the security of a project, and none of these are directly related to a project being open or closed source It is shocking to see the vulnerabilities in some closed source systems.  And although it certainly wouldn't mean open source software is quantitatively &more secure& than closed source software, it just means that there is a doubt in the source code auditing principles and otherwise the general security practices of certain closed source operating system vendors. However, the issue here isn't specifically related to the operating system being open or closed source, but to the processes with which the vendor approaches security Although this might seem to imply that open source projects are going to have less vulnerabilities than closed source projects that's not really the case either the number of vulnerabilities present in a given system can!t be simply associated with the openness of its source code Ultimately it's about the way the project and its developers handle and integrate security 4  S  ELECTING THE RIGHT TOOLS FOR DATA ANALYTICS  T he factors discussed in the paper have a significant impact on technology selection.Organizations are not ready to make risky investment strategies in expensive alternatives just in case there is something more to be discovered. This is where multiple alternatives come into play. Existing exclusive * and generally expensive  storage space and data resource alternatives are being formulated by some of the more costeffective growing technology generally from the Apache Hadoop atmosphere Initial discovery and research of large information amounts where the nuggets are well invisible can be performed in a Hadoop atmosphere Once the nuggets have been discovered and produced a decreased and more organized information set can then be fed into a current information factory or statistics system From that viewpoint it makes overall sense for providers of current storage space data resource and information warehousing and statistics software to provide connections and APIs to Hadoop alternatives And also put together incorporated promotions that work with both the exclusive and free components While some of them hurry to accept Hadoop, there is no evidence that it is a sensible and suitable move As already described many of the new big data technology are not ready for popular business utilization, and organizations without the IT abilities of the trailblazers or common early adopters will welcome the support from recognized providers 5  CONCLUSION To conclude after the analysis of both closed and open source Big Data Tools, it is pretty evident that it's all about the usage and needs of an individual or the company It is impossible to afford a few tools at a personal level because of the prices and complications, while using open source systems might pose an outdating and modifications problem. There is also the security issues involved in choosing the tool Open source promotes development and innovation and supports developers Big data is on every CIO!s mind and for good reasons companies have spent more than 4 billion on big data technologies in the year 2012 These investments will in turn trigger a domino effect of upgrades and new initiatives that are valued for $34 billion for 2013  References  1  Colin White, BI Research, January 2012:MapReduce and the Data Scientist 2  Anthony M. Middleton, Ph.D. LexisNexis Risk Solutions Date: May 24, 2011HPCC Systems:Introduction to HPCC 3  US Department of Energy Labs, and TerraEchos, Inc www.terraechos.com 4  hhtp://workloadoptimization.com 5  IBM Systems and Technology Group 6  Solution Brief; TerraEchosKairos on IBM PowerLinux servers 7  http://www.paraccel.com/technology/paraccel-analyticplatform.php 8  http://www.greenplum.com 9  www.ibm.com/software/analytics/cognos 10  http://www.pentaho.com 11  http://community.pentaho.com 12  http://www.cloudera.com 13  Google, Inc.: Dremel Interactive Analysis of WebScale Datasets Sergey Melnik, AndreyGubarev, Jing Jing Long, Geoffrey Romer,Shiva Shivakumar, Matt Tolton, Theo Vassilakis 14  http://forum.gridgain.com/index.html 15  http://hadoop.apache.org 16  http://incubator.apache.org/drill 17  http://www.zettaset.com 18  http://hortonworks.com Proceedings of 2013 IEEE Conference on Information and Communication Technologies \(ICT 2013 978-1-4673-5758-6 13/$31.00 \251 2013 IEEE 377 


  7  Lorenz, R  D., Experi m e n t s i n Timel a pse C a m e r a  Observations of Dust Devil Activity at El Dorado Playa Nevada, Abstract #1573, 42nd Lunar and Planetary Science Conference, Lunar and Planetary Institute Houston, TX, 2011  Lorenz R  D., B  Jackson and J. B a rnes, Inexpensi v e Timelapse Digital Cameras for Studying Transient Meteorological Phenomena : Dust Devils and Playa Flooding, Journal of At mospheric and Oceanic Technology, 27, 246-256, 2010  C a st ano, A., A. F ukanag a J. B i esadecki, L. Neakrase, P Whelley, R. Greeley, M. Lemmon, R. Castano, S. Chien Automatic detection of dust devils and clouds on Mars Machine Vision and Applications, 19, 467-482, 2008  Lorenz R  D. and A Val d ez, Var i abl e W i nd R i p p l e  Migration at Great Sand Dunes National Park, Observed by Timelapse Imagery, Geomorphology, 133, 1-10, 2011 19 B a l m e  M. R A  Pa th a r e, S.M Me tzg e r  M C. To w n er  S.R. Lewis, A. Spiga, L.K. Fenton, N.O. Renno, H.M Elliott, F.A. Saca, T.I. Michae ls, P. Russell, J. Verdasca Field measurements of horizontal forward motion velocities of terrestrial dust devils: Towards a proxy for ambient winds on Mars and Earth, Icarus, 221, 632–645 2012  Koch, W  On B a y e si an Tracki ng and Dat a Fusi on  A Tutorial Introduction with Examples, IEEE Aerospace and Electronics Systems, 25, 29-51, 2010  Biographies Ralph Lorenz is a planetary scientist at the Johns Hopkins University Applied Physics Laboratory, with interests in atmospheres surfaces and their interactions, especially on Titan and Mars.  He worked for the European Space Agency on Phase B of the development of the Huygens probe to Titan, and subsequently built part of the instrumentation of the probe’s Surface Science Package SSP\. Prior to joining APL in 2006, he spent 12 years in various positions at the Lunar and Planetary Laboratory at the University of Arizona, where he led observation planning for the Cassini RADAR investigation, and served on the science team of the New Millennium DS-2 mission to Mars. He is th e author of several books including ‘Spinning Flight’, ‘Titan Unveiled’, and ‘Space Systems Failures. He has a B.Eng in Aerospace Systems Engineering from the University of Southampton \(UK and a Ph.D. in Physics from the University of Kent at Canterbury \(UK\. He is the recipient of 5 NASA Group Achievement Awards   


  8  


Virtual Social Networks Analysis in Computational Social Network Analysis  ser Computer Communications and Networks A Abraham A.-E Hassanien and V Sn  ael Eds London Springer London 2010 ch 1 pp 3…25  J K orner  Fredman-k olmo s bounds and information theory   SIAM J Algebraic Discrete Methods  vol 7 no 4 pp 560…570 Oct 1986  T  Leighton and S Rao Multicommodity max-”o w min-cut theorems and their use in designing approximation algorithms J ACM  vol 46 no 6 pp 787…832 Nov 1999  M Bastian S He ymann and M Jacomy  Gephi An open source software for exploring and manipulating networks 2009  A.-L Barabasi and R Albert Emer gence of scaling in random networks Science  vol 286 no 5439 pp 509…512 1999 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Of“ce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily re”ect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared in”uence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Pro“ling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of “Daily” Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data – Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average “daily“ operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rolling… I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators – Data Element Methods – Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Today’s cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlight’s data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlight’s hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlight’s method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





