FPGA-Based HPC Application Design for Non-Experts David Uliana Krzysztof Kepa and Peter Athanas Virginia Polytechnic Institute and State University Blacksburg VA 24061 USA f duliana,kepa,athanas g vt.edu Abstract 227In the current era of big-data computing most non-engineer domain experts lack the skills needed to design FPGA-based hardware accelerators to address big-data problems This work presents bFlow a development environment that facilitates the assembly of such accelerators speci\002cally those targeting FPGA-based hybrid computing platforms such as the Convey HC series This framework attempts to address the above problem by making use of an abstracted graphical 
front-end more friendly to users without computer engineering backgrounds than traditional tools as well as by accelerating bitstream compilation by means of incremental implementation techniques bFlow's performance usability and application to big-data life-science problems was tested by participants of an NSF-funded Summer Institute organized by the Virginia Bioinformatics Institute VBI In about one week a group of four non-engineering participants made signi\002cant modi\002cations to a reference Smith-Waterman implementation adding functionality and scaling theoretical throughput by a factor of 32 I I NTRODUCTION Today's world is one of 223information everywhere,\224 in which the size and availability of valuable data sets is rising 
rapidly especially in the life sciences Consider the genome of a single human individual\226processing this data set involves the analysis of three billion DNA base pairs This fact is extremely relevant given the expectation that in the near future nextgeneration sequencing machines will produce whole human genomes in a matter of hours This 003ood of information creates the need for exceptional computing performance in order to process such data sets within tolerable times At the forefront of big-data analysis efforts are domain experts e.g bioinformaticians who explore genome hypotheses by analyzing large quantities of genome data Such domain experts may be skilled programmers however their productivity\226hypothesis discovery and veri\002cation rate\226is lim 
ited by the performance capacity of available computing resources For example the architecture of data-center class computing platforms is designed for speed over a general mix of problems and ef\002ciently applying such platforms to domainspeci\002c data structures and algorithms such as DNA/protein sequence alignment is often not straightforward Heterogeneous computing machines like the Convey Hybrid-Core HC servers have potential to address this gap since they enable the creation of application-speci\002c FPGAbased accelerators tightly coupled with general-purpose processing resources i.e CPUs This tight coupling is achieved through the use of custom instructions and cache-coherent shared memory space  Unfortunately  de v elopment 003o ws for such accelerators require extensive computer engineering and digital design 
expertise rendering the use of these platforms impractical for most experts in the life sciences community In this work a development 003ow targeting hybrid-core systems such as the Convey HC servers is proposed with the purpose of aiding non-engineering domain experts in the assembly of big-data hardware accelerators Such a 003ow should help such experts address omics problems on an unprecedented computational scale  Since graphical design environments often provide a simpler more intuitive mapping to parallel hardware than text-based languages a graphical design entry tool namely DataIO's Azido was selected as the 003ow's front-end Accelerators are described in Azido using a graphical algorithmic language built on simple canonincal primitives and the tool facilitates development for heteroge 
neous platforms within a single environment In this w ork an Azido System Description SD was prepared to target the Convey HC architecture This description acts as a plugin to Azido encapsulating speci\002cations of the HC platform organization and interface details Most of these interfaces are abstracted to simpler higher level structures within Azido to ease development All abstractions are complemented with software helper routines C/C that abstract the co-processor API into a framework more intuitive to the big-data user II B ACKGROUND Design productivity for FPGA-based computing has suffered from the contemporary ASIC 223design productivity gap\224 and has unique needs and opportunities not adequately ad 
dressed by existing FPGA design tools In Nelson et al proposed a productivity model that exposes three key contributors to high design productivity multi-level design reuse high-level design abstractions and a more interactive veri\002cation environment that increases the number of development turns per day All of these are necessary to improving design productivity e.g the use of high level above HDL languages would signi\002cantly reduce functional simulation time thereby increasing turns per day Also describing designs in a hierarchical high level language can promote reuse by making designs more portable High-level synthesis is a very active area of research and many high level design tools and approaches exist e.g 
002 
2013 IEEE 
ASAP 2013 
ImpulseC AutoESL SysGen catapultC The primary focus of these tools is to reduce time to solution for designers who have considerable computer engineering expertise While working from a high level of abstraction and guaranteeing functionally correct output they require design input e.g pragma statements concerning low-level hardware detail in order to produce ef\002cient RTL output Hence such languages are generally inappropriate for use by those in the lifesciences community which is comprised primarily of software programmers with no hardware design experience 261 
978-1-4799-0493-8/13/$31.00 


A Convey hybrid-core architecture and design 003ow The use of heterogeneous computing architectures like the Convey HC platform CPU  FPGA for 002xedpoint and Nebula CPU  GPU for 003oating point algorithms constitutes one approach to meeting big-data processing demands The Convey HC-1 system which is considered in this work consists of a commodity Intel Xeon host server extended with a custom coprocessor board This board contains four large FPGAs Xilinx part XC5VLX330 called Application Engines AE each augmented with cache-coherent high bandwidth memory access 8 memory controllers per AE Each con\002guration of the Convey co-processor is called a 223personality,\224 and makes application-speci\002c functions available to the host server processor as custom instructions Convey provides a Personality Development Kit PDK to enable the creation of custom personalities The PDK includes a comprehensive simulation environment with busfunctional models of all interfaces enabling veri\002cation of the complete system\226host CPU software with the four coprocessor FPGAs\226prior to bitstream compilation The PDK supports HDL and synthesized netlists e.g EDIF as design entry formats Final personality compilation is performed by the Xilinx FPGA design 003ow while software routines for the host CPU are compiled using Convey's cnyCC compiler The personality can execute on up to all four FPGAs providing signi\002cant computational capability however the PDK does not address the needs and skills of big-data users most of whom have little FPGA development experience Fig 1 Movement of data within bFlow B Azido hardware design environment Azido is a graphical object-oriented design environment based on the Implementation Independent Algorithm Description Language I2ADL The tool abstracts low-level complexities of digital hardware design to intuitive algorithmic objects built on its implementation-independent primitive library the CoreLib  Bitstream implementation is accomplished with Azido System Descriptions SDs which are script-based 223plugins\224 to Azido Among several existing graphical design environments Azido was selected for three reasons 1 it provides a 003exible design environment and core library capable of servicing many application domains at multiple levels of abstraction 2 the System Description-based implementation framework facilitates extention of the tool to many target platforms and 3 beyond standard schematic-capture abilities Azido provides some dynamic features such as automatic data typing and graphical polymorphism These characteristics distinguish Azido from tools such as LabVIEW and Simulink The form er w as not considered because of its constraint to speci\002c target platforms The latter though capable of producing platform independent HDL lacks most of the dynamic features mentioned above C bFlow contributions This work presents bFlow an approach that provides a simpli\002ed and portable accelerator development 003ow that supports rapid prototyping of big-data algorithms in hardware by nonengineers Design productivity is improved with a high-level graphical front-end i.e Azido and an accelerated compilation process employing incremental implementation strategies facilitated by qFlow and Xilinx Hierarchical Design Flow Futhermore this frame w ork supports the growth of the third-party 223personalities ecosystem\224 through the distribution of opensource accelerator implementations Thus no user is limited to closed-source personalities provided by the system vendor III R APID B IO ACCELERATOR DEVELOPMENT FLOW This approach is divided into three efforts 1 con\002guring Azido to target the Convey HC-1 platform 2 the acceleration of the back-end compilation process and 3 a hands-on test of the complete 003ow The full design framework is shown in Figure 1 The accelerator is designed and synthesized within Azido on the user's local machine compiled and packaged as a HC-1 personality on VBI's Shadowfax cluster and simulated and ultimately executed on the HC server Local simulation of individual logical blocks may be performed using Azido's built-in x86 system description and very rough system simulation can also be done locally by instantiating custom Component Object Model COM objects in place of HC memory abstractions e.g stream characters from a 002le on the desktop instead of co-processor memory Of course cycleaccurate system simulation is possible on the HC server A HC-1 system description and software helper routines The Azido System Description SD for the HC platforms can be divided into three components 1 a communication implementer for the transfer of probe results and stimuli between Azido and the personality at run-time 2 an abstracted 262 


view of the co-processor dispatch interface and 3 streaming abstractions for the memory interface 1 Azido communication implementer The 002rst component is invisible to the designer and handles communication between the Azido environment and a running personality in order to enable diagnostic probing and stimuli This is achieved using a COM object within the Azido HC-1 SD an SSH tunnel and several utilities on the HC platform itself The Convey HC management ring interface is used to probe and stimulate the design in a non-obtrusive manner Fig 2 Collection of Convey dispatch interface objects in Azido left and example usage stream inverter of the streaming memory abstractions right 2 Convey dispatch interface abstraction The second consists of the Azido abstraction of the AE dispatch interface Figure 2 Logic external to the Azido-generated netlist decodes incoming dispatch instructions and presents a simpli\002ed interface to the Azido designer The AE general registers are exposed as simple read/write blocks in Azido and the coprocessor custom instructions are exposed as blocks with a 223Start\224 output 3 Memory streaming abstraction Lastly to conceal the complexities of random memory access and address arithmetic bFlow contains a simpli\002ed streaming abstraction of the memory controllers available to each AE These 223streamers\224 are comprised of 223source\224 and 223sink\224 modules to be used to stream memory blocks in and out of memory see Figure 2 In addition to the SD objects available to the designer within Azido several software routines were developed to simplify the con\002guration and execution of a custom personality This is done by wrapping the Convey-provided low-level assembly routines in higher-level C/C routines contained in a C class B Extension of Smith-Waterman reference implementation To verify the usability and productivity that bFlow attempts to realize it was placed in the hands of four non-engineering participants of the NSF-funded Summer Institute organized by VBI The students were asked to review and g ain a basic understanding of the systolic array approach to implementing the Smith-Waterman SW local sequence alignment algorithm  The students were then gi v en a reference SW accelerator in Azido and tasked with accelerating the design and extending its functionality As in this accelerator consisted of a lar ge systolic array to store the short 50-200 bases query sequence and compute the scoring matrix values as the reference millions of bases is streamed through The output is the value and location of the highest-scoring cell C Partial implementation 003ows with qFlow and Partitions Acceleration of the bitstream compilation process is achieved by two methods Xilinx Hierarchical Design Partitions 003ow and qFlo w 13 both incremental partial implementation frameworks that reduce build times through high-level management of the Xilinx ISE implementation process Both methods exploit the Convey AE architecture which contains static unchanging interface logic that consumes roughly 25 of each AE and is reimplemented on every run of Convey's standard compilation 003ow 1 Partitions 003ow The 002rst approach taken makes use of the Xilinx Hierarchical Design Partitions 003ow T w o par titions were selected for this 003ow 1 top partition containing the entire FPGA design and 2 the Azido-generated logic The 002rst is preserved while the second is constantly updated by the Azido designer The implementation of this 003ow is very simple consisting of some Make\002le extensions and a couple constraint 002les ucf 2 qFlow This second utilizes a subset of the qFlow framework a tool for acceleration back-end compilation The tool assumes a hierarchical design methodology similar to that which the Convey PDK encourages That is the use of interface logic that is designed and con\002gured once experiencing few evolutions throughout the entire design process during which the core computational logic that is the focus of the design undergoes many evolutions Compared to the partitionsbased approach discussed above qFlow provided generally faster compilations but was unable to 002t some of the larger designs that the partitions framework was able to IV R ESULTS The participants were successful in their attempt to improve the performance and functionality of the reference SW implementation Modi\002cations included logic to maintain the index of the highest scoring alignment and a transition from a single cell array to multiple arrays in order to search multiple partitions of the reference sequence in parallel The 002rst modi\002cation was simple and included the use of Azido's counter maximum multiplexer and register objects however the second involved signi\002cant change to the high-level structure of the implementation This resulted in a realized 4x bandwidth increase from 150 million to 600 million bases per second bps with a feasible speedup of 32x to 4.8 billion bps given enough parallel cell arrays These reported throughput rates are for co-processor performance only and assume that the reference sequence\(s are available in co-processor memory One dif\002culty experienced by the students during their use of the 003ow was synchronous design\226 especially synchronizing multiple 003ows of data Azido's CoreLib contains objects to address these dif\002culties however they lack elegance consume excessive FPGA resources and are visually messy Another complication of the design process was timing closure Under the standard parameterization the Convey PDK enforces a clock rate of 150 MHz Such a tight constraint is easily and often broken by long chains of asynchronous operations since Azido neither analyses the design nor enforces any timing restrictions at compile time The performance of the alternative build 003ows is given in Table IV and visualized in Figure 3 Each Smith-Waterman 263 


TABLE I B UILD TIMES  MEAN OF THREE RUNS  FOR C ONVEY  S STANDARD FLOW  THE P ARTITIONS FLOW  AND Q F LOW  T HE SPEEDUP OVER STANDARD FLOW IS GIVEN IN PARENTHESES  N OTE  D EVICE UTILIZATION LISTED IS THE UTILIZATION DUE ONLY TO USER LOGIC   Design Cells Device Util  Mean Build Time min   LUTs FFs Standard Partitions qFlow  sw  1x8 8 1.83 2.16 89.10 65.60 1.36 26.58 3.35 sw  1x12 12 2.43 2.44 89.90 54.16 1.66 26.12 3.44 sw  1x16 16 3.02 2.73 104.09 57.92 1.80 32.94 3.16 sw  1x24 24 4.22 3.30 98.80 76.85 1.29 34.75 2.84 sw  1x32 32 5.41 3.87 102.20 72.89 1.40 38.27 2.67 sw  4x8 32 5.62 4.10 96.90 61.58 1.57 39.20 2.47 sw  1x48 48 7.79 5.01 128.50 82.95 1.55 43.02 2.99 sw  1x64 64 10.18 6.15 129.73 87.92 1.48 53.75 2.41 sw  4x16 64 10.36 6.34 130.08 84.74 1.54 53.94 2.41 sw  4x32 128 19.85 10.81 165.99 173.62 0.96 97.33 1.71 sw  4x48 192 29.34 15.29 173.22 sw  4x64 256 38.83 19.76 208.89  Fig 3 Build times for Convey's standard 003ow the Partitions-based 003ow and qFlow for twelve different variations of the Smith-Waterman Azido implementation con\002guration is named with convention sw  MxN where M is the number of parallel systolic arrays and N is the length of each array The median speedup over the standard Conveyprovided 003ow for the partitions-based approach 1.51 while that of qFlow was 2.76 The last two con\002gurations tested 4 002 48 and 4 002 64  could not be placed into the dynamic region\226 the same dynamic region constraints were used for both 003ows Note the jump in build time from con\002guration 4 002 16 to 4 002 32 due to the increased utilization of the dynamic region V C ONCLUSIONS AND FUTURE WORK This work presents bFlow an FPGA-based big-data accelerator development environment signi\002cantly more usable by non-engineers than traditional design tools Design productivity is increased by promoting object reuse providing an high-level abstracted graphical design environment and reducing bitstream compile times The framework was applied to the Convey HC-1 platform and its usability and productivity tested by participants of the NSF-funded bioinformatics Summer Institute at the Virginia Bioinformatics Institute In a week the participants successfully extended and accelerated a reference Smith-Waterman FPGA accelerator designed in Azido achieving a theoretical throughput speedup of 32x and additional functionality However signi\002cant de\002ciencies in Azido's graphical syntax speci\002cally the poor synchronization abstractions proved to be major barriers to the participants creation of complex logic Future research efforts include the consideration of other design front-ends such as LabVIEW e xploration of memory abstractions other than simple streaming objects and gaining an understanding of the in\003uence of domain-speci\002city on the usability and performance of accelerator design 003ows A CKNOWLEDGMENT This work was supported in part by the I/UCRC Program of the National Science Foundation under Grant Nos EEC0642422 and IIP-1161022 and by NSF Award No OCI1124123 High Performance Computing in the Life/Medical Sciences R EFERENCES  L Liu Y  Li S Li N Hu Y  He R Pong D Lin L Lu and M La w  223Comparison of next-generation sequencing systems,\224 BioMed Research International  vol 2012 2012  L J McIv er  J W  F ondon III M A Skinner  and H R Garner  223Evaluation of microsatellite variation in the 1000 genomes project pilot studies is indicative of the quality and utility of the raw data and alignments,\224 Genomics  vol 97 no 4 pp 193\226199 4 2011 A v ailable htt p://www sciencedirect.com/science/article pii/S0888754311000024  M S Rosenber g Sequence alignment  methods models concepts and strategies  Berkeley University of California Press 2009  Con v e y Computer 223The con v e y hc-1 comput er architecture o v ervie w  224 http://conveycomputer.com/Resources/ConveyArchitectureWhiteP.pdf  Data I/O 223 Azido beta 224 http://azido.net 2012  B Nelson M W irthlin B Hutchings P  Athanas and S Bohner  223Design productivity for con\002gurable computing,\224 in ERSA 08 Proceedings of the International Conference on Engineering of Recon\002gurable Systems and Algorithms  2008 pp 57\22666  K Pereira P  Athanas H Lin and W  Feng 223Spectral method characterization on fpga and gpu accelerators,\224 Recon\002gurable Computing and FPGAs ReConFig 2011 International Conference on  pp 487\226492 Nov 30 2011-Dec 2 2011  W  chun Feng and K W  Cameron 223The green500 list Encouraging sustainable supercomputing,\224 Computer  vol 40 no 12 pp 50\22655 Dec 2007  J D Bak os 223High-performance heterogeneous computing with the convey hc-1,\224 Computing in Science  Engineering  vol 12 no 6 pp 80\22687 Nov.-Dec 2010  Con v e y Computer  223P ersonality de v elopment kit pdk for convey hybrid-core computers,\224 http://conveycomputer.com/Resources PersonalityDevelopmentKit.pdf  National Instruments 223Ni lab vie w impro ving the producti vity of engineers and scientists national instruments,\224 http://www.ni.com/labview 2012  MathW orks Inc 223Simulink simulation and model-based design 224 http www.mathworks.com/products/simulink 2012  T  Frangieh and P  Athanas 223 A design assembly frame w ork for fpg a back-end acceleration,\224 in International Conference on Recon\002gurable Computing and FPGAs ReConFig 2012  to appear  Xilinx 223Hierarchical design methodol ogy guide 224 http www.xilinx.com/support/documentation/sw  manuals/xilinx13  1 Hierarchical  Design  Methodology  Guide.pdf  V ir ginia Bioinformatics Institute 223P artnership supercomputing program,\224 https://www.vbi.vt.edu/high  performance  computing  P  Zhang 223Implementation of the smith-w aterman algorithm on a recon\002gurable supercomputing platform,\224 Proceedings of the 1st international workshop on High-performance recon\002gurable computing technology and applications held in conjunction with SC07 HPRCTA 07  p 39 2007  T  F  Smith and M S W aterman 223Identi 002cation of common molecular subsequences.\224 J Mol Biol  vol 147 no 1 pp 195\226197 Mar 1981 264 


  features has been generated using SMOTE balancing of survived and non-survived classes As evident from the figures, there are many classification schemes that perform well. After combining the top 3 perfor ming classification schemes on the SMOTE balanced dataset and using ensemble voting to combine their predictive powers we noticed that ensemble voting shows the best results in our study The ensemble voting model has predictive percentage accuracy of 90.38%, 88.01%, and 85.13% for 1 year, 2 years, and 5 years respectively and an AUC of 0.96 0.95, and 0.92 for 1 year, 2 years, and 5 years respectively Fig. 4.   1 year survivability area under the curve \(AUC ison of 65 attributes, 13 attributes and 13 attributes after SM OTE class balancing Fig. 3.  1 year survivability percentage accuracy comparison of 65 attributes, 13 attributes and 13 attributes after SMOTE clas s balancing 13 


VII  C ONCLUSION AND F UTURE W ORK  In this paper, we used different basic and meta classification schemes to construct models for survival prediction for colon cancer patients. Prediction accuracies of 90.38%, 88.01 and 85.13%and an AUC of 0.96, 0.95, and 0.92 were obtained for the 1-year, 2year and 5-year colon cancer survival prediction using the ensemble voting classification scheme. We have identified 13 attributes that have approximated the predictive power of 65 attributes. We also demonstrate how balancing the classes in the dataset yields better results if the imbalance is big Future work includes exploring more techniques to deal with imbalanced data. Also, we plan to build a colon cancer outcome calculator. An outcome calculator can accurately estimate survivability of a colon cancer patient. Moreover, it can aid doctors in decision-making and provide a better understanding of the risks involved in a particular treatment procedure, based on patientspecific attributes. Further more we also plan to do similar analysis for other cancers VIII  A CKNOWLEDGMENTS  This work is supported in part by the following grants: NSF awards CCF-0833131, CNS-0830927, IIS0905205, CCF-0938000, CCF-1029166, and OCI1144061; DOE awards DE-FG02-08ER25848, DESC0001283, DE-SC0005309, DESC0005340, and DESC0007456; AFOSR awar d FA9550-12-1-0458 R EFERENCES  1  Parkin DM, Whelan SL, Ferlay J, Teppo L, Thomas DB. Cancer incidence in five continents. Lyon: International Agency for Research on Cancer. Vol. VIII. IARC Scient. Publ. No. 155 2002 2  Surveillance, Epidemiology, and End Results \(SEER 1973-2009 Cancer Institute, DCCPS, Surveillance Research Program Surveillance Systems Branch, released April 2012, based on the November 2011 submission 3  Z.H. Zhou and Y. Jiang, Medical diagnosis with c4.5 rule preceded by artificial neural network ensemble, IEEE Transactions on Information Technology in Biomedicine 7\(1 2003 4  D. Delen, G. Walker and A. Kadam, Predicting breast cancer survivability: a comparison of three data mining methods 2005 5  A. Endo, T. Shibata and H. Tanaka, Comparison of seven algorithms to predict breast cancer survival, Biomedical Soft Computing and Human Sciences 13\(2 2008 6  D. Chen, K. Xing, D. Henson, L Sheng, A. Schwartz and X Cheng, Developing prognostic systems of cancer patients by ensemble clustering Journal of Biomedicine and Biotech nology 2009 2009 7  D. Fradkin, Machine learning methods in the analysis of lung cancer survival data, DIMACS Technical Report 2005-35 February 2006 8  Fathy, Sherif Kassem. "A predication survival model for colorectal cancer." In Proceedings of the 2011 American conference on applied mathematics and the 5th WSEAS international conference on Computer engineering and applications, pp. 36-42. World Scientific and Engineering Academy and Society \(WSEAS 9  Stojadinovic, Alexander, John S. Eberhardt, Elizabeth Ben Ward, Aviram Nissan, Eric K. Johnson, Mladjan Protic, George E. Peoples, Itzhak Avital, and Scott R. Steele. "Clinical Decision Support and Individualized Prediction of Survival in Colon Cancer: Bayesian Belief Network Model." Annals of surgical oncology 20, no. 1 \(2013 10  Wang, Samuel J., Clifton D. Fuller, Rachel Emery, and Charles R. Thomas Jr. "Conditional survival in rectal cancer: a SEER database analysis." Gastrointestinal cancer research: GCR 1, no 3 \(2007 11  M. Hall, Correlation-based feature selection for machine learning, PhD thesis, Citeseer, 1999 12  Chawla, Nitesh V., Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. "SMOTE: synthetic minority oversampling technique arXiv preprint arXiv:1106.1813 2011 13  J. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993 14  I. Witten and E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, Morgan Kaufmann, San Fran cisco, CA, 2005 15  L. Breiman. Bagging predictors Machine Learning, 24\(2 140, 1996 16  Y. Freund and L. Mason, The alternating decision tree learning algorithm, in: Proceeding of the 16th International Conference on Machine Learning, Morgan Kaufmann, Citeseer, 1999, pp 124–133 17  J. Friedman, T. Hastie and R. Tibshirani, Special invited paper Additive logistic regression: a statistical view of boosting Annals of Statistics 28\(2 2000 18  Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. 1996 19  T. Ho, The random subspace method for constructing decision forests, IEEE Transactions on Pattern Analysis and Machine Intelligence 20\(8 1998 20  J. Kittler, Combining classifiers: a theoretical framework Pattern Analysis and Applications 1\(1 1998 21  M. Hall, E. Frank, G. Holmes B. Pfahringer, P. Reutemann and I.H. Witten, The weka data mining software: an update SIGKDD Explorations 11\(1 2009 22  Bradley, Andrew P. "The use of the area under the ROC curve in the evaluation of machine learning algorithms." Pattern recognition 30, no. 7 \(1997 14 


  Fig. 6.   2 years survivability area under the curve \(AUC ison of 65 attributes, 13 attributes and 13 attributes after S MOTE class balancing Fig. 5.  2 years survivability percentage accuracy comparison of 65 attributes, 13 attributes and 13 attributes after SMOTE clas s balancing 15 


   Fig. 8.   5 years survivability area under the curve \(AUC ison of 65 attributes, 13 attributes and 13 attributes after S MOTE class balancing Fig. 7.  5 years survivability percentage accuracy comparison of 65 attributes, 13 attributes and 13 attributes after SMOTE cla ss balancing 16 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 1455…1456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593…605  Y  Loukas  Arti“cial neural netw orks in liquid chromatography Ef“cient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119…129 2000  N Serbedzija Simulating arti“cial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 56…63 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 90…97  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 24…34 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 1726…1730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179…187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281…288 2007  U Seif fert  Arti“cial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135…150 2004  D Calv ert and J Guan Distrib uted arti“cial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 2…10  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534…537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 2…16  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810…818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Ef“cient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285…296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 2…16 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


