Efficient Association Rule Mining using Indexing Support  VEDULA VENKATESWARA RAO 1  ASSOCIATE PROFESSOR DEPARTMENT OF CSE SRI VASAVI ENGINEERING COLLEGE TADEPALLIGUDEM-534101 Mobile No: +91-9866958546 1 venkatvedula@yahoo.com  Abstract  This paper presents the Btree index, a general and compact structure which provides tight integration of item set extraction in a relational DBMS The Data bases may be Transactional Data bases or Relational Data bases. Since no constraint is enforced during the index creation phase, Btree index provides a complete representation of the original database. The Btree Index creates prefix tree like structure and stores addresses of disk blocks where relational data base records are stored. Along with the disk blocks Btree index maintains pointers to disk blocks. Using these pointers it extracts required data base records from data base into main memory and finds corresponding item sets for association rule mining. To reduce the I/O cost data accessed together during the same extraction phase are clustered on the same disk block. The Btree index structure can be efficiently exploited by different item set extraction algorithms. In particular, Btree index data access methods currently support the FP-growth IBTree, FPMax  and LCM v.2 algorithms, but they can straightforwardly support the enforcement of various constraint categories. The Btree index has been integrated into the SQL SERVER, PostgreSQL, Oracle DBMS and exploits its physical level access methods Experiments, run for both sparse and dense data distributions, show the efficiency of the proposed index and its linear scalability also for large data sets. Item set mining supported by the Btree index shows performance always comparable with, and often \(especially for low supports\ter than, state-of-the-art algorithms accessing data on flat file Key words Data Mining, Market Basket Analysis Association Rule Mining, Item Set Extraction, Indexing, Btree Fpgrowth, LCM 1. Introduction In Todayís Business environment, it is very desirable for larger Business organizations to know the customer interest in purchasing the items. This is particularly useful in Market Basket analysis. The Association rules using Item Set data are useful in Business Analysis, Business Intelligence, Business Strategy, Market Basket Analysis, Strategic Planning, and Catalog Design.  Association rule mining discovers correlations among data items in a transactional database D. Each transaction in D is a set of data items Association rules are usually represented in the form A B, where A and B are item sets, i.e., sets of data Items Item sets are characterized by their frequency of occurrence in D, which is called support. Research activity usually focuses on defining efficient Algorithms for item set extraction, which represents the most computationally intensive knowledge extraction task  in association rule mining Th e data to be ex tracted an d  analyzed using data mining is usually stored into binary files, possibly extracted from a DBMS. Item Set Mining involves three phases. 1\ata Extraction 2\m Set Preparation 3\ Finding Association Rules.Most Algorithms identified from [1      ploit  ad hoc main memory data structures to efficiently extract item sets from a flat file . Recently, disk-based extraction algorithms   av e been propos ed to s u pport th e  extraction from large data sets but still dealing with data stored in flat files. To reduce the computational cost of item set extraction, different constraints maybe enforced among which the most simple is the support constraint which enforces a threshold on the minimum support of the extracted item sets. Algorithms identified from       du ces li m itat i on s su c h as If th e  query patterns change it does not provide better result Query response does not perform well if Query patterns changes. i.e   speed of execution, Efficiency and reliability is less, performance may degrade if the database size gets increases, Tradition feature selection technique may offer less or no data pruning capability given query attributes, Query response does not perform well if query patterns changes and   performance may degrade if the database size gets increased 1.1  Characteristics of Item Set Mining using BTree Index Relational DBMSís exploit indices, which are ad-hoc data structures to enhance query performance and support the execution of complex queries. In this paper, a similar approach to data mining queries is presented. The IEEE-International Conference on Recent Trends in Information Technology,   ICRTIT 2011 978-1-4577-0590-8/11/$26.00 ©2011 IEEE MIT,  Anna University, Chennai.  June 3-5, 2011 683 


Association Rule mining on relational data base is proposed to perform Item Set mining efficiently and extract item sets from relational data base. The BTree index is a novel data structure that provides a compact and complete representation of transactional data supporting efficient item set extraction from a relational DBMS. It is characterized by the following properties 1.  It is a covering index That means, nothing like support constraint is enforced during the Index creation phase. Because of this reason BTree supports to perform Queries of different Patterns The Data Extraction algorithms can extract data items with out   accessing the original data base 2. It is incremental index also called as  I-Forest Index. A technique for the incremental update of the index, suitable for association rule extraction on evolving databases, has been proposed and implemented. By means of the proposed index called I-Forest, incoming data blocks are stored on disk in appropriate \(compact\tructures. During the mining phase, only the data required by the current mining process is actually loaded in main memory. To allow different kinds of analysis and easy incremental insertion of new blocks \(or deletion of obsolete ones each data block is represented separately and independently of all others 3. A control feedback technique is introduced for measuring the performance 4.  Through this a database could benefit from an index change. The index selection minimizes the cost of the queries in the work load 5. Online index selection is designed in the motivation if the query pattern changes over time 6. Btree supports the enforcement of various constraint categories 8. The Btree   Physical organization supports efficient data access during item set extraction. Correlation analysis is used for data access together during pattern extraction. This minimizes number of physical data blocks read during the Mining process   This paper is organized as follows: Section 2 completely describes The Item Set Mining using Btree index System Design and Implementation. Section 3 describes Btree index architecture and work flow by addressing its components.  Section 4 describes data access methods Section 5 describes Btree index structure, by addressing its structure, its data access methods, and its physical organization.  Section 6 describes how the IB-Tree and LCM v.2 algorithms may exploit Btree index to perform efficiently the extraction of item sets. It also describes how the Btree index supports the enforcement of various constraint types. Section 7 presents description about experiment of Btree index. Section 8 presents conclusion and future scope 2.  System Design and Implementation The overall system design of Item Set Mining using BTree Index is  described in Figure 1  Figure 1. Layered System Design Item Set Mining using BTree Index is composed of 5 layers 1\r Interface Layer  It is responsible for interaction with the user and various calls   to various graphical and visualization utilities. This Module provides an Interface for each user to invoke with the system and to execute Queries for generating Frequent Item Sets \(Frequent Item Patterns\ides the following  services 1.1  To design interface for IBtree algorithm to generate frequent patterns The Interface receives parameters Minimum support and Minimum confidence called Interesting measures\(interesting constraints\ and produces frequent patterns as output 1.2  To design interface for LCM algorithm to generate frequent patterns The Interface receives parameters Minimum support and Minimum confidence called Interesting measures\(interesting constraints\ and produces frequent patterns as output 1.3\n Interface to displays the performance results of IBTree and LCM algorithms 2\a Mining Algorithms Layer This is the main module connecting all system components. In this module there are set of Data Mining Algorithms to execute Query and generate frequent Item Sets. Here there are  two algorithms to generate frequent item sets for finding Association rules among them 1\Tree Algorithm 2\CM Algorithm 2.1  IBTree Algorithm It receives set of transactional data items from relational data model and two interesting measures Min Support Min Confidence as input, and then it constructs an FPTree IEEE-ICRTIT 2011 684 


for indexing and generates frequent item sets. During this process it communicates with Index Layer which constructs Index for extracting Items from Data base 2.2  LCM Algorithm  This is Linear Time Closed Item Set Mining. It receives set of transactional data items from relational data model two interesting measures Min Support, Min Confidence and then generates Frequent Item Sets with the help of BTree Index at Index Layer. During the process of generating Frequent Item Sets, it uses array based structure than tree structure 3\ Layer This Module mainly focuses on creating index for data items located in data base which is used by Data Mining algorithms to extract data items in faster way for generating frequent item sets. Here the Index is Incremental Index and it is constructed using BTree and ITree data structures and it is called as BTree Index 4\ata Base Server Layer  SQL Server relational data base. Contains data base of transactional data items Receives Queries from the user interface design and transfers results from transactional data base  5\ Data Base Layer C urrently available in the proposed system are Transactional data about market basket analysis  3. Btree Index Architecture and Flow The general flow and architecture of Btree index is described in figure 2 and figure 3 System Architecture-I \(Level-I  Figure2.Work Flow System Architecture-II \(Level-II  Figure 3. System Architecture as Two components Architecture of Item Set Mining using BTree Indexing is divided into two parts   1.\Tree Index Structure   2.\s Methods The BTree index actually organizes addresses of disk blocks where data items regarding relational databases or Transactional databases are stored in disk in such a way that data access methods can directly read required data into memory to perform item set mining. More over they read only required data into memory. The structure of BTree index is characterized by two components 1  The Item set-Tree 2  The Item B-tree The two components are responsible for physical organization of database data. The two components provide two levels of indexing. The item set tree also called I-tree is a prefix tree which represents relation R as toss less compact structure it represents addresses of disk blocks in the form of tree according to item sets involved in association rule mining. The item set-tree can be based on different kinds of trees like FP-tree, H-tree, Inverted Matrix, Patricia-trees  4\Access Methods  The BTree index structure is independent of the item set Extraction algorithm and Association rule mining algorithm. For  this reason different state of art algorithms different item set extraction algorithms\ may be used in support of BTree index. The algorithms like FPtree for Fpgrowth, IB tree algorithm, array based structure for LCM can be used in support of BTree index to perform item set extraction and association rule mining Depending  on support and data items chosen in algorithm a different portion of I-tree is accessed for reading data item from the disk 5\dex Structure  The transactional data set D is represented, in the relational model, as a relation R. Each tuple in R is a pair TransactionID, ItemID\he Btree index provides a compact and complete  representation of R. It is two level indexing and incremental indexing.  Hence, it allows the efficient extraction of item sets from R, possibly enforcing support or other constraints. In Section 5.1,  the general structure of the BTree index is presented; while in Section 5.2,how data access takes place is discussed. The physical organization of the index is presented in Section 5.3 together with a discussion of access cost  5.1 Btree Index structure The structure of the Btree index is characterized by two components: the Item set-Tree and the Item-Btree. The two components provide two levels of indexing. The Item set-Tree \(I-Tree\ a prefix-tree which represents relation R by means of a succinct and lossless compact structure The Item-Btree \(I-Btree\ a B+Tree structure which allows reading selected I-Tree portions during the extraction task. For each item, it stores the physical locations of all item occurrences in the I-Tree. Thus, it supports efficiently loading from the I-Tree the transactions in R including the item. In the following Efficient Association Rule Mining using Indexing Support 685 


section details of  the I-Tree and the I-Btree structures is described  5.1.1 I-Tree An effective way to compactly store transactional records is to use a prefix-tree. Trees and prefix-trees have been frequently used in data mining and data warehousing indices, including cube forest, FP-t Htree Inverted Matrix d P a triciaT ries  T h e cu rren t  implementation of  I Tree is based on FP Tree and it is supported to be efficiently accessed by IB Tree and LCM algorithms The I-Tree associated to relation R is actually a forest of prefix-trees, where each tree represents a group of transactions all sharing one or more items. Each node in the I-Tree corresponds to an item in R. Each path in the ITree is an ordered sequence of nodes and represents one or more transactions in R. Each item in relation R is associated to one or more I-Tree nodes and each transaction in R is represented by a unique I-Tree path  Figure 4   Figure 5 BTree Index for example set in Figure 4  a\I-Tree b\Tree The Figure 4 shows a small data set and Figure 5 shows complete structure of BTree Index. In the I-Tree paths Fig. 5a\nodes are sorted by decreasing support of the corresponding items. In the case of items with the same support, nodes are sorted by item alphabetical order Each I-Tree node is  associated with a node support value representing the number of transactions which contain all the items in the sub path reaching the node. For example in sub path [e:3, h h e s u pport of n ode [h  3 i s 3 Hence, this sub path represents three transactions \(i.e transactions 3, 4, and 9\ Each item is associated to one or more nodes. The item support is obtained by adding the support of all nodes including the item Nodes in the ITree are linked by means of pointers which allow selectively loading from disk the of the corresponding node 5.1.2 I-Btree The I-Btree allows selectively accessing the I-Tree disk blocks during the extraction process. It is based on a B+Tree structure. Fig. 5b shows the I-Btree for the example data set and a portion of the pointed I-Tree. For each item i in relation R, there is one entry in the I-Btree In particular, the I-Btree leaf associated to i contains iís item support and pointers to all nodes in the I-Tree associated to item i 5.2 Data Access Methods BTree Index is general index structure. Hence, different data mining algorithms may   be applied, once   data is loaded into memory. The in memory representation is suitable for the selected extraction algorithm such as FPtree for FP-growth, IBTree for IBTree algorithm, arraybased structure for LCM 5.2.1  Loading the Frequent-Item Projected Database The frequent-item projection of relation R with respect to an arbitrary item includes the transactions in R where  occurs, intersected with the items having higher support than or equal support but preceding in lexicographical order. In the I-Tree paths, items are sorted by descending support and lexicographical order. Thus, the projection is represented by the I-Tree prefix paths of item   Figure 6. Loading the Frequent-Item Projected Database 5.2.2 Loading the Support-Based Projected Database The support-based projection of relation R contains all transactions in R intersected with the items which are frequent with respect to a given support threshold MinSup. The I-Tree paths completely represent the database transactions. Items are sorted by decreasing support along the paths. Thus, the support-based projection of R is given by the I-Tree sub paths between the I-Tree roots and the first node with an unfrequented item IEEE-ICRTIT 2011 686 


 Figure 7. Loading the Support-Based Projected Database 5.3 Physical Organization The physical organization of the BTree  index is designed to minimize the cost of reading the data needed for the current  extraction process. The I-Btree allows a selective access to the I-Tree paths of interest. Hence, the I/O cost is mainly given by the number of disk blocks read to load the required I-Tree paths. The I-Tree physical organization is based on the following correlation types 1  Intratransaction correlation Extraction algorithms   consider items occurring in the same transaction. Items appearing in a transaction are thus intrinsically correlated 2\tertransaction correlation Transactions with some items in common will be accessed together when item sets including the common items are extracted 5.3.1 I-Tree Layers I-Tree paths are partitioned into three layers, based on node access frequency Top Layer This layer includes nodes that are very frequently accessed during the mining process. These nodes are located in the upper levels of the I-Tree. They correspond to items with high support, which are distributed over few nodes with high node support Middle Layer This layer includes nodes that are quite frequently accessed during the mining process. These nodes are typically located in the central part of the tree They correspond to items with relatively high support, but not yet dispersed on a large number of nodes with very low node support Bottom Layer This layer includes the nodes corresponding to rather low support items, which are rarely accessed during the mining process. Nodes in this layer are analyzed only when mining frequent item sets for very low support thresholds  6. The IB-Tree and LCM v.2 algorithms 6.1 LCM  Algorithm LCM is an abbreviation of Linear Time Closed item set miner. This is used for enumerating all frequent item sets In LCM we define parent-child relation ship between frequent closed item sets Algorithm LCM \(X: frequent closed item set 1  output X 2  For each i>i\(X 3  if X\(i\ frequent an T\(X[i en  4  Call LC  5  end for The following is detailed description of algorithm Global  T,DT   /* Global Lists  Algorithm LCM 1  X:=I\(T The Root 2  For i:=1 to   |E 3  if a tisf i e s con d2 d c on d3 T H EN  4  Call LC X[i  or  5  Call  LC T  X[i I,DT  Based on the criteria 6. end  for Algorithm LCM_Iter\(X,T\(X\i\(X 1  Output X 2  for each T T\(X 3  for each j T,j>i\(X\Insert I to T\(j 4  for each j,T\(j in the decreasing order 5  if\(|T|j and \(cond2\olds then 6  LCM_Iter\(T\(T\(j\T\(j\j 7  delete T\(j 8  end for 7. EXPERIMENTAL RESULTS The proposed approach is validated by means of a large set of experiments addressing the following issues 1. Performance of the Btree index creation, in terms of both creation time and index size. 2. Performance of frequent item set extraction, in terms of execution time memory usage. 3. Scalability of the approach Figure 8.1, 8.2 describes the results \(Frequent Item Sets IBTree Algorithm   Figure 8.1 Results of IBTree     Figure 8.2 Results of IBTree Figure 9.1, 9.2, describes the results \(Frequent Item Sets LCM Algorithm   Figure 9.1 Results of LCM      Figure 9.2 Results of LCM   Figure 10 shows the performance results of IBTree Algorithm and LCM algorithm \(Support and Time Analysis and Figure11 shows the performance results of IBTree Efficient Association Rule Mining using Indexing Support 687 


Algorithm and LCM algorithm \(Support and Memory Size Analysis    Figure 10. Time complexity        Figure 11.Space Complexity Figure12 shows Support and time analysis of IBTree Algorithm and Figure 13 shows Support and time analysis of IBTree Algorithm    Figure 12.Time complexity  Figure13. Time complexity Figure 14 shows Support and memory size analysis of IBTree Algorithm and Figure 15 shows Support and memory size analysis of LCM Algorithm    Figure 14. Space Complexity Figure 15. Space Complexity  Figure 16 and Figure17 shows the results of IBTree Algorithm and LCM algorithm \(Support and Time Analysis using Bar chart    Figure 16 Time complexity Figure 17 Time complexity 8. CONCLUSIONS AND FUTURE WORK The BTree index is a novel index structure that supports efficient item set mining into a relational DBMS. It has been implemented into the PostgreSQL open source DBMS, SQL Server, Oracle by exploiting its physical level access methods. The BTree index provides a complete and compact representation of transactional data. It is a general structure that efficiently supports different algorithmic approaches to item set extraction Selective access of the physical index blocks significantly reduces the I/O costs and efficiently exploits DBMS buffer management strategies. This approach implemented into a relational DBMS, yields better performance than the state-of-the-art algorithms \(i.e Prefix-Tree and LCM v.2 accessing data on a flat file and is characterized by a linear scalability also for large data sets with incremental update of the index. When the transactional database is updated, the BTree index needs not to be rematerialized. It incrementally updates the index when new data become available. Since no support threshold is enforced during the index creation phase, the incremental update is feasible without accessing the original transactional database REFERENCES 1  A g ra w a l a n d R Srik an t F a s t  Algo ri t h m for M i n i n g A s soc i at i on Rules,î Proc. 20th Intíl Conf. Very Large Data Bases \(VLDB í94 Sept. 1994 2 R A g r a w a l  T   I m il ie nski an d A   S w am i M i n i n g A s so ciat i o n  Rules between Sets of Items in Large Databases,î Proc. ACM SIGMOD í93, May 1993 3 J  H a n  J  P e i, a n d Y  Y i n M i n i n g F r e que nt P a t t e r ns w i tho u t  Candidate Generation,î Proc. ACM SIGMOD, 2000 4  M a nn i l a  H T o i von en   a n d A  I  Verk a m o E ffi ci en t A l gori t h m s  for Discovering Association Rules,î Proc. AAAI Workshop Knowledge Discovery in Databases \(KDD í94\, pp. 181-192 1994 5  S a va s e re E  Om i eci n s k i  and S  B  Na va th e A n E f fi ci en t Algorithm for Mining Association Rules in Large Databases Proc. 21st Intíl Conf. Very Large Data Bases \(VLDB í95\, pp. 432444,1995 6 o i v o n e n  S am p l i n g L a rg e Dat a b a se s fo r A sso ci at i o n R u l e s  Proc. 22nd Intíl Conf. Very Large Data Bases \(VLDB í96\, pp. 134145,1996 7 M  El H aj j a n d O  R Z a ia ne I nv er te d M a tr ix  Ef f i cie n t D i s c o v e ry  of Frequent Items in Large Datasets in the Context of Interactive Mining,î Proc. Ninth ACM SIGKDD Intíl Conf. Knowledge Discovery and Data Mining \(SIGKDD\, 2003 8 G  G r ahne an d J  Z h u M i n i n g F r e que nt  I t e m s e ts f r o m S e co ndar y  Memory,î Proc. IEEE Intíl Conf. Data Mining \(ICDM í04\, pp. 91-98 2004 9 G  Ram e s h W  M a nia t ty a n d M   Z a ki I nde x i ng an d D a ta A c ce s s  Methods for Database Mining,î Proc. ACM SIGMOD Workshop Data Mining and Knowledge Discovery \(DMKD\ 2002  Y L  C h eun g   M in in g F r eq u e n t  I t em s e t s  w i t h out S u pp ort Threshold: With and without Item Constraints,î IEEE Trans Knowledge and Data Eng., vol. 16, no. 9, pp. 1052-1069, Sept. 2004 11 G  Co ng a n d B  L i u S pe e d U p I t e r ativ e F r e que nt I t e m s e t Mi ni ng  with Constraint Changes,î Proc. IEEE Intíl Conf. Data Mining ICDM í02\, pp. 107-114, 2002 12  C.K S  L e ung L  V  S  L a kshm a n an a n d R  T  N g  E x p l o iti ng  Succinct Constraints Using FP-Trees,î SIGKDD Explorations Newsletter, vol. 4, no. 1, pp. 40-49, 2002 13  R. S r ik an t Q  V u  an d R. A g r a w a l   M i n i n g A sso ciat io n R u l e s with Item Constraints,î Proc. Third Intíl Conf. Knowledge Discovery and Data Mining \(KDD í97\, pp. 67-73, 1997 14 T  U n o   M. K i y o m i an d H  A r im ur a L CM v e r  2 Ef f i cie n t  Mining Algorithms for Frequent/Closed/Maximal Itemsets Proc. IEEE ICDM Workshop Frequent Itemset Mining Implementations FIMI\, 2004 15 J  P e i, J  H a n a n d L  V  S   L a ks h m an an  P us hi ng Co nv e r tibl e  Constraints in Frequent Itemset Mining,î Data Mining and Knowledge Discovery, vol. 8, no. 3, pp. 227-252, 2004 IEEE-ICRTIT 2011 688 


 Figure 4  Compression of the action table from Nursery dataset of size 250Mb into FP-Tree with varying minimum support values VII  C ONCLUSION AND FUTURE WORK  In this paper, we propose the action table as the ideal search domain for action rules mining. The action table transforms the complex problem of finding action rules from a plain decision table, into finding action rules from an action table. As a result, the problem of action rules mining is reformulated into association-mining In practice, we applied FAARM on the Hepatitis and Nursery datasets and compared the results and performances with AAR and ARD. Although the space and time complexity associated with generating the action table are O\(n 2 experiments show that FAARM has a better execution time on relatively small dataset, over ARD and AAR Generating the action table directly into the FP-Tree could mitigate the space complexity associated with action table. As a future work, we propose to look at parallel implementation of Apriori and FP-Growth to test the scalability of using the action table with large datasets R EFERENCES  1  Z. Pawlak, çInformation systems - theoretical foundationsé, Information Systems Journal, Elsevier, Vol 6, 1981, 205-218 2  J. Han, J. Pei, and Y. Yin, çMining frequent patterns without candidate generationé, ACM SIGMOD International Conference on Management of Data, 2000, 1 12 3  Z.W. Ras and A. Wieczorkowska,  "Action-Rules: How to Increase Profit of a Company",  The Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases, 587-592 4  Z. He, X. Xu, S. Deng, R. Ma, çMining action rules from scratché, Expert Systems with Applications, Elsevier, Vol 29, No. 3, 2005, 691-699 5  Z.W. Ras and A. Dardzinska,  "Action Rules Discovery without Pre-existing Classification Rules",   The Sixth International Conference on Rough Sets and Current Trends in Computing, 2008, 181-190 6  Z.W. Ras, A. Dardzinska, L. Tsay,  and H. Wasyluk Association Action Rules",   IEEE International Conference on Data Mining Workshops, 2008, 283-290 7  Qiang Yan, Jie Yin, Charles Ling, Tielin Chen,"Postprocessing Decision Trees to Extract Actionable Knowledge", IEEE International Conference on Data Mining, 2003,  685-688 8  Z.W. Ras and L. Tsay,  "Discovering Extended ActionRules \(System DEAR\,   International IIS IIPWM'03 Conference, 2003, 293-300 9  L. Tsay and Z.W. Ras,  "Action rules discovery: system DEAR2, method and experiments",   Journal of Experimental & Theoretical Artificial Intelligence, 2005 119-128   Z.W. Ras, E. Wyrzykowska,  and H. Wasyluk,  "ARAS Action Rules Discovery Based on Agglomerative Strategy",   Third International Workshop on Mining Complex Data, 2007, 196-208   http://archive.ics.uci.edu/ml/datasets   S. Im and Z.W. Ras,  "Action Rule Extraction from a Decision Table: ARED",   International Syposium on Methodologies for Intelligent Systems, 2008, 160-168   J. S. Deogun, V. V. Raghavan, and H. Sever, çRough set based classification methods and extended decision tables International Workshop on Rough Sets and Soft Computing, 1994, 302-309   R. Agrawl and R. Srikant, çFast algorithm for mining assocation rules,é International Conference on Very Large Data Bases, 1993, 487-499  
404 


a  Figure 1.  The original Share-struct and the steps of S?s change C. Share-FPM algorithm In this subsection, we will develop an efficient algorithm for mining all frequent patterns Algorithm 2 Share-FPM Input: S?, the Share-struct constructed based on Algorithm 1 and s, the minimum support threshold Output: The complete set of frequent patterns Method:Call Share-FPM \(S?,?, s Procedure Share-FPM \(S? ,?, s 1 for each entry si in S 2   if \(si.local-count<s and si->new! =NULL 3     add all si ->news children to the new fields of relevant entries in S 4   else ? =?si; ?-SD = ?-SD 5     if \(si ->new != NULL 6       if si is the only active entry 7         ?-SD=?-SDsi 8         add all children of si to relevant entries in S 9         si ->old= S 10      else ?-Postfix = ?-Postfix ?si 11        if \(si ->old == S 12          call Inherit\(si, S TID Items Bought \(Ordered 100 f, a, c, d, g, i, m, p f, c, a, m, p 200 a, b, c, f, l, m, o f, c, a, b, m, l, o 300 b, f, h, j, o f, b, o 400 b, c, k, s, p c, b, p 500 a, f, c, e, l, p, m, n f, c, a, m, p, l 1431 13        else call Initialize \(S?, si->new, si->old 14        if \(si ->old == NULL 15          flag_upload =TRUE 16        si->old = S 17        call Share-FPM \(S?, ?, s 18        if \(flag_upload ==TRUE 19          call Upload \(S?, S 20    else if \(si ->old ? S 21      ?-SD=?-SD?the items that appear after ? in ?-SD 22      ?-Postfix=the items that appear after ? in ?-Postfix 


23      for each pi in ?-Postfix 24        ? = ??pi 25        ?-SD = the items that appear before pi in ?-SD 26        call Share-FPM \(pi->old, ?, s 27      call Generate-FP \(?, ?-SD, ?-Postfix 28 call Generate-FP \(?,?-SD, ?-Postfix the end of Share-FPM The procedures Inherit, Upload and Generate-FP are shown in the following Procedure Inherit \(s, S if s->old can be released then //memory management S? = s->old else create a new Share-table S? by inheriting s->old call Initialize \(S?, s->new  Procedure Upload \(S?, S for each entry si in S upload all new and old fields in S? to old fields in S add si.local-count in S? to si.local-count in S  Procedure Generate-FP \(?,?-SD, ?-Postfix for each nonempty combination ? of the items in ?-SD generate pattern ?? ? with support minimum support of items in it for each item pi in ?-Postfix for each combination ? of the items which appear before pi generate pattern ?? ? ? pi with support minimum support of items in it  D. Share-UFPM Algorithm According to our mining model description, each utility frequent pattern is also frequent. After Share-FPM algorithm finds all frequent patterns, the Share-UFPM algorithm scans the database once to check whether each frequent pattern candidate algorithm is as follow Algorithm 3 Share-UFPM Input:   S?, s Output:   UFP, utility frequent patterns in DB Method:  Call Share-UFPM\(S?, s 


Procedure Share-UFPM\(S?, s UFP FP = Share-FPM \(S?,?, s for each transaction Ti ? DB for each candidate c ? FP if \(c ? Ti and u \(c, Ti c.support for each candidate c ? FP if \(c.support ? s UFP = UFP + c return UFP  IV. PERFORMANCE STUDY To evaluate the efficiency and effectiveness of our algorithms, we have done extensive experiments on various kinds of datasets with different features. The experiments are based on a 2.4GMHz Pentium IV PC with 512MB main memory and 60 GB hard driver, running on Microsoft Windows 2000 Server. All the programs are written in Microsoft/Visual C++6.0 The measured performance is algorithms execution time on the datasets with different minimum support threshold. The execution time only includes the disk reading time \(scan datasets output frequent patterns speed of disk writing A. Datasets and characteristics We use real world and synthetic data for our performance study. The basic characteristics of datasets are listed in the following The real world dataset called Retail is achieved from a retailing company. Retail contains products from various categories. There are 16469 items and 88162 transactions in the dataset. Each transaction consists of the products purchased by a customer at a time point. Its average transaction size and average maximal potentially frequent patterns size are 10.3 and 3. The size of this dataset is 4M The synthetic data sets which were used for the experiments were achieved from the online FIMI repository See the RUL: http://fimi.cs.helsinki.fi/. The data sets are T10I4D100K and T40I10D100K. In T10I4D100K, the average record size and average maximal potentially frequent patterns 


size are 10 and 4. In T40I10D100K, they are 40 and 10. The numbers of transactions in both two dataset are set to 100K There are exponentially numerous frequent patterns when the support threshold goes down B. Experimental results In order to mine the utility frequent patterns, we randomly generate the count of each item between 1 and 6. In fact, most items are in the low profit range, we synthetically generate utility values of each item from 0.01 to 10.00, using a log normal distribution. For instance, Fig.2 shows the distribution of the utility values of items in T10I4D100K 1432 0 1 2 3 4 5 6 7 8 9 10 0 20 40 60 80 100 120 140 160 180 N um be r o f i te m s Utility value  Figure 2.  Utility value distribution in T10I4D100K For selecting appropriate utility thresholds, we use the average transaction utility value to constraint the utility threshold instead of randomly choosing it. For example, in Table 1, where the average transaction utility value is 40. If the utility threshold is equal to 25%, it represents that ? = 10 40  25% =10 We compare the performance of Share-FPM with BUUFM [5], an up to date algorithm for utility frequent patterns 


mining. Fig.3 through Fig.5 show the performance curves of two algorithms on three datasets respectively. We can see that the Share-UFPM algorithm outperforms BU-UFM on all datasets, and the performance gap becomes significant when the minimum support threshold drops low enough 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0 10 20 30 40 50 60 70 80 90 100 110 120 Ti m e S ec on ds  Minimum support \(%0 Share-UFPM BU-UFM Utility threshold=5  Figure 3.  Fig.12 Utility frequent patterns mining on Retail 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 350 


400 450 500 550 600 Ti m e S ec on ds  Minimum support \(%0 Share-UFPM BU-UFM Utility threshold=5  Figure 4.  Fig.13 Utility frequent patterns mining on T10I4D100K 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0 50 100 150 200 250 300 350 400 450 500 550 Ti m e S ec on ds  Minimum support Share-UFPM 


BU-UFM Utility threshold=5  Figure 5.  Utility frequent patterns mining on T40I10D100K V. CONCLUSIONS In this paper, we introduce a utility frequent pattern mining model based on a share strategy to find the combination of items with high frequencies and utilities. This model first find all patterns with a given minimum support threshold. In this step, a share strategy gives a way to share most of the results from the previous mining process instead of separating them distinctively, thereby dramatically reducing the cost of computation. And then all patterns that do not satisfy a minimum utility threshold are pruned The extension of our technique, for maintenance of the already mined utility frequent patterns when updating databases, is an interesting topic for future research REFERENCES 1] R. Agrawal, T. Imielinski, A. Swami, Mining association rules between sets of items in large databases, In: Proceedings of the 1993 ACM-SIGMOD, Washington, DC, 1993, 207216 2] J. Han, H. Cheng, D. Xin, X. Yan, Frequent pattern mining: current status and future directions, Data Min knowl Disc 2007, 55-86 3] Y. Liu, W. Liao, A.Choudhary, A two-phase algorithm for fast discovery of high utility itemsets, Lecture Notes in Artificial Intelligence 2005, 3518:689-695 4] Y. Liu, W. Liao, A. Choudhary, A fast high utility itemses mining algorithm, In: Proceeding of the 2005 ACM SIGKDD workshop on utility-based data mining, Chicago, Illinois, USA, 2005, 90-99 5] J. Yeh, Y. Li, C. Chang, Two-phase algorithms for a novel utilityfrequent mining model, Lecture Notes in Artificial intelligence 2007 4819: 433-444 6] C, Aaron, F. John, Association mining, ACM Computing Surveys 2006, 38\(2 7] H.Yao, H. Hamilton, C. Butz, A foundational approach to mining itemset utilities from databases, In: Proceeding of the 4th SIAM International Conference on Data Mining, Lake Buena Vista, Florida 2004, 428-486 8] H. Yao, H. Hamilton, L. Geng, A unified framework for utility based measures for mining itemsets, In: Proceedings of ACM SIGKDD 2nd workshop on utility-based data mining, New York, NY, 2006, 28-37 9] J. Han, M. Kamber, Data mining: concepts and techniques, 2nd edn 


Morgan Kaufmann. 2006 10] J. Han, J. Pei, Y. Yin, Mining frequent patterns without candidate generation, In: Proceeding of the 2000 ACM-SIGMOD international conference on management of data, Dallas, TX, 2000, 112 


2] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur Dynamic Itemset Counting and Implication Rules for Market Basket Data," in Proceedings of the 1997 ACM SIGMOD international conference on Management of data, Tucson, Arizona, United States 1997, pp. 255-264 3] J. S. Park, M. S. Chen, and P. S. Yu, "An Effctive Hash based Algorithm for mining association rules in Prof. ACM SIGMOD Conf Management of Data New York, NY, USA, 1995, pp. 175 - 186 4] R. Agrawal, T. ,PLHOL?VNL DQG $. Swami, "Mining Association Rules between Sets of Items in Very Large Databases," in Proceedings of the 1993 ACM SIGMOD international conference on Management of data, Washington, D.C., 1993, pp. 207-216 5] H. Mannila, H. Toivonen, and A. I. Verkamo Efficient Algorithms for Discovering Association Rules," in AAAI Workshop on Knowledge Discovery in Databases, 1994, pp. 181-192 6] R. Srikant and R. Agrawal, "Mining Generalized Association Rules," in In Proc. of the 21st Int'l Conference on Very Large Databases, Zurich Switzerland, 1995 7] R. Srikant, Q. Vu, and R. Agrawal, "Mining association rules with item constraints," in In Proc 3rd Int. Conf. Knowledge Discovery and Data Mining, 1997, pp. 67--73 8] A. Savasere, E. Omiecinski, and S. B. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 21th International Conference on Very Large Data Bases 1995, pp. 432 - 444 9] H. Mannila, "Database methods for data mining," in The Fourth International Conference on Knowledge Discovery and Data Mining, 1998 10] B. Liu, W. Hsu, and Y. Ma, "Mining Association Rules with Multiple Minimum Supports.," in SIGKDD Explorations, 1999, pp. 337--341 11] H. Yun, D. Ha, B. Hwang, and K. H. Ryu, "Mining association rules on significant rare data using relative support.," Journal of Systems and Software archive vol. 67, no. 3, pp. 181 - 191, 2003 


12] M. Hahsler, "A Model-Based Frequency Constraint for Mining Associations from Transaction Data Data Mining and Knowledge Discovery, vol. 13, no 2, pp. 137 - 166, 2006 13] L. Zhou and S. Yau, "Association rule and quantitative association rule mining among infrequent items," in International Conference on Knowledge Discovery and Data Mining, San Jose, California 2007, pp. 156-167 14] C. Ordonez, C. Santana, and L. d. Braal, "Discovering Interesting Association Rules in Medical Data," in Proccedings of ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 2000, pp. 78-85 15] L. J. Sheela and V. Shanthi, "DIMAR - Discovering interesting medical association rules form MRI scans," in 6th International Conference on Electrical Engineering/Electronics, Computer Telecommunications and Information Technology 2009, pp. 654 - 658 16] C. Ordonez, N. Ezquerra, and C. A. Santana Constraining and summarizing association rules in medical data," Knowledge and Information Systems vol. 9, no. 3, pp. 259 - 283, September 2005 17] H. Pan, J. Li, and Z. Wei, "Mining Interesting Association Rules in Medical Images," Lecture Notes In Computer Science, vol. 3584, pp. 598-609, 2005 18] S. Doddi, A. Marathe, S. S. Ravi, and D. C Torney Discovery of association rules in medical data Medical Informatics and the Internet in Medicine, vol 26, no. 1, pp. 25-33, January 2001 86 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


