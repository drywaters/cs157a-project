On-body IE A Head-Mounted Multimodal Augmented Reality System for Learning and Recalling Faces Daniel Sonntag German Research Center for Arti“cial Intelligence Saarbruecken Germany Email sonntag@dfki.de Takumi Toyama German Research Center for Arti“cial Intelligence Kaiserslautern Germany Email takumi.toyama@dfki.de 
Abstract 
We present a new augmented reality AR system for knowledge-intensive location-based expert work The multi 
modal interaction system combines multiple on-body input and output devices a speech-based dialogue system a head-mounted augmented reality display HMD and a head-mounted eyetracker The interaction devices have been selected to augment and improve the expert work in a speci“c medical application context which shows its potential In the sensitive domain of examining patients in a cancer screening program we try to combine several active user input devices in the most convenient way for both the patient and the doctor The resulting multimodal AR is an on-body intelligent environment IE and has the 
potential to yield higher performance outcomes and provides a direct data acquisition control mechanism It leverages the doctors capabilities of recalling the speci“c patient context by a virtual context-based patient-speci“c external brain for the doctor which can remember patient faces and adapts the virtual augmentation according to the speci“c patient observation and nding context In addition patient data can be displayed on the HMD„triggered by voice or object/patient recognition The learned patient faces and immovable objects e.g a big medical device de“ne the environmental clues to make the context 
dependent recognition model part of the IE to achieve speci“c goals for the doctors in the hospital routine 
I I NTRODUCTION In ubiquitous computing it can be said that most profound technologies are those that disappear by weaving themselves into the fabric of everyday professional life It would be even better if we could carry and wear those technologies on our bodies which would make us rather independent of the location 
Keywords Augmented Reality Medical Healthcare Realtime Interaction Input Devices and Strategies 
 
in which they are used For documentation purposes in the myths of the paperless of“ce  digital pens for e xample have been invented and used to implement paper-based interactions in digital environments 14 The problem is that you cannot easily replace a screen-based laptop computer because the display is often missing for a proper interaction and time-based transient interaction modes such as speech dialogue cannot replace them properly Recent display systems are available as head-mounted displays HMDs which provide new ubiquitous possibilities for interaction and real-time sys 
tems often referred to as cyber-physical systems In this paper we propose a new multimodal interaction system that can also learn important task-relevant visual information Fig 1 Patient Examination Scenario where the doctor wears a head-mounted eye-tracker and an HMD The multimodal interaction system combines multiple on-body input and output devices a speech-based dialogue system a head-mounted augmented reality display and a head-mounted eye-tracker The interaction devices have been selected to aug 
ment and improve the expert work for a particular application domain the physical examination of patients during cancer screening The scenario is shown in gure 1 Our main contribution is that we bring two major normally separated goals of intelligent interaction together thereby combining active and passive user input devices in the most convenient way for both the patient and the doctor First we want to leverage the doctors capabilities to recall patients and inform himself about a speci“c patient record For this purpose we implemented the rst online head-mounted face learning 
system which uses a mobile eye-tracker In the most advanced interface mode we allow for a real-time interactive face detection Second in the interactive experience with the doctor and the patient the system should improve the performance of the human-computer interaction and the usability in the patient context no disturbing computer screen between the patient and the doctor For this purpose we use the gaze position on the HMD in combination with an automatic speech recognizer 
2013 9th International Conference on Intelligent Environments 978-0-7695-5038-1/13 $26.00 © 2013 IEEE DOI 10.1109/IE.2013.47 151 
2013 9th International Conference on Intelligent Environments 978-0-7695-5038-1/13 $26.00 © 2013 IEEE DOI 10.1109/IE.2013.47 151 


ASR as part of the multimodal interaction structure As a result we implemented the rst multimodal interaction system which combines a mobile eye-tracker with a head-mounted display and this in combination with speech-based interaction which can now be evaluated for its task-based usability Towards this goal we provide a preliminary evaluation of the face learning and detection mode to identify avenues for further improvements of the technical machine learning approach The design of the learning and detection system could lead to new visions of other application domains interacting with mobile HMD technology and provide more dedicated scenarios where the object/face detection task can be facilitated II R ELATED W ORK Motivated by previous ndings showing the relevance of eye-gaze in multimodal conversational interfaces we e xtended the passive input idea to active user input in the medical augmented reality realm This also extends the work of using the gaze information to resolve the ambiguities of users speech  In the medical domain se v eral e xperimental settings with HMDs neither using eye-trackers nor speech input have been investigated Consider the following application example with the standard working position at the ultrasound machine with a normal display a doctor can look at the display to see the results of a scan But he or she has to turn the head towards the patient when repositioning the probe Thus in the usage of a HMD in ultrasound scanning task has been investigated as there is no need to turn the head any more Image guided surgery has been introduced successfully in many modern operating rooms In order to improve the navigation on a standard computer monitor enhanced the direct sight of the physician by an HMD overlay of the virtual data onto the doctors view in the context of the patient In HMDs have been used in various forms to assist surgeons and other medical personnel to support and improve the visualisation of the workplace related procedures III C OMPLETE S CENARIO AND M ULTIMODAL D IALOGUE S YSTEM A RCHITECTURE Over the last several years the market for speech technology has seen signi“cant developments and po werful commercial off-the-shelf solutions for speech recognition ASR or speech synthesis TTS For industrial application tasks such medicine discourse and dialogue infrastructures are available  The run-time environment we use in this special I/O scenario is based on a middleware platform which connects system components and follows a hub-and-spoke architecture The architecture comprises a number of functional components that deal with tasks like modality-speci“c interpretation contextbased interpretation interaction and task management target control presentation management and modality-speci“c generation All functional components are generally applicationindependent and are con“gured by respective models We use a distributed dialogue system architecture where every major component can be run on a different platform increasing the scalability of the overall system gure 2 Thereby the dialogue system also acts as middleware between the clients and the backend services This architectural characteristic is very suitable to monitor internal and external messages of the three major parts the multimodal interface the dialogue system and the event bus The event bus basically provides a data streaming functionality for the automatic speech recogniser ASR/NLU and the text-to-speech synthesis TTS One of the functional components of the dialogue system is the fusion and discourse engine FADE while another is the reaction and presentation component REAPR Both are important for the mobile eye-tracker and mobile HMD scenario more information about the hub-and-spoke dialogue infrastructure can be found in 13 and 12 FADE is needed to compute the discourse-context information It is based on a fusion approach which waits for appropriate multimodal input to be fused together for a interpretation that can be stored in the question and answer QA database Dialogue processing in REAPR means that the dialogue information state is implemented Information state theory of dialogue modelling basically consists of a description of informal components e.g obligations beliefs desires intentions their formal representations data structures such as ontology instances as in our case dialogue moves often conceived of as speech acts update rules trigger moves and updates of state information and update strategies in which order to apply the applicable update rules While obligations and dialogue moves do no have a role to play in the current version the integration with context-based presentation of patient information or the recognition itself is straightforward Essentially the display context de“nes what the user sees in the HMD Upon recognition of an object or person by the eyetracker and recognisers REAPR triggers the context-dependent display in the mobile HMD Other context factors such as patient and examination context can be smoothly integrated into the context model In addition REAPR is also responsible for initiating a potential TTS synthesis For example thats patient x age 37 In our scenario we focus on the multimodal dialogue interactions which directly relevant to active learning part of the HMD and eye-tracker scenario 1 The user gazes at the microphone button and starts the ASR we use dwell time for selection 500ms the Midas touch problem is circumvented by the size and positioning of the microphone 2 The user says learn a new person which issues a respective command in the multimodal interface and the eye-tracker connection 3 Upon face recognition FADE gets informed about a face and remembers the database instance which is stored in the service backend 4 The user looks again on the microphone and starts the ASR 5 The user says This is a new patient Peter Meier which the FADE module fuses into a face image database command now containing the face classi“cation features and the name of the newly created patient database instance IV E YE T RACKER AND HMD D ISPLAY Over several decades researchers investigated a lot in the area of eye tracking and gaze-based interfaces As a result of recent progress of this research area a light-weight and 
new 
152 
152 


Mobile Eye Tracker Mobile HMD Patient: Gerda Meier Age: 41 Last examination Pa Pa t A A Ag Ag Ag g A A A A A A A A L L L L L a a a Gaze Cursor 
A Display Calibration a Gaze-based Calibration b Scene-based Calibration in the HMD view 
Fig 2 HCI Architecture and Online Learning Framework The red boxes indicate the active dialogue modules during the HMD interaction The resulting augmentation in the see-through HMD contains patient name age and last examination information background scene photo courtesy of Siemens AG The gaze cursor is not visible in the HMD compact mobile eye-tracker is available today it enables us to use gaze as an interface in various scenarios 2 In our multimodal dialogue system we use the SMI Eye Tracking Glasses ETG 1 in order to recognise which person the doctor is looking at in the examination room and to obtain the gaze position in the HMD ETG is a binocular eye tracker which captures the images of both eyes and computes the gaze position in a scene image which in turn is captured by the scene camera located in the center of the glasses In order to obtain accurate gaze positions the user is required to do a system calibration before using it The calibration is done by looking at one or three point\(s indicated by the system Brother recently released a product of new head mounted display whose feature is the transparency of the display The user can see the environment through the display when nothing is displayed We combined this HMD with the ETG 
One of the most primitive and intuitive ways for calibrating HMDs is to use a gaze cursor By using user gaze as an input source for commands the user can control the system intuitively In order to display a gaze cursor on the HMD we need to calibrate the HMD so that the system can compute gaze position on the HMD Examples of scene image and the user view are shown in gure 3 From the user perspective the HMD can be seen as shown in the top of the image light blue rectangle but for the scene camera where the HMD is located it is unknown We propose two different methods for display calibration gaze based calibration and scene image based calibration as shown in gure 3 We implemented both of them for a comparison From the SMI eye tracker we then receive a scene image 1280x960 and a gaze position coordinate of the scene image e.g x:220,y:341 The gaze-based calibration can be seen on the the lower right image of gure 3 A red point appears in the HMD when the calibration process starts The user has to stare at the point which means he has to look at it for a few seconds Then the system computes the mean position of the gaze samples received from the eye tracker By repeating this process four times with different point locations we get four corresponding points between eye tracker and the HMD These corresponding points supply us with the calibration parameters in the mathematical formulas of perspective transformation The drawback of this method is that the performance dependents fully on the accuracy of the eye tracker If you get noisy gaze data you will most likely get wrong transformation parameters In scene-based calibration the whole scene image captured by the camera is shown in the HMD gure 3 lower left The user has to click on the position of each corner of the HMD In this way the calibration system knows where the HMD is located in the scene image We use a mouse for the calibration interaction Once you get the location of the HMD in the scene image you can easily transport the gaze position from the eyetracker to the HMD This method seems to be more promising 1 http://eyetracking-glasses.com 
153 
153 


Fig 3 Display Calibration because you dont have to rely on the performance of the eye tracker Furthermore you may also overlay an image in the augmented reality vision We used the second method in the application system and we think this is a better option from the performance point-ofview However the process of calibration might be complicated for users in a working environment Therefore we still need to evaluate the different methods from a usability and ease-of-use perspective After calibration we get the gaze position in the HMD so we can trigger the microphone event when the user gazes at the microphone button which appears in the left top of the HMD V F ACE R ECOGNITION In this system we combine a face recognition framework with the eye-tracking system in order to recognise the patient being examined i.e to provide the doctor with the imagecontent-based external brain The face recognition procedure illustrated in gure 4 follows the following procedure 1 The scene image and the gaze position is obtained from the eye tracker Then 2 Find the face closest to the gaze position The detection of the faces in the video image\(s is done via Haar-like features and an AdaBoost learning cascade see for further details we used the OpenCV2.3 implementation in combination with a raster scan method 3 Compute the Local Binary Pattern LBP features from the face images 4 Execute a nearest neighbor search and nd the nearest face from the database to improve the speed of search we may later use an approximate nearest neighbour search like once distincti v e features are identi“ed on a larger population 5 Get a result and send it from the multimodal input interface to the REAPR display context which triggers the presentation of real-time results in the HMD It is to be noted that in video streams the system sometimes returns false results for example if we have 85 accuracy 15 frames out of 100 frames are expected to be false results In order to remove this kind of noisy results and to obtain more accurate results we also applied an attention detection approach This simple heuristic counts the number of frames that have the same recognition results and if the number reaches the threshold the system assumes that the user is really looking at an object or person report that this approach effectively detects in combination with a 3D object recognition framework the object that drew the users attention In our tests we observe this also holds for the face recognition framework in a mobile non-stationary environment of the head-mounted camera 
154 
154 


1 Methods and Materials 
Fig 4 Face Recognition VI P RELIMINARY E VALUATION OF THE L EARNING S YSTEM We conducted a preliminary test for online face recognition Thereby we focussed on the general applicability of the technical methods for a mobile head-mounted active learning environment In this preliminary test test users wore the eyetracker and we recorded two video 223les with eye-tracking data from eight different person\220s faces in the same condition in the same room and with the same lighting conditions similar to the examination room in 223gure 1 The 223rst video is used to extract training face images 5 face images from each person and the second is used for testing We then checked in how far the faces from the test video can be recognised while using our method As a result we achieved 100 precision and 68 recall on average This result indicates that if the training is done in the same condition as the use case this online face recognition method performs well We found out why the recall rate is lower when the patient\220s face is too dark the face classi\223cation cannot be performed successfully However in our examination scenario we can provide for suitable conditions for face recognition good lighting to reveal many individual face textures thus this problem could be solved in a real examination environment 
We recorded two videos for each person 8 persons One video was used for trainning where 5 face imgages were extracted Then we checked how accurately the faces from the other video test video were recognised Both videos were recorded in the same indoor environment The study was conducted in our research lab We used two standard laptop computers ThinkPads X201 to connect to the eye tracker and the HMD to classify the incoming data streams and to synchronise the behaviour with the speech-based dialogue system The multimodal dialogue system except for the multimodal interface runs on a third ThinkPads X201 laptop computer Standard Java network packages take care of the data exchange between the headmounted I/O devices and the dialogue system VII C ONCLUSION We combined multiple on-body input and output devices namely a speech-based dialogue system a head-mounted augmented reality display and a head-mounted eye-tracker and implemented a complete IE scenario in a speci\223c medical application context which shows its potential The choice of the HMD device and the eye-tracker is very important because of the calibration need One has to consider the precision of the mapping of the HMD result to the screen as well as the handling of the eye-tracker gaze position The resulting interaction should however be very natural to the user like looking on thinks and imagining their relevance to the working context Using gaze and speech input has the potential to make daily routine a bit more effective and yield higher performance outcomes on similar knowledge intensive tasks 
155 
155 


 12 2006 2037…2041  Bonino D Castellina E Corno F  Gale A Garbo A Purdy K and Shi F A blueprint for integrated eye-controlled environments  4 2009 311…321  Ha vukumpu J V  ah  akangas P Gr  onroos E and H  akkinen J Midwives experiences of using hmd in ultrasound scan In  A I Mørch K Morgan T Bratteteig G Ghosh and D Svanaes Eds ACM 2006 369…372  Indyk P  and Motw ani R Approximate nearest neighbors Towards removing the curse of dimensionality In Dallas Texas USA 1998 604…613  K eller  K State A and Fuchs H Head mounted displays for medical use  4 Dec 2008 468…472  Larsson S and T raum D R Information state and dialogue management in the TRINDI dialogue move engine toolkit  3-4 2000 323…340  Lee E A Cyber physical systems Design challenges Tech Rep UCB/EECS-2008-8 EECS Department University of California Berkeley Jan 2008  Pieraccini R and Huerta J Where do we go from here Research and commercial spoken dialog systems In September 2005 1…10  Praso v  Z and Chai J Y  What s in a g aze the role of eye-gaze in reference resolution in multimodal conversational interfaces In  IUI 08 ACM New York NY USA 2008 20…29  Sellen A J and Harper  R H  MIT Press Cambridge MA USA 2003  Signer  B and Norrie M C P aperPoint a paper based presentation and interactive paper prototyping tool In  ACM New York NY USA 2007 57…64  Sonntag D  AKA and IOS Press Heidelberg 2010  Sonntag D Huber  M M  oller M Ndiaye A Zillner S and Cavallaro A  NOVA Publishers New York 2010 ch Design and Implementation of a Semantic Dialogue System for Radiologists  Sonntag D Liwicki M and W eber  M Interacti v e paper for radiology ndings In  IUI 11 ACM New York NY USA 2011 459…460  Sonntag D Reithinger  N Herzog G and Beck er  T   Springer LNAI 2010 ch A Discourse and Dialogue Infrastructure for Industrial Dissemination 132…143  T o yama T  Kieninger  T  Shaf ait F  and Dengel A Gaze guided object recognition using a head-mounted eye tracker In  ETRA 12 ACM New York NY USA 2012 91…98  T raub J and Sielhorst T  Adv anced display and visualization concepts for image guided surgery 2008  V iola P  A and Jones M J Rapid object detection using a boosted cascade of simple features In 2001 511…518  Zhang Q Imamiya A Go K and Mao X Overriding errors in a speech and gaze multimodal architecture In  IUI 04 ACM New York NY USA 2004 346…348 
IEEE Trans Pattern Anal Mach Intell 28 Universal Access in the Information Society 8 NordiCHI Proceedings of the Thirtieth Annual ACM Symposium on the Theory of Computing J Display Technol 4 Nat Lang Eng 6 Proceedings of the 6th SIGDdial Workshop on Discourse and Dialogue Proceedings of the 13th international conference on Intelligent user interfaces The Myth of the Paperless Of“ce TEI 07 Proceedings of the 1st international conference on Tangible and embedded interaction Ontologies and Adaptivity in Dialogue for Question Answering Semantic Web Standards Tools and Ontologies Proceedings of the 16th international conference on Intelligent user interfaces Proceedings of IWSDS2010„Spoken Dialogue Systems for Ambient Environment Proceedings of the Symposium on Eye Tracking Research and Applications Display Technology    CVPR 1 Proceedings of the 9th international conference on Intelligent user interfaces 
Fig 5 User with head-mounted HMD  eye-tracker combination Currently we use a simple nearest neighbour search method but this can be extended to an approximate nearest neighbour method such as in order to e xpand the size of the test database to become productive By using this kind of approximate method we assume that we can cover more than 100 person faces which would in combination with the clinical records that can be retrieved from the QA database become an external brain in terms of an on-body intelligent environment gure 5 The recognition model is locally dependent on the peoples faces and medical objects present in the environment The generalised domains are cognitively loaded working domains VIII A CKNOWLEDGEMENTS This research has been supported in part by the THESEUS Program in the Radspeech Project which is funded by the German Federal Ministry of Economics and Technology under the grant number 01MQ07016 It has also been supported by the EIT ICT Labs and ERmed http://www.dfki.de/RadSpeech/ERmed.html R EFERENCES  Ahonen T  Hadid A and Pietik  ainen M Face description with local binary patterns Application to face recognition 
156 
156 


map reduce 
A m n B l C A i A A A A A B,l j  A A  A   A A A  A   A A B l j  A l c l B l b l l l/c b   c l k l k k A l k k l k k 
Algorithm 4 Input Output foreach foreach in foreach in 
1 2 3 4 5 7 emit 8 9 For all values 10 11 13 emit 
1 2  c  1 2  c  020 
Distributed CSS on MapReduce Matrix of size  Concise representation  Number of columns Selected columns GeneralizedCSS  6 GeneralizedCSS     12 from each mapper are selected using the generalized CSS algorithm A second phase of selection is run over the 011 where is the number of input blocks columns to nd the best columns to represent  Different ways can be used to set for each input block  In the context of this paper the set of is assigned uniform values for all blocks i.e  Other methods are to be considered in future extensions Algorithm 4 sketches the MapReduce implementation of the distributed CSS algorithm It should be emphasized that the proposed MapReduce algorithm requires only two passes over the data set and its moves a very few amount of the data across the network VII R ELATED W ORK Different approaches have been proposed for selecting a subset of representative columns from a data matrix This section focuses on brie”y describing these approaches and their applicability to massively distributed data matrices The Column Subset Selection CSS methods can be generally categorized into randomized deterministic and hybrid The randomized methods sample a subset of columns from the original matrix using carefully chosen sampling probabilities Frieze et al w as the rst to suggest the idea of randomly sampling columns from a matrix and using these columns to calculate a rankapproximation of the matrix where  That work of Frieze et al was followed by different papers 11 that enhanced the algorithm by proposing different sampling probabilities Drineas et al proposed a subspace sampling method which samples columns using probabilities proportional to the norms of the rows of the top right singular vectors of  Deshpande et al proposed an adapti v e sampling method which updates the sampling probabilities based on the columns selected so far Column subset selection with uniform sampling can be easily implemented on MapReduce For non-uniform sampling the ef“ciency of implementing the selection on MapReduce is determined by how easy are the calculations of the sampling probabilities The calculations of probabilities that depend on calculating the leading singular values and vectors are time-consuming on MapReduce On the other hand adaptive sampling methods are computationally very complex as they depend on calculating the residual of the whole data matrix after each iteration The second category of methods employs a deterministic algorithm for selecting columns such that some criterion function is minimized This criterion function usually quanti“es the reconstruction error of the data matrix based on the subset of selected columns The deterministic methods are slower but more accurate than the randomized ones In the area of numerical linear algebra the column pivoting method exploited by the QR decomposition permutes the columns of the matrix based on their norms to enhance the numerical stability of the QR decomposition algorithm The rst columns of the permuted matrix can be directly selected as representative columns Besides methods based on QR decomposition different recent methods have been proposed for directly selecting a subset of columns from the data matrix Boutsidis et al proposed a deterministic column subset selection method which rst groups columns into clusters and then selects a subset of columns from each cluster C ivril and Magdon-Ismail presented a deterministic algorithm which greedily selects columns from the data matrix that best represent the right leading singular values of the matrix Recently Boutsidis et al presented a column subset selection algorithm which rst calculates the topright singular values of the data matrix where is the target rank and then uses deterministic sparsi“cation methods to select columns from the data matrix Besides other deterministic algorithms have been proposed for selecting columns based on the volume de“ned by them and the origin 25 The deterministic algorithms are more complex to implement on MapReduce For instance it is time-consuming to calculate the leading singular values and vectors of a massively distributed matrix or to cluster their columns using means It is also computationally complex to calculate QR decomposition with pivoting Moreover the recently proposed algorithms for volume sampling are more complex than other CSS algorithms as well as the one presented in this paper and they are infeasible for large data sets A third category of CSS techniques is the hybrid methods which combine the bene“ts of both the randomized and deterministic methods In these methods a large subset of columns is randomly sampled from the columns of the data matrix and then a deterministic step is employed to reduce 
S S S S S S 
               1   2       0 1   2       0 0  1         
      0          017        0    1 2 
b i b b i b b b j c c j c b b b b b 
 012 013 S S 012 013   S S 012 013 016 017\015 002 020 020 
177 


RCV1-200K TinyImages-1M RCV1-200K TinyImages-1M key-value 
O l l l l l A A A A A A A A  A l A l A l l l l 
columns based on probabilities calculated using the leading right singular vectors and then employs a deterministic algorithm to select exactly columns from the columns sampled in the rst stage However the algorithm depends on calculating the leading right singular vectors which is time-consuming for large data sets The hybrid algorithms for CSS can be easily implemented on MapReduce if the randomized selection step is MapReduce-ef“cient and the deterministic selection step can be implemented on a single machine This is usually true if the number of columns selected by the randomized step is relatively small In comparison to other CSS methods the algorithm proposed in this paper is designed to be MapReduce-ef“cient In the distributed selection step representative columns are selected based on a common representation The common representation proposed in this work is based on random projection This is more ef“cient than the work of C ivril and Magdon-Ismail which selects columns based on the leading singular vectors In comparison to other deterministic methods the proposed algorithm is speci“cally designed to be parallelized which makes it applicable to big data matrices whose columns are massively distributed On the other hand the two-step of distributed then centralized selection is similar to that of the hybrid CSS methods The proposed algorithm however employs a deterministic algorithm at the distributed selection phase which is more accurate than the randomized selection employed by hybrid methods in the rst phase VIII E XPERIMENTS Experiments have been conducted on two big data sets to evaluate the ef“ciency and effectiveness of the proposed distributed CSS algorithm on MapReduce The properties of the data sets are described in Table I The is a subset of the RCV1 data set which has been prepared and used by Chen et al to e v aluate parallel spectral clustering algorithms The data set contains 1 million images that were sampled from the 80 million tiny images data set and con v erted to grayscale Similar to previous work on CSS the different methods are evaluated according to their ability to minimize the reconstruction error of the data matrix based on the subset of selected columns In order to quantify the reconstruction error across different data sets a relative accuracy measure is de“ned as Relative Accuracy where is the rankapproximation of the data matrix based on a random subset of columns is the rankapproximation of the data matrix based on the subset of columns and is the best rankapproximation of the data matrix calculated using the Singular Value Decomposition SVD This measure compares different methods relative to the uniform sampling as a baseline with higher values indicating better performance The experiments were conducted on Amazon EC2 2 clusters which consist of 10 instances for the data set and 20 instances for the data set Each instance has a 7.5 GB of memory and a two-cores processor All instances are running Debian 6.0.5 and Hadoop version 1.0.3 The data sets were converted into a binary format in the form of a sequence of pairs Each pair consisted of a column index as the key and a vector of the column entries That is the standard format used in Mahout 3 for storing distributed matrices The distributed CSS method has been compared with different state-of-the-art methods It should be noted that most of these methods were not designed with the goal of applying them to massively-distributed data and hence their implementation on MapReduce is not straightforward However the designed experiments used the best practices for implementing the different steps of these methods on MapReduce to the best of the authors knowledge In speci“c the following distributed CSS algorithms were compared  is uniform sampling of columns without replacement This is usually the worst performing method in terms on approximation error and it will be used as a baseline to evaluate methods across different data sets  and  are different distributed variants of the hybrid CSS algorithm which can be implemented ef“ciently on MapReduce In the randomized phase the three methods use probabilities calculated based on uniform sampling column norms and the norms of the leading singular vectors rows respectively The number of selected columns in the randomized phase is set to  In the deterministic phase the centralized greedy CSS is employed to select exactly columns from the randomly sampled columns  is an extension of the centralized algorithm for sparse approximation of Singular Value Decomposition SVD The distrib uted CSS algorithm presented in this paper Algorithm 4 is used 2 Amazon Elastic Compute Cloud EC2 http://aws.amazon.com/ec2 3 Mahout is an Apache project for implementing Machine Learning algorithms on Hadoop See http://mahout.apache.org 
Table I T HE PROPERTIES OF THE DATA SETS USED TO EVALUATE THE DISTRIBUTED CSS METHOD  RCV1-200K Documents 193,844 47,236 TinyImages-1M Images 1 million 1,024 the number of selected columns to the desired rank For instance Boutsidis et al proposed a tw o-stage hybrid CSS algorithm which rst samples 
003  003 003  003 003  003 003  003  U S 
Data set Type  Instances  Features 
UniNoRep HybirdUni HybirdCol HybirdSVD DistApproxSVD 
F F F l F l 
U S U U S    
 log   037 037 037 037 100 037 037 037 log   
178 


B U 
DistGreedyCSS 
 The use of the distributed CSS algorithm extends the original algorithm proposed by C ivril and Magdon-Ismail to work on distributed matrices In order to allow ef“cient implementation on MapReduce the number of leading singular vectors is set of   is the distributed column subset selection method described in Algorithm 4 For all experiments the dimension of the random projection matrix is set to  This makes the size of the concise representation the same as the DistApproxSVD method Two types of random matrices are used for random projection 1 a dense Gaussian random matrix rnd and 2 a sparse random sign matrix ssgn For the methods that require the calculations of Singular Value Decomposition SVD the Stochastic SVD SSVD algorithm is used to approximate the leading singular values and vectors of the data matrix The use of SSVD signi“cantly reduces the run time of the original SVDbased algorithms while achieving comparable accuracy In the conducted experiments the SSVD implementation of Mahout was used Table II shows the run times and relative accuracies for different CSS methods It can be observed from the table that for the data set the DistGreedyCSS methods with random Gaussian and sparse random sing matrices outperforms all other methods in terms of relative accuracies In addition the run times of both of them are relatively small compared to the DistApproxSVD method which achieves accuracies that are close to the DistGreedyCSS method Both the DistApproxSVD and DistGreedyCSS methods achieve very good approximation accuracies compared to randomized and hybrid methods It should also be noted that using a sparse random sign matrix for random projection takes much less time than a dense Gaussian matrix while achieving comparable approximation accuracies Based on this observation the sparse random matrix has been used with the data set For the data set although the DistApproxSVD achieves slightly higher approximation accuracies than DistGreedyCSS with sparse random sign matrix the DistGreedyCSS selects columns in almost one-third of the time The reason why the DistApproxSVD outperforms DistGreedyCSS for this data set is that its rank is relatively small less than 1024 This means that using the leading 100 singular values to represent the concise representation of the data matrix captures most of the information in the matrix and accordingly is more accurate than random projection The DistGreedyCSS however still selects a very good subset of columns in a relatively small time IX C ONCLUSION This paper proposes an accurate and ef“cient MapReduce algorithm for selecting a subset of columns from a massively distributed matrix The algorithm starts by learning a concise representation of the data matrix using random projection It then selects columns from each sub-matrix that best approximate this concise approximation A centralized selection step is then performed on the columns selected from different sub-matrices In order to facilitate the implementation of the proposed method a novel algorithm for greedy generalized CSS is proposed to perform the selection from different submatrices In addition the different steps of the algorithms are carefully designed to be MapReduce-ef“cient Experiments on big data sets demonstrate the effectiveness and ef“ciency of the proposed algorithm in comparison to other CSS methods when implemented on distributed data R EFERENCES  A K Jain and R C Dubes 
Table II T HE RUN TIMES AND RELATIVE ACCURACIES OF DIFFERENT CSS METHODS T HE BEST PERFORMING METHOD FOR EACH 0.6 0.6 0.5 0.00 0.00 0.00 0.8 0.8 2.9 2.37 1.28 4.49 1.6 1.5 3.7 4.54 0.81 6.60 1.3 1.4 3.6 9.00 12.10 18.43 16.6 16.7 18.8 41.50 57.19 63.10 5.8 6.2 7.9 61.92 67.75 2.2 2.9 5.1 40.30 1.3 1.3 1.3 0.00 0.00 0.00 1.5 1.7 8.3 19.99 6.85 6.50 3.3 3.4 9.4 17.28 3.57 7.80 52.4 52.5 59.4 3.59 8.57 10.82 71.0 70.8 75.2 22.1 23.6 24.2 67.58 25.18 20.74 to select columns that best approximate the leading singular vectors by setting 
Run time minutes Relative accuracy  RCV1 200K Uniform Baseline Hybird Uniform Hybird Column Norms Hybird SVD-based Distributed Approx SVD Distributed Greedy CSS rnd 51.76 Distributed Greedy CSS ssgn 62.41 67.91 Tiny Images 1M Uniform Baseline Hybird Uniform Hybird Column Norms Hybird SVD-based Distributed Approx SVD 70.02 31.05 24.49 Distributed Greedy CSS ssgn 
Algorithms for Clustering Data 
 002 100 100 
l l l l l l l 
10  100  500 10  100  500 
 Upper Saddle River NJ USA Prentice-Hall Inc 1988 
 
k k 
RCV1-200K TinyImages-1M TinyImages-1M 
IS HIGHLIGHTED IN BOLD  AND THE SECOND BEST METHOD IS UNDERLINED N EGATIVE MEASURES INDICATE METHODS THAT PERFORM WORSE THAN UNIFORM SAMPLING  Methods 
179 


 L Kaufman and P  Rousseeuw  Clustering by means of medoids Technische Hogeschool Delft Netherlands Department of Mathematics and Informatics Tech Rep 1987  S Deerwester  S Dumais G Furnas T  Landauer  and R Harshman Indexing by latent semantic analysis  vol 41 no 6 pp 391…407 1990  C Boutsidis J Sun and N Anerousis Clustered subset selection and its applications on it service metrics in  2008 pp 599…608  C Boutsidis M W  Mahone y  and P  Drineas  An impro v ed approximation algorithm for the column subset selection problem in  2009 pp 968…977  C Boutsidis P  Drineas and M Magdon-Ismail Near optimal column-based matrix reconstruction in  2011 pp 305 314  J Dean and S Ghema w at MapReduce Simpli“ed data processing on large clusters  vol 51 no 1 pp 107…113 2008  T  White  1st ed OReilly Media Inc 2009  A Frieze R Kannan and S V empala F ast Monte-Carlo algorithms for nding low-rank approximations in  1998 pp 370 378  P  Drineas A Frieze R Kannan S V empala and V  V inay  Clustering large graphs via the singular value decomposition  vol 56 no 1-3 pp 9…33 2004  P  Drineas R Kannan and M Mahone y  F ast Monte Carlo algorithms for matrices II Computing a low-rank approximation to a matrix  vol 36 no 1 pp 158…183 2007  P  Drineas M Mahone y  and S Muthukrishnan Subspace sampling and relative-error matrix approximation Column-based methods in  Springer Berlin  Heidelberg 2006 pp 316…326  A Deshpande L Rademacher  S V empala and G W ang Matrix approximation and projective clustering via volume sampling  vol 2 no 1 pp 225…247 2006  A C  i vril and M Magdon-Ismail Column subset selection via sparse approximation of SVD  vol 421 no 0 pp 1  14 2012  A K F arahat A Ghodsi and M S Kamel  An ef cient greedy method for unsupervised feature selection in  2011 pp 161 170   Ef cient greedy feature selection for unsupervised learning  vol 35 no 2 pp 285…310 2013  T  Elsayed J Lin and D W  Oard P airwise document similarity in large collections with MapReduce in  2008 pp 265…268  A Ene S Im and B Mosele y  F ast clustering using MapReduce in  2011 pp 681…689  H Karlof f S Suri and S V assilvitskii A model of computation for MapReduce in  2010 pp 938…948  S Dasgupta and A Gupta An elementary proof of a theorem of Johnson and Lindenstrauss  vol 22 no 1 pp 60…65 2003  D Achlioptas Database-friendly random projections Johnson-Lindenstrauss with binary coins  vol 66 no 4 pp 671…687 2003  P  Li T  J Hastie and K W  Church V ery sparse random projections in  2006 pp 287…296  G Golub and C V an Loan  3rd ed Johns Hopkins Univ Pr 1996  A Deshpande and L Rademacher  Ef cient v olume sampling for row/column subset selection in  2010 pp 329 338  V  Gurusw ami and A K Sinop Optimal column-based lo wrank matrix reconstruction in  2012 pp 1207…1214  D D Le wis Y  Y ang T  G Rose and F  Li Rcv1 A ne w benchmark collection for text categorization research  vol 5 pp 361…397 2004  W Y  Chen Y  Song H Bai C.-J Lin and E Chang Parallel spectral clustering in distributed systems  vol 33 no 3 pp 568 586 2011  A T orralba R Fer gus and W  Freeman 80 million tin y images A large data set for nonparametric object and scene recognition  vol 30 no 11 pp 1958…1970 2008  N Halk o P G Martinsson Y  Shk olnisk y  and M T ygert An algorithm for the principal component analysis of large data sets  vol 33 no 5 pp 2580…2594 2011 
Journal of the American Society for Information Science and Technology Proceedings of the Seventeenth ACM Conference on Information and Knowledge Management CIKM08 Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms SODA09 Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science FOCS11 Communications of the ACM Hadoop The De“nitive Guide Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science FOCS98 Machine Learning SIAM Journal on Computing Approximation Randomization and Combinatorial Optimization Algorithms and Techniques Theory of Computing Theoretical Computer Science Proceedings of the Eleventh IEEE International Conference on Data Mining ICDM11 Knowledge and Information Systems Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies Short Papers HLT08 Proceedings of the Seventeenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD11 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODA10 Random Structures and Algorithms Journal of computer and System Sciences Proceedings of the Twelfth ACM SIGKDD international conference on Knowledge Discovery and Data Mining KDD06 Matrix Computations Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science FOCS10 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODA12 The Journal of Machine Learning Research Pattern Analysis and Machine Intelligence IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE Transactions on SIAM Journal on Scienti“c Computing 
180 


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even goodŽ partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity … the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the clouds elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPs synchronous barrier between supersteps offers a window for dynamic scaleout and …in at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an oracleŽ approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workers time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440…442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


