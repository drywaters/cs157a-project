  
    
North Carolina State University Raleigh NC MathWorks Inc 
  
  
FChain Toward Black-box Online Fault Localization for Cloud Systems 
Distributed applications running inside cloud systems are prone to performance anomalies due to various reasons such as resource contentions software bugs and hardware failures One big challenge for diagnosing an abnormal distributed application is to pinpoint the faulty components In this paper we present a black-box online fault localization system called that can pinpoint faulty components 
Hiep Nguyen  Zhiming Shen  Yongmin Tan  Xiaohui Gu 
hcnguye3 zshen5 ncsu.edu gu@csc.ncsu.edu yongmin.tan@mathworks.com 
Abstract FChain 
after a performance anomaly is detected FChain rst discovers the onset time of abnormal behaviors at different components by distinguishing the change point from many change points caused by normal workload uctuations Faulty components are then pinpointed based on the patterns and relationships FChain performs runtime validation to further lter out false alarms We have implemented FChain on top of the Xen platform and tested it using several benchmark applications RUBiS Hadoop and IBM System S Our experimental results show that FChain can quickly pinpoint the faulty components with high accuracy within a few seconds FChain can achieve up to 90 higher precision and 20 higher recall than existing schemes FChain is non 
immediately abnormal abnormal change propagation inter-component dependency 
intrusive and light-weight whic h imposes less than 1 overhead to the cloud system 
I I NTRODUCTION Infrastructure as a Service IaaS clouds 2 a l l o w multiple tenants to share a commo n physical computing infrastructure in a cost-effective way However applications running inside the IaaS cloud are prone to performance anomalies such as service level objective SLO violations due to various reasons such as resource contentions software bugs or hardware failures It is particularly challenging to localize the faulty components in a complex distributed application that consists of many inter-dependent components Previous work on distributed system debugging can be categorized as white-box grey-box or black-box techniques White-box and grey-box techniques e.g   r equi re modiìcations or instrumentations to applications or underlying middleware platforms Those intrusive techniques often impose signiìcant runtime overhead and are difìcult to deploy 
fault localization in the production IaaS clouds Existing black-box techniques   mos t l y focus o n a nomal y d et ect i o n o r s ys tem metric attribution In contrast our goal is to pinpoint faulty components in a distributed application Other schemes  onl y w ork f or cert a i n t ypes o f f aul t s or appl i cat i ons  W e will discuss related work in detail in Section IV In this paper we present FChain a black-box online fault localization system for diagnosing performance anomalies in IaaS clouds FChain can pinpoint faulty components after a performance anomaly is detected FChain does 
online immediately 
which make them impractical for performing 002\003\004\005 006\007\010\011\012\013\007\010\014\015 
016\014\013\017\020\021\012\013\017\022\005\023\012 024\011\013\005\022\007\012\020\014\010\015\007\015 004\014\011\007\014\017\005\025\007\014\025\010\007\014\013\007\014\020\005 006\012\011\007\022\012\013\007\010\014 002\003\004\005\006\007\010\011\005\012\013\014\015 
026\007\014\025\010\007\014\013\017\022\005\023\012\024\011\013\027\005\030\010\031\025\010\014\017\014\013\015 032\033\014\010\021\031\012\011\005\030\034\012\014\020\017\005 025\010\007\014\013\005\015\017\011\017\030\013\007\010\014 035\010\014\007\013\010\021\007\014\020\005 031\017\013\021\007\030\015 036\010\021\031\012\011\005\023\011\024\030\013\024\012\013\007\010\014\005 031\010\022\017\011\007\014\020 037\010\015\013 
035\005 032\025\025\011\007\030\012\013\007\010\014 035\005 032\025\025\011\007\030\012\013\007\010\014 002\003\004\005\006\007\010\012\016\005\017\014 
003\007\014\024"\005!\005#\017\014 010\031\012\007\014\005 
Fig 1 The overall architecture of the FChain system The FChain slave continuously collects system-level metrics e.g CPU memory on different application VMs running inside the cloud and learns normal uctuation patterns When a SLO violation is detected the FChain master triggers all related FChain slaves to look for abnormal change points The FChain 
master then pinpoints faulty compone nts based on the change point timing information and inter-component dependencies FChain also performs online validation to remove possible false alarms not perform any intrusive tracing in the cloud system and only relies on low-level system metrics e.g CPU memory network statistics that can be easily obtained from the guest OS or hypervisor FChain does not assume any prior application knowledge which makes it practical for IaaS clouds Moreover FChain does not require any training data for anomalies which can diagnose both previously seen and unseen performance anomalies Figure 1 shows the overall architecture of the FChain system When a performance anomaly is detected FChain rst discovers abnormal changes at all system metrics of different components Next FChain extracts abnormal change propagation paths for the diagnosed performance anomaly and localizes the faulty components based on the abnormal change propagation patterns Our design is based on two 
key observations 1 performance anomalies often manifest as abnormal system metric uctuations that are distinctive from the normal uctuation patterns and 2 the abnormal system metric changes often start from the faulty components and then propagate to other non-faulty components via inter-component interactions To achieve robust fault localization FChain needs to address a challenging problem how to distinguish the abnormal change point that marks the onset of the fault manifestation from many other change points that are caused by normal workload uctuations It is insufìcient to use a xed ltering threshold since some applications are inherently more dynamic than others To address this problem FChain captures the normal uctuation patterns using online system metric value prediction models and u s e s a metric to iden 
predictability 
2013 IEEE 33rd International Conference on Distributed Computing Systems 1063-6927/13 $26.00 © 2013 IEEE DOI 10.1109/ICDCS.2013.26 198 
2013 IEEE 33rd International Conference on Distributed Computing Systems 1063-6927/13 $26.00 © 2013 IEEE DOI 10.1109/ICDCS.2013.26 21 
2013 IEEE 33rd International Conference on Distributed Computing Systems 1063-6927/13 $26.00 © 2013 IEEE DOI 10.1109/ICDCS.2013.26 21 


3 6 2 6 2 
t t W t W t 
v v  v v 
Fig 2 Abnormal change propagation in an IBM System S application The fault propagates along the path  The propagation is caused by the back-pressure effect 
012\033\014\010\021\031\012\011\005\030\034\012\014\020\017\005\025\021\010\025\012\020\012\013\007\010\014 013 013 013 026  026 026 026 026 026\\023\012\024\011\013\027\005\030\010\031\025\010\014\017\014\013 012\025\025\011\007\030\012\013\007\010\014\005\022\017\025\017\014\022\017\014\030\027\005\025\012\013\034 011\007\017\014\013 010\0210\011\010\012\022 020\017\014\017\021\012\013\010\021 1$\026 025\012\0300\017\013\015 
online A System Overview 
   100 
tify abnormal uctuations As we will show later in Section III this abnormal change detection scheme can achieve higher accuracy than traditional anomaly detection schemes 10  Due to inter-component interactions abnormal changes in the faulty component\(s often propagate to other normal components If we examine each component in an isolated way we might produce many false alarms by mistakenly pinpointing normal components as faulty ones Our previous work has s ho wn t h at i t i s feas i b l e t o pi npoi nt f a ul t y components by sorting all affected components based on their fault manifestation time and identifying the component with the earliest manifestation time as the faulty one However we observe that it is insufìcient to only rely on the chronological order to perform fault localization since we may derive spurious abnormal change propagati ons between two independent components To address the problem FChain integrates the dependency relationships with the fault propagation model to achieve more accurate pinpointin g than existing schemes This paper makes the following contributions We present FChain a practical fault localization system for large-scale IaaS clouds FChain does not require any intrusive application monitoring which can diagnose both previously seen and unseen anomalies We describe a predictability based abnormal change point selection scheme that can distinguish abnormal change points that are related to the fault manifestation from those normal change points that are caused by normal workload uctuations We introduce a new integrated fault localization scheme considering both fault propagation patterns and intercomponent dependencies to achieve higher pinpointing accuracy We have implemented FChain on top of the Xen platform and tested it on NCSUês Virtual Computing Lab VCL 2  a production cloud computing system that operates in a similar way as Amazon EC2 W e conduct e d e xt ens i v e e xperi ment s using a set of common faults and different types of applications IBM System S data stream processing system Hadoop a nd R U B i S onl i n e a uct i o n b enchmark 16 Our experimental results show that 1 FChain can achieve up to 90 higher precision and 20 higher recall than existing black-box fault localization s chemes 2 FChain can complete the fault localization within a few seconds and works for different types of applications and 3 FChain is light-weight imposing less than 1 CPU overhead during system runtime execution The rest of the paper is organized as follows Section II describes the design of the FChain system Section III presents our experimental evaluation Section IV compares our work with related work Finally the paper concludes in Section V II S YSTEM D ESIGN In this section we rst present an overview of the FChain system We then describe the abnormal change point identiìcation algorithm and the integrated faulty component pinpointing scheme FChain is decentralized consisting of a set of slave modules i.e normal uctuation m odeling abnormal change point selection and master modules i.e integrated fault diagnosis online pinpointing validation shown by Figure 1 The slave modules run inside the domain 0 of different cloud nodes while the master modules run on dedicated servers FChain treats each guest virtual machine VM as one component The normal uctuation modeling module continuously monitors the system metrics for each VM to capture its normal uctuation pattern We employ a light-weight online learning model  t o cont i nuous l y l earn t he e v ol vi ng pat t e rn of each system metric value If the change is caused by normal workload uctuations the prediction model must have seen and learned the change before Thus the prediction errors on those normal change points will be small In contrast the uctuations caused by faults ar e not captured by the online learning model which will probably incur high prediction errors When a performance anomaly  e.g SLO violations is detected 1  the FChain master is invoked to pinpoint the faulty components in the failing distributed application The FChain master rst contacts the slaves on all related distributed hosts to identify whether a component exhibits abnormal changes and when the abnormal change begins If the performance anomaly occurs at time  the FChain slaves check a window  e.g  of recent metric values before  The abnormal change point selection module uses the normal uctuation model to lter out those change points that are caused by normal workload uctuations The integrated fault diagnosis module comprehensively examines the abnormal change point information from all components and the inter-component dependency information to pinpoint the culprit component\(s FChain rst derives the abnormal change propagation pa ttern in the examined distributed application by sorting the timestamps of the abnormal 1 Note that FChain focuses on fau lt localization rather than performance anomaly detection that has been addressed by previous work\(e.g  18 
PE PE PE PE PE 
   
002 002 002 
199 
22 
22 


t t Q t Q t 
 
3 3 1 6 2 2 3 1 2 3 3  
Fig 3 Abnormal change point selection for the DiskWrite metric of a faulty map node and the CPU usage metric of a normal reduce node in a Hadoop sorting application change points of different components 2 For example Figure 2 shows the abnormal change propagation path in a sample IBM System S application consisting of seven distributed components called processing elements PEs The application consists of one faulty component 
PE PE t PE t PE t t t t PE x X x   x Q X k x 
with a memory leak bug The memory usage metric of shows the abnormal change at time  The anomaly rst propagates to the component at time and then to the component at time Since  FChain pinpoints as the suspicious root cause component Note that our scheme does not assume any knowledge about the application including its topology FChain leverages the black-box dependency discovery tool t o  l t e r out s puri ous abnormal c hange propagations between independent components Finally FChain performs online pinpointing validation using the dynamic resource scaling technique in a similar way as 20  S in ce FCh a in m o n ito rs v a rio u s sy stem m e trics it can not only pinpoint faulty components but also identify which system metrics are related to the fault We can then adjust those metrics on the faulty components to validate the accuracy of the pinpointing results by observing the resource adjustment impact to the applicationês SLO violation status To localize the faulty components FChain rst examines i exhibit abnormal changes in different system-level metrics and ii those abnormal changes start As mentioned before system-level metrics are inherently uctuating under dynamic workloads If we apply standard change point detection algorithms we often discover many change points For example Figure 3 shows the set of change points discovered on the Disk Write metric of a map task node and the CPU usage metric of a reduce task node in a Hadoop application using the common change point detection algorithm CUSUM  Bootstrap W e can s e e m an y change points are just random peak and bottom values which are not actually related to the fault We can use smoothing 2 Since FChain relies on timing information to infer abnormal change propagations we synchronize the clock s of all hosts using the network time protocol NTP which has an error of less than 0.1 ms in LANs and less than 5 ms on the Internet  I n our e xperim e nts we observ e that all of the anomaly propagation delays between two dependent components are at least several seconds Therefore our system can tolerate small time skews i.e tens of milliseconds Fig 4 Expected prediction error for the CPU usage metric on a dual-core host and change magnitude outlier detection to lter some normal change points  H o w e v er  i t i s i ns uf  c i e nt t o onl y r el y on smoothing and outlier detec tion to select abnormal change points since some metrics e.g the Disk Write metric in Figure 3 have large variations during normal execution FChain uses a predictability metric to achieve robust abnormal change point selection We employ an online learning model  to continuous ly learn t he change pattern of each system metric The online learning model can capture the transition probability between different metric values using a discrete time Markov chain model Intuitively the value transition patterns at a normal change point should be able to be captured by the online learning model and thus easier to predict We calculate a prediction error for each outlier change point by comparing the predicted value with its true value If the prediction error is high we consider this outlier change point as an abnormal change point However it is a non-trivial problem to pick a proper prediction error threshold for ltering normal change points since some metrics e.g bursty ones are inherently harder to predict than others and vary from application to application Thus it will be imprecise to apply a xed ltering threshold To address this problem FChain dynamically computes a proper prediction error thres hold for each change point based on the burstiness of the time series surrounding the change point The intuition behind our scheme is that a bursty metric is expected to have a higher prediction error than a nonbursty metric Thus we want to use a higher prediction error threshold when the metric values are bursty Speciìcally we extract a small window of time series data surrounding the change point  e.g seconds and apply the fast Fourier transform FFT algorithm on to determine the coefìcients that represent the amplitude of each frequency component We consider the top e.g 90 frequencies in the frequency spectrum as high frequencies We apply inverse FFT over the high frequency components to synthesize the burst signal We then use the burst signal magnitude e.g 90th percentile of the burst value as the expected prediction error for the change point  If the real prediction error exceeds the e xpected prediction error the change point is selected as one abnormal change point For example Figure 4 illustrates the expected prediction errors for a system metric time series We can see that the expected prediction error is higher when the original time series are 
B Abnormal Change Point Selection which components when 
 20 
200 
23 
23 


C Integrated Faulty Component Pinpointing back-pressure 
1 2 1 2 1 3 
002 
 we roll back to the preceding change point We repeat the roll-back process un til the tangents of two adjacent change points are distinct If a component has multiple metrics exhibiting abnormal changes we pick the earliest abnormal change start time as the componentês abnormal change start time Our faulty component pinpointing algorithm consists of three steps 1 deriving the abnormal change propagation paths in the examined distributed application by sorting the start time of abnormal changes at different components 2 pinpointing faulty components based on the selected abnormal change propagation paths and 3 reìning pinpointing results by ltering out spurious abnormal change propagation paths using inter-component dependency information For example Figure 5 shows the pinpointing process for the RUBiS online auction benchmark application  If the abnormal change onset time of is earlier than that of  we say that the abnormal change propagates from the component to the component  For example in Figure 5 the application server 1 starts to exhibit abnormal change at time seconds and the database server starts to show abnormal change at time  Thus we can infer that the abnormal change starts at the application server 1 and propagates to the database server To pinpoint faulty components FChain rst sorts all the components into a chain based on their fault manifestation time We rst pinpoint the source component in this chain as the faulty component since it has the earliest fault manifestation We continue to examine the other components in the chain following the time order If the fault manifestation time of the next component is close to the pinpointed component e.g time difference seconds we infer that the componentês abnormal behavior is probably not caused by the anomaly propagation but a concurrent fault that occurs at a similar time Thus we will pinpoint this component as one faulty component as well If all the application components contain fault manifestations and the changes at all the components follow the same upward or downward trend FChain infers that the performance anomaly is probably caused by some external factors such as workload increases i.e upward trend or a NFS server problem i.e downward trend In this case FChain will not pinpoint any component within the application as faulty Although previous work has a l s o a ddres s e d t he probl em of distinguishing workload changes from anomalies within a single component our work provides workload change detection for distributed applications We may derive spurious abnormal change propagations between independent components For example in Figure 5 we will derive an abnormal change propagation path from the application server 1 to the application server 2 based on the timestamps of their abnormal change points However this propagation actually does not exist We propose to use inter-component dependency relationships to lter out spurious propagation paths For each suspicious component that contains the abnormal change point we examine whether there is a path in the dependency graph from any pinpointed faulty components to this component If no path in the dependency graph can be found we pinpoint this component as a faulty one since the anomaly propagation is unlikely and the componentês anomalous behavior must have been caused by an independent fault We leverage previous black-box dependency discovery tools  t o di s c o v e r i nt er component dependenci e s  3 However we cannot solely rely on the dependency information for fault localization since the abnormal change propagation does not always follow the dependency path For example in RUBiS the faulty application server can cause its upstream component the web server to exhibit abnormal behavior due to a effect that is a faulty component might cause its upstream component to show anomalous behavior 4 If we only rely on the dependency information we will pinpoint the normal web server as the faulty component and miss the true culprit component that is the application server Our experimental results in Section III will conìrm this observation We also found that existing network trace based depen3 To achieve high accuracy the black-box dependency scheme needs to accumulate sufìcient amount of network tr ace data L uckily  t he application dependency information rarely change during application runtime We perform the dependency discovery ofîine and store the results in a le for later reference 4 The cause of the back-pressure varies among different applications One common reason is that after the input buffer of the faulty component becomes full it forces the upstream compone nt to drop data or pause processing 
  C C C C t t 
0 1  200  210 2 
002 003 004\005\006\007\007\005\010  010\011\012\013\014\015\012\010\005\016\017\020\015\013\021\016\022\005\023\024\016\020\025\026\005\011\013\015\011\016\025\016\002\014\015\020 002 006 004\005\006\007\027\005\010 016\011\011\022\014\023\016\002\014\015\020\005\030\026\011\026\020\030\026\020\023\031\005\011\016\002\024 032\016\012\022\002\031\005\023\015\021\011\015\020\026\020\002 033\011\011 010\026\013\034\026\013\005\003 033\011\011 010\026\013\034\026\013\005\006 035\026\017 010\026\013\034\026\013 036\037 010\026\013\034\026\013 022\014\026\020\002 015\013"\022\015\016\030 025\026\020\026\013\016\002\015\013 002\002\011 013\026$\012\026\010\002\010 002 006 004\005\006\003\007\005\010 002\013\012\026\005\016\017\020\015\013\021\016\022\005\023\024\016\020\025\026\005\011\013\015\011\016\025\016\002\014\015\020  
Fig 5 Faulty component pinpointing for the RUBiS online auction benchmark application bursty and is lower when the time series become stable In Figure 3 our scheme correctly lters out the outlier change point on the normal reduce node and only selects the abnormal change point on the faulty map node After identifying a component exhibiting any abnormal change we need to know when the abnormal change starts During our experiments we found that the selected abnormal change point sometimes resides in the middle of the fault manifestation process instead of at the beginning depending on the evolving pattern of the fault manifestation e.g gradual change or bursty change Thus FChain performs tangentbased rollback to identify the precise start time of the abnormal change Speciìcally starting from the abnormal change point we compare the tangent of the current change point with that of its preceding change point If their values are close e.g 
201 
24 
24 


dency discovery scheme fails to discover any dependency information in the data stream processing system T he reason is that the dependency discovery algorithm relies on the gap between network packets to separate network ows However the stream application processes continuous data packets which do not contain gaps between network packets Note that FChain can still pinpoint faulty components based on the abnormal change propagation paths when the dependency information is unavailable In contrast the dependency-only scheme will fail the fault localization task for distributed stream processing systems III E XPERIMENTAL E VA L UAT I O N We have implemented the FChain system on top of the Xen platform  a nd conduct e d e xt ens i v e e xperi ment s u s ing the RUBiS multi-tier online auction benchmark EJB version t he IB M S ys t e m S dat a s t ream proces s i ng system a nd t h e H adoop MapR e duce frame w o rk 15 In this section we rst describe our evaluation methodology followed by the experiment results Our experiments were conducted on the NCSUês Virtual Computing Lab VCL a production cloud computing infrastructure that operates in a similar ways as Amazon EC2 1 All the VCL hosts used in our experiments have a dual-core Xeon 3.00GHz CPU 4GB memory and 30GB disk which are connected to Gigabit networks Each host runs 64 bit CentOS 5.2 with Xen 3.0.3 The guest VMs also run 64 bit CentOS 5.2 FChain monitors each guest VM from Domain 0 using the and libraries Monitored metrics are cpu usage memory usage network in network out disk read and disk write The metric sampling interval is 1 second To evaluate FChain in multi-tenant cloud computing environments we run three benchmark systems concurrently on the same set of VCL hosts We rst describe the benchmark systems used in our experiments as follows We use the three-tier online auction benchmark system RUBiS EJB version The topology of the RUBiS system is shown in Figure 5 We run each application component in one guest VM In order to evaluate our system under workloads with realistic time variations we use a client workload generator that emulates the workload intensity observed in the NASA web server trace beginning at 00:00:00 July 1 1995 from the IRCache Internet trafìc archive t o modul at e t he reques t rat e of our RUBiS benchmark The client workload generator also tracks the response time of the HTTP requests it made An SLO violation is marked if the average request response time is larger than ms We run Hadoop sorting application one of the sample applications provided by the Hadoop distribution The application consists of three map nodes and six reduce nodes The data size we process is 12GB which is generated using the RandomWriter application We measure the progress score of the job by calling the Hadoop API An SLO violation is marked when the job does not make any progress for more than 30 seconds We use the commercial high-performance data stream processing system S ys t e m S  d e v el oped b y IBM In our experiments we used a tax calculation application one of the sample applications provided by System S product distribution The topology of the System S application is shown in Figure 2 Each PE runs in a separate guest VM In order to evaluate our system under workloads with realistic time variations we used the workload intensity observed in the ClarkNet web server trace beginning at 1995-08-28:00.00 from the IRCache Internet trafìc archive t o modul at e t he data arrival rate We measured the average per-tuple processing time and an SLO violation is marked if the average processing time is larger than a pre-deìned threshold e.g 20ms We inject different f aults e.g common software bugs bottleneck during an application runtime Each application run lasts one hour We inject one fault at a random time instant to test FChain under different workload conditions For each fault we use 30 to 40 application runs We test both single-component faults and multi-component concurrent faults For RUBiS single-component faults include 1  we start the program that has a memory leak bug in the VM running the database server 2  a CPU-bound program competed CPU with the database server inside the same VM and 3  we use httperf  t ool t o send a large number of HTTP requests to the web server Multi-component concurrent faults include 1  the application server 1 wants to ofîoad some EJBs to the application server 2 However the program bug JIRA JBAS1442 in the application server 1 makes the remote server lookup return the local server binding by mistake and 2  a load balancing bug mod jk 1.2.30 causes the web server to dispatch requests unevenly These two faults are real software bugs found in the JBoss and Apache load balancer software For Hadoop we injected concurrent faults in all the map nodes 1  we injected a memory leak bug into all the map tasks which allocated memory from the heap without releasing 2  we injected an inìnite loop bug in all the map tasks and 3  we start a disk I/O intensive program in the Domain 0 of each host running the map tasks For System S we inject the following single-component faults 1  We inject a small snippet of code that has a memory leak bug into a randomly selected PE 2  a CPU-bound program competes CPU with a PE within the same VM 3  we make one randomly selected PE the bottleneck by setting a low CPU cap over the PE The multi-component concurrent faults include 1  we start the memory leak program simultaneously in two randomly selected PEs and 2  we start the CPU intensive program simultaneously in two randomly selected PEs We compare FChain with a set of existing black-box fault 
A Evaluation Methodology MemLeak CpuHog NetHog OfîoadBug LBBug Concurrent MemLeak Concurrent CpuHog Concurrent DiskHog MemLeak CpuHog Bottleneck Concurrent MemLeak Concurrent CpuHog 
libxenstat libvirt 
RUBiS online auction benchmark Hadoop IBM System S Fault injection 
100 
202 
25 
25 


2 1 1 
1 Histogram 2 NetMedic 3 Topology 4 Dependency 5 PAL 6 Fixed-Filtering 
002 002 C C C N N N P recision N N N  Recall N N N W Q 
localization schemes  This scheme computes an anomaly score for each system-level metric us ing Kullback-Leibler divergence  between the h is togram of the m os t r ecent d ata contained in the same look-back window as FChain and the histogram of the whole data It then pinpoints abnormal components based on the anomaly scores We vary the anomaly score threshold to show the tradeoff between the true positive rate and the false positive rate This scheme has been used by previous work for detecting anomalies e.g  It is a r ecently de v e loped a pplicationagnostic multi-metric fault localization tool The abnormal component pinpointing is based on the application topology and the inter-component impact learned from the historical data This scheme needs to assume the knowledge of the application topology For estimating the inter-component impact we use the same 1800 seconds of recent data as speciìed in  Dif f erent from F C h ain NetMedic jus t gi v e s a rank ed lis t of all components based on their likelihood of being the faulty components We rst pinpoint the top impact component as the faulty component We also pinpoint the following components whose impact difference with the top ranked component is less than a certain threshold  We adjust the value of to show the tradeoff between the true positive rate and the false positive rate that can be achieved by NetMedic  This scheme assumes the knowledge of the application topology It rst detects abnormal components using the outlier change point detection algorithm developed in our previous work PAL  I t t hen p i npoi nt s f aul t y components based on the topology information that is if the abnormal component depends on the abnormal component  we pinpoint as the faulty component By comparing FChain with this scheme we want to show that it is insufìcient to just consider the application topology for pinpointing faulty components  Instead of assuming the application topology knowledge this scheme uses the black-box dependency discovery tool t o dynami cal l y e x t r act t h e i nt er component dependency information It rst detects abnormal components using the same outlier change point detection algorithm as the Topology scheme It then pinpoints faulty components based on the discovered dependency i nformation If no dependency information is discovered this scheme will output all the components that have outlier change points as faulty components By comparing FChain with this scheme we want to show that it is insufìcient to just rely on the dependency information for pinpointing faulty components 13  T h i s i s t h e in itial v ersio n o f o u r ch an g e p r o p a gation based fault localization system However different from FChain PAL does not perform pr edictability-based abnormal change point selection or consider the dependency information in fault localization It also does not support online validation  This scheme uses the same pinpointing algorithm as FChain except that it employs a xed prediction error ltering threshold to select the abnormal change points We varied the ltering threshold to show different accuracy reFig 6 Fault localization accuracy comp arison for the single-component faults for RUBiS Fig 7 Fault localization accuracy comp arison for the single-component faults for System S sults that can be achieved by thi s scheme We compare FChain with this xed ltering scheme to show the effectiveness of our burst based ltering scheme To quantify the accuracy of diff erent fault localization schemes we use the standard precision and recall metrics Let  and denote the number of true positives correctly pinpoint a faulty component false negatives miss a faulty component and false positives pinpoint a normal component as faulty respectively We calculate the precision and recall metrics in the standard way as follows 1 We evaluate the accuracy of diff erent pinpointing algorithms using the commonly used receiver operating characteristic ROC curve whose X-axis and Y-axis show the recall and precision respectively A perfect pinpointing scheme should achieve 100 precision and 100 recall In our experiments we conìgure the FChain system as follows We set the look-back window   to be 100 seconds for all the tested faults except the DiskHog fault in the Hadoop application The reason is that the DiskHog fault takes much longer time to manifest than the other faults The 100 seconds look-back window will not cover the initial stage of the fault manifestation Thus we used a longer lookback window 500 seconds for the DiskHog fault We use a concurrency threshold of 2 seconds to classify concurrent faults that is if the abnormal change point time difference between two components is less than 2 seconds we consider these two components as concurrent faulty components The burst extraction window is set as 20 seconds We use the 
tp fn fp tp tp fp tp tp fn 
    
203 
26 
26 


top 90 frequencies to synthesize the burst signal and use the 90th percentile of the burst value as the expected prediction error We found those parameter conìgurations work well for all the applications tested in our experiments We also conduct sensitivity study on those parameters and will show the results in Section III-F Figure 6 shows the pinpointing accuracy results for RUBiS under three single-component faults We observe that FChain consistently achieves the highest precision and recall for all the faults We observe that the Histogram scheme does not work well for the CpuHog and NetHog faults that manifest very quickly The reason is that wh en the performance anomaly is detected the histogram of the recent data has not shown signiìcant difference from the histogram of all historical data yet since the fault manifestation duration is very short The histogram scheme works better for gradually changing faults such as memory leak although it is still less accurate than FChain We observe that NetMedic could not achieve high accuracy during this set of experi ments After examining the logs we found that the pinpointing errors are caused by unseen states that make the system assi gn inaccurate impact values 5 During the fault injection the faulty component and the other affected components often have a state that is not present in the historical data In contrast we observe that FChain is not susceptible to the problem of unseen values By employing predictability-based ltering and leveraging dependency information FChain effectively removes irrelevant change points caused by normal workload uctuations Thus FChain can achieve higher accu racy than the other change point based schemes such as Topology Dependency and PAL Since the dependency discovery scheme accurately identiìes all the dependencies in the RUBiS system the Dependency scheme has the same accuracy as the Topology scheme in this case Particularly the Topology and the Dependency schemes have very low accuracy for the MemHog and CpuHog faults The reason is that we injected t hose two faults at the database server that is the last tier in the RUBiS system We observed the back-pressure symptom mentioned in Section II-C The faulty database server causes its upstream component the web server or the application server to exhibit anomalous behaviors If we perform pinpointing based on the dependency or topology we will mistakenly pinpoint the upstream component of the culprit component as the faulty one We injected the NetHog fault in the web server that is the rst tier in RUBiS Thus both Topology and Dependency perform well since the back-pressure problem does not exist In contrast FChain is not sensitive to the location of the faulty component which can achieve high accuracy for all situations Although FChain also considers the dependency information we observe that when a fault propagates back to the upstream components its impact becomes smaller The abnormal change point selection step can effectively lter out those change points 5 NetMedic assigns a default high impact value 0.8 to an edge connecting to the abnormal component with a previously unseen state Figure 7 shows the fault localization accuracy comparison results for the System S single component faults Similar to the RUBiS experiments FChain consistently achieves the highest precision and recall value s for all the tested faults The dependency discovery scheme fails to detect any dependency relationship for System S due to the reason mentioned in Section II-C Thus the Dependency scheme pinpointed all the components that have outlier change points This is reason why the Dependency scheme has low precision for all the cases The Topology scheme does not perform well for the MemHog and the bottleneck faults because of the same back-pressure problem mentioned in the RUBiS results We also observe that all the schemes have low precision for the bottleneck fault The reason is that the fault propagates very quickly due to highthroughput communication between stream processing components Thus it is difìcult to distinguish single-component faults from concurrent multi-component faults which explains why the precision is low Luckily we can use the online validation to quickly remove those false alarms which will be shown in Section III-D We now evaluate FChain using multi-component concurrent faults Figures 8 9 and 10 show the pinpointing accuracy results for RUBiS System S and Hadoop respectively We observe that FChain consistently achieves high precision and recall results in all the tested cases except the concurrent CPUHog in System S After examining the log le we nd the diagnosis errors are mostly caused by the side-effect of smoothing Although our previous work s h o w ed t h at smoothing helps to remove the random noise in the raw monitoring data smoothing in this case causes the time of the abnormal change point in the affected normal component to become earlier than those of true culprit components We need to perform adaptive smoothing to address this problem which is part of our on-going work Compared to RUBiS and System S Hadoop is much more dynamic with highly uctuating system metrics In this case the simple change point detection schemes such as PAL have low accuracy especially for the CpuHog and DiskHog faults In the Hadoop experiments we inject faults into all the map nodes that are the rst components in the topology order The back-pressure problem does not exist in this case This explains why Topology and Dependency achieve high accuracy NetMedic also achieve s high precision and recall values in the MemLeak and C PUHog faults The reason is that the default high impact value for unseen states happen to be correct However for the DiskHog fault the default high impact value is incorrect whi ch causes NetMedic to have low accuracy In contrast FChai n can handle previously unseen values and consistently achieve high accuracy We now evaluate our online validation scheme We pick two most challenging faults where all the schemes do not perform well They are the Bottleneck fault and the concurrent CpuHog 
B Single-Component Fault Localization Results C Multi-Component Concurrent Fault Localization Results D Online Pinpointing Validation Results 
204 
27 
27 


We now compare FChain with the Fixed-Filtering scheme Due to space limitations we only show a subset of our results Fig 11 Online validation effectiveness for two challenging System S faults Fig 12 Fault localization accuracy comparison with the Fixed-Filtering scheme for LBBug in RUBiS and DiskHog in Hadoop Figure 12 shows the accuracy of the Fixed-Filtering scheme for a subset of faults in RUBiS and Hadoop We observe that the Fixed-Filtering scheme is very sensitive to the prediction error ltering threshold value In contrast FChain can automatically infer the optimal or near optimal ltering threshold based on the burstiness in the metric values 
E Comparison with Fixed Filtering Schemes F Sensitivity Study 
FChain parameters NetHog CPUHog Diskhog RUBiS System S Hadoop 100 P=1 R=1 P=0.97 R=1 P=0.88 R=0.92 2 P=1 R=1 P=0.97 R=1 P=0.88 R=0.92 
W 
Fig 8 Fault localization accuracy comparison for the multi-component faults in RUBiS Fig 9 Fault localization accuracy comparison for the multi-component faults in System S Fig 10 Fault localization accuracy comparison for the multi-component faults in Hadoop fault in System S Figure 11 shows the pinpointing accuracy results for different schemes The FChain+VAL denotes the FChain scheme with the online pinpointing validation activated Note that the results for the FChain scheme shown before are the results achieved by FChain without the online validation We observe that our online validation scheme can successfully remove most false alarms for these two faults FChain can quickly identify the true faulty component\(s by properly scaling the right resource metric However our current online validation scheme cannot help to improve the recall value which is pa rt of our on-going work Look-back P=0.56 R=0.63 window 300 P=0.98 R=1 P=0.95 R=0.95 P=0.88 R=0.9 sec 500 P=0.98 R=1 P=0.92 R=0.95 Concurrency threhold 5 P=1 R=1 P=0.93 R=1 P=0.88 R=0.88 sec 10 P=0.97 R=1 P=0.93 R=1 P=0.83 R=0.88 TABLE I P RECISION P AND R ECALL R VALUES UNDER DIFFERENT CONFIGURATIONS OF THE KEY FC HAIN PARAMETERS  We have conducted sensitivity study to evaluate the impact of key FChain system parameters to its pinpointing accuracy Due to space limitations we only show a subset of our results in Table I with the optimal parameter settings highlighted in bold Overall we found that FChain is not sensitive to different parameter values We observe that FChain can achieve the optimal performance using default setting 100 seconds lookback window 2 seconds concurrency threshold for all the tested faults except one case that is the look-back window 
205 
28 
28 


We now evaluate the overhead of the FChain system Table II lists the CPU cost of each key module in FChain We observe that most modules in FChain is light-weight The most computation-intensive module is the abnormal change point selection component which is triggered only when a performance anomaly occurs F Chain also distributes the change point computation load on different hosts and executes them in parallel to achieve scalability The online validation takes about 30 seconds for each component since we need some time to observe scaling impact for deciding whether we have made a pinpointing error However the online validation is only performed on those suspicious components pinpointed by the integrated fault diagnosis module The FChain daemon running inside the Domain 0 of each host imposes less than 1 CPU load and consumes about 3MB memory during normal execution IV R ELATED W ORK Our work is rst closely related to previous black-box fault localization schemes F or example NetMedic p rovided detailed application-agnostic fault diagnosis by learning inter-component impact NetMedic rst needs to assume the knowledge of the application topology To perform impact estimation NetMedic needs to nd a historical state that is similar to the current state for each component However for previously unseen anomalies we might not be able to nd a historical state that is similar to the current state for the faulty components Under those circumstances NetMedic assign a default high impact value whic h sometimes lead to inaccurate diagnosis results as shown in Section III In comparison FChain can diagnose previously unseen anomalies and does not assume any prior application knowledge Oliner et al proposed to compute anomaly scores using the histogram approach and correlates the anomaly scores of different components to infer the inter-component inîuence graph As shown in Section III it is difìcult for the histogram-based anomaly detection to perform online fault localization over suddenly manifesting faults Moreover unrelated components can have indirect correlations caused by workload uctuations which will cause their system to raise false alarms In comparison FChain is more robust to different types of faults and workload uctuations To achieve black-box diagnosis researchers have also explored various passive network trafìc monitoring and analysis techniques such as Sherlock  O ri on 27 S N A P  28 However those analysis schemes can only achieve coarsegrained machine-level fault localization Additionally during our experiments we found that previous network trace analysis techniques cannot handle continuous data stream processing applications due to the lack of gaps between packets for extracting different network ows Project5 and E2EProf performed cros s correl a t i ons bet w een mes s a ge traces to derive causal paths in multi-tier distributed systems WAP5 e x t e nds t h e b l ack-box caus a l p at h a nal y s i s t o support wide-area distributed systems Orion di s c o v e rs dependencies from network trafìc using packet headers and timing information based on the observation that the trafìc delay distribution between dependent services often exhibits typical spikes LWT propos ed t o di s c o v e r t he s i mi l a ri t y of the CPU usage patterns between different VMs to extract the dependency relations hips between different VMs However as shown in our experiments dependency-based fault localization techniques are not robust which can make frequent pinpointing mistakes due to various reasons e.g the back pressure effect in distributed applications common network services pinpointed as culprits Furthermore existing dependency discovery techniques need to accumulate a large amount of trace data to achieve reasonable accuracy Particularly network trace based techniques only support requestand-reply types of applications which fail to discover any dependency in continuously running applications such as data stream processing systems In contrast FChain provides online fault localization which does not require any training data for anomalies or a large amount of training data for normal behaviors FChain is fast which can quickly localize faulty components with high accuracy af ter the performance anomaly is detected A urry of research work has proposed to use end-to-end tracing for distributed system debugging Magpie  i s a request extraction and workload modelling tool that can record ne-grained system events and correlate those events using an application speciìc event sch ema to capture the control ow and resource consumption of each request Pinpoint t ak es a request-oriented approach to tag each call with a request ID by modifying middleware platform and applies statistical methods to identify components that are highly correlated with failed requests Monitor t racks t he reques t s e xchanged b et ween components in the system and performs probabilistic diagnosis on the potential anomalous components X-Trace i s a n integrated cross-layer crossapplication tracing framework which tags all network operations resulting from a particular task with the same task identiìer to construct a task tree Spectroscope can di agnos e p erformance anomal i e s b y comparing request ows from two executions In contrast our approach does not require any instrumentation to the 
G FChain System Overhead Measurements 
System Modules CPU cost 
VM monitoring 6 attributes 1.03 0.09 m illisec onds Normal uctuation modeling 22.9 2 millis econds 1000 samples Abnormal change point selection 602.4 105.2 m illisec onds 100 samples Integrated fault diagnosis 22 1 microseconds Online validation per-component 30 1 seconds TABLE II FC HAIN OVERHEAD MEASUREMENTS  size for the DiskHog fault in Hadoop The reason has been described in Section III-A Generally the look-back window should be long enough to capture the fault manifestation We are currently investigating an adaptive look-back window conìguration scheme by examining the metric changing speed 
     
206 
29 
29 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





