An Improved Incremental Mining Algorithm Based on Risk Analysis of the Association Rules for Bank Cost Analysis  Mei Chunguo 1    1 Maoming University Maoming, 525000, China E-mail:yzsmcg@sina.com    Mei Ying 2,3  2 College of Traffic and Communications, SCUT Guangzhou , 510641, China 3 Computer and Information Engineering Department Guangzhou Maritime High College Guangzhou, 510725, China     Abstract 
This paper introduces improving rate and proposes the incremental mining algorithm with the weighted model for optimizing association rules based on CBA mining algorithm. The risk analysis of the strong association rules is proposed for trend forecasting. And the risk degree of the lost rules based on the incremental mining is also analyzed. Comparing with the traditional algorithm the improved algorithm is fast, efficient in incremental data mining and can find trends in association rules. The decision making reliability is enhanced by the association rules obtained from the improved algorithm. The algorithm was used in bank cost analysis with test results showing that the 
prediction precision of the algo rithm is better than that of the traditional algorithm   Keywords-incremental mining; risk analysis; bank cost analysis  001\011\000\021\000\003 I NTRODUCTION   Data Ming means extracting potential and valuable information from a mass of data and forecasting development trends. Data mining is the new field with fast development, and it is the crossover application field of many study fields. Data mining is very important for the decision-making system in every walk of life Association rules mining in transaction data is the important portion of data mining research 
Apriori algorithm is the way that is commonly used for mining association rules’ frequent item set. But it mostly finds association rules from the static information database. In factual application, this algorithm has some shortcoming. The paper analyzes the change of the strong association rules with data updating in database by incremental mining algorithm according to factual need of decision-making. We analyze the strong association rules by the weighted model and consider the risk degree of the lost association rules. Accordingly, we improve the significance of incremental updating association rules mining and efficiently forecast the trend of rules 000\003 001\012\000\021\000\003 T RADITIONAL 
CBA ALGORITHM  000\003 Although association rules roots in POS, it can be used in many fields. The basic data mining processes of association rules are: first choose right element, decide different statistic level, choose specific degree, and only with element appearance degree in data is the same. The effect of association rules is the best; second, produce rules, then propose sup. and cof. Given one transaction data set D association rules mining’s work is to find association rules that is not less than the appointed minsup and min-cof CBA is one traditional mining algorithm using in bank cost analysis. CBA is classification based on association 
This algorithm integrates classification mining algorithm and association rules mining. CBA algorithm produces classifier through two processes. The first process, find classification association rules\(CAR\. The second process, choose high prior degree rules for cover training set from the discoverable CAR, that is, if the left of the associations rules are the same and the right of them are different, we choose high-cof. rules as possible rules After obtaining association rules that satisfy the min-sup and min-cof. between the conditional attributes and the decision attributes, the importance of the rules is defined as[4 e i m p o r tan ce o f ru le A is  imp A rule A  and 
B is given If   cof A cof B  then   imp A imp B   If   cof A cof B   sup  sup  AB  then   imp A imp B   If   cof A cof B   sup  sup  AB  rule A is created before rule B then   imp A imp B   The algorithm is as follows 
 R sort R   for each rule rR 002 in sequence do tcmp 003  for each case dD 002 do if d satisfies the conditions of r then store  did in tcmp and mark r if it correctly classifies d  if r is marked then insert r at the end of C  
2009 International Conference on Computer and Communications Security 978-0-7695-3906-5/09 $26.00 © 2009 IEEE DOI 10.1109/ICCCS.2009.35 10 


delete all the cases with the ids in temp from D  select a default class for the current C  compute the total number of errors of C  end end find the first rule p in C with the lowest total number of errors and drop all the rules after p in C  add the default class associated with p to end of C and return C  our classifier  Traditionally, CBA was used in bank cost analysis.  In part 2 and part 3, we’ll discuss a new mend of the original algorithm 000\003 001\013\000\021\000\003 I NTRODUCTION OF THE IMPROVING RATE FACTOR 000\003 000\003 The higher the cof. is, the stronger the association rules is, namely, the strong association rule. But sometimes it is wrong. In circumstance of precondition and conclusion both with high-sup, although the precondition and conclusion are irrelative, their association cof. is high. One better way to judge the intensity of association rules is to compare the cof. with the standard value of the rule, here we suppose that relative item set’s generant probability created with every rule and generant probability of former item set is independent. We can use frequent item set’s frequency to calculate the standard value. Standard cof. is the concomitantly generant association’s sup divided by the number of transaction in database. And this can help us calculate “the improving-rate” of the rule. “The improving-rate” is the cof. of the rule that is divided by the cof. of generant association supposed to be independent. If the rate is bigger than 1, it means that the rule is useful. And the bigger “the improving rate’’ is, the higher the association rule’s intensity is 000\003 001\014\000\021 I NCREMENTAL MINING ALGORITHM   Apriori algorithm is the way that mines and analyzes the static information data. When the data in database keeps invariability, it is useful. But when the data is changed, it needs to scan the new database over again for adapting the increase of data in database. With the updating of the database, the original rules are not likely to keep identical. In the modern data management circumstance that information increases rapidly, it results in much repeated work and over-load of database 000 s operation. And it wastes the original result obtained by old database mining We use the incremental mining algorithm to mend the original algorithm. The incremental mining algorithm takes full advantage of the old mining results and mines updating association rules with the incremental portion of the database. And it can efficiently decrease the database scanning times and sufficiently improved the efficiency of data mining. Suppose D for the original database, and d for changing data set. The incremental updating of the association rules mostly means how to get association rules of D d 004 when the new data set d is added to the original database D or d is deleted from D with the changeless min-sup and the changeless min-cof sometimes they can changed\e incremental updating database, incremental mining concerns the following four instances of item set: \(1\requent in D frequent in d , then frequent in D d 004 2\requent in D not frequent in d , then uncertain in D d 004 3\ot frequent in D, frequent in d , then uncertain in D d 004  4\ not frequent in D , not frequent in d , then not frequent in D d 004  For satisfying high efficiency updating of association rules, it creates many kinds of data mining algorithm as FUP, IUA, AIUA, NEW FUP and FUFIA 2\d \(4\t it can not solve the third. IUA and NEW FUP algorithm solve \(3 but the efficiency isn't high  001\015\000\021\000\003 I MPROVEMENT OF ORIGINAL ALGORITHM BY INCREMENTAL MINING ALGORITHM   In Apriori algorithm of original association rules mining, if the database is updated, the rules created by original database mining will be not appropriate. The ecumenical way is to mine the new database again using the algorithm. In the modern data management circumstance that information increases rapidly, it results in much repeated work and over-load of database’s operation. And it wastes original result obtained by old database mining. The ameliorative algorithm pays attention to the trends of rules development when mines database by association rules algorithm. The new algorithm reduces the number of database scan, and improves efficiency of data mining. It has better precision in decision-making support We use incremental mining algorithm to mend the original algorithm. For protecting rules, we present a rule protection model with weight. This rule protection model with weight will exert forecast function in the latter work of incremental mining. Suppose 1 W and 2 W are the weight of D and d respectively. For association rule XY 000 we define the sup and cof. as the following 11 22       Supp w X Y W Supp X Y W Supp X Y confw X Y Supp w X Y Supp w X   000   000*\000 000 000<\000 000  
11 


Here 1  Supp X Y 000 and 2  Supp X Y 000 are the rule’s XY 000 sup in D and d respectively  Suppw X Y 000 and  confw X Y 000 are the rule`s XY 000 sup. and con. in D d 004 respectively For association rule XY 000 its sup. and cof. are defined as the following  11 11       nn nn Supp w X Y W Supp X Y W Supp X Y Confw X Y W Conf X Y W Conf X Y  000  000  000 000*\000*\000 000 000  Here  i Supp X Y 000 and  n Conf X Y 000 are rule’s  XY 000 p. and  cof. in incremental set i D  First, we use AIUA\( advanced incremental updating algorithm \ mend  incremental association rules when min-sup isn’t changed. The algorithm need respectively scan only once the original database and the new incremental database The algorithm is as the following Input D B the original transaction database L the set of D B s frequent item set db  0 S the min-sup Output  L the set of D Bdb 000 s frequent item set 1  L 005   for each XL 002 do begin if X count 0     L SDBdb 006  then 000\003\000\003\000\003\000\003\000\003     L LX     F L LX   Else max max sup   Dii SxDxL  002  end 2 min 0 max 0  dD SStSS     for each x db 002 do begin if X count 0     db S DB db 006  then     L LX    db db x   else  min      id L x db x count db S DB db  002 006    end 3\or each F XL 002 do begin for each  x L 002 do begin if X count F L X  count  0     L SDBdb 006   then    L LX     L LX   else  FF L LX   end end 4  min 0    dd L L x S x Supd S   007 \007  5  d L 005  then output  L  else for each  d x L 002 do begin max max sup   dii SxdxL  002  min 0 max 0 1   Dd SStSS      000\003 end end 6 min   DkD L aproiri gen L S    7\or each  d x L 002 do begin for each D XL 002 do begin if X count D L X  count  0     d L SDBdb 006   then   L LX  000\003  else delete X  end end 8\tput  L  Here: X .count L , X .count db X count F L  X count L 010  X count D L and X .count d L 010 are item set X sup in L , db F L  L 010  D L and d L 010 respectively Then, we calculate significative association rules 000  sup and cof in new database using the rule protection model with the weight. Calculate sup\( x  sup  D dx 004 in 1 D  and cof\( x \n  D dcofx 004 in  1 D  then we get 1 T and 2 T  Here we call them T  T reflects change trend of the strong association rules on the basis of incremental mining. We analyze and forecast the useful association rules by the change trends with database updating every time. Thereby, it supports the decision-making better On the other side, we study the lost rules that were the strong association rules in the original database after database updating. Calculate sup\( x \n D d 004 and cof x \ in D d 004 and get T . The absolute value of T shows the eliminated risk when the strong association rules in the original database are changed with updating data. The 
12 


absolute value of  T for the lost association rules, it is defined as risk degree of the lost. In practical application people often concern the risk of the strong association rules in the original database, which is the risk of being weak with periodic increase of database. We can also regard the risk degree as the risk that the strong rules turn into the weak rules or the weak rules turn into the strong rules. Higher the risk degree is, lower the universality of the rule is. The introduction of risk degree is for the need of solving the practical problem. It forecasts the trends of association rules based on incremental mining from the different aspect and increases the reliability of forecasting  001\016\000\021\000\003 T HE APPLICATION OF IMPROVED ALGORITHM IN BANK COST ANALYSIS   In process of bank cost analysis, we analyze the basic product attributes and product cost. Based on it, we get the rule that product’s profit is relative to one or some of the attributes and obtain idiographic association rules. The data mining process of bank cost analysis includes gathering of data, data pretreatment, model training and model evaluating We analyze the six months statistic information of bank product. The former five months’ data is the original database. And the sixth months’ data is the incremental data. We use the original algorithm and the improved algorithm to mine the bank cost analysis database The result shows that the improved algorithm is better than the original CBA algorithm. The precision of cost analysis is improved 30 percent. So, with the improved algorithm, we can efficiently mine association rules and mend incremental data mining in database, then improve precision of decision-making  001\017\000\021\000\003 C ONCLUSION   We introduce the improving-rate factor to increase the precision for optimizing the original algorithm, and present the improved algorithm for mining incremental data in database. With the data of one bank’s six months statistical information, we test the new algorithm and compare it with the original algorithm. The result shows that the improved algorithm improves the reliability of data mining, and provides more information for decisionmaker  R EFERENCES   1 H a n J i a w ei  a nd Mi ch eli n e K a m b er  D a t a  M i n i n g C o n c ep ts  a nd Techniques, Morgan Kauffman Publishers, 2001 2 L i u Yi a n Yan g Bi n R es ea rc h  of a n I m p r ov ed A p ri ori Algo ri t h m  in Mining Association Rules”, Computer Application, Feb. 2007, pp 418-420 3 D i ng W e iping  S h i Q u a n G u a n Z h ij i n   A lg o r ithm o f Ef f e ctiv e  Association Rules Mining Based on Transaction Rule-tree”, Application Research of Computers, May 2007, pp. 83-86 4 Re n X i ul i  S h i Z h o n g z h i  B a n k Co s t A n al y s is Ba s e d o n D a ta  Mining”, Application Research of Computers, Sept. 2007, pp. 53-57 5 W a ng J i nc he ng W a ng X i ao l i n  P a ng G u f e ng  A s s o ciatio n Rul e s  Mining Algorithm for Cold-rolling Processes”, Tsinghua Univ\(Sci&Tech\, 2007, 47\(S2\. 1761-1765 6 Me ng Re n  S u Y i j ua n, Z h u X i ao f e n g  Z h ang J i l ia n A n E f f i cie n t  Incremental Updating Algorithm in Data Mining for Maintaining Association Rules”, Journal of Guangxi Academy of Sciences, May 2006, pp. 125-128 7 o hns o n R i c h ar d, a n d D e an W i che r n, .A ppl ie d Mul t i v a r i a t e  Statistical Analysis, 5th, ed.Prentice-Hall, 2002 8 M a t t h e w Er ic O t e y Sr in iv as an P a r t h a sar a t h y  M e mb e r I EEE, Ch ao  Wang, Adriano Veloso, Wagner Meira, Jr., “Parallel and Distributed Methods for Incremental Frequent Itemset Mining”, IEEE Transactions on Systems, Man, and Cybernetics---part B: Cybernetics, Dec. 2004, pp 2439-2450 9 an Dy n a m i c G r o w i n g Da ta  Mi ni n g of M o re F r eq u e n t  Itemset in Large Database”, Computer Engineering, Jan. 2006, pp. 7678   M a Z h an xin  L u Yu c h a n g  E xp lod i n g Nu m b er of F r eq u e n t  Itemsets in the Mining of Negative Association Rules”, J Tsinghua Univ\( Sci & Tech\ 2007, 47\(7\:pp. 1212-1215  Z h a n g H u i z h e  W a n g J i an   R es ea rc h of Mu lt ip le M i ni m u m  Supports Frequent Item Sets Minging”, Computer Applications, Sept 2007, pp. 2290-2293  o u L i k u n  Z h an g Qi s h an A l gori t h m of W e i g h t ed A s s o c i at i on Rules Mining with Multiple Minimum Supports”, Journal of Beijing University of Aeronautics and Astronautics, May 2007, pp. 590-593    
13 


   


Group 050b\051 employs a 002tness function based on R precision Group 050c\051 is based on association rules Group 050d\051 are those related to the proposed framework F c  F cA and F cB correspond to the proposed 002tness functions F c 050 Q C 051  F cA 050 Q C 051  F cB 050 Q C 051 given by the Equations 2 3 4 and respectively The evolutionary search was set to 100 individuals evolving along 400 generations in experiments 1 and 2 Experiment 3 employed 50 individuals and 250 generations According to the dimensionality of the datasets the parameter d employed in F cA 050Equation 3\051 was set to 50 in the 002rst and second experiments and to 20 in the third experiment The Euclidean distance was employed in the experiments to measure the similarity between the feature vectors 5.1 Experiment 1 ROI-102 image dataset This dataset consists of 102 images with r egions o f i nterest 050ROI\051 taken from mammograms collected from the Breast Imaging Reporting and Data System of the Department of Radiology of University of Vienna 050http://www.birads.at\051 classi\002ed into three levels of BIRADS 0503 4 and 5\051 The BIRADS 050Breast Imaging Reporting and Data System\051 categorization was developed by the American College of Radiology to standardize mammogram reports and procedures The BIRADS categorization is summarized in Table 1 The dataset has been divided into a 68\226image training set and a 34\226image test set Each image has been characterized by a 850\226dimensional feature vector including features generated by Haralick descriptors 050140 features\051 wavelets 05064 features\051 zernike moments 050255 features\051 histogram 050256 features\051 features of 002rst order derived of histogram 0506 features\051 Run length 05044 features\051 and Edge Histogram MPEG7 05080 features\051 Figure 2 shows the P&R curves for the test dataset and also the number of features selected for each method It can be seen that the proposed framework with 002tness function derived from order-based ranking evaluation function yielded superior results when compared with the traditional evaluation criteria given by the average classi\002cation error improving up to 22 the precision of the query answers The proposed framework also outperforms the 002tness function derived from non order-based ranking evaluation function 050FR-Precision\051 and all features combined Also the proposed framework 002nds the sets with the fewest features in comparison with the other methods 5.2 Experiment 2 ROI-250 image dataset This experiment investigates the performance of the proposed technique for a 250\226image mammography dataset with ROIs comprising lesions taken from the Digital Database for Screening MamTable 1 BIRADS categorization value  description  0  Need Additional Imaging Evaluation 1  Negative 2  Benign Finding 3  Probably Benign Finding Short Interval Follow Up Suggested 4  Suspicious Abnormality Biopsy recommended 5  Highly Suggestive of Malignancy Proper Action  Must be Taken Figure 2 Precision-recall curve in ROI-102 image dataset mography of the University of South Carolina 050http://marathon.csee.usf.edu/Mammography/\051 classi\002ed into two classes mass-benign and mass-malign A 739\226dimensional feature vector has been computed for each sample including features generated by Haralick descriptors 050140 features\051 zernike moments 050255 features\051 histogram 050256 features\051 features of 002rst order derived of histogram 0506 features\051 Run length 05044 features\051 and invariant moments 05038\051 The dataset was divided into a 166\226image training subset and a 64-image test subset Figure 3 shows the precision-recall curves obtained and also the number of features selected in each method The graphs of Figure 3 show that the proposed methods 050 F cA  F cB and F c 051 increased the precision of the queries in about 15 in the region of 5 of recall in comparison with the other methods while decreasing the number of features from 739 to around 50 This is using about 7 of the previous memory space for the images representation These results indicate that ranking evaluation functions are well-suited to be employed in genetic feature selection for CBIR 


Figure 3 Precision-recall curve in ROI-250 image dataset 5.3 Experiment 3 Mammograms-1080 image dataset This experiment employed a dataset composed by 1080 mammograms images collected in the Clinical Hospital of University of Sao Paulo at Ribeiro Preto The dataset was previously classi\002ed into 4 levels of breast tissue density 0501\051 mostly fatty 050362 images\051 0502\051 partly fatty 050446 images\051 0503\051 partly dense 050200 images\051 and 0504\051 mostly dense 05072 images\051 Figure 4 Precision-recall curve in Mammography-1080 image dataset Breast density is an important risk factor in the development of breast cancer In this experiment the images are represented by the feature set proposed in b uilding a vector of 85 features including shape and size of the breast the conditions of the breast contour nipple position and the distribution of 002broglandular tissue This dataset was divided in training set and test set The training set is composed of 720 images and test set is composed of 360 images Figure 4 shows the P&R curves over test dataset and also the number of features selected in each method Again the proposed methods reached the highest values of precision and select the smallest number of features 050a\051 050b\051 050c\051 050d\051 Figure 5 Queries in Mammography dataset 050a\051 226 is the query image 050b\051 using the features selected through 002tness function F cB  050c\051 using the features selected through classi\002cation error of C4.5 and 050d\051 using the all features extracted Results for the retrieval of the 5 most similar images from a query image are also provided in this experiment as illustrated in Figure 5 The image 5.\050a\051 is the query image taken from the mostly fatty image class Images shown in 5.\050b\051 are the 5 most similar images retrieved for the proposed 002tness function F cB  The row 050c\051 shows the results for C 4  5 classi\002er whereas 050d\051 illustrate the images resulting from all features 050no selection applied\051 In Figure 5 the images surrounded by dashed lines are false positives 050not relevant images\051 For this query the proposed method achieved the highest precision 050100%\051 when compared the results of C4.5 and the original feature vector 050precision of 40%\051 


6 Conclusions This work proposed a novel genetic feature selection framework for CBIRs It employs a wrapper strategy that searches for the best reduced feature set while optimizing 050or preserving\051 the quality of the solution From a ranking evaluation function three new 002tness functions namely F cA  F cB and F c have been proposed and evaluated in three experiments The proposed genetic feature selection approach which encompasses F cA  F cB and F c  has been compared with 050a\051 traditional methods found in the literature 050b\051 the StARMiner feature selector and 050c\051 the whole feature vector and signi\002cantly outperformed them The proposed approach has been able to optimize the accuracy of similarity queries while selecting a signi\002catively reduced number of features Additionally the proposal of combining the quality of the query results with the criterion of minimizing the number of selected features F cA and F cB  led to high accurate query answers while reducing the number of features more than the 002tness function F c  Therefore the 002nal processing cost of the queries is also reduced References  P  M d Aze v edo-Marques N A Rosa A J M T raina C Traina-Jr S K Kinoshita and R M Rangayyan Reducing the semantic gap in content-based image retrieval in mammography with relevance feedback and inclusion of expert knowledge International Journal of Computer Assisted Radiology and Surgery  3\0501-2\051:123\226130 June 2008  R Baeza-Y ates and B Ribeiro-Neto Modern Information Retrieval  Addison-Wesley Essex UK 1999  B Bartell G Cottrell and R Bele w  Optimizing similar ity using multi-query relevance Journal of the American Society for Information Science  49:742\226761 1998  O Cord 264 on E Herrera-Viedma C L 264 opez-Puljalte M Luque and C Zarco A review on the application of evolutionary computation to information retrieval International Journal of Approximate Reasoning  34:241\226264 July 2003  J G Dy  C E Brodle y  A Kak L S Broderick and A M Aisen Unsupervised feature selection applied to content-based retrieval of lung images IEEE Transactions on Pattern Analysis and Machine Intelligence  25\0503\051:373\226 378 March 2003  W  F an E A F ox P  P athak and H W u The ef fects of 002tness functions on genetic programming-based ranking discovery for web search Journal of the American Society for Information Science and Technology  55\0507\051:628\226636 2004  W  F an P  P athak and M Zhou Genet ic-based approaches in ranking function discovery and optimization in information retrieval a framework Decision Support Systems  2009  D E Golber g Genetic algorithms in search optimization and machine learning  Addison Wesley 1989  R L Haupt and S E Haupt Practical Genetic Algorithms  John Wiley  Sons New Jersey United States second edition edition 2004  J Horng and C Y eh Applying genetic algorit hms to query optimization in document retrieval Information Processing  Management  36:737\226759 2000  S K Kinoshita P  M d Aze v edo-Ma rques R R PereiraJr J A H Rodrigues and R M Rangayyan Contentbased retrieval of mammograms using visual features related to breast density patterns Journal of Digital Imaging  20\0502\051:172\226190 June 2007  F  K orn B P agel and C F aloutsos On the  dimensionality curse and the self-similarity blessing IEEE Trans on Knowledge and Data Engineering  13\0501\051:96\226111 2001  H Liu and L Y u T o w ard inte grating feature select ion algorithms for classi\002cation and clustering IEEE Transactions on Knowledge and Data Enginnering  17\0504\051:491\226502 April 2005  M X Ribeiro A J M T raina C T raina-Jr  and P  M Azevedo-Marques An association rule-based method to support medical image diagnosis with ef\002ciency IEEE Transactions on Multimedia  10\0502\051:277\226285 2008  U S Cancer Statistics W orking Group United states cancer statistics 1999-2005 incidence and mortality webbased report atlanta 050ga\051 Department of health and human services centers for disease control and prevention and national cancer institute 2009 Available in http://apps.nccd.cdc.gov/uscs   L T amine C C and M Boughanem Multiple query evaluation based on an enhanced geneticnext term algorithm Information Processing  Management  39\0502\051:215\226 231 2003  R S T orres A X  F alc 230 ao M A Gonc\270alves J P Papa Z B W Fan and E A Fox A genetic programming framework for content-based image retrieval Journal of the American Society for Information Science and Technology  42\0502\051:283\226292 2009  A Tsymbal P  Cunningham M P echenizkiy  and S Puuronen Search strategies for ensemble feature selection in medical diagnostics In Proceedings of the 16th IEEE Symposium on Computer-Based Medical Systems  pages 124\226 129 June 2003  A Tsymbal M Pechenizkiy  and P  Cunningham Sequential genetic search for ensemble feature selection In Proceedings of the International Joint Conferences on Arti\002cial Intelligence  pages 877\226882 August 2005  C.-M W ang a and Y F  Huang Ev olutionary-based feature selection approaches with new criteria for data mining A case study of credit approval data Expert Systems with Applications  36\0503 Part 2\051:5900\2265908 2009  H Y an J Zheng Y  Jiang C Peng and S Xiao Selecting critical clinical features for heart diseases diagnosis with a real-coded genetic algorithm Applied Soft Computing  8:1105\2261111 2008  T  Zhao J Lu Y  Zhang and Q Xiao Feature selection based on genetic algorithm for cbir In IEEE Congress on Image and Signal Processing  volume 2 pages 495\226499 2008 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnt create sound clips  A5.1 We dont need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIs   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





