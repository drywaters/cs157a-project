Hysteresis Re-chunking Based Metadata Harnessing Deduplication of Disk Images Bing Zhou and Jiangtao Wen State Key Laboratory on Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology TNList Department of Computer Science and Technology Tsinghua University Beijing China Email zhoubing09@mails.tsinghua.edu.cn jtwen@tsinghua.edu.cn 
Abstract 
Metadata-related overhead can signi“cantly impact the performance of data deduplication systems including the real duplication elimination ratio and the deduplication throughput The amount of metadata produced is mainly determined by the chunking mechanism for the input data stream In this paper we 
Keywords 
propose a metadata harnessing deduplication MHD algorithm utilizing a duplication-distribution-based hysteresis re-chunking strategy MHD harnesses the metadata by dynamically merging multiple non-duplicate chunks into one big chunk represented by one hash value while dividing big chunks straddling duplicate and non-duplicate data regions into small chunks represented with multiple hashes Experimental results show that the proposed algorithm achieves a lower metadata overhead and a higher deduplication throughput for a given duplication elimination ratio as compared with other state-of-the-art algorithms such as the Bimodal SubChunk and SparseIndexing algorithms Data Deduplication Metadata Harnessing 
I I NTRODUCTION Data deduplication refers to the process of optimizing a data storage system by dividing the data into chunks so as to identify and eliminate redundant storage of identical chunks The performance of a data deduplication system is mainly measured in terms of the duplication elimination ratio as well as the deduplication throughput Generally the smaller the average chunk size the more the redundancies that can be identi“ed and removed at a cost of higher metadata related overheads Because xed-sized chucking algorithms such as those used in the Venti and OceanStore algorithms 2 are not able to handle the boundary-shifting problem contentde“ned-chunking CDC techniques dividing a data stream 
into variable-sized chunks based on ngerprints computed using a sliding window were proposed in systems such as LBFS and ha v e become the basic chunking algorithm in virtually all state-of-the-art deduplication algorithms or systems including 6 7 8 9 10 11 In a typical deduplication system metadata are generated and used for two purposes i.e metadata for data storage management MS and metadata for content description MD including the hash values as well as the address and size of each chunk The MD that can not t into the main memory will be stored on disk The related disk I/O and look-up operations would reduce deduplication speed a.k.a the disk bottleneck To address the disk bottleneck the Data Domain and 
SparseIndexing algorithms utilized inherent data locality to improve the throughput without explicitly addressing the issue of metadata generation In the Data Domain algorithm the amount of the metadata grows linearly with respect to the number of chunks The rate of growth is even higher for the SparseIndexing algorithm as the hash value of a chunk could be stored multiple times if the chunk is present in multiple segments To reduce the size of the MS Fingerdiff coalesce contiguous non-duplicate chunks up to a maximal number into one big chunk stored on the disk The Bimodal FBC 16 and SubChunk algorithms rst search for duplications 
using a large chunk size with the non-duplicate big chunks selectively processed again using a small chunk size Although such algorithms are capable of reducing the MS they do not necessarily reduce the MD or the overall amount of metadata and I/Os In addition in Fingerdiff a database is needed to index each chunk The assumption that the database can t into the RAM might not be realistic in practical systems In the Bimodal FBC and Subchunk algorithms before rechunking a big chunk a query for the non-duplicate big chunk is introduced Metadata related overhead also greatly impacts the deduplication performance in distributed systems related applications such as large scale data backup To improve the trade 
is the only le to be processed the optimal chunk should be set to  But when a second le 
off between duplication elimination ratio and deduplication overhead including the storage space occupied by the extra metadata and the corresponding I/Os in this paper we propose a duplication-distribution-based metadata harnessing deduplication MHD algorithm It uses hysteresis re-chunking taking into account both data that have been processed as well as incoming data As an illustrative example consider the case in Fig 1 with 3 les When File-1 File-1 File-2 
that matches 
of the rst le is present the rst le should be re-chunked into two chunks namely and  Similarly if there is a third le which matches of the second le the second le should be re-chunked into three chunks  and  In this example only 5 hash values are required 
Slice-1 Slice-1 Slice-2 File-3 Slice-3 Slice-3 Slice-4 Slice-1 
2013 42nd International Conference on Parallel Processing 0190-3918/13 $26.00 © 2013 IEEE DOI 10.1109/ICPP.2013.48 389 
2013 42nd International Conference on Parallel Processing 0190-3918/13 $26.00 © 2013 IEEE DOI 10.1109/ICPP.2013.48 389 
2013 42nd International Conference on Parallel Processing 0190-3918/13 $26.00 © 2013 IEEE DOI 10.1109/ICPP.2013.48 389 
2013 42nd International Conference on Parallel Processing 0190-3918/13 $26.00 © 2013 IEEE DOI 10.1109/ICPP.2013.48 389 
2013 42nd International Conference on Parallel Processing 0190-3918/13 $26.00 © 2013 IEEE DOI 10.1109/ICPP.2013.48 389 
2013 42nd International Conference on Parallel Processing 0190-3918/13 $26.00 © 2013 IEEE DOI 10.1109/ICPP.2013.48 389 


Fig 1 Illustration for Hysteresis Re-chunking While utilizing tools such as manifests and hooks as in the SparseIndexing algorithm MHD differs from the Bimodal FBC and SubChunk algorithm in the following two aspects Fig 2 System Architecure Extreme Binning uses one chunk from each le to represent the corresponding le If the representative chunk is found to be a duplicate data locality information of the corresponding le is loaded into the RAM As only one disk access is needed per le the throughput of the Extreme Binning algorithm is comparatively high Data Domain uses the Bloom Filter to a v oid unnecessary disk lookups A stream-dependent segment layout is used to maintain high spatial locality so that each retrie v al from the disk is more fully utilized for multiple data chunks as opposed to requiring a separate disk access for each chunk In SparseIndexing instead of building a full index for every chunk a small portion of chunks are extracted as hooks Each hook was mapped to a few manifests or recipes which preserve the data locality present in the original input chunks When deduplicating SparseIndexing divides the incoming stream into large segments and processed each segment with reference to several most similar existing segments referenced by the hooks By keeping only hooks in the RAM and by requiring only a few disk seeks per large segment the disk look-up bottleneck is avoided Meister et al proposed an entrop y coding based postprocess compression method for le recipes It should be noted that le receipts is only one of many types of metadata generated during deduplication The Bimodal FBC and SubChunk algorithms rst divide the input stream into big chunks for duplication detection Bimodal then re-chunks the non-duplicate big chunks adjacent to duplicate chunks termed transition points into smaller chunks followed by the deduplication process on the smaller chunks FBC performs selective re-chunking using several strategies based on the frequency information of chunks estimated from data that have been previously processed SubChunk on the other hand re-chunks every non-duplicate big chunk into small chunks for deduplication The nonduplicate small chunks belonging to one big chunk would then be coalesced together III M ETADATA H ARNESSING D EDUPLICATION The MHD algorithm consists of two phases 1 the sampling and hash merging phase in which large chunks are formed by merging multiple smaller chunks and represented by a single 
  
    
MHD reduces unnecessary queries for non-duplicate big chunks by replacing the widely used big-chunk“rst-small-chunk-second deduplication strategy with a bi-directional-match-extension mechanism Using this mechanism when a small duplicate chunk is detected duplication detection is conducted using its neighboring data and a relatively large chunk size Big chunks would be re-chunked when and only when they straddle duplicate and non-duplicate data The rest of this paper is organized as the following Section II presents a brief overview of related work The proposed algorithm is described in detail in Section III followed by analysis in Section IV and experimental results obtained using real-world disk image data and the proposed algorithm as well as the Bimodal SubChunk and SparseIndexing algorithms are given in Section V Section VI contains the conclusions II R ELATED W ORK The widely used Rabin Fingerprint chunking algorithm uses a sliding window to scan the input data stream and calculate a ngerprint at each position The current position is recorded as a cut point if either of the following is true 1 the ngerprint matches with a pre-de“ned value and the length of the current data block is larger than a lower bound or 2 when the length of the current data block is equal to or larger than a upper bound and no matching ngerprint is found The data stream between two cut points are extracted as chunks An improved chunking algorithm named TTTD was proposed in In TTTD candidate cut points corresponding to chunk sizes between the lower and the upper bounds are recorded and used as cut points only if no pre-de“ned ngerprints are detected when the corresponding chunk size reaches the upper bound Nohhyun and David e v aluated the characteristics of a dataset using a metrics for the variation of data patterns in the dataset Considering the processing overhead of CDC for mobile devices Lee and Park proposed a chunking method adaptively selecting the CDC and FSP algorithms based on the le type and the computational capabilities of the devices 
015\004\003\006\016\017\020\004\021\006\022\023 024\025\004\023\006 026\027\023\006\030\002\023\025\006\017\031 024\025\004\023\006 032\006\033\034\035\003\004\005\017\023\036\025 037 \034\020!\022 037 \034\020!\006\025 034\006\025\027 016\017\035\035\006\033\030\023\036 024\025\004\023\006 024\025\004\023\006 006\017\033 006\017\033 032\004\022!\037 \034\020!\030 002\023\036\025\006 032\004\022!\037 \034\020!\007 016\017\020\004\021\006\022\023 036\036!\022 002\006\035\017\025\017\023\006\030 015\004\003\006\022 016\017\035\035\006\033\030\023\036 022\006\033\030\023\036\030\025\006\005\036\020\022\023\025\034\005\023\030 036\025\004&\004\020\017\003\030\022\006\035\017\025\017\023\006\030\021\004\003\006\022 002\023\036\025\004\020&\030\017\003\003\030\023 \006\030\020\036\020\007 033\034\035\003\004\005\017\023\006\030\033\017\023\017 017\022 \030\022\006'\034\006\020\005\006\022\030 034\022\006\033\030\023\036\030\033\006\022\005\025\004\(\006\030 006\017\005 \030\032\004\022!\037 \034\020 037\036\020\023\006\020\023\030\017\033\033\025\006\022\022\017\(\003\006\030 036\036!\022\030\034\022\006\033\030\021\036\025\030\033\034\035\003\004\005\017\023\006\030 033\006\023\006\005\023\004\036\020\030\036\020\030\033\004\022 
002\003\004\005\006\007\010 002\003\004\005\006\007\011 002\003\004\005\006\007\011 002\003\004\005\006\007\012 002\003\004\005\006\007\013 002\003\004\005\006\007\013 002\003\004\005\006\007\014 
002\003\004\005\006\007 002\003\004\005\006\010 002\003\004\005\006\011 
390 
390 
390 
390 
390 
390 


006\017\033\0300\0061\023\030 037 \034\020 2\022\030\032\034\0353 4\034\023\030\023 \006\030 037 \034\020!\030\004\020\023\036\030 026\034\021\021\006\025 0\036 2\022\030\026\034\021\021\006\025\030 015\034\003\0033 0\036 002$\016 5\006\022 026\0166 015\0166 0\006\006\033\030$$#3 0\006\006\033\030$$#3 0\036  5\006\022  5\006\022 5\006\022 0\036 
Hash 1 Hash 10 Chunk 1 Chunk 
012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 017\020\021\013 010\006\022 017\020\021\013 023 017\020\021\013 007 017\024\024\016 017\024\024\016 017\020\021\013 025\006\007\026 
1 1 1 1 1 
is referred to as simply the  The Manifest is a sequence of hash values representing the data blocks within the corresponding DiskChunk while a Hook is a hash value sample extracted from the Manifest Each DiskChunk Manifest or Hook is implemented as a separate hash addressable le and the formats of the metadata are shown in Figure 3 Each Hook named using its hash value is mapped to only one Manifest and each Manifest is mapped to one DiskChunk The DiskChunk and the Hook les that have been written to disk will not be further modi“ed Each entry in the Manifests introduces a one-byte Hook ag to indicate whether this entry is a Hook Re-chunking is only carried out for data blocks corresponding to the non-Hook hash values with the original hash values replaced by multiple new values created from data blocks after re-chunking The only les updated during the deduplcation process are the Manifest les In terms of RAM access besides the Manifest which might be reloaded into the RAM to take advantage of data locality some data blocks from the DiskChunks might also be reloaded into the RAM during hysteresis hash re-chunking The deduplication process of MHD is shown in Fig 4 is calculated and used for duplication detection using an inmemory bloom lter and cache The cache contains a number of Manifests each of which is organized as a hash table An incoming duplicate chunk is detected if its hash matches a Manifest in the cache When no such hit has been found would be checked in the bloom lter to determine whether it is a new value If the bloom lter indicates that the hash is probably duplicate MHD continues to query whether there is a duplicate Hook on disk When such a Hook is found the Manifest it points to would be loaded into the RAM If the cache becomes full during this process one Manifest would be freed following the Least-Recently-Used LRU polic y  A Manifest that has been set dirty is written back to the disk before it is freed After the above steps an incoming chunk detected as nonduplicate would be temporary buffered in the RAM until it is con“rmed as a non-duplicate after backward direction match extension Although the size of the DiskChunk can be arbitrarily large we only merge the non-duplicate chunks belonging to one le into one DiskChunk and write it to the disk Each DiskChunk has a corresponding Manifest each of which with at least one Hook that serves as the entry point to the Manifest The Hooks for a Manifest are selected by uniformly sub-sampling of the sequence of hash values in the Manifest The period for the sub-sampling is called the Sample Distance SD Since the hash values between two neighboring Hooks would not be directly accessed by the system they are replaced by a single hash calculated over the corresponding chunks of the hash values effectively merging these hash values into one value representing a larger chunk This process is termed Sampling and Hash Merging SHM In our implementation we set an in-memory chunk buffer with the size of two times of  When the buffer is full the rst half of chunks which can not be backward extended as duplicates would be written to the DiskChunk with two hash values created corresponding to the rst and the last chunks respectively It should be noted other SHM strategies also exist for example SHM can be performed on the contiguous non-duplicate chunks of the original input stream to guarantee each non-duplicate data slice of the input stream owns at least one Hook Fig 5 shows an example for SHM using 10 non-duplicate chunks with SD  5 hashes The original Manifest of 10 hashes to corresponding to to 
DiskChunkManifest Manifest 
002 010+\030\026\027\023\006\022 026\027\023\006\030\002\023\017\025\023 030\026\027\023\006\022 026\027\023\006\030\002\004.\006 030\026\027\023\006\022 036\036!\030\015\003\017 013\030\026\027\023\006 002 010+\030\026\027\023\006\022 017,\030/ \006\030\021\036\025\031\017\023\030\036\021\030\006\017\005 \030\006\020\023\025\027\030 004\020\030\032\004\022!\037 \034\020!\016\017\020\004\021\006\022\023 030/ \006\030\021\036\025\031\017\023\030\036\021\030\023 \006\030\006\020\023\025\027\030 004\020\030$\036\036 002 010+\030\026\027\023\006\022 026\027\023\006\030\002\023\017\025\023 030\026\027\023\006\022 026\027\023\006\030\002\004.\006 030\026\027\023\006\022 005,\030/ \006\030\021\036\025\031\017\023\030\036\021\030\006\017\005 \030\006\020\023\025\027\030 004\020\030\015\004\003\006\016\017\020\004\021\006\022\023 
013 010 011 7 8 014 012 013 9 032\004\022!\037 \034\020 016\017\020\004\021\006\022\023 036\036!\022 
h h h SD SD SD SD SD SD SD 
     
Fig 3 Formats of Metadata Fig 4 MHD process hash value and 2 the hysteresis hash re-chunking phase in which large chunks at the boundary of duplicate and nonduplicate regions are adaptively divided into smaller chunks Fig 2 shows the architecture of the proposed system The input to the system is the byte stream created by concatenating the content of the les in the unprocessed le system This byte stream is divided into chunks by the chunker and sent to the deduplicator which are optimized deduplicated by the system The outputs of the system include non-duplicate chunks saved in the DiskChunkStore the DiskChunkManifests the FileManifests used for reconstructing each of the les in the original system and the Hooks In the remainder of the paper the Fig 5 Example for SHM For each new chunk extracted from the input byte stream using the Rabin Fingerprint algorithm a SHA-1 hash value 
391 
391 
391 
391 
391 
391 


0\011 014 012 09 8 7 2\020\005\036\031\004\020&\030\037 \034\020!\022 2\020\030\026\034\021\021\006\025 
HitHash HitChunk HitChunk HitHash HitHash 
   
  
 are converted to a new Manifest containing 4 hash values after the chunks between and as well as between and are merged into two big chunks each with a single hash value When an incoming chunk is detected as a duplicate BiDirectional Match Extension would be performed on the Manifest We term the hash that has been hit on the Manifest as the and the incoming duplicate chunk as  For Backward Match Extension BME new hash values are calculated for the buffered chunk bytes before the and compared with the hash values for the corresponding byte before the in the Manifest until a mismatch is found If the old chunk represented by the mismatched hash which is not a Hook on the Manifest covers an edge between the duplicate and non-duplicate chunks in the buffer data from the old chunk would be loaded into the RAM from the disk for byte comparison followed by the Hysteresis Hash Re-chunking HHR process At least one hash named the representing the non-duplicate border chunk one hash representing the duplicate chunk\(s and one hash representing the remaining bytes of the old chunk would be re-calculated after the HHR In our system the EdgeHash is created to prevent the same duplicate data slice from triggering identical and duplicate HHR in the future We introduce byte comparisons to remove the duplicate data between two chunks with different sizes as the exact boundary in the big chunk is not knowm Each reloading of chunk bytes would one HHR process with the Manifest on which HHR is performed set dirty forcing it to be written back on to the disk as an update for the Manifest After BME a similar process called the Forward Match Extension FME is carried out in the forward direction New chunks would be pre-fetched into the buffer to calculate new hashes for hash comparison with the hashes after on the Manifest until one mismatched hash is found followed by the forward direction HHR Chunks that have been prefetched but not extended as duplicates during the HHR are again processed using the deduplication process as showed in Fig 4 The duplicate chunks detected via Bi-Directional Match Extension and HHR are removed from the buffer As shown in the example of Fig 6 the BME process stops at  as it is not a Hook and its byte size is larger than the total size of the three chunks before the HitChunk in the buffer The data of represented by are then reloaded from the disk into the RAM for byte comparison The buffered and are detected duplicate while is detected as non-duplicate after byte comparison Then is rechunked into a after duplicates in and are removed an EdgeHash representing the boundary data block with the same size of  and a representing the remaining bytes in the old are created After the FME and HHR is re-chunked into another three new hash values In MHD a new entry will only be written into the FileManifest at the terminating point of neighboring chunks of duplicate or non-duplicate data slices within one le or when a data slice reaches the end of the le IV A NALYSIS AND C OMPARISON In this section we present an analysis of the trade-off between the duplication elimination ratio and the deduplication overhead for the MHD and CDC algorithms such as SubChunk Bimodal and SparseIndexing In our comparison widely used deduplication acceleration methods such as the bloom lter and data locality preservation are considered implemented in the algorithms We consider the total amount of metadata generated and the disk access times required by different algorithms when the same duplication elimination ratio is achieved by all algorithms on the input data The sampling distance used by SHM in MHD is denoted as  while the expected big chunk size in the Bimodal and SubChunk algorithms is assumed to be  where is the basic expected chunk size so as to keep the deduplication ef“ciency of different algorithms at the same granularity Given an weuse and to denote the nal number of the non-duplicate and duplicate chunks respectively in all algorithms,regardless of how chunks are generated processed and stored by different algorithms The corresponding duplication elimination ratio can be roughly calculated as  We use to denote the number of detected duplicate data slices each of which is a sequence of continuous chunks in the input stream Let denote the nal number of the input les which are not completely duplicate which is also the number of Manifests We assume further that each Hook contains a 20-byte SHA1 address to the Manifest it belongs to each metadata le requires one inode for storage management each inode costs 256 bytes and each entry in the Manifest costs 36 bytes including the hash value byte start position and byte size of a chunk Each input le which cannot be deduplicated completely would generate one DiskChunks with a corresponding Manifest pointed to by one or more Hooks in all the algorithms analyzed In SubChunk the non-duplicate small chunks generated by dividing the same big chunk would be coalesced into one DiskChunk stored on the disk resulting in at least DiskChunks each of which has the maximal expected size of  Since each Manifest containing the small-chunk-to-container-chunk mappings would cover 
Fig 6 Example for HHR 
003\033\030\016\017\020\004\021\006\022\023 026\0166 015\0166 0\006;\030\016\017\020\004\021\006\022\023 
012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 012\013\014\015\016 
10 Chunk 2 Chunk 5 Chunk 7 Chunk 10 Hash 2-5 Chunk 6 Chunk 2-5 Hash 2-5 Chunk 4 Chunk 5 Chunk N3 Hash 2-5 Hash 4-5 Chunk 4 Chunk 5 Hash 3 Chunk N3 Hash 2 Chunk 2-5 Hash 7-10 
EdgeHash SD ECS SD ECS ECS N D D N N L F N/SD SD ECS 
017\020\021\013 010\006\022 017\020\021\013 023 017\020\021\013 007 017\020\021\013 025\006\007\026 017\020\021\013 027\006\022 017\020\021\013 023 017\020\021\013 007 017\020\021\013 025\006\030 017\020\021\013 011 017\020\021\013 007\026 017\020\021\013 010 017\020\021\013 031 
392 
392 
392 
392 
392 
392 


2 512 312 2 74 148 3    3 3 
2  2  
2  1 20 20 20 20 74  148 36 28 36 72  1 36 512  424 532  280 36 512  312  624  1 512  312  2 2 1       2 1  2 6   2 3   2 4 1  2 3 2 2 6  2 3  2 2 1  2 3  
N SD N D SD N SD N SD N D SD N SD 
  
    
Algorithms MHD SubChunk Bimodal CDC Inodes for DiskChunks Inodes for Hooks Bytes for each Hook Inodes for Manifests Bytes for Manifests summary TABLE I M ETADATA S IZE C OMPARISON SD 2 Algorithms MHD SubChunk Bimodal CDC Chunk Output Times Chunk Input Times 0 0 0 Hook Output Times Hook Input Times Manifest Output Times Manifest Input Times Big Chunk Query Times 0 0 Small Chunk Query Times Summary without Bloom Filter Summary with Bloom Filter TABLE II D ISK A CCESSING T IMES C OMPARISON multiple DiskChunks the entries for the small chunks belonging to the same DiskChunk in the Manifests need to share 28 bytes to indicate the address and the number of the chunks contained in the same DiskChunk Each Manifest is conservatively allocated with one Hook In Bimodal the big chunks adjacent to the duplicate data slices would be divided into small chunks Assuming all the resulting small chunks are non-duplicates the numbers of the big and small chunks will be 
N/SD L L SD F N N/SD N/SD N/SD L SD L F N/SD N D SD L L<D/SD 
and respectively Bytes from nonduplicate chunks belonging to the same le are stored in one DiskChunk Because each chunk big or small is represented by one entry in the Manifests as well as one Hook the number of inodes for the Hooks is identical to the number of chunks while the size of the Manifests is the number of chunks multiplied by 36 On the other hand the total size for metadata in the CDC algorithm can be calculated as bytes In MHD the number of inodes for the Hooks is and the number of entries in the Manifests is  Each entry each requires one more byte for the Hook ag resulting in a total of bytes plus bytes introduced by HHR We ignore the metadata for the FileManifests in the analysis of different algorithms as the amount is identical for all algorithms under our assumption A detailed quantitative comparison between the metadata needed for different algorithms are given in TABLE I From TABLE I it can be easily calculated that when is set suf“ciently high the amount of metadata required by MHD becomes much smaller than those required by the other algorithms The disk bottleneck is resulted from frequent disk accesses for duplication queries and metadata I/O during deduplication When a new hash hits a Hook on the disk the address of the Manifest will be read into the RAM followed by the entire Manifest which is also loaded into the RAM During this process three disk accesses are required triggered by every detection of duplicate data slice As a result the total number of disk accesses is  on top of disk accesses required for Manifest output as required by all the algorithms As shown in TABLE II SubChunk has at least DiskChunks to be written and requires disk look-up for big chunk duplication queries The Manifests in SubChunk record only the small-chunk-to-container-chunk mapping without preserving data locality between the big chunks This loss of data locality lead to additional disk accesses for duplicate big chunks We assume each duplicate data slice would also cause one Manifest loading in SubChunk in the worst case In MHD on the other hand for each HHR operation each duplicate data slice detected would produce three disk accesses two for reloading chunk bytes in Bi-Directional Match Extension and one for writing the updated Manifest back to disk for a total of disk accesses in the case TABLE II compares the disk accesses in detail Assuming the bloom lter eliminates all queries for non-duplicate hash values when  the number of disk accesses for MHD is lower than all other algorithms compared In SparseIndexing instead of being written onto the disk all sampled Hooks are buffered in the sparse index Different from the bloom lter sparse index is another in-memory data structure used for checking if a hash value is a duplicate If the sparse index indicates the hash is duplicate no con“rmation by disk look-up is needed Since the Manifests in SparseIndexing preserve data locality within the backup byte stream one hash may be recorded for multiple times if the corresponding chunk appears multiple times in the stream Because the Hooks in SparseIndexing are also sampled based on the Manifests each Hook may point to one or more Manifests Compared with SparseIndexing MHD generates fewer bytes for the Manifests 
at most worst 
F N/SD F F N/SD F N/SD L SD N F F F F N/SD L N N/SD N/SD L SD N F N/SD F N/SD N F N/SD L SD F N  F N/SD F F L N/SD F N/SD SD L N L L L L F L F F F L L L L N D SD N/SD N L N L SD L N L F L N F L N F SD L F L N F L F L F SD L F L N 
393 
393 
393 
393 
393 
393 


      
ECS SD ECS SD ECS SD ECS write ECS SD ECS ECS SD SD ECS SD SD ECS ECS ECS 
       
The Manifests will only retain the hash values for nonduplicate chunks and are utilized by SHM for deduplication Because the number of the Hooks produced in SparseIndexing are larger than in MHD using the same sample distance in the situation when a Hook stored in the spare index needs to be written to the disk more disk I/Os are needed for the Hooks than in MHD We dont give the quantitative results for SparseIndexing in this paper Comparing the duplication elimination ability of the algorithms the maximal sizes of the data blocks represented by a single SHA-1 hash in the MHD SubChunk Bimodal and CDC algorithms are   and respectively The actual metadata and throughput comparisons using realworld disk image data are given in the next Section In our analysis the I/O overhead is compared on the basis of the number of I/Os required without considering the amount of data accessed in each I/O V E XPERIMENTAL E VALUATION Experiments were designed to examine the following Utilization of metadata by MHD The best deduplication ef“ciency achieved by the algorithms compared The trade-off between deduplication ef“ciency and the amount of metadata required by the algorithms The trade-off between deduplication ef“ciency and deduplication throughput for the algorithms Whether the Hooks and Manifests generated by MHD t into the RAM Characteristics of the real-world disk images used in the experiments In the experiments MHD as well as the SubChunk Bimodal and SparseIndexing algorithms were implemented as prototype programs in the user space of the Ext3 le system In our simulations algorithms read data from and write the outputs to local directories The test dataset were taken from the disk image backups of a group of 14 PCs running the Windows Linux or Mac operating systems with NTFS FAT FAT32 Ext3 Ext4 or MFS le systems over a period of two weeks The size of the dataset was 1.0 TB The deduplication ef“ciency was measured using two numbers The is de“ned as the size of input data divided by the size of output data from the perspective of the le system including all metadata whereas the does not consider the metadata To measure the metadata overhead and the deduplication speed we de“ne the as the ratio between the size of the total amount of metadata generated by the algorithm and the size of input data and the as the time to pass the input data through the deduplication system without deduplication operation e.g by simply copying data divided by the time taken for deduplication A larger ThroughputRatio means a higher deduplication speed In this work the deduplication throughput refers to the throughput In order to measure the the duplication distribution of the dataset we de“ne the as the number of duplicate bytes divided by the number of the duplicate data slices The larger the DAD the more concentrated the duplicate data are For more accurately evaluating the throughput improvement achieved by the proposed algorithm we compared MHD with the improved bloom lter based Bimodal and SubChunk algorithms incorporating widely used deduplication acceleration methods including the bloom lter and data locality We note that the MHD algorithm can also be implemented in conjunction with the sparse index data structure in SparseIndexing In order to distinguish a sparse index based MHD implementation we denote the bloom lter based implementation used in the experiments the BF-MHD algorithm We set the parameters in accordance with those used in the analysis above for different algorithms so as to achieve similar granularities for deduplication by the different algorithms Speci“cally when the BF-MHD algorithm was con“gured by the parameters and  the expected sizes of small and big chunks in the Bimodal and SubChunk algorithms were set as and respectively We used an in-memory bloom lter of 100 MB for the Bimodal SubChunk and BFMHD algorithms In SparseIndexing the sample distance for the Hooks is set as based on the input the segment size was set to and the maximal allowable number of champions for each segment was set as 10 as in The maximal number of the Manifests one Hook can point to was set to 5 following the LRU cache policy The RAM used for spare index was not limited and reported in the experiments Fig 7 demonstrates the comparison for the amount of metadata required to achieve similar deduplication ef“ciency by the different algorithms with 1000 hashes and 512 1024 2048 4096 and 8192 bytes The HHR process of BF-MHD only increases the sizes of the Manifests but does not introduce new inodes whereas every time Bimodal performs re-chunking new inodes are introduced along with new bytes in the Manifest The number of inodes required per MB of input data v.s the is showed in Fig 7\(a in which the curves for BF-MHD and SubChunk overlapped while the curve of Bimodal was comparatively higher Since SparseIndexing sampled the Hooks based only on the input chunks but not on the non-duplicate chunks the curve for SparseIndexing was the highest The relationship between the number of bytes contained in the Hook and Manifest les normalized by the input data size and the is given in Fig 7\(b As expected SparseIndexing produced the most bytes for the Manifest and Hooks SubChunk produced the second most bytes due to the small-chunk-to-container-chunk mappings which recorded information for all small chunks The number of bytes for Bimodal was lower than SubChunk while BF-MHD produced the least bytes This is because the HHR process in MHD was 
 1 5   
real Duplication Elimination Ratio DER data-only DER MetaDataRatio ThroughputRatio Duplication Aggregation Degree DAD A Metadata Harnessing 
394 
394 
394 
394 
394 
394 


three new hash values In contrast in Bimodal the big chunks at the transitional points were broken up In addition the number of small chunks then generated which was more or less equal to ratio between the expected big and small chunk sizes was usually larger than 3 These two factors combined led to a much higher storage requirement for the Manifests Fig 7\(c shows the MetaDataRatio of the FileManifests as a function of the  BF-MHD attempts to represent contiguous data block in the same DiskChunk with one entry in the FileManifest therefore as can be seen BF-MHD generated the least bytes for the FileManifests Fig 7\(d depicts the overall MetaDataRatio combining bytes for the inodes the bytes contained in the Hook and Manifest and the FileManifest les The overall performance of the BFMHD algorithm was the best among the algorithms compared Fig 8 shows the comparison of the trade-offs between the DER and the amount of metadata required as well as the reduced deduplication throughput We can see from the gure that BF-MHD achieved the best real DER For the other algorithms the peak DERs were reached under various conditions It can be more clearly observed in Fig 8\(a which shows that the maximal metadata required by the SparseIndexing SubChunk Bimodal and BF-MHD algorithms were about 3.8 1.7 1 and 0.2 of the size of the input respectively Smaller values usually lead to more duplicate However the real DER taking into account the metadata generated will peak when the amount of metadata generated equals to the duplication in data found Fig 8\(a and Fig 8\(b show the data-only DER and the real DER as a function of the MetaDataRatio As expected more metadata lead to better dataonly DER for all the algorithms However when the amount of metadata is considered the rapid growth of metadata of the Bimodal SubChunk and SparseIndexing algorithms negated the increase in data-only DER resulting in decreasing real DER as the metadata continues to grow The Bi-Directional Match Extension and HHR process in MHD on the other hand contributed to elimination of duplicate data inside big chunks using less metadata and with new metadata generated only when duplicate data were found making the introduction of extra metadata more worthwhile Although SparseIndexing can nd more duplicate chunks 
at most always B Trade-Off between Deduplication Ef“ciency and Deduplication Overhead 
ECS ECS 
512 1024 2048 4096 8192 10 0 10 1 10 2 ECS \(Bytes Number of Inodes per MB BF\025MHD Bimodal SubChunk SparseIndexing 512 1024 2048 4096 8192 10 0254 10 0253 10 0252 10 0251 10 0 ECS \(Bytes Manifest MetadataRatio BF\025MHD Bimodal SubChunk SparseIndexing 512 1024 2048 4096 8192 10 0254 10 0253 10 0252 ECS \(Bytes File Manifest MetadataRatio BF\025MHD Bimodal SubChunk SparseIndexing 512 1024 2048 4096 8192 10 0253 10 0252 10 0251 ECS \(Bytes Total MetadataRatio BF\025MHD Bimodal SubChunk SparseIndexing 
a Number of Inodes vs ECS b Manifest and Hook Size vs ECS c FileManifest Size vs ECS d Total Metadata size vs ECS Fig 7 Metadata comparison ECS in Bytes SD=1000 hashes performed only when duplicate data might be found in a big chunk and with the corresponding hash value replaced by 
395 
395 
395 
395 
395 
395 


C RAM D Characteristics of the test dataset 
              values The DER of BF-MHD is determined by the value of  From Fig 9\(a we can see smaller led to better trade-offs between the real DER and MetaDataRatio This is because the growth rate of the metadata became very small as decreased while the size of the duplicate data detected rapidly increased Although smaller value of also resulted in more disk I/Os  overall the tradeoff between the real DER and the ThroughputRatio improved as decreased due to the much improved real DER as seen in Fig 9\(b TABLE III RAM USED FOR SPARE INDEX IN S PARSE I NDEXING SD=1000 HASHES  ECS Bytes 1024 2048 4096 8192 RAM KB TABLE III lists that the RAM space used for sparse index was about 0.01 of the input data size in our experiments consistent with the results in the work of Lillibridge et al TABLE IV shows that the size for all the Hooks and Manifests in BF-MHD was between 0.007 to 0.02 of the input data size If all Hooks and Manifests were stored in RAM the RAM space for the bloom lter and the disk accessing time for reading the Manifest from disk into the RAM showed in TABLE V could be avoided The BF-MHD algorithm requires less RAM which also helps relieving the disk bottleneck The characteristics of the test data affects the deduplication speeds of different algorithms The maximal data-only DER of our dataset is about 4.15 as detected by SparseIndexing For a comprehensive understanding of the dataset we used the DAD detected by BF-MHD versus is plotted in Fig 
SD ECS SD SD SD ECS SD SD ECS 
106618 102638 101183 100132 
  
0 1 2 3 4 3.4 3.5 3.6 3.7 3.8 3.9 4 4.1 4.2 MetaDataRatio Data\025only DER BF\025MHD Bimodal SubChunk SparseIndexing 0 1 2 3 4 3.4 3.5 3.6 3.7 3.8 3.9 4 MetaDataRatio Real DER BF\025MHD Bimodal SubChunk SparseIndexing 0.2 0.25 0.3 0.35 0.4 0.45 0.5 3.4 3.5 3.6 3.7 3.8 3.9 4 4.1 4.2 ThroughputRatio Data\025only DER BF\025MHD Bimodal SubChunk SparseIndexing 0.2 0.25 0.3 0.35 0.4 0.45 0.5 3.4 3.5 3.6 3.7 3.8 3.9 4 ThroughputRatio Real DER BF\025MHD Bimodal SubChunk SparseIndexing 
a Data-only DER vs Metadata             b Real DER vs Metadata                 c Data-only DER vs Throughput               d Real DER vs Throughput Fig 8 The comparison of trade-off between deduplication ef“ciency and deduplication overhead SD=1000 hashes because it does not regulate the the size of the Manifests as more Manifests need to be reloaded into the RAM and as the computational complexity related to computing the champions for each segment increases the real DER achieved became lower than BF-MHD even though the data-only DER for the two were similar Fig 8\(c and Fig 8\(d For the Bimodal algorithm duplicate data inside the deduplicated big chunks that were not at the transition points will be missed Similarly in SubChunk when one small-chunk-tocontainer-chunk mapping was not hit the duplicate data inside the big chunks covered by the mapping would be missed As a result for a given ThroughputRatio both the Bimodal and SubChunk algorithms provided the worst DER with and without taking the metadata into account Fig 9 shows the performance of BF-MHD for different 
396 
396 
396 
396 
396 
396 


136512 98340 83940 72996 174883 112160 90199 75897 226151 139154 103162 82067 4439 3289 3073 2985 5024 3390 3094 2996 5706 3530 3134 3009 
3 
512 768 1024 2048 4096 8192 0 50 100 150 200 250 ECS \(Bytes DAD \(KB DAD 512 768 1024 2048 4096 8192 0 2 4 6 8 10 x 10 6 ECS \(Bytes HHR Cost and Number of Dup. Slice HHR Cost Dup. Slice 
            
10\(a from which we can see the DAD of the dataset was between 90 bytes to 220 bytes Smaller helped to nd shorter duplicate data slices resulting in a smaller detected DAD As analyzed previously the performance of BF-MHD is related to the distribution of the duplications The more concentrated the duplication distribution is the fewer the disk accesses required by HHR Even though the theoretical upper bound for the number of disk assesses brought by HHR in the worst case is times the actual disk accessing times caused by HHR for our test data as shown in Fig 10\(b was much smaller than  the number of duplicate data slices detected VI C ONCLUSIONS In this paper we propose an in-line metadata harnessing deduplication scheme named MHD using hysteresis rechunking The SHM Bi-Directional Match Extension and the HHR modules of the algorithm lead to better overall real deduplication ratio with lower metadata and I/O related overheads and better throughputs as compared with other algorithms such as Bimodal SubChunk and SparseIndexing A CKNOWLEDGMENT We would like to thank Youyou Lu of Tsinghua University and our reviewers for their kindly helpful feedback 
ECS L L 
0.24 0.26 0.28 0.3 0.32 3.5 3.6 3.7 3.8 3.9 4 4.1 MetaDataRatio Real DER BF\025MHD\025SD\0251000 BF\025MHD\025SD\025500 BF\025MHD\025SD\025250 0.25 0.3 0.35 0.4 0.45 0.5 3.5 3.6 3.7 3.8 3.9 4 4.1 ThroughputRatio Real DER BF\025MHD\025SD\0251000 BF\025MHD\025SD\025500 BF\025MHD\025SD\025250 
a Real DER vs Metadata              b Real DER vs Throughput Fig 9 BF-MHD performance at different SD values SD 1000 500 and 250 hashes respectively             a DAD detected by BF-MHD vs ECS SD=1000 hashes             b The extra disk accessing times caused by HHR vs The number of detected duplicate data slice SD=1000 hashes Fig 10 Dataset characteristics and HHR cost statistics TABLE IV B YTE SIZE FOR ALL THE H OOKS AND M ANIFESTS IN BF-MHD ECS Bytes SD=1000 1024 2048 4096 8192 Size KB ECS Bytes SD=500 1024 2048 4096 8192 Size KB ECS Bytes SD=250 1024 2048 4096 8192 Size KB TABLE V D ISK ACCESSING TIMES FOR M ANIFESTS LOADING IN BF-MHD ECS Bytes SD=1000 1024 2048 4096 8192 Times K ECS Bytes SD=500 1024 2048 4096 8192 Times K ECS Bytes SD=250 1024 2048 4096 8192 Times K 
397 
397 
397 
397 
397 
397 


R EFERENCES  Sean Quinlan and Sean Dorw ard V enti a ne w approach to archi v al storage 2002 FAST08  John K ubiato wicz Da vid Bindel Y an Chen Ste v en Czerwinski P atrick Eaton Dennis Geels Ramakrishan Gummadi Sean Rhea Hakim Weatherspoon Westley Weimer Chris Wells and Ben Zhao Oceanstore an architecture for global-scale persistent storage 
 vol 35 no 11 pp 190…201 Nov 2000  Ka v e Eshghi and Hsiu K T ang  A frame w ork for analyzing and improving contentbased chunking algorithms in  2005  Athicha Muthitacharoen Benjie Chen and Da vid Mazi  eres A lowbandwidth network le system  vol 35 no 5 pp 174…187 Oct 2001  Jef fery C Mogul Y ee Man Chan and T erence K elly  Design implementation and evaluation of duplicate transfer detection in http Berkeley CA USA 2004 NSDI04 pp 4…4 USENIX Association  F ahad R Dogar  Amar Phanishayee Himabindu Pucha Olatunji Ruwase and David G Andersen Dittoa system for opportunistic caching in multi-hop wireless networks   no SI 2008  Siddhartha Annapureddy  Michael J Freedman and Da vid Mazi  eres Shark scaling le servers via cooperative caching Berkeley CA USA 2005 NSDI05 pp 129…142 USENIX Association  Cezary Dubnicki Leszek Gryz Lukasz Heldt Michal Kaczmarczyk Wojciech Kilian Przemyslaw Strzelczak Jerzy Szczepkowski Cristian Ungureanu and Michal Welnicki Hydrastor a scalable secondary storage Berkeley CA USA 2009 FAST 09 pp 197…210 USENIX Association  Ashok Anand Vyas Sekar  and Aditya Ak ella Smartre an architecture for coordinated network-wide redundancy elimination  vol 39 no 4 pp 87…98 Aug 2009  Cristian Ungureanu Benjamin Atkin Akshat Aran ya Salil Gokhale Stephen Rago Grzegorz Calkowski Cezary Dubnicki and Aniruddha Bohra Hydrafs a high-throughput le system for the hydrastor contentaddressable storage system 2010 FAST 10 pp 225…238  Jiansheng W ei Hong Jiang K e Zhou and Dan Feng Mad2 A scalable high-throughput exact deduplication approach for network backup services may 2010 pp 1 14  Benjamin Zhu Kai Li and Hugo P atterson  A v oiding the disk bottleneck in the data domain deduplication le system Berkeley CA USA 2008 FAST08 pp 18:1…18:14 USENIX Association  Mark Lillibridge Ka v e Eshghi Deepa v ali Bhagw at V inay Deolalikar  Greg Trezise and Peter Camble Sparse indexing large scale inline deduplication using sampling and locality Berkeley CA USA 2009 FAST 09 pp 111…123 USENIX Association  Deepak R Bobbarjung Suresh Jagannathan and Cezary Dubnicki Improving duplicate elimination in storage systems  vol 2 no 4 pp 424…448 Nov 2006  Erik Kruus Cristian Ungureanu and Cezary Dubnicki Bimodal content de“ned chunking for backup streams Berkeley CA USA 2010 FAST10 pp 18…18 USENIX Association  Guanlin Lu Y u Jin and D.H.C Du Frequenc y based chunking for data de-duplication in  aug 2010 pp 287 296  Romanski Bartlomiej Heldt Lukasz Kilian W ojciech Lichota Krzysztof and Dubnicki Cezary Anchor-driven subchunk deduplication New York NY USA 2011 SYSTOR 11 pp 16:1…16:13 ACM  Niraj T olia Michael K ozuch Mahade v Satyanarayanan Brad Karp Thomas Bressoud and Adrian Perrig Opportunistic use of content addressable storage for distributed le systems in  2003 pp 127…140  Michael O Rabin Fingerprinting by random polynomials  T ech Rep TR-15-81 Center for Research in Computing Technology Harvard University 1981  Nohhyun P ark and D.J Lilja Characterizing datasets for data deduplication in backup applications in  dec 2010 pp 1 10  W  Lee and C P ark  An adapti v e chunking method for personal data backup and sharing in  2010 FAST10  D Bhagw at K Eshghi D.D.E Long and M Lillibridge Extreme binning Scalable parallel deduplication for chunk-based le backup in  sept 2009 pp 1 9  Andrei Broder and Michael Mitzenmacher  Netw ork applications of bloom lters A survey in  2002 pp 636…646  Andre w S T anenbaum Structured computer or ganization  chapter 4,THE MICROARCHITECTURE LEVEL Prentice Hall 5 edition June 25 2005  Dirk Meister Andre Brinkmann and Tim S File recipe compression in data deduplication systems 2013 FAST13  Andre w S T anenbaum Modern operating systems  chapter 4,MEMORY MANAGEMENT Prentice Hall PTR SECOND EDITION 2001 
SIGPLAN Not Tech Rep HPL-200530\(R.1 Hewlett Packard Laboratories Palo Alto SIGOPS Oper Syst Rev Mobicom SIGCOMM Comput Commun Rev Trans Storage Modeling Analysis Simulation of Computer and Telecommunication Systems MASCOTS 2010 IEEE International Symposium on In Proceedings of the 2003 USENIX Annual Technical Conference Workload Characterization IISWC 2010 IEEE International Symposium on Proceedings of the 8th USENIX conference on File and storage technologies post session Modeling Analysis Simulation of Computer and Telecommunication Systems 2009 MASCOTS 09 IEEE International Symposium on Internet Mathematics 
398 
398 
398 
398 
398 
398 


  unallocated planning would be required Finally, in the third interface phase, the O P installed and ready to operate on its ELC w its full suite of interfaces available. They i n avionics and software interfaces, which operations to commence OPALS will be the first payload installed o Even with a standardized specification means many questions, capabilities, and i addressed for the first time in p arallel w development and in collaboration with I engineers Safety Interface JPL engineers regularly design complex sent far into space never to interact wi t launch.  As a result, the human safety c o OPALS responded to have introduced n e every aspect of the design.   This experie n only learning how to navigate the pro approval from the technical experts on th e Review Panel \(PSRP\ the design, but solve engineering problems from a safety p not compromising technical goals The formal interaction between the pay l such as the OPALS project, and the PSRP series of Flight Safety Reviews \(FSRs  reviews the project and the panel iden t hazards created b y the payload and dec controls to keep those hazards from causi n either personnel or the ISS.   A safety submitted 45 days prior to a formal FSR review and comment. The submission i s presentation and hazard report walk-thro u takes a full day or two as all safety concer n are discussed and addressed The first meeting with the PSRP was a which focused on identifying applicab verifying that the appropriate requirem e referenced.  OPALS started at a time whe Figure 12: OPALS project milestones re l 11 P ALS FS will be w here it will have n clude electrical will enable full o n this new ELC a new interface i ssues have been w ith the OPALS I SS and SpaceX systems that are t h humans after o nsiderations that e w challenges in n ce involved not cess of gaining e Payload Safety also learning to p erspective while l oad developers occurs through a   th es e t ify all potential ide on a set of n g any damage to data package is for the PSRP to s followed by a u gh, which often n s and comments Phase 0 review le hazards and e nts were being n documentation was transitioning from shuttle req u documents so the initial meeting which standards are applicable to t h initial hazard assessment with th e some potential hazards the proj e Additionally, it helped guide th e choices that later could have sno w would have been difficult to progressed toward the more formal OPALS\222 Phase 1 review was s e project\222s PDR and reflected a fair time leading up to this review the p class 4 laser that required the h classification \(catastrophic\and i controls for all possible operation a the safety review OPALS discuss e with various members of the PRS P design approaches for controls r hazards from actualizing.  These meetings were essential leading i n the design then required minimal c h 1 review. This preparation had t h developing a strong relationship b members and the OPALS team The Phase 2 review typically fol l Design Review \(CDR\OPAL S formal technical interchange meeti n to review its hazards and solid i method of controlling them. With identified and most of the contro l debate, the Phase 2 review covere d testing and the supporting doc u p articularly strong focus on veri avionics and the hazard controls computer system controls a haz a Control System \(CBCS\trix m outlines the requirements for that meets the requirements, and the demonstrate complian h e into the high-level software archite level registers and memory man a reviewed by the Computer Safety l ative to the safety milestones outlined in NSTS 13830 and Data Submittal Requirements u irements to ISS-specific focused on establishing h e OPALS payload.   The e PSRP helped identify e c t had not considered e project\222s early design w balled into hazards that change as the design Phase 1 review e veral months after the l y mature design. In the p rojec t settled on using a h ighest level of hazard i dentified a number of a l scenarios.   Going into e d these choices in depth P to identify options and r equired to prevent the earlier discussions and n to the formal review as h anges to pass the Phase h e additional benefit of b etwee n the safety panel l ows a projec t 222s Critical S however, requested a n g \(TIM\with the PSRP i fy concurrence on the the hazard sources well l s requi r ing little further d the details of hardware u mentation and had a fication of the OPALS it governs.   When a a rd, a Computer Based m ust be developed that syste m the design that verifications planned to completed matrix delves cture as well as the lowa gement. This system is Panel \(CSP\ and if the Payload Safety Review 


  12 panel chair agrees with the approach he or she recommends it for approval to the PSRP. After many tweaks and focused discussion outside of the full PSRP the OPALS CBCS matrix was approved for Phase 2 As Phase 3 approaches, all agreed-to verifications of hazard controls must be performed, documented, and submitted Any deviations from the agreed-to verifications must be documented and approved by members of the PSRP In addition to the Flight Safety Reviews \(FSRs\und Safety Review schedule is also negotiated and a package is submitted.  This process is very similar to the PSRP one, but the scope of the hazards is limited to ground operations at the launch and processing facilities The key to successfully passing the FSRs has been constant early, and open communication with the PSRP. It has allowed for the quick identification of hazards and it has prevented the project from locking itself into designs that either could have adversely impacted safety or could have made compliance significantly more difficult. The PSRP and everyone involved in the safety side of the interface have shown great flexibility in allowing OPALS design freedom to ensure its mission success while enforcing a high standard for the safety of the ISS and those inhabiting it 5  D RIVERS   C ONSTRAINTS  AND R ESULTING I MPLEMENTATION  OPALS is no exception when it comes to facing the typical challenges that most space flight projects have to overcome throughout their lifecycle. Cost and schedule always seem insufficient. The workforce is sometimes stretched thin. All these challenges create constraints, end up driving the decision-making process and inescapably shape the final product. There are, however, many aspects that set OPALS apart from the average space borne instrument, and they too came with their own drivers and constraints Overall, the drivers and constraints can be classified as internal and external, with each category further divided into process- or technically-flavored This section is dedicated to examples, spanning each of those categories, of drivers and constraints that have in one way or another influenced the implementation of the project Internal, Process-Derived Drivers and Constraints The aspects falling under this section refer to those whose common denominator is the internally-mandated JPL process. The most influential and prevailing of these processes are documented in the JPL Design Principles and JPL Flight Project Practices. These documents provide guidelines on everything from cost, schedule reserves and technical margins to the pedigree of the parts and components used by the flight article and the level of documentation required Class D 227The biggest impact on the project, second only to the stringent cost cap, was the decision to implement it as a Class D payload risk classification per NASA NPR 8705.4 This decision was less of a constraint than it was a driver and it ultimately gave the project more flexibility in making decisions. The ramifications of this decision are many including the risk posture of the project in parts selection sparing philosophy, hardware handling requirements qualification program, fault protection, and extent of formal verification of requirements Early on in its lifecycle, the project sought to define the Class D implementation approach within the guidelines set by NASA procedural requirements. Specifically the project was allowed to 200  Build one proto-flight model with limited engineering models \(EMs\he only subsystem to have a full EM is the avionics. Other EMs include the acquisition camera one motor driver, and the in-house developed power board 200  Limit parts sparing to long-lead items only. The project only purchased one spare actuator and a few of the avionics cards. There are no spares for the laser, power board, camera, or any structural components 200  Use COTS hardware wherever possible. This aspect is discussed in further detail in a subsequent section 200  Forgo radiation or parts support. The extensive usage of COTS components implies, to a large extent, the absence of radiation susceptibility data, along with other environmental information. Thus, as early as the Mission Concept Review, the project and its stakeholders have accepted the potential missionending risk of unrecoverable single event upsets 200  Limit hardware and software Quality Assurance prior to system level integration and test. Typically, HQA/SQA involvement begins early in subsystem development well in advance of system I&T 200  Limit flight tech support to final torqueing and high risk procedures. The philosophy here is similar to the HQA/SQA involvement Requirements and V&V\227 Likely the most unglamorous aspect of producing a space-wo rthy payload is developing the set of requirements that govern its function and behavior. While Class-D status does not give the project a pass on developing requirements, it can shape the way the process is conducted and the extent to which requirements are developed For example, higher-class projects typically develop requirements down to level five or six, and bin them in dozens, if not hundreds of sets or modules. By contrast OPALS currently only has 782 requirements spread over 33 modules, with none of them containing more than 105 items. Of the 694 ISS requirements only 376 are applicable to OPALS, accounting for 48% of the total OPALS set Similarly, of the 117 launch vehicle requirements, only 58 are levied on OPALS. This means that the project has only 


  348 project-specific requirements that it in t verify. Figure 13 breaks down the requirements, including ISS and launc h residing at the various levels OPALS, however, is no poster child f development as it fell into the same trap t h projects\227regardless of size and risk verification and validation \(V&V\ time: it unnecessary requirements At CDR time OPALS realized that requirements it was signing up to verify d and testing, totaling almost 1300 \(796 OP A LV\, had become unmanageable give n schedule and cost-to-go. Shortly there a embarked on a months-long, projectevaluating its requirement set with the go a down to those that it couldn\222t justify not ve r By the time the Test Readiness Review c a later, the project had achieved an overall r Table 2\n the number of items it dee m verification Once again leveraging its Class D status from JPL management, the project decided 796 requirements it had written on itself 227 the ISS and launch vehicle ones\227 p rovide d others when it came to verification D explicitly verify a requirement meant one o f 1  The statement was deemed to be, in of a requirement and more of proced u on how something was to be done less of an instruction on how well s work and more of a documentatio n Without engaging in a detailed sem a the project changed the status of thes e requirement to policy. This meant t h still had to be implemented, but it di formally verified Figure 13: Two-thirds of the 782 OPAL S reside at the level 4 FS subsystem level the ISS and launch vehicle requi r 13 t ends to formally distribution of h vehicle ones f or requirements h at cripples many posture\227come developed many the number of d uring integration A LS + 529 ISS n the remaining a fter, the team wide effort of a l of trimming it r ifying explicitly a me nine months r eduction of 37 m ed important for and with bu y in that some of the 227 which excludes d less value than D eciding not to f two things some cases, less u ral specification while in others omething had to n on the design a ntics discussion e statements from h at the statement i d not have to be 2  The statement remained a r e not be explicitly verified at t written as the state-ofp r a Instead, its verification wou l parent at a higher level was order for a requirement to be a higher level it had to meet o criteria 200  It is enveloped by a larg e at a higher level; e.g flowdown, such as mass 200  Functionality/performan c higher level test\(s\ A N requirement will be app level tests. Put another cannot be verified at a h i meet that requirement w o in higher level test, the n requirement cannot be m o This process was a very time-int e number of individuals on the te thought to have saved the project a s and money in Phase D when the m a takes place There were, however, two excepti o applied to externally-levied IS S requirements, and those requirem e safety standards to be followed It is important to point out that the 421 requirements which were e v verification via this process shoul d but rather that the mission risk o f  one the project was willing t o underperformance of the system higher level testing. In doing so h had to accept an increased cost p roblems that might have been ca u caught until later during syste m significantly more expensive to fix S requirements These include r ements Table 2: Reduction achieved bet w number of requirements needi n Set  CDR  L1  1  L2 \(all  283  L3 \(all  216  L4 GS  66  L4 FS IRDs  158  L4 FS STR  188  L4 FS FSW  167  L4 FS Other  218  OPALS TOTAL  1296  ISS Total  468  LV Total  59  e quirement, but it would t he level at which it was a ctice usually dictates d become implicit if its successfully verified. In allowed to be verified at o ne of the following two e r \221capstone\222 requirement part of an allocation pointing budgets e can be rolled into N D failure to meet the arent in higher, syste m way, if a requirement i gher level OR failure to o uld not become apparent n the verification of that o ved to a higher level e nsive one for a limited am, but it is generally s a whole significant time a jority of the V&V effort o ns to this process which S and launch vehicle e nts which specified the argument is not that the v entually descoped from d not have been written f not verifying them was o accep t because any would be identified in h owever, the project also and schedule risk that u ght earlier, would not be m I&T when they are  w een CDR and TRR in n g explicit verification TRR  Change  1  0  230  19  138  36  28  58  61  61  101  46  109  35  158  28  823  37  411  12  39  34  


  Internal, TechnicallyD erived Drivers and C Equally instrumental in shaping the outco m were those decisions that were driven b aspects over which the project had decisio n In the same way that certain policy de c subsequent ones \(see Class D\ techni c a cascading effect throughout the design, p a Flight System COTS Usage\227 The most clear and pervas i the decision to utilize COTS parts whereve of the early conceptual architecture and des i was inherited from aircraftb orne syste m operated by JPL as recently as 2011.  In a n cost and complexity, the use of COTS c considered an attract ive option for the avio n risk of latent failure and unfamiliarity w i was essentially traded against the extrem e short lead times made possible by orderin g While the FS would evolve over time t o custom JPLb uilt boards and compon e implementation has left an indelible mark o n Traditional spacecraft and instrument elect r on strong conductive coupling to a temp e interface to maintain safe operat i Ruggedization is also common practi reliability in the vibration- and r a environments of launch and low-Ear t p ractices, however, are often impractical o r with COTS avionics, where the cost o ruggedizing for flight would surely dwar f components themselves. The OP A implementation, for example, comprises a and electrical components that are desi g forced-convection cooling in a r o environment. They have relatively little or Figure 14: Cross-sectional view of fligh t container showing arrangement of c 14 C onstraints m e of the project b y the technical n making control c isions impacted c al decisions had a rticularly for the i ve example was e r possible. Much i gn of the project m s designed and n effort to reduce c omponents was nics design \226 the i th the hardware e ly low cost and g 223off-the-shelf.\224 o include several e nts, the COTS n the FS design r onics boxes rely e rature-controlled i ng conditions ce to improve a diation-intensive t h orbit. These r even impossible o f modifying or f the cost of the A LS avionics series of boards g ned to receive o o m temperature no tolerance for extreme temperature, humidity, o r thus present significant technical c thermal control The ISS in particular poses an ad d and constraints owing to the n a payloads are installed and opera t docking at the ISS, OPALS is rem o SpaceX Dragon trunk.  During thi s p oints throughout the subsequent r o attach site on ELC1, the FS must a loss of power for up to six h architecture is therefore built maintaining a \223laboratory envi operations.  The flight avionics su b critical components like the l a distribution circuitry, is conta i cylindrical enclosure that maintai n environment throughout the nomin a highlighted in Figure 14, 120mm D found in everyday desktop compu t the air across the avionics compon e heat exchanger assembly.  By rec fashion, waste heat is transferre d avionics boards and components, t h and into the heat exchanger where i external radiator.  Further, the loss from a loss of electrical power to effective means of insulating the loss during said periods \226 gas c dominant, though negligible, mod e microgravity environment p rovi d natural convection The decoupling of heat transfer structural interfaces renders stan d finite volume heat transfer c necessitates an iterative anal y CAD/CAE and computational flui d to solve for characteristics of the fl u of the OPALS avionics enclosure  envisioned a cuboid structure wit h heated air from the electronics assembly.  Early rounds of CFD a n unacceptable pressure losses in t h turning and relatively narrow cro s the duct.  Over time, the iterate d streamline the flow path, result i b etween the fans and heat exchan cable routing to eliminate as ma n main flow path as possible. This i t results in a design that is com p cheaper to analyze, as well as more build. Figure 16 shows a velocity final design The relatively severe vibration e launch vehicle posed yet another avionics \226 most of which do  t system sealed c omponents r vacuum operation, and hallenges in maintaining d itional set of challenges a ture in which external t ed.  Upon arrival and o ved robotically from the s process, and at various o botic transfer to its final a ccommodate a complete h ours. The OPALS FS around creating and r onment\224 for nominal b system, along with other a ser diode and power i ned within a sealed n s a 1-atmosphere \(STP a l life of the payload.  As D C fans, not unlike those t ers, drive circulation of e nts and through a ducted irculating the air in this d convectively from the h rough the air mass flow i t is then conducted to an of circulation that resul t s the payload provides an flight system from heat c onduction becomes the e of heat transfer, as the d es no mechanism for paths from traditional d ard finite element and c odes ineffective, and y sis process between d dynamics \(CFD u id flow.  Early concepts  Figure 15\or example h a long duct to transfer to the heat exchanger n alysis, however, showed h e flow due to repeated s s-sectional areas within d design has evolved to i ng in a single \223turn\224 g er, and optimization of n y obstr u ctions from the t erative process naturally p utationally simpler and practical to fabricate and map of fluid flow in the e nvironment within the challenge for the COTS not even ship with a 


  15 manufacturer specification for vibration tolerance.  To deal with this issue, the rigid tray assembly that houses the avionics and fans is mounted on a system of wire-rope isolators designed to dampen random vibrations within the specific frequency spectra produced by the launch vehicle These isolators protect delicate solder joints and other nonrugged components from being torn apart by launch loads Further, in much the same way as the loss of circulation during power outages provides a natural means of insulation against heat loss, the wire rope isolator system\222s poor thermal conductivity also impedes conduction to the outer structure of the sealed container External, Process-Derived Drivers and Constraints While many decisions were internally motivated by the JPL processes and the OPALS paradigm, a significant number also came via the ISS and launch vehicle interfaces, both programmatically and technically As one might anticipate, the ISS interface in particular impacted the OPALS day-to-day operations as well as the design features it implemented. The most clear example of this aspect came via the over 400 requirements imposed by the ISS program on the OPALS payload, followed by the V&V process mandated to demonstrate compliance Requirements 227As one of the first payloads to negotiate both the ISS and Dragon interfaces, OPALS has been routinely involved in technical changes beginning and ending with its interface requirements.  A considerable ongoing effort within OPALS and its partners has been to solidify the understanding of the requirements levied When OPALS began in 2009, the Dragon had neither flown nor been fully designed.  This was best represented in the miniscule 16 provided requirements that were largely placeholders for what would  become the Dragon\222s interface specification.  At that point, expectations were that OPALS would be completed well before its Dragon flight. This schedule disconnect led to a considerable technical disconnect In order to proceed with OPALS development despite the lack of a full set of design requirements assumptions had to be made in three key areas: power, connectivity, and the launch environment.  The first question of power came about for an obvious reason: OPALS has thermally sensitive optics and commercial equipment, and these components need heater power to be kept warm in space.  Without requirements on Dragon power provisions, the project\222s first assumption was that the thermal control hardware designed for the on-orbit ISS environment would also prove valid during a Dragon ascent.  After exhaustive integrated thermal analysis, this assumption, and the resulting engineering implementation, has proven to be valid, and an additional 12 pages of requirements and supporting data now reflect this interface design After requirements were negotiated and an agreement was reached to provide sufficient thermal power during ascent the question remained of how this power would be routed into the OPALS payload.  To direct the efforts towards a solution of connectivity, OPALS turned to the JAXA H-II Transfer Vehicle \(HTV\d Space Shuttle designs that powered FRAM-based payloads similar to OPALS.  These additional carrier requirements allowed several connectivity options and it was thus taken to be a valid assumption that the Dragon team could design around OPALS\222 desired cabling pin assignments.  The OPALS design was thus submitted to SpaceX and, as this was the first request for a specific option, the OPALS connectivity solution has become the primary power distribution option amongst external Dragon cargo requirements The project\222s greatest challenge, however, was the need to design the FS prior to receiving any validated launch  Figure 15: Early design of the sealed container did not optimize component pl acement or airflow  Figure 16: CFD analysis led to a new component layout that allowed for more efficient heat removal 


  16 loading requirements.  To address this, OPALS began with JPL and NASA\222s extensive historical data on expected launch loads, amended with ample margin and overlaid with standard levels on workmanship. The resulting crude spectrum was then supplied to the ISS and SpaceX dynamics teams who over the course of several months helped refine test plans to a conservative but agreeable level, which again proved to be valid \(and indeed conservative\via integrated analysis and testing. At the time of this writing OPALS had successfully completed vibration testing Lastly, the project has dealt with the uncertainty that comes via requirements variability from the ISS interface. Since OPALS began, 37% of the 694 ISS interface requirements have changed. Even though only 376 of them are applicable to OPALS, the project has had to maintain cognizance in this regard over the full set.  Despite the fact that the ISS is an operational facility and many components have been flying in orbit for several years, both OPALS and the ISSP are still learning about the function and performance of these components.  As such, there have been ample updates to the electrical and robotic interface requirement segments These requirement changes are executed methodically in a thoroughly reviewed manner.  After internal ISS coordination, pending requirement updates are quickly disseminated amongst payload developers.  After external review by existing and prospective payloads, signatures are requested and requirements are enacted V&V 227Verification and Validation includes the final work done to certify both that the payload meets the word of the requirements, and that it performs the intended function 55% of OPALS\222 requirements to be verified are provided by its external interfaces, including the ISS and Dragon Though verified along with all other project requirements these interface requirements demand a different balance of focus.  As these requirements are provided by external entities they are managed outside of JPL and subject to external verification.  OPALS, therefore, has no formal validation program for these requirements beyond ongoing interface systems engineering diligence.  While this externalized approach minimizes internal efforts, it simultaneously increases verification  overhead Verification of each external interface requirement falls under the cognizance of an external Discipline Engineer DE\d the OPALS verification program is built around this external approval process, which is diagramed in Figure 17.  These external engineers represent the external hardware expertise necessary to ultimately understand validate and approve all interactions with their hardware.  It is important to note that these engineers support a multitude of projects across geographic boundaries, and their ability to provide dedicated attention to OPALS is attenuated by the breath of the projects they support Internal to OPALS, an abundance of verification data is generated for these DEs in the verification of the 452 external requirements.  The planning, technical coordination, and direction of this work is coordinated between the OPALS External Interface Engineer and the cognizant engineers within OPALS. As data and plans have been generated over years of interaction among these organizations, it is incumbent upon the EIE to ensure internal consistency and remedy problems across evolving interfaces External, Technically-Derived Drivers and Constraints The ISS-imposed requirements provide the specification for the physical interface to the station, and, via the PSRP process, also provide direction on meeting the safety considerations that come with the Class A status of the ISS This section will highlight how technical specifications from the ISS impacted aspects of the OPALS design Mass & Power Budgets 227Most payloads will find the ISS allocation for mass and 28V operational power to be quite generous at 490 lbs. \(excluding the top half of the FRAM\227 the ExPA plate\d 504 watts, respectively. The ISS also provides 750 watts on a 120V operational line and an additional 300 watts of power on a 120V heater line Expected to come in under 400 lbs., OPALS fits well within its mass allocation. In fact, at no point in its development did the OPALS mass margin\227calculated as \(AllocationCBE\llocation\227fall below 15%, and since CDR it has been hovering around 30%. The 28V ops power margin has been even more generous, hovering at or above 60% for much of the projects lifecycle, while the 120V ops and 120V heater lines have seen equally generous margins of over 60% and 70 %, respectively In space, mass and power are drivers more often than not For OPALS the opposite has been the case, to the point where the excess allocations has been used to solve problems in other areas. One such example has been the  Figure 17:  The OPALS ISS requirement verification process is built around th e ISS approval process 


  design of our gimbal. At nearly 70 lbs., th e to be severely overdesigned for the 3 lb supports. However, the need for such a b forced by the early selection of the actuat o long-lead items and had to be large enou g the optical head in the absence of launch lo time, by the yet unknown design of the opt under certain configurations, could have b e lbs. Knowing that mass would likely not p roject was thus able to save cost by avo the mass of the gimbal while at t accommodating an unknown optical hea d lack of launch locks The power draw of the electronics was concern since one of the benefits of havin g is that their power draw is well known a n fluctuate greatly over time. If there was o n p ower draw required more active manage m 120V heater line which is shared by the IS S launch vehicle, of which the latter had si stringent and fluctuating requirements Designing to a Class A Interface 227Of all t h interface, the safety-related ones likely impact on the OPALS design. There are features that owe their existence to the nee d the two-fault tolerant design against cat a mandated by the PSRP. The payload h a combination of three hardware and softwar each of the two catastrophic hazards that w e the inadvertent usage of the 2.5 watt clas s shining the laser outside of its approved a r The hardstops limiting the travel of each a x constitute two of the inhibits required to pr e Via a negotiation process with the PSRP, t h upon the area around the ISS where it wo u Figure 18: OPALS Field of Regard \(red  Motion \(yellow\ver the O C station 17 e gimbal appears optical head it b eefy design was o rs\227which  were g h to immobilize cks\227and, at that ical head, which e en well over 10 be an issue, the iding optimizing t he same time d design and the even less of a g COTS avionics n d is unlikely to n e area where the m ent was on the S and the SpaceX gnificantly more h e aspects of the had the largest a number of FS d to comply with a strophic failures a d to include a e inhibits against e re identified: \(1 s 4 laser, and \(2 r ea of operations x is of the gimbal e vent the latter h e project agreed u ld be allowed to p oint the laser, referred to as the From this not-to-exceed area, the p allocation to allow for uncertai manufacturing. Using these derate gimbal to keep the laser within Figure 18\ela t nadir\he ROM extends from a p in the gimbal azimuth axis  approximately 1 373 to 38 373 in the crosstrack\erally speaking range limits the length of each pas s while the size of the elevation frequently a pass occurs. This is mission success is at odds with m yield to it. Were it not for the ne e operation of the laser due to safet y would have enjoyed longer and m o improving its chances of meeting it s In addition to the hardstops, meeti n mandate requires that the des i microswitches, two relays, soft w p rotection, and operational const r with the Huntsville Operations Sup p and function of these inhibits were d MOS Interface 227The main drive r implementation was the availabili t operations tools, processes, and in fr payloads. Although drivers are so m negative impact on development opposite was true. The OPALS leverage existing capabilities as mu development of new tools and proc e Several remote operations tools ar e from MSFC and JSC for telemet r management, schedule viewing, tel e over-IP. Table 3 details the a v function. The OPALS MOS has i tools with only a minor amou n required. Given the wide array of t responsibilities of OPALS MOS simplified to development of the ephemeris processing, generati o generation of telemetry and comm a decision-making and execution  analysis In addition to tools, OPALS has al s great deal in the area of operati o depicts the interface support pr o Marshall Space Flight Center and Directorate \(MOD\e Johnso n  general, MOD handles the overall with tasks including crew operat i orbital reboosts, docking activiti e MOD defines its interface with O P Rules that govern power, attitu d restrictions on payload activity T   and Range of C TL ground Field of Regard \(FOR r oject further derated the n ties in modeling and d values it designed the the expected Range of t ive to the ISS +Z axis p proximately -34 373 to +73 373   alongtrack\d from gimbal elevation axis the size of the azimuth s over the ground station range determines how a clear example of how m ission safety and must e d to restrict the area of y considerations, OPALS o re frequent passes, thus s level one requirement  n g the two-fault tolerance i gn also include four w are ar m fire command r aints to be coordinated p ort Center. The purpose d iscussed in sectio n 3  r for the OPALS MOS t y of significant existing fr astructure for ISS-based m etimes seen as having a in this case quite the MOS has attempted to u ch as possible to avoid e sses and thus save cost e available free of charge r y processing, command e metry query, and voicev ailable tools and their i ncorporated all of these n t of tool configuration t ools available, the prime development has been operations concept, ISS o n of file products a nd databases, operations  and troubleshooting s o been able to leverage a o ns processes. Figure 19 o vided by the POIC at the Mission Operations Space Flight Cente r In ISS platform operations i ons, robotic operations e s, and attitude control P ALS via a set of Flight d e, trajectory, and crew T he POIC, on the other 


  hand, provides the infrastructure to commands and file products to the paylo a infrastructure to receive and relay real-tim e the payload. The agreements between the POIC to ensure proper operation, m monitoring of the payload are docume n Regulations. In addition, the POIC maintai n p rocess for planning payload activities a n those activities into the larger ISS schedule Activity planning and scheduling begins 8 launch and continues up to 7 days before t h it is included in the ISS Short Term Pl OPALS operations are time critica l commanding by members of the POIC O will be designated with priority over activities in the STP. During real-time Payload Operations Director at the PO I interface for the OPALS payload and provi d inputs for activity coordination with t h Director. The expectations of the payload d it will provide a clear set of science object i set of data and planning requirements need e those objectives. For OPALS, these documented in the OPALS Ops Conce p delivered to the POIC and MOD 18 mo n anticipated launch date. It is also expected operations team will submit a formal t r participate in training exercises with the P other ISS payloads. The POIC offers two f l activities: scenario training and simulation t Scenario training exercises MOS execu t specific processes while simulation trainin g familiarity with ISS interfaces, coordinatio n MOD entities, and decision-making in t h anomalies. The scenario activity resembles operational readiness test \(ORT\ in Table 3: Existing operations t o Tool Operations F Telescience Resource Kit \(TReK Telemetry Proc e Telemetry Disp l Telemetry Data b Command Man a Command Data b Enhanced HOSC System \(EHS Telemetry Que r Database Sync h HOSC Networ k Internet Voice-over-IP IVoDS Voice Loops Payload Information Management System PIMS File Transfer Operations Cha n On-board Short Term Plan Viewer \(OSTPV ISS Schedule V 18 relay real-time a d as well as the e telemetry from payload and the m aintenance, and n ted as Payload n s a well-defined n d incorporating  8 months before h e activity, when an \(STP\ince l and require O PALS activities conflicting ISS operations, the I C is the prime d es the necessary h e MOD Flight d eveloper are that i ves along with a ed to accomplish prod u cts were p t document and n ths prior to the that the payload r aining plan and P OIC, MOD, and l avors of training t raining t ion of payloadg exercises MOS n with POIC and h e event of ISS what is called an the JPL flight development process, while the si m to the ISS program due to the diversity of the ISSP interfaces. F members are available on conso l OPALS MOS in a flight-like oper a role of the Flight MOS and Grou n react appropriately to nominal and relayed by the POIC over the voic e expected to obey all Flight Rules a in the execution of their acti v p roficiency with all operations pr o OPALS and external \(e.g. ISS 6  C ONCLUS I The development of the flight payl o two factors: \(1\ interfacing with th e sense and a programmatic one, an d as a Class D risk classification payl o The ISS physical interface provide d but one that was greatly facilit a standardized FRAM and by the g e allocations. The latter have afford e flexibility in making design decis i the expense of the still generous m a Operating on the ISS also mean t facility and of the astronauts it ho s all other considerations. As such O in a series of inhibits to provide t w catastrophic hazards identified despite being a Class D payloa d typically affords significantly mo r project\222s risk posture for mission s u on safety. The ISS-driven pro c scrutinizes this aspect can be d a navigate it, projects need to pla n significant impacts in both tech n areas, while keeping in close c PSRP The internal decision to implemen t p ayload also had significant impli c led to the widespread use of CO T o ols F unctions e ssing l ay b ase Build a gement b ase Build r y h ronization k Access n ge Requests iewer Figure 19: OPALS cross-agenc y m ulation activity is unique sheer complexity and or both exercises, POIC l e to interface with the a tional environment.  The n d MOS teams will be to off-nominal notifications e loop. The teams will be a nd Payload Regulations v ities and demonstrate o cesses, both internal to I ONS  o ad was largely driven by e ISS, both in a physical d 2\ng implemented o ad d a significant constraint a ted by the use of the e nerous mass and power e d the project significant i ons that reduced cost at a ss and power margins t that the safety of the s ts took precedence over O PALS has had to design w o fault tolerance for all This limitation comes d a classification which r e latitude in defining a u ccess but has no bearing c ess that oversees and a unting. To successfully n and accommodate for n ical and programmatic ommunication with the t the project as a Class D c ations for the design. It S components, which in  y operations interfaces 


  19 turn lead to a design that required forced convective cooling and vibration dampening, both uncommon for modern-day space payloads. Furthermore, it allowed the project flexibility in tailoring its verification and validation program for non-ISS requirements to save cost and schedule The focus of space-borne instruments is usually on the flight articles, which in the case of OPALS is the FRAM-based payload to be externally installed on the ISS. However most, if not all, projects must also develop a ground infrastructure to support the operations of the flight asset For OPALS, despite being more akin to an instrument than a spacecraft, that was certainly the case as significant development was required for both the optical ground station to enable the transmissi on of a beacon and receipt of the optical downlink for the former, and for the MOS to support commanding and telemetry The use of the OCTL facility in California greatly reduced the amount of development required to receive and process the optical downlink, and to transmit the beacon needed by the FS to locate the ground station. The development was thus restricted to the optics needed to condition the light beam for processing by the hardware receiver and the software decoder The design of the MOS was, by and large, dictated by the interface to the ISS\222s expansive payloads operations infrastructure located at Ma rshall Space Flight Center Because the interface was already mature and well understood, the project\222s efforts were primarily focused on ensuring compatibility rather than developing its own capability. The savings afforded by leveraging the existence of the already operating infrastructure cannot be understated A CKNOWLEDGEMENTS  The authors express their gratitude to the entire OPALS team for their dedication and hard work, and to the mentors that have guided the team on a path to success. The authors acknowledge all of the OPALS team members, whose technical contributions made the writing of the paper possible.   The work described in this paper was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration R EFERENCES    J  C e s a r o n e  D  S   A b ra h a m s  S  S h am b a y a ti  a n d J   Rush, \223Deep space communications, visions trends and prospects,\224  In Proc. Int. Conf. Space Opt. Syst. Appl  Santa Monica, CA, May 11\22613, 2011, pp. 412\226425  m ati 223 S pecial s ec tion guest editorial: free space optical communications.\224 Opt. Eng 0001 51\(3\031201-1-031201-1 doi:10.1117/1.OE.51.3.031201 3 Hen n i g er, an d O. W ilf ert A n i n tro d u ctio n to f r ee space optical communications Radioengineering 19.2 pp. 203-212, 2010 4 Risk Classification for NASA Payloads NASA Procedural Requirements NPR 8705.4, 2008  P h aeton Early C a reer Hire Dev e lop m ent P r og ram  R. M Jones, J. Lim. 2012. Jet Propulsion Laboratory California Institute of Technology. 19 Oct. 2012 http://phaeton.jpl.nasa.gov/internal/index.cfm 6 ard  B  So lish  M. Seib ert, K. Kilb rid e  S. Do n g   and T. Coffee. \223Bridging the Generation Gap : A Rapid Early Career Hire Training Program,\224 in AIAA Space 2008 Conference pp. 1-13   B i s w a s H. He mm ati S. P i azzolla, B  Mois ion  K  Birnbaum, and K. Quirk, \223Deep-space Optical Terminals DOT\ms Engineering.\224 IPN Progress Rep. 42183. Nov. 15, 2010  Hem m a ti A B i s w as I.B D j o r d j ev ic  D eepS pace Optical Communications: Future Perspectives and Applications," in Proceedings of the IEEE  vol.99 no.11, 2011, pp.2020-2039 9 Lucke, Robert L., et al. "Hyperspectral Imager for the Coastal Ocean: instrument description and first images Applied Optics 50.11 \(2011\ 1501-1516 10 M  C o r s o n  D  K o r w a n  R  L u c k e  W  S n y d e r  C   Davis, "The Hyperspectral Imager for the Coastal Ocean HICO\ the International Space Station," in IGARSS 2008 pp.IV-101-IV-104  T e les c ien ce R e s o u r ce K i t J. Onken. 2012. Marshall Space Flight Center. <http://trek.msfc.nasa.gov 12  Payload Safety Review and Data Submittal Requirements National Space Transportation System NSTS 13830, 1998 13 Computer-Based Control System Safety Requirements  Space Station Program SSP 50038, 1995 B IOGRAPHIES  Bogdan Oaida is the Project Systems Engineer for OPALS. He received a B.S.E. in Aerospace Engineering in 2007 and a M.Eng in Space Engineering in 2008, both from The University of Michigan. Since joining JPL in 2008, Bogdan has also worked on a number of Earth-sensing mission proposals and has participated in several TeamX studies JPL\222s concurrent engineering environment 


  Matt Abrahamson i s Operations Lead for received a B.S E ngineering With Technology in 2006 a A eronautics and Astro n both from MIT. He L aboratory Fellow from since joining J PL has worked on flight o p EPOXI, Stardust-NExT, Juno, and Daw n mission navigator  Robert Witoff is the I S Systems Engineer for received a B.S E ngineering from the Colorado at Boulder i p ursuing a Masters in S o E ngineering at Stanford A pigy Inc, Robert joine d has additionally worked on the Daw n Interplanetary missions J essica Bowles Martin e and Mission Assuranc e OPALS. She receive d  E lectrical Engineering Science and B.S. in Co m Studies in 2003 at th e I nstitute of Technolog y earned a M.Eng in Elec Engineering from Johns Hopkins Universi t J PL in 2008, she has held various ro l radiation testing for Juno, research in N A Parts Packaging program, and an ISS b a assembled telescope project  Daniel Zayas is the T h both OPALS and t h L aboratory, a planne d Condensate experime n I nternational Space received B.S. degrees E ngineering and Physic s A erospace Engineerin g M assachusetts Institute of Technology.  H e contributed to a number of planetary m including JUNO and the Mars Science L a Curiosity   20 s the Mission OPALS. He in Aerospace Information a nd a M.S. in n autics in 2008 was a Draper 2006-2008 and p erations for the n projects as a S S and Dragon OPALS.  He in Aerospace University of i n 2009 and is o ftware Systems  After founding d JPL where he n and Rosetta e z is the Safety e Manager for d a B.S.E. in and Computer m parative Media e Massachusetts y In 2006 she trical Computer t y. After joining l es working on A SAs Electronic a sed robotically h ermal Lead for h e Cold Atom d Bose-Einstein n t aboard the Station.  He in Aerospace s and a M.S. in g all from the e has previously m issions at JPL a boratory rover 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


