Image Annotation by Learning Rules from Regions Patterns  Stefan Udristoiu, Anca Loredana Ion Software Engineering Department Faculty of Automation, Computers and Electronics Craiova, Romania s_udristoiu@yahoo.com  anca_soimu@yahoo.com    Abstract The modeling of multimedia and especially the semantic gap between the visual features and semantic concepts become an important domain due to the quantity of visual digital content, which speedily grows. In this paper, the analysis and semantic annotation of images are studied. The 
development of methods for colour image annotations based on learning represents the main contribution of the paper. The developed algorithms generate semantic pattern rules that identify high-level image concepts. A semantic pattern rule is a combination of imagesí region patterns that identifies semantic concepts. Our methods are not limited to any specific domain and they can be applied in any field  Keywords- image annotation, semantic association rules image segmentation, visual features, colour, texture, and shape I   I NTRODUCTION  In this paper, an automatic method that generates 
annotations based on visual features of image regions is described. The correlation between the low-level features and high-level concepts is a challenge due to the ìsemantic gapî [5 The visual characteristics canít be directly interpreted in the terms of high-level concepts understood by humans [3 In other words, the human canít understand the visual features, because they are represented by multidimensional vectors. Also, the visual similitude does not always mean semantic similitude. If a user interrogates a visual-based retrieval system for a ìred roseî, then it is possible that images that contain a ìred ballî could be also retrieved. This 
is the semantic gap between the visual characteristics and semantic information The fundamental scope of image retrieval is to provide efficient and simple modalities for searching in the image databases [2 s it is m e n tio n e d a b o v e  it is d i f f i cu lt to g e t this target using the traditional retrieval image systems which do not take into account the semantic aspects of images The developed prototype permits of expert users to generate rules specific to their domain, by submitting significant categorized images, from which the system can learn semantic pattern rules. The semantic rules map the combinations of visual characteristics \(colour, texture, shape 
position, etc.\ to semantic concepts The semantic pattern rules capture the meaning and understanding of a domain by an expert, like the visual primitives are definitive for the semantic concepts of an image category. They are represented in Prolog and can be shared and modified depending on the updates in the respective domain The remainder of this paper is structured as follows Section 2 presents the selection of visual features and the segmentation algorithm. Section 3 presents the mapping between visual features and semantic concepts. Section 4 details the generation of semantic pattern rules, the semantic 
classification of images, and discusses the experiments Finally, section 5 summarizes the conclusions of this study II  THE SELECTION OF IMAGE VISUAL CHARACTERISTICS  A  The image model The model of knowledge representation constitutes the principal element of the annotation system. The image I  objects are modeled in two levels: physical and logical one The physical representation R F I of an image is related to the image data sequence described by the pixel values The logical representation R L I of the image represents the generalization of the physical representation and is also 
divided into levels: at the low-level of the hierarchy, the image is represented as a set of visual low-level features F{I In our chosen representation, each image is described as a set of colour regions R\(I\={r ij  The transition from the regions set to the semantic concepts recognition represents the big challenge. The semantic concept recognition is placed at the highest level of the model representation hierarchy The selection of the visual feature set and the image segmentation algorithm are the definitive stage for the semantic annotation process of images f t e r perf or m i ng a large set of experiments for computing the visual features of images, we inferred 
 the importance of semantic concepts in establishing the similitude between images. Even if the semantic concepts are not directly related to visual features colour, texture, shape, position, dimension, etc.\ese attributes capture the information about the semantic meaning  B  The image segmentation The ability and efficiency of the colour feature for characterizing the colour percep tual similitude is strongly influenced by the colour space and quantization scheme selection. A set of dominant colours determined for each image provides a compact description, easy to be implemented. The HSV colour space quantized to 166 
2010 International Conference on Complex, Intelligent and Software Intensive Systems 978-0-7695-3967-6/10 $26.00 © 2010 IEEE DOI 10.1109/CISIS.2010.43 124 


colours is used to represent the colour information. Before segmentation, the images are transformed from RGB to HSV colour space and quantized to 166 colours. The colour regions extraction is realized with the colour set back projection algorithm [6 h is alg o rith m d e tects t h e reg i o n s  of a single colour and has four stages 1  Transformation, quantization, filtering 2  Colour set back projection algorithm 3  Label the detected colour regions 4  Extraction of region characteristics The figures 1 and 2 illustrate the image segmentation process of an image from Ñcliffî category For each colour region, the spatial coherency represents the spatial homogeneity of the region in an image, like in the figure 3. It is computed for identifying the 8-connected pixels of the same colour in a region   a\                                        \(b Figure 1. \(a\ Original image from category ìsunsetî. \(b\ Quantized image to 166 colours in HSV space      Figure 2. The segmentation results   Weak spatial coherency     Strong spatial coherency Figure 3. Spatial coherency C  The region texture The texture is another feature that is taken into consideration for classifying and recognizing the material surfaces. The texture describes the surface attributes like regularity, thinness, granularity, and roughness.  In this paper, we use the method based on co-occurrence matrices The co-occurrence matrix is based on repeated occurrence of some configurations of pixels intensity in the image These configurations vary with rapidity for thin texture and slower for roughly texture. The co-occurrence matrices can describe the occurrences of these intensity configurations and in the case of colour images, a matrix was computed for the index colour of region. The classification of texture is based on the characteristics extracted from the cooccurrence matrix \(Haralick et al., 1973 The vector of texture characteristics extracted from the co-occurrence matrix is created using 6 characteristics The maximum probability detects the most frequent motif The energy describes the uniformity of the texture. The energy of an image is high when the image is homogeneous The entropy measures the randomness of the elements in the matrix; when all elements of the matrix are maximally random, entropy has its highest value. So, a homogeneous image has lower entropy than a non-homogeneous image. In fact, when energy gets higher, entropy should get lower The contrast is bigger for images with big contrast Cluster shade and Cluster prominence  are measures of the skewness of the matrix, in other words the lack of symmetry. When cluster shade and cluster prominence are high, the image is not symmetric Correlation  measures the correlation between the elements of the matrix. When correlation is high, the image will be more complex than when correlation is low D  The region shape Shape is an important characteristic of an object. The goal of shape descriptors is to uniquely characterize the object shape. Two shape descriptors are used in our experiments   Eccentricity is the length ratio between the major and minor axes of the objects; smaller for rounded shapes and greater for distorting ones   Compactness is the ratio between the length of objectís boundary and the objectís area E  The spatial information The spatial information of the region is also considered for image annotation. It provides the necessary information in the process of region indexing and semantic definition like upper left, upper right, centre, etc. The spatial information of each region is represented by two parameters: the centroid of the region Cx,y = \(Xc, Yc\d the minimum bounding rectangle  \(Xl,Xr,Yt,Yb\where \(l,r represents the coordinates of the upper left corner and  \(t,b represents coordinates of the bottom right corner 
125 


F  The region indexing In conformity with the defined characteristics, a region is described by   The colour characteristics are represented in the HSV colour space quantized at 166 colours. A region is represented by a colour index which is, in fact an integer number between 0Ö165. It is denoted as descriptor F1   The spatial coherency represents the region descriptor, which measures the spatial compactness of the pixels of same colour. It is denoted as descriptor F2   A seven-dimension vector \(maximum probability energy, entropy, contrast, cluster shade, cluster prominence correlation\ represents the texture characteristic. It is denoted as descriptor F3   The region dimension descriptor represents the number of pixels from region. It is denoted as descriptor F4   The spatial information is represented by the centroid coordinates of the region and by minimum bounding rectangle. It is denoted as descriptor F5   A two-dimensional vector-eccentricity and compactness - represents the shape feature. It is denoted as descriptor F6 III  M APPING THE VISUAL FEATURES TO SEMANTIC INDICATORS  The visual vocabulary developed for image annotation is based on the concepts of semantic indicators, whereas the syntax captures the basis models of human perception about patterns and semantic categories In this study, the representation language is simple because the syntax and vocabulary are elementary. The language words are limited to the name of semantic indicators. Being visual elements, the semantic indicators are, by example, colour \(colour-light-red\patial coherency spatial coherency-weak, spatial coherency-medium, spatial coherency-strong\, texture \(energy-small, energy-medium energy-big, etc.\mension \(dimension-small, dimensionmedium, dimension-big, etc.\osition \(vertical-upper vertical-center, vertical-bottom, horizontal-upper, etc shape \(eccentricity- small, compactness-small, etc The syntax is represented by a model, which describes the images in terms of semantic indicators values. The values of each semantic descriptor are mapped to a value domain which corresponds to the mathematical descriptor The value domains of visual mathematical characteristics are experimentally determined using images of WxH dimension The colour correspondence between the mathematical and semantic indicator values is determined based on experiments effectuated on a training image database. The colour correspondence is illustrated by the following examples   light-red \(108\edium-red \(122\ dark-red \(139   light-yellow \(109\edium-yellow \(125\yellow \(141   light-green \(112\edium-green \(130\dark-green 142   Light-blue \(116\edium-blue \(136\-blue 148 A hierarchy of values, which are mapped to semantic indicator values, is also determined for texture: textureenergy-big, texture-energy-medium, texture-energy-small texture-entropy-big, texture-entropy-small, texture-entropymedium, etc The texture correspondence is illustrated by the following examples   If 0  entropy  1, then the region has entropy-small   If 1<entropy  1.67, then the region has entropymedium   If 1.67<entropy, then the region has entropy-big   If 0  energy  0.22, then the region has energy-small   If 0.22<energy  0.42, then the region has energymedium   If 0.42<energy  1, then the region has energy-big   If 0<contrast  300, then the region has contrast small   If 300<contrast  900, then the region has contrastmedium   If contrast>900, then the region has contrast-big A hierarchy of values, which are mapped to semantic indicator values, is also determined for dimension feature dimension-big, dimension-small, dimension-medium The correspondence between the mathematical and the semantic indicator values is also determined, based on experiments   If \(PixelCnt  WxH/10\hen the region has dimension-small   If \(WxH/10<PixelCnt  WxH/5\, then the region has dimension-medium   If \(PixelCnt>WxH/5\, then the region has dimension-big where PixelCnt is the number of region pixels, W and H are the width, respectively the height of the image A hierarchy of values, which are mapped to semantic indicator values, is also determined for the position feature of region: position-horizontal-right, position-horizontalcentre, position-horizontal-left, position-vertical-upper position-vertical-centre, and position-vertical-bottom The correspondence is illustrated by the following examples   If 0<C x  W/4, then the region is situated horizontalleft   If W/4<C x  3W/4, then the region is situated horizontal -centre   If 3W/4<C x  W, then the region is situated horizontal-right   If 0<C y  H/4, then the region is situated verticalupper   If H/4<C y 3H/4, then the region is situated vertical centre   If 3H/4<C y  H, then the region is situated verticalbottom where C x C y represent the coordinates of the centroid region 
126 


A hierarchy of values, which are mapped to semantic indicator values, is also determined for spatial coherency feature. The correspondence between the mathematical and the semantic indicator values is also determined based on experiments   If PixelDif  PixelCnt/8, then the region has spatialcoherency-big   If PixelCnt/8<PixelDif  PixelCnt/4, then the region has spatial-coherency-medium   If PixelDif>PixelCnt/4, then the region has spatialcoherency-small where PixelDif represents the number of region pixels which have its 8 neighbors of different colour, PixelCnt represents the total number of region pixels  A hierarchy of values, which are mapped to semantic indicator values, is also determined for shape, as in the figure 4 and 5: eccentricity-big, eccentricity-medium eccentricity-small, and compactness-small, compactnessmedium, compactness-big At the end of the mapping process, a figure is represented in Prolog by means of the terms figure\(ListofRegions where ListofRegions is a list of image regions The term region\(ListofDescriptors is used for region representation, where the argument is a list of terms used to specify the semantic indicators. The term used to specify the semantic indicators is of the form  descriptor\(DescriptorName, DescriptorValue  In the bellow example, the representation of an image from cliff category can be observed figure region\([descriptor\(colour,dark-brown descriptor\(horizontal-position, center descriptor\(vertical-position,center descriptor\(dimension,big\criptor\(shapeeccentricity, small descriptor\(texture-probability, medium descriptor\(texture-inversedifference, medium descriptor\(texture-entropy,s mall\, descriptor\(textureenergy,big descriptor\(texture-contrast,big descriptor\(texture-correlation, big   Figure 4. Correspondence between mathematical values of the visual shape feature \(eccentricity\ and semantic indicators   Figure 5. Correspondence between mathematical values of the visual shape feature \(compactness\ and semantic indicators IV  THE GENERATION OF SEMANTIC PATTERN RULES FOR IMAGES  The knowledge discovery in databases, an important part of data mining, is defined as the automatic discovery of useful information, unknown, non-trivial [4  The main component in image discovery is the identification of similar image objects  T h e as s o ciation rules discover the information about the elements that are frequent. The formal representation of an association rule is the following. Consider I={i 1 i 2 i m  as  a set of distinct elements and D a transaction set, where each transaction T is a subset of I or T  I A transaction T contains X if and only if X  T An association rule is an implication A=>B where A  T, B  T, A  B  A and B are called the body respectively the head of the rule. The support of the rule is defined as the percent of transactions in which both the body \(A\d head \(B\ the rule are present. If a rule A=>B has support s this means that s of transactions from D contain A  B The confidence is defined as the ratio between the number of transactions in which both the body A\d head \(B\ the rule are present, and the number of rules in which only the body is present If the rule A=>B has the confidence c this means that c transactions from D contain A  B The support estimates the probability P\(A  B and the confidence estimates the conditional probability P\(B|A  We suppose to have an image database DB In the learning/training phase, the set U = {S1,Ö,Sn is a subset of DB and contains n image-examples, labelled by semantic concepts, for which we train the system and generate semantic pattern rules. In the learning phase, the scope is to automatically generate semantic rules R based on categorized image-examples, which identify the semantic concepts of images.  A rule determines the set of semantic indicators, which identify the best a semantic concept. In the testing/annotation phase, for each image of the testing subset, namely the images from DB but that are not in U  we use the generated semantic pattern rules to label them with one or more concepts The process of the automated generation of semantic pattern rules and image annotation is the following 1. The learning phase: semantic pattern rules generation  A semantic rule is of the form semantic indicators -> category  The steps of the learning process are relevant images for a semantic concept are used for training the system 
127 


each image is automatically segmented and the primitive visual features are computed, as it is described in section 2 for each image, the visual primitive features are mapped to semantic indicators, as it is described in section 3 the determination of pattern regions for each image belonging to the transaction set, as it is described in section 4 the rule generation algorithms are applied to produce semantic pattern rules, which will identify each semantic category from database 2  The image testing/annotation phase has as scope the automatic annotation of images each new image is automatically segmented into regions for each new image, the low-level characteristics are mapped to semantic indicators the classification algorithm is applied for identifying the image category/semantic concept In the developed system, the learning of semantic pattern rules is continuously made, because when a categorized image is added in the training database, the system continues to generate semantic rules A  The description of the algorithm for rule generation The method for the generation of the semantic pattern rule is based on A-priori algorithm for finding the frequent itemsets [1 T h e sco p e o f im ag e asso ciatio n ru le s is to f i n d  semantic relationships between image objects The rule generation algorithm takes into account the entire region features. This algorithm is based on Ñregion patternsî and necessitates some computations, being necessary a pre-processing phase for determining the visual similitude between the image regions of the same category In the pre-processing phase, the region patterns, which appear in the images, are determined. So, each image region Reg ij is compared with other image regions from the same categories. If the region Reg ij matches other region Reg km  having in common the features on n 1 n 2 n c positions then the generated region pattern is SRj \( -, -, n 1 n 2 n c   and the other features are ignored The generation of image region patterns is described in pseudo-code by the following algorithm  Algorithm IV.1   Generation of region patterns for each image Input  set of n images {I 1 I 2 I n laying in the same category; each image I is a set of regions Reg im  Output  set  of region patterns for each image Method  for i 1 1, n-1 do for j 1 1, I i1 nregions do for i 2 i 1 1, n do  for j 2 1, I i2 nregions do if Reg j1 matches Reg j2 in the components n 1 n 2 n c then  i=i+1  RSi 1 RSi 1 n 1 n 2 n c RSi 2 RSi 2 n 1 n 2 n c  end  end end end A database with five images relevant for a category is used. For this category, an example of images representation using region patterns can be observed in the Table I By applying the algorithm IV.1 on the database from Table I, the results from Table II are obtained As it can be observe, the number of region patterns is big, but it is pruned in the phase of rules elimination process, described in the next section The image modelling in terms of itemsets and transaction is the following the transactions represent the set of  region patterns determined by the previous algorithm the itemsets are represented by region patterns of the images laying in the same category the frequent itemsets represent the itemsets with the support greater than the minimum support the itemsets of cardinality between 1 and k are iteratively found the frequent itemsets are used for rule generation The algorithm for rules generation based on region patterns is described in pseudo-code  Algorithm IV.2  The generation of  semantic pattern rules based on region patterns Input the image set represented as I =\(RS 1 RS k  m is a region pattern Output the set of semantic pattern rules Method  Ck: the set of region patterns of k-length Lk: the set of frequent region patterns of k-length PatternRules: the set of pattern rules constructed from frequent itemsets, for k>1 L1= {frequent region patterns for\(k=1; Lk!=null; k Ck+1=candidates generated from the set Lk foreach transaction t in the database do Increment the number of all candidates that appear in t end Lk+1=candidates from Ck+1 that has the support greater or equal than suport_min end PatternRules=PatternRules+{Lk+1->category end T ABLE I  R ELEVANT IMAGES FOR A CERTAIN CATEGORY  ImgID Image Regions 1 R\(a,b,c,d\, R\(aí,bí,cí,d 2 R\(a,b,c,f\, R\(aí,bí, cí,d 3 R\(a,b,c,fíí\R\(aí,bí,cí,i 4 R\(a,b,c,fí\R\(aí,bí,cí,i 5 R\(a,b,e,d\, R\(m,bí,cí,d 
128 


 T ABLE II  R EGIONS P ATTERNS OF IMAGES OF A CERTAIN  CATEGOY   ID  Region Patterns  1 R\(a,b,c,-\R\(a,b,-,d\R\(aí,bí,cí,dí\ R\(aí,bí,cí,-\ R\(-,bí,cí,d  2 R\(a,b,c,-\,R\(a,b,-,-\,R\(aí,bí,cí,dí\, R\(aí,bí,cí,-\, R\(-,bí,cí,d  3 R\(a,b,c,-\  R\(a,b,-,-\  R\(aí,bí,cí,-\ R\(-,bí,c  4 R\(a,b,c,-\ R\(a,b,-,-\R\(aí,bí,cí,-\ R\(-,bí,c  5 R\(a,b,-,d\,R\(a,b,-,-\ R\(-,bí,cí,dí\ R\(-,bí,c  The category of an image  is determined by the following semantic association rules R\(a,b,c,-\ and R\(aí,bí,cí,-\>category R\(a,b,c,-\ and R\(a,b,-,-\->category R\(aí,bí,cí,-\ and R\(a,b,-,-\ ->category R\(a,b,-,-\ and R\(-,bí,cí,-\ ->category R\(a,b,c,-\ and R\(aí,bí,cí,-\d R\(a,b,-,-\ ->category The rules are represented in Prolog as facts of the form PatternRule\(Category, Score, ListofRegionPatterns The patterns from ListofRegionPatterns are terms of the form: regionPattern\(ListofPatternDescriptors The patterns from the descriptors list specify the set of possible values for a certain descriptor name. The form of this term is descriptorPattern\(descriptorName,ValueList One of the semantic pattern rules used to identify the cliff category is illustrated bellow. This semantic pattern rule has the score \(confidence\al to 100 PatternRule \(cliff,100 regionPattern descriptorPattern\(colour,[dark-brown descriptorPattern\(horizontal-position,[center,left right  descriptorPattern\(vertical-position,[center,bottom descriptorPattern\(dimension,[big descriptorPattern\(eccentricity-shape,[sma descriptorPattern\(texture-probability, [medium  descriptorPattern\(texture-inversedifference medium descriptorPattern \(texture-entropy,[big descriptorPattern\(texture-energy,[big descriptorPattern \(texture-contrast,[big descriptorPattern \(texture-correlation, [big  regionPattern descriptorPattern\(colour,[medium-brown  descriptorPattern\(horizontal-position,[center,left right descriptorPattern\(vertical-position,[center, bottom descriptorPattern\(dimension,[smal descriptorPattern\(eccentricity-shape,[sma descriptorPattern \(texture-probability,[big descriptorPattern\(texture-inversedifference,[big descriptorPattern\(texture-entropy,[medium descriptorPattern \(texture-energy, [big  descriptorPattern \(texture-contrast,[medium descriptorPattern \(texture- correlation,[big  B  Semantic pattern rules elimination The number of semantic pattern rules that could be generated is usually big. In this case two problems exist: the first one is that the set of rules can contain noise information and affect the classification time. Another problem is the big number of rules that can also affect the classification time This is an important problem in the real applications, which necessitate rapid response. On the other side, the elimination of rules can affect the classification accuracy. The elimination methods are the following elimination of  specific rules and keeping the rules with big confidence elimination of rules that can introduce errors in the classification process The following definitions introduce some notions used in this section [11   Definition 1   Given two rules R 1 C and R 2 C, the first rule is called general if R 1  R 2 The second one is called specific Definition 2  Given two rules R 1 and R 2 R 1 is called stronger than R 2 or R 2 is weaker than R 1  1\ R 1 has confidence greater than R 2  2\If the confidences are equal, but the support\(R 1  greater than support\(R 2  3\ the supports are equal, support\(R 1 upport\(R 2  and confidences confidence\(R 1 fidence\(R 2  equal, but R 1 has fewer attributes than R 2   The elimination of weak and specific semantic pattern rules is described in pseudo-code Algorithm IV.3  Elimination of weak and specific rules Input  set of semantic pattern rules, S generated for each category, C   Output  the set of rules which will be used for classification Method Sort the rules for C category in conformity to Definition 1 foreach rule in S do begin Find the most specific rules Eliminate the rules with  smallest confidence end C  Semantic image classification The classifier represents the set of semantic pattern rules used to predict the category of images from the test database. Being given a new image, the classification process searches in the rules set for finding the most appropriate category. Images are processed and are represented by means of semantic indicators as Prolog facts The semantic pattern rules are applied to the set of images facts, using the Prolog inference engine A semantic pattern rule matches an image, if all characteristics which appear in the body of the rule also appear in the image characteristics The algorithm for image categorization is described in pseudo-code 
129 


Algorithm IV.4  Semantic classification of an image Input new unclassified image and the set of semantic pattern rules; each pattern rule has the confidence R i conf Output the classified image, and the score of matching Method  S = null foreach rule R in PatternRules do begin if R matches I then Keep R and add R in S I.score = R.conf  Divide S into subsets one foreach category: S 1 S n   foreach subset S k from S do Add the confidences of all rules from S k  Add I image in the category identified by the rules from S k with the greatest confidence I.score = max 000 S k conf end end  end In the experiments realized through this study, two databases are used for testing the learning process. The database used for learning contains 200 images from different nature categories and is used to learn the correlations between images and semantic concepts. All the images from the database have JPEG format and are of different dimensions. The database used in the learning process is categorized into 50 semantic concepts. The system learns each concept by submitting appreciatively 20 images per category. The testing database contains 500 unclassified images The results of the semantic image retrieval after the sunriseî category are observed in the figure 6. These images from the test database are correctly classified by the algorithm The results of the semantic image retrieval after the mountainî category are observed in the figure 7. Two images from the test database are incorrectly classified since they represent ìclouds The performance metrics, precision and average normalized modified retrieval rate \(ANMRR\, are computed to evaluate the efficiency and accuracy of the semantic pattern rule generation and annotation methods The precision is defined as the ratio between the number of images correctly classified by system, and the total number of classified images. The precision is in the range of 0 an d g r eater v a lu es repres en t a better retriev a l performance. Averaged Normalized Modified Retrieval Rate \(ANMRR\ is an overall performance calculated by averaging the result from each query h e A N M R R is in the range of d s m a ller v a lu e s repres en t a better retrieval performance These parameters are computed as average for each image category as in the table III    Figure 6. The results of semantic image retrieval after the ìsunrise category  Figure 7. The results of semantic image retrieval after the ìmountains category  
130 


T ABLE III  T HE PRECISION AND ANMRR COMPUTED FOR EACH IMAGE CATEGORY  Category Precision ANMRR Fire 0.77 0.39 Iceberg 0.71 0.34 Tree 0.65 0.45 Sunset 0.89 0.14 Cliff 0.93 0.11 Desert 0.89 0.11 Red Rose 0.75 0.20 Elephant 0.65 0.43 Mountain 0.85 0.16 See 0.91 0.09 Flower 0.77 0.31  Also, for each category from the database, the percent of images correctly classified by the system is computed as in Figure 8 As it can be observed from the experiments, the results are strongly influenced by th e complexity of each image category. Actually, the results of experiments are very promising, because they show a small average normalized modified retrieval rate and a good precision for the majority of the database categories, making the system more reliable V  CONCLUSION  In this study we propose methods for semantic image annotation based on visual content. For establishing correlations with semantic categories, we experimented and selected some low-level visual characteristics of images. So each category is translated in visual computable characteristics and in terms of objects that have the great probability to appear in an image category  Figure 8. Category vs. percent of images correctly classified   The algorithm that generates the semantic pattern rules selects the image characteristics with the greatest probability of apparition, offering a better generality By comparison to other image annotation methods, our proposed and developed methods have some advantages: the entire process is automated, and a great number of semantic concepts can be defined; these methods can be easily extended to any domain, because the visual features semantic indicators remain unchanged, and the semantic pattern rules are generated based on the set of example labeled images used for learning semantic concepts; the spatial information is taken into account and it offers rich semantic information about the relationships of the image colour regions \(left, right, center, bottom, and upper The proposed methods have the limitation that they canít learn every semantic concept, due to the fact that the segmentation algorithm is not capable to segment images in real objects. Improvements can be brought using a segmentation method with greater semantic accuracy R EFERENCES  1  A  Be r s o n S  J  S m it h, D a ta W a r e ho us i n g  D a ta Mi ni ng a n d O L A P  McGraw-Hill. New York, 1997 2 G  C a rn e i ro A  C h a n  P  M o re n o a n d N Va s c on c e los  S u p e rvi s e d  learning of semantic classes for image annotation and retrieval IEEE Pattern Analysis Machine Intelligence, vol. 29\(3\, 2007, pp 394ñ410 3  A  H o og s  J  Ri tts c he r  G  S t e i n  a n d  J  S c hm ie de r e r   V ide o co nte n t  annotation using visual analysis and a large semantic knowledge base,î Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003, pp. 327 ñ 334 4  W  J. F r aw l e y   G  P i ate t sky S h apir o  an d C  J Mat h e u s  K n o w le dg e Discovery in Databases, chapter Knowledge Discovery in Databases  An Overview, MIT Press, 1991 5  A   W  S m e u l d e r s  M  W o r r i n g  S   S a n t i n i  A  G u p t a  a n d R  J a i n   Content-Based Image Retrieval at the End of the Early Years IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22\(12 2000, pp. 1349ñ1380 6  J  R  S m it h S  F C h a n g   V is ual S E E k a f u l l y auto m a te d co nte n tbased image query system,î The Fourth ACM International Multimedia Conference and Exhibition, Boston, MA, USA, 1996 7  V  Me z a r i s   I   K o m p ats i ar is  an d M G  S t r i ntz   R e g io nbas e d I m age Retrieval using an Object Ontology and Relevance Feedback EURASIP JASP, 2004 8  B. S   Ma nj u n a t h  P  S a l e m b ie r  a nd T  S i ko r a I n tr o duc tio n to MP E G 7: Multimedia Content Description Standard, Wiley, New York 2001 9  O  Mar q ue s  N  B a r m a n  S e m iau t o m a ti c S e m a n t i c A nno ta tio n o f  Images Using Machine Learning Techniques,î D. Fensel, K. Sycara and J. Mylopoulos, editors, Proceedings of the International Semantic Web Conference, 2003, pp. 550ñ565, Florida 10 N. R a si w a si a  P. J. Moreno  N  Vasconcelos  Bridging the Gap Query by Semantic Example,î IEEE Transactions On Multimedia vol. 9\(5\, 2007, pp. 923-938   O   R Z a ian e  M L  A n t o ni e  A   C o m a n A pp l i cati o n o f D a ta   Minin g  Techniques for Medical Image Classificationî, Proceedings of Workshop on Multimedia Data Mining \(MDM/KDDí2002\, San Francisco, USA, 2002 
131 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnêt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnêt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnêt create sound clips  A5.1 We donêt need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIês   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





