Scalable Context-Aware Role Mining with MapReduce Zhiwei Yu School of Computer Science and Engineering University of New South Wales Sydney Australia Raymond K Wong School of Computer Science and Engineering University of New South Wales Sydney Australia Chi-Hung Chi ISSL CSIRO Tasmania Australia Abstract Cloud computing platforms facilitate efìciently processing complicated computing problems of which the time cost used to be unacceptable Recent research has attempted to use role-based approaches for context-aware service recommendation yet role mining problem has been proven to be difìcult to compute Currently proposed role-mining algorithms are inefìcient and may not scale to cope with the huge amount of data in the real-world This paper proposes a novel algorithm with much better runtime complexity and in MapReduce style to take advantage of popular distributed computing platforms Experiments running on a medium-sized high performance computing cluster demonstrate that our proposed algorithm works well with both running time complexity and scalability I I NTRODUCTION Recently cloud computing platforms are widely applied for dealing with various of mining missions on big data They involve the internet for providing on-demand access to shared resources such as compute units data storage and applications MapReduce programming model is one of the most popular distributed computing technologies for distributing big data processing across a large cluster of compute nodes in the Cloud It is quite effective in simplifying the development of data-intensive applications through the use of a simple API and with a robust runtime system These infrastructures truly facilitate efìciently processing complicated computing problems of which the time cost used to be unacceptable Role-based approaches are widely applied to data security domain 9 8 Further  recent research has attempted to use role-based approaches to recommend mobile services to other members among the same context group 21 However role mining problem has been proven to be difìcult to compute it is in fact NP-hard Two context-aware role mining algorithms based on matrix calculation have been recently proposed one is an approximation algorithm and another one is a complete algorithm for veriìcation Both of them have their weaknesses and may not applicable under some circumstances and therefore this paper is a continuation work of Let u to be the number of users b to be the number of behaviors c to be the number of contexts and r to be the total number of roles although algorithm C2  A C 2  in is a complete algorithm that can cover all roles it has a complexity of O  u 2  2 c  because of the combination procedure for listing the power set S   context behavior  couples of the user   As a result if the number of contexts becomes large A C 2 will not be practical any more Besides the algorithm V2  A V 2  has a complexity of O  u 2  c   which is much faster than A C 2  However A V 2 is not complete which means that its role coverage could be very low   10  in certain situation e.g when the number of contexts and users are very large In this paper similar to and 21 we e xtend the behavior pattern recognition method to identify conte xtaware roles from multi-user behavior patterns Different from  21 and all other related w ork to this paper  the proposed algorithm A D 1 is complete as well as with applicable and practical running time complexity To apply compute cluster for parallel computing we have applied MapReduce framework in this algorithm which can be deployed to an Apache Hadoop environment Leverage on high performance compute cluster algorithm A D 1 shows strong scalability Experiments are also performed to demonstrate the capacity and scalability of this algorithm The organization of this paper is as follows Section II summarizes the related work Section III deìnes the contextaware role mining problem Section IV presents our proposed new algorithm for context-aware role mining and it is divided into ve subsections The rst subsection talks about the main challenges for the algorithm design the second subsection talks about main procedure and the third subsection explains the partition method in the fourth subsection methods for reducing the search space are discussed Lastly we present this map-reduce-style code in detail After that Section V presents experiment results to show the performance and scalability of the new algorithm Finally Section VI concludes the paper II R ELATED W ORK The concept of roles has been used in many areas such as linguistics knowledge representation relational database and access control 10 14 especially  roles ha v e been widely used in data security domain under RBAC e.g Numerous role mining approaches have been proposed e.g  9 17 21 Recently context which used to be mostly disregarded in RBAC is considered as an important factor To the best of our knowledge 12 18 are the only papers addressing context-aware RBAC However both and 12 did not describe how to discover or identify context-aware roles Traditional RBAC-based role mining algorithms such as and  cannot be directly applied since traditional RB A C models only consider 2-D parameters namely  user permission   while context-aware role mining approach needs to consider 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 467 


Fig 1 User-context-behavior matrix  BiUCB  Fig 2 Role-context-behavior matrix  CBA  at least 3-D dataset  user context behavior  where context includes time and scene etc Apache Hadoop is an open source system that allows for the processing of big data across cluster of workstation class computers T w o o f the k e y ideas of Apache Hadoop are originated from Google le Systems GFS that allo ws for the distribution of data across thousands of computers in a cluster and MapReduce that handles the breakdo wn of a problem into subtasks so that they can be processed by the cluster MapReduce is a programming model from Google The model comprises of two main functions Map and Reduce functions The Map function takes in a key/value pair and outputs an intermediate list of key/value pairs The Reduce functions will than take all values associated to the same key and produce the nal output list of key/values Users are tasked with providing the Map and Reduce implementation Apache Mahout is part of the Hadoop family which provides scalable machine learning libraries to handle recommendation mining problems under Hadoop platform Apart from typical clustering and singular value decomposition SVD functionality it also provides Naive Bayes decision tree etc type of classiìers Besides frequent itemset mining is also supported On the other hand Chen et al  and Wang et al  are recent e xamples of MapReduce-based solutions to frequent itemset mining problem To the best of our knowledge this is the closest related mining problem considered in Hadoop and context-aware role mining is a new area due to be explored This paper is a signiìcant extension of the work done in  21 in which 21 proposed a simpler and more ef cient algorithm to mine roles than Ho we v e r  the matrix-based algorithms presented in does not guarantee completeness i.e the algorithm may fail to uncover some role patterns This paper provides a scalable algorithm that is based on the ideas of divide and conquer and also Map-Reduce paradigm distributed computing as solutions to role mining problem III P ROBLEM DEFINITION A service recommendation engine must know the services consumption behaviors in the user community before any recommendation can be made We rstly study the knowledge acquisition by deìning the role mining problem Fig 3 User-role matrix  UA  We follow the deìnition of the context-aware role mining problem from and 21 Ho we v e r  for the simplicity of presenting and computing a traditional u  cUCB matrix with b behaviors in cells is transformed to a u   c  b  matrix by spreading each column in UCB to b columns and setting the respective values in the new column to be 1 according to the old cell values in UCB  We call this matrix BiUCB  of which each column represents a different context-behavior pair cb pair As a result a role can be presented as a set of users plus a set of column numbers of BiUCB  Deìnition 1 Context-aware role mining problem CARMP Given a user behavior pattern conìguration bpc  U  C  B BiUCB   where U is a set of users C is a set of contexts B is a set of behaviors BiUCB U◊C  B is the user-context-behavior relation CA-RMP is to nd a state  R U A CBA  that is consistent with bpc  such that for any  R  UA  CBA   that is consistent with bpc   R  R    where R  R  is a set of roles UA  UA  U R is the user-role assignment matrix CBA  and CBA   R C  B is the role-context-behavior assignment matrix The state is consistent with bpc  if every user in U has the same set of context-behavior assignment as in bpc  Each role must have at least two u U  minu  2  and two  c b C  B  minc  2  to ensure generality One can summarize the relationship between BiUCB  UA and CBA by the following formula BiUCB  UA  CBA where  represents a boolean matrix multiplication between two boolean matrices in which X  Y   k  X ik  Y kj  13 Suppose there is a user set U   u 1 u 2 u 3 u 4 u 5  a context set C   c 1 c 2 c 3 c 4 c 5   and a behavior set B   1  2  3   A sample UCB can be constructed based on users behavior patterns as shown in Figure 1 where each column denotes a  c b C  B  each row denotes a u U  and a value of 1 in cell  i j  represents that the user u i exhibit the  c b  j  otherwise a value of 0  After mining roles from the behavior pattern conìguration we obtain a state  R U A CBA   where R   r 1 r 2 r 3 r 4   the resulting CBA and UA matrices are recorded in Figures 2 and 3 respectively As shown in Figure 2 four mined roles r 1  r 2  r 3 and r 4 are listed in a CBA matrix e.g r 2 has behavior 1 under context c 1 and c 3  and behavior 2 under context c 2  Figure 3 shows a UA matrix where a 1 in cell  i j  represents that user u i can play role r j  e.g user u 1 can play role r 2  IV M AP R EDUCE M ERGING S TYLE A LGORITHM  A D 1  Generally a modern high performance computing cluster could have over ten thousands of computing units By applying a powerful cluster and well designed distributing algorithms 468 


with strong scalability we expect to process huge input data sets with hundreds times of scale bigger than that for some standalone algorithms but with the same time cost A Challenges in Distributing Role Mining Algorithm Design The Divide and Conquer theory is often applied for obtaining strong scalability By this approach an essential issue is how to divide the data into parts and merge the intermediate results later A naive divide policy is to separate the matrix by selecting parts of the columns and rows rst and then simply apply some standalone mining methods to nd all the valid roles in the sub matrices and nally merge all the roles generated from the sub matrices together However the roles related to the data across multiple sub matrices will be omitted As a result this kind of partition method can not lead to a complete algorithm Generally the original input data set of a role mining job is not very huge A typical one should be no more than 1 million users 100 contexts and 100 behaviours However the number of roles r is sensitive to all of b  c  u  minu and minc and r could be a combination of b  c in the worst case The huge computing load comes from the tremendous number of 1 on 1 comparing between roles minu and minc are the key parameters to control the output scale and the algorithm complexity In most of the cases for getting a practical output the role number should always not be very huge and could be well controlled by adjusting minu and minc  However since the data is partitioned it is hard to apply them to lter the data distributed in different machines B The Main Procedure of Algorithm  A D 1  To avoid the problem of the naive partition method above a bottom up merge-style algorithm A D 1 is proposed to completely cover all the roles The main idea of A D 1 is to rstly compute all the intermediate roles with fewer cb pairs and then generate the more complex roles with more cb pairs by merging them For example suppose we have two roles role 1  u 1 u 2 u 5 u 6  cb pair 1  role 2  u 3 u 4 u 5 u 6  cb pair 2  after one round of merging a new role is generated role 3  u 5 u 6  cb pair 1  cb pair 2  The user set of role 3 is generated by getting the intersection set of the user sets of role 1 and role 2  while the cb pair set of role 3 is generated by getting the union set of the cb pair sets of role 1 and role 2  The essential process of A D 1 is that in each round of iterations all the roles generated from last round are merged with each other During this process the logic is simple so that the tasks are easy to split and distribute to multiple computing units The running time complexity of A D 1 is O log b  c   r 2  u  The factor log b  c  is the round number of the merge iteration for after each round of this iteration the maximum Fig 4 Partition Policy in Algorithm A D 1 number of cb pairs of the covered roles will become twice until it reaches b  c  Generally for the strong requirement of minu and minc  there should always be no new qualiìed role generated after looping for only very few rounds far less than log b  c   The factor r 2 is for the number of all the role pairs The factor u is for merging the user set operations between the two roles When BiUCB is a random matrix roles with more users and cb pairs have less probability to be formed Further this probability reduces fast exponential with the increasing of user and cb pair numbers in roles Hence r is close to the number of roles with very few users and cb pairs which means r is close to a polynomial function with u  c and b  C The Partition Policy of Algorithm  A D 1  In each round of the merging process we distribute the huge number of merging tasks to multiple computing units The total number of merging operations for each round is O  r 2   We do not simply generate the data of all the role pairs and split it by the build-in partition tools of Hadoop because it will deìnitely generate huge intermediate data and bring unacceptable I/O cost Instead each mapper job loads all the roles to the memory and choose respective different role pairs according to their job IDs A partition example is shown in gure 4 All the role pairs are separated uniformly to each mapper the numbers of role pairs assigned to each mapper are nearly even By the merging operations in the mapper most of the role pairs will not generate any new role As a result the scale of the mapper output is very small which brings extra beneìts for reducing the cost of network transferring within the cluster D Search Space Reduction The primary search space for each round of the merge iteration is O  r 2   which is still very large Actually only very small part of this search space is useful for generating new roles It can be dramatically reduced by the operations in this subsection For facilitating the expression basic necessary formulations are introduced in table I Deìnition 2 u pairs u 1  012 u  S cb  u   S cb  u 1  minc  u pairs is the essential concept for applying minc to reduce the search space We also introduce the following lemmas that are useful for reducing the search space 469 


Lemma 1 012 r 1 r 2 n,r 2  Succ n  r 1  n>R  r 1  1  015 m r 3 m  n  r 3  Succ m  r 1 r 2  Succ n  r 3 Lemma 2 012 r 1 r 2   S u  r 1  S u  r 2  minu  012 n r 3  Succ n  r 1 r 4  Succ n  r 2   S u  r 3  S u  r 4  minu Lemma 3 012 u 1 u 2 r 1 u 1  r 1  u 2  r 1  S cb  r 1  minc  u 1  u pairs  u 2  u 2  u pairs  u 1 Lemma 4 012 u 1 r 1 u 1  r 1  S cb  r 1  minc  S u  r 1  u pairs  u 1 Lemma 5 012 u 1 r 1   u pairs  u 1  S u  r 1  minc  012 n  u pairs  u 1  S u  Succ n  r 1  minc These lemmas are apparent From them we have the following operations for reducing the search space  From Lemma 1 in each round we only process the new roles generated in the last round  From Lemma 2 by applying a reverse index for nding roles related to a single user we only process the role pairs have more than minu common users  From Lemma 3,4 and 5 if  u pairs  u 1  S u  r 1   minc  we delete u 1 from r 1 and then if  S u  r 1   minu  we delete role 1  This step is very effective to reduce the number of useless intermediate roles To summarize by the above approaches for search space reduction the running time complexity of A D 1 becomes O log b  c   r   ave-user  ave-role  The ave-user is the average user number of each role and ave-role is the average roles number related to each user ave-user  ave-role is for nding all the related roles having more than minu common users with current role If the BiUCB matrix is sparse it will gain much performance beneìt The total memory cost for loading the reverse index and u pairs could be well controlled for they are both static data so that multiple jobs can just load these data with general share memory methods without copying them once for each job As a result the total memory cost on a single cluster node will not increase with the number of CPU cores increasing TABLE I F ORMULATION FOR S EARCH S PACE A NALYSIS  Expression Explanation N cb  r 1 the cb pairs amount of role r 1 N u  r 1 the user amount of role r 1 S cb  u 1 the cb pairs set of user u 1 S cb  r 1 the cb pairs set of role r 1 S u  cb 1 the user set of cb pair cb 1 S u  r 1 the user set of role r 1 R  r 1 the round number that role r 1 is generated Succ n  r 1 the role set generated by merging role r 1 and other roles in round n  we also have  r  Succ n  r 1   r 2 r  r 1  r 2 Fig 5 Main Procedure of Algorithm A D 1 E The Pseudocode of Algorithm  A D 1  In the following pseudocodes in this paper the parameter roles is a list to store the known roles Each item in this list is an object composed of 4 attributes users  cb  nstatus  ostatus  The attribute users is the set for storing all the users acting this role and the attribute cb is the set for storing all the context-behavior pairs  cb pairs acted by this role The attribute nstatus is for storing the status being processed in the current round of iteration while the attribute ostatus is for storing the status marked in the last round of iteration The status of a role could be old new or sub of which the old and new status marks are used for avoiding the role comparing operations that have been executed before and the sub status is used for deleting the detected sub roles The pseudocode of the main procedure of A D 1 is summarized in Figure 5 and a detailed explanation is given below  line 3 we get the column number of BiUCB   line 4-5 the role set as initialized For each role in this set its user set is the same as one column in BiUCB  470 


and it has only one respective column number in its cb pairs The status of each role is set to new so that any role pair will be compared in the next round  In line 7-19 and line 22-34 some commands for submitting jobs to Apache Hadoop are applied  line 7-19 u pairs restricted by minu and minc for all users are generated In line 8 and line 9 all the rows and columns of BiUCB are loaded by each mapper job as the indexes with the name of ucbIndex and cbuIndex  In line 10 we set the number of cores used In line 11 the input directory is assigned which contains several les each one of them contains the allocated part number for a single mapper to read In line 12 the output data directory is assigned In line 13-15 the source code les of mapper and reducer are assigned In line 16-19 the partition and sort rules are assigned In line 18 the reducer job number is set to 1 so that there is only one output le and could be load in line 24 of the next round of iteration  line 20 of the main procedure there is a level-1 loop of maximum log b  c  times  line 22-34 one round of role merge job is submitted In line 23 all the known roles will be loaded by each mapper job as a dictionary with the name of oldrole  In line 24 u pairs will be loaded by each mapper job as a dictionary with the name of upair   line 35 the roles generated in this round are counted If no role is generated the iteration in line 20 stops  line 37 roles with less than minc cb pairs are deleted The u pairs mapper pseudocode of A D 1 is summarized in Figure 6 and a detailed explanation is given below  line 3-4 two indexes for nding all cb pairs of each user and all user s of each cb pair are loaded  line 5 each mapper read a different ID and the total part number from respective le in the input directory  line 6 there is the level-1 loop of u times  line 7 input users are separated by allcount and selected by no  By the modulus and remainder different mappers are allocated different users  line 8 we print the u pair of the role itself  line 9 we get all the cb pairs related to u 1  line 10-13 all the users which have some common cb pairs with u 1 are selected and the number of common cb pairs are counted  line 14 there is the level-2 loop of maximum u times  line 15 the users with smaller id than u 1 and less than minc cb pairs are skipped  line 16-17 the u pairs are printed to standard out The output of u pairs mapper are partitioned by the rst user id  and sorted by the whole line The u pairs reducer pseudocode of A D 1 is summarized in Figure 7 and a detailed explanation is given below Fig 6 u pairs Mapper Code of Algorithm A D 1 Fig 7 u pairs Reducer Code of Algorithm A D 1  line 4-6 u pair text lines are read and parsed from the standard I/O input one by one  line 7-8 add uid 2 to the u pair list of uid 1  line 10 and 14 only the users with more than minu u pairs will be printed to standard I/O output  line 11 and 15 one single user and its u pair list are printed to standard I/O output  line 12-13 start to build u pair list for the next user The Merger mapper pseudocode of A D 1 is summarized in Figure 8 and a detailed explanation is given below  line 3 and 4 the known roles and sub roles are loaded from the output of the Merger reducer of last round  line 5 from the data in roles  a index for nding all the roles of a user is build and it is loaded in the share memory  line 6 all u pairs are loaded 471 


Fig 8 Merger mapper code of algorithm A D 1  line 8 each mapper reads a different ID and the total part number from respective le in the input directory  line 9 and 19 there are the level-2 and level-3 loops of maximum r times But we ignore the role pairs that have been merged in the former rounds by line 22-23  line 10 input roles are separated by allcount and selected by no  By the modulus and remainder each mapper is allocated a different part of role pairs  line 11-13 the sub roles are outputted to the stdout and skipped in the following processing  line 14 output the current role with status  old    line 15-18 all the related roles which have some same user with the roles  i  are selected And the number of common users are counted  line 20-21 only the roles with bigger position in the role list are merged with Fig 9 Merger reducer code of algorithm A D 1  line 24-25 the user intersection and cb pairs union of these two roles is computed  line 26 and 30 the roles with only 1 cb pair will not be processed after the rst round  line 27-29 and 31-33 a sub role is detected and added to the sub role list  line 34-36 if we can not nd enough u pairs for a single user in the user intersection of the two roles this user is deleted from the intersection  line 37 and 38 a new role is created Its user set is the pruned user intersection of these two roles and its cb pair set is the union of that of these two roles In the Merger mapper output one role is presented as one text line with three parts separated by tab s The rst part is its cb pair list the second part is its user list and the last part is its status The input of the Merger reducer is sorted by the rst 2 parts of a role text line so that the same roles will be place in the neighbor lines which allows further status merging The function of Merger reducer is mainly about removing duplicate roles and merging the status marks for the same roles Its pseudocode is summarized in Figure 9 and a detailed explanation is given below  line 4-6 role text lines are read and parsed from the standard I/O input one by one  line 8-11 the statuses of the same roles are merged For each role the sub mark replaces the old and new mark while the old mark only replaces the new mark  line 14 and 17 the roles are printed to standard output V E XPERIMENTS The performance and scalability of A D 1 are tested on the cluster at the Faculty of Engineering in University of New South Wales 472 


 Leonardi Cluster Torque 5.4.0 Python 2.4.3 Leonardi is a medium-sized high performance Computing cluster It currently consists of 2,944 AMD Opteron 6174 2.20GHz processor cores with a total of 5.8TB of physical memory essentially 2GB of memory per core and 100TB of usable disk storage Leonardi runs the Rocks clustering platform on top of CentOS Linux The maximum of 192 processor cores are used exclusively for our experiments Restricted by the platform of Torque in the Leonardi cluster we do not use Hadoop to manage the mapper and reducer jobs Instead we develop a simple module to nish splitting sorting and merging work to simulate the mapreduce computing model on this platform Since most of the time cost of A D 1 are generated by its mapper jobs the implementation of our module will not affect the measurement of the experiments We also believe that these experiments can be easily replicated using Hadoop A Performance trend of algorithm A D 1 In Figure 10\(a X-axis is set to user number from 2000 to 10000 and Y-axis is set to time cost The input BiUCB matrices are all random sparse matrices with 50 contexts 3 behaviors and the same density minu 8 and minc 3 are applied to control the total time cost In Figure 10\(b X-axis is set to context number from 50 to 100 and Y-axis is set to time cost The input BiUCB matrices are all random sparse matrices with 1000 users 3 behaviors and the same density minu 3 and minc 2 are applied to control the total time cost In both of them CPU core number is 48 The time cost of A D 1 increases according with increasing of context and user numbers We also simulate a polynomial trend for them These lines could approximately act as a O n 3  incremental trend B Time Cost Reduction with minu and minc In all the following experiments the input BiUCB matrix is a random sparse one with 50000 users 30 contexts and 3 behaviors the average number of cb pairs for each user is 9 In Figure 11 the number of CPU cores is 48 In Figure 11\(a X-axis is set to the minu from 4 to 32 and Y-axis is set to the time cost minc 5 is also applied to control the total time cost In Figure 11\(b X-axis is set to the minc from 3 to 6 and Y-axis is set to the time cost minu 17 is also applied to control the total time cost The time cost of A D 1 decrease dramatically according with the increasing of minu and minc  C Time Cost Reduction with Reverse Index In Figure 12 the number of CPU cores is 48 X-axis is three algorithms and Y-axis is set to the time cost minu 16 and minc 5 are also applied to control the time cost A  D 1 is the algorithm without the revert index A D 1 obtains considerable performance improvement with search space reductions D Scalability test of algorithm A D 1 In Figure 13 time costs seconds are recorded according with the increasing of CPU cores from 3 to 192 and minu  17 and minc 4 are applied to control the total time cost The second row is the total time cost the third row is the time Fig 12 Time Cost Reduction with Revert Index Fig 13 Execution time vs CPU cores cost of all the mappers while the fourth row is the time cost of all the reducers the last row is the time cost of standalone steps in the process such as generating the share revert index In Figure 14 X-axis is set to be the number of CPU cores and Y-axis is set to the time cost reduced multiples The total time cost when CPU core number is 3 is set as the baseline while the value for any other CPU core numbers are 26162 divided by their total time cost The time cost for task deploying data transferring and the job management increases with the increasing of CPU core number and also there are some standalone steps in the algorithm As a result the total time cost is not exactly reduced linear related to the increasing of CPU cores yet still reduced dramatically The values in the last row in Figure 13 are almost consistent For this test data set after the the CPU core number become laster than 96 the total time cost will not reduce very much more for the time cost portion of standalone steps becomes considerable VI C ONCLUSIONS In this paper we have proposed a novel algorithm for context-aware role mining Role mining problem has been Fig 14 Scalability of algorithm A D 1 473 


a Execution time vs number of contexts b Execution time vs number of users Fig 10 Performance trends of algorithms A D 1 a Execution time vs minu b Execution time vs minc Fig 11 Time Cost Reduction with minu and minc proven to be difìcult to compute it is in fact NP-hard and also shown to be hard to approximate The algorithm proposed in this paper offers complete role coverage and yet provides efìcient and scalable performance in which the partition and search space reduction problems are also overcomed A D 1 is a distributed computing algorithm that has been deployed on a high performance cluster to test and conìrm its performance on tens thousands of users and complex context-behaviors structures R EFERENCES  S Aich S Mondal S Sural and A K Majumdar  Role based access control with spatiotemporal context for mobile applications Transactions on Computational Science  4:177Ö199 2009  B J Biddle Role Theory Expectations Identities and Behaviors New York Academic Press 1979  H Cao T  Bao Q Y ang E Chen and J T ian An ef fecti v e approach for mining mobile user habits In CIKM  pages 1677Ö1680 2010  G.-P  Chen Y B Y ang and Y  Zhang Mapreduce-based balanced mining for closed frequent itemset In ICWS  pages 652Ö653 2012  J Dean and S Ghema w at Mapreduce Simpliìed data processing on large clusters CACM  51\(1 2008  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters Commun ACM  51\(1 Jan 2008  A Ene W  G Horne N Milosa vlje vic P  Rao R Schreiber  and R E Tarjan Fast exact and heuristic methods for role minimization problems In SACMAT  pages 1Ö10 2008  M Frank J M Buhmann and D A Basin On the deìnition of role mining In SACMAT  pages 35Ö44 2010  M Frank A P  Streich D A Basin and J M Buhmann A probabilistic approach to hybrid role mining In ACM Conference on Computer and Communications Security  pages 101Ö111 2009  Y  Fukaza w a  T  Naganuma K Fujii and S K urakak e Construction and use of role-ontology for task-based service navigation system In International Semantic Web Conference  pages 806Ö819 2006  S Ghema w at H Gobiof f and S.-T  Leung The google le system In Proceedings of the nineteenth ACM symposium on Operating systems principles  SOSP 03 pages 29Ö43 New York NY USA 2003 ACM  D K ulkarni and A T ripathi Conte xt-a w are role-based access control in pervasive computing systems In I Ray and N Li editors SACMAT  pages 113Ö122 ACM 2008  H Lu J V aidya and V  Atluri Optimal boolean matrix decomposition Application to role engineering In ICDE  pages 297Ö306 2008  C Masolo L V ieu E Bottazzi C Catenacci R Ferrario A Gangemi and N Guarino Social roles and their descriptions In KR  pages 267 277 2004  J B Orlin Contentment in graph theory Co v ering graphs with cliques Indagationes Mathematicae Proceedings  80\(5 1977  R S Sandhu E J Co yne H L Feinstein and C E Y ouman Rolebased access control models IEEE Computer  29\(2 1996  J V aidya V  Atluri Q Guo and Q Guo The role mining problem nding a minimal descriptive set of roles In SACMAT  pages 175Ö184 2007  J W ang C Zeng C He L Hong L Zhou R K W ong and J T ian Context-aware role mining for mobile service recommendation In SAC  pages 173Ö178 2012  S.-Q W ang Y B Y ang Y  Gao G.-P  Chen and Y  Zhang Mapreducebased closed frequent itemset mining with efìcient redundancy ltering In ICDM Workshops  pages 449Ö453 2012  T  White Hadoop The Deìnitive Guide  OêReilly Media Inc 1st edition 2009  R K W ong V  W  Chu T  Hao and J W ang Conte xt-a w are service recommendation for moving connected devices In International Conference on Connected Vehicles and Expo ICCVE  2012 474 


a Outside View b Inside View Figure 13  Mock cave test site Tunnel extends approximately 300m Cave 003oor is covered in rocky material to emulate planetary terrain Site contains surface terrain and tunnel inside building Exploration Performance Experiments A comparison of exploration performance between distributed centralized and uncoordinated task allocation was conducted using a 2-robot team of 2D mapping robots For the uncoordinated runs tasks were randomly assigned to a robot and the robot randomly decided whether to keep the task not taking into account any costs associated with that robot's performance of the task Maps at a resolution of 0.05 meters per pixel were built from 5 runs of each type Each run lasted 15 minutes The operator indicated tasks that should be performed and the system assigned these tasks to a robot In 3 out of 5 runs for each set the 002rst selected task was in the direction of the bridge in the other 2 it was in the direction of the dead-end to the right of the starting position in Figure 12 The same operator selected tasks for all runs Tables 1 2 and 3 show results for each run Percent explored is the percentage of the explorable area as determined from the ground truth model that the robot team explored Unique to total is the ratio of the area explored by a single robot to the total area explored by the team This metric gives a sense of how much overlapping work the robots are doing The average percent explored for runs with a 002rst task in the direction of the bridge was 68 and for runs with the 002rst task in the direction of the dead-end 67 The average ratio of unique to total explored for these cases was 0.45 and 0.44 respectively The average over all runs was 67 of explorable area covered and a ratio of 0.45 unique to total explored area Figure 14 shows merged maps for the runs with the largest and smallest explored areas in this experiment Table 1  2D Mapping Results Distributed Run Bridge 1st  Explored Unique:Total 1 1 80 0.57 2 1 52 0.43 3 1 94 0.45 4 0 97 0.76 5 0 60 0.37 Mean 77 0.52 Std Dev 20 0.15 Table 2  2D Mapping Results Centralized Run Bridge 1st  Explored Unique:Total 1 1 75 0.58 2 0 52 0.46 3 1 55 0.29 4 0 49 0.15 5 1 62 0.59 Mean 59 0.41 Std Dev 10 0.19 Table 3  2D Mapping Results Uncoordinated Run Bridge 1st  Explored Unique:Total 1 1 51 0.20 2 0 54 0.26 3 1 73 0.49 4 0 87 0.65 5 1 68 0.42 Mean 67 0.41 Std Dev 15 0.18 These results show high performance variation within run sets and do not show signi\002cant difference between sets The direction of the 002rst assigned task did not signi\002cantly affect results Limited navigation and path planning capabilities on individual robots common for all runs likely introduces signi\002cant randomness If the robots could more reliably complete their assigned tasks differences between task allocation strategies would likely become more evident Exploration of larger areas could also make differences clearer as more tasks would need to be assigned The lack of signi\002cant differences between allocation methods is somewhat encouraging however It indicates that distributed task allocation the method believed to be most promising for planetary missions does not perform any worse than other methods in early tests The uncoordinated method while by far the simplest would fail once robots with different capabilities are introduced Failure would occur for example if a 3D modeling task were assigned to a 2D mapping robot Mapping and Modeling Experiments An experiment including both 2D mapping and 3D modeling was conducted at the patio test site In this experiment the two 2D mapping robots were operated as described in section 6 A mapped area was then selected by the operator for 3D modeling and the 3D modeling robot was sent to complete that task There was no time limit on the run Figure 15 shows 9 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


