Distributed Storage Evaluation on a Three-Wide Inter-Data Center Deployment Yih-Farn Chen Scott Daniels Marios Hadjieleftheriou Pingkai Liu Chao Tian Vinay Vaishampayan AT&T Labs-Research Shannon Laboratory 180 Park Ave Florham Park NJ 07932 Email  chen,daniels,marioh,pingkai,tian,vinay  research.att.com Abstract The demand for cloud storage is exploding as an ever increasing number of enterprises and consumers are storing and processing their data in the cloud Hence distributed object storage solutions e.g QFS Swift HDFS are becoming very critical components of any cloud infrastructure These systems are able to offer good reliability by distributing redundant information across a large number of commodity servers making it possible to achieve 10 nines and beyond with relative ease One drawback of these systems is that they are usually designed for deployment within a single data center where node-to-node latencies are small Geo-replication i.e distributing redundant information across data centers for most open-source storage systems is to the best of our knowledge accomplished by asynchronously mirroring a given deployment Given that georeplication is critical for ensuring very high degrees of reliability e.g for achieving 16 nines in this work we evaluate how these storage systems perform when they are directly deployed in a WAN setting To this end three popular distributed object stores namely Quantcast-QFS Swift and Tahoe-LAFS are considered and tested in a three-wide data center environment and our ndings are reported I I NTRODUCTION Modern distributed object storage solutions like  Quantcast-QFS[3 T ahoe-LAFS[4 Riak[5  Colossus[7 and Amazon Dynamo[8 of fer v ery good availability and reliability at a low price point by distributing data across a very large set of inexpensive commodity servers that consist of unreliable components Despite their success many of these systems have been designed to distribute data across large clusters of servers redundantly either by using replication or erasure coding primarily within a single data center  Geo-replication i.e distribution of redundant information across data centers is typically handled by asynchronously mirroring a given deployment There are several limiting factors that affect the performance of distributed object store deployments in a WAN setting in the presence of TCP slow start and shared bandwidth as will become clear after an in-depth description of these systems in Section II In this work we evaluate three distributed object stores namely Quantcast-QFS Swift and Tahoe-LAFS in a three-wide data center environment in order to assess the impact of WAN latencies on these systems We focus on the read/write performance of these systems and ignore other features such as repair ease of use maintainability recovery performance and compatibility to other system components These considerations though important are more subjective and application dependent Moreover our eventual goal is to understand the weaknesses of these distributed storage systems  IL NJ GA 32.6 MiB/s 60.2 MiB/s 25ms P D F i l l  P D F  E d i t o r  w i t h  F r e e  W r i t e r  a n d  T o o l s Fig 1 Multi-site Data Center Deployment with Network Bandwidths in a WAN setting and thus we make the conscious choice on these restricted but most fundamental issues II Q UANTCAST QFS T AHOE LAFS AND S WIFT In this section we provide some background information for the three open-source storage systems that we chose to evaluate and brieîy describe the characteristics that are relevant to this work A Quantcast-QFS Quantcast-QFS is a high-performance fault-tolerant distributed le system developed by Quantcast corporation implemented in C that underlies its MapReduce infrastructure 1  File storage in QFS utilizes the 9  6 Reed-Solomon RS codes or simple replication codes The system consists of a large number of chunk servers and a single meta-data repository residing on a dedicated meta-server Each chunk server is responsible for storing erasure-coded chunks of les The meta-server is responsible for balancing the placement of chunks across the set of chunk servers maintaining the chunk placement information and issuing repair requests In order to understand the processes underlying a read/write operation in QFS we rst need to introduce some basic terminology QFS uses the concepts of chunk blocks chunks stripes and strides for storing data in an erasure-coded form across a set of chunk servers Figure 2 shows how this terminology translates into the physical layout of the data stored on the chunk servers Assume that we are given a 1GiB 1 Note that Quantcast-QFS is not related to SAN-QFS developed by Sun corporation In what follows we will refer to Quantcast-QFS as QFS 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


Server 1 D 1   D 7  D 13   Server 2 D 2  D 8  D 14    Server 6 D 6  D 12  D 18   Server 7 P 1  P 4  P 7   Server 8 P 2  P 5  P 8   Server 9 P 3  P 6  P 9   D 1 D 2 D 6 P 1 P 2 P 3 Stride  Data Parity Chunk block Chunk = 64MiB Stripe = 64KiB D 6139   Fig 2 QFS physical data layout le we store at each chunk server up to 64MiB chunks we use the default 9  6 coding and we set the stripe size i.e the size of each individual cell in Figure 2 to be 64KiB The le will consist of several chunk blocks where each chunk block is treated independently In our example a chunk block consists of 9 chunks i.e rows 6 chunks containing data and 3 chunks containing parity information If possible each chunk will be disseminated to a distinct failure domain failure domains can be deìned to be individual nodes racks zones etc such that loss of up to 3 data/parity chunks will not result in any data loss A write happens in a streaming fashion When a client needs to store a le it rst contacts the meta-server and requests 9 chunk servers that will be responsible for storing the rst set of 9 64MiB chunks Then once 6 stripes of data i.e a data stride which is equal to 6  64  384 KiBs in our example become available for writing the client erasure codes the data stride resulting in a full stride data and parity Finally the client appends each data/parity stripe within the stride to the corresponding chunk on each of the 9 chunk servers so chunks ll up from left to right in the gure one column at a time Once a chunk is full i.e once 64MiB/64KiB=1024 strides have been appended the chunk block becomes full and the chunk servers notify the meta-server that the block has been written when and how often the meta-server is contacted by the chunk servers is an interesting design issue in itself Then the process repeats from the beginning using a new set of 9 servers until the whole le is written Notice that depending on application characteristics one can tune the chunk size and stripe size parameters accordingly in order to minimize the main memory needed for erasure coding strides the total number of roundtrips between client chunk servers and the meta-server etc Notice that each write request in QFS results in possibly multiple round-trips between the client and the meta-server as well as the meta-server and the chunk servers Data reads are accomplished by choosing 6 out of 9 chunks based on load availability etc retrieving those chunks and reconstructing all data strides Notice also that a client can request to read a random offset from a given data object in which only the corresponding partial chunks from each chunk block will be returned to the client The meta-server in QFS is a single-point-of-failure SPOF however there exists a checkpointing mechanism such that the whole system can recover fairly quickly without losing les that had been successfully stored before the last checkpoint In addition to achieve high reliability QFS allows for certain placement policies to be speciìed such that chunks are placed onto different failure domains e.g across nodes racks and zones Finally given that chunk servers issue frequent keepalive heartbeats to the meta-server the meta-server has a very accurate view of the status of the cluster and is hence responsible for issuing repair requests for lost chunks B OpenStack Swift As part of the OpenStack architecture Swift was originally designed to provide a reliable distributed storage facility for maintaining virtual machine images Despite its roots it can be used as a generic object store that uses replication for increased reliability It is implemented in Python Swift comprises of proxy servers object servers container servers and account servers Swift also depends on three rings one for accounts one for containers and one for objects The rings are based on consistent hashing cf Amazon Dynamo  and are used for identifying the serv ers that are responsible for storing information about an account a container and an object respectively Each ring conceptually divides the entity namespace into a pre-speciìed number of partitions and each partition is assigned to multiple object servers hence entities falling within a given partition are replicated multiple times the replication factor is conìgurable Each partition replica is guaranteed to reside in a different failure domain or zones in Swift terminology where the zones are statically deìned during system conìguration Swift rings are static and they only change manually In other words when a server fails the corresponding ring data structure is not updated but the proxy server handling a request that maps to the failed server will detect the failure and use the existing ring to identify a handoff server which will temporarily replace the failed server When the failed server rejoins the cluster a repair process will be initiated involving the previously failed server and the handoff server to bring the system back to steady state If the failed server is permanently removed from the cluster a system administrator will have to update the corresponding ring and another repair process will guarantee that all objects that were stored on the failed server are replicated three times Notice that the ring architecture implies that each account/container/object server is responsible for only a fraction of objects Hence objects are uniformly spread across all servers without the need for a load balancing mechanism The proxy servers are responsible for handling user requests and also act as load-balancers rewalls and a caching layer for the Swift cluster for caching accounts containers and objects Writing an object involves locally storing the object at the proxy server consulting the appropriate ring to nd the locations where that object needs to be stored and transmitting the object to that location If the object is an account it is transmitted to the appropriate account servers Account servers are responsible for storing a list of container names associated 18 


with each account This information is maintained in a SQLite database and replicated in three account servers Similarly if an object is a container it is transmitted to the appropriate container server Container servers are responsible for storing a list of object names associated with each container Once again this information is maintained in a SQLite database on each container server Finally for rst class objects the object is copied to three object servers as a binary le on the local le system Delete requests are symmetric to writes Notice that in Swift each write/delete request needs to update the appropriate container database and store/delete the actual object in the appropriate object server which results in two round-trips per request Object read requests do not involve the container servers In terms of repair Swift takes a pro-active approach Each object server periodically checks whether each partition the server is responsible for needs to be repaired For each partition the server polls all other object servers that should also be replicating this partition and if any objects within the partition are missing or if an object server has failed rsync is run to repair the partition or replicate the partition to a temporary handoff server C Tahoe-LAFS Tahoe-LAFS Least-Authority File System is an opensource distributed le system implemented in Python that can tolerate multiple data server failures or attacks while preserving the privacy and security of the data The underlying idea is that users can store data on the Tahoe-LAFS cluster in an encrypted form using standard cryptographic techniques Clients are responsible for maintaining the necessary cryptographic keys needed to access the data and without those keys no entity is able to learn any information about the data including its placement across the cluster In addition data is erasure-coded for increased reliability and erasure coding parameters can be modiìed in the client conìguration le on a per-client basis A Tahoe-LAFS cluster consists of a set of storage peers and a single coordinator node called the Introducer whose primary purpose is to announce the addition of new storage peers to the existing pool of peers by following a publish/subscribe paradigm and relay relevant node information to clients upon read/write requests The storage servers are responsible for storing le shares Shares are encoded pieces of les Each le has a conìgurable number of shares When a client issues a write request it rst encrypts the le locally and breaks it up into segments It then erasure codes each segment creating blocks and sends one block from each segment to a particular server The set of blocks on a given server is one share Notice that so far the only difference between QFS and Tahoe-LAFS is that Tahoe-LAFS does not split the les into multiple chunk blocks The client also computes a set of cryptographic hashes on the encrypted les and each le segment and creates Merkle trees that are stored along with each share The client can read a le by reversing this process Tahoe-LAFS chooses the servers that will store the shares of the le using a hashing algorithm The hash of the encryption key of the le is used to generate a list of servers First the introducer removes all servers that do not have enough capacity to store a given share Then the introducer returns a list of servers to the client The client generates a hash value for each server using a concatenation of the hashed encryption key and the server name and sorts the resulting hash values to create a random server permutation Subsequently the client asks each server in that order whether it can hold a share of the le or not and repeats until all shares have been assigned to a server A single server can be responsible for multiple shares When a client needs to read a le it has to ask all known servers whether they are holding any shares of the le or not So far Tahoe-LAFS is a pure key/value store that ofîoads the responsibility of managing keys to the clients In other words it does not maintain directories and le names In order to make Tahoe-LAFS behave more like a le system it implements a le system layer which is essentially meta-data information about directories and les organized into a data structure that is then stored as a native object within TahoeLAFS itself On the outset this seems similar to the SQLite databases in Swift At closer inspection though this is not the case Notice that in Tahoe-LAFS all les are encrypted and erasure coded Hence writing a le involves rst updating the le system meta-data object which in turn implies retrieval of the whole object decryption and updating at the client side and writing the updated version back to Tahoe-LAFS and then writing the actual le Hence each write/delete request involves rst accessing and updating the le system metadata object and then updating the shares on the corresponding storage servers This results in two round-trips per server in the whole cluster Reading a le also involves accessing the le system meta-data object since in order to locate the le shares the client needs to nd the hashed encryption key used to store the le which is stored in the le system meta-data object Hence reads also require two round-trips between the client and all servers in the cluster The introducer in Tahoe-LAFS is a single-point-of-failure for new clients or new storage peers since they need to rely on it to be able to join the storage network However it is not a SPOF in the traditional sense because in Tahoe-LAFS the le placement information is decided by hashing the keys held by the clients and the introducer does not have to maintain any le or chunk placement information therefore losing the introducer does not jeopardize the normal operation of existing clients and storage peers The downside of this introducer-storage-nodes architecture is that due to the lack of a centralized meta-server TahoeLAFS is not able to provide complete statistics and details for le storage and placement though from a security point of view this is a choice by design Moreover only a lazy repair policy can be implemented in other words the clients are responsible for frequently iterating through their le space and verifying that all les are complete however when a storage peer fails the system is not able to initiate an efìcient repair mechanism speciìcally to replace the given peer III T HE T EST E NVIRONMENT For our tests two physical conìgurations are used The main conìguration consists of three geographically distant data centers The baseline conìguration consists of a single data center 19 


The three-site layout is depicted in Figure 1 The sites selected for the test were roughly arranged in a geographically equilateral triangle with several hundred miles separating each Site 1 is in IL site 2 is in NJ and site 3 is in GA The connectivity between sites varies signiìcantly from site to site and we observe that network characteristics are not symmetric Maximum throughput between sites varied depending on direction Furthermore in one case throughput varied signiìcantly from one measurement to the next even when measurements were taken within very short time spans While the network was far from ideal and prevented us from determining the best performance that could be obtained from each storage system direct comparison of the three systems is still possible and meaningful For the three-site conìguration within each data center we used three hosts as storage servers and one host referred to as meta host in what follows for supporting tasks i.e the QFS meta-server the Swift proxy and the Tahoe-LAFS introducer The meta host in GA was also used to drive the tests Each host has an Intel Xeon E5-2690 8-Core processor 2.9 GHz 64GiB of main memory and nine 7200RPM SATA drives of 2TB each in a JBOD conìguration running XFS The operating system is Ubuntu 12.04 LTS with Linux kernel 3.2.0 For the single site conìguration we used a total of ten hosts nine as storage servers and one meta host for everything else All hosts within a data center were connected to the same switch using 10-gigabit Ethernet All the tests were driven using Cosbench Cloud Object Store benchmarking an open-source tool de v eloped by Intel Cosbench implements a driver that simulates user-deìned workload conìgurations Users can choose the characteristics of the workload in terms of the size of les the type and percentages of create/write/read/delete operations and the number of concurrent threads of execution We extended Cosbench by adding plug-ins to provide interfaces to QFS and Tahoe-LAFS the interface to Swift is provided as part of the Cosbench package The tests were organized such that data written to the object store was randomly generated and data read from the object store was discarded no disk I/O on the test host impacted the throughput measurements Results presented in this paper with regard to performance are measured in MB/s powers of 10 and are referred to by Cosbench as bandwidth In Cosbench bandwidth is not computed similarly to the traditional throughput measurement i.e total bytes over elapsed time but it is a summation over the throughput of each individual thread of execution This method of calculation can yield larger values than would be observed by the traditional computation especially for tests involving les of variable sizes because it does not capture the idle time for threads in-between job scheduling Clearly bandwidth is a good measure in practice because it does not reîect any design choices related to the Cosbench job scheduler itself IV R ESULTS Measurements were collected for each system either reading or writing xed-sized objects Tests with concurrent reading and writing were not conducted In our tests we used workloads consisting of 100MB and 1GB objects For brevity we Read Bandwidth  1500 241 38 6 0 MB/s Log Scale 10 20 30 40 50 Threads QFS Swift Tahoe Fig 3 QFS/Swift/Tahoe-LAFS Multi-Site Read Performance Write Bandwidth 1500 241 38 6 0 MB/s Log Scale 10 20 30 40 50 Threads QFS Swift Tahoe Fig 4 QFS/Swift/Tahoe-LAFS Multi-Site Write Performance present averages over all workloads The tests were executed with a varying number of concurrent threads In most cases the addition of threads up to a point increased overall bandwidth but also resulted in the degradation of the throughput of each individual thread as expected due to contention on resources A Multi-site Test Results The results are shown in Figures 3 and 4 There are a few observations that should be noted  Writing of larger objects 1 GB vs 100 MB not shown in the gures was observed to be slightly faster for all three storage systems for the same total volume of data written This is likely due to the reduced number of interactions per MB with the meta/proxy server The difference though was not signiìcant  Read performance from multiple sites was slightly better than write performance for all three storage systems This is expected in erasure-coded systems since reads in fact transfer less data than writes  Performance increased as more threads to a point were allowed to access the objects Overall we can see that QFS exhibits the best performance and scalability with respect to concurrent requests Note that the y-axis is in log-scale Also notice that as more threads are used the read/write performance attens to a point where the available bandwidth dictates what is the maximum throughput that can be obtained as expected B Single-Site Test Results In order to understand the impact that the network imposes on a multi-site environment we established a single site with 20 


Read Bandwidth  1500 241 38 6 0 MB/s Log Scale 10 20 30 40 50 Threads QFS Swift Tahoe Fig 5 QFS/Swift/Tahoe-LAFS Single-Site Read Performance nine storage hosts and one meta host Two sets of tests were conducted one is when the reads/writes are initiated from a node located in the same site which is referred to as local read/write the other is when they are initiated from a node located in a different geo-location which is referred to as remote read/write For the former QFS Tahoe-LAFS and Swift are all tested while for the latter we focus on QFS Figures 5 and 6 show the single-site test results while Figures 7 and 8 show the remote read/write performance for QFS A few observations are noted below  In terms of local read/write performance QFS is the clear winner among the three systems Notice the logarithmic scale on the y-axis once again  Accessing the single site object stores from a local host results in nearly one order of magnitude increase in performance as shown in Figures 7 and 8 Even taking into account the bandwidth limitation between data centers this signiìcant difference is remarkable We suspect that this is due to the system optimization done in QFS for single-site deployment further discussion is included in Section V  When accessing the single site object store from a remote location the performance drops slightly below the performance observed with the object store distributed across multiple sites This small difference might be caused by the fact that when testing on the multi-site conìguration one set of storage hosts is local to the host running the Cosbench driver and thus one third of the data transfers are local Notice that in any application scenario where we expect the majority of client requests to originate from remote locations this implies that in fact deploying these systems in a multi-site conìguration is preferable than the single-site conìguration both in terms of reliability and performance Of course this is not the case for example for MapReduce deployments V D ISCUSSION There are certain limiting factors in terms of performance when trying to deploy distributed storage systems in a WAN setting mainly due to the latency introduced by the physical connection TCP slow start and of course due to shared bandwidth Systems that rely on a meta-server like QFS and for example HDFS introduce large latencies when reading/writing Write Bandwidth 1500 241 38 6 0 MB/s Log Scale 10 20 30 40 50 Threads QFS Swift Tahoe Fig 6 QFS/Swift/Tahoe-LAFS Single-Site Write Performance Read Bandwidth 1500 241 38 6 0 MB/s Log Scale 10 20 30 40 50 Threads Multi-site Local Remote Fig 7 QFS Single-Site Read Performance objects across the WAN because every read/write request for each chunk of the le incurs one round-trip delay to the metaserver for the client and each storage server involved although storage servers will typically aggregate multiple chunk replies into a single message digest for the meta-server If the metaserver is located in a remote data center from the client we expect this architecture to add a signiìcant overhead for read/write requests In addition given that QFS splits les into 64MiB chunks and chunks are uniformly distributed across all storage servers we expect QFS to suffer signiìcantly from TCP slow start Systems that use consistent hashing to determine object placement e.g Riak and Amazon Dynamo need to use a distributed consensus protocol e.g Paxos in order to keep the state of the cluster up-to-date which requires at least one round-trip delay per server for each cluster update But read/write requests happen independently of the cluster management protocol Hence read/write requests go directly to the relevant storage servers without any additional round-trip delays From that respect even though Swift uses consistent hashing in order to determine object placement the ring is statically allocated during system conìguration and can change only manually Hence Swift does not have the overhead of running a distributed consensus protocol to keep the consistent hashing ring up-to-date On the other hand Swift does have to maintain meta-data information accounts and containers as native objects within the object store itself hence incurring at least one round-trip delay for every update of each replica of the meta-data object Although in Swift this latency can be hidden completely due to the caching layer at the Proxy servers We also observe that under-provisioning the proxy setup in Swift can have a detrimental effect in terms of scalability Swift does not scale well as the number of concurrent threads increases resulting in a large number 21 


Write Bandwidth  1500 241 38 6 0 MB/s Log Scale 10 20 30 40 50 Threads Multi-site Local Remote Fig 8 QFS Single-Site Write Performance of dropped operations not shown in our gures since we plot averages across all workloads for successful operations only This is because all data transfers have to go through the proxy server which eventually becomes oversubscribed and starts to shed requests Clearly this is an indication that they proxy server is not designed to scale gracefully as the number of clients increases given that ostensibly for all storage systems eventually all data has to go through the sole Cosbench driver running on the meta host in which case we would expect Cosbench to become oversubscribed and we should be observing the same behavior for QFS and TahoeLAFS which is not the case Nevertheless a more robust Swift conìguration would include several proxy servers that would load-balance requests but this is something that we did not test in our conìguration and we plan to do as future work since it would require a multi-driver conìguration of Cosbench Tahoe-LAFS is similar to Swift in that the meta-data objects are stored within Tahoe-LAFS itself necessitating at least one round-trip delay for each erasure-coded share of the meta-data object for every write/delete request and an additional round-trip to all relevant servers to execute the request On the other hand reads are accomplished by submitting requests to all known storage peers given by the introducer simultaneously hence the relevant peers are found with one round-trip to every server In Tahoe-LAFS currently a second round-trip is incurred after choosing the peers to read the le from the intention here is to be able to select which peers to read from after the initial negotiation phase based on various heuristics But the poor performance of Tahoe-LAFS on the multi-site and single-site environment can be attributed to several factors already pointed out by the developers themselves First the default stripe size is optimized for reading/writing small objects the stripe size determines the granularity at which data is being encrypted and erasure coded Second there are several implementation issues such as inefìcient message passing and the expensive and frequent hash read/write seeks that are needed in order to reconstruct shares on the storage peers Third Tahoe-LAFS has to deal with the overhead of reading/writing the le system meta-data objects i.e the mutable directory objects every time an object is accessed Fourth when creating new objects Tahoe-LAFS has to generate a new public/private key which is an expensive operation Surprisingly reads exhibit the same performance as writes even though reads ideally have to transfer less data than writes This is probably because both reads and writes of shares happen simultaneously to all relevant storage peers hence the extra data transfers are hidden by parallelism Moreover this is an indication that pinging all available peers and requesting shares within two round-trips as well as the fact that every read request has to read the mutable directory object dominate the overall cost VI C ONCLUSION We conducted extensive experiments with QFS Swift and Tahoe-LAFS which are three very popular distributed storage systems Our focus was to deploy these systems in a multi-site environment and measure the impact of WAN characteristics on these systems In addition as a baseline we also measured performance on a single-site conìguration Overall we observe that WAN characteristics have an even larger than expected impact on the performance of these systems mainly due to several design choices of these systems Ideally across the WAN we would like to reduce the amount of round-trips to a minimum which is something not particularly important on a LAN In addition we notice that good system design and extensive optimizations can have a signiìcant effect on performance as is seen by the relative difference between QFS Swift and Tahoe-LAFS It is important to point out here that QFS is implemented in C Swift and Tahoe-LAFS in Python In addition QFS is heavily optimized for MapReduce style processing For future work we are planning to also test Riak and HDFS as well as our own proprietary solution that is designed from the ground up for WAN deployment R EFERENCES  D Borthakur  The hadoop distrib uted le system Architecture and design http://hadoop.apache.org/docs/r0.18.0/hdfs design.pdf 2007  Swift  http://docs.openstack.or g/de v eloper/swift  QFS  https://www quantcast.com/engineering/qfs  T ahoe-LAFS  https://tahoe-lafs.or g/trac/tahoe-lafs  Riak  http://basho.com/riak  B C J W ang A Ogus N Nilakantan A Skjolsv old S McK elvie Y Xu S Srivastav J Wu H Simitci et al  Windows azure storage a highly available cloud storage service with strong consistency in Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles  ACM 2011 pp 143Ö157  A Fik es Storage architecture and challenges  F aculty Summit 2009  G DeCandia D Hastorun M Jampani G Kakulapati A Lakshman A Pilchin S Sivasubramanian P Vosshall and W Vogels Dynamo Amazonês highly available key-value store in SOSP  vol 7 2007 pp 205Ö220  J Dean and S Ghema w at MapReduce simpliìed data processing on large clusters Communications of the ACM  vol 51 no 1 pp 107 113 2008  S B W ick er and V  K Bhar ga v a  Reed-Solomon codes and their applications  Wiley.com 1999  J Duan COSBench A benchmark tool for cloud object storage services OpenStack Summit Fall 2012 2012  L Lamport The part-time parliament  ACM Transactions on Computer Systems TOCS  vol 16 no 2 pp 133Ö169 1998 22 


100 1000 None 10 Test Data Accuracy Rotation interval 95 93 96 94 Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2   Figure 9  Results on the Page-blocks data with ten attributes \(Setting 1 100 1000 10 Test Data Accuracy 92 88 86 90 94 Rotation interval Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2 None  Figure 10  Results on the Segment data with 19 attributes \(Setting 1 100 1000 10 Test Data Accuracy 86 82 80 84 88 Rotation interval None Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2  Figure 11  Results on the Satimage data with 36 attributes \(Setting 1 C  Experimental Results from Setting 2 In the second setting D 1 has no missing attribute while about n 6 attributes are missing in the other six data subsets Similar results are obtained from the first and second settings for the phoneme data with five attributes since the number of missing attributes is one in both settings. Figs. 12-14 show experimental results on the other data sets in Setting 2 Since much more attributes are missing in Figs. 12-14 i.e., two in Fig. 12, four in Fig. 13, and six in Fig. 14\han Figs. 9-11 with a single missing attribute, positive effects of using all the seven data subsets decrease from Figs. 9-11 to Figs. 12-14. However, better results are still obtained by the use of all the seven data subsets \(black and red open circles than the use of D 1 blue closed circles\gs. 12-14 100 1000 10 Test Data Accuracy Rotation interval 95 93 96 None 94 Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2  Figure 12  Results on the Page-blocks data with ten attributes \(Setting 2 100 1000 10 Test Data Accuracy 92 88 86 90 94 Rotation interval None Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2  Figure 13  Results on the Segment data with 19 attributes \(Setting 2 100 1000 10 Test Data Accuracy 86 82 80 84 88 Rotation interval None Seven Subsets \(Scenario 1 Use of only D 1 Seven Subsets \(Scenario 2  Figure 14  Results on the Satimage data with 36 attributes \(Setting 2 One interesting observation in Figs. 12-14 is that better results are obtained in many cases from Scenario 2 than Scenario 1. This may be because new fuzzy rules are always generated from D 1 with no missing attributes in Scenario 2 In Scenario 1, new fuzzy rules are generated from D 1 with no missing attributes and also from the other six data subsets D i   i  2, 3, ..., 7\th missing attributes V  C ONCLUSION  In this paper, we examined the potential usefulness of our parallel distributed fuzzy GBML algorithm for fuzzy rulebased classifier design from multiple data sets with different missing attributes. We also examined its potential usefulness under the assumption of the severe privacy preserving policy where data sets were handled as black-box data sets. More 69 


specifically, we assumed that a black-box data set was used only for calculating the error rate of each classifier. No other information \(e.g., attribute values of each pattern, its true class, and its classification result\ were available In our computational experiments, we divided training patterns into seven data subsets: one complete data subset with no missing attributes and six incomplete data subsets with different missing attributes. We observed that classifier performance was improved by the use of all the seven data subsets in comparison with the use of only the complete data subset. This improvement was also observed even when the six incomplete data subsets were black-box data sets It was always assumed in our computational experiments that one data subset was fully available. It is an interesting future research issue to examine the learning from multiple data subsets where no data subsets are fully available \(i.e., all data subsets have different missing attributes and the severe private preserving policy\ch a difficult situation, more sophisticated handling of missing values may be needed since all data subsets have missing attributes. More efficient evolutionary learning may be also needed since we cannot generate fuzzy rules directly from training patterns R EFERENCES  1  R. Agrawal and R. Srikant, ìPrivacy-preserving data mining Proc of the 2000 ACM SIGMOD International Conference on Management of Data pp. 439-450, Dallas, May 15-18, 2000. DOI: 10.1145 342009.335438 2  Y. Lindell and B. Pinkas, ìPriva cy preserving data mining Proc. of the 20th Annual International Cryptology Conference \(LNCS 1880 Advances in Cryptology - CRYPTO 2000 pp. 36-54, Santa Barbara August 20-24, 2000. DOI: 10.1007/3-540-44598-6_3 3  Y. Lindell and B. Pinkas, ìPrivacy preserving data mining Journal of Cryptology vol. 15, no. 3, pp. 177-206, June 2002. DOI 10.1007 s00145-001-0019-2 4  V. S. Verykios, E. Bertino, I. N. Fovin, L. P. Provenza, Y. Saygin and Y. Theodoridis, ìState-of-the-art in privacy preserving data mining Sigmod Record vol. 33, no. 1, pp. 50-57, March 2004. DOI 10.1145/974121.974131 5  M. Kantarcioglu and C. Clifton, ìPrivacy-preserving distributed mining of association rules on horizontally partitioned data IEEE Trans. on Knowledge and Data Engineering vol. 16, no.9, pp. 10261037, September 2004. DOI: 10.1109/TKDE.2004.45 6  J. Vaidya and C. Clifton, ìPrivacy preserving association rule mining in vertically partitioned data Proc. of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pp. 639-644, Edmonton, July 23-25, 2002. DOI: 10.1145 775047.775142 7  A. Fern·ndez, S. GarcÌa, J. Luengo, E. BernadÛ-Mansilla, and F Herrera, ìGenetics-based machine learning for rule induction: State of the art, taxonomy, and comparative study IEEE Trans. on Evolutionary Computation vol. 14, no. 6, pp. 913-941, December 2010. DOI: 10.1109/TEVC.2009.2039140 8  S. GarcÌa, A. Fern·ndez, J. Luengo, and F. Herrera, ìA study of statistical techniques and performance measures for genetics-based machine learning: Accuracy and interpretability Soft Computing  vol. 13, no. 10, pp. 959-977, August 2009 DOI: 10.1007/s00500008-0392-y  9  J. Bacardit and N. Krasnogor, ìPerformance and efficiency of memetic Pittsburgh learning classifier systems Evolutionary Computation vol. 17, no. 3, pp. 307-342, Fall 2009. DOI: 10.1162 evco.2009.17.3.307 10  M. A. Franco, N. Krasnogor and J. Bacardit, ìGAssist vs. BioHEL critical assessment of two paradigms of genetics-based machine learning,î Soft Computing, vol. 17, no. 6, pp. 953-981, June 2013 DOI: 10.1007/s00500-013-1016-8 11  F. J. Berlanga, A. J. Rivera, M J. del Jesus, and F. Herrera, ìGPCOACH: Genetic Programming-based learning of COmpact and ACcurate fuzzy rule-based classification systems for Highdimensional problems Information Sciences vol. 180, no. 8, pp 1183-1200, April 2010. DOI:10.1016/j.ins.2009.12.020 12  J. Alcal·-Fdez, R. Alcal·, and F. Herrera, ìA fuzzy association rulebased classification model for high-dimensional problems with genetic rule selection and lateral tuning IEEE Trans. on Fuzzy Systems vol. 19, no. 5, pp. 857-872, October 2011. DOI: 10.1109 TFUZZ.2011.2147794 13  J. A. Sanz, A. Fern·ndez, H. Bustince, and F. Herrera, ìIVTURS: a linguistic fuzzy rule-based classification system based on a new Interval-Valued fuzzy reasoning method with TUning and Rule Selection IEEE Trans. on Fuzzy Systems In Press. Available from http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6420926 DOI: 10.1109/TFUZZ.2011.2147794 14  F. Herrera, ìGenetic fuzzy systems: Taxonomy, current research trends and prospects Evolutionary Intelligence vol. 1, no. 1, pp. 2746, March 2008. DOI: 10.1007/s12065-007-0001-5 15  H. Ishibuchi and Y. Nojima, ìAnalysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning International Journal of Approximate Reasoning  44 \(1\ 2007. DOI: 10.1145/775047.775142 16  M. Fazzolari, R. Alcal·, Y. Nojima, H. Ishibuchi, and F. Herrera, ìA review of the application of multiobjective evolutionary fuzzy systems: Current status and further directions IEEE Trans. on Fuzzy Systems vol. 21, no. 1, pp. 45-65, February 2013. DOI: 10.1109 TFUZZ.2012.2201338 17  Y. Nojima, H. Ishibuchi, and I. Kuwajima, ìParallel distributed genetic fuzzy rule selection Soft Computing vol. 13, no. 5, pp. 511519, March 2009. DOI: 10.1007/s00500-008-0365-1 18  H. Ishibuchi, S. Mihara, and Y. Nojima, ìParallel distributed hybrid fuzzy GBML models with rule set migration and training data rotation IEEE Trans. on Fuzzy Systems vol. 21, no. 2, pp. 355-368 April 2013. DOI: 10.1109/TFUZZ.2012.2215331 19  E. Alba and M. Tomassini, ìParallelism and evolutionary algorithms IEEE Trans. on Evolutionary Computation vol. 6, no. 5 pp. 443-462, October 2002. DOI: 10.1109/TEVC.2002.800880 20  S. Cahon, N. Melab, and E. G. Talbi, ìParadisEO: A framework for the reusable design of parallel and distributed metaheuristics Journal of Heuristics vol. 10, no. 3, pp. 357-380, May 2004. DOI 10.1023/B:HEUR.0000026900.92269.ec 21  H. Ishibuchi, T. Yamamoto, and T. Nakashima, ìHybridization of fuzzy GBML approaches for pattern classification problems IEEE Trans. on Systems, Man, and Cybernetics - Part B vol. 35, no. 2, pp 359-365, April 2005. DOI: 10.1109/TSMCB.2004.842257 22  H. Ishibuchi and T. Nakashima, ìEffect of rule weights in fuzzy rulebased classification systems IEEE Trans. on Fuzzy Systems vol. 9 no. 4, pp. 506-515, August 2001. DOI: 10.1109/91.940964 23  H. Ishibuchi and T. Yamamoto, ìRule weight specification in fuzzy rule-based classification systems IEEE Trans. on Fuzzy Systems vol 13, no. 4, pp. 428-435, August 2005. DOI: 10.1109/TFUZZ.2004 841738 24  H. Ishibuchi, K. Nozaki, and H. Tanaka, ìDistributed representation of fuzzy rules and its application to pattern classification Fuzzy Sets and Systems vol. 52, no. 1, pp. 21-32, November 1992. DOI 10.1016/0165-0114\(92\90032-Y 25  J. Alcal·-Fdez, L. S·nchez, S. GarcÌa, M. J. del Jesus, S. Ventura, J M. Garrell, J. Otero, C. Romero, J. Bacardit, V. M. Rivas, J. C Fern·ndez, and F. Herrera, ìKEEL: A software tool to assess evolutionary algorithms for data mining problems Soft Computing  vol. 13, no. 3, pp. 307-318, February 2009. DOI: 10.1007/s00500008-0323-y  70 


Copyright © 2009 Boeing. All rights reserved  Architecture Server-1 Server-2 DB2 SURVDB XML Shredder WebSphere Message Broker Ext.4 H Ext.3 G Ext.2 F Ext.1 E C WebSphere MQ TCP/IP Live ASDI Stream IBM Cognos Server-3 IBM SPSS Modeler SPSS Collaboration Deployment Services 


Copyright © 2009 Boeing. All rights reserved  Database Modeling Schemas for correlated ASDI messages translated into equivalent relational schemas  Database tables generated based on classes created from schema definitions  Nine main, eleven supporting tables  Each main table contains FLIGHT_KEY 


Copyright © 2009 Boeing. All rights reserved  Database Modeling 


Copyright © 2009 Boeing. All rights reserved  Correlation Process To archive received ASDI data  Track messages must be correlated with flight plan messages FLIGHT_KEY assigned Uncorrelated data tagged Approx 30 minutes to correlate one day of data 


Copyright © 2009 Boeing. All rights reserved  Historical Data Processing To load correlated data  Uncompress, unmarshall  Create a list of files containing the correlated data  Write data to warehouse 


Copyright © 2009 Boeing. All rights reserved  Live Data Processing Processed using IBM MQ IBM Message Broker and a technique called XML Shredding Message Broker Compute Nodes  Uncompress Node  Extract correlated messages  Shred Node adds to DB Stored Procedure ìshreds XML docs and adds to tables 


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


