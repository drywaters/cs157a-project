Minimum Storage BASIC Codes: A System Perspective Xianxia Huang, Hui Li, Tai Zhou, Yumeng Zhang, Han Guo, Hanxu Hou, Huayu Zhang, Kai Lei Shenzhen Engineering Lab of Converged Networks Technology, SPCCTA Shenzhen Graduate School, Peking University ShenZhen, China e-mail: lih64@pkusz.edu.cn, {hxx, zhoutai, zym, guohan, hhx, zhy}@sz.pku.edu.cn   Abstract  The explosion of big data stored in distributed file systems calls for more efficient storage paradigms. While replication is widely used to ensure data availability, erasure codes provide a much better tradeoff between storage and availability. Reed-Solomon \(RS\ codes are the standard design choice, however, their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability. BASIC codes can achieve the optimal tradeoff between storage capacity and repair bandwidth with much less complexity of regenerating codes, which is first proposed in [1  This paper integrate one construction of the minimum storage BASIC \(MS-BASIC\des [2 to a Ha do o p HDFS clu s te r testbed with up to 22 storage nodes.  We demonstrate that MSBASIC codes conform to the theoretical findings and achieve recovery bandwidth saving compared to the conventional recovery approach based on RS codes Keywords-big data; distributed file systems; minimum storage BASIC codes; Hadoop I  I NTRODUCTION  With the development of massive storage system and its application in complex environm ents, there is a big challenge in the reliability of storage systems. Distributed storage systems \(e.g., [3  4   5   f o r l a rg e clu ste r s ty pica lly u s e  replication to provide reliability. But it offers low storage efficiency as the amount of data becomes larger and larger Recently, as the main technology for fault tolerance in storage systems, erasure codes \(e.g., [6  7    a r e d r aw in g  more and more attention For this reason, Facebook and many other enterprises are transitioning to erasure coding techniques \(typically classical RS codes\ to introduce redundancy while saving storage [8  E r asu r e c o des  ar e def i n e d by  tw o p o si tiv e  integers k and n  n>k They are a kind of the k, n  maximum distance separable \(MDS\ codes [9 w h ich ar e  optimal in terms of the redundancy-reliability tradeoff. In a distributed storage system, the n packets are stored at different storage nodes \(e.g., disks, servers, or peers\ spread over a network, and the system can tolerate any n-k ode failures without data loss While deploying RS codes in data centers improves storage efficiency, it however results in a significant increase in the usage of disk and network bandwidth. This phenomenon occurs due to the considerably high download requirement during recovery of any missing block. In general under a k, n code, recovering a single block involves the download of the whole size of the original file, from which the required missing block is recovered. Thus repairing cost of RS codes is considered an unavoidable price to pay for high storage efficiency and high reliability There have been extensive studies on improving the recovery performance of erase-codes-based storage systems Huang et al 10 p r op os e l o c al re co v e ry  cod e s th a t re du c e  the bandwidth and I/O when reconstructing a lost data fragment. They evaluate the codes atop the Windows Azure Storage system. Sathiamoorthy et al 11 als o p r op os e l o c a l  recovery codes, and evaluate the codes atop HDFS-RAID 12  N o t e th at th e c o des in  b o t h stu d ies a r e n o n MDS c o des  with additional parties added to storage. Besides, much extra information is added in the system to indentify the two different kinds of parties Regenerating codes [13 ca n m i n i m ize th e r e c o v e ry  bandwidth. Li et al 14  p r o p o se CORE, w h ich re tain s existing optimal regenerating code constructions, and experiment on HDFS-RAID. Although CORE can save recovery bandwidth even in concurrent failure recovery, it is based on Galois Field GF  q   q 2\ and has to handle large multiplication operations. Therefore, it results in high computation complexity and energy costs In this paper, we have employed MS-BASIC codes for big data storage in Hadoop HDFS [15 test b ed  w ith  u p  t o 22  nodes. Our experiments demonstrate that MS-BASIC codes conforms to the theoretical findings and achieves recovery bandwidth saving. In summary, our work makes the following contributions  Theoretical analysis While this work builds on MS-BASIC codes [2  w e g i v e th e o re tic ally analy sis  and compare them with traditional erasure codes  Implementation We implement a prototype by modifying the source code of HDFS and its erasure coding extensions HDFS-RAID. We specify three key steps: \(1\ile upload; \(2\ file download; \(3\file recovery  Experiments We experiment on a cluster testbed with up to 22 storage nodes. Our experiments take into account a combination of several metrics including encoding/decoding performance, recovery time and network traffic The rest of the paper proceeds as follows. Section II first provides the basics of codes an d theoretical analysis findings then reviews the construction example of MS-BASIC codes Section III illustrates the implementation details of our prototype. The experimental results are discussed in Section IV. We finally conclude the whole paper and present our future work in Section V 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


  Stripe 1 D  Stripe            k data nodes m parity nodes        encode encode 2 D k D m P 1 P Figure 1 An erasure-code-based distributed storage system II B ASICS We first provide some background about erasure codes and their employment in distri buted storage system, followed by a description of MS-BASIC codes. Finally, we make a comparison between MS-BASIC and RS codes A MDS codes In a typical k, n erasure code, a file of size B is split into k blocks and encoded to generate n blocks. Specifically, the object D  D 1 D 2 D k  is a linear transformation defined by a  k n generator matrix G such that we can obtain an n dimensional code word C  12  n CC C   by applying the linear transformation C  D G The generator matrix G usually has the form k  GIG where k I is the identity matrix and  G is a k*m matrix Thus, the code word C becomes C  D P  where D is the original data and P is a parity vector Fig. 1 shows an erasure-code-based distributed storage system [16 co m posed of a co llection of n odes, each of which refers to a physical storage device. We assume the storage system contains n nodes, in which k nodes \(called data nodes\ the original data and the remaining m  n-k nodes \(called parity nodes\tor e parity data. A stripe is a collection of data on k data nodes and the corresponding encoded data on m parity nodes. Each stripe is independently encoded and decoded, which means erasure codes are constructed in the block of stripe. For load balancing, the identities of the data/parity nodes are rotated so that they can be distributed to different nodes For data availability, storage systems often employ an k n de, meaning that the stored data of any k out of the n nodes can be used to reconstruct the original data. That is, system can tolerate m concurrent failures without data loss. MDS codes also ensure optimal storage efficiency such that each node stores B  k per stripe. RS codes are a classical example of MDS codes and have been widely implemented in different systems B MS-BASIC codes Regenerating codes lie on an optimal tradeoff curve between storage cost and recovery bandwidth [17 B A SIC  codes can achieve the full benefit of regenerating codes with much less complexity. In this work, we focus on the minimum storage  BASIC codes in which each  node  stores  1 D  1 D  4 D  3 D  2 D  1 P  1 D  4 P  3 P  2 P                 Figure 2 An example of \(4, 8\ MS-BASIC code the minimum amount of data on the tradeoff curve. With the limited parameters of MSR, MS-BASIC codes must satisfy that k is even n 2 k The construction is carried out as follows 1 Fragment the file into k data blocks D   D 1  D 2  D k   with each size L=B/k 2 Construc t k redundancy blocks P   P 1  P 2  P k  by the formula  2-1  mo d    1 2  i kk ijij ji P Dr i k  1 where  i j r represents the number of cyclic shift in the packet D j for the parity P i   ij r is given as follows 1 21      0   21  mo d  i j i j i j k rr r i k ip  2 where p is the minimum prime, p k/2 3 Distribute the n packets to n different storage nodes According to the demonstration in an y  k nodes are able to reconstruct the original file. Besides, we can recover a single failure though k 2 nodes in the related sequence Fig 2 gives a simple example of \(4, 8\BASIC code and p 3. First, the file is fragmented into packets D 1  D 2  D 3  D 4 Then, parity P 1 can be produced though cyclic shift and addition of D 1 and D 2 that is P 1  D 1 0 D 2 1 We can get P 2  D 2 0 D 3 2 P 3  D 3 0 D 4 0\d P 4  D 4 0 D 1 1\ in the same way. If D 1 is lost, we can recover it though D 2 and P 1 if both of D 1 and D 2 are lost, we can first recover D 2 though D 3 and P 2 then recover D 1 All of the 8 packets are stored in differen t nodes and we can reconstruct the original file from any 4 nodes C Theoretical Analysis According to the description above, we know MSBASIC codes are based on GF 2\us they only need XOR operations. RS codes [6 h a ve t w o k i n d s of g e n e rato r m atrix to construction As Cauchy-based RS \(CRS\des eliminate the expensive multiplications of RS codes by converting them to extra XOR operations, we consider them as a comparison for MS-BASIC codes Table I presents theoretical analysis of MS-BASIC and CRS codes in several metrics with B original data being stored. Note that parameter w in CRS codes is a bit-word and the value of w must satisfy 21 w n  The performance of these two codes is typically quantified by the number of XOR operations performed. We can get that encoding 40 


operations of CRS codes is w times larger than that of MSBASIC codes from the table.  To repair a failure, repairing operations of CRS codes is kw times more than that of MSBASIC codes. As both of these two codes are k, n  codes, they suffice optimal storage efficiency, such that their storage cost in each node is B  k and the reconstruction bandwidth is B  In practical systems, network bandwidth is usually a valuable resource which is defined as the total amount of transmitted data. CRS codes employ conventional recovery that repairing a single failed block needs to download an amount of information equivalent to the size of the file from k different nodes. The recovery bandwidth of CRS codes is k times larger than the amount of repaired block. Meanwhile MS-BASIC codes can bring twice bandwidth reduction as they only need to contact with k 2 surviving nodes for recovery TABLE I T HEORETICAL A NALYSIS O F MS-BASIC AND CRS C ODES  Comparisons CRS MSBASIC Encoding Operations   2  On k wB    2  On k B Repairing Operations  2  O k wB  2  OB Storage Cost  Bk  Bk Reconstruction Bandwidth B B Recovery Bandwidth B  2 B III I MPLEMENTATION We complement our theoretical analysis with prototype implementation. We modify the source code of HDFS-RAID an open-source erasure code module developed by Facebook A Overview of HDFS-RAID In an HDFS-RAID cluster, there are a single NameNode and multiple DataNodes. The NameNode stores and manages the metadata for HDFS blocks, while the DataNodes store HDFS blocks. HDFS-RAID adds a new node called the RaidNode, which performs the striping operation. It also periodically checks any lost blocks to ensure the availability of blocks. HDFS-RAID uses a distributed RAID file system \(DRFS\to handle all read/write requests for the erasure-coded data stored in HDFS. If a lost block is requested, it performs degraded reads to the lost block which requires the RaidNode to perform the recovery operation One node in the cluster is generally designed to run the daemon of the RaidNode which relies on the underlying component: ErasureCode. ErasureCode module performs the encoding/decoding operations employing CRS and XOR codes by default To integrate MS-BASIC codes into HDFSRAID, we implement MS-BASIC codes though the interface of the ErasureCode module Consequently, ErasureCode module can execute our coding scheme B File upload Once the RaidNode detects a file which is suitable for raiding \(according to parameters set in the configuration file of raid.xml\unches the striping operation for the file The striping operation is carried out as follows. For a given k, n e RaidNode first downloads a group of k blocks \(from one of the replicas for each block\en encodes the k blocks into n blocks on a per-stripe basis using a specified coding scheme. The encoded data is spread across the cluster according to Hadoop  s configured block placement policy We rotate node identities when we place the blocks so that the parity blocks are evenly distributed across different DataNodes to achieve load balancing. The RaidNode repeats the same process for another group of k blocks. Depending on the size of the file, the last stripe may contain fewer than k blocks We can consider the last stripe as  zero-padded  stripe as far as the parity calculation is concerned. Unused replicas of the k blocks will later be removed from HDFS to reduce the storage cost C File Download When a DRFS client wants to download a file from the cluster, it first gets the related metadata information which contains the locations of the sp ecified file blocks. Then it downloads all the original data from k data nodes on a perstripe basis. Finally, it just simply merges the k original data and removes the  zero-padded  in the last stripe if needed Note that some of the k data nodes and the corresponding data may be unavailable, possibly because of the nodes being overloaded, network outage, or repair actions being yet to be carried ou t. In this case, the degraded read operation is performed by the DRFS client. Suppose that t nodes fail, where t k we have the DRFS client request a lost HDFS block on one of the failed nodes. The lost block will be reconstructed from the data of other surviving nodes Through the above theoretical analysis CRS and MS-BASIC codes need k and k 2 surviving nodes respectively. After reconstruction, we merge the k original blocks as previously described D File Recovery In the cluster, we manually delete some blocks to make the RaidNode starts a repairing process. The process for recovery is similar to degraded read operation except that the reconstructed data should be uploaded to new nodes. The followings describe the steps for file recovery 1 Inquire the NameNode to get all the data information about the lost blocks 2 Download the related inform ation for the lost blocks from surviving nodes 3 Reconstruct the lost blocks through the decoder in the RaidNode 4 Upload the reconstructed data to new nodes The new nodes should be different from the nodes that already store the related  blocks of the file for load balancing 5 Report the new data info rmation to the NameNode 41 


               k,n Encoding speed\(MB/s  CRS \(1.64GB  MS-BASIC \(1.64GB  CRS \(2.52GB  MS-BASIC \(2.52GB Figure 3 Encoding speed IV P ROTOTYPE E XPERIMENTS We experiment MS-BASIC codes on an HDFS testbed with one NameNode, one RaidNode and up to 20 DataNodes. NameNode runs on a quad-core server equipped with an Intel\(R\ Xeon\(R\5-2609 2.40GHz CPU, 24GB RAM, and a Seagate ST31000524AS 7200RPM 1TB SATA harddisk. The others run on a quad-core PC equipped with AMD A8-5600k 3.0GHz CPU, 4GB RAM, and a 500GB SATA harddisk. All machines are equipped with a 100Mbps Ethernet card and interconnected over a gigabit Ethernet switch. They all run in the same software environment HDFS release 0.22.0 with HDFS-RAID enabled and Centos5.5 We run two sets of experiments, one set to study MSBASIC codes, and the other to execute CRS codes as a reference We compare them using three different sets of coding parameters k, n 4, 8\, \(6, 12\\(8, 16\, and two test files with sizes of 1.64GB and 2.52GB, respectively The size of a block is 64MB by default and the bit-word used for CRS codes chooses w 8 Our experiments take into account a combination of several metrics, including encoding performance, download performance and recovery performance. The reported results are the average of 10 runs A Encoding  performance To evaluate the computational encoding overhead of CRS and MS-BASIC codes in construction, we measure how long they take for file striping. We define the encoding speed as the original size of the file divided by the total time for the entire striping operation, including disk I/O time From the results shown in Fig. 3 we can see that MSBASIC codes have a little smaller encoding speed than CRS codes in all the implementations. It is not matched the analysis in table I, however, it is interesting to notice that the generator matrix of CRS codes has been optimized and the number of XOR operations is far less than the theoretical value [18 W ith th e in crease of  k, n h encoding speeds show a little lower The reason is that we have to produce more parity blocks for the given file as the parameters becomes larger. In addition, the file of size 1.64GB has a higher encoding speed than the file of size 2.52GB in all cases. We can get that big files have higher encoding overhead and hence show worse performance than small files                k,n Degraded read speed MB/s  CRS \(1.64GB  MS-BASIC \(1.64GB  CRS \(2.52GB  MS-BASIC \(2.52GB Figure 4 Degraded read speed B Download performance We first download a file in the case of no data loss to evaluate the normal read speed. We compute the download speed which is defined as the amount of data being requested divided by the download time. As our machines are equipped with a 100Mbps Ethernet card, the average speed of download is 11.1MB/s. Besides, the two codes keep almost the same download time for a specified file, as they always download k original blocks We next shut down one node to perform degraded read The degraded read speed is shown in Fig. 4, and it has only half the normal download speed on average. MS-BASIC codes have a speed gain with respect to the reduction in XOR operations. Meanwhile, with the increase of k, n e speed of both codes becomes a little lower, mainly due to the need to contact with more nodes It is different from the encoding performance that the speed of big file is higher than that of small one. Take \(8,16 for example, the small file has a speed of 5.81MB/s smaller than 5.92MB/s of big file in MS-BASIC codes. Moreover MS-BASIC codes show degraded read speed gain of 1.49 and 1.44× for the small and large file, respectively. Our experimental results show slightly less gains, mainly due to disk I/O cost in the download C Recovery performance We further evaluate the recovery performance in the presence of data loss. We ta ke the recovery speed and network traffic for comparison standards between the two codes. We define the recovery speed as the total size of lost blocks divided by the total recovery time. For simplicity, we only delete one block and obs erve the recovery for the single failure As is shown in Fig. 5, the experimental data conforms to the analytical results presented in table I. Overall, MSBASIC codes show higher recovery speed than CRS codes The gain is nearly 1.5× in the average case for all k, n due to less XOR operations for reco vering data. The speeds of both decrease with the increasing parameter k, n ing to more blocks involved. We can also get that the big file has a lower recovery speed, just because it needs to process more data in big file As network bandwidth is usually the bottleneck of a large cluster, we now study the bandwidth saving of MSBASIC codes over conventional recovery. We use the traffic 42 


               k,n Recovery speed\(MB/s  CRS \(1.64GB  MS-BASIC \(1.64GB  CRS \(2.52GB  MS-BASIC \(2.52GB Figure 5 Recovery speed with a failure          k,n Network traffic \(MB  CRS  MS-BASIC Figure 6 Network traffic per block recovery monitoring tool, iftop, to measure the network traffic during recovery. The traffic is consumed by the receiving/sending data and some other control information. As we only repair one block, the amount of sending data is just the size of the new block \(64MB On the other hand, the amount of receiving data is 64 k MB and 32 k MB for CRS and MSBASIC codes, respectively As shown in Fig 6 MS-BASIC codes have a significantly lower network traffic than CRS codes for per block recovery and show a highest reduction of approximately 3.1× in \(8,16\. Besides, the reduction is 2.86× and 2.67× for \(6,12\ and \(4,8\ respectively. The traffic increases with th e increasing parameter k, n both implementations. As shown above, the amount of data for recovery is depending on k and the larger k leads to more data V C ONCLUSIONS AND F UTURE W ORK In this paper, we explore the use of MS-BASIC codes to provide fault-tolerant storag e and minimize the recovery bandwidth. Also, we employ MS-BASIC codes in Hadoop HDFS We theoretically show that MS-BASIC codes not only achieve high encoding/decoding efficiency, but also minimize the recovery bandwidth. The results of experiments confirm to the theoretical analysis and demonstrate the superior performance of MS-BASIC codes for construction and maintenance In the future, we will explore the properties of MSBASIC codes to achieve better performance in terms of data updates and disk I/Os Moreover, we will study and implement the minimum bandwidth BASIC codes, from both theoretical and applied perspectives A CKNOWLEDGMENT This work is supported by National Basic Research Program of China \(973 Program No.2012CB315904\nal Natural Science Foundation of China \(No.NSFC61179028 SZ JCYJ20130331144502026 and Guangdong Natural Science Foundation GDNSF No.S2011010000923 R EFERENCES 1 H a n xu Hou  K  W  Sh u m  M in ghua Ch en and Hui L i  B A S I C  Regenerating Code: Binary Addition and Shift for Exact Repair. In Proc. of the IEEE ISIT, 2013 2 H a n xu Hou K  W  S h u m   M i n ghu a Ch en  a nd Hui L i  C o n s t r u ct i o n of Exact-BASIC codes for Distributed Storage System at the MSR point. Submitted to IEEE Bigdata Workshop 2013 3 B  C hu n F  Da b e k  et a l  E f fic i ent R e pli c a Ma in t e n a n c e for Di s t ri bu t e d Storage Systems. In Proc. of USENIX NSDI, May 2006 4 S  G h em a w a t  H. G o b i off a nd S L e u n g  T h e G oog le F i le S y s t em   I n  Proc.  of ACM SOSP, Dec 2003 5 B C al de r  J  W a ng  e t  al W i n d o w s A z ur e S t o r ag e  A H i g h l y  A v ail a bl e Cloud Storage Service with Strong Consistency. In Proc. of ACM SOSP, Oct 2011 6 R e e d an d S o l o m o n P o ly no m ial co de s o v e r cer t ain f i n ite f i e l ds   Journal of the Society for Industrial and Applied Mathematics vol. 8 no. 2, pp. 300  304, 1960 7 H H o u, H  L i and K  W  S hum  G e ne r a l s e l f r e pair ing co de s f o r distributed storage systems.  In IEEE ICC, Budapest, Jun, 2013 8 O  K h a n   R  Bu rn s  J  Plan k  et  a l   R e t h i n k i n g era s u r e c o d e s for c l oud file systems: Minimizing I/O for recovery and degraded reads InFAST 2012 9 B l a um  an d Ro t h  O n l o w e s t de ns ity m d s co de s  I E E E T r ans a ct io n o n  Information Theory, vol. 45, no. 1, pp. 45  59, Jan 1999  C  Huan g H Si m i tc i Y Xu  et a l  E r a s u r e C o d i n g in W i nd ow s  Azure Storage. In Proc. of USENIX ATC, Jun 2012  M a h e s w a r a n  Sat h i a m o ort h y  M e ga s t h e ni s  A s t e ri s   et a l  XOR in g  Elephants: Novel Erasure Codes for Big Data. In proc. of the VLDB Endowment,  vol. 6, no. 5, Aug 2013  HDF S R A I D w i k i   h t tp  w i k i a pa ch e.org h a d oop HDF S R A I D    K  R a s h m i N. Sha h   a nd P K u m a r Opt i m a l E x ac t R e gen e ra t i n g  Codes for Distributed Storage at the MSR and MBR Points via a Product Matrix Construction. IEEE Trans. on Information Theory 57\(8\:5227  5239, Aug 2011  R unh ui L i  J i a n L i n  P a t r ic k P  C  L e e C O RE A u gm e n t i n g  Regenerating-Coding-Based Recovery for Single and Concurrent Failures in Distributed Storage Systems. In Proc. of the 29th IEEE Conference on Massive Data Storage \(MSST 2013\ong Beach, CA May 2013  K  Sh va c h k o  H. K u a n g  et a l  T h e Ha d oop D i s t ribu t e d F i le Sy s t em   In Proc. of the IEEE MSST, May 2010  L i u Xi a n gh o n g an d Shu J i w u   S u m m a r y of R e s e a r c h for E r a s u r e Code in Storage System. ISSN 1000-1239/CN, 49\(1\:1-11,2012  A  Di m a k i s P. G o d fre y   et a l  Net w ork C odi n g for Di s t r i bu t e d Storage Systems. IEEE Trans. On Information Theory, 56\(9\:4539  4551, Sep 2010  P L A N K  J  S   A NDXU L  Op t i m i zing C a u c h y R eed Solom on c o d e s  for fault-tolerant network storage applications. In NCA-06: 5th IEEE International Symposium on Network Computing Applications Cambridge, MA, July 2006 43 


with the increase of parallel clients. Every event index published in test is of the same size, and every event is relevant to 20 objects. In Mode 2, when DS accepts a publish request, it writes the information about this event index into several tables and then updates the OID index table. While in Mode 3, DS appends the event index to the rows of the concerned OIDs. Figure 10 shows the experiment results of parallel publishes. Tests under 200 and 400 parallel clients are only executed on Mode 3 for similar reason in Experiment 3. The maximum number of parallel publishes that Mode 2 can handle per second is less than 80 while the maximum number of parallel publishes that Mode 3 can handle is nearly 400  Figure 9  Comparison of parallel queries handled per second  Figure 10  Comparison of parallel publishes handled per second Experiment 1 to experiment 4 proved that both the ability to discover information from a big volume of DS records and the ability to handle concurrent requests of the proposed DS system are much better than DS based on RDBMS and its extra memory cost is acceptable V  C ONCLUSION  This paper works on improving the central-indexing DS which is faced with the challenge of massive data and large amount of parallel requests. The paper presents a new storage schema of DS data based on HBase. The new storage schema uses OID as row key, event time as column identifier, and event index as cell value to improve the discovery efficiency. A recursive discovery algorithm is specified to realize the full tracing of object which consists of both the events happened to the specific object and the events happened to its containers. The recursive discovery is included in the DS system as a service to reduce the client’s workload. A prototype of the proposed DS system is implemented. A series of comparison experiments are conducted to prove the performance advantage of the proposed DS system in both efficiency and concurrency A CKNOWLEDGMENT  The research activities as described in this paper were funded by national 863 high technology plan of China Grant No. 2011AA100701\ and the DNSLAB research project of China Internet Network Information Center R EFERENCES  1  EPCGlobal. The EPCGlobal Achitecture Framework Final Version 1.4, 2010 http://www.gs1.org/gsmp/kc/epcglobal/architecture/architecture_1_4framework-20101215.pdf 2  EPCGlobal. EPC Information Services \(EPCIS\rsion 1.0.1. 2007 http://www.gs1.org/gsmp/kc/epcglobal/ons/ons_1_0_1-standard20080529.pdf 3  EPCGlobal. EPCglobal Object Naming Service \(ONS\. 2008 http://www.gs1.org/gsmp/kc/epcglobal/ons/ons_1_0_1-standard20080529.pdf 4  Bridge. WP02 High level design of Discovery Services, 2007 5  Wen Zhao, XueYang Liu, Sen Ma, ChongYi Yuan, LiFu Wang. A Distributed RFID Discovery System: Achitecture, Component and Application. The 14th IEEE International Conference on Computational Science and Engineering, 2011 6  A Cheung, K Kailing, and S Schonauer, Theseos: A Query Engine for Traceability across Sovereign, Distributed RFID Databases. Proc. of the 2007 IEEE 23rd International Conference on Data Enginering Istanbul, Apr 2007. 1495-1496 7  Liu Dongdong Study on the information discovery service of Internet of Things based on P2P. Zhengzhou University, 2011 8  Kong Ning. Research on Key Technology of the Resource Addressing in the Internet of Things. Graduate University of Chinese Academy of Sciences, 2008 9  Zhao Wen, Li XinPeng, Liu Dianxing, Zhang Shikun, Wang Lifu. A Distributed RFID Discovery Service for Supply Chain. ACTA ELECTRONICA SINICA. Vol. 38, No. 2A, P99-106, Feb 2010   Zheng Linjiang, Liu Weining, Lu Yanliang. Link-style discovery service method based on extended ONS.Computer Engineering and Applications. 2010, 46\(6\204-207   Sergei Evdokimov, Benjamin Fabian, Steffen Kunz, Nina Schoenemann. Comparison of Discovery Service Architectures for the Internet of Things. 2010 IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing   HBase. http://hbase.apache.org   F. Chang, J. Dean, S. Ghemawat, w.e. Hsieh, D.A. Wallach, M Burrows, T. Chandra, A. Fikes and RE. Gruber. Bigtable: a distributed storage system for structured data. ACM Transactions on Computer Systems, vol 26, Jun. 2008, doi:10.1145/1365815.1365816   EPCGlobal. EPC Tag Data Standard Version 1.5, 2010 http://www.gs1.org/gsmp/kc/epcglobal/tds/tds_1_5-standard20100818.pdf  
759 


Figure 9  A map of the area with 002elds of view and localization points Figure 10  A frame and the image points from Camera 1 for the vessel without AIS By applying the same procedure to one of the previously estimated position for both passenger ships as shown in Figure 13 b and c we obtain a much more stretched estimated covariance Clearly localization accuracy in range is much smaller than in azimuth and it is a function of target distance from the sensors All the results are superimposed as green points in Figure 13 d observing that the crosses keep their shape near the coast and collapse with increasing distance 8 C ONCLUSIONS This paper has proposed an innovative calibration technique for cameras displaced on harbour coastline in which the calibration points are obtained from AIS data from ships sailing in the cameras 002elds of views Our experimental results have showed that the procedure localizes well vessels appearing in the two camera 002elds of view Future work will focus on the deployment of larger arrays of cameras A con\002guration with N cameras implies a longer calibration phase but any increase in view diversity would contribute to an increase in localization precision The procedure must be inherently adaptive also because a generic vessel could be seen only by a subset of the sensor array at any given time Therefore localization of a vessel requires a combination of solutions of different systems 13 for best use of available information Figure 11  A frame and the image points from Camera 2 for the vessel without AIS Figure 12  Pixel cross-variation for Camera 1 a and Camera 2 b to evaluate localization accuracy Figure 13  Error position covariance for vessel without AIS a Abundo b and Nomentana c 7 


Another important issue is the positioning of cameras in the harbour It is well-known that the localization accuracy is strongly dependent on the distance from the vessels range but an intelligent displacement of cameras with their 002elds of view properly intersected to cover almost the same portion of the sea plane could increase the strength of the whole process A CKNOWLEDGMENTS We would like to thank Prof Carlo Regazzoni from Universit 264 a di Genova for very stimulating discussions on data fusion and for letting us use his precious class notes R EFERENCES  R Tsai 223A versatile camera calibration technique for high-accuracy 3d machine vision metrology using offthe-shelf tv cameras and lenses,\224 IEEE Journal of Robotics and Automation vol 3 no 4 pp 323-344  August 1987  Z Zhang 223A 003exible new technique for camera calibration,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence vol 22 no 11 pp 1330-1334  November 2000  Technical characteristics for an automatic identi\002cation system using time-division multiple access in the VHF maritime mobile band  Recommendation ITU-R M.1371-4 April 2011  IALA Guideline No 1082 On an Overview of AIS  International Association of Marine Aids to Navigation and Lighthouse June 2011  International Convention for the Safety of Life at Sea SOLAS  IMO May 2011  R Hartley and A Zisserman Multiple View Geometry in Computer Vision 2ed  Cambridge 2003  E Trucco and A Verri Introductory Techniques for 3-D Computer Vision  Prentice Hall 1998  G Medioni and S B Kang Emerging Topics in Computer Vision  Prentice Hall 2004  223Marine traf\002c,\224 http://www.marinetraf\002c.com/ais 2012 B IOGRAPHY  Francesco A.N Palmieri received his Laurea in Ingegneria Elettronica from Universita degli Studi di Napoli Federico II Italy in 1980 In 1981 he served as a 2nd Lieutenant in the Italian Army in full\002llment of draft duties In 1982 and 1983 he was with the ITT 002rms FACE SUD Selettronica in Salerno currently Alcatel Italy and Bell Telephone Manufacturing Company in Antwerpen Belgium as a designer of digital telephone systems In 1983 he was awarded a Fulbright scholarship to conduct graduate studies at the University of Delaware USA where he received a Master's degree in Applied Sciences and a PhD in Electrical Engineering in 1985 and 1987 respectively He was appointed Assistant professor in Electrical and Systems Engineering at the University of Connecticut Storrs USA in 1987 where he was awarded tenure and promotion to Associate Professor in 1993 In the same year he was awarded the position of Professore Associato at the Dipartimento di Ingegneria Elettronica e delle Telecomunicazioni at Universit 264 a degli Studi di Napoli Federico II Italy where he has been until October 2000 when he became Professore Ordinario di Telecomunicazioni and moved to the Dipartimento di Ingegneria dellInformazione  Seconda Universit 264 a di Napoli Aversa Italy His research interests are in the areas of signal processing data fusion communications information theory and neural networks Francesco Castaldo received his Laurea in 2009 and his Laurea Magistrale in 2012 both in Ingegneria Informatica from Seconda Universita degli Studi di Napoli SUN He has cooperated during his thesis work with the PRISMA lab at Universit 264 a degli Studi di Napoli Federico II with prof B Siciliano and prof V Lippiello He is currently a postgraduate fellow with the Dipartimento di Ingegneria Industriale e dell'Informazione at SUN His research interests are in image processing computer vision and data fusion applied to robotics and home automation Guglielmo Marino received his Laurea in Ingegneria Elettronica at Seconda Univerit 264 a degli Studi di Napoli SUN in December 2012 His 002nal thesis project has been on data fusion of digital images with AIS data in harbor management 8 


boards of several journals including IEEE Transactions on Service Computing and the Journal of Performance Evaluation   Zhen has given keynotes and distinguished lectures in various conferences and universities. He was an adjunct professor at University of Science and Technology of China and Beijing University of Post and Telecommunications While he was in France Zhen was  also an adjunct professor of the University of Paris VI \(University of Pierre & Marie Curie\and the University of Nice  Sophia Antipolis, France   His areas of expertise include mobile computing, mobile services, cloud computing, stream processing re al time analytics performance modeling stochastic optimization service oriented architecture and semantic Web      
lxxxi 


en-US Keynote VI I I  en-US GreenCom iThings CPSCom 2013   Towards Carrier Cloud   Dr. Tarik Taleb  Senior Researcher and 3GPP Standards Expert  NEC Europe Ltd, Heidelberg, Germany  Email tarik.taleb@nw.neclab.eu    Abstract   Mobile operators are in need of means to cope with the ever increasing mobile data traffic, introducing minimal additional capital expenditures on existing infrastructures, principally due to the modest Average Revenues per User ARPU Network virtualizat ion and cloud computing techniques along with the principles of the latter in terms of service elasticity on demand and pay per use could be important enablers for various mobile network enhancements and cost reduction This talk discusses the recent tr ends the mobile telecommunications market is experiencing showcasing some of the emerging consumer products and services that are facilitating such trends. The talk also discusses the challenges these trends are representing to mobile network operators. T he talk also demonstrates the possibility of extending cloud computing beyond data centers towards the mobile end user providing end to end mobile connectivity as a cloud service. The talk introduces a set of technologies and methods for the on demand pro vision of a decentralized and elastic mobile network as a cloud service over a distributed network of cloud computing data centers; federated cloud. The concept of Follow Me Cloud whereby not only data but also mobile services are intelligently following t heir respective users is also introduced. The novel business opportunities behind the envisioned carrier cloud architecture and service are also discussed, considering various multi stakeholder scenarios   Bio   Tarik Taleb is currently working as Senior Researcher and 3GPP Standards Expert at NEC Europe Ltd Heidelberg, Germany. Prior to his current position and till Mar. 2009, he worked as assistant professor at the Graduate School of Information Sciences, Tohoku University, Japan, in a lab fully funded by KDDI, the second largest network operator in Japan From Oct 2005 till Mar 2006 he was working as research fellow with the Intelligent Cosmos Research Institute Se ndai Japan He received his B E   degree in Information Engineering with distinction M.Sc and Ph.D degrees in Information Sciences from GSIS Tohoku Univ., in 2001, 2003, and 2005, respectively   Dr Taleb  s research interests lie in the field of architectural enhancements to mobile core networks particularly 3GPP  s mobile cloud net working mobile multimedia streaming congestion control protocols handoff and mobility management inter vehicular communications and social media networking Dr Taleb has been also directly engaged in the development and standardization of the Evolved  Packet System as a member of 3GPP  s System Architecture working group. Dr. Taleb is a board member of the  IEEE Communications Society Standardization Program Development Board  As an attempt to bridge the gap between academia and industry Dr Taleb has f ounded and has been the     Dr Taleb  is/was on the editorial board of the IEEE Wireless Communications Magazine IEEE Transactions on Vehicular Technology, IEEE Communications Surveys & Tutorials, and a number of Wiley journals. He is serving as vice chair of the Wireless Communications Tech nical Committee, the largest in IEEE ComSoC He also served as Secretary and then as Vice Chair of the Satellite and Space Communications Technical Committee of IEEE ComSoc 2006  2010 He has been on the technical   
lxxxii 


program committee  of different IEEE c onferences including Globecom, ICC and WCNC and chaired some of their symposia   Dr Taleb is the recipient of the 2009 IEEE ComSoc Asia Pacific Best Young Researcher award Jun 2009 the 2008 TELECOM System Technology Award from the Telecommunicati ons Advancement Foundation Mar 2008 the 2007 Funai Foundation Science Promotion Award Apr 2007 the 2006 IEEE Computer Society Japan Chapter Young Author Award Dec 2006 the NiwaYasujirou Memorial Award Feb 2005 and the Young Researcher's Enc ouragement Award from the Japan chapter of the IEEE Vehicular Technology Society \(VTS\\(Oct. 2003\ Some of Dr. Taleb  s research work has been also awarded best paper awards at prestigious conferences. Dr. Taleb is a senior IEEE member      
lxxxiii 


en-US Keynote I X  en-US GreenCom iThings CPSCom 2013   How Densely Should the Data Base Stations  B e Deployed in Hyper Cellular Networks   Professor Zhisheng Niu  Tsinghua National Lab for Information Science and Technology  Tsinghua University, Beijing 100084, China  E mail niuzhs@tsinghua.edu.cn    Abstract   One of the key approaches to make the mobile communication networks more GREEN Globally Resource optimized and Energy Efficient Networks\is to have the cellular architecture and radio resource allocation more adaptive to the environment and traffic varia tions including making some lightly loaded base stations \(BSs\go to sleep. This is the concept of so called TANGO \(Traffic Aware Network planning and Green Operation and CHORUS Collaborative and Harmonized Open Radio Ubiquitous Systems published by th e author earlier. To realize this, a new cellular framework, named hyper cellular networks HCN has been proposed in which the coverage of control signals is decoupled from the coverage of data signals so that the data coverage can be more elastic in ac cordance with the dynamics of traffic characteristics and QoS requirements. Specifically, the data base stations \(DBSs\in HCN can be densely deployed during peak traffic time in order to satisfy the capacity requirement, while a portion of DBSs can be swi tched off or go to sleep mode if the traffic load is lower than a threshold in order to save energy. A fundamental question then arises how densely should the DBSs be deployed in order to balance the QoS requirements and the energy consumption in hyper ce llular networks     In this talk, we characterize the optimal DBS density for both homogeneous and heterogeneous hyper cellular networks to minimize network cost with stochastic geometry theory For homogeneous cases both upper and lower bounds of the optimal DBS density are derived For heterogeneous cases our analysis reveals the best type of DBSs to be deployed for capacity extension or to be switched off for energy saving. Specifically, if the ratio between the micro DBS cost and the macro DBS cost  is lower than a threshold which is a function of path loss and their transmit power then the optimal strategy is to deploy micro DBSs for capacity extension or to switch off macro DBSs \(if possible\for energy saving with higher priority Otherwise the  optimal strategy is the opposite Based on the parameters from EARTH numerical results show that in the dense urban scenario compared to the traditional macro only homogeneous cellular network with no DBS sleeping deploying micro DBSs can reduce about 40 of the total energy cost, and further reduce about 20% with DBS sleeping capability   Bio   Zhisheng Niu graduated from Northern Jiaotong University currently Beijing Jiaotong University Beijing China in 1985 and got his M.E and D.E degrees fr om Toyohashi University of Technology Toyohashi, Japan, in 1989 and 1992, respectively. After spending two years at Fujitsu Laboratories Ltd Kawasaki, Japan, he joined with Tsinghua University, Beijing, China, in 1994, where he is now a professor at the  Department of Electronic Engineering and the deputy dean of the School of Information Science and Technology. His major research interests include queueing theory, traffic engineering, mobile Internet radio resource management of wireless networks, and g reen communication and networks   Dr Niu has been an active volunteer for various academic societies including council member of Chinese Institute of Electronics 2006 10 vice chair of the Information and Communication Network Committee of Chinese In stitute of Communications 2008 12 Councilor of IEICE Japan 2009 11 and membership development coordinator of IEEE Region 10 \(2009 10\ In particular, in IEEE Communication 
lxxxiv 


Society, he has been serving as an editor of IEEE Wireless Communication Magaz ine \(2009 12\ director of Asia Pacific Region \(2008 09\ director for Conference Publications \(2010 11\ chair of Beijing Chapter 2001 08 and members of Award Committee 2011 13 Emerging Technologies Committee 2010 12 On line Content Committee 20 10 12 and Strategy Planning Committee He has also been serving as general co   co    chairs o f    Prof. Niu is a co recipient of the Best Paper Awards from the 13th and 15th Asia Pacific Conference on Communication APCC in 2007 and 2009 respectively and received Outstanding Young Researcher Award from Natural Science Foundati on of China in 2009 He is now the Chief Scientist of the National  Energy and Resource Optimized Hyper Cellular Mobile Communication System 2012 2016 which is the first national project green communications in China He is the fellow of IEEE and IEICE and a distinguished lecturer of IEEE Communication Society \(2012 2013  
lxxxv 


