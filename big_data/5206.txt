A Concept of Generic Workspace for Big Data Processing in Humanities Jedrzej Rybicki Benedikt von St Vieth Daniel Mallmann Forschungszentrum Juelich GmbH JSC Juelich Germany Email j.rybicki b.von.st.vieth d.mallmann}@fz-juelich.de Abstract 227Big Data challenges often require application of new data processing paradigms like MapReduce and corresponding software solutions e g Hadoop This trend causes a pressure on both cyber-infrastructure providers to quickly integrate new services and infrastructure users to quickly learn to use new tools In this paper we present the concept of DARIAH Generic Workspace for Big Data Processing in 
eHumanities which alleviates the aforementioned problems It establishes a common integration layer thus enables a quick integration of new services and by providing uni\002ed interfaces allows the users to start using new tools without learning their internal details We describe the overall architecture and implementation details of the working prototype The presented concept is generic enough to be applied in other emerging cyber-infrastructures for humanities Keywords cyber-infrastructures for humanities architecture Big Data I I NTRODUCTION The European Commission supports emerging research 
infrastructures for a wide range of disciplines by funding the European Strategy Forum on Research Infrastructures ESFRI This body supports among others the digital humanities project DARIAH Digital Research Infrastructures for the Arts and Humanities D ARIAH embraces a number of national initiatives also a Germany-based effort DARIAH-DE where the described work was conducted It is safe to say that all those bodies support a vision of an infrastructure helping researchers to cope with the challenges that the digital humanities bring One of the broadly acknowledged challenges is the one of Big Data i e data that is so large and complex that it exceeds the limits of the commonly used hardware and software 
systems Big Data is sometimes characterized by its volume velocity and variety Clearly in case of humanities in DARIAH the variety of the data is given as it stems from various disciplines linguistics musicology history and many others By combining the data sets from this different disciplines under the umbrella of a common cyberinfrastructure we observe a constant increase of the data volume connected 223small data pieces\224 become Big Data To this end tools supporting ef\002cient handling of large data sets must be established DARIAH has built a data Grid solution to provide the researchers with a place to store their data in a safe manner It was primarily designed for bit preservation and the current 
solution is based on iRODS with an HTTP-based inter face towards the end-user Data storing and preserv ation is an important service but not suf\002cient for research work in times of Big Data Imagine a researcher who uploads a collection of documents to the storage service to assure their safety and wants to perform a simple quantitative analysis on the data set e g calculate word frequency or search for documents containing given terms iRODS does not provide an ef\002cient way of processing the data In particular there is no easy way to execute parallel processing tasks which become more and more important in the digital humanities for instance for text processing Ev en such a 
simple quantitative analysis requires beside some advanced programming skills the sequential processing of each single object from a collection resulting in a higher server load and long response times On the other hand there are emerging paradigms for ef\002cient parallel processing of unstructured data e g MapReduce implemented in the Hadoop framework b ut the y are not s o strong in terms of data preservation In this paper we describe the concept of a Generic Workspace for Big Data processing which can be understood as an active storage that not only stores the data but also provides a means to ef\002ciently process them A high-level goal of DARIAH is to provide generic easy 
accessible services This holds for both storage and data processing Thus our motivation is twofold the users don't want to learn new tools and change their work\003ows but rather prefer to stick to well-know principles and abstractions The service providers on the other hand seek the ways of reducing the complexity and heterogeneity of their infrastructure It is not feasible from both user's and service provider's perspective to add new independent stand-alone components in the common infrastructure each time a new functionality is required Having separate components the users wishing to process the data would have to download them 002rst from the common storage and then upload to a 
processing component like Hadoop Such manual operations are prohibitively complicated time consuming errorprone and inherently not scalable The Generic Workspace hides the details of data movement by establishing a common service layer for both storing and processing of the data and a uniform interface to interact with underlying resources  978-1-4799-1293-3/13/$31.00 \2512013  IEEE 


To help the users to get to grips with the new services we decided to hide the underlying components from them and expose only their functionality through a well-know abstraction of a distributed 002le system The Workspace offers the view of a hierarchical 002le system which contains two classes of 002les Some of the 002les are ordinary 002les storing content others we call special 002les model computation resources Their content are results of processing The users wishing to start processing have to simply create a special 002le To get the results of such processing they retrieve the content of the 002le The corner stone of this approach is to use meaningful declarative 002le names For instance the creation of a special 002le named wordCount should trigger the computation of a word count of all documents in a directory where the 002le was created and the results should be written back into the 002le In this declarative approach users only de\002ne the result they are expecting to get from the system like number of words and not how the result should be derived The mixture of the well-know abstraction of a 002le system with the declarative meaningful 002le names allows the end-user to quickly integrate the new functionality into his work\003ow without even knowing how the actual processing is done and without learning the details of the new tool DARIAH infrastructure must serve a very heterogeneous user base Not only in terms of disciplines they represent but also in terms of how they interact with the infrastructure Some of them already use some kind of Virtual Research Environments VRE b ut others do not Thus in our design we aimed at bringing the processing close to the storage rather than to rely on a high-level VRE-like solution By following this approach we can offer generic processing to both groups of users accessing the generic infrastructure via an VRE or directly To some extend this is an implementation of a more general principle of uniting the humanand computer-readable services in one common interface The rest of this paper is structured as follows In Section II we present the high-level view of the proposed solution and de\002ne the main goals and functionalities for the system In the next Section we describe the concrete implementation of the Workspace For the prototype two well-know products for storing and processing iRODS and Hadoop were selected We discuss their strengths and weaknesses and explain how they can be combined to constitute the basis for a generic Workspace for Big Data Processing in Humanities Section IV summarizes the results and put them in a broader perspective by referring to existing related work We conclude our paper with a summary in Section V II A RCHITECTURE As already explained the two main driving forces of our architecture were the urgent need to reduce the integration effort of adding new services into an existing cyber-infrastructure and the user-friendliness allowing for instant use of the new functionalities In this section we explain the main architectural choices we made and how they correspond to the driving forces The user-friendliness is addressed mainly by a declarative approach and ReST-like architectural style In short the declarative approach boils down to performing the actions by describing their results rather than by describing how they are to be implemented and conducted as in case of an imperative approach The well-known examples of declarative approaches are HTML describing how the website should looks like and not how the rendering shall be done or SQL to create and manipulate the relations without describing how they are stored and processed in the RDBMS The natural remaining question is where and how the declarative de\002nitions should be put The answer is provided by the next architectural decision to exploit a well-known abstraction of a 002le system The idea bears some similarities with the ReST architectural style In short ReST proposes to use set of HTTP operations like GET POST PUT with a well-de\002ned semantics to manipulate resources identi\002ed by the URLs and interconnected by the hypermedia In our case we use resources modeled as 002les and directories in a 002le system and users can manipulate those naturally by issuing commands like open close read  The abstraction together with a set of methods de\002ne a uniform interface  The de\002nitions of the expected computation results should also be written in the common 002le system by using methods listed above The rationale of using 002le system abstraction was that there was already an iRODS based distributed storage in DARIAH and it offered a 002le-system-like interface The architecture of our system is depicted on Figure 1 The users on the left hand side of the picture face a common DARIAH namespace and have tools to access and manipulate objects in it Objects are identi\002ed by logical names The integration layer provides the namespace and thus hides the actual physical resources on which the objects reside It also distinguishes between the different classes of objects in the namespace ordinary and special 002les In case of an access to an ordinary 002le the logical name is translated to a physical path on the respective storage resource and depending on the command issued by the user either the content of a 002le is served back or modi\002ed Some of the logical names however are mapped not on ordinary storage resources but on processing clusters Access requests or modi\002cations on such objects are intercepted by the integration layer and passed to the processing services like Hadoop The contract between the integration layer and the processing services states that the objects should have meaningful names following the declarative approach The names are passed over to the processing services and interpreted by them The name can also include some parameters required by the service to conduct the particular computation The parameters are placed in brackets at the end of the 002le name 64 


Figure 1 Architecture overview similarly to the convention known from some programing languages The results are put into the special 002les and can be read from them To make the differentiation between normal 002les and computation objects easier for both users and the developers of the integration layer it was agreed that the special 002les should follow a simple naming convention e g they should contain a common segment in their logical name  proc   Examples of the meaningful declarative names within the convention are as following 017 home/user/proc/wordCount 017 path/proc/contains\(foo 017 coll1/proc/writtenBy\(Goethe,Rilke A nice side-effect of making the results of processing available as objects with logical names in a common namespace is that it is easy to re-use and share such results Existence of a special 002le proc/wordCount in a common collection indicates that the word count has been already calculated for this collection and can be reused With the declarative approach we aim at providing initial set of usable functions for data processing but it should also be possible for advanced users to de\002ne and share their processing functions In other words it should be possible to add new classes of special 002les Also the integration of the new services is made much easier by establishing a common integration layer Typically in a distributed environment the integration of a new service means that it has to be 223connected to\224 other existing services Only after the integration steps are completed it is possible for the user to seamlessly use the infrastructure To make such connections easier developers usually de\002ne Application Programming Interfaces API but still some glue code must be added to the new service to handle the APIs of the remote services The existing infrastructure components must be extended as well to become aware of the newly added service and this usually requires a connection to the new API Therefore the integration effort in an infrastructure comprised of n components can be approximated as O  n 2  connecting all pairs of components Such an additional effort might lead to a situation in which new services are reluctantly added to avoid the expensive changes and the whole infrastructure becomes in\003exible or even obsolete The Workspace helps in keeping the infrastructure 003exible and ready for the new challenges like Big Data This goal is achieved by the substantial reduction in the integration effort In the Workspace model the new service needs only be integrated into the integration layer In other words it has to become responsible for a part of the common namespace and can be unaware of the existence of other services There is also no need for the new service to offer any user interface as it is integrated into common namespace and the end-users already have a means to manipulate its content The communication between end-users and new services is conducted via common namespace the same hold for the communication between the services Thus the namespace can be also viewed as a substitute of a common API III I MPLEMENTATION Previous section presented the rationale behind the architecture of the Workspace and justi\002ed its most important design principles In this section we will present the experiences gathered while implementing a working prototype of the system It was decided early on that DARIAH will use iRODS for bit preservation As a processing candidate for our prototype we decided to use Hadoop due to its popularity proliferation and broad acceptance in the world of Big Data The experience gathered from building a prototype shows that the architecture is generally applicable and can be used to integrate other products as well We will 002rst present the main features of iRODS and discuss its inapplicability for Big Data processing Subsequently we elaborate on a processing paradigm which is quite well-established in the Big Data processing environment MapReduce We will focus on an existing open 65 


source framework implementing the paradigm Hadoop Due to space limitations we only present main features of both products for more details the readers are referred to 7 Finally we will show how both example technologies were integrated to provide a working prototype of a DARIAH Workspace A About iRODS iRODS is a data management middleware developed by Data Intensive Environments DICE group iRODS provides data management functions required in a distributed environment such as a 002le transfer service data replication and metadata management Files in iR ODS are organized into hierarchical collections in a way similar to 002le systems offering a logical namespace for 002les stored on different distributed storage resources Currently many different types of storage resources are supported UNIX and Windows 002le systems but also secondary storage solutions like HPSS It is also possible to integrate new types of storages by writing drivers The typical deployments of iRODS comprise of one or multiple servers maintaining storage resources and exactly one metadata server called iCAT  The later component is responsible for logical-tophysical mapping for stored data objects and user access management To improve fault tolerance iRODS can be con\002gured to replicate the data objects across multiple storage resources A unique feature of iRODS is its ability to use userde\002ned rules for implementing data management policies They follow event-condition-action model iRODS de\002nes a list of events like 002le ingestion The data administrator can use those to de\002ne actions like 002le format veri\002cation executed under certain conditions e g ingested 002le has given extension Each event triggers iRODS rule engine to execute corresponding rules In this model it is possible to implement even complex data management policies It is worth mentioning here that the term policy is a broad one and it is usually understood as a 223statement of intent\224 User-de\002ned rules de\002ne ways of achieving the intended effects Syntactically the rules are composed of calls to microservices and other rules Microservice is de\002ned as the smallest unit of work in iRODS that embodies single autonomous operation iRODS software package brings a rich set of microservices that can be reused to compose own rules It is also possible to implement own microservices but this require C programming skills and recompilation of the server to make the new functionalities available An example of simple iRODS rule that makes a copy of a given directory  zone/monitored  to pre-de\002ned target location  zone2/safe-copy  each time a new 002le is created and respective iRODS action is triggered  acPostProcForPut  is shown on Listing 1 a c P o s t P r o c F o r P u t  ON  o b j P a t h l i k e   z o n e  m o n i t o r e d  003    m s i S p l i t P a t h   o b j P a t h  003 c o l l e c t i o n  003 f i l e N a m e   m s i C o l l R s y n c  003 c o l l e c t i o n    z o n e 2  s a f e 000 copy       IRODS_TO_IRODS   003 s t a t u s   m si W ri t e Ro d sL o g   Backup o f 003 c o l l e c t i o n done  s t a t u s  003 s t a t u s       Listing 1 Example policy Beside the 223global rules\224 triggered by the system-wide events there are also user-de\002ned rules Such rules are executed manually via command-line tools and not each time a given combination of action-condition occurs The user-de\002ned rules use the same syntax as the global rules and can be used to implement data processing work\003ows Two obvious and severe limitations of user-de\002ned rules when it comes to analyzing larger amounts of data are following First of all iRODS does not provide support for parallel execution of rules and microservices Thus to analyze the content of a 002le a microservice has to read it sequentially and the performance is upper bounded by the disk throughput Secondly iRODS does not provide a convenient way of creating the microservices The source code must be upload to the server and the server must be recompiled This limitations renders rules almost unusable for researchers to do processing B About Hadoop Hadoop is an open-source framework that facilitates scalable distributed processing of large data sets It leverages clusters of computers to store and process the data in a robust fashion The robustness is not achieved by the highend hardware but rather by the redundancy and effective handling of failures Two main component of Hadoop are distributed 002le system typically HDFS and a framework for job scheduling and resource management Large data sets stored in a distributed fashion on HDFS can be ef\002ciently analyzed using simple programming models Most prominent among them is the MapReduce To understand the strengths and weaknesses of Hadoop it is worth discussing the computation with MapReduce in a more detailed fashion The 002rst prerequisite for ef\002cient processing is the availability of the data in the HDFS HDFS stores 002les as a sequence of blocks of the same size except the last one The blocks are distributed across the resource servers  storage nodes  within a cluster To improve the fault tolerance blocks are replicated The replication factor for each 002le can be de\002ned either upon the 002le ingest or globally The distribution of the 002le segments across the storage nodes in cluster enables quicker access to 002les content The operation is not bounded by the throughput of a single resource since the reading can be done simultaneously on different storage nodes To process the data stored in the HDFS MapReduce jobs can be used In the 002rst step map function transforms job input into key-value pairs Map divides the input data into 66 


smaller portions of work and act on them Afterwards the framework sorts the intermediate results by the key and passes them to the reduce function which produces the 002nal output again in form of key-value pairs Reduce combines the many results from the map step into a single output hence the name Since the input data and usually the intermediate results are handled as 002les and stored in a distributed fashion in the HDFS it is possible to split map and reduce functions into smaller tasks and execute them in parallel on storage nodes where the data resides The data locality improves the performance and reduces the network load by avoiding data movement Word counting could be implemented in MapReduce as following In the map step the input text would be tokenized and pairs of word and value of 1 would be emitted The framework would sort the map results by the key i e by the word and pass all the key-value pairs in form of  word 1  to the reduce step It is guaranteed that all the pairs with the same key value would be processed by the same reducer task For different keys words however different reducing tasks can be used Reducing function can simply iterate through the input map and sum the 1 s emitting current word and the sum Finally the outputs of single reduce tasks will be merged and made available to the user The Hadoop framework monitors storage nodes and processing tasks and takes care of the failures seamlessly e g by re-spawning failed tasks so that the end-user can use service despite the failures Also the data integrity is periodically checked and the detected errors with help of checksums are corrected by overwriting from replicas Hadoop and HDFS however do not provide convenient tools for de\002ning data management policies and thus are less suited for long time archiving of the data C About Pig Since writing native MapReduce jobs require programming in Java against Hadoop API there is a number of projects aiming at lowering this hurdle One example is Apache Pig 12  13 pro viding a high-le v el declarati v e language for expressing data analysis programs Pig offers an interactive shell and a possibility to execute pre-de\002ned scripts Programs written in the high-level Pig language are translated into MapReduce jobs and submitted to the cluster Example of a simple Pig script to calculate word count is presented on Listing 2 The readers can notice the similarity between Pig and SQL d a t a  LOAD  p a t h  f i l e  t x t  USING T e x t L o a d e r    t o k e n  FOREACH d a t a GENERATE FLATTEN  TOKENIZE  0   AS word  words  FILTER t o k e n BY word MATCHES    w   g r  GROUP words BY word  c  FOREACH g r GENERATE COUNT words  AS c n t  g r  r e s  ORDER c BY c n t  STORE r e s INTO  p a t h  o u t p u t  d a t   Listing 2 Example of a Pig script D Integration Concrete implementation of the Workspace is depicted on Figure 2 In our experiment we used iRODS in version 3.2 Hadoop in 1.0.4 and Pig in 0.10 The prototype uses iRODS to establish common integration layer exposing 002le system like interface to the user and abstracting both storage and processing resources The 002rst step of adding ef\002cient processing functionality into iRODS was to integrate the HDFS as a storage resource The rationale behind this step was simple ef\002cient MapReduce processing is only possible when the data are distributed As already explained iRODS supports different types of physical storages unfortunately there is no support for HDFS Thus it was necessary to write a new storage driver We decided to implement the driver by using Universal Mass Storage System driver interface offered by iRODS This interface is usually used to access tape systems and other storage technologies with sequential access to 002les Since iRODS offers random access to 002les the resources offering sequential access can only be used as compound resources together with so called disk cache Before any operation on the content of a 002le is performed it is staged to the cache method stageToCache  after the operation the 002le is moved back to archive  syncToArch  to save storage space Such a movement can be handled transparently by iRODS with help of rules using microservice msiSysReplDataObj  The actual driver implementation was quite simple We only had to write a limited set of methods with self-explanatory names like syncToArch stageToCache mkdir chmod rm mv stat  Due to the space limitation we only present how syncToArch method that moves the data from cache to HDFS can be implemented remaining methods are trivial as well We decided to use the standard HDFS client tool provided by Hadoop It could be also possible to use C library and write a native iRODS HDFS driver syncToArch    echo  syncToArch 1 2    tmp  u n i v D r i v e r  l o g hadoop f s 000 c o p yF r o m L o c al 1 2 r e t u r n  Listing 3 Implementation of syncToArch method After we made the data available on the distributed storage the next step was to enable processing Let us 002rst discuss a simple use case and then extend it further As already explained the processing in the Workspace should be triggered by creation of a special 002le For instance the creation of a 002le called my/collection/proc/wordCount would trigger the process of counting words across all documents in my/collection  We already have shown iRODS's feature that detects a creation of 002les Similarly to the policy from Listing 1 we use the pre-de\002ned iRODS 67 


Figure 2 Concrete implementation action acPostProcForPut and de\002ne additional condition a c P o s t P r o c F o r P u t  ON  o b j P a t h l i k e  003  p r o c  003         Listing 4 Rule to trigger processing details omitted Listing 2 shows a Pig script that can be used for the word calculation The script however has a prede\002ned input and output  path/file.txt and path/output.dat respectively This had to be changed Pig scripts can take external parameters they are marked by  in the script We agreed on a convention to call the input of a Pig script input and output output  The input of the script can be extracted from the name of the special 002le the output should be directed to a temporary directory 002rst and then moved to overwrite the special 002le after the computation is completed Now both parts have to be put together and the creation of the special 002le should trigger the execution of the Pig script iRODS provides a microservice that can be convenient here msiExecCmd can be used to call batch scripts stored on the iRODS server It was quite simple to write a short batch script called executing script  that takes the logical and physical name of the special 002le as parameters start a Pig processing script and move its results back to overwrite the special 002le Surprisingly the most challenging task was the last one iRODS stores the 002le size in the iCAT database Since the process of overwriting changes the size of the special 002le the iCAT content has to be updated This was done by issuing direct SQL query to the iCAT database Listing 5 shows how the executing script runs the processing script  u s r  b i n  p i g 000 p i n p u t    i n p u t  000 p o u t p u t    o u t p u t  000 l  v a r  p i g  l o g PIG_SCRIPTS   s c r i p t N a m e  p i g Listing 5 Fragment of the executing script Let us now extend the simple example presented above to show more advanced features of the Workspace The Pig script for word counting produces output in form presented on Listing 6 where the 002rst column contains the number of occurrences of the given word from the second column 1775 t h e 1040 o f 730 i n 677 and 457 t o 343 was 334 a 331 und 248 d i e 223 he    Listing 6 Processing results For the sake of discussion let us assume that the researcher is only interested which words occurred more often than 200 times and less than 300 times It is a simple 002ltering job that should analyze the 002rst column of the 002le First challenge to tackle is the need to provide more parameters to a Pig script than just input and output In Section II we mentioned that the parameters should be part of the logical name of the special 002le This is certainly true for the input and output parameters that can be derived from the full path of the 002le But it is also possible to include more parameters in the full name Particularly for the 002ltering job it would be reasonable to encapsulate the 002ltering criteria in the 002le name like this path/proc/filter\(coll1>200 AND coll1<300  Within this convention it is quite simple to write a Pig script for the required analysis that allow for 003exibility in terms of 002ltering e g change of the 002ltering boundaries The script is presented on Listing 7 A  LOAD   i n p u t  USING P i g S t o r a g e    t   AS  c o l l 1  c o l l 2   B  FILTER A BY   p a r a m s   STORE B INTO   o u t p u t   Listing 7 Pig script for 002ltering 68 


The executing script that is used for starting the job must be extend as well to pass not only the input and output parameters but also the 002ltering conditions to Pig It must also determine which script to run either wordCount or filter  This is done based on the name of the special 002le We sought after an extensible solution for the Workspace and it is to expect that sooner or later the researchers needs would not be satis\002ed by the two scripts presented so far and they will look for ways of de\002ning own processing scripts This functionality is easily enabled by storing the Pig scripts in ordinary iRODS collections The more advanced users could then upload their Pig scripts to a iRODS directory The scripts must only follow the convention of 003exible input and output paths and parameter passing if required The whole processing work\003ow does not change much First a creation of a special 002le with name in form collection/proc/function\(parameters is detected by iRODS Subsequently this logical name is passed to the executing script that extracts the function name which is mapped on the Pig script name and job parameters The appropriate Pig script is fetched from the iRODS collection and executed with extracted parameters In the last step the result is written back to the special 002le The 002nal solution is extensible and future-proof it allows to easy extend the set of processing function run them on new data by all the users of Workspace and share the results of the computation IV R ELATED W ORK In the previous sections we presented the idea architecture and a prototype of the Generic Workspace for Big Data Processing in Humanities The Workspace can be seen as an active storage i e a storage that not only stores the data but also provides a means for ef\002cient processing of the data Let us compare the presented approach to some previous work in the 002eld Our concept is clearly related to the idea of Virtual Research Environments An example of VRE is TextGrid although it is much more on the organizational level than just a tool TextGrid comprises two main components a web front-end and a repository TextGridRepo that abstracts underlying storage resources and services Usually the VREs provide high-level complex services like XML editing or text comparison tools The Workspace does not 002t into this category It can be seen as a service of underlying middelware that is exposed as a service to the user to enable ef\002cient processing before the ingest of the data for preservation in the TextGridRepo or further processing with high-level tools There are also plans for migrating the TextGrid repository infrastructure to Fedora Commons and iRODS that would allow to even reuse some parts of our prototype The digital humanities workbench pro vides a wide collection of useful text processing tools as on-line services Making the existing tools available without the need of installing them locally is a central concept of all kinds of VREs The authors of presented a general-purpose Virtual Research Environments that facilitates the integration of information resources and tools for supporting research activities We believe that our Workspace could easily be integrated into VREs as a tool that provides ef\002cient processing of large amounts of data Most of the VREs foresee the possibility to extend the installations by adding new storage resources Since the Workspace provides a 002le system interface it can be integrated into a VRE There are already examples of the application of MapReduce in digital humanities The moti v ation of the authors were the performance problems of the tool called Text Analysis Portal for Research TAPoR which is a web-based application that provides a suite of text-analysis tools to scholar and researcher in the digital humanities The authors identi\002ed that the problem was caused by the Ruby-based backend services that performed the actual processing They decided to migrate the services to Hadoop and achieved substantial performance improvements This work shows that the Hadoop-based services can be successfully used to solve problems in digital humanities The main architectural difference between this work and our solution is the application of the declarative approach in the Workspaces We have shown that our approach has serious implications with regard to the user-friendliness and the costs of integration of new services Despite its high popularity and broad application it becomes clear that MapReduce is not always the right tool and has its limitations 19 Our w ork w as moti v ated by the experience of cyber-infrastructure providers in a digital humanities project The rationale behind the Workspace is to enable quick integration of new services into an emerging infrastructure We used Hadoop MapReduce and Pig to build a working prototype but the same approach can be used to integrated alternative processing solutions Especially while we use a declarative approach the user is not even aware how the actual computation is done Thus changes or additions of new services are seamless to the end-users Adding new services would be an interesting avenue for further research The work discusses generic processing en vironments the solution however strives for general applicability of the remote method execution pattern The discussion focusses on the low level technologies which is out of scope for our work Admittedly the user interface was not a primary concern of our work Given the heterogeneous user-base of DARIAH it would be hard to come up with an interface that could be suitable and acceptable for all users We decided to use the well-known abstraction of a 002le system as a primary lowlevel interface to the Workspace The decision was motivated by the presence of such a 002le system offered by iRODS The abstractions of the web and 002le system resources however 69 


are quite close to each other thus it would be easy to build an HTTP-based ReST interface above the 002le-system-based Workspace in the future This might become a relevant topic given the DARIAH efforts to provide an HTTP-based user interface to storage resources and common portal 21 V C ONCLUSION We presented a generic concept of Workspace for Big Data Processing in Humanities Our work was inspired by the experiences collected during the establishing of a distributed service-oriented cyber-infrastructure in DARIAHDE We proposed a solution that ful\002lls two main design goals it is user-friendly and reduces the burden of integrating and providing new services The working demonstrator of the concept helps users to process large amounts of data by employing ef\002cient data processing based on Hadoop and Pig two emerging data processing products often applied to deal with the challenges of Big Data The two products were used together with a well-established data Grid solution iRODS The paper presents a provider-centric view on enabling Big Data processing tools which is the 002rst step of their application in digital humanities As long as the tools are not available there is little chance that they will be used to solve the actual research problems A CKNOWLEDGEMENTS The work has been supported by DARIAH-DE which is partially funded by the German Federal Ministry of Education and Research BMBF fund number 01UG1110A-M and by EUDAT funded by the European Union under the Seventh Framework Program contract number 283304 R EFERENCES  D ARIAH Digita l Research Infrastructures for the Arts and Humanities A v ailable http://www dariah.eu  D Lane y  2233D Data Management Controlling Data V olume Velocity and Variety,\224 Gartner Tech Rep 2001  A Rajase kar  R Moore C.-Y  Hou C A Lee R Marciano A de Torcy M Wan W Schroeder S.-Y Chen L Gilbert P Tooby and B Zhu iRODS Primer Integrated RuleOriented Data System  ser Synthesis Lectures on Information Concepts Retrieval and Services Morgan  Claypool Publishers 2010  D T onne J Rybicki S E Funk and P  Gietz 223 Access to the DARIAH bit preservation service for humanities research data,\224 in PDP 13 21st Euromicro International Conference on Parallel Distributed and Network-Based Processing  2013 pp 9\22615  M Sarw ar  M Ale xander  J Anderson J Green and R Sinnott 223Implementing MapReduce over language and literature data over the UK National Grid Service,\224 in ICET 11 7th IEEE International Conference on Emerging Technologies  2011 pp 1\2266  D Jef fre y and G Sanjay  223MapReduce simpli\002ed data processing on large clusters,\224 Communications of the ACM  vol 51 no 1 pp 107\226113 Jan 2008  T  White Hadoop The de\002nitive guide  O'Reilly Media Inc 2012  T  Blank e L Candela M Hedges M Priddy  and F  Simeoni 223Deploying general-purpose virtual research environments for humanities research,\224 Philosophical Transactions of the Royal Society A Mathematical Physical and Engineering Sciences  vol 368 no 1925 pp 3813\2263828 2010  R Fielding 223 Architectural styles and the design of netw orkbased software architectures,\224 Ph.D dissertation University of California Irvine 2000  R Fi elding J Gettys J Mogul H Frystyk L Masinter  P Leach and T Berners-Lee 223Hypertext Transfer Protocol 226 HTTP/1.1,\224 RFC 2616 Draft Standard Internet Engineering Task Force Jun 1999 A v ailable http://www.ietf.org/rfc/rfc2616.txt  B Zhu R Marciano R Moore L Herr  and J P  Schulze 223Digital repository preservation environment and policy implementation,\224 International Journal on Digital Libraries  vol 12 no 1 pp 41\22649 2012  C Ol ston B Reed U Sri v asta v a R K umar  and A T omkins 223Pig latin a not-so-foreign language for data processing,\224 in SIGMOD 08 ACM International Conference on Management of Data  2008 pp 1099\2261110  Apache Pig Project Online A v ailable http://pig.apache.or g  H Neuroth F  Lohmeier  and K M Smith 223T e xtGrid 226 vir tual research environment for the humanities,\224 International Journal of Digital Curation  vol 6 no 2 pp 222\226231 2011  Fedora commons repository softw are Online A v ailable http://www.fedora-commons.org  A Bia 223The digital humanit ies w orkbench 224 in INTERACCION 12 13th ACM International Conference on Interacci\363n Persona-Ordenador  2012 p 50  H V ashishtha M Smit and E Stroulia 223Mo ving te xt analysis tools to the cloud,\224 in SERVICES-1 6th IEEE World Congress on Services  2010 pp 107\226114  A P a vlo E P aulson A Rasin D J Abadi D J DeW itt S Madden and M Stonebraker 223A comparison of approaches to large-scale data analysis,\224 in SIGMOD 09 ACM International Conference on Management of Data  2009 pp 165\226 178  A Ro wstron D Narayanan A Donnelly  G O'Shea and A Douglas 223Nobody ever got 002red for using Hadoop on a cluster,\224 in HotCDP 12 ACM 1st International Workshop on Hot Topics in Cloud Data Processing  2012 pp 1\2265  B Dumant F  Horn F  D T ran and J.-B Stef ani 223Jonathan an open distributed processing environment in Java,\224 Distributed Systems Engineering  vol 6 no 1 p 3 1999  D ARIAH Porta l Online A v ailable https://portalde.dariah.eu 70 


A CKNOWLEDGEMENTS The authors acknowledge the Texas Advanced Computing Center TACC at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported within this paper URL:http://www.tacc utexas.edu This research was partially supported by the National Institutes of Health NIH Grants R01GM08533703 R EFERENCES  J Dean and S Ghema w at Mapreduce simpli“ed data processing on large clusters Commun ACM  vol 51 no 1 pp 107…113 Jan 2008  C Olston B Reed U Sri v asta v a  R  K umar  and A T omkins Pig latin a not-so-foreign language for data processing in Proceedings of the 2008 ACM SIGMOD international conference on Management of data  ser SIGMOD 08 New York NY USA ACM 2008 pp 1099…1110  M Blum R W  Flo yd V  Pratt R L Ri v est and R E Tarjan Time bounds for selection J Comput Syst Sci  vol 7 no 4 pp 448…461 Aug 1973  C A R Hoare  Algorithm 65 nd  Commun ACM  vol 4 no 7 pp 321…322 Jul 1961  C A R Hoare Quicksort  The Computer Journal  vol 5 no 1 pp 10…16 1962  M Caf aro V  De Bene and G Aloisio Deterministic par allel selection algorithms on coarse-grained multicomputers Concurr Comput  Pract Exper  vol 21 no 18 pp 2336 2354 Dec 2009  S Stadtm  uller S Speiser A Harth and R Studer Datafu a language and an interpreter for interaction with read/write linked data in Proceedings of the 22nd international conference on World Wide Web  ser WWW 13 Republic and Canton of Geneva Switzerland International World Wide Web Conferences Steering Committee 2013 pp 1225…1236  R J T ibshirani F ast computation of the median by successive binning Computing Research Repository CoRR  vol abs/0806.3301 2008  H Prodinger   Multiple quickselect&mdash;hoare s nd algorithm for several elements Inf Process Lett  vol 56 no 3 pp 123…129 Nov 1995  S G Akl  A n optimal algorithm for parallel selection  Inf Process Lett  vol 19 no 1 pp 47…50 Sep 1984  P  Gupta and G P  Bhattacharjee  A parallel selection algorithm BIT  vol 24 no 3 pp 274…287 1984  D A Bader   An impro v ed randomized algorithm for parallel selection with an experimental study J Parallel Distrib Comput  vol 64 no 9 pp 1051…1059 Sep 2004  S Rajasekaran Randomized parallel selection  i n Proceedings of the tenth conference on Foundations of software technology and theoretical computer science  ser FST and TC 10 New York NY USA Springer-Verlag New York Inc 1990 pp 215…224  L Monroe J W endelber ger  and S Michalak Randomized selection on the gpu in Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics  ser HPG 11 New York NY USA ACM 2011 pp 89…98  T e xas Adv anced Computing Center   T e xas adv anced computing center http://www.tacc.utexas.edu 2006 On A v ailable  http://www.tacc.utexas.edu/user-services user-guides/stampede-user-guide  420 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of “Daily” Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data – Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average “daily“ operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rolling… I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators – Data Element Methods – Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Today’s cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlight’s data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlight’s hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlight’s method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





