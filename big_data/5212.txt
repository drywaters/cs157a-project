BitApriori: An Apriori-Based Frequent Itemsets Mining Using Bit Streams Le Thi Thanh Nhan Department of Computer Science Kyunghee University, Global Campus Yongin, Gyeonggi 446-701, Korea nhanltn@khu.ac.kr Nguyen Thi Thanh Thuy Department of Computer Science Posts and Telecommunications Institute Hanoi, Vietnam thuyr205@gmail.com Chung Tae Chong Department of Computer Science Kyunghee University, Global Campus Yongin, Gyeonggi 446-701, Korea tcchung@khu.ac.kr 
Abstract 
204 Generating, pruning and counting itemset candidates are important steps in Apriori frequent itemsets mining Unfortunately, their computation time is too expensive. In this 
paper, we propose a new method using Bit Stream to improve their speed. At the beginning, the 1-itemsets are found out and sorted according to the decline of count. By that way, a map of all attributes would be created. After that, each attribute will be presented by 1 bit. At last, the generating and pruning itemset candidates are processed by LOGIC operations which are not cost much of computation time. For experiments we compare our method with some Apriori-based state of the arts 
I I NTRODUCTION Frequent itemsets mining is a crucial step in association rule mining. It plays an important role in many data mining 
Keywords-component; formatting; Apriori, Association Rules Data Mining, Frequent Itemsets Mining \(FIM 
In dense database, the frequent itemsets are long Therefore, the candidate generating dominates the run time 
it means each transaction have to be scanned 
If the longest frequent itemset is 
n n 
200 200 
tasks including strong rules, correlations and sequential rules One of ordinary algorithms is Apriori [1  It a d op ts b r e a d th  search and finds all frequent itemsets. When the database is sparse and the frequent itemsets are short, Apriori and its improvements give high performance. However, with dense medium size database, the efficiency of Apriori-based algorithms goes down dramatically. There are some main reasons times. In other words 
extra frequent itemsets. The generation of them costs lot of time 
the entire database has to be scanned 
Each 
m-2 
The computation cost for counting support of candidates is so high In order to solve those shortcomings of Apriori, many improvements had been published. It is useful to store the candidates in a special data structure and save numerous transaction traversals. Firstly, in the ordinary paper [1  h a sh  tree had been used to store frequent itemsets and candidate frequent itemsets. Only candidate frequent itemsets, whose 
candidate is generated from 2 
n 
200 200 
m-length 
time 
subsets are all frequent, are generated in each database scan Based on the hash tree, candidate frequent itemsets are generated and their subsets are tested. Secondly, the trie-based Apriori [1 w e re  re p o r t e d  A f ter  th at  w ith in de pen d en t  researchs, Borgelt   a nd Bod o n  12 1 4  p u b l i s hed t h e fi r s t  open-source of Apriori algorithm. And then, a parallel Apriori algorithm for Frequent Itemsets Mining had been proposed   In our research, we propose a method which presents all candidates, frequent itemsets, and transaction in same-length Bit Streams. By that way, the generating candidates, and the support counting will be done by LOGIC operations with low cost of computation. The rest of this proposal will be organized 
I T 
I 
as follow. Section II introduces the description of frequent itemsets mining. Section III presents the Apriori property and Apriori algorithm. In section IV, we present our algorithm BitApriori II PROBLEM DESCRIPTION Let The set 
For any 
I = {i 1 i 2 i n  
we say that a transaction 
be a set of items and 
be a multiset of transactions, where each transaction 
contains X if 
D T T X T X 
is a set of items such that 
002 002 002 
Thus, if the total number of transactions in 
is the proportion of transactions in 
divided by 
n . 100 percent 
is the minimum support 
that contain An itemset 
is called an itemset. The count of an itemset 
is the number of transactions in is the count of 
The support of an itemset 
where 
that contain 
is called frequent if its support is greater than or equal to some given percentage 
X X D X X D X D n X X X s s 
is 
then the support of 
III ORIGINALLY APRIORI ALGORITHM Apriori finds frequent itemsets according to a user-defined minimum support. In the first pass of the algorithm, it constructs the candidate 1-itemsets. Then the algorithm generates the frequent 1-itemsets by pruning some candidate 1-itemsets if their support values are lower than the minimum support. After the algorithm finds out all the frequent 1-itemsets, it joins the frequent 1-itemsets with each other to construct the candidate 2itemsets and then prunes some infrequent itemsets from the 978-1-4244-5943-8/10/$26.00 \2512010 IEEE 


candidate 2-itemsets to create the frequent 2-itemsets. This process is repeated until no more candidate itemsets can be created. Fig. 1 shows original Apriori pseudo code Figure 1 Apriori pseudo code The apriori-gen function \(Fig.  2\ takes L k-1 the set of all large k-1 itemsets, as argument. It returns a superset of all k itemsets. The function is in following order. Firstly, in the join step L k-1 is joined with itself. Next, in the prune step, all the itemsets c  C k that some k-1 subset of c are not in L k-1 are deleted Figure 2 Candidate generating  pseudo code The count function counts support of each itemset in C. If an itemset c  Ck and with every transaction T contains c support of c will be increase 1. Counting process is shown in Fig.  3 Figure 3 Counting pseudo code An example on how Apriori discoveries frequent itemsets is shown in Fig. 4. The database in this example denoted D will be used in future examples Database D  support = 2  L 1 TID Item  Item Support  1 1 3 4 scan 1 3 2 2 3 5  2 4 3 1 2 5  3 3 4 1 2 5  4 2 5 2 3 4  5 3  C 2  L 2 Itemset  Itemset Support 1 2   1 2 2 1 3   1 5 2 1 4   2 3 2 1 5  scan 2 5 3 2 3   3 4 2 2 4     2 5     3 4     3 5     4 5     C 3 L 3 Itemset  Itemset Support 1 2 5  scan 1 2 5 2 1 2 3     2 3 5     2 3 4     Figure 4 An Apriori frequent itemset mining example IV BITAPRIORI ALGORITHM A BitApiori BitApriori is an approach in which the input file contains numerical data. If the data in the input file are not numerical 


an index file for all items should to be created. After that, the algorithm will work with this file instead of input file All candidates and transactions will be presented by a bit stream with same number of bits \(same length\he length \(by bit\ of a bit stream is n which the number of frequent items is To implement bitApriori, we use an array of STL library in C bitset<16 container to present each bit stream, the number of elements needs for this array is nE which is the smallest, nearest positive integer and greater than n div 16  Each bit stream also contains 2 other parameters start and end which indicate the first and the last non-zero element of bit stream. All operations onto every bit stream only work in range end … start + 1\ element, instead of nE element. The order of bits in a bit stream is presented in Fig. 5 Figure 5 An array of bitset<16 has nE elements With n=5 on the above example, we only need nE = 1 for each bit stream, so bit stream has only 1 element start = end = 0; with n =18, nE = 2 B Map of bit stream creation The database will be scannd to create the list of all items After that, non-frequent items are deleted and the list of items is sorted by the descent of support count. This list is set of frequent items will be used as a map of bit stream for later processes. This map presents the order of bits in each bit stream C Database conversion to bit streams BitApriori scans the database once more time again. All transactions are presented by bit streams. If transaction T contain item i which has position p-th in item list, the p-th bit of Ts bit stream will be set 1 otherwise will be set 0 Assuming that the first transaction {1, 3} contains 2 frequent items 1 and 3 which are respectively the 1-st and the 4-th in the list. Therefore, it will be presented by bit stream 1001, the 1-st bit is the most right one D The 1-length frequent itemset Each frequent item at position p-th in the map will be shown as a bit stream with p-th bit is 1 and the rest others are 0 then start = end By that way L 1 is created E Candidate generation In original Apriori [1   C k is generated by joining L k-1 with itself. It leads to many itemsets need to be pruned. There are two kinds of these itemsets  Duplicated itemsets  Itemsets have more than k items Example L 3 1 2 3}, {1 2 4}, {1 2 5},  {1 3 4}, {1 3 5 1 4 5}, {2 3 4}}, six pairs of itemsets \({1 2 3}, {1 2 4}\, \({1 2 1 2 4}, {2 3 4}\ and \({1 3 4}, {2 3 4}\  generate itemset {1 2 3 4 Besides, eight other pairs \({1 2 3}, {1 3 5}\, \({1 2 3}, {1 4 5 1 2 4}, {1 3 5}\, \({1 2 5}, {1 3 4}\ \({1 2 5}, {2 3 4}\, \({1 3 4}, {1 4 5}\, \({1 3 5}, {2 3 4}\, \({1 4 5}, {2 3 4}\eate a 5length itemsets {1 2 3 4 5}. Finding and pruning these spare itemsets cost too much time To overcome above shortcomings, we propose an improvement for candidates generating \(Fig. 6\ in which contains two main rules  In a k-length itemset I = {i 1 i 2 i 3 i k  the item i j j=1,k have to be after the item i j-1 in the map of items Therefore, an itemset we have index l only join with itemset I 1 have index l 1 l in L k-1  An k length itemset is only generated by two itemsets have the same k-2 first items Therefore, in above L 3 we join L 31 1 2 3}, {1 2 4}, {1 2 5}} and L 32 1 3 4}, {1 3 5}} with themselves Figure 6 Candidates generating without duplications Suppose that subset\(I check whether all k-1 length itemsets of itemset I = {i 1 i 2 i k  are frequent. We see easily that itemset I have k k-1\-length sub-itemsets k-1 of them begin with item i 1 and absent one items i j j=2,k and the rest one begins with i 2 i 2 i 3 i k   F Support counting For counting support for all candidate in C k bitApriori algorithms does AND operation between candidate c i and transaction bit stream b j then compares result with candidate Because the AND operation only shows the common bits from 2 bit streams, if result bit stream equals to the candidate stream c i  the transaction b i contains c i and c i s support increases 1. Providing that c i s support  minsup c i wil be inserted into L k  The details of support counting process are described in Fig. 7  


Figure 7 Support counting process Below is an example which shows how bitApriori finds frequent itemsets for database D in Section III Database D support = 2 Frequent items TID Item  Item Support  1 1 3 4 1 s t scan 1 3 2 2 3 5  2 4 3 1 2 5  3 3 4 1 2 5  4 2 5 2 3 4  5 3 Map of bit stream Sorted frequent items Position Item Support  0 2 4  nE = 1 1 1 3  Start = end = 1 2 3 3  3 5 3  4 4 2  With TID = 1, T = {1, 3, 4}, item 1 , item 3 item 4 are at positions 1, 2, 4 respectively in map of bit stream, so bit 1-th  bit 2-th bit 3-th in bit stream presents for T is 1. bitT 0000000000010110, start = end = 1 Because D has only 5 frequent items, so each bit stream presents one itemset \(transaction, candidate, frequent itemset only has 5 value bits. From now, only 5 low bit will be shown for each bit stream of this example Database D  Present D by bit streams  TID Item  TID Bit stream 1 1 3 4  1 10110 2 2 3 5 2 n d scan 2 01101 3 1 2 5  3 01011 4 1 2 5  4 01011 5 2 3 4  5 10101 Sorted frequent items   L1 Position Item Support  Itemset Support 0 2 4 00001 4 1 1 3 00010 3 2 3 3 00100 3 3 5 3 01000 3 4 4 2 10000 2 C 2    L 2 Itemset  Items Support 00011 00011 2 00101 00101 2 01001 01001 3 10001 01010 3 00110 01100 2 01010   10010   01100   10100   11000   C 3 L 3 Itemset  Items Support 00111 01011 2 01011   01101   01110   Frequent Itemsets Itemset Items 00001 2 00010 1 00100 3 01000 5 10000 4 00011 2 1 00101 2 3 01001 2 5 01010 1 5 01100 3 5 01011 2 1 5 Figure 8 An example of bitApriori frequent itemset mining V EXPERIMENTAL RESULTS In our experiments, we used four sets of data. Among them, T10I4D100K, T40I10D100K are synthetic data. They resemble market basket data with short frequent itemsets. The other two datasets, Mushroom and Connect-4 data, are real data which are dense in long frequent itemsets. These data sets were often used in the previous studies of frequent itemsets and asscociation rules mining. We downloaded them from http://fimi.cs.helsinki.fi/testdata.html Some characteristic of these datasets are shown in table I TABLE I T HE DATASETS Data set Items Avg Length Trans Type Size T10I4D100K 1000 10 100000 Sparse 3.93 MB T40I10D100K 1000 40 100000 Sparse 14.8 MB Mushroom 120 23 8124 Dense 557 KB Connect-4 130 43 67557 Dense 8.89 MB 


Our algorithm has mainly compared with three popular Apriori-based algorithms. Two of them, AprioriTrie [14   de p t h  first search Apriori \(dfApriori\ [15 w e r e  do w n l o a d e d f r o m  http://fimi.cs.helsinki.fi/src  The rest is the newest Borgets Apriori implementation from http://www.borgelt.net//apriori.html version 5.8, 2009.11.13 All experiments were performed on a Intel Core 2 Quad 2.4GHz with 2GB of memory. All times include time for outputting all the frequent itemsets. Tables II through V and figure 9 through 12 present the results TABLE II RUN TIME  S  FOR T10I4D100K DATA Support  AprioriTri e Borget's Apriori Depth Apriori bitApriori 25 2.577 0.5 10.703 0.626 20 2.75 0.51 10.715 0.632 15 2.965 0.515 10.71 0.638 10 3.225 0.558 10.716 0.685 5 4.218 0.672 10.658 0.798 0 2 4 6 8 10 12 25 20 15 10 5 Support Time \(s AprioriTrie Borget's Apriori Depth Apriori bitApriori Figure 9 Run time \(s\ for T10I4D100K data Table II and Fig. 9 show the performance comparison of the compared algorithms on T10T4D100K data. AprioriTrie runs fifth faster than Depth Apriori while both bitApriori and Borgets Apriori make a fair comparison and run twice faster than AprioriTrie TABLE III RUN TIME  S  FOR T40I10D100K DATA Support  AprioriTrie Borget's Apriori Depth Apriori bitApriori 25 19.869 1.975 40.231 2.305 20 19.878 2.047 40.032 2.382 15 20 2.198 40.256 2.521 10 20.553 2.605 40.436 2.943 5 20.475 3.438 46.197 4.225 0 5 10 15 20 25 30 35 40 45 50 25 20 15 10 5 Support Time \(s AprioriTrie Borget's Apriori Depth Apriori bitApriori Figure 10 Run time \(s\ for T40I10D100K data The comparison results on T40I10D100K data are presented by Table III and Fig. 11. While AprioriTrie performs twice as Depth Apri ori, bitApriori and Borgets Apriori performance are nearly the same and nine times as AprioriTrie TABLE IV RUN TIME  S  FOR M USHROOM DATA Support  AprioriTri e Borget's Apriori Depth Apriori bitApriori 25 1.355 0.17 0.656 0.285 20 6.312 0.37 1.576 0.482 15 12.264 0.48 1.86 0.605 10 105.894 4.55 10.34 7.875 5 571.67 40.16 74.01 52.015 0 100 200 300 400 500 600 700 25 20 15 10 5 Support Time \(s AprioriTrie Borget's Apriori Depth Apriori bitApriori Figure 11 Run time s\shroom data TABLE V RUN TIME  S  FOR CONNECT 4 DATA Support  AprioriTrie Borget's Apriori Depth Apriori bitApriori 90 13 1.547 1.125 1.672 80 44.798 2.688 35.876 2.802 70 334.485 9.628 255.638 10.171 60 675.695 47.512 585.762 49.205 50 1387.672 189.548 1201.879 192.235 


         0 200 400 600 800 1000 1200 1400 1600 90 80 70 60 50 Support Time \(s AprioriTrie Borget's Apriori Depth Apriori bitApriori Figure 12 Run time s\onnect-4 data VI CONCLUSION AND FUTURE WORK In this paper, we have presented a new approach to Apriori frequent itemset mining, bit Apriori, using bit stream as main data structures to store database and itemsets. Besides, LOGIC operations are used for joining candidates and support counting. By that way, instead of checking one item by one time, our algorithm checks a group of items at once Therefore, the performance is considerable improved In future, we will improve bitApriori so that it can be fairly compare with other frequent itemsets mining R EFERENCES 1 R  A g ra w a l T  I m i e li n s k i   a n d A   S w a m i  M in in g a s s o c i a t i o n r u les  between sets of items in large databasesŽ. In P. Buneman and S. Jajodia editors, Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC: ACM Press 1993, 207-216 2 R. A g r a w a l  R. S r ik an t F ast al g o r ithm s f o r m i n i ng asso cia tio n r u l e s  In J. B. Bocca, M.Jarke, and C. Zaniolo, editors, Proceedings 20th International Conference on Very Large DataBases\(VLDB'94 Santiago, Chile: Morgan Kaufmann, 1994, 487-499 3 J H a n, J  P e i an d Y  Y i n M in i n g f r e que nt pa tte r n s w itho u t c a n d i d a te  generationŽ. In Proc. 2000 ACM-SIGMOD Int. Conf. Management of Data \(SIGMOD00\, Dallas, TX: ACM Press, 2000, 1-12 4 h tt p  fi m i  c s  h e ls i nki  f i 2 003  5 C   B o rgelt  E ffi c i ent I m p l em en t a tion s of A p ri ori  a nd E c lat   P r oc  IEEE ICDM Workshop Frequent Itemset Mining Implementations CEUR Workshop Proc., vol. 80, Nov. 2003 6 B G o et h a ls  Su rvey on freq u e nt pa tt ern m i nin g Uni vers i t y of  Helsinki, Tech. Rep., 2003 7 G  G r ahne J  F  Z hu F as t al g o r ithm s f o r f r e que nt ite m s e t m i ni ng us ing  FP-treesŽ, IEEE Transactions on Knowledge and Data Engineering 17 10\ \(2005\1347…1362 8 F Bo do n  A s u r v ey o n f r e que nt i t e m s e t m i ni ng  B u d a pe s t  20 0 6   9 J  Han  Da ta m i nin g C onc ep t s and T e c h ni qu es    20 06 10 S e r g ey Br in R a je e v Mo tw an i Je f f r e y D  U l l m an a n d S h al o n T s ur   Dynamic itemset counting and implication rules for market basket dataŽ. In Joan Peckham, editor, SIGMOD 1997. Proceedding ACM SIGMOD International Conference on Management of Data, May 1315, 1997, Tucson, Arizona, USA, pages 255-264. ACM Press, 05/ 1997 11 S a l v ato r O r l a ndo P a o l o P a l m e r ini, an d Raf f a e l e P e re go  E nha nc ing t h e  apriori algorithm for frequent set countingŽ. In DaWak01: Proceedding of the Third International Conference on Data Warehousing and Knowledge Discovery, page 71-82, London, UK, 2001. Springer-Verlag  F  B o d o n and L a j o s R  n y a i  Tr i e: a n a l t e r n a t i v e da ta st r u ctu r e f o r d a ta mining algorithmsŽ. Hungarian Applied Mathematics and Computer Application, October 2003  Y Ye  C   Ch i a n g   A p a ra lle l a p ri ori a l gori t h m for fre q u e n t  i t e m se t s  miningŽ, Proceeddings of the Fourth International Conference on Software Engineering Research, Management and Applications SERA06\, 2006 14 F  Bo d o n   S u r p r i s i n g r e su l t s o f tr ie b ase d FI M al g o r ith ms I EEE ICDM Workshop on Frequent Itemset Mining Implementations FIMI'04\, in Bart Goethals and Mohammed J. Zaki and Roberto Bayardo editors, CEUR Workshop Proceedings, volume 90, Brighton UK, 1. November 2004  W a lt er  A  K o s t er s a n d W i m P i j l s  A p r i o r i a d e p t h f i r s t implementationŽ, IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'04\, in Bart Goethals and Mohammed J. Zaki and Roberto Bayardo editors, CEUR Workshop Proceedings, volume 90 Brighton, UK, 1. November 2004 


world databases presents a future direction for further research  Figure 1: Execution time v/s Minimum Support  References 1 R. A g r a w a l a nd R. Srik a n t F a s t A l g o rithm s  f o r Mining  Association Rules,” In Proc. of the 20th VLDB Conference  Santiago, pp. 487-499, Chile, 1994 2 R. A g ra w a l T  Im i e linsk i, a nd A. Swami. “Mining Association Rules between Sets of Items in Large Databases.” In Proceedings of ACM SIGMOD pages 207-216, May 1993 3 R. A g ra wa l, R. Srik a n t M in i n g se que ntia l pa t t e r ns In proc. of the 11th International Conference on Data Engineering \(ICDE'95  pages 3-14, March 1995 4 M Blu m R.W  Flo y d   V   P r att, R.L  Riv e st an d R E. T a rjan  Time bounds for selection,” J. Comput. Syst. Sci. 7\(1973\, pp 448-461  Total number of data items Number of elements in MEDIUM Number of elements in LOW 10 139 0 20 3238 127 30 21899 1923 Figure 2: Size of MEDIUM and LOW v/s number of data items 5 Fe r e nc B odo n  A f a s t  A P R I O R I i m plem e n ta tion I n pr oc o f  IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'03 Melbourne, Florida, USA, 2003 6 Feren c Bo d o n   A T r i e b as ed A P RIORI Im p l e m en tatio n f o r  Mining Frequent Item Sequences.” In ACM SIGKDD Workshop on Open Source Data Mining Workshop \(OSDM’05 pages 56-65 Chicago, IL,USA, 2005  M  C h en  J Han an d  P  S   Yu  Dat a M i n i n g  A n o v ervi e w  from a Database Perspective IEEE Transactions on Knowledge and Data Engineering vol. 8, no. 6, pp. 866-883, Dec. 1996 8 A r on Culotta A ndre w Mc Ca llum Jona tha n Be tz  I nte g ra tin g probabilistic extraction models a nd data mining to discover relations and patterns in text,” In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics p.296-303, June 04-09, 2006, New York  M M S u fy an Be g P aral l e l an d Di st ri b u t ed Di sco v er y o f  Association Rules In Artifical Intelligence Application Book  Fadzilah Siraj, Eds\ersity Utara Malaysia 10 X ita o Fa n k os Fe ls v  l y i Ste phe n A  Siv o  M onte C a r l o   SAS® for Monte Carlo studies: a guide for quantitative researchers 11 U  Fa y y e d  G  P i a t e t s k y Sha p iro P. Sm y t h a nd R. U t h u ra s a my  eds.\. “Advances in Knowledge Discovery and Data Mining AAAI Press / The MIT Press, 1996 1 W  J F r aw l e y  G  P i at et sk y S h ap i r o an d C M a t h eu s   Knowledge “Discovery In Databases: An Overview. In Knowledge Discovery In Databases eds. G. Piatetsky-Shapiro, and W. J Frawley, AAAI Press/MIT Press, Cambridge, MA., 1991, pp. 1-30 13 V  K u m a r  a nd M. J o s h i T utor ia l o n H i g h P e r f or m a nc e D a t a  Mining,” In proc. of International Conference on High Performance Computing \(HiPC-98 Dec. 1998  a v i d L  O l s o n a n d D e s h e n g Wu   D eci s i on  m a k i ng  with uncertainity and data mining.” In X. Li, S. Wang and Z.Y Dong\(Eds Lecture notes in Artificial Intelligence pp. 1-9 Berlin: Springer\(2005 15  P  W r ig ht. K now le dg e D i s c ov e r y I n D a ta ba s e s  T ools a n d  Techniques ACM  Crossroads Winter 1998   
408 


 7. Reference    Fetzer C,Hagstedt, K,Felb er P  Autom atic Deteciton  and Masking of Non-Atomic Exception Handling International Conference On Dependable Systems and Networks, \(DSN2003\10-116    Y e n S J, Lee Y S. “Mining Interesting  Associatio n  R u l es  and Sequential Patterns”. International Journal of Fuzzy Systems, 2004-6 \(4   Alasf f ar A  H, Deogun J S. “Concept-b a sed Retr iev a l with Minimal Term Sets”. Foundations of Intelligent Systems: 11th Int’l Symposium, Springer, Poland, 2004 114- 122   Qiu Y ong gang,Frei H P  Concept B a sed Quer y   SIGIR’03,2003:16 0-169   Saltom G  W ong A, Y a ng C  S. “A V ector Sp ace Model for Automation Indexing”. Communications of the ACM 2005, 18\(5\-620   Agrawal R, Srikant R. “Fast Algorithm f or Mining Association Rules in Large Databases.” Proceedings of the 20th International Conference on Very Large DataBases Santiago , Chile , 2004   Park J S. “Using A Hash-Based Method with Transaction Trimming forMining Association Rules.” IEEE Transactions on Knowledge and Data Engineering, 2007   Savasere A, Omiecinski E Navathe S  An Ef ficient Algorithm for Mining Association Rules in Large Databases.” Proceedings of the 21st International Conference on Very large Database, Switzerland, 2002  


              


   


                        





