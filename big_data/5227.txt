AVI:Based on the vertical and intersection operation of the improved Apriori algorithm Yan  Zhang School of Computer Science and Technology China University of Mining and Technology Xuzhou, China e-mail: zyan@cumt.edu.cn Jing  Chen School of Computer Science and Technology China University of Mining and Technology Xuzhou, China e-mail: 506187212@qq.com Abstract Association Rules is the extraction of data mining in the important research, analysis and research in the Apriori algorithm for association rule the algorithm for computing efficiency is not high, proposing an improved algorithm AVI Apriori with vertical and intersection operation\hat use itemset union and identification intersection, just scanning the transaction database one time that can get the identity of a set of first-order and large set of vertical. K-order designate the options set for this operation as long as the first order of the large itemsets, scanning the database without having to repeat thereby improving the efficiency of the mining algorithms Keywords- data mining; association rules; Apriori algorithm AVI algorithm I I NTRODUCTION Date Mining often referred to as knowledge discovery in databases \(KDD\, precisely, and KDD of knowledge in the learning stage, known as data mining. Data mining is a very important processing step of KDD, and it is a crossover field of study, Integration of the database, AI, Machine Learning Statistics, Knowledge Engineering, Object-Oriented Method High Performance Computing and Data visualization. The latest technology of research results is shown in [1  II ASSOCIATION RULES ANALYSIS An association rule is an important element of Data Mining research. Association rules showing attribute - value is frequently given along with data set conditions for the emergence , can be described as Supposing I is a k-sets composed of a collection of different projects A Transaction database D, in which each transaction T is a set consisting of a group items of I, namely T  I, T has the only identification TID. If itemsets X  I and X  T, then transaction set T contains the itemsets X. An association rule is an implication, such as the form X => Y   and    Association rule X => Y condition is established: It has degree of support for Supp, namely, there is at least Supp% of the transaction in database D that contains X  Y. It has degree of confidence for Conf namely, the transaction database D contains X, there are at least Conf% transaction also contains Y Association rule mining problem is found to have userspecified minimal support\(min_sup\nd minimal confidence\(min_conf\ association rules. This problem is usually decomposed into two subproblems. One is to find those itemsets whose occurrences exceed a predefined threshold in the database; those itemsets are called frequent or large itemsets. The second problem is to generate association rules from those large itemsets with the constraints of minimal confidence III A PRIORI ALGORITHM The greatest impact of mining association rules algorithm is the Apriori algorithm presented by RakeshAgrawal and other  I t is a Bo olea n a s s o c i a t i o n r u l e m i ning fr e q ue nt  itemsets algorithm, That is, first generate frequent itemsets completed by the two-step connection and pruning\, and then generate association rules. The basic idea of the algorithm is that scanning the transaction database many times, by using the principle that in a given transaction database D, any frequent itemset is a subset of frequent itemsets; superset of any weaknesses is the weaknesses set This principle is scanned many times on the transaction database. The Apriori algorithm uses breadth-first search and a tree structure to count candidate item sets efficiently. It generates candidate itemsets of length i from itemsets of length i-1  First of all, to identify a group of frequent sets, the collection is recorded as L 1 L 1 is used to find the set of frequent 2-itemsets L 2 L 2 is used for L 3 If it goes on, until you can not find frequent k-itemsets L k L i needs to find a database for each scanning, L i-1 to find out how the L i is divided into two processes composed of connections and pruning. According to the core idea of the Apriori algorithm the functional block diagram of the Apriori algorithm is shown in Fig.1   By the Apriori algorithm can be seen, Each C i have to scan the database once, but this time some business have no effect on the generation of frequent itemsets. Thus reducing the number of scanning the database D, For improving the efficiency of the algorithm it is necessary [4  bas ed  on  th e  Apriori algorithm is low operating efficiency of existing inadequate, This paper presents an improved association rule mining algorithm - AVI algorithm V2-718 978-1-4244-5824-0/$26.00 c  2010 IEEE 


Figure 1 the function block diagram of the Apriori algorithm IV AVI A LGORITHM D ESIGN AND I MPLEMENTATION A A  Algorithm Optimization Definition 1 For itemset X, t \(X\ = {tid | tid is the identity of t, t D and t supports X}; For the identification set Y, i\(Y y Y itemset\(y\itemset \(y\ that y corresponding to the transaction itemset[4  F o r ex am ple   t\({ A ,B ,D}\ = { 2 , 6 i\({2,6}\A,B,D A,B,C,D,E}={A,B,D See Table  TABLE I TRANSACTION DATA SHEET TID R-listitem TID  1 A,B,E 2 B,D 3 B,C From Table we can get Itemset X = {A}: the vertical itemsets of corresponding identification sets is {1 Itemset X = {B}: the vertical itemsets of corresponding identification sets is {1, 2, 3 Itemset X = {C}: the vertical itemsets of corresponding identification sets is {3 Itemset X = {D}: the vertical itemsets of corresponding identification sets is {2 Itemset X = {E}: the vertical itemsets of corresponding identification sets is {1 AVI algorithm uses the vertical itemsets, by a secondorder frequent itemsets generated set of candidate processes Through the key set and operations, identification sets of intersection operations are completed, for example A B={1  1,2,3}={1 B The counting of support and the description of the AVI algorithm Let the database of transactions D = \(T1, T2, ..., Tn\ ,it consist of n transactions. Symbol X.sup means the degree of support, itemset X in D, Symbol X.count means the degree of support counts, itemset X in D For each candidate k-itemsets, connected with it as long as from the frequent \(k-1\key focus, Optional 2 sets of vertical identity representations are bitwise intersection operations, statistical results and then identify the number of concentrated elements, as a support for counting [6   Input k-1\requent itemsets candidate k-itemsets to connect nodes in R, two-node P and Q identification sets vertical are P b and Q b  Output the counts of support of the candidate itemsets k nodes R.count Algorithm C The illustration of the AVI Algorithm Supposing min_sup 2/9=22%, minimum support count is 2, sample data are shown in Table  TABLE II T RANSACTION  DATABASE D TID Itemset TID Itemset 1 ABE 6 BC 2 BD 7 ACD 3 BC 8 ABCE 4 AB 9 ABC 5 AC   The AVI Algorithm steps are shown as follows Step 1 Scanning transaction database D, the entry in D will be converted to table and to count the number of elements identified focus Step 2 Because of min_sup 2, can get the candidate 1-itemsets. Remove items that are not frequent 1-itemsets Generate the vertical set of frequent 1-itemsets.\(See Table  TABLE III C ANDIDATE 1ITEMSETS VERTICAL Itemset Identification vertical Sup.count A} {145789 6 B} {1234689 7 C} {356789 6 D} {27 2 E} {18 2 Step 3 According to candidate set of trees and are known to frequent 1-itemsets to find frequent 2-itemsets Process is as follows: according to the first layer of the tree nodes is frequent 1-itemsets node, that the second layer of each node in each subset is frequent, so generated by the Table that for any two vertical lines marking sets intersection operations. \(See Table Step 4 Then counting the number of elements and the count as its degree of support, If support  min_sup, the Volume 2010 2nd International Conference on Fu ture Computer and Communication V2-719 1 R b P b Q b 2 For \(i=0;i<n and R b 0;i++\   do begin 3 If \(\(R b Q b 0\ //Indicated that Rb and Qb are included the first same element 4 R..count 5 Shr\(R b 1 To shift to the right one 6 End 


corresponding combination is a frequent itemset 2-itemsets 8   B y f r eq u e nt 2 item sets get a new id entif i cat io n of  vertical. \(See Table  TABLE IV C ANDIDATE 2I TEMSETS VERTICAL Itemset Identification vertical Sup.count  A,B} {1489 4 A,C} {5789 4 A,D} {7 1 A,E} {18 2 B,C} {3689 4 B,D} {2 1 B,E} {18 2 C,D} {7 1 C,E} {8 1 D,E 0 TABLE V FREQUENT 2 ITEMSETS VERTICAL Itemset Identification vertical Sup.count A,B} {1489 4 A,C} {5789 4 A,E} {18 2 B,C} {3689 4 B,E} {18 2 Step 5 Found the frequent 3-itemsets. The first step generate a candidate 3-itemsets, according to candidate set of trees, for the third layer node, if it is the second layer of nonfrequent 2-itemsets linked nodes, pruning a link with the second-tier node, the remaining node with the second level linking to third-layer nodes is a candidate 3- itemsets, can get ABE nodes, choose any two of frequent 2-itemsets linked to the identity of the second-tier of nodes in the vertical set of operations that a transaction, count the number of elements if support  min_sup then it is a frequent 3-itemsets, or not , by processing we can get  L 3 A B C A B E\}.According to the calculations we get a frequent 3itemsets. \(See Table  REQUENT 3-V ERTICAL Itemset Identification vertical Sup.count A,B,C} {89 2 A,B,E} {18 2 Step 6 Find frequent 4-itemsets According to the above steps, there is no candidate 4-itemsets  therefore, there is no frequent 4-itemsets Step 7 The process of frequent itemsets found in the end, get all the frequent itemsets L1= {A, B, C, D, E L2={{A,B},{A,C},{A,E},{B,C},{B,E L3= {{A, B, C}, {A, B, E V E XPERIMENT AND R ESULT A NALYSIS After the text edit has been completed, the paper is ready for the template. Duplicate the template file by using the Save As command, and use the naming convention prescribed by your conference for the name of your paper. In this newly created file, highlight all of the contents and import your prepared text file. You are now ready to style your paper In order to facilitate research and experience the AVI algorithm, this article uses Visual FoxPro to achieve the AVI algorithm A design ideas In this method, for ease of operation, the introduction of the following data table, And made the following assumptions  Define a transaction data table TRAN.DB, it consists of two fields, field names respectively T, ITEMS which are character [7  In  tu r n beh alf  of th e  P a n e l  ID, things included in the project record  Create a project data table ITEM.DBF, it contains a field, called 11, Data type is character, used to store all the items, each record in this table represents a project. The number of records in the table is the number of projects  Create a number of empty tables ITEM1.DBF,ITEM2.DBF,ITEM3.DBF,ITEM4.DB F,ƒƒ, these were used to store an item frequent sets,2 project frequent itemsets,3 project frequent itemsets,4 project frequent itemsets. Before you run the program, these tables, only the structure, there is no data  Generate a set of frequent. By 1 project frequent, we can construct a candidate itemset tree  Generate two frequent sets. The use of ITEM1.DBF And the candidate item set the length of the tree produces two candidate itemsets and the corresponding identification set into the ITEM2.DBF.count the number of identification namely, the degree of support the candidate itemsets remove support for less than a given minimum support of all records, Find all the length of two of the frequent itemsets into ITEM2.DBF, The rest of analogy, until you find all the frequent itemsets. If it is found the number of candidate itemsets is zero then stop operation  Finally, the output of all frequent itemsets.The programs need only scan the transaction database one time B Experimental results In this paper, AVI algorithms experiments, The execution time of the algorithm were compared, use a real experimental data\(March 2008 non-computer professional computer grade test scores library in China University of Mining and Technology \,A total of 1200 experimental database records, in order to facilitate excavation, the number of attributes and types of property data into Boolean types, Transformed total of 20 properties, The length of each transaction 4, taking into account the actual situation respectively, a minimum degree of support for the 1%,2%,3%,4%,5%and 6%, Tested this algorithm and the Apriori algorithm to improve execution time, shown in Fig.2 V2-720 2010 2nd International Conference on Fu ture Computer and Communication Volume TABLE VI F VI 


Figure 2 Different supports for  the algorithm execution time In order to improve the performance of mining frequent closed itemsets, AVI can be used in the algorithm to sort the child nodes, as well as dynamic optimization strategy. The newly generated child nodes according to their support for the size of the small to large re-sequencing, Thereby reducing the number of branches, speed up the frequent closed itemsets mining process This paper uses a comparison with the Apriori algorithm the experimental data are from http://www.Cs.Rpi.Edu/zaki/software/. Experiments show that degree of support between the two larger only slight changes, reduce the degree of support when the change will be sharply increased In particularly, for the minimum degree of support of 45 AVI faster than the Apriori is about 8 times, shown in Fig.3 Figure 3 AVI performance analysis VI C ONCLUSION The AVI algorithm presented in this paper, it is an identity-based set of vertical, through the intersection operations, and generate candidate itemsets tree and the corresponding degree of support. Comparing with classic Apriori algorithm, AVI algorithm only needs to conduct a scan of the transaction database, to significantly reduce I / O frequency, so the AVI algorithm is simple and moderate memory overhead. Through relevant experiments, for different minimum support, to compare the execution time of these two algorithms, the superiority of the AVI algorithm in the larger minimum degree of support is not very clearly however, with the minimum degree of support reduction, its superiority gradually increases R EFERENCES 1 Z e n g X i ao w e n the st udy o f the as so ciat io n r u l e s an d d a t a mi n i ng  methods[J  C o m put er a n d M o d e rni za t i on 200 6 9  90  9 3 2 J ia w e n H a n,J ia n pe i, Y i w e n Y i n   Mi ni ng f r e que n t p a t t e r ns w itho ut  candidate generation [J  SI G M OD 2 0 0 1  1  12 3 C he n Y u s he ng D e n g X i ao g ua ng J i a ng X i ao y ao A pr io r i A l go r ithm  for Frequent Itemsets Mining in the Realization.[J   Computer Technology and Development,2006,16\(3\:58~60 4 Z aki M J  H s iao  CJ   CH A R M A n Ef f i cie n t A l g o r ithm f o r Cl o s e d  Association Rules Mining. Technical Report 99-10. Computer Science Department of Rensselaer Polytechnic Institute , 1999 5 C he nA n Che nN ing  D a t a M i n i ng T e chn o l ogy andA ppl ic at io n  M Be i j i n g: Science Press,2006 6 Z aki M J  G e ne r a ting N o nr e du nd an t A s s o ciat io n Rul e s  Proc. of the 6th ACM sIGKDD Intl Conf. on Knowledge Discovery and Data Mining. 2000 :34-43 7 Z ho u Cu iho ng H e J ianju n.A n im pr o v e m e nt o f t h e A p r i o r i al g o r ith m  in Mining Association Rules[J u n a n Ci ty  College,2006,15\(4\:68~69 8 B a nks J  Be n n am o un M. A co ns tr ai nt to im pr o v e t he r e l i abil i t y o f  stereo matching using t he rank transform IEEE International Conference. Vol. 06, 1999Huang Hai-yan, Gu Xing-sheng, Liu Mandan. Research on Cultural Algorithm for Solving Nonlinear Constrained Optimization [J  A C T A  A U T O M A T I C A S I NI C A 2007   33 \(10\115~1120 Volume 2010 2nd International Conference on Fu ture Computer and Communication V2-721 


7# 7#\012##**%-2 7!!&6      78            7  0    77     1    7 2      0\012  77 48*$!#! 7      7   8  8     8 37  7                 8       8 8!#$ 7        8 8 8               3    7    1  2    74   87  8 4! *##!7     1       7     1         78        7      8 7   8    8 4 I4 I      7       7  8 I4 I4 I  78  M 015  1 015  2  E 8  8 I  78 M 3 4 3 N           1  23       M 015 0 4 015  N           7   8 9#-6 7 3=E\015   E\015  3\015         F A       A F F  F  A F A  3=E\015  I E\015  3\015         F A        F F  F A A F A A 3=E\015  I E\015  3\015        F A        F F  F A A  A A x 1 x 2   x 1   x 2 ref  MCHFL controller Inverted pendulum  Gains 3=E\015  I E\015  3\015         F A  F A  F A      F A 3           F A F     F A    A  F A       3=E\015  I E\015  3\015         F A  F A  F A      F A 3        F F  F A F  F F F F F F F A A  F A F F A A A A 


3=E\015  I E\015  3\015  0  015*3   F A 015*5     F F  F A A F A A        8  012  3          8  8  C      4   4     7 8J  7     8     8        8          3=E\015  J 3\012  FFG	E\015	;=\015   8  2  2 012E        7  8 J       7  8    K    K  012#!9!4  I 012 012E                    8 7  8       8   8    8          8    7! 78 8 012   3 012H:E\015<;\015           8    7 4!4 3  3 AA\015J   6     O   O  015\015\015\012\015     4 3  34 MA#7      N4 4    015 015 5 012\015  4 3\0124 0P 4  43;34M*#7$#!#N4 4#$343!C47:A4 \012 5  6 015 5\015 4  I!!4 66/4 P  012:4EJ:! 4M3!##!!7**#+#!8 N4!$##!!47#&\-4**\.&P\.24\666    4 3  34 M  3**8    012   012   7   N4    3  012#*! 4 7#54 4 P&\4\65  J?F! 43H?#!4G<?#!4AEM3**#+#!\012*8   4     3   N4    012   7#&4!#&4**\P.&4\6 0 G 4  G 4  4 M 012    012        7 3**#+#N4 015\015\015 4     0128!\(*=@\0128!47#-64!#04**2/1P20.4\66 1 3  34  84 M      N4\012#!#!#*7#\-4!#\4**.2 4&55 2 012 3#4 3  34  H4 3 C4 M3        8 8   N4 A  012#!$#!"#,47#\4**&\.0\(&\4\6 5  H4  4 3  34 M3      N A   015\015\015  012  4  0128!7#14**-/2\(-0-4 


6 3  34 M     38  4NA##$\015\015\015!\012#!$#!"#,4\0127#-4 51\(&.664&551 4E=# #4\015*#43;34M\012#!8    7   3   N4 E   012 4 7#-21\4 14 660   F#4  E 4  G E#4 3  34 M 7    7  87  8      N4 E   012   8 E   3   E   47#-6114**\\(\4\6  F#4  E4  GE#43;34M   7  8 N4 E   012   8 E   3 E#!=#!$#4 7#\54 0\(-6/4 66   4 3  34  H4 M8    8     7    N4 660 015\015\015 012  0157 012#*#!4 012\015\012 6604!#&022-0\4**/2&\(/224\660 44;443;34M38   N4    015#!4 7#214 04 01/\(02\4 666 0'A#!#!43;34AG 4E4M;#! 7    7  7 N4  0128!47#054!#\**&&5\(&\24&55 1'A#!#!43;34M 7!N4 7   4 7#\64  15\(-.54 551 2  H4  E 4 3  34 4  3 4 M 8#!C#!7##! N4 A!# !#!E47#\!#/4**/26\(/5.4\662 5'!4H43;34;4M3   8    N4    47#&/14!#.4**/6&\(/&/4\60 6'!4H43;34;4M3          77N4 015\015\015!#!#!4;!4!\0128!A3 47#--4!#\4**&06\(&024\6  E 4     4 M3    N 7\0127 47#&.4!#\4**\P\4\60 H4;?!3;34ME 3  3**#N4        012 4<47#&4!#&4**&.\P&.14\6   74 M     N4 5514 5  3  34 M\0157       7 N4 015\015   4 7#.24  4  2 504\66                  4 3'$\015  Q#!!4\66   012 012!4 012  4   4 M   012  E 8N4  015\015\015  012  4   0128!47#.4**-&\\(-&--4\6 1':! 4J?F! 4?3H!4M!I8 E!! $#N4\015\015\015 47#&14!#\4**.02P.104\665 2';=!?4H!#!43;34;#4M#!7   38  E       N4 A    012   3  4\012347#\4!#.-116204**&&6-\(&&614\661    4  I F8!4  4 M    4 8   N4   4**-15\(.6/4\66 6  H4 3  34 M;3<3 3 E    8#!;\(3 !<!3 #N4 012#*! 43#!#$#!#!4 3**#!4        A  7 3**#4*! \(I 4I#&64#54**1/1\(11\4\60 H4; !43;34R#"\0157 3*7 #$!\0157#7! 3 !R4 0157#7! <\015\24:!\(=##4 4**\5\(-.4\662 H4=,#43;34M3A7! !*8  8     012   A 8 012#!#N4 A      0157#7 015S\6604E,4	H4**-&\\(-&14\660   H#!4  H4 3  34 M3    3     012 A#8 3N4   012    012#!#4 3  8 012\012S\14!4**-5.\(-514\661   H#!4  H4  0124 3  34 M3   3   H  HN4 5  012  3$! !!#$\012#*! \0123\012T\24!\012 A#8   4 012    3**#!4F,#*!\(A#!4**/6-\(/&.4\662  4  H 4 M\012 012    N4   A 4 3A&5!\012#!$#$#3!4**-2-P-214\6 0  4   E4 M	!7  8   N4!47 64**&1/\(&224\6 1 4  4 M     N4     7 6&4 1 04&555 2'IE#4\012#!U$#V#*+WW@#!78 4E33*!#6-0.-4\66 


  Fig. 4. Experimental results on iris dataset     Fig. 5. Experimental results on breast \(breast cancer\ataset     Fig. 6. Experimental results on pima dataset  93 93.5 94 94.5 95 95.5 FACISME CPAR CMAR CBA C4.5 Ripper FURIA FR3 SLAVE 94 94.5 95 95.5 96 96.5 FACISME CPAR CMAR CBA C4.5 Ripper 71 72 73 74 75 76 FACISME CPAR CMAR CBA C4.5 Ripper FURIA SLAVE 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





