Mining Frequent Closed Itemsets Usin g Antecedent-Consequent Constraint and Length-Decreasing Support Constraint Zhi Liu, Qiuying Li, Mingyu Lu  Information Science and Technology Dalian Maritime University Dalian, China e-mail: lzsgmsc@126.com Hao Xu National Integrated Center of Cardiovascular Disease Chin-Japan Friendship Hospital Beijing, China AbstractÑAt present, many frequent itemsets mining algorithms adopt a constant support threshold value strategy which is not convenient to potential valuable long itemsets discovery. Length-decreasing support constraints can address this problem probably. Existing algorithms make improvements on classical algorithm, which result to low efficiency. Tailored to the medical data, this paper proposes a frequent closed itemsets mining algorithm called ACLCMiner which uses the antecedent-consequent constraint and the length-decreasing support constraint. With the antecedent consequent constraint, the algorithm greatly reduces the number of generated frequent itemsets. The experimental results show that ACLCMiner is efficient and can find more long itemsets with potential values Keywords- frequent closed itemsets; antecedent-consequent constraint; length-decreasing support constraint; FP-tree I I NTRODUCTION Association rule mining is an important research subject in data mining. The study of association rules mainly contains two research directions: frequent itemsets discovering and rules correlation analysis. The contributing factor of the low algorithm efficiency is frequent itemsets discovering, so many frequent itemsets discovering algorithms have been raised to address the problem including constraint-based frequent pattern mining[1  frequent closed itemsets mining algorithm[3  m a xim a l frequent itemsets discovering algorithm a n d s o  o n T h e s e algorithms make support threshold a constant, so they can not find long itemsets which have a low support with potential values. In practical applications, the users are often actually interested in the rules generated from long itemsets Thus, the length-decreasing support constr is  presented, which can help to find the long itemsets. Then many algorithms based on the length-decreasing support constraint are proposed, such as, LPMiner S L P M i n e r[6  WLPMiner[7-8 B A MB O O  9  a nd S CCM ET re e 10   Association rules can be used in the medical field to improve the accuracy of diagnosis and treatment. Each rule represents a simple prediction model, and the decision attribute \(diagnosis type\ is determined by non-decision attribute \(dialectical factors\. In another word, the position which the attributes appear in the rule is limited. Therefore according to the characteristics of medical data, this paper presents a frequent closed itemsets mining algorithm based on the antecedent- consequent constraint and the length-decreasing support co nstraint\(ACLCMiner\. The antecedent-consequent constraint  is used t o  re stric t th e  number of generated frequent itemsets, and the length-decreasing support constr  is use d t o ge t m o re valuable long itemsets. ACLCMi ner adopts the strategy of CLOSET o ge t freq u e n t  c l o s ed  pa tt e r ns a n d use s s o m e  pruning strategies to reduce the search space. The compound frequent pattern tree and Rtree are used to store transaction data and generated frequent closed itemsets, respectively Experimental results show that the efficiency of ACLCMiner is significantly higher than BAMBOO II P ROBLEM S TATEMENT AND R ELATED W ORK A Problem Statement Here we give the definition of association rules. Assume I={i1, i2,É, in}is an item set, which parameters i1, i2,É, in are called items. Transaction T is a set of item, T I. The set of transactions is denoted by TDB, as in Table  An itemset X is a non-empty subset of I and it is called k-itemset if it contains k items \(i.e., |X|=k\. A transaction T is said to contain itemset X if X  T. Association rules are those implications depicted as X Y, where X I, Y I, and X Y  TABLE I A transaction database TDB tid Set of items Ordered item list 001 a, c, f, m, p f, c, a, m, p 002 a, c, f, i, m, p f, c, a, m, p, i 003 a, b, c, f, m f, c, a, b, m 004 b, f f, b 005 b,c,p c, b, p Definition 1 The support of the rule X Y is the ratio between the number of transactions containing X and Y in TDB and all the transaction number \(i.e., P\(X  Y\\ which can be written as support\(X Y\ The confidence of the rule X Y is the ratio between the transaction number including X and Y and those including X \(i.e., P\(Y|X\\, which can be written as confidence \(X Y Definition 2 Find out all Xês non-empty subset t to each frequent itemset X. Association rule t X-t can be obtained when sup\(X\/sup\(t\>=minconf and minconf is defined as the minimum confidence of the rule. Sup\(X\/sup \(t\ is defined as the confidence of the rule t X-t where t is the antecedent of the rule and X-t is the consequent of the rule V1-580 978-1-4244-5824-0/$26.00 c  2010 IEEE 


Definition 3\(length-decreasing support constraint A function  fl with respect to a transaction database TDB is called a length-decreasing support constraint w.r.t. TDB, if it satisfies  01\(\1 fl fl   for any positive integer l  An itemset X is frequent w.r.t. length-decreasing support constraint of  fl if sup    XfX   Property 1 Smallest Valid Extension 5 SVE\ Given itemsets X and X  where XX   and sup    XfX   Then the necessary condition which X  is a frequent set w.r.t  fl is that the length of X  is greater or equal to        1 sup min  sup fX lflX    Definition 4 \(closed itemset An itemset X is a closed itemset if there exists no proper superset XX   such that   sup sup XX    Definition 5 \(frequent closed itemset If an itemset X is a closed itemset and frequent w.r.t  fl X is called a frequent closed itemset which satisfies the length-decreasing support constraint of  fl in short, X is a frequent closed itemset w.r.t  fl  B Related Work Mining frequent itemsets using the length-decreasing support constraint was first proposed in  w h ic h a l s o  proposed the algorithms with the length-decreasing support constraint \(LPMiner and SLPMiner\hese algorithms could effectively find all itemsets that satisfy a length-decreasing support constraint. LPMiner used the SVE property to add several pruning methods to compress the search space on the basis of FP-Growth algorithm. However, LPMiner has a number of limitations. Firstly, the efficiency of the algorithm is not high; secondly, when the data is large, the constructed FP-tree is too large to be placed in memory; thirdly, its result set may contain a large number of redundant \(i.e non-closed\ itemsets. WLPMiner[7-8 w e i g h t ed each ite m i n  the itemsets differently on the basis of LPMiner algorithm To tackle the shortage of LPMiner, BAMBOO[9 w a s  proposed, which is a frequent closed itemsets mining algorithm with length- decreasi ng support constraint. Taking advantages of the property of length-decreasing support constraint, Invalid Item Pruning method and Unpromising Prefix Itemset Pruning method were proposed to make BAMBOO more efficient than LPMiner. But the overall efficiency was affected due to the functionally overlapping in pruning of aforementioned methods in BAMBOO III ACLCM INER A LGORITHM A Length-Decreasing Support Constraint According to definition 2, the length-decreasing support constraint  fl is defined as    max 1 min max min min max  max min  min max min max sll sll ss ll fl lll sll         wh e n  when wh e n  1 Where l is called the length of an itemset  max min 0,1 max min ss s s  and lmax is the maximum length while lmin is the minimum length. When the length of frequent itemsets is smaller or equal to lmin, the threshold of support gets the maximum value smax. When the length of frequent itemsets is greater or equal to lmax, the threshold of support will be the minimum valu e smin and the support will no longer reduce. Figure 1 gives the function curve diagram for length-decreasing support constraint Figure 1 Function curve of the minimum support B Antecedent-Conse quent Constraint Based on the characteristics of the data, we will divide the attributes of the database into decision attributes and non-decision attributes in accordance to these attributes as the different aspects in the rules. Non-decision attribute appears in the antecedent of rule, while the decision attribute appears in the consequent. For the rule X Y, we need to calculate support \(Sup\(X Y\=P\(X  Y\and confidence conf\(X Y =P\(Y|X\\(X  Y\/P\(X\\, without the need to calculate P\(Y\lue. So we do not need to generate the frequent itemsets which only contain decision attributes Antecedent-consequent constraint restricts each itemset generate only one rule, which greatly reduces the number of generated rules Our algorithm adds the constraint of the antecedent and consequence, so we need to encode the attribute values block by block in TDB. For an itemset I={i 1 i 2 i n we encode the decision attributes those act as the antecedent of rules from zero to m and the non-decision attributes those act as the consequents of rules from m+1 to n C Storage Structure 1 Compound Frequent Pattern Tree Since the use of length-decreasing support constraint, if the itemset X is frequent, the support of X must satisfy that support threshold corresponding to the length of X, which is   fX  Therefore, we re-define the structure of f_list to make the determinaton of frequent 1-itemset and the length of transitions containing this itemset correlated. According to the data in Table we give the structure of f_list, as 1 lmax minsup smax=f\(lmin smin=f\(lmax lmin     Length of itemsets Volume 1 2010 2nd International Conference on Future Computer and Communication V1-581 


shown in Figure 2. Itemset is counted acco rding to the length of transactions Assume the parameters of  fl are max 0.8 s   min 0.4 s   min 3 l   max 5 l  So the function  fl is   0.8 3 fl l     0.6 4 fl l     0.4 5 fl l  it is easy to figure out that {f, e, a, g, h} is the complete set of frequent closed 1-itemsets w.r.t  fl  For {b}, the support of any length does not satisfy  fl so any itemset containing b is not frequent. The reason is given in the invalid item pruning strategy. Therefore, {b} is non-frequent itemsets f 4 e 4 a 3 b 3 g 3 h 3 i 1 Figure 2  Compound f_list structure Figure 3.  FP_tree.               Figure 4.   CFP_Tree The compound nodes in the compound frequent pattern tree \(CFP_Tree\ are obtained by merging  Theorem 1 Given CFP_Tree. For nodes P and C, if each path containing P has C and there is   sup sup PC  in each branch, C can be merged with P to form a new node and C can be removed from the CFP_Tree Proof If each path containing P in the CFP_Tree has C and there is   sup sup PC  in each branch. Then P and C always appear together, or both of them do not appear. So all of the closed itemsets generated must include both P and C or neither. Therefore, P and C can be merged Figure 3 shows the FP_tree constructed with the data in Table while Figure 4 is the CFP_Tree corresponding to the FP_tree. CFP_Tree has fewer nodes than FP_tree. Also in the f_list P and C can be merged into an item. Therefore CFPTree need smaller space than FP_tree 2 RTree Structure To speed up the closure checking we should choose the tree-like structure to store the generated frequent closed itemsets. This structure has good performance in the search speed and storage space Therefore, we propose a structure called RTree to store the generated frequent closed itemsets. Each node stored the item and its support. Figure 5 shows an example which stores totally four closed itemse ts: {f,e,a,g,h:2}, {f,e,a,g:3 e:4}, {f:4 Figure 5  RTree structure In RTree, if the supports of parent node and the supports of child nodes are different, a frequent closed itemsets from the root node to the parent node is then formulated. For example, in Figure 5, the support of {f} node is 0.8 and the support of {e} node is 0.6, then we have a frequent itemset f: 0.8}. Also, if the current node is a leaf node, a frequent closed itemsets from the root node to the current node is formulated, such as {f, e, a, g, h: 0.4 D Pruning Strategy In ACLCMiner, an efficient pruning method, Invalid item pruning, is adopted, which is based on the SVE property. Meanwhile, accord ing to the antecedent consequent constraint and the requirements of closed itemsets, ACLCMiner uses decision attribute pruning, item merger and sub-itemset pruning to reduce the search space 1 Invalid Item Pruning Definition 7\(Invalid item Given a conditional database TDB P w.r.t. a particular prefix itemset P. For any item x which appears in TDB P We maintain a total number of max_ x l  supports, denoted as  s1 max_ x x l  where max_ x l is the maximal length of the transactions which contain item x and   s1max_ x x kk l  records the support of item x in transactions which the length is k. Item x is called an invalid item w.r.t. TDB P if    max_ sup s   x l xx jk kk jfkP    Here is Invalid item pruning. For a particular prefix itemset P and its conditional database TDB P there is no hope to grow P with any invalid item w.r.t. TDB P to get closed itemsets satisfying the length-decreasing support constraint. Invalid Item Pruning is used in the original transaction database and the conditional transaction database When building a frequent pattern tree, other than LPMiner algorithm based on smin, the support and the length of transactions are taken into account at the same time. This constructs a smaller CFP-tree. From Figure 2, we know that  f e a,g h root f: 0.8 e: 0.6 h: 0.4 a,g: 0.6 e: 0.8 f: 4 e: 3 a: 3 h: 2 g: 3 e: 1 h: 1 f: 4 e: 3 a,g}: 3 h: 2 e: 1 h: 1 root root Total support count \(sup 6 1 Length of transaction Support count   s k  2 1 6  1 5  2 3 1 6  1 5  2 6  1 5  2  6  1 5  2  3  1 6  1 5  1 2 1 6  1 3 1 V1-582 2010 2nd International Conference on Future Computer and Communication Volume 1 


 s5 0.2 b    s3 0.2 b  and  s2 0.2 b  then  sup 5 0.2 b    sup 3 0.4 b  and  sup 2 0.6 b  We can get   sup b kkfk  so {b} is a invalid items 2 Decision Attribute Pruning Thanks to the antecedent-consequent constraint, we do not need calculate the support of itemsets including only the decision attributes. Therefore, we use a pruning strategy for the decision attributes. First, the algorithm scans the transaction database to generate the frequent 1-itemsets and the support counts. Then the frequent 1-itemsets are respectively sorted in descending order of support according to decision attributes and non-decision attributes, and all decision attributes are ranked in front of non-decision attributes Hence, the non-decision attributes are the children of the decision attributes in the constructed CFP-tree. In the process of mining frequent itemsets, we only generate conditional CFP-tree for the decision attributes, while the decision attributes are not considered. This makes that there are not itemsets which contain only the decision attributes in the generated frequent itemsets. This will reduce the number of the conditional pattern tree generated and improve the efficiency of algorithm 3 Unpromising Preìx Itemsets Pruning Since we are only interested in closed itemsets, we choose two existing pruning methods to stop mining patterns with unpromising preìx itemsets as soon as possible. They have been popurlarly used in several closed itemset mining algorithms such as CLOSET The ìrst pruning technique is item merging. Given a conditional database TDB P w.r.t. a particular preìx itemset P. If the itemsets X can be contained in each of the transactions in the TDB P X can be merged with P to form a new preìx, and we do not need to mine closed itemsets containing P but no X The second pruning technique is sub-itemset pruning. Let X be the frequent itemset currently under consideration. If X is a proper subset of an already found frequent closed itemset Y and sup\(X\=sup\(Y\, then X and all of Xês descendants in the TDB X cannot be frequent closed itemsets and thus can be pruned E The Algorithm The ACLCMiner algorithm is shown as follow ACLCMiner TDB, smin, smax, lmin, lmax, m INPUT TDB: a transaction database smin, smax, lmin, lmax are the parameters of   fl and m: the coding boundary for the decision attributes and the non-decision attributes OUTPUT the complete set of closed itemsets that satisfy the length-decreasing support constraint BEGIN 01 ITree NULL; f\(l F\(smin, smax, lmin, lmax 02 ACLCMiner TDB 03 Out\(ITree ALGPRITHM aclcminer P, PTDB INPUT P: a prefix itemset PTDB: the conditional database w.r.t prefix P BEGIN 04 f_list Create_acconstraint_flist\(PTDB, m 05 f_list invalid_item_pruning\(f_list, f\(l\|P 06 S item_merging\(f_list P S; f_list f_list-S 07 if \( P  08 if \(sup\(P f\(|P 09 if\(sub_itemset_pruning\(P, ITree 10 return 11 else 12 insert\(ITree, P 13 end if 14 end if 15 if \(f_list  16 CFPTree Create_cfptree\(f_list, PTDB 17 for all \(x f_list and x m\ do 18  P  P x 19  PTDB  build_cond_database P  CFPTree 20 ACLCMiner P   PTDB   21 end for 22 end if END At first, ACLCMiner uses the input values of smin smax, lmin, lmax to get the function  fl and then calls the subroutine aclcminer\(P, PTDB\. The subroutine first builds f_list according to the decision attributes pruning strategy line 03\. Then it applies the invalid item pruning method and the set of valid items is denoted as f_list \(line 04\. Next aclcminer\(\ uses the item merging technique to identify the set of items that has the same support as P, and denoted as S S will be merged with P and removed from f_list \(line 05 While getting a new itemset, the algorithm will first check if it can satisfy the support constraint, if not, the algorithm do not need to check if it is clos ed and it will not be inserted into the frequent closed itemsets\(lines 08-13\. If an itemset is a closed itemset that satisfies the support constraint, it will be inserted into the RTree \(line 12\. Next the conditional CFP tree is built \(line 16\, and aclcminer\(\ recursively calls itself to mine the closed itemsets with the length-decreasing support constraint by growing prefix P under the bottom-up divide-and-conquer paradigm \(lines 17-25\. Finally according to the RTree, we will output the frequent closed itemsets satisfied requirements. ACLCMiner removed the unpromising prefix pruning strategy in BAMBOO, because it repeats with the invalid item pruning strategy IV E XPERIMENTAL R ESULTS This experiment is carried out on an Intel Pentium PC 2GB memory, 2.4GHz. All procedures were implemented using JAVA language. Data is the real data from the HIS system database of China-Japan Friendship Hospital, and we selected two groups of data , which are the effectiveness of Traditional Chinese Medicine\(TCM\ and the herbal nature in the coronary heart disease database. The characteristics of Volume 1 2010 2nd International Conference on Future Computer and Communication V1-583 


the data table are shown in Table the last column shows the average and maximal transaction length\. Table shows the two parameters of  fl i.e. sim and smax. For the data of TCM effectiveness lm in is 8 and lmax is 20, and for the data of the herbal nature lmin is 3 and lmax is 8. We ran respectively BAMBOO and ACLCMiner on the same environment, and the experimental results are shown in Table The comparison results for the two algorithms are shown in Figure 6 and Figure 7  TABLE II D ATASET C HARACTERISTICS TABLE III E XPERIMENTAL R ESULTS                     0 50 100 150 200 250 300 support Number of itemsets thousand 0 30 60 90 120 150 180 Runtime \(s Figure 6  Comparison between ACLCMiner and BAMBOO TCM effectiveness  0 5 10 15 20 25 30 35 support Number of itemsets thousand 0 3 6 9 12 15 18 Runtime \(s Figure 7  Comparison between ACLCMiner and BAMBOO herbal nature  From Figure 6 and Figure 7 we could see that ACLCMiner generates a small number of frequent closed itemsets, and the algorithm runtime is faster than BAMBOO And as decreasing the value of smin and smax the effect is increasingly apparent. ACLCMinerês higher performance stems from two aspects: on the one hands, it adopts the antecedent-consequent constraint to mine much small number of valid itemsets than BAMBOO, on the other hand, it adopts effective pruning methods and storage structure, which greatly improves the running efficiency From the comparison of Figure 6 and Figure 7, we can see when the decision attributes are more and the number of items contained in the transaction is large, the effect is particularly evident V C ONCLUSIONS In this paper, according to the characteristics of the medical data, a freqnet closed itemsets mining algorithm based on the antecedent-consequent constraint and the length-decreasing support constraint \(ACLCMiner\ is proposed. The proposed algorithm greatly saves memory space and improves the efficiency. Meanwhile, the algorithm generates interesting rules and the number of generated rule is small. The rules mined from ACLCMiner can help to improve the accuracy of clinical syndromes, so it has very important theoretical and practical value. At the same time ACLCMiner can also be extended to other areas A CKNOWLEDGMENT This work was supported by a grant from the National Science Foundation of China \(No. J0724003 60773084 60603023\ and a grant from the Ph.D. Programs Foundation of Ministry of Education of China \(No. 20070151009 R EFERENCES 1 F Bo nch i C  L unn he s e  P us hi ng to ug he r co ns tr ai nts  i n f r e que n t  pattern mining,é PAKDDê05, 2005, pp. 114-124 2 J i a n y on g W a n g  J i an w e i H a n  an d J i an P e i  C L O S E T S e a r c h in g f o r  the Best Strategies for Mining Frequent Closed Itemsets,éSIGKDDê03, Washington, DC, USA,2003 3 G r a hne G   Z hu J   E f f i cie n tl y us ing pr e f ix t r e e s in m i ni ng f r e que nt  itemsets,é Proc. ICDMê03 international workshop on frequent itemset mining implementations \(FIMIê03\, Melbourne, FL, 2003, pp 123Ö132 4 B ur di ck D  C a l i m l im  M G e hr ke J   M A F I A  a m a x i m a l f r e q u e n t  itemset algorithm for transactional databases,é Proc. 2001 international conference on data engineering \(ICDEê01\, Heidelberg Germany, 2001, pp. 443Ö452 5 M S e n o  G  K a ry pis  L P M ine r   A n A l g o r ithm f o r F i nd ing F r e q ue nt  Itemsets Using Length- Decreasing Support Constraint,é ICDMê01 Nov.2001 6 M S e n o  G  K a ry pis   F ind i ng F r e que nt P a tte r n s U s ing L e ng th Decreasing Support Constraints ,é Data Mining and Knowledge Discovery,2005, pp. 197-228 7 U nil Y u n, J o h n J  L e g g e tt W L P Mine r   W e ig hte d F r e que nt P a t t e r n  Mining with Length-decreasing support constraints,é PAKDD`05 2005, pp. 555-567 8 U nil Y u n A n e f f i cie n t m i ni ng o f w e ig hte d f r e que n t  pat t e r n w ith  Length-decreasing support constraints,é Knowledge-Based Systems 2008,21, pp. 741-752 9 J ia ny o n g  W a ng G e o r ge  K a r y pis B A M BO O  A ccel e r ating Cl o s e d  Itemset Mining by Deeply Pushing the Length-Decreasing Support Attributes Transactions Dataset Total Non-decisi on attributes Decision attributes Total A.\(M TCM effectiveness 212 25 187 2547 58 105 herbal nature 59 25 34 2547 15 24 Number of frequent closed itemsets Runtime\(ms Dataset smin smax ACLC Miner BAM BOO ACLC Miner BAM BOO 0.2\(0.45\ 40712 261055 17698 157604 0.2\(0.5\ 10755 98575 14875 127802 0.25\(0.5\ 9294 87747 7880 43609 TCM effective ness 0.25\(0.55\ 2479 32518 7302 38458 0.02\(0.05\ 20757 30453 8385 15932 0.03\(0.06\ 18632 24937 4906 6532 0.04\(0.08\ 15511 21312 3172 4031 herbal nature 0.05\(0.1\ 12842 18344 2406 2828 V1-584 2010 2nd International Conference on Future Computer and Communication Volume 1 


Constraint,é In: Proceedings of the Fourth SIAM International Conference on Data Mining, Lake Buena Vista, Florida, USA, 2004 10 G e nl in J i Y i ng w e n Z hu  M i n ing cl o s e d a n d m a x i m a l f r e que n t  embedded subtrees using length-decreasing support constraint,é In Proceedings of the Seventh International Conference on Machine Learning and Cybernetics, Kunming, 2008, pp. 268-273 11 C.O r do ne z  A s s o ciatio n R u l e D i s c o v e ry  W ith t h e T r ain an d T e s t  Approach for Heart Disease Prediction,é IEEE Transactions on Information Technology in Biomedicine, 2006,10\(2\, pp. 334-343  Volume 1 2010 2nd International Conference on Future Computer and Communication V1-585 


preMinsup Fig 3 Experimental results runtimes All experiments were run in a time-sharing environment in an 800 MHz machine The reported 336gures are based on the average of multiple runs Runtime includes CPU and I/Os it includes the time for both tree construction and frequent itemset mining steps We evaluated different aspects of the proposed algorithms which were implemented in C First we compared the performance of the three proposed algorithms using four different constraints one from each type of the above constraints Experimental results showed that the runtimes for both UF-streaming 100   150   200   250   10   20   30   40   50   60   70   80   90  Selectivity \(i.e., percentage of items selected CUF-streaming \(w=5 batches, each with 1M transactions Type IV constraint C4  Type II constraint C2                      Type III constraint C3                      Type I constraint C1                                 100   150   200   250   300   350   400   450   10   20   30   40   50   60   70   80   90  Selectivity \(i.e., percentage of items selected CUF-streaming \(w=50 batches, each with 1M transactions Type IV constraint C4  Type II constraint C2                      Type III constraint C3                      Type I constraint C1                                 50   55   60   65   70   75   80   85   90   0.002   0.003   0.004   0.005  preMinsup \(in percentage Runtime vs. existential probability & preMinsup Items take on an average number of existential probability values                      005 005    t t 327 327 005 items All 322extensions\323 of valid items were valid Due to the item ordering the algorithm stopped checking constraints whenever it detected the 336rst invalid items However for on the mining results For example using 0.8 C C C C C C C C C C w w C C w 0.9 preMinsup  90 of the mined constrained 322frequent\323 itemsets were truly frequent When and UF-streaming Asitexplored properties of these four constraints and pushed the constraints inside the mining process CUF-streaming required shorter runtimes than the other two algorithms As shown in Fig 3\(a the runtimes for handling all four types of constraints increased when the selectivity increased Among them a Runtime vs selectivity  a Type I constraint incurred the lowest runtime among the four types of constraints because CUF-streaming formed fewer 322extensions\323 as they consisted of only valid items Again due to the item ordering the algorithm stopped checking constraints whenever it detected the 336rst valid items Next we repeated the above experiment with a different the window size was low say 10 only a few small UF-trees were constructed and mined as the algorithm only 322extended\323 valid items and a shorter runtime 50 c Runtime vs  and the convertible monotonicity of  the monotonicity of  the convertible anti-monotonicity of 110 sec cf 160 sec in Fig 3\(a was required As another example for 5 batches when  the algorithm applied constraint checking on projected DBs for valid items as well as their 322extensions\323 because not all 322extensions\323 of valid items were valid  the algorithm 322extended\323  many bigger UF-trees were constructed and mined as the algorithm formed projected DBs for both valid as well as invalid domain items which took or having more batches in the sliding window had the bene\336ts of increasing the chance of not pruning relevant expected support information for truly frequent itemsets Moreover as shown in Fig 3\(c when increased fewer itemsets had expected support performed constraint checking as an intermediate step prior to storing the 322frequent\323 itemsets into the UF-stream structure In contrast CUF-streaming was more interesting as it runtimes depended on the type of constraints as well as the constraint selectivity Speci\336cally the algorithm explored the anti-monotonicity of a Type II constraint and a Type III constraint incurred the next two highest runtimes For  and thus shorter runtimes were required The 336gure also showed the effect of the distribution of item existential probability When items took on a few unique existential probability values the UF-tree b ecame smaller Thus times for both UF-tree construction and mining became shorter In addition we also measured the number of nodes in each UF-tree The experimental results showed that the total number of nodes in a UF-tree was no more than the total number of items with their existential probability in all transactions in the current batch of uncertain data stream Furthermore we measured the number of nodes in the UF-stream structure as well As UF-streaming   126 400 sec cf 230 sec in Fig 3\(a As all three algorithms are approximate algorithms we evaluated the effect of  95 of the mined constrained 322frequent\323 itemsets were truly frequent However lowering 5 b Runtime vs selectivity  were constant regardless of the constraint selectivity because these two algorithms did not explore property nor did they push the constraints inside the mining process Speci\336cally UF-streaming only valid preMinsup minsup preMinsup minsup preMinsup preMinsup preMinsup performed constraint checking as a postprocessing step whereas UF-streaming a Type IV constraint incurred the highest runtime because CUF-streaming 322extended\323 i.e formed projected DBs for both valid and invalid items performed constraint checking at a post-processing step the size of UF-stream was observed to be independent of the constraint selectivity In contrast as Items take on many different existential probability values  50 336xed-sized batches with each batch containing 0.1M transactions instead of using 5 336xed-sized batches with each batch containing 1M transactions With this setting each batch was smaller 0.1M vs 1M transactions Thus each batch required lower runtime e.g for constructing and mining UF-trees However the number of batches was higher 50 vs 5 batches than the previous setting This explains why the runtimes see Fig 3\(b took on a broader range than the previous experimental results For example when the selectivity of 1 2 3 4 4 2 3 2 3 1 2 4 w w 0   Runtime \(in seconds 0   Runtime \(in seconds 0   Runtime \(in seconds Items take on a few unique existential probability values                     50   50   0.001   


 ch 6 AAAI/MIT Press 2004  G  G r ahne L  V  S  L aks h m a nan and X  W ang 322E f 336 ci ent m i n i n g o f constrained correlated sets,\323 in ACM TKDD  Proc KDD 2009 Proc VLDB 1994 Proc KDD 2009 Proc IEEE ICDE 2008 Proc PAKDD 2007 Proc VLDB 2008 Proc IEEE ICDE 2000 Proc VLDB 2008 Proc IEEE ICDE 2009 Proc IEEE ICDM 2006 Proc IEEE ICDE 2002 Proc IEEE ICDE 2001 Proc IEEE ICDE 2008 preMinsup minsup Proc U  09 Proc PAKDD 2008 Proc ACM SIGMOD 2008 Data Mining and Knowledge Discovery Proc ACM SIGMOD 1993 Proc SSTD 2005 Proc ACM SIGMOD 2000 Proc ACM SIGMOD 2009 Proc ACM SIGMOD 1998 Proc ACM SIGMOD 2008 2 pp 18\32026 June 2005 12 C G ia n n e lla e t a l 322 M in in g f r e q u e n t p a tte r n s in d a ta s tr e a m s a t m u ltip le time granularities,\323 in 4 pp 337\320389 Dec 2003  C  K  S  L eung 322F r e quent i t e m s et m i ni ng w i t h cons t r ai nt s  323 i n 127 1 batch containing the entire dataset Then we compared our algorithms with UF-growth 22 b y as s i g n i n g t o each i t em i n e v er y t r an s act i o n in a dataset an existential probability of 1 i.e all items are de\336nitely present in the dataset and 005 005 Encyclopedia of Database Systems queries on uncertain streams,\323 in 34 28 34  pp 29\32037 2 R  A gr aw al et al   322M i n i n g a s s o ci at i o n r ul es bet w een s e t s of i t e ms i n large databases,\323 in  pp 207\320216 3 R  A gr aw al and R  S r i kant  322 F a s t al gor i t h ms f o r m i n i n g a s s o ci at i o n rules,\323 in  pp 487\320499 4 R  J  B ayar do J r   R  A g r a w a l  and D  G unopul os  322 C ons t r ai nt b as ed rule mining in large dense databases,\323 2\3203 pp 217\320240 July 2000 5 T  B e r n e c k e r e t a l 322 P r o b a b ilis tic f r e q u e n t ite m s e t m in in g in u n c e r ta in databases,\323 in  pp 119\320127 6 R  C h e n g e t a l 322 P r o b a b ilis tic v e r i\336 e r s  e v a lu a tin g c o n s tr a in e d n e a r e s tneighbor queries over uncertain data,\323 in  pp 47\32058 8 G  C or m ode and M  H adj i e l e f t her i ou 322F i ndi ng f r e quent i t e m s i n dat a streams,\323 in  pp 1530\3201541 9 G  C o r m o d e e t a l 322 F in d in g h ie r a r c h ic a l h e a v y h itte r s in s tr e a m in g d a ta  323  pp 400\320417  M M G a ber  A  B  Z a s l a v s k y  and S  K r i s hnas w am y  322Mi n i n g d at a streams a review,\323  pp 512\320521  J  H a n J  P e i  and Y  Y i n  322 Mi ni ng f r e quent pat t e r n s w i t hout candi dat e generation,\323 in  pp 1\32012 15 J  H u a n g e t a l 322 M a y B M S  a p r o b a b ilis tic d a ta b a s e m a n a g e m e n t s y s tem,\323 in  pp 1071\3201074  C  J i n e t a l   322 S l i di ngw i ndo w t op pp 301\320312  L  V  S  L a ks hm anan C  K  S  L e ung and R  T  N g 322E f 336 ci ent dynam i c mining of constrained frequent sets,\323  pp 9\320 18  C  K  S  L eung and B  H ao 322 Mi ni ng of f r e quent i t e m s et s f r o m s t r eam s of uncertain data,\323 in  pp 1663\3201670  C  K  S  L eung and Q  I  K han 322D S T r ee a t r e e s t r uct u r e f o r t he m i ni ng of frequent sets from data streams,\323 in  pp 928\320 933  C  K  S  L eung M A  F  Mat e o and D  A  B r a j czuk 322 A t r eebas e d approach for frequent pattern mining from uncertain data,\323 in  pp 13\320 24  J  P e i  J  H a n and L  V  S  L aks h m a nan 322Mi n i n g f r e quent i t e m s et s w i t h convertible constraints,\323 in  pp 433\320442  C  R 253 e et al 322Event queries on correlated probabilistic streams,\323 in  pp 715\320728 27 A  D  S a r m a  M  Th e o b a ld  a n d J  W id o m  322 Ex p lo itin g lin e a g e f o r con\336dence computation in uncertain and probabilistic databases,\323 in  pp 1023\3201032  K  Y i et al   322S m a l l s ynops es f o r g r oupby quer y ver i 336 cat i o n o n outsourced data streams,\323  pp 819\320832 322frequent\323 itemsets from uncertain data streams In terms of ef\336ciency the experimental results showed that UF-streaming was slightly faster because it did not perform any constraint checking whereas our three proposed algorithms performed the extra constraint checking step Among them CUF-streaming only performed constraint checking on some 322frequent\323 itemsets and the other two performed constraint checking on all 322frequent\323 itemsets However in terms of the mining results we observed that all four algorithms returned the same collection of 322frequent\323 itemsets This illustrated that our proposed algorithms could be used for mining unconstrained frequent itemsets from uncertain data streams Moreover it is important to note that while the UF-streaming is con\336ned to 336nding 322frequent\323 itemsets satisfying constraints with 100 selectivity our algorithms are capable of 336nding 322frequent\323 itemsets that satisfy constraints having lower selectivity Along this direction we set and CUF-streaming both pushed the constraint early the corresponding size of UF-stream was proportional to the selectivity of constraints Finally we evaluated the functionality and applicability of our proposed algorithms We again used four different constraints and we also set the constraint selectivity be 100 i.e all items are selected Then we compared our three proposed algorithms with UF-streaming  w hi c h w a s designed to mine 1 UF-streaming 4 article 2 Jan 2008 10 X  D a i e t a l 322 P r o b a b ilis tic s p a tia l q u e r ie s o n e x is te n tia lly u n c e r ta in data,\323 in 3 article 15 Aug 2009 29 Q  Zh a n g  F  Li a n d K  Y i 322 F in d in g f r e q u e n t ite m s in p r o b a b ilis tic data,\323 in  pp 1179\3201183 Springer 2009  C  K  S  L eung and D  A  B r a j czuk 322E f 336 ci ent a l gor i t h m s f o r m i n i n g constrained frequent patterns from uncertain data,\323 in Data Mining Next Generation Challenges and Future Directions w  UF-streaming unconstrained and CUF-streaming\321 which integrate i mining of uncertain data ii constrained mining and iii mining of data streams These algorithms effectively mine constrained frequent itemsets from uncertain data streams A CKNOWLEDGMENT This project is partially sponsored by Natural Sciences and Engineering Research Council of Canada NSERC and the University of Manitoba in the form of research grants R EFERENCES 1 C  C  A ggar w al et al   322F r e quent pat t e r n m i ni ng w i t h uncer t a i n dat a  323 i n  pp 653\320661  C  K  S  L eung R  T  N g  a nd H  Manni l a  322 O S S M  a s e gm ent a t i o n approach to optimize frequency counting,\323 in SIGMOD Record ACM TODS ACM TODS k 4   Again we observed that all four algorithms returned the same collection of frequent itemsets This illustrated that our proposed algorithms could also be used for mining unconstrained frequent itemsets from static uncertain datasets VII C ONCLUSIONS Frequent itemsets generally serve as building blocks for various patterns in many real-life applications Most of the existing algorithms 336nd unconstrained frequent itemsets from traditional static transaction databases consisting of precise data However there are situations in which ones are uncertain about the contents of transactions There are also situations in which users are only interested in some subsets of all the mined frequent itemsets Furthermore a 337ood of data can be easily produced in many situations To deal with all these situations we proposed three tree-based algorithms\321 namely UF-streaming   pp 973\320982 7 C  K  C hui  B  K ao a nd E  H ung 322Mi n i n g f r e quent i t e m s et s f r o m uncertain data,\323 in     pp 583\320592  R  T  N g et al   322E xpl or at or y m i n i n g a nd pr uni ng opt i m i zat i ons of constrained associations rules,\323 in  


              


   


                        





