Exploring Sketches for Probability Estimation with Sublinear Memory Anthony Kleerekoper Mikel Luj  an and Gavin Brown School of Computer Science The University of Manchester UK Email  kleereka mikel.lujan gbrown  cs.man.ac.uk Abstract As data sets become ever larger it becomes increasingly complex to apply traditional machine learning techniques to them Feature selection can greatly reduce the computational requirements of machine learning but it too can be memory intensive In this paper we explore the use of succinct data structures called sketches for probability estimation as a component of information theoretic feature selection These data structures are sublinear in the number of items but were designed only for estimating the frequency of the most frequent items To the best of our knowledge this is the rst time they have been examined for estimating the frequency of all items and we nd that often some information theoretic measures can be estimated to within a few percent of the correct values Keywords  big data information theoretic feature selection machine learning memory ef“ciency sketch data structures I I NTRODUCTION Information is being gathered and stored in larger and larger amounts making it harder to apply traditional machine learning algorithms to the new very large data sets Information theoretic feature selection can greatly reduce the complexity and memory requirements of machine learning by identifying only those items which are most signi“cant 1 2 In order to apply information theoretic feature selection however the probability of the items in the data set must be known In the traditional approach a counter is maintained for each item but this requires very large amounts of memory The problem is exacerbated for more sophisticated methods which require the joint probabilities of two or three items which raises the memory cost even further In this paper we consider the use of sketches for estimating the underlying probabilities of large data sets A sketch is a compact data structure capable of approximately summarising the frequencies of data In particular we explore the two most widely discussed sketches Count Sketch and CountMin Sk etch  These sk etches were designed for identifying the heavy-hitters the most frequent items within a data set but our aim is to examine their appropriateness for estimating the probabilities of all items The rest of this paper is organised as follows In Section II we describe the related work and discuss how the two sketches we are using work In Section III we describe why using sketches for probability estimation is not a straightforward choice In Section IV we describe the experiments we conducted and present the results in Section V Finally we discuss our results in Section VI and draw some conclusions in Section VII II R ELATED W ORK Sketches are designed as a form of synoptic data structure The idea is that the sketch can provide a view of the data in a succinct manner thereby saving signi“cant memory and improving processing times Since sketches are only synopses the responses they provide are only approximate and much of the work in this eld is involved in the analysis of the guarantees that can be provided for the accuracy of the response Another consequence of sketches being only synopses is that they are designed to answer only a small subset of possible queries and cannot be used to answer any given query on the data Sketches are normally accompanied with analysis proving bounds on their error and the probability of that error bound being exceeded This is usually in the form of     pairs where  is the error bound and  is the probability that the error will be exceeded The desired values of  and  determine the size of the sketch The rst sketch was proposed in the foundational paper by Alon Mattias and Szegedy It w a s this work that created the interest in sketch data structures and started research in this area The original paper proposed a sketch since referred to as the AMS sketch designed to estimate the F 2 norm The basic idea was to create a random variable whose expected value was equal to the desired quantity the F 2 norm in this case and by considering a number of independent variables a good approximation can be found This concept is the basis for all sketches and was utilised in the Count Sketch CS structure which was designed to approximately nd the most frequent items in a data set The sk etch is constructed from a set of counters with each counter tracking the count of a number of items The counters are arranged into rows and each item is mapped to one counter per row 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


so that whenever an items is seen one counter per row is updated Two hash functions are needed per row the rst to spread the items evenly across the counters and a second to determine whether an item increments or decrements the counter The rst hash function h j  i  assigns item i in row j to one of the counter 1 w  The second hash function g j  i  assigns item i in row j to  1   1   indicating whether an items increments or decrements the counter The value of a given counter k in row j is C  j k  N  i 0 g j  i  f  i   h j  i  k 1 where f  i  is the true frequency of item i and N is the number of unique items When recovering the approximate frequency the appropriate counter in each row is found and its value is multiplied by g j  i   The median of all counters is returned as the estimated frequency  f  i   In some cases  f  i  can be negative in which case the estimated frequency is 1 It is provable that E   f  i   f  i   i  Intuitively it is convenient to think of each counter as containing the true count of the item of interest together with some noise By sometimes incrementing and sometimes decrementing the counter and by spreading the items evenly across the counters the expected value of the noise is zero because frequencies of items cancel each other Of course the estimate has some variance and to provide     guarantees it has been proven that CS must have O 1  2  counters per row and O log\(1   rows A C S s k etch of this size guarantees that with probability 1   the estimate will be no more than    F 2 larger or smaller than its true value It should be noted that its error guarantees are not in terms of the true frequency but in terms of the L2 norm the sum of squares of all items Cormode and Muthukrishnan adapted the CS sketch to greatly reduce the size of the sketch at the cost in theory at least of accuracy Their sk etch called the Count Min Sketch CM uses only one hash per row and always increments counters as illustrated in Fig 1 The result is that the counters are biased estimators and do not have an expected value equal to the true frequency In fact the value of each counter is always equal to or greater than the true frequency As its name suggests CM provides frequency estimates by nding the smallest counter from all the rows which is guaranteed to be the one with the smallest overcount Analysis shows that CM can provide an     guarantee with only O 1   counters per row and O log\(1   rows The error  h o w e v er  i s i n terms of the L1 norm the sum of all items which is usually greater than the root of the L2 norm CM uses less space but provides a weaker guarantee of accuracy than CS Fig 1 The Count Min sketch combines the counts of multiple items and returns the counter containing the smallest overcount Image taken from In practice the sketches usually perform better than the mathematical analysis suggests see for a thor ough empirical examination of sketch performance There has also been signi“cant research into other forms of sketches and Cormode has provided an excellent survey recently III S KETCHES FOR P ROBABILITY E STIMATION The CS and CM sketches were designed to identify the so-called heavy-hitters the most frequent items in a data set or stream Although they can provide estimations for the frequency of all items and indeed these sketches have been identi“ed as solving the frequency estimation problem it seems to ha v e been implicitly understood that the estimations would be more accurate for the most frequent items This is intuitively understood using the noise concept For both sketches each counter can be thought of as containing the true count of each item with some added noise The most frequent items have by de“nition the highest true counts and therefore are least affected by the noise from the other items So long as frequent items can be kept to separate counters then the estimates of their frequency will probably be good ones By spreading the items evenly across the counters in each row and using a different spread in each row the probability of two frequent items always colliding to the same counter is greatly reduced leading to good estimates for these items In this paper however we consider the appropriateness of the sketches for estimating the underlying probability densities of all the data items There are many applications where estimating the probability of an event or item is important both for its own sake in data processing and as a component in other calculations Our work is primarily motivated by the use of probability estimation in information theoretic feature selection 2 in which the probability of every item and frequently of pairs or even triples of items is needed The traditional method for calculating the probability of different items is to accurately count the frequency of each item and divide by the total frequency In very large data sets the number of counters needed can be very large and this is made worse when the joint probability of two or three items are needed as is required for some feature selection algorithms 80 


Sketches that can provide estimates of the frequency of any item with fewer than one counter per item are attractive for this problem It is not clear however how accurate they can be at estimating the frequency of lower frequency items and what impact errors of low frequency items have on information theoretic measures The aim of this paper is to examine the performance of sketches in estimating the probability densities of large data sets and attempt some evaluation of their appropriateness for this task To the best of our knowledge this is the rst examination of sketches for this purpose IV E XPERIMENTAL S ETUP In order to investigate the ability of sketches to estimate probability densities we created synthetic data sets of varying sizes with known distributions We considered uniform Gaussian and Zip“an distributions with various standard deviations and skews in the cases of Gaussian and Zip“an distributions respectively For each distribution type we generated data sets of between 100 and 500,000 items Our aim is to consider the use of sketches to estimate probabilities using less memory space Therefore we de“ned the size of the sketches not in terms of  and  but in relative terms to the traditional method of probability estimation That is we consider sketches that use approximately 100 75 and 50 as many counters as there are items To do this we calculate the required number of counters in the sketches  C   and then nd the value of  and  needed to create sketches with that number of counters We assume that     x to provide a single unknown variable and then we can construct an equation for each sketch of the sketch size in terms of x  These are given by equations 2 and 3 for CS and CM respectively  C  log 2  1  x  3 x 2  2  C  log 2  1 x  2 x  3 Equations 2 and 3 can be rearranged to nd x  the value of  and  used in the sketches Equations 4 and 5 give the results for CS and CM where W    is the Lambert W-function This result is only approximate because the number of counters per row and the number of rows must be integer values and so x is too low Therefore the calculated values of x are used as a starting point and is slowly incremented in 1 steps until the difference between the resultant sketch size and the required size is minimised x  exp   0  5 W  2  C  3  4 x  exp   W   C  2  5 To provide some context to the estimates from the sketches we also considered a naive sampling estimator which uses half as many counters as the traditional method This estimator which we call half-count HC records the accurate count of every second item only When the count of an item is sought it returns the accurate count if it has recorded it and the count of the preceding item if it has not it uses the next item if there is no preceding item For evaluating the estimates we compare each method CS CM and HC to the traditional method which accurately records the count of every item We utilise two measures for the difference in the probability estimations the mean square error MSE equation 6 and the mean relative error MRE equation 7 MSE  1 N N  i 0  f  i    f  i   2 6 MRE  1 N N  i 0    f  i    f  i     f  i  7 We would also like to consider how these differences combine and so we use the probability estimates to calculate the Shannon entropy equation 8 which relies on the probability of all items and nd the difference between the estimated entropy and the true entropy as calculated using the traditional method H  I   N  i 0 p  i  log p  i  8 We also examined the error in entropy estimation when the memory space was reduced by one and two orders of magnitude\(relative size of 10 and 1 For these sizes we do not report MSE or MRE since the errors become either extremely small for MSE or extremely large for MRE For these results we also tune HC to the same relative size by counting every tenth or hundredth item only V R ESULTS A Uniform Fig 2\(a shows the mean square error MSE for the sketches and HC in a uniform distribution Since the distribution was perfectly uniform the HC method resulted in no errors at all because all items had the same frequency The MSE for the sketches are also small and fall as the number of items increases This is to be expected because MSE is an absolute measure and the more items there are the smaller the probability of all items It is clear also that CM outperforms CS With a relative size of 50 the MSE of CM is between 97.43 and 99.04 lower than CS and the difference increases linearly with 81 


the logarithm of the number of items  r 0  802  p 0  017  The results for mean relative error MRE Fig 2\(b again show that HC has no error and that CM outperforms CS MRE is statistically independent of the number of items for CS  r  0  099  but falls with the number of items for CM  r   0  82  p 0  013 when the relative size is 50 This results in a slightly increasing gap between the MRE of CM and CS  r 0  813  p 0  014  with CM having an error between 86.20 and 93.34 lower than CS With a relative size of 50 the average MRE for CS is 91.0 while for CM it is still signi“cant at 8.21 The results for entropy Fig 2\(c give an idea of the way the errors in individual frequencies combine The results show that despite relatively large MRE when all the items are combined the errors are much smaller Once again HC has no errors and CM outperforms CS The error in entropy estimation also falls with an increasing number of items because the individual probabilities become smaller and smaller meaning that errors in individual frequency estimates have an increasingly smaller impact on the entropy estimation The average error for CS with a relative size of 50 is 7.92 and for CM it is only 0.1 B Gaussian For the Gaussian distribution there is a second factor aside from the impact of the sketch size namely the standard deviation Initially we x the standard deviation at  2 and vary the size and then x the size at 50 and vary the standard deviation Fig 3\(a shows that MSE falls as the number of items increases for the same reason as with the uniform distribution Here though HC does not have zero error though it does have a lower error than CM or CS CM again outperforms CS When the relative size is 50 CM outperforms CS by an average of 98.73 and the difference between CM and HC is on average 97.0 In both cases there is no statistically signi“cant relationship between the differences and the number of items  p  0  067  A similar pattern emerges with MRE Fig 3\(b HC again has the lowest error averaging just 1.14 and CM outperforms CS The error for CM falls as the number of items increases  r   0  786  p 0  021 for a relative size of 50 but for CS there is no statistically signi“cant dependence  p  0  059  The difference between CM and CS therefore increases as the number of items increases  r 0  854  p  0  007 when relative size is 50 while the difference between CM and HC falls  r   0  806  p 0  016 for the same size although the smallest MRE under CM with relative size of 50 is still a signi“cant 6.81 As with the uniform distribution the error in entropy estimation is smaller than MRE for all methods as                 a MSE                 b MRE                 c Entropy Fig 2 Performance of CM CS and HC for a synthetic perfect uniform distribution relative size shown in brackets shown in Fig 3\(c and falls with the number of items In fact for HC the error in entropy estimation is correct up to at least 5 decimal places For CM the average error is just 0.1 for relative size of 50 whereas it is 8.04 for CS for the same size Fixing the size at 50 and varying the standard deviation shows Fig 4\(a that the standard deviation has little impact on MSE for the sketches but is signi“cant for HC As before MSE falls with more items because it is an absolute measure but for HC the value is also signi“cantly lower when the standard deviation is smaller In the worst case for HC when  3  HC still outperforms CM by an average of 93.72 while CM outperforms CS by 98.6 The gap between CM and HC is questionably dependent on the number of items  r   0  704  p 0  051  while 82 


                 a MSE                 b MRE                         c Entropy Fig 3 Performance of CM CS and HC for a synthetic Gaussian distribution with  2 relative size shown in brackets the gap between CM and CS statistically signi“cantly increases with the number of items  r 0  716  p 0  046  The results for MRE Fig 4\(b are similar to MSE HC has the smallest error and this is smallest when  is smallest In the worst case   3  MRE for HC averages just 1.72 and this falls very slowly with more items For CS in the best case the average MRE is 90.72 while for CM it is 8.19 For CS there is no statistically signi“cant dependence on the number of items  p  0  253  while for CM there is a slight fall as the number of items increases  r  0  786  p  0  023  When it comes to entropy estimation Fig 4\(c the results follow the same pattern as seen before The errors are much smaller than MRE and fall as the                       a MSE                       b MRE                       c Entropy Fig 4 Performance of CM CS and HC for a synthetic Gaussian distribution with xed relative size of 50   is shown in brackets number of items increase The estimate using HC is correct up to at least four decimal places while for CM the average error is 0.1 for all values of  For CS the error is between 7.95 and 8.34 but further experiments with more values of  are required to determine whether there is a relationship between the error and standard deviation C Zip“an The results for the Zip“an distribution are in many ways very different to the previous results Fig 5\(a shows the results for MSE which like with the other distributions always decreases as the number of items increases With the Zip“an distribution however HC has the highest error and CS the lowest Furthermore the difference between the errors with different size 83 


sketches is far more pronounced MSE is on average 48.9 higher with HC than CM with relative size of 50 and the difference between CM and CS is on average 69.26 for the same size In both cases any relationship between the difference and the number of items is doubtful  r  0  5  For MRE CS again has the lowest error but this time HC is similar to CM with a relative size of 100 and outperforms it when the size is 50 The errors are extremely high when compared to the Gaussian distribution with the smallest error being 78.55 and the largest 650.23 MRE is invariant with the number of items for CS  p  0  16  but is larger for smaller sizes For CM the error increases with the number of items  r  0  968  p  2  98 e  5  the same relationship appears for HC  r 0  913  p 0  0016  When it comes to the entropy estimation however while CS continues to outperform CM HC has comparable error to CS In fact HC averages a 28.5 smaller error than the best case of CS Unlike with the other distributions the error in entropy estimation actually increases with the number of items for CM but for CS the relationship is doubtful with statistical signi“cant for one size 75 but not the others With a xed relative size of 50 and varying skew the picture is slightly complex For MSE Fig.6\(a the skew has a large impact on the error with larger skews producing much smaller errors In all cases HC performs similarly to CM with CS having the lowest error For MRE Fig 6\(b the dependence on skew is still very evident but the order of performance changes With low skew CS has the smallest error followed by HC then CM but as the skew increases CS performs increasingly badly relative to CM and HC When the skew is 2.5 CM and CS have similar performance and HC is the best performing When the skew increases to 3 CS now has the largest error followed by CM and then HC The story for the entropy estimation Fig 6\(c is different again It remains the case that as the skew increases the errors decrease but this time HC always has the smallest error followed by CS and then CM When the skew is 2 the errors for the sketches increase with the number of items  p  0  0038  but when the skew is 3 the errors fall as there are more items  p 0  0008  for CM but there is no statistical signi“cance to the relationship for CS  p 0  659  At low skew the average error for CM is 42.92 11.81 for CS and 6.07 for HC When the skew is high the averages become 3.25 0.27 and 0.23 for CM CS and HC respectively D Orders of Magnitude The results for the uniform distribution Fig 7\(a show that CS has the largest errors followed by CM with HC having no errors at all The errors for                  a MSE                 b MRE                 c Entropy Fig 5 Performance of CM CS and HC for a synthetic Zip“an distribution with skew=2 relative size shown in brackets CM are virtually zero for all magnitudes with the average across all data set sizes being just 0.22 with a relative size of 1 In comparison the average error for CS is 10.15 for the same size and is a signi“cant 5.88 when no memory savings are being made A similar pattern is seen for the Gaussian distribution in Fig 7\(b Again HC has no error and the error for CM is an average of 0.22 even with just 1 relative size CS has slightly higher errors with the Gaussian distribution compared to a uniform one at 10.87 for 1 relative size and 6.12 for 100 relative size Things change dramatically in the Zip“an distribution shown in Fig 7\(c Here CM has the largest error averaging between 7.94 with 100 size and 8.90 84 


                       a MSE                       b MRE                       c Entropy Fig 6 Performance of CM CS and HC for a synthetic Zip“an distribution with xed relative size of 50 skew shown in brackets with 1 size HC and CS have lower errors with CS outperforming HC only at smaller sizes For 100 size CS has an average error of 1.44 and HC has 0.27 but the averages rise to 3.39 and 4.47 at 1 relative size VI D ISCUSSION The results presented in the previous section provide the rst examination of the performance of sketches for probability estimation The results show that there is a signi“cant performance difference between uniform and Gaussian distributions on the one hand and Zip“an distributions on the other In a perfect uniform distribution CM performs extremely well with virtually no errors whereas CS has signi“cant errors The frequencies estimated by CM                       a Uniform                       b Gaussian                       c Zip“an Fig 7 Performance of CM CS and HC for different synthetic data sets with relative sizes decreasing by two orders of magnitude relative size shown in brackets are the sum of a number of counters and therefore when all counters have the same frequency CM will perform very well so long as the items are evenly spread among the columns by the hash functions The more items there are the better the spread is likely to be and small uctuations in the number of items hashed into each column have less impact on the total counts Thus CM will overestimate the frequencies but will correctly nd the correct probabilities In contrast for CS the expected value of all counters would be zero but small uctuations caused by an uneven spread has a larger impact It can happen that the frequency estimated by CS is negative in which case there is no information available about the true frequency except the lower-bound that the 85 


item was seen If the true frequency is high then the discrepancy can become very large This cannot happen with CM The results for the Gaussian distribution are very similar to those of the uniform distribution but the errors are a little larger This is because the true frequencies in the Gaussian distribution do not vary by very large amounts Increasing the standard deviation lowers the errors because the frequencies become even more similar With the Zip“an distribution however the frequencies are scale-free That is there are very large differences in the true frequencies with a small number of items appearing a very large number of times and most items appearing only once For these distributions CM has the largest error because the heavy hitters can have a very large impact on a large number of items This is particularly true as the sketches become smaller and it is harder to avoid collisions with the most frequent items On the other hand CS can detect when an item has collided with a heavy hitter because its frequency is estimated as a negative number When this happens the frequency used is just one which is likely to be close to the true count for a non-heavyhitter CS therefore only needs to avoid heavy-hitters colliding with other heavy-hitters too often which is easier to do Although some measures have been used in this paper to examine the performance of sketches for probability estimation they are not objective The best that can be hoped for is to compare one or more estimators and consider their performance relative to each other but there can be no objective method of declaring an estimator to be good enough The question of whether an estimator is good enough depends entirely on what it will be used for We have therefore provided a straightforward estimator HC for comparison which we hope places the performance of the sketches into some context but we remain cautious about drawing strong conclusions from these results VII C ONCLUSION In this paper we have considered the use of sketches for estimating the probability of all items in very large data sets The sketches are memory-ef“cient data structures designed for identifying and estimating the frequency of so-called heavy-hitters They are however in theory capable of estimating the frequency of all items To the best of our knowledge this is the rst attempt to examine their applicability to this problem In order to gain some understanding of the performance of the sketches we considered three metrics mean square error mean relative error and the error in entropy estimation Our results show that for uniform and Gaussian distributions the CM sketch performs very well even with very small sketch sizes whereas CS performs less well estimating the entropy incorrectly by an average of over 10 For Zip“an distributions however it is CS that performs best with increasingly better performance as the data set becomes more skewed The measures we have used are not objective and inherently it is impossible to declare an estimator good enough without reference to a speci“c task Thus the results in this paper are illustrative and suggestive only Additionally we considered only pure distributions which may not be re”ected in real data In particular data may be mixtures of different distributions We do conclude however that the sketches produce estimates with smaller errors than might seem likely from their construction which suggests that for some applications at least sketches need not be restricted only to the problem of nding the most frequent items In the future we would like to examine how the sketches perform with real data sets and whether even small errors as seen in the entropy estimations might affect the ranking of items for feature selection algorithms A CKNOWLEDGMENT Anthony Kleerekoper gratefully acknowledges funding from the EPSRC APT Platform Grant EP/G013500/1 Dr Luj  an is supported by a Royal Society University Research Fellowship R EFERENCES  Ga vin Bro wn A n e w perspecti v e for information theoretic feature selection In International Conference on Arti“cial Intelligence and Statistics  pages 49…56 2009  Thomas M C o v e r and Jo y A Thomas Elements of information theory  John Wiley  Sons 2012  Isabelle Guyon and Andr  e Elisseeff An introduction to variable and feature selection The Journal of Machine Learning Research  3:1157…1182 2003  Moses Charikar  K e vin Chen and Martin F arach-Colton Finding frequent items in data streams In Automata Languages and Programming  pages 693…703 Springer 2002  Graham Cormode and S Muthukrishnan An impro v e d data stream summary the count-min sketch and its applications Journal of Algorithms  55\(1 2005  Noga Alon Y ossi Matias and Mario Sze gedy  The space complexity of approximating the frequency moments In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing  pages 20…29 ACM 1996  Ilya Katso v  Probabilistic data structures for web analytics and data mining 2012  Graham Cormode and Marios Hadjieleftheriou Methods for nding frequent items in data streams The VLDB Journal  19\(1 2010  Graham Cormode Sk etch techniques for approximate query processing Foundations and Trends in Databases NOW publishers  2011 86 


 Lecture 4.3 INTERNATIONAL TEST CONFERENCE                                        9    signals including high-speed digital waveforms. Unlike high-end oscilloscopes, ATE usually does not have CDR capability integrated into the hardware. While a waveform sampler performs under-sampling of the test signal, the captured waveform is influenced by slow jitter. The purpose of this work is to remove slow jitter effects such as SSC in a high-speed digital signal and restore a fine waveform and eye pattern. The test signal is a PRBS bit stream. Slow jitter reduction is realized by post processing whose steps are  PRBS becomes extremely wideband multi-tone  Jitter is PM onto each tone  Original tones can be recovered by Equation \(11 which is constructed by the modulation signal e measured signal \(cos  d its orthogonal signal sin    Each modulated tone is extracted as a group and demodulated by the ODM, recovering the modulation signal   The sin  is generated by 90 degree rotation from the cos  The FFT & IFFT method takes care of this processing  Recovered tones are equalized power-wise to their original modulated state  CWR by the factor of Nx reconstructs a PRBS waveform  A second CWR by the factor of the number of bits eg. 127\onstructs an eye pattern Finally the developped processing procedure succeeded to reconstruct clear refined PRBS waveforms and wide open eye patterns. The key in the processing is that Equation 11\ is derived as a relatively simple combination of acceptable processing components. This paper mainly discussed an example of a 7 Gbps 127-bit PRBS signal containing a jitter of 5kHz. The waveform sampler employed in the experiment has an actual sampling rate of 110 Msps and the ABW of 10 GHz. Reduction of slow jitter and SSC is demonstrated in the examples. The multitone spectrum appearance in under-sampling situation greatly depends on the PRBS bit length and the bit rate of the signal. The sampling condition should be carefully chosen for the aliased multi-tone spectra to fall in the baseband with as even spacing as possible to avoid overlapping of the jitter-modulated spectrum with each other. If the Fs and ABW of sampler are given, the higher the PRBS bit rate and the shorter the PRBS bit length, the better for wider tone spacing 6. References 1  Takashi Ito, Hideo Okawara, and Jinlei Liu, \223RNA Advanced Phase Tracking Method for Digital Waveform Reconstruction,\224 Proceedings IEEE Int Test Conference 2012, Paper18.3 2  223V93000 System Reference Manual,\224 Advantest 2013 3  Matthew Mahoney, \223Chapter 4: Coherence and Coherent Sampling,\224 DSP Based Testing of Analog and Mixed-Signal Circuits IEEE Computer Society Press 1987, pp.45-58 4  Hideo Okawara, \223DSP-Based Testing Fundamentals 47: Coherent Waveform Reconstruction,\224 Advantest\222s Go/semi Newsletter December 2012 5  Hideo Okawara, \223DSP-Based Testing Fundamentals 8: Under-sampling,\224 Verigy\222s Go/semi Newsletter  December 2008 6  Hideo Okawara, \223Elegant Construction of SSC Implemented Signal by AWG and Organized Undersampling of Wideband Signal,\224 Proceedings IEEE Int Test Conf 2011, Paper 11.2    


  


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even goodŽ partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity … the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the clouds elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPs synchronous barrier between supersteps offers a window for dynamic scaleout and …in at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an oracleŽ approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workers time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440…442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


