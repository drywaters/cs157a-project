Scalable Sentiment Classiìcation for Big Data Analysis Using Na  ve Bayes Classiìer Bingwei Liu   Erik Blasch   Yu Chen   Dan Shen  and Genshe Chen   Intelligent Fusion Technology Inc Germantown Maryland USA Email  bingwei.liu dshen gchen  intfusiontech.com  Air Force Research Laboratory Rome New York USA Email erik.blasch.1@us.af.mil  Binghamton University Binghamton New York,USA Email ychen@binghamton.edu Abstract A typical method to obtain valuable information is to extract the sentiment or opinion from a message Machine learning technologies are widely used in sentiment classiìcation because of their ability to learn from the training dataset to predict or support decision making with relatively high accuracy However when the dataset is large some algorithms might not scale up well In this paper we aim to evaluate the scalability of Na  ve Bayes classiìer NBC in large datasets Instead of using a standard library e.g Mahout we implemented NBC to achieve ne-grain control of the analysis procedure A Big Data analyzing system is also design for this study The result is encouraging in that the accuracy of NBC is improved and approaches 82 when the dataset size increases We have demonstrated that NBC is able to scale up to analyze the sentiment of millions movie reviews with increasing throughput Keywords  Cloud computing Big data Polarity mining sentiment classiìcation I I NTRODUCTION Data on the web has been explosively increasing in the past few decades The ability to automatically mine useful information from massive data has been a common concern for organizations who own large datasets The MapReduce framework is commonly used to analyze e xtremely lar ge datasets such as tweets collections online documents or largescale graphs 3 4 The frame w o rk pro vides a simple and powerful interface for programmers to solve large-scale problems using a cluster of commodity computers A typical method to obtain valuable information is to extract the sentiment or opinion from a message Sentiment classiìcation is useful for the business consumer industry or online recommendation systems For example online product reviews are usually analyzed by manufacturers to decide what products they will produce in the future to reduce risk Machine learning technologies such as Na  ve Bayes and support vector machine are widely used in sentiment classiìcation  7 8 9 10 because of their ability to learn from the training dataset to make decisions with on-line data to predict salient features and to provide real-time analysis with relatively high accuracy Na  ve Bayes NB classiìers are widely used in information fusion Current trends in data fusion include machine analytics for big data use of NB for cloud computing applications of simultaneous target tracking and classiìcation  and robotics control 13 T echniques for big data analysis are needed imaging text and cyber analysis which includes scalable and elastic learning methods When the datasets are large some information fusion algorithms might not scale up well For example if an algorithm needs to load data into memory constantly the program may run out of memory for large datasets One promising approach is to utilize and adapt MapReduce for some machine learning technologies to resolve these large-scale problems Apache Mahout 1 is a machine learning library for clustering classiìcation and ltering implemented on top of Hadoop 2  the open source version of MapReduce Although there are some machine learning algorithms implemented in Mahout it is still helpful to study how to convert a machine learning algorithm to a Hadoop program and to optimize the algorithm scalability in large datasets In this paper we aim to evaluate the scalability of Na  ve Bayes classiìer NBC in large-scale datasets Instead of using Mahout library we implemented NBC to achieve ne-grain control of the analysis procedure for a Hadoop implementation The result is encouraging in that the accuracy of NBC is improved and approaches 82 when the dataset size increases We have demonstrated that NBC is able to scale up to analyze the sentiment of millions movie reviews with increasing throughput The rest of this paper is organized as follows Section II introduces the background of this study Section III illustrates the system we design to analyze big datasets The system is built on top of Hadoop basic components The details of implementing Na  ve Bayes classiìer are addressed in section IV Section V shows the experiment setup and results Finally we conclude in Section VI II B ACKGROUND A MapReduce The MapReduce framework has been used to process large datasets since the original paper w a s published Google s clusters process more than 20 Petabytes of data every day by running one hundred thousand MapReduce jobs on average  Using this frame w ork programmers only need to focus  1 http://mahout.apache.org 2 http://hadoop.apache.org 99 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


on problem solving versus implementation The MapReduce runtime system will take care of the underlining parallelization fault tolerance data distribution and load balance Google le system GFS is a distributed le system that MapReduce uses for the storage of large amount of data across inexpensive hard drives The availability and reliability of underlining unreliable hardware are provided by replicating le blocks and distributing them across different nodes A MapReduce job consists of at least a map function and a reduce function called mapper and reducer respectively The mapper takes as input a pair of key/value and produces a set of key/value pairs All key/value pairs are sorted by their keys and sent to different reducers according to the key Each reducer receives a key and a set of values that has the same key This makes MapReduce an excellent tool for computations that need sorting or counting The map and reduce functions are left to the user to implement their desired functionalities to process each key/value pair Hadoop is an open source implementation of the MapReduce framework that is commonly used by academic and industry for Big Data analysis In the core of Hadoop are Hadoop MapReduce and Hadoop Distributed File System HDFS the open source counterpart of GFS There are also a bundle of Hadoop-related projects supported by Apache Foundation such as HBase database Hive data warehouse Pig highlevel data-îow Zookeeper high-performance coordination and Mahout scalable machine learning and data mining Therefore we choose Hadoop as the develop platform to study the scalability of Na  ve Bayes classiìer B Na  ve Bayes Classiìcation Na  ve Bayes has proven to be a simple and effective machine learning method in previous text classiìcation studies It is even optimal in some cases 17 Suppose there are m possible classes C   c 1 c 2   c m  for a domain of documents D   d 1 d 2   d n   Let W   w 1 w 2   w s  be the set of unique words each of which appears at least once in one of the documents in D  The probability of a document d being in class c can be computed using Bayes rule P  c  d  P  c  P  d  c  P  d   1 Since P  d  is a constant for the known data set size the denominator of 1 is typically not calculated for maximum a posteriori MAP common for parametric statistical problems ANa  ve Bayes model assumes that each term or word w k in a document occurs independently in the document given the class c  Therefore equation 1 becomes P  c  d   P  c  n d  k 1  P  w k  c  t k  where n d is the number of unique words in document d and t k is the frequency of each word w k  To avoid oating point underîow we use the equivalent equation log P  c  d   log P  c  n d  k 1  t k log P  w k  c   2 Hadoop HDFS Data Node Name Node Work Flow Controller Data Node Data Node Data Node Result Collector User Terminal Raw Data Data Parser Fig 1 A simple system to process data using Na  ve Bayes classiìer on Hadoop The class of d is decided as the class c   which maximizes log P  c  d  in equation 2 c   argmax c  C  log P  c  n d  k 1  t k log P  w k  c   3 When applying Na  ve Bayes classiìer NBC we can estimate P  c  and P  w k  c  as  P  c  N c N and  P  w k  c  N w k  w i  W N w i where N is the total number of documents N c is the number of documents in class c and N w i is the frequency of a word w i in class c  With these estimations the calculation of the right hand side of equation 3 is essentially a counting problem This makes MapReduce a suitable framework for the implementation of NBC in large-scale datasets III S YSTEM D ESCRIPTION On top of Hadoop MapReduce and HDFS we designed a Big Data analyzing system to evaluate whether Na  ve Bayes classiìer can scale up to classify millions of movie reviews This section explains the four modules of the system and important steps of the work ow A System Components As shown in Fig 1 the system adds four modules on top of Hadoop the work ow controller WFC the data parser the user terminal and the result collector We design this system based on our need to generate different sizes of datasets and test the Hadoop program on them respectively We also need to perform ten-fold cross validation for accuracy computation that requires calling the same program multiple times Our raw data comes from large sets of movie reviews collected by research communities The data parser is responsible to produce the desired data format to assist the program to efìciently process each review The user submits jobs through the user terminal Experiment results are also accessible through the user terminal after the result collector nishes collection The work ow controller manages the work ow of the whole system which includes 100 


1 Instruct data parser of the format of input data and the desired output 2 Transmit source code to the name node and execute Hadoop jobs and 3 Trigger the result collector to collect computing results once they are available on Hadoop Distributed File System HDFS B Dataset In our experiments we use two datasets the Cornell University movie review dataset 3 and Stanford SNAP Amazon movie review dataset 4  The Cornell dataset has 1000 positive and 1000 negative reviews which makes P  c  in equation 3 0.5 for both class A reviewês class is then decided by the frequency of each word that appears in the model obtained from training dataset The Amazon movie review dataset is organized into eight lines for each review with additional information such product identiìcation ID user ID proìleName score summary etc Since it is a 5 points rating system we simply divide all reviews into positive and negative subset using 3.0 as a threshold We consider unigrams only for the Na  ve Bayes model C Overall Work Flow 1 Pre-processing Raw Dataset The data parser rst preprocesses all reviews into a common format After the processing each review is one line in the dataset with document ID and sentiment positive or negative preìxed This is useful because by default MapReduce splits the input les by line and passes each line to a mapper To pre-process a raw review from the dataset we simply delete unwanted context such as punctuation special symbols and numbers We did not introduce a lexicon or vocabulary to lter out meaningless words Table I lists four sample reviews after pre-processing Each review is one line in the dataset le We use two preìxed tags sentiment of either positive or negative and document ID which is a number The two tags are both surrounded by colons to help the program differentiate them from review text Note that there are some isolated single letters resulting from the remove of punctuation For example the p.s in review ID 41 becomes p s  after removing the two dots The preprocessing procedure didnêt remove these meaningless letters All pre-processed reviews are stored in the name node as a repository waiting for further sampling 2 Preparing Input Datasets The WFC and the data parser work together to prepare input datasets for all test trials When the WFC requests a dataset with certain size the data parser extracts from the repository the desired number of positive reviews as well as the same amount of negative reviews The result is an input dataset of two equal-size classes of movie reviews After a dataset is generated the data parser divides it into 10 subsets for the convenience of ten-fold cross validation The WFC then moves them to the right locations in HDFS for every trial and calls the Hadoop NBC program  3 http://www.cs.cornell.edu/people/pabo/movie-review-data 4 http://snap.stanford.edu/data/web-Amazon.html TABLE I S AMPLE R EVIEWS AFTER P RE PROCESSING  POS 41 i disagree with the reviewers who said the movie was predictable and drawn out it was a movie with heart and you could feel the main characters plight when he lost his companion being an animal lover i was pulling for the happy ending of course i am disney s biggest fan and i love this movie right along with the other spsiama grandmother to eleven thank heavens for disney movies POS 85 sit back and enjoy the interesting and exciting story of the count of monte cristo great rainy day movie POS 95 a very well done lm and an excellent cast i d put it right up with the three and four musketeers movies york reed chamberlain heston etc POS 96 this is an excellent movie and i never read the book the acting and the plot was very nice done it is one of my favorite movies Training Data Training Job Model Test Data Combining Job Intermediate Table Classify Job Positive Negative Fig 2 Job sequence of Na  ve Bayes Classiìer on Hadoop 3 Sentiment Classiìcation Using Hadoop The sentiment classiìcation is the key step in the work ow Fig 2 shows the job sequence of this step Once the training data and test data are ready in HDFS the WFC starts the training job to build a model The combining job then combines test data with the model resulting an intermediate table Finally the classify job simultaneously computes the probabilities of each review in the two classes respectively and makes a decision about the sentiment of this review The statistics of true positive true negative false positive and false negative are also recorded 4 Result Collection After the classify job nished the result collector retrieves the model intermediate table classiìcation results and statistics of the test from HDFS D Automatic Scheduling The WFC coordinates the automation of the whole system For the Amazon dataset we conducted 120 test trials in total on twelve sizes of datasets All test trials are automatically scheduled by the WFC without supervision The only parameters that need to be decided by human is the sizes of experiment datasets This automatic scheduling method can be easily applied to other programs with minor change of the parameters IV I MPLEMENTATION OF N A  IVE B AYES IN H ADOOP A Deìning the Problem We rst deìne our task as to classify the sentiment of a movie review positive or negative Hence we only have two classes of documents To simplify the problem we choose the same number of positive and negative reviews which makes the P  c  in equation 3 a constant The estimation of P  w k  c  is computed by the relative frequency of w k in all documents in c  The classiìcation problem is then converted to a counting problem on the training and test datasets 101 


 Algorithm 1 Training Job Input The training reviews Output Model 1 function MAPPER  key  value  2 s  parseSentiment value  3 for all w  d do 4 emit  w 1 s  5 end for 6 end function 7 function REDUCER  key  values  8 posSum 0 negSum 0 9 for all value  values do 10  s count   parse value  11 if s  positive then 12 posSum  count  13 else 14 negSum  count  15 end if 16 end for 17 emit key posSum negSum  18 end function Algorithm 2 Combining Job Input The test reviews and the model Output Intermediate results for classify job input 1 function MAPPER  key  value  2 if value  model then 3  word pos s um neg s um   parse value  4 emit word pos s um neg s um  5 else 6  docid  sentiment   parse value  7 for all word  value do 8 emit word 1 docid  sentiment  9 end for 10 end if 11 end function 12 function REDUCER  key  values  13 for all value  values do 14 if value  model then 15 outstr.append value  16 else 17  count  docid   parse value  18 docs docid add count  19 end if 20 end for 21 for all docid  docs do 22 outstr.append count  docid  23 end for 24 emit word  outstr  25 end function B Algorithms After the simpliìcation of the problem the task can be divided into three sequential jobs as follows 1 Training job Algorithm 1 All training reviews are fed into this job to produce a model for all unique words with their their frequency in positive and negative review documents respectively 2 Combining job Algorithm 2 In this job the model and the test reviews are combined to a intermediate table with all necessary information for the nal classiìcation 3 Classify job Algorithm 3 This job classiìes all reviews simultaneously and writes the classiìcation results to HDFS Algorithm 1-3 are the pseudo-codes for the three jobs These jobs are executed in sequence because the dependencies between them as shown in Fig 2 The training job produces a model that is used to compute the probability of each word in the two classes The combining job then associates the test data with the model excluding the words that appear in test data but not in the model The intermediate table produced by the combining job is then fed to the classify job By the end of classify job all reviews are classiìed into positive or negative classes The actual Hadoop code we develop is less than ve hundred programming lines including comments and package import statements With such a simple program the results are surprisingly good considering the difìculties of mining sentiment using computer programs Algorithm 3 Classify Job Input The intermediate results Output Classiìcation results and accuracy 1 function MAPPER  key  value  2 for all item  value do 3  docid count pos s um neg s um   parse value  4 emit docid count pos s um neg s um  5 end for 6 end function 7 function REDUCER  key  values  8  docid trueSentiment parse key  9 for all value  values do 10 calculate pos prob 11 calculate neg prob 12 end for 13 predict  pos prob>neg prob  pos  neg 14 if predict  trueSentiment then 15 correct  true 16 correct count  17 else 18 correct  false 19 end if 20 emit docid predict correct  21 end function V E XPERIMENTAL S TUDY A Cloud Infrastructure Virtual Hadoop cluster is a fast and easy way to test a Hadoop program in the Cloud although the performance might be weaker compared to a physical Hadoop cluster Our cloud infrastructure is built on a Dell server with 12 Intel Xeon E52630 2.3GHz cores and 32G memory We use Xen Cloud Platform XCP 1.6 as the hypervisor On top of XCP 1.6 we built a virtual Hadoop cluster of seven nodes There is one special node the manager that we use to manage the Hadoop 102 


0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0.82 0.84 0.86 2 20 200 400 600 800 1000 1200 1400 1600 1800 2000 Accuracy Dataset size \(K Fig 3 The accuracy of Na  ve Bayes classiìer when the dataset size increases 0.00 5.00 10.00 15.00 20.00 25.00 30.00 35.00 40.00 45.00 50.00 2 20 200 400 600 800 1000 1200 1400 1600 1800 2000 DATASET SIZE \(K ACCURACY BREAKDOWN True Positive True Negative False Positive False Negative Fig 4 The accuracy breakdown of Na  ve Bayes classiìer when the dataset size increases cluster The rest of six nodes form the actual Hadoop cluster with one name node and six data nodes We allocate each VM two virtual CPU and 4GB of memory to provide sufìcient computing resources for each Hadoop node B Experiment Setup First we tested our code on Cornell dataset and resulted in a 80.85 average accuracy Without changing the Hadoop code our program was able to classify different subsets of Amazon movie review dataset with comparable accuracy To test the scalability of Na  ve Bayes classiìer the size of dataset in our experiment varies from one thousand to one million reviews in each class C Results The result statistics include the classiìcation accuracy the computation time and the throughput of the system Fig 3 shows the average accuracy of our NBC program on various sizes of datasets Each accuracy number is the average of ten trials When the dataset is relatively small the accuracy is unstable because the training data are not big enough for the model to learn enough knowledge about each class As the dataset size increases above 400K the 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 2 20 200 400 600 800 1000 1200 1400 1600 1800 2000 REVIEWS/SECOND DATASET SIZE \(K Fig 5 Throughput of the System with respect to dataset size TABLE II P ROCESSING TIME  SECOND  EVERY 1000 REVIEWS  Dataset size K 2 20 200 400 600 800 Second/10K reviews 455.1 48.15 6.57 4.24 3.47 3.11 Dataset size K 1000 1200 1400 1600 1800 2000 Second/10K reviews 2.88 2.77 2.47 2.4 2.37 2.33 accuracy gradually climbs above 80 and approaching 82 This demonstrates that the accuracy of NBC is stable when the dataset increases To further examine the classiìcation results we plot in Fig 4 the breakdown of accuracy into true positive and true negative accompanied by false positive and false negative As we expected the true positive and true negative increase with respect to the dataset size while the false positive and false negative decrease Table II shows that the processing time for every ten thousand reviews in our Hadoop NBC program decreases when the dataset size increases A dataset of 2K reviews did not beneìt from the parallelization of Hadoop because the input data is smaller than the size of one block in HDFS Although three replicas are distributed in different nodes there are at most three nodes in the Hadoop cluster that can access the input data locally After the input data increasing to a certain size the advantage of Hadoop starts to appear in that the processing time for the same amount of reviews is drastically reduced compare to the 2K case To observe the resutls in another dimension Fig 5 shows the throughput of the system with respect to the size of dataset The number of reviews that the system can processes in one second increases from 22 2K case to 4304 2000K case Overall our implementation of NBC is able to scale up to two million reviews sampled from the Amazon dataset The accuracy tends to stable when the dataset size increases These results are based on the simple processing of review texts Further ltering of the input data might be able to increase the accuracy VI C ONCLUSION In this paper we presented a simple and complete system for sentiment mining on large datasets using a Na  ve Bayes classiìer with the Hadoop framework We implemented the NBC on top of Hadoop framework with additional modules to automate the experiment We also provide the implementation 103 


details for converting a classiìer to Hadoop program of which any machine learning algorithm could be replaced for the NBC Our results show that NB classiìer can scale up easily even without a database Because of our simpliìed setup the average accuracy stays below 82 in all cases An intelligent lter might be helpful to increase the accuracy We believe that our work is just a beginning of employing machine learning technologies in large-scale datasets Future work will include using our framework for information fusion over imagery and text distributed robotics applications and cyber analysis using cloud computing R EFERENCES  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters in Proceedings of the 6th conference on Symposium on Opearting Systems Design  Implementation Volume 6  ser OSDIê04 Berkeley CA USA USENIX Association 2004  U Kang D H Chau and C F aloutsos Pe gasus Mining billionscale graphs in the cloud in Acoustics Speech and Signal Processing ICASSP 2012 IEEE International Conference on  2012 pp 5341 5344  S Suri and S V assilvitskii Counting triangles and the curse of the last reducer in Proceedings of the 20th international conference on World wide web  ACM 2011 pp 607Ö614  U Kang D H Chau and C F aloutsos Mining lar ge graphs Algorithms inference and discoveries in Data Engineering ICDE 2011 IEEE 27th International Conference on  2011 pp 243Ö254  B P ang L Lee and S V aithyanathan Thumbs up sentiment classiìcation using machine learning techniques in Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10  2002 pp 79Ö86  B P ang and L Lee  A sentimental education Sentiment analysis using subjectivity summarization based on minimum cuts in Proceedings of the 42nd annual meeting on Association for Computational Linguistics  2004 p 271  P  Chesle y  B V incent L Xu and R K Srihari Using v erbs and adjectives to automatically classify blog sentiment Training  vol 580 no 263 p 233 2006  M Gamon A Aue S Corston-Oli v e r  and E Ringger  Pulse Mining customer opinions from free text in Advances in Intelligent Data Analysis VI  Springer 2005 pp 121Ö132  A K ennedy and D Inkpen Sentiment classiìcation of mo vie re vie ws using contextual valence shifters Computational Intelligence  vol 22 no 2 pp 110Ö125 2006  M Thomas B P ang and L Lee Get out the v ote Determining support or opposition from congressional oor-debate transcripts in Proceedings of the 2006 conference on empirical methods in natural language processing  Association for Computational Linguistics 2006 pp 327Ö335  E Blasch A Steinber g S Das J Llinas C Chong O K essler  E Waltz and F White Revisiting the JDL model for information exploitation in Intêl Conf on Info Fusion  2013  E Blasch Y  Chen G Chen D Shen and R K ohler  Information fusion in a cloud-enabled environment High Performance Semantic Cloud Auditing Springer Publishing  2013  B Liu Y  Chen E Blasch K Pham D Shen and G Chen  A holistic cloud-enabled robotics system for real-time video tracking application in Intl Workshop on Enhanced Cloud Fusion in conjunction with Future Tech Korea Sept 2013  Sept  B Liu E Blasch Y  Chen A J A v ed A Hadiks D Shen and G Chen Information fusion in a cloud computing era A systemslevel perspective Submitted to IEEE AES Magazine  2013  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters Communications of the ACM  vol 51 no 1 pp 107 113 2008  D Le wis Na  ve bayes at forty The independence assumption in information retrieval Machine Learning ECML-98  pp 4Ö15 1998  P  Domingos and M P azzani On the optimality of the simple bayesian classiìer under zero-one loss Machine learning  vol 29 no 2-3 pp 103Ö130 1997  H Karlof f S Suri and S V assilvitskii  A model of computation for mapreduce in Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms  Society for Industrial and Applied Mathematics 2010 pp 938Ö948 104 


1 2 1 2 1 1 5 2 11 1 1 2 3 7 4 4 8 9 8 9 18 9 10 11 10 11 11 1 1 
D D D D D 
0 1,000 2,000 3,000 4,000 D1 D2 D3 D4 D5 0 20 40 60 80 D1 D2 D3 D4 D5 0 10 20 30 D1 D2 D3 D4 D5 0 5 10 15 D1 D2 D3 D4 D5 0 10 20 30 D1 D2 D3 D4 D5 0 10 20 30 D1 D2 D3 D4 D5 0 5 10 15 20 D1 D2 D3 D4 D5 0 10 20 30 40 50 D1 D2 D3 D4 D5 0 20 40 60 80 D1 D2 D3 D4 D5 0 10 20 30 D1 D2 D3 D4 D5 0 20 40 60 D1 D2 D3 D4 D5 
Q Q Q Q Q D D Q Q Q Q  Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q 
   1 2 3 4 5 
Table I D ATASET C HARACTERISTICS  Figure 3 Query Performance and Scalability 
C Query Evaluation Performance and Scalability Graphs Dependencies Artifacts optional optional ler union Processes optional union 
100,000 40,000,000 2.1 GB 200,000 80,000,000 4.2 GB 300,000 120,000,000 6.3 GB 400,000 160,000,000 8.4 GB 500,000 200,000,000 10.5 GB 
Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Legend 
 
Query performance and scalability of our approach are reported in Fig 3 Queries and were in the category and had basic graph patterns with one triple pattern Both and yielded larger results compared to other queries returned all graph identiìers in a dataset e.g 100,000 triples for and 500,000 triples for  and returned all triples in a particular provenance graph e.g returned around 400 triples for each dataset Even though these queries were the simplest queries among the UTPB test queries they showed to be more expensive due to larger result sets which is especially evident for query  the only query whose performance was on the order of seconds In the case of both and  which involved no joins the major factor in query performance is the transfer time of nal query results to a client machine and it is hardly possible to achieve better performance on the given hardware By contrast all other queries performed on the order of tens of milliseconds required joins and returned subsets of triples in a particular provenance graph  400 triples Queries were in the category and dealt with various dependencies among artifacts and processes in provenance graphs They all had similar complexities basic graph patterns with three triple patterns each They also returned comparable result sets in terms of the number of triples except query that returned an empty result set for the selected UTPB provenance graph template As a result these queries showed very similar query evaluation performance with being the fastest as it only required the evaluation of its rst triple pattern to compute the nal empty result Queries and were in the category and dealt with data artifacts in provenance graphs contained six triple patterns and two clauses had triple patterns two clauses two and one constructs is the most complex query of all yet it was shown to be efìcient and scalable with our approach The last two queries and  were in the category and dealt with processes in provenance graphs had two triple patterns and one clause While is a complex query with triple patterns and one clause it yielded an empty query result in our experiments due to the selected provenance template In summary the proposed approach and its implementation proved to be efìcient and scalable showed linear scalability and took the most time to execute due to a relatively large result set The other queries showed nearly constant scalability technically linear with a small scope This can be explained by the fact that each query except  dealt with a single provenance graph of xed size with minimal data transfers and fast index-based join processing VI C ONCLUSIONS AND F UTURE W ORK In this paper we studied the problem of storing and querying large collections of scientiìc workîow provenance graphs serialized as RDF graphs in Apache HBase We designed novel storage and indexing schemes for RDF data in HBase that are suitable for provenance datasets Our storage scheme takes advantage of the fact that individual provenance graphs generally t into memory of 
X-axis: Dataset Y -axis: Query execution time, ms 
Dataset of RDF graphs of RDF triples Size  of workîow runs 
7 
7 


a single machine and require no partitioning Our bitmap indices are stored together with graphs and support both selection and join operations for efìcient query processing We also proposed efìcient querying algorithms to evaluate SPARQL queries in HBase Our algorithms rely on indices to compute expensive join operations make use of numeric values that represent triple positions rather than actual triples with lengthy literals and URIs and eliminate the need for intermediate data transfers over a network Finally we conducted an empirical evaluation of our approach using provenance graphs and test queries of the University of Texas Provenance Benchmark Our experiments conìrmed that our proposed storage indexing and querying techniques are efìcient and scalable for large provenance datasets In the future we plan to compare our approach with other SQL and NoSQL solutions in the context of distributed scientiìc workîow provenance management as well as experiment with a multi-user workload to measure query throughput of our system R EFERENCES  Y  Simmhan B Plale and D Gannon  A surv e y of data provenance in e-science 
 vol 34 no 3 pp 31Ö36 2005  S B Da vidson S C Boulakia A Eyal B Lud  ascher T M McPhillips S Bowers M K Anand and J Freire Provenance in scientiìc workîow systems  vol 30 no 4 pp 44Ö50 2007  S B Da vidson and J Freire Pro v enance and scientiìc w orkîows challenges and opportunities in  2008 pp 1345Ö1350  V  Cue v as-V icentt  n S C Dey S K  ohler S Riddle and B Lud  ascher Scientiìc workîows and provenance Introduction and research opportunities  vol 12 no 3 pp 193Ö203 2012  L Moreau B Clif ford J Freire J Futrelle Y  Gil P  T  Groth N Kwasnikowska S Miles P Missier J Myers B Plale Y Simmhan E G Stephan and J V den Bussche The Open Provenance Model core speciìcation v1.1  vol 27 no 6 pp 743Ö756 2011  A Chebotk o S Lu X Fei and F  F otouhi RDFPro v A relational RDF store for querying and managing scientiìc workîow provenance  vol 69 no 8 pp 836Ö865 2010  J Zhao C A Goble R Ste v ens and D T uri Mining Tavernaês semantic web of provenance  vol 20 no 5 pp 463Ö472 2008   http://twiki.ipaw.info/bin/view Challenge/ThirdProvenanceChallenge  C Lin S Lu X Fei A Chebotk o D P ai Z Lai F  F otouhi and J Hua A reference architecture for scientiìc workîow management systems and the VIEW SOA solution  vol 2 no 1 pp 79 92 2009  T  M Oinn et al T a v erna lessons in creating a w orkîo w environment for the life sciences  vol 18 no 10 pp 1067Ö1100 2006  B Lud  ascher I Altintas C Berkley D Higgins E Jaeger M B Jones E A Lee J Tao and Y Zhao Scientiìc workîow management and the Kepler system  vol 18 no 10 pp 1039Ö1065 2006  S P  Callahan J Freire E Santos C E Scheide gger  C T  Silva and H T Vo Managing the evolution of dataîows with VisTrails in  2006 p 71  J Kim E Deelman Y  Gil G Mehta and V  Ratnakar  Provenance trails in the Wings/Pegasus system  vol 20 no 5 pp 587Ö597 2008  Y  Zhao et al Swift F ast reliable loosely coupled parallel computation in  2007 pp 199Ö206   http://hbase.apache.org  F  Chang J Dean S Ghema w at W  C Hsieh D A Wallach M Burrows T Chandra A Fikes and R E Gruber Bigtable A distributed storage system for structured data  vol 26 no 2 2008  A Chebotk o E D Ho yos C Gomez A Kashle v  X Lian and C Reilly UTPB A benchmark for scientiìc workîow provenance storage and querying systems in  2012 pp 17Ö24  M F  Husain L Khan M Kantarcioglu and B M Thuraisingham Data intensive query processing for large RDF graphs using cloud computing tools in  2010 pp 1Ö10  J Myung J Y eon and S Lee SP ARQL basic graph pattern processing with iterative MapReduce in  2010 pp 6:1Ö6:6  P  Ra vindra V  V  Deshpande and K An yanwu T o w ards scalable RDF graph analytics on MapReduce in  2010 pp 5:1Ö5:6  J Urbani S K otoulas E Oren and F  v an Harmelen Scalable distributed reasoning using MapReduce in  2009 pp 634Ö649  A Sch  atzle M Przyjaciel-Zablocki and G Lausen PigSPARQL mapping SPARQL to Pig Latin in  2011 p 4  J Huang D J Abadi and K Ren Scalable SP ARQL querying of large RDF graphs  vol 4 no 11 pp 1123Ö1134 2011  C Frank e S Morin A Chebotk o J Abraham and P  Brazier  Distributed semantic web data management in HBase and MySQL Cluster in  2011 pp 105Ö112  M Atre V  Chaoji M J Zaki and J A Hendler  Matrix Bit loaded a scalable lightweight join query processor for RDF data in  2010 pp 41Ö50 
SIGMOD Record IEEE Data Engineering Bulletin Proc of SIGMOD Conference Datenbank-Spektrum Future Gen Comp Syst Data Knowl Eng Concurr Comput  Pract Exper Third Provenance Challenge IEEE Transactions on Services Computing Concurr Comput  Pract Exper Concurr Comput  Pract Exper Proc of ICDE Workshops Concurr Comput  Pract Exper Proc of SWF Apache HBase ACM Transactions on Computer Systems Proc of SWF Proc of CLOUD Proc of MDAC Proc of MDAC Proc of ISWC Proc of SWIM PVLDB Proc of CLOUD Proc of the WWW 
8 
8 


Jorda Polo, David Carrera, Yolanda Becerra, Malgorzata Steinder  and Ian Whalley. Performance-driven task co-scheduling for  mapreduce environments. In Network Operations and Management  Symposium \(NOMS\2010 IEEE, pages 373 Ö380, 19-23 2010 12 K. Kc and K. Anyanwu, çScheduling hadoop jobs to meet deadlines  in 2nd IEEE International Conference on Cloud Computing  Technology and Science \(CloudCom\, 2010, pp. 388 Ö392 13 Xicheng Dong, Ying Wang, Huaming Liao çScheduling Mixed Real time and Non-real-time Applications in MapReduce Environment  In the proceeding of 17th International Conference on Parallel and  Distributed Systems. 2011, pp. 9 Ö 16 14 Xuan Lin, Ying Lu, J. Deogun, and S. Goddard. Real-time divisible  load scheduling for cluster computing. In Real Time and Embedded  Technology and Applications Symposium, 2007. RTAS ê07. 13th  IEEE pages 303 Ö314, 3-6 2007 15 HDFS  http://hadoop.apache.org/common/docs/current/hdfsdesign.html  16 Chen He, Ying Lu, David Swanson. çMatchmaking : A New  MapReduce Scheduling Techniqueé. In the proceeding of 2011  CloudCom, Athens, Greece, 2011, pp. 40 Ö 47 17 Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma and Khaled  Elmeleegy, Scott Shenker, and Ion Stoica, çDelay scheduling: a  simple technique for achieving locality and fairness in cluster  schedulingé. In the proceedings of the 5th European conference on  Computer systems, 2010.  pp 265-278 18 Zhuo Tang, Junqing Zhou, Kenli Li, and Ruixuan Li "A MapReduce  task scheduling algorithm for deadline constraints.", Cluster  Computing, Vol. 15,  2012 19 Eunji Hwang, and Kyong Hoon Kim. "Minimizing Cost of Virtual  Machines for Deadline-Constrained MapReduce Applications in the  Cloud." Grid Computing \(GRID\, 2012 ACM/IEEE 13th  International Conference on. IEEE, 2012 20 Micheal Mattess, Rodrigo N. Calheiros, and Rajkumar Buyya  Scaling MapReduce Applications across Hybrid Clouds to Meet Soft  Deadlines." Technical Report CLOUDS-TR-2012-5, Cloud  Computing and Distributed Systems Laboratory, the University of  Melbourne, August 15, 2012 21 
 
11 
                
Chen He, Ying Lu, David Swanson. çReal-Time Application Scheduling in Heterogeneous MapReduce Environments Technical Report TR-UNL-CSE2012-0004, University  of Nebraska-Lincoln, 2012 Available: http://cse apps.unl.edu/facdb/publications/TR-UNL-CSE20120004.pdf 22 T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, K  Elmleegy, and R. Sears. çMapreduce Onlineé. In NSDI 2010 23 A. D. Ferguson, P. BodÌk, S. Kandula, E. Boutin, and R  Fonseca. çJockey: Guaranteed Job Latency in Data Parallel Clusters. In EuroSys, 2012 24 G. Wang, A. R. Butt, P. Pandey, and K. Gupta. çA Simulation Approach to Evaluating Design Decisions in MapReduce Setupsé. In MASCOTS 2009 25 H. Herodotou and S. Babu. Profiling, çWhat-if Analysis and Cost-based Optimization of MapReduce Programs In VLDB 2011 26 H. Herodotou, F. Dong, and S. Babu. çNo One \(Cluster Size Fits All: Automatic Cluster Sizing for Dataintensive Analyticsé. In SoCC 2011  
1544 
1544 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


