Evaluating Parallel Logistic Regression Models Haoruo Peng HTC Research Center Beijing China penghaoruo@hotmail.com Ding Liang HTC Research Center Beijing China Ding  Liang@htc.com Cyrus Choi HTC Research Center Beijing China Cyrus Choi@htc.com Abstract Logistic regression LR has been widely used in applications of machine learning thanks to its linear model However when the size of training data is very large even such a linear model can consume excessive memory and computation time To tackle both resource and computation scalability in a big-data setting we evaluate and compare different approaches in distributed platform parallel algorithm and sublinear approximation Our empirical study provides design guidelines for choosing the most effective combination for the performance requirement of a given application Keywords Logistic Regression Model Parallel Computing Sublinear Method Big Data I I NTRODUCTION The logistic regression model plays a p i v otal role in machine learning tasks The model is suited for classiìcation problems and is supported by a substantial body of statistical theories A binary classiìcation problem modeled by LR can be easily extended to a multi-class classiìcation problem We focus our study on the binary LR model in this paper and the conclusions can be extended to a multi-class setting In recent years many modern datasets have grown drastically in both data volume and data dimensionality Large data volume and high data dimensionality bring both resource and computational challenges to machine learning algorithms For example a social networking site such as Facebook consists of tens of millions of records each is composed of hundreds of attributes For text and multimedia categorization we usually have to deal with billion-scale datasets in a feature space of thousands of dimensions In this work we evaluate approaches to scale up LR in a big-data setting The approaches we evaluate include three aspects 1 distributed platform 2 parallel algorithm and 3 sublinear approximation In the platform aspect we compare two well known distributed systems Hadoop and Spark 23 Hadoop employs HDFS and MapReduce 8 Spark promotes the efìciency of iterative algorithms and also supports HDFS In the parallel algorithm aspect we compare the sequential optimization algorithms stochastic gradient descent which can obtain optimal generalization guarantees with a small number of passes over the data The algorithm can easily be parallelized on either Hadoop or Spark and it can also be used in an online setting at which a data instance is seen only once in a streaming fashion Finally to further speed up computation we compare aforementioned platform/algorithm combinations with our previously proposed sublinear algorithms These sublinear algorithms access a single feature of a feature vector instead of all features at each iteration to reduce computation time Our evaluation and comparisons provide insights to facilitate an application designer for selecting a solution that best meets the performance requirement of the machine-learning task at hand II R ELATED W ORK We present related work in the two aspects platform and algorithm A Computing Platforms The two distributed platforms that we work with have their own advantages in the implementation of machine learning algorithms The Hadoop platform allo ws for distrib uted processing of large datasets across clusters of computers using simple programming models It utilizes MapReduce as its computational paradigm which is easily parallelized Additionally Hadoop provides a Distributed File System HDFS Both MapReduce and HDFS are designed to handle node failures in an automatic way allowing Hadoop to support large clusters that are built on commodity hardware Spark is a cluster computing system that aims to make data analysis fast It supports the in-memory cluster computing A job can load data into memory and query it repeatedly by creating and caching resilient distributed datasets RDDs Moreover RDDs achieve fault tolerance through lineage information about how RDDs are derived from other RDDs is stored in reliable storage thus making RDDs easy to be rebuilt if a certain partition is lost Spark can execute up to be two orders faster than Hadoop for iterative algorithms Table I compares different features between Hadoop and Spark platforms Spark supports two types of operations on RDDs Actions and Transformations As and Ts Actions include functions like count  collect and save  They usually return a result from input RDDs Transformations include functions like map  lter and join  They normally build new 119 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


RDDs from other RDDs which comply with the lineage rule In the table we use NFS to denote Normal File System Table I P LATFORM C OMPARISON H ADOOP VS S PARK  Hadoop Spark Computational Paradigm MapReduce As and Ts File System Supported HDFS HDFS and NFS Design Concept Key-value Pairs RDD Fault Tolerance Technique Redundancy Lineage It is worth mentioning that Apache Mahout runs on Hadoop and is a scalable machine learning library Mahout integrates many important algorithms for clustering classiìcation and collaborative ltering in its package It is highly optimized so that Mahout also achieves good performance for non-distributed algorithms We use Mahout as the baseline in our experiments B Sublinear Methods Sublinear methods have recently been proposed by several researchers Clarkson et al  rst presented the solution of approximation algorithms in sublinear time The algorithm employs a novel sampling technique along with a new multiplicative update procedure Hazan et al  e xploited the approach to speed up linear SVMs Cotter et al 7 extended the method to kernelized SVMs In Hazan et al applied the sublinear approximation approach to solve linear regression with penalties Garber and Hazan also developed sublinear method with Semi-Deìnite Programming SDP Peng et al  utilized the method in the LR model with penalties and developed sequential sublinear algorithms for both  1 penalty and  2 penalty C Other Related Work LR is fairly straightforward to be parallelized Parallelization efforts on several machine learning algorithms such as SVMs LD A 17 Spectral Clustering 5 and frequent itemset mining require much more sophisticated sharding work to divide a computational task evenly onto distributed subtasks For a comprehensive survey please consult and 1 III L OGISTIC R EGRESSION M ODEL AND S EQUENTIAL S UBLINEAR A LGORITHM A Logistic Regression Model In this paper we mainly concern the binary classiìcation problem We deìne the training dataset as X    x i y i  i 1 n   where x i  R d are input samples and y i  1  1  are the corresponding labels For simplicity we will use the notation X  x 1  x 2  x n  T and y  y 1 y 2 y n  T to represent the training dataset in this paper To t in the logistic regression model the expected value of y i is given by P  y i  x i  1 1+exp  y i  x T i w  b   g i  y i   where w  w 1 w d  T  R d is a regression vector and b  R is an offset term The learning process aims to derive w and b by solving an optimization problem It is a common practice to add penalties to LR model in order to avoid overìtting and make the optimization result more practical Typically for  2 penalty we solve the following optimization problem max w b  F  w b X    2  w  2 2   1 For  1 penalty we optimize the following criterion max w b  F  w b X     w  1   2 Here we deìne F  w b X   n i 1 log g i  y i  in both 1 and 2 To be brief we omit the derivation here B Sequential Sublinear Algorithm We use the following notations to deìne sequential sublinear algorithm for penalized logistic regression Function clip    is a projection function deìned as clip  a b   max min  a b    b  a,b  R  Function sgn    is the sign function namely sgn     1  0  1   g    is the logistic function g  x   1  e  x   In Algorithm 1 we give the sequential sublinear approximation procedure for logistic regression The algorithm is from our previous work Mathematical symbols we use here comply with deìnitions in Section III-A Parameter  controls learning rate T determines iteration number and  deìnes approximation level Details of the parameter initialization step are documented in Algorithm 1  Sub-Linear Logistic Regression SLLR 1 Input parameters  or X,Y 2 Initialize parameters T u 0  w 1  q 1 b 1 3 Iterations t 1  T 4 p t  q t   q t  1 5 Choose i t  i with probability p  i  6 coef  y i t g   y i t  w t T x i t  b t  7 u t  u t  1  coef  2 T x i t 8  t  argmax     p t T    if input  for  2 penalty 9 u t  soft-thresholding operations if input  for  1 penalty 10 w t  u t  max  1   u t  2  11 b t  sgn  p t T y  12 Choose j t  j with probability w t  j  2   w t  2 2 13 Iterations i 1  n 14   x i  j t   w t  2 2  w t  j t   t  i  y i b t 15    clip   1   16 q t 1  i   p t  i   1       2   2  17 Output  w  1 T  t w t   b  1 T  t b t Each iteration of the SLLR algorithm has two phases stochastic primal update and stochastic dual update  Steps from line 4 to line 11 consist of the primal part while the dual part is composed of steps from line 12 to line 16 We give the sublinear algorithm in a uniìed way for  2 penalty and  1 penalty If we are dealing with  2 penalty we ignore line 9 and accomplish the computation in line 8 by a simple 120 


greedy algorithm Here  represents a Euclidean space with conditions     R n  i 0   i  2     1  n  If we are faced with  1 penalty we ignore line 8 and expand the procedure of line 9 as the following  Procedure  Line 9 in Algorithm 1 Iterations j 1  d if uprev t  j   0 and u t  j   0 u t  j ax u t  j    0 if uprev t  j   0 and u t  j   0 u t  j in u t  j   0 uprev t 1  u t In this sequential mode each iteration takes O  n  d  time which is sublinear to the dataset size IV P ARALLEL S UBLINEAR L OGISTIC R EGRESSION In this section we describe our parallel sublinear algorithms implemented on Hadoop and Spark respectively We also formally introduce the traditional parallel gradient algorithm used in Spark and the online stochastic gradient descent method used in Mahout In all pseudo-code we follow symbol notations deìned in Section III-A A Parallel Sublinear Algorithms on Hadoop We develop an algorithm to achieve sublinear performance for learning logistic regression parameters using the architecture of MapReduce The pseudo-code of Algorithm 2 together with auxiliary procedures explains the critical parts of this algorithm Algorithm 2  PSUBPLR-MR 1 Input parameters  or X,Y,n,d 2 Initialize parameters T u 0  w 1  q 1 b 1 3 Iterations t 1  T 4 w t  storeInHdfsFile\(éhdfs://parawé\oDistributedCache 5 p t  storeInHdfsFile\(éhdfs://parapé\oDistributedCache 6 conf primal  new Conìguration 7 job primal  new MapReduce-Job\(conf primal 8 conf primal.passParameters T n d b t  9 job primal.setInputPath 10 job primal.setOutputPath\(étmp/primal t  11 job primal.run 12  w t 1 b t 1   Primal-Update w t b t  13 Choose j t  j with probability w t 1  j  2   w t 1  2 2 14 w t 1  storeInHdfsFile\(éhdfs://parawé\oDistributedCache 15 conf dual  new Conìguration 16 job dual  new MapReduce-Job\(conf dual 17 conf dual.passParameters d j t b t 1   18 job primal.setInputPath 19 job dual.setOutputPath\(étmp/dual t  20 job dual.run 21 p t 1  Dual-Update p t  22 Output  w  1 T  t w t   b  1 T  t b t This parallel design generally follows the framework of sequential sublinear algorithm It has two computational components in each iteration the primal update part from lines 4 to 12 and the dual update part from lines 13 to 21  Procedure  Primal-Map\(inputìle 1 Conìguration.getParameters T n d b t  2 w t  readCachedHdfsFile\(éparaw 3 p t  readCachedHdfsFile\(éparap 4 i t  parseRowIndx\(inputìle 5 x i t  parseRowVector\(inputìle 6 y i t  parseRowLabel\(inputìle 7 r  random\(seed 8 if p t  i t   r n 9 tmp coef  p t  i t  y i t g   y i t  w t T x i t  b t  10 else 11 tmp coef 0 12 Iterations j 1  d 13 Set key  j 14 Set value  tmp coef  2 T x i t  j  15 Output key value Procedure  Primal-Reduce\(key in value in 1 key out  key in 2 value out   for same key in value in 3 Output key out value out In the primal update part there is a parallel implementation segment from lines 4 to 11  and also an unavoidable sequential segment as on line 12  In the dual part it is the same situation that steps from lines 14 to 20 employ parallel implementation whereas the sequential bottleneck is from lines 13 to 21  As the primal part and the dual part do not exhibit any inter-dependency they can be executed simultaneously in each computation iteration This halves the computation time This framework is shown in Fig 1 In the parallelization segment of the primal mapreduce job  we process all data instances in parallel In the primal update step we compute gradients from almost all data instances and make the weighted average value according to vector p as the output gradient for update The details of this algorithm design is shown in Procedure Primal-Map and Procedure Primal-Reduce Here we employ a randomization strategy when we compute those almost all gradients From lines 7 to 8 in Procedure Primal-Map all data instances are computed if r 0  As the expectation value for p t  i t  is 1 n  we normally set r to range between zero to one Two more issues must be taken care of by the algorithm The rst issue is parameter passing It is critical to choose an efìcient way to pass the updated parameters between iterations and even between different MapReduce jobs The design of Hadoop requires passing parameters between Figure 1 Parallel implementation ow chart for PSUBPLR-MR 121 


 Procedure  Primal-Update w t b t  1  w t  readFromHdfsFile\(étmp/primal t  2 w t 1  w t  w t 3 w t 1  w t 1  max  1   w t 1  2  4 b t 1  sgn  p t T y  Procedure  Dual-Map\(inputìle 1 Conìguration.getParameters d j t b t 1   2 w t 1  readCachedHdfsFile\(éparaw 3 i t  parseRowIndx\(inputìle 4 x i t  parseRowVector\(inputìle 5 y i t  parseRowLabel\(inputìle 6   x i t  j t   w t 1  2 2  w t 1  j t  y i t b t 1 7    clip   1   8 res  1       2   2 9 key  i t 10 value  res 11 Output key value computation iterations through les This IO step between iterations is clearly a bottleneck The second issue is that datasets are generally sparse when the data dimension is high This characteristic makes us focus on dealing with data sparsity issue in our code Instead of naively writing the simple code in data intensive situations we only store pairs of index and value in a data vector This sparse format brings us great memory-use efìciency improvement B Parallel Sublinear algorithms on Spark The algorithm to solve sublinear learning for penalized logistic regression in Spark is shown below In the pseudocode of Algorithm 3 the procedure for Primal-Update and the procedure for Dual-Map are the same as those in Algorithm PSUBPLR-MR This parallel design is very similar to that of Algorithm PSUBPLR-MR The most important difference is the cache operation in line 3 To make it work in Spark we follow the rule to construct an RDD for each data instance Also to cater for data sparsity the design is that every data value correspond to its individual index  And the index is also involved in the computation along with the value  We omit the changes for  2 penalty and  1 penalty here to make the algorithm easier to be understood In parallel mode the primal update contains the update of w t  which takes O  n  time The dual update contains the  2 sampling process for the choice of j t in O  d  time and the update of p in O 1 time Altogether each iteration takes O  n  d  time Compared to the analysis of sequential algorithm parallelization does not necessarily change computational complexity Theoretically parallel sublinear algorithm can be two times faster than the sequential version as the time for both update procedures in each iteration is reduced to O 1  Moreover by starting two separate MapReduce jobs in one iteration simultaneously the running time can be reduced to O  max  n d    Procedure  Dual-Update p t  1 var  readFromHdfsFile\(étmp/dual t  2 Iterations j 1  n 3 p t 1  j   p t  j   var  j  Algorithm 3  PSUBPLR-SPARK 1 Input parameters  or X,Y,n,d 2 Initialize parameters T u 0  w 1  q 1 b 1 3 points   4 Iterations t 1  T 5 gradient  points.map  1 1 e  y  w T t x  b   1  y  p  index   reduce\(sum 6  w t 1 b t 1   Primal-Update w t b t  7 Choose j t  j with probability w t 1  j  2   w t 1  2 2 8 pAdjust  copy 9 p t 1  Dual-Update p t  10 Output  w b  C Parallel Gradient Descent in Spark The parallel gradient descent method to solve LR in Spark is shown below Algorithm 4  PGDPLR-SPARK 1 Input parameters  or X,Y,n,d 2 Initialize parameters T u 0  w 1  q 1 b 1 3 points   4 Iterations t 1  T 5 gradient  points.map  1 1 e  y  w T t x  b   1  y  reduce\(sum 6 w t 1  w t  gradient  x 7 b  b  gradient 8 Output  w b  The pseudo-code of Algorithm 4 shows that the algorithm is naturally parallelizable We can take in all data in the same iteration and just compute the gradient in a MapReduce fashion As for the cache operation and RDD design for data sparsity it is the same with Algorithm PSUBPLRSPARK D Online Stochastic Gradient Descent in Mahout Though SGD is an inherently sequential algorithm it is blazingly fast Thus Mahoutês implementation can handle training sets of tens of millions of instances The SGD system in Mahout is an online learning algorithm implying that we can learn models in an incremental fashion instead of traditional batching In addition we can halt training when a model reaches target performance Because the SGD algorithms need feature vectors of xed length and it is very costly to build a dictionary ahead of time most SGD applications use an encoding system to derive hashed feature vectors We can create a RandomAccessSparseVector  and then use various feature encoders to progressively add features to this vector In our implementation we use RandomAccessSparseVector for data sparsity and the function call by OnlineLogisticRegression to train the LR classiìer 122 


We also perform cross validation However to maintain consistency with other methods for a fair comparison we write our own code to ensure the same cross validation scheme is applied to all algorithms being evaluated V E XPERIMENTAL S ETUP This section presents the details of the dataset information and testing environment of our experiments A Dataset Information We choose ve open datasets to run all six test programs Details are shown in Table II Different from the simulated 2d dataset the other four datasets are all sparse We split each dataset into the training set and the testing set We randomly repeat such split 20 times and our analysis is based on the average performance of 20 repetitions In Table II Density is computed as Density  Dataset   of N onzeros   of all entries  which stems from Balance describes the binary distribution of labels We compute it as following Balance  Dataset   of P ositive Instances  of N egative Instances  These ve datasets are carefully selected with an incremental trend in size The simulated 2d dataset represents toy data situations and serves for the initial test of correctness of classiìers The 20NewsGroup dataset is best-known for the test of LR model which has a balanced distribution between positive and negative data instances It also shows the application towards topic classiìcation The Gisette dataset is relati v ely lar ger  and feature v ectors are less sparse The ECUESpam dataset is selected due to its imbalanced distribution between positive and negative data instances It has a higher data dimensionality than the Gisette dataset However because of data sparsity it has fewer nonzero values involved in the computation Finally the URL-Reputation dataset contains millions of data instances and features The raw data are stored in the SVMlight format which has the volume of more than 2GB It can be seen as a representative of massive datasets in the sense that it exceeds the scalability of Liblinear which will be shown below B Testing Environment There are all together 6 test programs for comparison Online stochastic gradient descent method is run on Mahout in the sequential mode Liblinear is also a baseline test program we choose It is sequential and outperforms many other programs for LR on a single machine SLLR performs the sequential sublinear algorithm on a single machine PSUBPLR-MR performs the parallel sublinear algorithm on Hadoop PGDPLR-SPARK is the test program for the parallel gradient descent run on Spark PSUBPLR-SPARK implements the parallel sublinear algorithm run on Spark We run our test programs on a six-node cluster conìured as shown in Table III This computing cluster is considered small in size Nevertheless past works documented in  and 1 in parallelization sho w that this conìguration sufìces to tell the trend of scalability Our future work will use a larger cluster to run on much larger scale datasets in production settings to validate our conjecture Table III C LUSTER I NFORMATION CPU Model Intel Xeon E5-1410 2.80GHz Number of node 6 Number of CPU per node 4 Cores 8 Threads RAM per node 16G Disk per node 4T HDD Interconnection Method Gigabyte Ethernet VI E XPERIMENTAL R ESULTS This section conducts analysis on experimental results We report and analyze results in 1 accuracy 2 efìciency 3 scalability and 4 robustness A Results on Precision The results of six test programs achieved on ve datasets are shown in Table IV These are the average results of Table IV A CCURACY R ESULTS T HE MEANINGS OF ABBREVIATIONS ARE AS FOLLOWS  20-N-G 20 N EWS G ROUP  URL-R URL-R EPUTATION  20-N-G Gisette ECUESpam URL-R Mahout 71.3 91.5 85.2 91.5 Liblinear 92.0 97.4 97.1  SLLR 91.5 94.8 92.3 94.2 PSUBPLR-MR 90.5 94.6 91.7 93.8 PGDPLR-SPARK 92.0 97.0 93.7 96.0 PSUBPLR-SPARK 90.5 95.8 91.7 94.0 running cross validation Note that Liblinear cannot be implemented with the full URL-Reputation dataset on our machines due to memory limitation In Figures 2 d we show the test error as a function of iteration number on each dataset for all six test programs B Results on Running Time The running time of six test programs used on ve datasets is shown in Table V The reported running time is the average of our corss-validation executions Fig 3 reports the same results graphically Observations From the above results we reach the following conclusions 1 Liblinear performs best It fully utilizes memory and the single machine implementation does not require any communication between machines However its scalability is limited by the memory size of the single 123 


Table II D ATASETS  Name Dimension  of instances Density  of nonzero values Balance  of training  of testing 2d 2 200 1.0 400 1.000   20NewsGroup 16428 1988 7  384  10  3 238511 1.006 1800 188 Gisette 5000 7000 0.12998 4549319 1.000 6000 1000 ECUESpam 100249 10687 2  563  10  3 2746159 5.882 9000 1687 URL-Reputation 3231961 2376130 3  608  10  5 277058644 0.500 2356130 20000 2 4 6 8 10 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4   Iteration Number Test Error Mahout Liblinear   SLLR PSUBPLR MR   PGDPLR SPARK  PSUBPLR SPARK 2 4 6 8 10 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18   Iteration Number Test Error   Mahout Liblinear   SLLR PSUBPLRäMR   PGDPLRäSPARK  PSUBPLRäSPARK 10 20 30 40 50 0 0.05 0.1 0.15 0.2 0.25 0.3   Iteration Number Test Error   Mahout Liblinear   SLLR PSUBPLRäMR   PGDPLRäSPARK PSUBPLRäSPARK 10 20 30 40 50 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18   Iteration Number Test Error   Mahout Liblinear   SLLR PSUBPLRäMR   PGDPLRäSPARK PSUBPLRäSPARK a 20NewsGroup  b Gisette  c ECUESpam  d URL-Reputation  Figure 2 Test error as a function of iteration number Table V R UNNING T IME T HE MEANINGS OF ABBREVIATIONS ARE AS FOLLOWS  20-N-G 20 N EWS G ROUP  URL-R URL-R EPUTATION  20-N-G Gisette ECUESpam URL-R Mahout 9.83s 131.8s 96s 10100s Liblinear 0.79s 2.4s 13s  SLLR 20.05s 130.5s 1028s 3248s PSUBPLR-MR 1360.85s 3687.9s 11478s 16098s PGDPLR-SPARK 10.52s 99.2s 924s 3615s PSUBPLR-SPARK 8.57s 89.1s 796s 2918s  2d 20NewsGroup Gisette ECUESpam URLäReputation 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 Algorithms Running Time/s   Mahout Liblinear SLLR PSUBPLRäMR PGDPLRäSPARK PSUBPLRäSPARK Figure 3 Running time machine that it runs on Nevertheless if the dataset can be fully loaded into the memory of a single machine this efìcient and direct sequential way of implementation is highly recommended for both fast speed and high accuracy 2 Mahoutês precision is not good especially on those datasets where positive and negative instances are imbalanced However it is a representative example of sequential algorithm that can train massive data in acceptable time We monitor the memory when running Mahout and nd that its memory use is much lower than Liblinear This is the advantage brought by both online algorithm and hashing operations on features Therefore Mahout offers good scalability guarantee When only single machine with limited memory is available sequential online training like Mahout is recommended 3 The precision of all sublinear methods is acceptable The developed parallel sublinear algorithm only has a small drop in precision 4 Hadoop system suffers from a drawback for running LR optimization methods Its cluster programming model is based on an acyclic data ow from a stable storage to a stable storage Though it has the beneìts of deciding where to run tasks and can automatically recover from failures in runtime the acyclic data ow is inefìcient for iterative algorithms All six test programs involve a number of iterations thus making PSUBPLR-MR perform poorly worse than sequential algorithms When we study the details of Hadoop implementation we nd that the starting time of MapReduce job in PSUBPLR-MR is about 20s including time for task conìguration and parameter passing This running time overhead is not negligible especially when a dataset is relatively small We also identify that the running of Primal-Map dominates one iteration time more than 66 It is the same situation for PSUBPLR-SPARK 5 Spark employs the all-in-memory strategy and it constructs RDD on demand We implement PGDPLRSPARK and PSUBPLR-SPARK in the normal le system instead of HDFS Current results on Spark show that it is much more efìcient than Hadoop and performs better than Mahout As a distributed platform we recommend Spark to process massive 124 


data Further we recommend to choose PSUBPLRSPARK for shorter running time in exchange for a slight precision degradation As the Spark platform is still under development we expect that our running time results for PGDPLR-SPARK and PSUBPLRSPARK can still be improved 6 Another interesting point we would like to raise is about dataset size versus sparsity When comparing ECUESpam dataset and GISETTE dataset the former has higher data dimensionality but it is more sparse Comparing the data point between Figures 4 b and c we can nd that algorithms on ECUESpam dataset enjoy shorter running time per iteration than on GISETTE dataset as ECUESpam has fewer nonzero values involved in the computation However algorithms on ECUESpam dataset require more iterations than on GISETTE dataset which is because of higher data dimensionality of ECUESpam dataset This high dimensionality even causes ECUESpam to take longer execution time in total In summary to get a general sense of running time for a dataset we must consider both data volume and sparsity C Results on Cluster Size In Figures 4 d we show the running time as a function of number of nodes It is evident as prior work e.g has pointed out that due to IO and communication overheads the beneìt of adding more nodes would eventually be wahed out Though we have only employed six nodes in this study the gures already indicate speedup slows down when more nodes are used Therefore to deal with big data future work should pay more attention on reducing IO and communication overheads D Fault Tolerance There are both system level techniques and algorithm level techniques to provide fault tolerance in parallel computation Results are shown in Table VI Here randomization is Table VI F AULT T OLERANCE A NALYSIS  System Level Algorithm Level PSUBPLR-MR   PGDPLR-SPARK   PSUBPLR-SPARK   employed in all sublinear methods thus providing algorithm level fault tolerance for PSUBPLR-MR and PSUBPLRSPARK Hadoop and Spark can both provide system level fault tolerance but in different ways Hadoop employs HDFS whereas Spark has RDDs Of all six test programs PSUBPLR-MR and PSUBPLR-SPARK are fault-tolerant in both system level and algorithm level Fig 5 shows the iteration time as a function of percentage of failed maps on URL-Reputation dataset for all three   PSUBPLRäMR PGDPLRäSPARK PSUBPLRäSPARK 0 50 100 150 200 250 300 350 400 450 Number of Failed Maps Iteration Time\(after failures   1 5 10 15 30 Figure 5 Iteration time as a function of percentage of failed maps on URL-Reputation Dataset run on 6 nodes parallel test programs running on six nodes The number of iterations of PSUBPLR-MR remains unaffected when maps fail Both PGDPLR-SPARK and PSUBPLR-SPARK increase number of iterations when maps fail because of RDD reconstruction However the increase is not signiìcant In general Hadoop spends more overhead than Spark to support fault tolerance and hence Hadoop enjoys less impact during failure recovery VII C ONCLUSION In this paper we analyzed three optimization approaches along with two computing platforms to train the LR model on large-scale high-dimensional datasets for classiìcation Based on extensive experiments we summarized key features of each algorithm implemented on Hadoop and Spark We can conclude that sequential algorithms with memory intensive operations like Liblinear can perform very well if datasets can t in memory For massive datasets if limited by machine resources Mahout with its online algorithm is a good choice with a slightly lower precision If machine resources are abundant as Spark outperforms Hadoop for LR model training we recommend choosing between parallel sublinear method and parallel gradient descent method both on Spark to trade off between speedup and precision Though we used only a six-node cluster to conduct experiments our conclusions are expected to hold for larger datasets and more computing nodes We will validate this conjecture when we evaluate these algorithms in a production setting with a substantial larger cluster to deal with much larger application datasets R EFERENCES  Ron Bekk erman Mikhail Bilenk o and John Langford Scaling Up Machine Learning  Cambridge University Press 2012  Dhruba Borthakur  Hdfs architecture guide Hadoop Apache Project http://hadoop apache org/common/docs/current/hdfs design pdf  2008  Edw ard Y Chang Foundations of Large-Scale Multimedia Information Management and Retrieval  Springerverlag Berlin Heidelberg and Tsinghua University Press 2011 125 


  1  2  3  4  5  6  0  100  200  300  400  500  600        Number of Nodes Used Running Time per Iteration/s   PSUBPLR MR   PGDPLR SPARK   PSUBPLR SPARK    1  2  3  4  5  6  0  200  400  600  800  1000  1200  1400  1600  1800        Number of Nodes Used Running Time per Iteration/s     PSUBPLRäMR   PGDPLRäSPARK   PSUBPLRäSPARK    1  2  3  4  5  6  0  200  400  600  800  1000  1200        Number of Nodes Used Running Time per Iteration/s     PSUBPLRäMR  PGDPLRäSPARK   PSUBPLRäSPARK    1  2  3  4  5  6  0  200  400  600  800  1000  1200  1400  1600  1800  2000        Number of Nodes Used Running Time per Iteration/s     PSUBPLRäMR  PGDPLRäSPARK   PSUBPLRäSPARK  a 20NewsGroup  b Gisette  c ECUESpam  d URL-Reputation  Figure 4 Running time as a function of used node number  Edw ard Y Chang Kaihua Zhu Hao W ang Hongjie Bai Jian Li Zhihuan Qiu and Hang Cui Psvm Parallelizing support vector machines on distributed computers Advances in Neural Information Processing Systems  20:213Ö230 2007  W en-Y en Chen Y angqiu Song Hongjie Bai Chih-Jen Lin and E.Y Chang Parallel spectral clustering in distributed systems Pattern Analysis and Machine Intelligence IEEE Transactions on  33\(3 2011  K.L Clarkson E Hazan and D.P  W oodruf f Sublinear optimization for machine learning In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science  pages 449Ö457 IEEE Computer Society 2010  A Cotter  S  Shale v-Shw artz and N Srebro The k ernelized stochastic batch perceptron Arxiv preprint arXiv:1204.0566  2012  Jef fre y Dean and Sanjay Ghema w at Mapreduce simpliìed data processing on large clusters Communications of the ACM  51\(1 2008  S J Delan y  P  Cunningham A Tsymbal and L Co yle A case-based technique for tracking concept drift in spam ltering Knowledge-Based Systems  18\(4Ö5 2005  Rong-En F an Kai-W ei Chang Cho-Jui Hsieh Xiang-Rui Wang and Chih-Jen Lin Liblinear A library for large linear classiìcation The Journal of Machine Learning Research  9:1871Ö1874 2008  D Garber and E Hazan Approximating semideìnite programs in sublinear time In Advances in Neural Information Processing Systems  2011  I Guyon S Gunn A Ben-Hur  and G Dror  Result analysis of the nips 2003 feature selection challenge Advances in Neural Information Processing Systems  17:545Ö552 2004  T  Hastie R T ishirani and J Friedman The Elements of Statistical Learning Data Mining Inference and Prediction  Springer-Verlag New York 2001  E Hazan and T  K oren Optimal algorithms for ridge and lasso regression with partially observed attributes Arxiv preprint arXiv:1108.4559  2011  E Hazan T  K oren and N Srebro Beating sgd Learning svms in sublinear time In Advances in Neural Information Processing Systems  2011  Hao yuan Li Y i W ang Dong Zhang Ming Zhang and Edward Y Chang Pfp parallel fp-growth for query recommendation In Proceedings of the 2008 ACM conference on Recommender systems  RecSys 08 pages 107Ö114 ACM 2008  Zhiyuan Liu Y uzhou Zhang Edw ard Y  Chang and Maosong Sun Plda Parallel latent dirichlet allocation with data placement and pipeline processing ACM Trans Intell Syst Technol  2\(3 May 2011  Justin Ma La wrence K Saul Stef an Sa v age and Geof fre y M Voelker Identifying suspicious urls an application of largescale online learning In Proceedings of the 26th Annual International Conference on Machine Learning  pages 681 688 ACM 2009  Apache Mahout Scalable machine-learning and data-mining library available at mahout apache org   Haoruo Peng Zhengyu W ang Edw ard Y Chang Shuchang Zhou and Zhihua Zhang Sublinear algorithms for penalized logistic regression in massive datasets In Machine Learning and Knowledge Discovery in Databases  pages 553Ö568 Springer 2012  Badrul Sarw ar  Geor ge Karypis Joseph K onstan and John Riedl Item-based collaborative ltering recommendation algorithms In Proceedings of the 10th international conference on World Wide Web  pages 285Ö295 ACM 2001  T o m White Hadoop The deìnitive guide  OêReilly Media Inc 2012  Matei Zaharia Mosharaf Cho wdhury  Michael J Franklin Scott Shenker and Ion Stoica Spark cluster computing with working sets In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing  pages 10Ö10 2010  T  Zhang Solving lar ge scale linear prediction problems using stochastic gradient descent algorithms In Proceedings of the twenty-ìrst international conference on Machine learning  page 116 ACM 2004 126 


  9 NASA AIRS and Suomi NPP science teams and the NPOESS Sounding Operational Algorithm Team. He is the Principal Investigator on the MicroMAS \(Micro-sized Microwave Atmospheric Satellite\ program, comprising a high-performance passive microwave spectrometer hosted on a 3U cubesat planned for launch in 2013.\302  He was previously the Integrated Program Office Sensor Scientist for the Advanced Technology Microwave Sounder on the Suomi National Polar Partnership that launched in 2011 and the Atmospheric Algorithm Development Team Leader for the NPOESS Microwave Imager/Sounder. Dr. Blackwell received the 2009 NOAA David Johnson Award for his work in neural network retrievals and microwave calibration and is co-author of Neural Networks in Atmospheric Remote Sensing published by Artech House in July, 2009.\302  He received a poster award at the 12th Specialist Meeting on Microwave Radiometry and Remote Sensing of the Environment in March 2012 for ``Design and Analysis of a Hyperspectral Microwave Receiver Subsystem'' and was selected as a 2012 recipient of the IEEE Region 1 Managerial Excellence in an Engineering Organization Award ``for outstanding leadership of the multidisciplinary technical team developing innovative future microwave remote sensing systems  Paul E. Racette has been the principal engineer responsible for the overall instrument concept development and deployment of highly-innovative remote sensing instruments. Each of these instruments has produced unique, scientifically rich data. Paul has participated in more than fifteen major field experiments around the world pioneering techniques to observe the Earth. As a member of the senior technical staff at Goddard, he has initiated technology developments research projects, and international collaborations that have advanced the state of th e art in microwave remote sensing and instrument calibration. For these efforts and accomplishments Paul recei ved the NASA Medal for Exceptional Service and was the first recipient of Goddard\222s Engineering Achievement Award established to publicly recognize Goddard\222s highest achieving engineers. In 2005 he completed the requirements for his Doctor of Science in elect rical engineering from The George Washington Universi ty. Recognizing the critical needs in education and a desire to seek new adventures Paul applied and was accepted into the NASA Administrator\222s Fellowship Program. As a NAFP fellow he returned to his home state to serve as a guest faculty at the Haskell Indian Nations University during the 2005 \226 2006 academic year Paul recently completed the s econd year of his fellowship working at NASA Headquarters as Special Assistant to the Deputy Assistant Administrator in the Office of Education  Paul is highly commited to serving the public through professional activities. Paul has served the IEEE in many capacities including secretary of the University of Kansas\222 IEEE student chapter, the Geoscience and Remote Sensing Society\222s New Technology Directions Committee Representative, Chair of the Instrumentation and Future Technologies Committee, and Professional Activities Committee for Engineers Representative. He now serves as Editor-In-Chief for Earthzine  Christopher J. Galbraith is a member of the Technical Staff at MIT Lincoln Laboratory in the RF and Quantum System s group where he develops microwave circuits for communications, radar, and radiometric systems, small form-factor packaging and antennas, and superconducting electronics  He  received the B.S.E.E., M.S.E.E. and Ph.D degrees from the University of Michigan, Ann Arbor. During the summers of 2001 and 2002, he was an intern with TRW Space and Electronics, Redondo Beach, CA, where he worked on satellite communications syst ems and microwave circuit design. He is active in the IEEE Microwave Theory and Techniques society \(MTT-S\ where he currently serves as the chair of the Boston chapter   Erik Thompson Assistant Staff at MIT Lincoln Laboratory. He r eceived a B.E. in Electrical Engineering from Stevens Ins titute of Technology. As a Stevens student T hompson was selected as the Cooperative Education and Internship Student of the Year award by the New Jersey Cooperative Education and Internship Association \(NJCEIA As an undergraduate at Stevens, Thompson took part in five Co-op internships. The first two assignments were with Datascope Patient Monitors, where he worked with the electrical engineering staff to test hospital products and implement fixes. Next, he worked as a computer engineer at the Armament Research, Development and 


  10 Engineering Center \(ARDEC\ at Picatinny Arsensal Finally, Thompson spent two semesters at Safe Flight Instrument Corporation. There, he served as project lead for the development of co ckpit sensors that prevent airplanes from stalling. He was primarily responsible for overseeing the design and testing of software and electronics systems    


  11  


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440Ö442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


