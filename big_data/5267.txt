Toward a Uni\002ed Object Storage Foundation for Scalable Storage Systems Cengiz Karakoyunlu University of Connecticut Storrs CT USA cengiz.k@uconn.edu Dries Kimpe Philip Carns Kevin Harms Robert Ross Argonne National Laboratory Argonne IL USA f dkimpe,carns,rross g mcs.anl.gov harms@alcf.anl.gov Lee Ward Sandia National Laboratories Albuquerque NM USA lee@sandia.gov Abstract 227Distributed object-based storage models are an increasingly popular alternative to traditional block-based or 002le-based storage abstractions in large-scale storage systems Object-based storage models store and access data in discrete byte-addressable containers to simplify data management and cleanly decouple storage systems from underlying hardware resources Although many large-scale storage systems share common goals of performance scalability and fault tolerance their underlying object storage models are typically tailored to speci\002c use cases and semantics making it dif\002cult to reuse them in other environments and leading to unnecessary fragmentation of datacenter storage facilities In this paper we investigate a number of popular data models used in cloud storage big data and high-performance computing 050HPC\051 storage and describe the unique features that distinguish them We then describe three representative use cases-a POSIX 002le system name space a column-oriented key/value database and an HPC application checkpoint-and investigate the storage functionality they require We also describe our proposed data model and show how our approach provides a uni\002ed solution for the previously described use cases I I NTRODUCTION Storage technology has improved rapidly particularly in terms of storage density but storage throughput has not kept pace with advances in computational performance This trend has led to increased demand for large-scale storage systems that aggregate and coordinate many storage devices in turn driving the need for better abstractions to manage those storage devices Object-based storage 2 has emer ged as a strong competitor for the block-based model quickly becoming a popular underlying model for referencing and accessing data distributed over large numbers of storage devices in these systems An object is an ordered logical collection of bytes with a numerical identi\002er Objects consist of data attributes describing the object such as QoS attribute and devicemanaged metadata such as security information 3 Objects have variable sizes and can be used to store any kind of data The object storage model abstracts away a variety of resource-speci\002c management tasks such as block allocation space management and various forms of atomicity However it still allows considerable 003exibility for a variety of higherlevel data models to be built atop it Although object models Fig 1 Example deployment scenario in which big data cloud storage and HPC data models share the same storage pool via a uni\002ed object storage abstraction were originally envisioned as a device-level interface today's large-scale storage systems more commonly repurpose the object model as a software interface atop a variety of storage substrates 5 6 7 Although several object-based storage models have been implemented and used as the basis for the popular storage and 002le systems 9 7 3 e xisting object-based storage models are typically tailored to a particular use case or data model making them dif\002cult to reuse in other contexts This situation also makes it dif\002cult to share a common storage pool for different big data cloud storage or HPC storage tasks increasing management overhead and adding complexity to the task of storage provisioning for facilities with diverse storage needs Ideally each data model would coexist using a shared object storage foundation as shown in Figure 1 To address this problem we 002rst identify some of the most popular large-scale data models in use today The following list divides them into four categories with representative examples 017 Parallel 002le systems  Lustre GPFS 11 Panasas PVFS 12 Ceph 8 017 Cloud object storage  Amazon S3 Swift 14 Rados Gateway 15 978-1-4799-0898-1/13/$31.00 c 015 2013 IEEE 


017 MapReduce  Google File System 050GFS\051 Hadoop HDFS 017 Key/value stores  Dynamo Redis 19 Hyper dex Cassandra 21 HBase 22 BigQuery 23 Note that these data models aren't necessarily mutually exclusive For example several parallel 002le systems have been extended to support MapReduce workloads We will refer to these classi\002cations in the remainder of the paper for clarity however in order to simplify the discussion of use cases and requirements that are shared across groups of storage systems TABLE 1 Requirements for popular scalable storage data models   Shared Requirements  Distinguishing Requirements    High Performance  Scalability  Fault Tolerance  Concurrent Read Access  Concurrent Write Access  Synchronization Primitives  Atomicity  Compute/Storage Locality  Record Oriented Access    Parallel File System  3  3  3  3  3  3  3      Cloud Object Storage  3  3  3  3    3      MapReduce  3  3  3  3     3     Key/Value Store  3  3  3  3  3  3  3   3   Table 1 breaks down the large-scale storage data models in terms of core requirements The 002rst four are general requirements that are shared across all such data models Concurrent write access refers to the ability to have multiple processes write simultaneously to the same 002le object or database Synchronization primitives are features such as 002le locking or conditional operations 25 that allo w multiple processes to explicitly coordinate concurrent writes Atomicity is the ability to modify data such that the write is applied in its entirety or not at all The granularity of atomicity can vary widely across data models For example cloud object storage systems may offer object-granular atomicity key/value stores may offer per-key granularity and 002le systems may not offer atomicity at all except in the directory name space Locality allows applications to execute on server nodes with local copies of data relevant to computation Record-oriented access is needed for storage systems that refer to units of data in terms of opaque keys rather than ranges of bytes In this paper we propose a new object-based storage API known as the Advanced Storage Group 050ASG\051 interface that seeks to unify features necessary to support the data models outlined above without compromising usability or limiting implementation 003exibility The contributions of this paper are as follows 017 Identify the requirements that differentiate four key largescale data storage models 017 Propose a new object storage API that uni\002es the features necessary to meet those requirements 017 Present a set of case studies that evaluate how the proposed API would be used as a foundation for a diverse set of storage constructs The rest of the paper is organized as follows Section II presents example drivers for this work Section III describes the proposed ASG API and how it can be used to implement the use cases given in Section II Section IV shows how our approach presents a uni\002ed solution for the use cases described in Section II Section V reviews related work in object-based storage systems Section VI summarizes our 002ndings and presents potential avenues for future work II M OTIVATION In this section we discuss a number of common storage uses that serve as one of the drivers for our work A Implementing POSIX Directories In a POSIX 002le system data 002les are located by looking them up by name in a directory POSIX directories have the following properties First creating or removing a 002le 050or subdirectory\051 in a directory is an atomic operation and duplicate entries are not allowed If multiple processes try to create or remove the same entry at the same time exactly one of them will succeed Second a directory entry has associated metadata for example the last access time or the size In addition to create and remove three other operations are possible on a directory opening 050lookup\051 of a name updating the metadata associated with a name and renaming an entry to a new name These operations are atomic as well As soon as an update completes all processes in the system see the updated information at any time before that the old information is preserved At no point will a lookup return a blend of old and new metadata The same is true for rename Either the old name will be in the directory or the new name but not both Existing object storage models typically do not directly support directory primitives nor do they support operations designed to implement structures and synchronization required for implementing a POSIX directory Consequently most data models requiring directory like indexes implement this functionality by using additional services 050for example metadata servers\051 using the object storage only for the actual 002le data B Column-Oriented Key/Value Store A column-oriented database differs from a traditional database in that records are stored in column order rather than row order as shown in Table 2 Data for a database entry is stored in a column each row stores the same data 002eld of a database entry and shards 050horizontal partitions of a database\051 represent a collection of rows This organization improves the performance of analysis-oriented workloads in which ad hoc queries are performed over all values in a column A columnoriented database will generate large contiguous disk access patterns in this case because there is no need to skip over interleaved column data for each entry In addition each row typically has a large number of columns and not every row needs to have the same set of columns Most column-oriented databases allow the creation of new columns at any time simply by writing to them Existing object storage models generally do not support column-oriented databases since record functionality is missing and applications are forced to manage the storage space 


TABLE 2 Example organization of a column-oriented key value store  Column 0  Column 1  Column 2  Column 3   Shard 1  Row 0  Alice  Bob  Brad  Charles   Row 1  Smith    Spring\002eld   Shard 2  Row 0  111-1111   144-1144  321-4321   Fig 2 Example of an HPC application writing in parallel to a replicated object As an example in T10 if each attrib ute represents a cell of the column-oriented database grouping certain attributes to represent the rows and columns of a column-oriented database is not an easy task since mapping attributes to rows and columns and keeping track of mapping information are challenging C HPC Application Checkpoint HPC application workloads are characterized by bursty highly concurrent write-intensive I/O patterns In par ticular many scienti\002c simulations periodically write checkpoint data for application resilience In these scenarios all application processes typically write simultaneously to the same shared data set as shown in Figure 2 Although the application processes are coordinated and do not generally write to overlapping byte ranges in the 002le the access patterns may be highly interleaved and are not necessarily block aligned Optimizations such as two-phase I/O and I/O forwarding can be used to mitig ate t h e le v el of concur rency observed by the storage system but data must still be written by many processes in order to leverage enough I/O paths to meet bandwidth requirements Metadata overhead and high concurrency are the key challanges for this type of concurrent write access pattern Existing data models tried N 000 N and N 000 1 checkpointing strategies N 000 N checkpointing is the case where each process writes to a separate checkpoint 002le and N 000 1 checkpointing is the case where all the processes write to a shared 002le Both checkpoint patterns pose challenges N 000 1 checkpointing suffers from limited bandwidth since all processes are trying to concurrently write to the same 002le On the other hand N 000 N checkpointing creates a lot of 002les increasing the metadata overhead III P ROPOSED API AND S TORAGE M ODEL In this section we describe the ASG storage model its fundamental building blocks and basic operations Fig 3 Architecture of the ASG Storage Model A Architecture The main architecture of the ASG storage model is shown in Figure 3 The core concepts are described as follows 017 The basic building block of the ASG storage model is a record  Each record consists of a key a version number data and length of data The key version number and length of data are represented with integers whereas data is an array of bytes of variable length Key is the numerical identi\002er of a record Version numbers are used to order the write operations to a record The data 002eld can be empty a record at its initial condition will have version number zero and will contain no data The records are not explicitly created in the ASG storage model they already exist in the system at their initial conditions before they are manipulated by the ASG storage model operations 017 A fork is a collection of records forming a distinct namespace for the records it contains Each fork is identi\002ed by an integer Forks allow related collections of data 050for example indexes metadata or header inforation\051 to be stored alongside the primary data stream for an object with the same security locality and atomicity 017 An object is a collection of forks It provides a distinct namespace for the forks it contains Each object is identi\002ed by an integer 017 A container is a collection of objects It provides a distinct namespace for the objects it contains Each container is identi\002ed by an integer Containers partition the storage system into logical units with different security domains as an example each container could contain a distinct 002le system The record fork and object identi\002ers in the ASG storage model are not global For example two different containers can have objects with the same identi\002ers Similarly two different objects or forks can have forks or records with the same identi\002ers There can be a maximum of 2 64 objects in a container 2 64 forks in an object and 2 64 records in a fork in the ASG storage model giving many options to translate the applications described in Section II to the storage model 


B Operations In this section we describe the ASG storage model operations a client can use to interact with the storage system All of the ASG storage model operations are atomic meaning that they either fully complete or have no effect at all which satis\002es the atomicity requirement in Table 1 1\051 write The write operation stores data in a sequential range of records The input arguments to this function are location information 050container object fork and starting record identi\002ers\051 local buffer that stores the data to be written number of records to be modi\002ed 050range of the write operation\051 conditional 003ag and user-speci\002ed version number The conditional 003ag can be set to one of the following four values in order to control the semantics of the write operation with respect to per-record version numbers 017 NONE  Write should succeed without checking any version number 017 ALL  Write should succeed only if the user-speci\002ed version number is greater than all the version numbers in the user-speci\002ed range 017 UNTIL  Write should continue until it comes across a record that has a version number greater than or equal to the user-speci\002ed version number 017 AUTO  In this case the user-speci\002ed version number can be ignored The biggest version number existing in the user-speci\002ed range is found and incremented and the new data is written with this incremented version number The input data of the write operation is divided into the same number of chunks as the number of records in the userspeci\002ed range The same length of data is written to each record in this range When the write operation is completed successfully it returns the size of the written data and the newly assigned version number 2\051 read The read operation retrieves data from a sequential range of records The input arguments to the read operation are location information 050container object fork and starting record identi\002ers\051 local buffer to store the read data number of records to be read conditional 003ag and user-speci\002ed version number The range in the read operation is identical to the range de\002ned in the write operation The conditional 003ag of the read operation can be set to one of the following three values 017 NONE  Read should succeed without checking any version number or conditional 003ags 017 ALL  Read should succeed only if the user-speci\002ed version number is greater than all the version numbers existing in the user-speci\002ed range 017 UNTIL  Read should continue until it comes across a record that has a version number greater than or equal to the user-speci\002ed version number When the read operation is completed successfully it returns the number of the records read in addition to the version number of these records 3\051 reset The reset operation returns an entity 050container object fork or record\051 back to its initial condition  In an entity at its original condition all the records will have version number zero and will contain no data The reset operation can work on any entity of the ASG storage model The reset operation takes in the identi\002er information of the entity to be reset as an input argument and it also supports conditional execution based on the existing version number and given conditional 003ag The conditional 003ags that can be used with the reset operation are the same as the conditional 003ags used in the read operation When the reset operation is completed successfully it returns the number of entities reset 4\051 probe The probe operation can be used to iteratively enumerate containers within a storage system objects within a container forks within an object or records within a fork The probe operation can work on any entity of the ASG storage model The probe operation takes in the identi\002er information of the entity 050container object or fork\051 to be probed as an input argument entity id to start with local buffer to store the retrieved information and maximum number of entities for which the information will be retrieved The probe operation returns various information about an ASG entity such as the range of existing records in that entity their version numbers and sizes C Relation to data model requirements In this section we show how the features provided by the ASG storage model make it possible to meet the requirements of the common data models listed in Table 1 We note that none of these features are new the ASG storage model just presents a reusable uni\002ed API bringing these features together while minimizing complexity The features provided by the ASG storage model and how they meet the requirements of common data models can be summarized as follows 017 Uni\002ed byte stream and key/value storage  The ASG storage model supports both byte-stream and key/value-based storage Each byte is stored as a one-byte record With the numerical identi\002ers and record contents each record can be also used as a key/value store As a result the ASG storage model supports both 002le-based and key/value-based access models and also enables record-oriented access for both of these models 017 Eliminating object attributes  Object-based storage models such as T10 use attrib utes to describe the objects meaning that object attributes are used to store metadata In the ASG storage model we still have metadata describing the records however we do not store the metadata in separate attributes as is normally done in object-based storage models Metadata can be stored in dedicated forks giving the opportunity to store data and metadata together to treat metadata in the same way as data and to have simple metadata management by alleviating the need for a separate metadata API Simpli\002ed metadata management reduces the metadata overhead and improves scalability  


017 Record versioning  Versioning in the ASG storage model enables sorting writes to a record as shown in previous studies 32 As the v ersion number changes with each write operation highly concurrent write operations will be consistent and the performance of the system will increase 017 Conditional operations  The conditional read and write operation 003ags provide synchronization primitives and atomicity in a data model as has been shown in a few storage models 33 18 19 20 Using the conditional 003ags with the ASG storage model operations multiple processes can coordinate concurrent writes without using any explicit locking method Using conditional 003ags also ensures that each ASG operation either fully completes or has no effect making sure the system does not end up in an inconsistent state and that fault tolerance is achieved 017 Independently addressable records  ASG is a recordoriented storage model similar to some other data models 35 Each entity in the ASG storage model has a numerical identi\002er when ASG primitives access a record they explicitly use the identi\002ers of the enclosing container object and fork along with the identi\002er of that record As a result each record in ASG has a distinct location and is independently accessable Records are the smallest units of storage an operation can access in the ASG storage model Having access to independently addressable records also makes it possible to support concurrent read and write access on them 017 Fork structure  Forks can be used to store metadata as discussed previously In addition they can be used to group records that store related data together  This approach impro v es performance by simplifying data management and enables collecting provenance from related records in an ef\002cient way to support fault tolerance  017 Server location  The ASG storage model exposes location information of its entities to higher-level applications Applications either can take control of the server location of ASG entities by using this information or they can let the storage system to handle localization and choose ASG entities randomly without worrying about server locations When the applications decide which ASG entities to use they can move computation closer to storage  IV E XAMPLE U SE C ASES In this section we show how ASG storage model can be used as a foundation for three example use cases A Implementing POSIX Directories In our 002rst use case we show how the ASG storage model features can be used to implement POSIX directory operations Entries in a directory can be represented with the ASG records as shown in Figure 4 In order to map a directory entry to an ASG record the name of the directory entry can be hashed Fig 4 Mapping directory entries to ASG entities into a record key The name inode information and data of the directory entry can be stored together in the ASG record Since each ASG record is independently addressable the uniqueness requirement of each entry in a directory can be satis\002ed Indeed one can implement POSIX directory operations using the ASG storage model We note that other namespace and directory implementations also can be supported by using the ASG storage model even though we show a POSIX directory implementation in this section In order to create a new 002le 050or subdirectory\051 in a directory an underlying ASG write operation is called with conditional 003ags Similarly in order to remove a 002le 050or subdirectory\051 from a directory underlying ASG read and reset operations are called with conditional 003ags The ASG read operation returns the version number of the ASG record representing a directory entry Checking the version number returned by the ASG read operation ASG write does not create a directory entry if it already has been created 050nonzero version number\051 and ASG reset does not remove a directory entry if it has not been created yet 050zero version number\051 As a result the ASG storage model ensures the atomicity of the POSIX create and remove operations and it prevents duplicate directory entries If multiple processes try to create or remove a directory entry at the same time only one of them succeeds Other POSIX directory operations such as updating the metadata of a directory entry and renaming an entry can be also supported by using the ASG storage model In order to update the metadata of a directory entry or rename a directory entry ASG read and write operations are called with conditional 003ags Again the ASG read operation returns the version number of the ASG record representing a directory entry The ASG write does not update the metadata of a directory entry if it has not been created yet 050zero version number\051 While renaming a directory entry ASG write creates the new directory entry with no conditional 003ags meaning that it overwrites the new entry if it already exists and ASG reset removes the old entry as soon as the new entry is successfully created As a result the ASG storage model ensures the atomicity of the POSIX update and rename operations All processes in the system see the updated metadata information as soon as update is done and they see the old metadata information at any time before update completes No process sees old and new metadata at the same time For the rename operation either the old or the new directory entry exists not both 


In order to lookup a directory entry or to stat a directory ASG read and probe operations are called Similar to the previous POSIX operation implementations the ASG read operation returns the version number of the ASG record representing a directory entry This version number is not important for the lookup operation which returns any data available in the entry at time it was called For the stat operation however ASG probe keeps track of this version number hence if the directory is modi\002ed while the stat operation is not done yet ASG probe identi\002es modi\002ed entries and returns updated information about them as a result of the stat operation Using conditional operations the ASG storage model ensures the atomicity of the lookup and stat operations by returning updated information with them at no point is old and new information for an entry returned together B Column-Oriented Key/Value Store Table 3 shows an example of how a column-oriented key/value database might be expressed using ASG primitives Rows are represented as ASG records columns are represented as ASG forks and shards are represented as ASG objects ASG records are variable-sized and any value in the database can be referenced by a unique f object ID fork ID record ID g triple Since ASG write operation can take zero-length data as input rows can have empty columns in the database TABLE 3 Example organization of a column-oriented key value store using the ASG storage model  Column:fork 0  Column:fork 1  Column:fork 2  Column:fork 3   Shard:object 1  Row:record 0  Alice  Bob  Brad  Charles   Row:record 1  Smith    Spring\002eld   Shard:object 2  Row:record 0  111-1111   144-1144  321-4321   Columns map well to forks in this example because the fork construct allows each column to be addressed independently while still ensuring that all records within a row are stored in the same object An entire row can therefore be accessed 050or added or removed\051 atomically The ASG object storage model does not dictate on-disk layout but it is expected that the storage system would organize data on disk such that forks are contiguous in this case Shards map naturally to objects because they partition the data set into discrete chunks that can be used to parallelize column-oriented queries if objects are distributed across different servers Forks and variable-sized records provided by the ASG interface are critical to expressing this use case Without these features a column-oriented key/value storage system would be forced to maintain an additional mapping index to translate between row column nomenclature and offset and size nomenclature This translation layer not only would add complexity for the data model implementor it would also prevent critical semantic information from being expressed to the storage system An ASG-based storage system for example may recognize a linear column-oriented access pattern and adapt its underlying storage layout accordingly while a traditional storage system making the same optimization would have to do so based on assumptions derived from generic byte range access patterns The ASG probe function also leverages the additional structured data semantic information provided by fork and record-oriented access to enable ef\002cient enumeration of both the rows and columns of a table C HPC Application Checkpoint Implementing HPC checkpointing strategies directly on top of the ASG storage model is straightforward because of its structure and explicit location control feature One can implement both N 000 N and N 000 1 checkpointing strategies using the ASG storage model and thus overcome the limitations of these methods as explained in Section II-C In the N 000 N checkpointing method each process writes to a separate checkpoint 002le As explained in Section III-C the ASG storage model exposes location information of its entities to higher-level applications Therefore any application trying to implement N 000 N checkpointing method can take advantage of this information to pick ASG entities that will store checkpoint data in a manner to balance the metadata load across the system without dealing with any dedicated location servers Additionally as explained in Section III-C object attributes are eliminated in the ASG storage model and as a result metadata management is simpli\002ed Having simple metadata management and explicit location control reduces the metadata overhead in the N 000 N checkpointing method If the N 000 1 checkpointing method is implemented each process writes to a shared checkpoint 002le As explained in Section III-C the ASG storage model has record versioning and conditional operation features Therefore processes trying to write to a checkpoint 002le concurrently can take advantage of record versioning and conditional operations to order their writes to the checkpoint 002le This strategy alleviates the need to set locks on the checkpoint 002le since it is possible to update the 002le atomically using the ASG storage model operations As a result having record versioning and conditional operations makes it possible to have highly concurrent writes to the checkpoint 002le in the N 000 1 checkpointing method V R ELATED W ORK Network-Attached Secure Disk 050NASD\051 is the primary work on object-based storage and it led to speci\002cations of standards for object-based storage N ASD introduces variable-length objects with attributes rather than 002xed-length traditional blocks to enable self-management and to obviate the need to know about the host operating system Moving data management to the storage disks increases the networking security and space management capabilities Numerous studies in the literature have similar scope to our work OSD 39 presents a model similar to t he one speci\002ed by the OSD standard e xcept for the addition of dedicated directory objects The directory objects in OSD store 002le names and attributes and support metadata-related operations The Panasas File System is b uilt on objectbased storage devices The OSD wire protocol of Panasas 


uses the operations from the OSD standard to enable byteoriented access to data to manipulate attributes and to create or delete objects Lustre 40 is a distrib uted 002le system based on object-based storage The object storage server in Lustre is responsible for providing access to 002le data stored in objects on object storage targets Ursa Minor is a parallel 002le system that supports versioned writes It keeps the existing object-storage interface mostly intact e xcept for introducing slices 050i.e fragments of object data\051 and it uses timestamps to distinguish different versions of data Datamods is a frame w ork that e xploits e xisting lar gescale storage system services to support complex data models and interfaces Datamods avoids duplicating services already provided in distributed storage systems in middleware and improves scalability since it is not limited to a single dimension at the 002le level Rados Gateway 15 is an object storage service forming the foundation of Ceph It pro vides the clients a single logical object store and of\003oads object replication failure detection and data management tasks to the underlying object store daemons The Ohio Supercomputing Center looked at mapping Parallel Virtual File System on top of an existing object-based storage emulation 5 This mapping moved the functionality of the common components of a traditional storage system such as I/O directory or metadata servers to OSDs and improved the performance of the overall system thanks to the capabilities of the objectbased storage devices 43 44 VSAM 35 supports both 002xed-sized and variable-sized records depending on the application Forks in NTFS are simil ar to records in the ASG storage model they are byte streams storing 002le data and auxiliary information such as metadata and security settings Conditional operations are used in Amazon SimpleDB Amazon DynamoDB Redis 19 and Hyperde x 20 A number of studies form the technical basis of our work Transactional Object Storage Device 050TOSD\051 sho ws that object-based storage is a common component of many parallel 002le systems and it introduces three optimizations to the object-based storage model in order to serve highly concurrent workloads better atomicity versioning and commutativity Goodell et al e xtended the POSIX API by or g anizing the storage around data objects in order to map complex data structures to these data objects and have direct access between the data objects and applications Carns et al in v estig ated conditional update operations as an alternative to distributed pessimistic locking operations in object-based storage systems VI C ONCLUSIONS AND F UTURE W ORK In this paper we presented a new object-based storage model ASG introduced its architecture and primitives and described a couple of use cases based on this model As the use cases clearly show the ASG storage model is 003exible and can act as a starting point for building complex storage applications Features supported by the ASG storage model make it possible to support requirements of common data models A CKNOWLEDGMENTS The authors thank the following people Matthew Curry Geoff Danielson Ruth Klundt and Justin Wozniak This material is based on work supported by the U.S Department of Energy's Oak Ridge National Laboratory and included the Extreme Scale Systems Center located at ORNL and funded by the DoD in part by contract number 4000111689 224Novel Software Storage Architectures\224 This work also was supported by U.S Department of Energy under contract DEAC02-06CH11357 R EFERENCES   M Mesnier G Ganger and E Riedel 223Object-based Storage,\224 Communications Magazine IEEE  vol 41 no 8 pp 84 226 90 aug 2003   T10 Technical Committee of the InterNational Committee on Information Technology Standards 223Object-based Storage Devices 3 050OSD-3\051.\224 A v ailable http://www t10.or g   B Welch M Unangst Z Abbasi G Gibson B Mueller J Small J Zelenka and B Zhou 223Scalable Performance of the Panasas Parallel File System,\224 in In FAST-2008 6th Usenix Conference on File and Storage Technologies  2008 pp 17\22633   A Devulapalli D Dalessandro P Wyckoff and N Ali 223Attribute Storage Design for Object-based Storage Devices,\224 in MSST 07 Proceedings of the 24th IEEE Conference on Mass Storage Systems and Technologies  Washington DC IEEE Computer Society 2007 pp 263\226268   A Devulapalli and N Ali 223Integrating Parallel File Systems with Object-based Storage Devices,\224 in Proceedings of Supercomputing  2007   223Librados API documentation.\224 A v ailable http://ceph.com docs/master/api/librados   Oracle Corporation 223Lustre File System.\224 A v ailable http://wiki.lustre.org   S A Weil S A Brandt E L Miller D D E Long and C Maltzahn 223Ceph A Scalable High-Performance Distributed File System,\224 in Proceedings of the 7th Symposium on Operating Systems Design and Implementation 050OSDI  2006 pp 307\226320   D Beaver S Kumar H C Li J Sobel and P Vajgel 223Finding a Needle in Haystack Facebook's Photo Storage,\224 in Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation  ser OSDI'10 Berkeley CA USA USENIX Association 2010 pp 1\2268 A v ailable http dl.acm.org/citation.cfm?id=1924943.1924947   223Lustre A Scalable High-Performance File System,\224 Cluster File Systems Inc white paper version 1.0 November 2002 Available http://www.lustre.org/docs/whitepaper.pdf   F Schmuck and R Haskin 223GPFS A Shared-Disk File System for Large Computing Clusters,\224 in Proceedings of the 1st USENIX Conference on File and Storage Technologies  ser FAST 02 Berkeley CA USA USENIX Association 2002 A v ailable http://dl.acm.org/citation.cfm?id=1083323.1083349   P H Carns W B Ligon III R B Ross and R Thakur 223PVFS A Parallel File System for Linux Clusters,\224 in Proceedings of the 4th Annual Linux Showcase and Conference  Atlanta GA USENIX Association October 2000 pp 317\226327 A v ailable http://www.mcs.anl.gov 030 thakur/papers/pvfs.ps   223Amazon Simple Storage System 050Amazon S3\051.\224 A v ailable http://aws.amazon.com/s3   L.-F Cabrera and D D E Long 223Swift A Storage Architecture for Large Objects,\224 U.C Santa Cruz Tech Rep UCSC-CRL-89-04 1990  A v ailable ftp://ftp.cse.ucsc.edu/pub/tr/ucsccrl8904.tar Z   S A Weil A Leung S A Brandt and C Maltzahn 223RADOS A Fast Scalable and Reliable Storage Service for Petabyte-scale Storage Clusters,\224 in Proceedings of the 2007 ACM Petascale Data Storage Workshop 050PDSW 07\051  Reno NV Nov 2007   S Ghemawat H Gobioff and S.-T Leung 223The Google File System,\224 in Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles  Bolton Landing NY ACM Press October 2003 pp 96\226108 A v ailable http://www cs.rochester edu/sosp2003 papers/p125-ghemawat.pdf 


  K Shvachko H Kuang S Radia and R Chansler 223The Hadoop Distributed File System,\224 in Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies 050MSST\051  ser MSST 10 Washington DC USA IEEE Computer Society 2010 pp 1\22610 A v ailable http://dx.doi.or g/10.1109/MSST 2010 5496972   223Amazon DynamoDB.\224 A v ailable http://a ws.amazon.com dynamodb   223Redis.\224 A v ailable http://redis.io   223Hyperdex A Searchable Distributed Key-Value Store.\224 Available http://hyperdex.org   A Lakshman and P Malik 223Cassandra A Decentralized Structured Storage System,\224 SIGOPS Oper Syst Rev  vol 44 no 2 pp 35\22640 Apr 2010 A v ailable http://doi.acm.or g/10.1145/1773912 1773922   L George HBase The De\002nitive Guide  2011 A v ailable http://proquest.safaribooksonline.com/9781449314682   223BigQuery.\224 A v ai lable https://de v elopers.google.com bigquery   D A Menasce and R R Muntz 223Locking and Deadlock Detection in Distributed Data Bases,\224 IEEE Trans Softw Eng  vol 5 no 3 pp 195\226202 May 1979 A v ailable http://dx.doi.org/10.1109/TSE.1979.234181   P Carns K Harms D Kimpe J M Wozniak R Ross L Ward M Curry R Klundt G Danielson C Karakoyunlu J Chandy B Settlemyer and W Gropp 223A Case for Optimistic Coordination in HPC Storage Systems,\224 in Proceedings of 2012 Parallel Data Storage Workshop 050PDSW 2012\051  IEEE 2012   B Atikoglu Y Xu E Frachtenberg S Jiang and M Paleczny 223Workload Analysis of a Large-Scale Key-Value Store,\224 in Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems  ser SIGMETRICS 12 New York NY ACM 2012 pp 53\22664 Available http://doi.acm.org/10.1145/2254756.2254766   J M del Rosario R Bordawekar and A Choudhary 223Improved Parallel I/O via a Two-Phase Run-time Access Strategy,\224 in Proceedings of the IPPS 93 Workshop on Input/Output in Parallel Computer Systems  Newport Beach CA 1993 pp 56\22670 also published in Computer Architecture News 21\0505\051 December 1993 pages 31\22638   N Ali P Carns K Iskra D Kimpe S Lang R Latham R Ross L Ward and P Sadayappan 223Scalable I/O Forwarding Framework for High-Performance Computing Systems,\224 in Proceedings of IEEE Conference on Cluster Computing  New Orleans LA September 2009   J Bent G Gibson G Grider B McClelland P Nowoczynski J Nunez M Polte and M Wingate 223PLFS A Checkpoint Filesystem for Parallel Applications,\224 in SC 09 Proceedings of the Conference on High Performance Computing Networking Storage and Analysis  New York ACM 2009 pp 1\22612   N Nieuwejaar and D Kotz 223The Galley Parallel File System,\224 in Proceedings of the 10th ACM International Conference on Supercomputing  Philadelphia ACM Press May 1996 pp 374\226 381 A v ailable http://www cs.dartmouth.edu 030 dfk/papers nieuwejaar:galley.ps.gz   M Abd-El-Malek W V Courtright II C Cranor G R Ganger J Hendricks A J Klosterman M Mesnier M Prasad B Salmon R R Sambasivan S Sinnamohideen J D Strunk E Thereska M Wachs and J J Wylie 223Ursa Minor Versatile Cluster-based Storage,\224 in FAST'05 Proceedings of the 4th Conference on USENIX Conference on File and Storage Technologies  Berkeley CA USENIX Association 2005 pp 5\2265   P Carns R Ross and S Lang 223Object Storage Semantics for Replicated Concurrent-Writer File Systems,\224 in Proceedings of 2010 Workshop on Interfaces and Architectures for Scienti\002c Data Storage 050IASDS 2010\051  IEEE 2010   223Amazon SimpleDB.\224 Onli A v ailable http://a ws.amazon.com simpledb   223HP OpenVMS Systems Documentation Files-11 On Disk Structure Concepts.\224 A v ailable http://h71000.www7.hp com/doc/731\002nal/4506/4506pro n  001.html   D Lovelace R Ayyar A Sala and V Sokal Vsam Demysti\002ed  1st ed Riverton NJ USA IBM Corp 2003   R Russon and Y Fledel 223NTFS Documentation,\224 2004   G Gibson D Nagle K Amiri J Butler F Chang H Gobioff C Hardin E Riedel D Rochberg and J Zelenka 223A CostEffective High-Bandwidth Storage Architecture,\224 in Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems  ACM Press 1998 pp 92\226104 A v ailable http://www acm.or g/pubs/citations proceedings/asplos/291069/p92-gibson   A Avil 264 es-Gonz 264 alez J Piernas and P Gonz 264 alez-F 264 erez 223Scalable Metadata Management through OSD Devices,\224 International Journal of Parallel Programming  pp 1\22626 10.1007/s10766-012-0207-8  A v ailable http://dx.doi.or g/10.1007/s1076601202078   227\227 223A Metadata Cluster Based on OSD Devices,\224 Computer Architecture and High Performance Computing Symposium on  vol 0 pp 64\22671 2011   J Lombardi and L Zhen 223DAOS Changes to Lustre.\224 Presented by Intel High Performance Data Division April 2013   N Watkins C Maltzahn S Brandt and A Manzanares 223DataMods Programmable File System Services,\224 in Proceedings of 2012 Parallel Data Storage Workshop 050PDSW 2012\051  IEEE 2012   N Ali A Devulapalli D Dalessandro P Wyckoff and P Sadayappan 223An OSD-based Approach to Managing Directory Operations in Parallel File Systems,\224 in IEEE International Conference on Cluster Computing  September 2008 A v ailable http://www cse ohio-state.edu 030 alin/papers/cluster2008.pdf   227\227 223Revisiting the Metadata Architecture of Parallel File Systems,\224 in Third Petascale Data Storage Workshop Supercomputing  November 2008 A v ailable http://www cse.ohiostate.edu 030 alin/papers pdsw2008.pdf   A Devulapalli D Dalessandro and P Wyckoff 223Data Structure Consistency Using Atomic Operations in Storage Devices,\224 IEEE International Workshop on Storage Network Architecture and Parallel I/Os  vol 0 pp 65\22673 2008   D Goodell S Kim R Latham M Kandemir and R Ross 223An Evolutionary Path to Object Storage Access,\224 in Proceedings of 2012 Parallel Data Storage Workshop 050PDSW 2012\051  IEEE 2012 


006\003\006 006\003\007 006\003\011 006\003\013 006\003\015 002\003\006 002\003\007 032\033\025\020+\033\034 017\020\021\022\023\024\024\025\026\020 017\020\021\022 023\024\024\025\026\020 032\\032#\(\017\020\021\022 023\024\024\025\026\020 032\033\025\020+\033\034,#\(\027\030\031\022 032\033\031\020\034\031\035 027\030\031\022 032\033\031\020\034\031\035 032\\032#\(\027\030\031\022 032\033\031\020\034\031\035 004%\020\027!\031\030\024\034#&\030'\020#>\024"'\033\026\0309\0201#\031\024#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034\025 033\0353\030\034\025 0\004 020\033'\030\034\025 032\020"$\024"'\033\034\027\020#\024$#;\033\0353\030\034\025 
Figure 8 
heuristic to execute BC on just 4 workers in roughly two-thirds the time as the baseline using 8 workers providing users with cost-performance tradeoffs in a pay-as-you-go cloud environment. The automation offered by the adaptive heuristic to the end user also eliminates the guesswork of picking a static baseline or any potential non-uniformity in sampling using the sampling heuristic C  Given the need to run computation as a series of smaller \(optimally sized\at hs, is important to decide when we initiate the next swath. Our initiation heuristics attempt to overlap execution of multiple swaths to flatten the resource \(memory, network, CPU\ usage variations causes by different supersteps within a single swath. In BC and APSP, we observe a triangle waveform with a central peak; this heuristic is not relevant for applications like PageRank with uniform resource usage. Besides improving resource utilization, overlapping consecutive swath iterations also reduce the cumulative supersteps required and thus reduces the total overhead spent on synchronization between supersteps Figure 6 compares the relative performance of our initiation heuristics for the BC application normalized to a baseline approach that runs strictly  non-overlapping iterations. These run on 8 workers Figure 7 shows the corresponding messages transferred between supersteps over time, spanning swath iterations. The initiates a new swath every supersteps while the  performs initiation when it detects a peak in the number of messages exchanged. Static-Nês performance depends on the graph and the value of that is chosen.  If the average shorte st path is greater than  we will be initiating new heur istics before the previous swath has hit its peak, thereby exacerbating the resource demand. If the average shortest path length is well distributed or is \(just\ shorter than it leads to better performance. So 4 for the larger CP graph actually works best. Our dynamic he uristic eliminates this guesswork as it picks the initiation point at runtime without user input or graph preprocessing. Using this dynamic initiation heuristic we achieve up to 24 speedup vs. sequential initiation for the WG graph. The message transfer plot in Figure 7 corroborates this While sequential shows the message transfers peak and fall to zero \(thus showing more variability and poorer utilization\, Static-6 \(which is optimal but handselected\ maintains a higher message rate while dynamic is a bit more conservative, but automated VII E VALUATING I MPACT OF G RAPH P ARTITIONING ON P REGEL NET Our Pregel.NET framework is agnostic to how the graphs are partitioned and assigned to workers. The default mode performs a simple hash over the vertex ID to determine the target worker partition. Several works have shown that intelligent graph partitioning can improve the performance of distributed graph algorithms [19  26 a nd i t is relevant to examine if these benefits carry over to the Pregel/BSP model also METIS is a commonly used strategy that provides good quality in-place partitioning that minimizes edge-cuts across partition  Rec e n t w o rk o n a p pr o x im ate partitioning using a single graph scan offers an alternative for partitioning online as the graph is read from storage P a ge R a nk is o f t e n used in l iter a tur e  to validate the effectiven ess of these partitioning strategies. However, as we have seen, PageRank implemented using Pregel/BSP has a uniform message profile while BC and APSP have a triangle waveform message profile. We analyze the consequence of this on the performance gains from intelligent graph partitioning Clearly, the benefit of partitioning comes in reduced communication time since messages to remote vertices incur additional delay due to serialization and network I/O when compared to in-memory messages sent to local vertices. Since many distributed graph algorithms are dominated by communication rather than computation, partitioning can improve overall performance However, the barrier synchronization model in Pregel/BSP means that the total time spent in a superstep is determined by the slowest worker in the superstep. Hence, the balance of work amongst workers in a superstep is as import ant as the cumulative number of remote messages generated in a superstep. Since vertices communicate with their neighbors along edges in the Pregel/BSP model and partitioning seeks to collocate a majority vertex neighbors in the same partition, there may arise çlocal maximasé in specific partitions where more vertices are active during the course of execution of a graph application. This difference in workload can cause underutilization of workers that wait for over utilized workers at the superstep barrier  
Relative time taken by PageRank, APSP and BC to run on WG and CP graphs partitioned using METIS and Streaming, normalized to Hashing approach. Smaller is better  
  
sequentially Static-N heuristic Dynamic heuristic 
Swath Initiation Heuristics N N N N N  
211 


006 002\006 007\006 010\006 011\006 012\006 013\006 014\006 015\006 016\006 002\006\006 006 012\006\006 002\006\006\006 002\012\006\006 007\006\006\006 007\012\006\006 010\006\006\006 010\012\006\006 011\006\006\006 033\0353\0201 0\004 031"\020\033'\030\034\025 0#8\031\030\026\0309\033\031\030\024\034 004%\020\027!\031\030\024\034#&\030'\020#\(\035\020\027\003 024'2!\031\020\005:AB 033""\030\020"#5\033\030\031 8\031\030\026 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\014 0202#\015 031\0202#\016 031\0202#\002\006 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 017\020\021\022\023\024\024\025\026\020#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\014 0202#\015 0202#\016 0202#\002\006 0\020\035\035\033\025\020\035 020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 017\020\021\022\023\024\024\025\026\020#0\004&:\#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 002\006 007\006 010\006 011\006 012\006 013\006 014\006 015\006 016\006 002\006\006 006 012\006\006 002\006\006\006 002\012\006\006 007\006\006\006 007\012\006\006 033\0353\0201 0\004 031"\020\033'\030\034\025 0#8\031\030\026\0309\033\031\030\024\034 004%\020\027!\031\030\024\034#&\030'\020#\(\035\020\027\003 024'2!\031\020\005:AB 033""\030\020"#5\033\030\031 8\031\030\026 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\016 031\0202#\002\006 031\0202#\002\002 031\0202#\002\007 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 027\030\031\022\032\033\031\020\034\031\035#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034 025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 031\0202#\016 031\0202#\002\006 031\0202#\002\002 031\0202#\002\007 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 027\030\031\022\032\033\031\020\034\031\035#0\004&:\#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 
We evaluate the impact of graph partitioning using the best-in-class METIS partitioner as well as the best heuristic \(linear-wei ghted deterministic, greedy approach partitioner from [26 an d com p a r e  them against a baseline that uses simple of vertices by their IDs. We run PageRank, BC and APSP over the WG and CP graphs on 8 workers for this evaluation. Hash, METIS, and Streaming produce 8 partitions whose percentage of remote edges are 87 18% and 35% for the WG graph and 86%, 17% and 65% for the CP graph; smaller this number, lower the edge cuts across partitions, and METIS proves a low edge cut for both graphs. Given the large sizes of the graphs, we run these experiments on the same set of vertices as our other experiments \(50 vertices for CP and 75 vertices for WG\. We report these results when using Pregel.NET without our swath heuristics however, the trends we observe are consistent even with heuristics turned on, though the absolute performance is uniformly better Figure 8 shows the relative time taken when using the METIS and streaming partitioning normalized to hashing for PageRank, BC and APSP running on WG and CP. We see that the WG graph sees a relative improvement of nearly 42-50% for METIS for the three applications, while this improvement drops to 24-35 for the streaming partitioning. When running Pregel.NET with heuristics turn ed on, we see a best case improvement of 5x in relative time taken by METIS for BC on WG compared to hashing \(graph not shown These are consistent with results reported in  However, we also see that the CP graph does not show such a marked improvement in performance due to better partitioning, despite its edge cut ratios from different partitioning being similar to WG. In fact hashing is faster than METIS and Streaming for APSP on this graph. It is worthwhile to investigate this consistent lack of improvement for the CP graph as opposed to WG. Figure 9 shows the runtime for BC broken into compute+I/O time and the synchronization barrier wait time components for the WG graph and Figure 12 does the same for CP. The plots also show the VM utilization %, calcul ated as the time spent in compute and I/O communication against the total time including barrier wait time\ on the secondary Y-axis We see that the VM utilization % for hashing is higher though the total time taken is also higher, for both WG and CP. METIS shows the inverse property, having lower utilization but also lower total time. This is explained by looking at the number of messages emitted by workers in a supe rstep for both hashing and METIS, shown in Figures 10 and 11 for WG, and in Figures 13 and 14 for CP. We expect that a hashed assignment of vertices to a partition would spread communication roughly evenly over all workers, while also increasing the number of remote communications required. The latter contributes to the increased total time while the former leads to a uniform number of    
Figure 9 Figure 10 Figure 11 Figure 12 Figure 13 Figure 14 
 taken for BC on a subset of with  shows the ratio of Compute+I/O time to total time   transferred by each worker in the peak supersteps of BC performed over using    transferred by each worker in the peak supersteps of BC performed over       taken for BC on a subset of with  shows the ratio of Compute+I/O time to total time   transferred by each worker in the peak supersteps of BC performed over using    transferred by each worker in the peak supersteps of BC performed over   
in-place streaming hashing 
Total time WG graph different partitioning Utilization Number of messages WG graph Hash partitioning Number of messages WG graph using METIS partitioning Total time CP graph different partitioning Utilization Number of messages CP graph Hash partitioning Number of messages CP graph using METIS partitioning 
212 


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even çgoodé partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity Ö the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the cloudês elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPês synchronous barrier between supersteps offers a window for dynamic scaleout and Öin at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an çoracleé approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workerês time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440Ö442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


