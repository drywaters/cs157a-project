An Area Concept Extraction Algorithm Based on Association Rule Qing  Yang  1 Kai-min Cai  1  Yan  LI  1  1 Department of Computer Science, Hua Zhong Normal University, Wuhan, China  E-mail:caikmin1985@163.com,   yangq@mail.ccnu.edu.cn  Rui-qing Liu 2  2 Overseas Chinese College, Capital University of Economics and Business, Beijing, China E-mail: yangq@mail.ccnu.edu.cn Abstract  Ontology learning is from a given area document sets automatic or semi-automatic extraction terms to construct a domain ontology. Area concept extraction is one of the most important aspects in building ontology. In this paper, we proposed an improved area concept extraction algorithm. In 
the algorithm, we firstly employed association rule algorithm to obtain the similarity between the sememes, and then used the similarity between the sememes to find the similarity between area concepts. Finally our paper achieves the whole area concepts extraction process. By analyzing the experimental results shows the effectiveness and correctness of the algorithm  Keywords: area concept extraction; association rule; sememe I  I NTRODUCTION  Ontology learning is a technology that automatically or semi-automatically obtains terms from existing data sources to build ontology, including extraction concepts, obtaining constraints and properties. Among them, concept extraction in ontology building is the one of the  most important aspects. At present, at abroad and home concept extraction is 
mainly three ways: linguistics-based approach, statisticalbased methods and hybrid methods. Linguistics-based approach is mainly based on area concept having special lexical structures or templates. In this method searches for strings that meet specific template is very important statistical-based method to identify area concept is mainly based on area concepts and common words having different statistical characteristics; hybrid approach is often combined linguistics-based approaches and statistical-based methods Most of them use single similarity calculation method and are not conducive to improving the accuracy of calculation In this paper, we have employed a new way to extraction concepts According to HowNet, concept is composed of sememe and thus calculating the similarity between concepts could 
be by calculating the similarity between sememes. In this paper, using area concept extraction in Open Dictionary as an example introduced the conception of the area concept extraction and of association rule, and on this we proposed a new area concept extraction algorithm. The idea of our algorithm is to use multi-valued attribute association rules algorithm to calculate the similarity between sememes, and thus introduces the similarity between the area concepts Through analysis the result s in the experiment, this paper achieves the correct and efficient extraction of domain concepts II  A REA C ONCEPT E XTRACTION B ASED ON A SSOCIATION R ULES  A 
 Area Concept Extraction Ontology Learning is from a given area of the document sets automatic or semi-automatic extraction area concepts attributes and relationships between concepts to achieve ontology building. In general, ontology learning model has three modules: concept extraction module, relationship extraction module, and domain ontology library. Concept extraction module is the most important part in ontology building and the efficiency and correctness of the concept extraction directly impacts the efficiency of ontology building. At present, there are three main ways to extraction 
concept: \(1\ linguistics-based approach; \(2\ statistical-based method; \(3\ybrid methods. The connection between area relevance degree and area accordance degree receives a nice effect[1  B u t  there are still inadequate: too much reliance on filtering documents; the filter document quality has significant impacts to the accuracy of the area concept extraction. If the differentiation is not high many irrelevant concepts will be treated as the area concept to be chosen out According to HowNet [2   w e can s ee th a t s e m e m e s  constitute concept. HowNet is an on-line common-sense 
knowledge base that describes objects with Chinese words and English words and reveals the relationships between the concepts as well as between attributes and attribute. As a knowledge system that it describes relations between concepts. HowNet attempts to construct a graph structure of its knowledge base from the concept relations and attribute relations. This is the fundamental distinction between HowNet and other tree-structure lexical databases. In HowNet, there are two center terms: concept and sememe Sememe can be understood as the smallest unit meaning which is the most fundamental and difficult to be repartitioned. In HowNet, a concept is described by a group of 
sememe and the similarity between the concepts is based upon the sememe meaning. there has defined a total of more than 1,500 sememes and all the Chinese and English words can be explain by one or more 1500 sememes. More than 1,500 sememes have been divided into 10 categories, each expressed by a tree structure. The sememe in each tree is superior-subordinate relationship, and the sememes in different trees form a network structure, associated each other by explaining the sememes [3 
2010 International Conference of Information Science and Management Engineering 978-0-7695-4132-7/10 $26.00 © 2010 IEEE DOI 10.1109/ISME.2010.71 230 


In order to calculating the similarity between area concepts, our paper at first calculates the sememes which constitute of the concept B  The Basic Conceptions of Association Rules Association rule mining is R.Agrawal who first proposed in the literature in 1993. Data mining association rule is one of the current main research modes. It focuses on identifying the data relationship between the different areas to find content the multiple dependencies between the fields under the given conditions. It describes a potential relationship rules between a set of data items in the database In general, association rule mining is from a large data set to find some association or relationship, that is, from the data sets identifying attribute values sets frequently appears also known as frequent item sets, and then using these frequent sets create and describe association rules. It describes the models of the law between attributes occurred at same time [4   Set I 12  I I  n I is a collection of items, items known as the individual elements; the relevant data set D 12  TT  m T is a transaction database, each of which is a collection of items made I T 002 Each transaction has a unique identifier denoted by TID Definition 1: If A, B is an item set AI 003  B I 003 and 000 001 B 004 then the expression of the form A 005 B is called association rule Definition 2: The task-related items in the percentage of the total item is called the support MinSup, expression MinSup\(A 005 B 002 P 002ƒ\000 001 B 002  Definition 3: Rules A 005 B has credible C, that is the percentage of D contains A also contains an item B expression: MinConf \(A 005 B\=P\(A/B Definition 4: Supposed the minimum degree of support is MinSup, then in the D, k-frequent pattern set and the k non-frequent pattern set are defined as k L   1 A 001 2 A 001#\001\002\001 k A  i AI 006 S 1 A 001 2 A 001#\001\002\001 k A D 000\014\0011 MinSup 000\036\000\003 1 k k L L    The rules which at the same time satisfy the minimum support threshold and minimum confidence MinSup threshold are called strong rules. Mining association rules is from a given data set D finding out association rules which satisfy the minimum support threshold and minimum confidence threshold. Supposed A,B is the item set in the data set D, if B A 003 then the support degree of A is greater than the support degree of B; if AB 003 and B is a nonfrequent pattern set, then A is a non-frequent pattern set; if A B 003 and B is frequent pattern set, then A is a frequent pattern set C  An Association Rule Algorithm Based on Multi-Valued attribute clustering In the ontology study, area concept extraction is extraction concepts which have field association with the given concept in the existing data source The task to find strong association rules can be divided into two sub-tasks: \(1\ finding out all frequent item sets in the data set D. \(2\nding out strong association rules generated on the basis of the frequent item sets. The second step is the key to the algorithm which includes two steps: \(1 generating association rules; \(2\nding out a strong association rules. The way to find frequent item sets is mainly through determining the occurrence number of item sets in the data set D. The more the number of occurrences the greater the possibility of frequent item set is selected Association rules mining can be divided into Boolean mining and multi-valued attributes mining. Multi-valued attribute can be divided into category attribute and number attribute and so on. Weather or not the relation between Multi-valued attributes exists mainly convert it to Boolean 1î is yes and ì0î is no. Clustering data mining algorithms is a non-supervised learning algorithm, that is, according to some similarity measure makes the samples with the same characteristics as a class. There is a smaller difference in similarity within class and biggest difference between classes [5 T h e c l ust e ri ng a l gor i t h m  i n c l ude s d i vi s i o n based, level-based, density-based, grid-based and modelbased and so on. Among them, the K-means algorithm based on division method is used widely The algorithm idea of K-means is as follows: Firstly from n data objects selects arbitrary k data objects as initial cluster centers; according to the other objects similarity with cluster centers, assign them to their most similar clustering Then calculate cluster centers of each new clustering formed in the last step. Continuously repeats the processes until the standard measure function begins convergence. Generally use the Euclidean formula as a standard measure function Mining association rule algorithm based on k-means as follows a Conversion multi-valued attribute value in the data set D according to the needs of cluster; Selecting m samples from D and assigning them to initial cluster centers 1,2 j cj     m  b Using the Euclidean formula calculating the distance between i I i= 1 , Ö, n\ in the D and the cluster center The formula is 2     ij i j dI c I c   1 c Find out the distance between i I and j c  and move i I to the corresponding clusters in which each item to j c  has close distance. And form k results meets distance convergence d According to formula \(1\calculate the square error E\(t of all items in the D and compare with last square error E\(t-1\ E\(t\E\(t-1\en turn b\erwise turn e e Forming association rules f Extracting  association  rules 
231 


D  Area Concept Extraction Algorithm Based on Multivalue Attribute Association Rule Algorithm Description: The user at first inputs area ontology concept sets; and then how to from existing data source efficiently and correctly extract area sets interested by the user is a core question we must solve The idea of the algorithm and steps as follows a  Supposed the candidate set of area concept D 12  D D  n D  002 i D  12  dd  m d is the sememe set of the concept i b  User inputs area concept set G. Delete the same concepts from G which are similar with the concepts in D, gain G c  Isolating sememe set named i S of concepts from G and given to cluster centers 1,2 j cj     m 002  d  Does association rule algorithm based on K-means on i S and gain association rule set based on sememe i S  e  Separation the source and target of the association rule in the set i S and diverting sememes to concept Supposed concept set i D  12  dd  m d and j G   12  g g  m g are arbitrary concept node; Sim i D   j G e semantic similarity between i D and j G  If i D and j G are the same node, then Sim=1; if i D  and j G have no relations each other, then Sim=0. The formula of Sim as follows 2     ij M ax depth c d Sim D G Sim c d    002 2 002  In the formula \(2 i cD 006 002 j dG 006 002 j=1,2,Ö,m; i=1,2 002 n 002 depth \(c, d\ is the depth of c and d in the sememe tree  M ax depth c d is the maximum of depth \(c\nd depth \(d\ The   s im c d can be calculated by the association rule algorithm which we have listed in above III  E XPERIMENT A NALYSES  We at first input the area concept set G1 = {grammar language, automata, compilation},G2={Internet, software hardware}, and convert the concepts in G into sememe. The Compiler Principle tutorial and the Computer domain in the Open Directory are used to ca ndidate area concept set. We use Precision, Recall and F1 test value as our testing tool to test the experimental results. The formula of Precision Recall and F1 test value is as follows Recall-rate=accurate concept number/ total number of tested concept                                                    \(3 Precision-rate=accurate concept number/total number of extracted concept                                               \(4 F1-test value = 2*Precision-rate*Recall-rate/\(Precision-rate Recall-rate\                                      \(5 For concept set G1 we run th e algorithm to extract area concept from the Compiler Prin ciple tutorial. For concept set G2 we extract area concept fr om the Open Directory Partial experimental data is showed in the table In this table, UIA represen ts the area concept set user inputs; CDS represents the candidate data source. HEC represents extraction area concepts with hand. OLC represents extraction concepts with our algorithm  Table 1  some experimental data  UIA/ CDS G1/ Compiler Principle G2/ Open Director HEC 271 66 OLC/total 314/320 73/76 Precision 86.3 90.4 Recall 84.7 86.8 F1 test value 85.5 90.0  IV  C ONCLUSIONS  Ontology learning is a technology that automatically or semi-automatically obtains terms from existing data sources to build ontology. The technology of extraction area concepts is one of most hot research in these days. Other area technologies such as  Data Mining, Artificial Intelligence are used to concept extraction. In our paper, the algorithmis a method that association rule is applied to the area extraction. Combined with the sememe in the HowNet the algorithm largely improves the accuracy of the concept extraction and the stability. Due to the specificity of ontology learning task in this area there still have many issues to be resolved R EFERENCES  1  Xiu-ling Jia, Dun-wei Wen,. A Survey of Ontology Learning from Text[D C h a n g s h a  Ce ntr a l S o ut h U n i v e r s ity 200 7  2  HowNet [R  How N e t s Home Pa g e  h tt p  www  k e e n a g e  c o m  3  WANG Dong, WU Jun-hua, Automatic update method to semantic similarity computation between ontology[D Co m p u t e r E n g i ne e r ing  and Design. 2009,30 \(19 9 4420 4  QIU Min-xia, Algorithm and Application of the Association Rules in Data Mining. Journal of Suzhou University of Science and Technology Natural Science; Vol 9 26 No.4  Dec 9 2009 5  L IXiang, ZELL A. H 000 filtering for a mobile r obot tracking a free rolling ball[ C   R o b o C up 2 0 06  Ro bo t S o c c e r W o r l d Cu p X    S   l  S p r i ng e r 2 V e r l ag 2 0 0 6    
232 


value of confidence. Top-NFTDS algorithm is designed for mining top-k fault tolerant rules that the k is not the number of fault-tolerant frequent patterns but the number of fault-tolerant association rules. Top-NFTDS algorithm need not input the minimum support threshold and minimum confidence threshold Property 2 shows that fault-tolerant itemsets are unlike traditional itemsets that is no anti-monotone. Here, we construct a specific example to illustrate that calculation of fault tolerant negative itemsets is different from calculation of traditional negative itemsets  Figure 1  Transactions in sliding windows The transaction data stream TDS consists of two consecutive sliding windows, i.e SW 1 and SW 2 Let window size W is 6 transactions [8  L e t f a u lt t o l e r a n t  factor is 1. For mining k most frequent itemsets, TopNFTDS algorithm need not input the minimum support threshold and minimum confidence threshold. Let all generated positive itemsets and negative itemsets are frequent. In Fig.1, let itemset \(a b, c  X and b  c  d  Y  Table 2 shows the positive support, negative support positive fault tolerant support, negative fault tolerant support, confidence, and fault tolerant confidence of X and Y  TABLE II  R EDUNDANCY IN R ULE G ENERATION  Itemsets Supp Conf FT-Supp FT-Conf X 16.7  83.3  Y 33.3 50  X 83.3 16.7  Y 66.7 50  X 001 Y 0 0 50 60 X 001 Y 16.7 100 0 20 X 001 Y 66.7 80 0 0 By calculating above, we can get more useful information. For negative rule X 001  Y traditional calculation method can draw the conclusions that rule  X 001  Y is true. However, by fault tolerant theory, rule  X 001  Y is false, the same as rule X 001  Y If we extend the theories of negative itemsets to fault-tolerant space then we can get higher value of fault tolerant support and fault tolerant confidence. More useful information can help us to judge the effectiveness of discovered rules. By means of Top-NFTDS algorithm, we may achieve the goal of redundant pattern disambiguation more effectively B  Mining Top-k Fault-Tolerant Association Rules In preliminary work, we has proposed Top-NFTDS algorithm. Using this algorithm, we can acquire k fault tolerant itemsets. Then the following work is to generate fault-tolerant association rules. We propose an efficient algorithm called Top-NFTDS to generate rules using technology of fault tolerant negative itemsets. The algorithm consists of two phases: generating fault tolerant rules phase and redundant pattern disambiguation phase Algorithm 1 is designed for generating fault tolerant rules phase and it is given  Algorithm 1: Generating fault tolera n t rules Input F T I 1  F T I 2  F T I k Output: fault tolerant positive rules and fault tolerant negative rules 1  Get-FTitemsets FTI 1  FTI 2  FTI k  2  Initialize PFTR  NFTR  3  For each fault tolerant itemset FTI i do 4  Calculate_Psupp 5  Calculate_Nsupp 6  For each fault tolerant itemset FTI i 001 FTI k  X do 7  Calculate_Psupp 8  Calculate_Nsupp 9  Calculate_Pconf 10  Calculate_Nconf 11  Sort_Psupp 12  Sort_Nsupp 13  Sort_Pconf 14  Sort_Nconf 15  If X Psupp X K  Psupp then 16  If X Pconf X K  Pconf then 17  Creat _PFTrule PFTR  18  End if 19  End if 20  If X Nsupp X K  Nsupp then 21  If X Nconf X K  Nconf then 22  Creat _NFTrule NFTR  23  End if 24  End if 25  End for 26  End fo r In first phase, Top-NFTDS output both fault tolerant positive rules and fault tolerant negative rules with maximum value of support and confidence. Generally these patterns must be exist invalid patterns or expression repeated patterns. Next step Top-NFTDS will prune the fault tolerant space C  Disambiguating redundant patterns In procedure of redundant pattern disambiguation phase, Top-NFTDS draws the fault tolerant rules finally which express useful information and is easy to understand for users Algorithm 2 is designed for disambiguating redundant patterns phase and it is given Algorithm 2: Disambiguating redundant patterns Input: fault tolerant positive rules and fault tolerant negative rules, and K  Output k fault tolerant rule 1  For each rule X do 2  For k 000  K do 3  Calculate_corr FTR i 001 FTR k  4  If X.corr 1 then 5  Remove_rule X  
472 


6  End i f  7  If X.corr 1 then 8  Remove_rule X  9  End if 10   k 0 and k  11  End for 12  End fo r  In algorithm 2, line 3-9 achieve the goal of identifying and removing redundant rules. The final phase of TopNFTDS algorithm is described as follows. First, TopNFTDS calculates the correlation of all fault tolerant rules generated by algorithm 1. Second, according to definition 7, Top-NFTDS removes false fault tolerant rules when the value of correlation less than or equal 1. Finally, TopNFTDS draws K fault tolerant rules with higher value both of fault tolerant support and fault tolerant confidence IV  E XPERIMENTAL A NALYSIS  In this section, we will describe the performance of the proposed algorithms. Same as preliminary experiment of mining top-k fault tolerant frequent itemsets, all the programs are implemented using Microsoft Visual C Version 6.0 and performed on a 3.0 GHz Intel Celeron processor with 512 MB memory in a windows XP system In preliminary work, we used the synthetic data set T7.I4.D1000K, T10.I6.D1000K and T15.I10.D1000K generated by IBM synthetic data generator. T, I and D mean the average transaction length, the average length of the maximal frequent itemset, and the total number of transactions, respectively. In this paper, we still use three transaction sets above We draw the conclusion of relationship between fault tolerant factor and support or confidence when K is a determination value  Figure 2  Relationship between 005 and support  Figure 3  Relationship between 005 and confidence  Fig.2 shows the support change of fault tolerant rules with the change of fault tolerant  factor  005 when K is 3000 First, it may mean that the greater value of allowed mismatches, the higher value of support for fault tolerance rules. Second, it may mean that the longer average length of transaction, the higher value of support for fault tolerance rules. Fig.3 shows the confidence change of fault tolerant rules with the change of fault tolerant  factor On the contrary, with the increasing of value of mismatch the confidence of fault tolerant rules become lower. It is clear that the property of fault-tolerant rule conform to the objective fact in real-world data stream V  C ONCLUSION  In order to solve the problem of mining top-k faulttolerance frequent rules over data streams, this paper has developed a novel algorithm called Top-NFTDS. It is integrated with previous research of mining top-k faulttolerant itemsets into an integrity new method. TopNFTDS can discover the true fault-tolerant rules without minimum support threshold and minimum confidence threshold specified by user. The Top-NFTDS algorithm discovers generalized fault-tolerant rules to express more abundant information, and then by negative fault-tolerant itemsets to disambiguate redundant patterns, ensure the validity of discovered rules. The experiment shows the algorithm is effective. The theoretical analysis shows our algorithm is nice with high efficiency and scalability. At last, by the verification experiment, we prove that TopFTDS is rational and available R EFERENCES  1  A. K. Poernomo  and V. Gopalkrishnan, ìMining statistical information of frequent fault-tolerant patterns in transactional databases,î Data Mining \(DM 07\, Oct. 2007, pp. 272-281, Doi 10.1109/ICDM.2007.48 2  X. Wu, C. Zhang, and S. Zhang, ìEfficient mining of both positive and negative association rules,î ACM Transactions on Information Systems, Vol. 22, July 2004, pp. 381-405, Doi 10.1145/1010614.1010616 3  O. Daly and D. Taniar, ìException Rules Mining Based On Negative Association Rules,î Lecture Notes in Computer Science Vol. 3046,  Apr. 2004, pp 543-552, doi:10.1007/b98054 4  Y. Y. Zhao and H. Jiang, ìResearch of Mining Positive and Negative Weighted Association Rules Based on Chi-Squared Analysis,î International Conference on Distributed Computing and Applications for Business \(ICIC 09\, May 2005, pp. 344-347, Doi 10.1109/ICIC.2009.94 5  H. Li and X. G. Hu, ìEfficient mining of strong negative association rules in multi-database,î Computational Intelligence and Software Engineering \(CISE 09\, Dec. 2009, pp.1-4, Doi 10.1109/CISE.2009.5364801 6  M. Charikara, K. Chenb, and M. F. Coltone, ìFinding frequent items in data streams,î Theoretieal Computer Science, Vol. 312 Jan. 2004, pp. 3-15,  Doi:10.1016/S0304-3975\(03\00400-6 7  C. C. Aggarwal and P. S. Yu, ìA New Approach to Online Generation of Association Rules,î IEEE Transactions on Knowledge and Data Engineering \(ITKDE 01\, Aug. 2001, pp 527ñ540, Doi: 10.1109/69.940730 8  G. H. Li and H. Chen, ìMining the frequent patterns in an arbitrary sliding window over online data streams,î Journal of Software Vol.19, No.10, Oct. 2008, pp. 2585-2596, Doi 10.3724/SP.J.1001.2008.02585  
473 


Moreover we can say that F C is comparable to E C and the modi\002ed version of E C i.e vE C  is correct and effective  The combinations of F C  E C  and vE C with 006 conf improve slightly F C  E C  vE C  and 006 conf  We can say that the sum of con\002dences is an important measure and these combined measures are useful in particular in the context where the accuracies of the classi\002cation by the sum of con\002dence by F C  and by E C are almost very good For conclusions 002rstly the adapted weight of evidence is a good class membership measure built on the gain of information F C  a measure built on the revisited 037 2 test provides another view of information gain It is comparable to the adapted weight of evidence The sum of con\002dence is a simple and natural measure with the good performance The combinations of the sum of con\002dence with the previous measures are interesting and useful to improve their performance Next based on the average accuracy values of different measures we recommend to use the combined measures because the average accuracy values of the combined measures are in general better than that of the non-combined measures Finally through the results on each dataset between the combined measures based on 037 2 and the weight of evidence we suggest the following propositions 017 For the 13 small datasets though the average accuracy value of cE C is slightly better than that of cF C  we can observe that cF C is much often wins cE C  Indeed cE C wins cF C on only 3 datasets while cF C wins cE C on 6 datasets Hence we can recommend cF C for the small datasets 017 For the 10 large datasets the average accuracy value of cvE C is slightly better than that of cF C  and cvE C wins cF C on 4 datasets and cF C wins cvE C 3 datasets Hence we can recommend cvE C for large datasets References  B Lent A Sw ami and J W idom Clustering association rules Proc Intl Conf on Data Engineering ICDE'97 IEEE Computer Society 1997 pp 220-231  W  Li J Han and J Pei CMAR Accurate and Ef 002cient Classi\002cation based on multiple class-association rules Proc IEEE Intl Conf on Data Mining ICDM'01 San Jose CA IEEE Computer Society 2001 pp 369-376  B Liu W  Hsu and Y  Ma Inte grating Classi\002cation and Association Rule Mining Proc 4th Intl Conf on Knowledge Discovery and Data Mining KDD'98 AAAI Press 1998 pp 80-86  Y  Sun Y  W ang and A.K.C W ong Boosting an Association Classi\002er in IEEE Transactions on Knowledge and Data Engineering vol 18 no 7 IEEE Computer Society 2006 pp 988-992  J W ang and G Karypis HARMONY  Ef 002ciently Mining the Best Rules for Classi\002cation Proc SIAM Intl Conf on Data Mining SDM'05 2005 pp 205-216  J W ang and G Karypis On Mining Instance-Centric Classi\002cation Rules in IEEE Transactions on Knowledge and Data Engineering vol 18 no 11 2006 pp 1497-1511  Y  W ang and A.K.C W ong From Association to Classi\002cation Inference using Weight of Evidence in IEEE Transactions on Knowledge and Data Engineering vol 15 no 3 2003 pp 764-767  F  Coenen The LUCS-KDD Implementations of the FOIL PRM and CPAR algorithms http://www.csc.liv.ac.uk frans/KDD/Software/FOIL PRM CPAR/foilPrmCpar.html Computer Science Department University of Liverpool UK 2004  Y  Basti de R T aouil N P asquier  G Stumme and L Lakhal Mining Frequent Patterns with Counting Inferences in ACM SIGMOD Explorations vol 2 no 2 2000 pp 66-75  R Agra w al and R Srikant F ast algorithms for mining association rules Proc 20th Intl Conf on Very Large Databases VLDB'94 Santiago Chile 1994 pp 487-499  V  Phan-Luong and R Messouci Building Classi\002ers with Association Rules based on Small Key Itemsets Proc 2nd IEEE International Conf on Digital Information Management ICDIM'07 France 2007 pp 200-205  J Quinlan and R Cameron-Jones FOIL A Midterm Report Proc European Conf on Machine Learning ECML'93 1993 pp 3-20  X Y in and J Han CP AR Classi\002cation based on Predicti v e Association Rules Proc 3rd SIAM Intl Conf on Data Mining SDM'03 San Francisco CA SIAM 2003 pp 369-376  C Cortes and V  V apnik Support-V ector Netw orks  in Machine Learning vol 20 no 3 1995 pp 273-297 
690 


Figure 3 Precision-recall curve in ROI-250 image dataset 5.3 Experiment 3 Mammograms-1080 image dataset This experiment employed a dataset composed by 1080 mammograms images collected in the Clinical Hospital of University of Sao Paulo at Ribeiro Preto The dataset was previously classi\002ed into 4 levels of breast tissue density 0501\051 mostly fatty 050362 images\051 0502\051 partly fatty 050446 images\051 0503\051 partly dense 050200 images\051 and 0504\051 mostly dense 05072 images\051 Figure 4 Precision-recall curve in Mammography-1080 image dataset Breast density is an important risk factor in the development of breast cancer In this experiment the images are represented by the feature set proposed in b uilding a vector of 85 features including shape and size of the breast the conditions of the breast contour nipple position and the distribution of 002broglandular tissue This dataset was divided in training set and test set The training set is composed of 720 images and test set is composed of 360 images Figure 4 shows the P&R curves over test dataset and also the number of features selected in each method Again the proposed methods reached the highest values of precision and select the smallest number of features 050a\051 050b\051 050c\051 050d\051 Figure 5 Queries in Mammography dataset 050a\051 226 is the query image 050b\051 using the features selected through 002tness function F cB  050c\051 using the features selected through classi\002cation error of C4.5 and 050d\051 using the all features extracted Results for the retrieval of the 5 most similar images from a query image are also provided in this experiment as illustrated in Figure 5 The image 5.\050a\051 is the query image taken from the mostly fatty image class Images shown in 5.\050b\051 are the 5 most similar images retrieved for the proposed 002tness function F cB  The row 050c\051 shows the results for C 4  5 classi\002er whereas 050d\051 illustrate the images resulting from all features 050no selection applied\051 In Figure 5 the images surrounded by dashed lines are false positives 050not relevant images\051 For this query the proposed method achieved the highest precision 050100%\051 when compared the results of C4.5 and the original feature vector 050precision of 40%\051 


6 Conclusions This work proposed a novel genetic feature selection framework for CBIRs It employs a wrapper strategy that searches for the best reduced feature set while optimizing 050or preserving\051 the quality of the solution From a ranking evaluation function three new 002tness functions namely F cA  F cB and F c have been proposed and evaluated in three experiments The proposed genetic feature selection approach which encompasses F cA  F cB and F c  has been compared with 050a\051 traditional methods found in the literature 050b\051 the StARMiner feature selector and 050c\051 the whole feature vector and signi\002cantly outperformed them The proposed approach has been able to optimize the accuracy of similarity queries while selecting a signi\002catively reduced number of features Additionally the proposal of combining the quality of the query results with the criterion of minimizing the number of selected features F cA and F cB  led to high accurate query answers while reducing the number of features more than the 002tness function F c  Therefore the 002nal processing cost of the queries is also reduced References  P  M d Aze v edo-Marques N A Rosa A J M T raina C Traina-Jr S K Kinoshita and R M Rangayyan Reducing the semantic gap in content-based image retrieval in mammography with relevance feedback and inclusion of expert knowledge International Journal of Computer Assisted Radiology and Surgery  3\0501-2\051:123\226130 June 2008  R Baeza-Y ates and B Ribeiro-Neto Modern Information Retrieval  Addison-Wesley Essex UK 1999  B Bartell G Cottrell and R Bele w  Optimizing similar ity using multi-query relevance Journal of the American Society for Information Science  49:742\226761 1998  O Cord 264 on E Herrera-Viedma C L 264 opez-Puljalte M Luque and C Zarco A review on the application of evolutionary computation to information retrieval International Journal of Approximate Reasoning  34:241\226264 July 2003  J G Dy  C E Brodle y  A Kak L S Broderick and A M Aisen Unsupervised feature selection applied to content-based retrieval of lung images IEEE Transactions on Pattern Analysis and Machine Intelligence  25\0503\051:373\226 378 March 2003  W  F an E A F ox P  P athak and H W u The ef fects of 002tness functions on genetic programming-based ranking discovery for web search Journal of the American Society for Information Science and Technology  55\0507\051:628\226636 2004  W  F an P  P athak and M Zhou Genet ic-based approaches in ranking function discovery and optimization in information retrieval a framework Decision Support Systems  2009  D E Golber g Genetic algorithms in search optimization and machine learning  Addison Wesley 1989  R L Haupt and S E Haupt Practical Genetic Algorithms  John Wiley  Sons New Jersey United States second edition edition 2004  J Horng and C Y eh Applying genetic algorit hms to query optimization in document retrieval Information Processing  Management  36:737\226759 2000  S K Kinoshita P  M d Aze v edo-Ma rques R R PereiraJr J A H Rodrigues and R M Rangayyan Contentbased retrieval of mammograms using visual features related to breast density patterns Journal of Digital Imaging  20\0502\051:172\226190 June 2007  F  K orn B P agel and C F aloutsos On the  dimensionality curse and the self-similarity blessing IEEE Trans on Knowledge and Data Engineering  13\0501\051:96\226111 2001  H Liu and L Y u T o w ard inte grating feature select ion algorithms for classi\002cation and clustering IEEE Transactions on Knowledge and Data Enginnering  17\0504\051:491\226502 April 2005  M X Ribeiro A J M T raina C T raina-Jr  and P  M Azevedo-Marques An association rule-based method to support medical image diagnosis with ef\002ciency IEEE Transactions on Multimedia  10\0502\051:277\226285 2008  U S Cancer Statistics W orking Group United states cancer statistics 1999-2005 incidence and mortality webbased report atlanta 050ga\051 Department of health and human services centers for disease control and prevention and national cancer institute 2009 Available in http://apps.nccd.cdc.gov/uscs   L T amine C C and M Boughanem Multiple query evaluation based on an enhanced geneticnext term algorithm Information Processing  Management  39\0502\051:215\226 231 2003  R S T orres A X  F alc 230 ao M A Gonc\270alves J P Papa Z B W Fan and E A Fox A genetic programming framework for content-based image retrieval Journal of the American Society for Information Science and Technology  42\0502\051:283\226292 2009  A Tsymbal P  Cunningham M P echenizkiy  and S Puuronen Search strategies for ensemble feature selection in medical diagnostics In Proceedings of the 16th IEEE Symposium on Computer-Based Medical Systems  pages 124\226 129 June 2003  A Tsymbal M Pechenizkiy  and P  Cunningham Sequential genetic search for ensemble feature selection In Proceedings of the International Joint Conferences on Arti\002cial Intelligence  pages 877\226882 August 2005  C.-M W ang a and Y F  Huang Ev olutionary-based feature selection approaches with new criteria for data mining A case study of credit approval data Expert Systems with Applications  36\0503 Part 2\051:5900\2265908 2009  H Y an J Zheng Y  Jiang C Peng and S Xiao Selecting critical clinical features for heart diseases diagnosis with a real-coded genetic algorithm Applied Soft Computing  8:1105\2261111 2008  T  Zhao J Lu Y  Zhang and Q Xiao Feature selection based on genetic algorithm for cbir In IEEE Congress on Image and Signal Processing  volume 2 pages 495\226499 2008 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnêt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnêt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnêt create sound clips  A5.1 We donêt need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIês   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





