   A Probabilistic Approach to Apriori Algorithm Vaibhav Sharma   M.M. Sufyan Beg  Computer Science Department                Department of Computer Engineering Institute of Technology and Manage ment Jamia Millia \(A Central University Gurgaon, Haryana, India  New Delhi, India  shvaibhav@yahoo.com    mbeg@jmi.ac.in     Abstract  We consider the problem of applying probability concepts to discover frequent itemsets in a transaction database The paper presents a probabilistic algorithm to discover association rules. The proposed algorithm outperforms the apriori algorithm for larger databases without losing a single rule. It involves a single database scan and 
significantly reduces the number of unsuccessful candidate sets generated in apriori algorithm that later fails the minimum support test. It uses the concept of recursive medians to compute the dispersion in the transaction list for each itemset. The recurs ive medians are implemented in the algorithm as an Inverted V-Median Search Tree IVMST\. The recursive medians are used to compute the maximum number of common transactions for any two itemsets. We try to present a time efficient probabilistic mechanism to discover frequent itemsets  Keywords  Data mining, KDD, association rules frequent itemsets, probability, statistics, apriori algorithm 
1. Introduction  The exponential increase in disk space and availability of cheap storage has contributed to the accumulation of large quantities of data. Knowledge Discovery in Databases KDD\he process of extraction of frequent patterns from database. KDD is an important tool to transform the vast amount of data into information. KDD is also referred to as Data Mining. In different contexts, data mining is referred as knowledge extraction, data analysis, data dredging, data fishing and data snooping. Data mining finds its application in marketing, surveillance, fraud detection, scientific discovery, pattern recognition and 
customer analytics. Data mining techniques were classified on the basis of databases to be mined by Chen et al Culotta et al. proposed an integrated supervised machine learning method that learns both contextual and relational patterns to extract relations [8  w i d e  v a riet y o f d a ta  mining techniques have evolved but they all share the same basic features 2   Data Mining is a very challenging task. The most time consuming step in data mining is the discovery of frequent itemsets. Itemsets that satisfy the criterion of minimum support are called frequent itemsets. Minimum support gives a lower limit to the occurrences of a set of items in a 
given database. Agrawal et al. proposed a mathematical model to address the problem of mining association rules in a transaction database [2 ciatio n ru le d i sco v er y i s  the most important aspect of data mining a ri ou s techniques have been developed to discover frequent itemsets in a database. But Apriori algorithm[1  w a s t h e  first algorithm to discover the frequent itemsets in a time efficient manner. It was the first algorithm that significantly improved the data mining techniques to identify frequent itemsets. Apriori algorithm is explained in Section 2 of the paper. Different techniques have been used by different researchers to implement apriori 
algorithm. Monte Carlo simulation techniques using statistical methods have been successfully applied to apriori algorithm [10 Olso n a n d W u al so co rrelated  Monte Carlo simulation techniques with data mining concepts prop os ed a v e r y  f a st i m pl e m en t a t i on technique for apriori algorithm al s o propos ed a  trie based apriori algorithm for mining frequent itemset sequences The paper proposes a probabilistic approach to apriori algorithm. Using concepts of probability and statistics, the paper presents an implementation faster than the apriori algorithm. The structure of the proposed algorithm is kept 
similar to the apriori algorithm for ease of understanding Though the pr oposed algorithm inherits many concepts from apriori algorithm, but its mechanism for mining frequent itemsets is completely different. It is based on the fact that not all \(k-1\itemsets have equal probability of generating k-candidate sets with high support. This is opposite to the concept of apriori algorithm which gives equal priority to all the itemsets. The proposed algorithm uses concept of ìbimodal distributionî to classify the itemsets into two groups. The first group contain itemsets 
that have very high support relative to others. The second group contain itemsets that have moderate support. The itemsets in both the groups are already above the minimum support. The itemsets in the second group are further processed to identify those combinations of itemsets that will not generate high support candidate sets. These low support combinations are later pruned from the possible candidate sets. The algorithm provides a time efficient method to discover frequent itemsets in a transaction database 
2010 IEEE International Conference on Granular Computing 978-0-7695-4161-7/10 $26.00 © 2010 IEEE DOI 10.1109/GrC.2010.69 402 


The paper is divided into 6 sections. The Second Section gives only a brief account of apriori algorithm. The Third Section discusses the key concepts used in the proposed algorithm. The proposed algorithm is presented in detail in Section 4. A detailed explanation along with pseudocode is provided in this section. The Fifth Section analyse the performance results for the proposed algorithm and compares them with the apriori algorithm. The paper concludes with the Sixth Section giving directions for future research  2. Apriori Algorithm  The general structure of the apriori algorithm is given below. The apriori algorithm involves multiple passes. For k-passes, it scans the database k times. In the first database scan large 1-itemsets with frequency at least equal to minimum support are generated. A subsequent pass through the algorithm is a two step process. In the first step, lexicographically similar large itemsets L k-1 are combined to generate candidate sets C k using the apriori_gen function. The second step counts the frequency of each candidate set in C k This step scans the entire database. This counting operation can be performed in a faster way by using a candidate hash tree. After the counting operation, all the candidate sets with support less than minimum support are pruned to give large k-itemset These two steps are repeated until no more large itemsets are formed. The application of this counting technique was extended beyond transaction databases in [3 L 1 large 1-itemsets for \( k=2; L k-1  000  005 k++\in C k apriori_gen \(L k-1  for all transactions t 005 D do begin C t subset \( C k t for all candidates c 005 C t do  c.count End L k c 005 C k c.count 000 minsup End Answer 005 000\017 013 k  3. Key Concepts used in the proposed algorithm Probabilistic and statistical concepts applied in the proposed algorithm identify and eliminate candidate sets that will otherwise fail in the minimum support test. The single database scan and the data sets used to store information after the scan are also discussed 3.1. Bimodal Distribution A multimodal distribution is a mixture of two or more different unimodal distributions. A bimodal distribution is a specific case of multimodal distribution with only two modes depicted as two distinct peaks \(local maxima\n the probability density function Condition for bimodality in a probability density function states: ìA mixture of two normal distributions with equal standard deviations is bimodal only if their means differ by at least twice the common standard deviation.î The concept of bimodality is used in the function classify of the algorithm The apriori algorithm combines all the lexicographically suitable \(k-1\itemsets with equal probability to generate kcandidate sets. During candidate generation, there is no differentiation among \(k-1\-itemsets. But statistically, \(k-1 itemsets with large support value have relatively higher probability of generating k-candidate sets with larger support values. The \(k-1\itemsets with high support values are independently present in relatively large number of transactions, and this contributes towards higher probability for any two of them being present together in a transaction. Using bimodal distribution in the probability density function for support, we try to classify the \(k-1 itemsets into two groups ñ HIGH and MEDIUM based on their probability to generate k-candidate sets with large support values. This is discussed in more detail in Section 4.3 3.2. Recursive Medians as a measure of dispersion for a given distribution  Median divides a distribution into two new distributions with equal number of elements. If we, recursively, find out all the possible medians for a list, then, we can say with conformity that only one element is present between any two successive medians. This fact becomes complicated when the distribution is large and many values in the distribution turn out to be medians. But still it holds true The concept of recursive medians was first given in Median of Medians Algorithm 4 h is co n cep t is w i d e l y  used in Selection algorithms and Quick sort Median of Medians Algorithm is not used as such in the proposed algorithm. It is used in a much simpler way and with little modification in the proposed algorithm The recursive medians concept is used in the function low_probable_combine_itemsets In this function recursive medians are calculated for a list of transactions against an itemset. It is used to find the maximum number of exactly same transaction ids for any two transaction lists within well defined boundaries. Recursive medians are used to evaluate dispersion of elements in a given distribution. Any two successive medians indicate a range containing only one element. This concept when used repetitively with all the possible medians for a distribution deduces the maximum number of elements present within a given range. Though, it cannot predict the exact number of sameî elements for two distributions but it gives a maximum limit to the number of elements that can be sameî. This concept is explained in detail in Section 4.5 
403 


3.3. Inverted V-Median Search tree \(IVMST It is a V-shaped binary search tree where splitting nodes hold the value of recursive medians calculated for a distribution The concept of recursive medians in the context of algorithm is implemented in the form of an Inverted V-Median Search Tree. The root node of the IVMST holds the first median value for the distribution. The root node is the only binary node with two children. Rest of the nodes are unary with only one child. IVMST follows the basic principle of binary search tree. The left subtree of a node contains nodes with keys less than the nodeís key. The reverse is true for the right subtree of a node. Thus, leftmost and rightmost leaf nodes hold the smallest and largest median value respectively. The implementation details of IVMST are given in Section 4.5 3.4. Single Data Scan  Unlike apriori algorithm which performs k data scans for k passes, the proposed algorithm performs only one data scan The database is not used to count the support for the kcandidate sets after the first pass. Rather, the information is collected in the data sets L 1 C 1 and 000\006 015 1 The information stored in L 1 C 1 and 000\006 015 1 is modified in further passes. For small values of k, the size of data sets L k C k and 000\006 015 k is larger than the database. But for large values of k, the size of data sets becomes significantly smaller than the database. This feature may appear similar to AprioriTid, but its implementation in the proposed algorithm is completely different. The format of the entries in L 1 C 1 and 000\006 015 1 is different from that in AprioriTid. The format for these data sets is given in Table 1 Instead of storing information in form of transactions \(done in AprioiTid\e store information in form of itemsets along with the corresponding transactions in which they are present. The first field in C 1 represents the 1-itemset and the second field is a list of all the transaction ids which contain that itemset 000\006 015 1 is same as C 1 except that it contains the count of all the transaction ids as ësupportí in its second field. This way of storing information is inherent to the success of the algorithm as it proves to be time efficient in later passes Also, this format is a key requirement, keeping in mind the probabilistic approach of the algorithm   Table 1: Format of Data Sets  Data Set Format L 1 1-itemset C 1  1-itemset, list of transaction ids 000\006 015 1  1-itemset, support 4. Algorithm This section discusses the proposed algorithm in detail. It is divided into 6 sub sections. The first sub section explains the notations used in the algorithm. The second subsection presents the general structure of the proposed algorithm. Sub sections 4.3 - 4.6 explain the various functions invoked in the algorithm 4.1. Notations used in the algorithm The algorithm uses various data sets to perform different operations. Also, values returned from the functions called from the main need to be stored in appropriate data sets. The notations used in the algorithm are given in Table 2 Table 2: Notations Data sets Format of each entry L k 1 Large \(k-1\mset C k-1  Large \(k-1\mset, list 000\006 015 k-1  Large \(k-1\-itemset, support 000\020\000\010\000\007\000\014\000\030\000\020 015 015 015 015 015 015 015 015 015 015 015 015  k-1\-itemset, support MEDIUM {\(k-1\mset, list C k New k-candidate set LOW k-candidate sets Created from the lexicographic association of two \(k-1\-itemsets 005  MEDIUM C k List  k-candidate set, list HIGH_SUPPORT { support 1 support 2 support n  MEDIUM_SUPPORT support1, support2,Ö, support n  4.2 Proposed Algorithm L 1  022 000\017\000É\000î\000â\000á\000\003\003s 015F 000ã\000ñ\000á\000è\000ï\000á\000ñ\000 022  000\006 015 1  022 000\017\000É\000î\000â\000á\000\003\003s 015F 000ã\000ñ\000á\000è\000ï\000á\000ñ\000ï\001 000ï\000ó\000í\000í\000ë\000î\000 022  C 1  022 000\017\000É\000î\000â\000á\000\003\003s 015F 000ã\000ñ\000á\000è\000ï\000á\000ñ\000ï\001 000é\000ã\000ï\000 022  For 022 000ç\015L\003t\000\003 L k 000\003\015M 005 k++} do begin  000\020\000\010\000\007\000\014\000\030\000\020 015 015 015 015 015 015 015 015 015 015 015 015 classify 000\006 015 k-1  see Section 4.3 For all c 005 C k-1 do begin For all m 005  000\020\000\010\000\007\000\014\000\030\000\020 015 015 015 015 015 015 015 015 015 015 015 015 do begin  If \( m.itemset = c.itemset \hen  Insert 022 000\003\000Ö\001‰\000ã\000ñ\000á\000è\000ï\000á\000ñ\001·\000Ö\001‰\000é\000ã\000ï\000ñ\000\003 022 into MEDIUM End End C k New apriori_gen\( L k-1  see Section 4.4 LOW=low_probable_combine_itemsets\(MEDIUM see Section 4.5 Delete all k-candidate sets present in LOW from C k New  C k List gen_candidates_with_list \( C k-1 C k New  see Section 4.6 For all c 005 C k List do begin If \( c.count \(list 015R minsup \Then L k  022 000\003\000Ö\001 000ã\000ñ\000á\000è\000ï\000á\000ñ\000ï\000\003 022  C k  022 000\003\000Ö\001 000ã\000ñ\000á\000è\000ï\000á\000ñ\000ï\001 000Ö\001 000é\000ã\000ï\000ñ\000\003 022   000\006 015 k  022 000\003\000Ö\001‰\000ã\000ñ\000á\000è\000ï\000á\000ñ\001·\000Ö\001‰\000Ö\000ë\000ó\000ê\000ñ\000\003 022 000é\000ã\000ï\000 022 000\003 022  End if End End Answer 006 000\017 013i k   
404 


L 1 C 1 and 000\006 015 1 are the data sets obtained after the database scan. A subsequent pass through the algorithm, say pass k consists of 4 phases. The first phase is the classification phase In this phase, the large itemsets L k-1 from previous pass are classified in two groups on the basis of bimodality in probability density function for support. These two groups are HIGH and MEDIUM. The second phase calls the apriori_gen function and generates k-candidate sets from lexicographic association of large itemsets L k-1 These kcandidate sets are stored in C k New The next phase is performed in two steps. First, the large k-itemsets present in MEDIUM are processed in the function low_probable_combine_itemsets to identify those kcandidate sets that will fail in the minimum support test These k-candidate sets are stored in data set LOW. In the second step, all the entries present in LOW are deleted from C k New In the last phase, the list of transactions corresponding to each k-candidate set in C k New is generated using function gen_candidates_with_list by intersection of the list of all its immediate \(k-1\ubsets present in C k-1 Pruning of all the kcandidate sets with support below minimum support gives the large k-itemsets. This process is repeated until no new large k-itemsets are produced by the previous pass 4.3 Classification of Large \(k-1\itemsets In the function classify  000\006 015 k-1 is passed as an argument. The data set 000\006 015 k-1 contains support for each large \(k-1\itemset This function generates a probability density function \(pdf for support. We check the condition for bimodality in this probability density function. If such a condition exists, we divide the support distribution into two groups HIGH_SUPPORT and MEDIUM_SUPPORT. The point of division is given by the condition of bimodality HIGH_SUPPORT contains all the support values above the point of separation and MEDIUM_SUPPORT contains all support values below it Select max_val 005Z MAXIMUM \(c.support min_val 005Z MINIMUM \( c.support mean 005Z AVG \( c.support where c 005  000\006 015 k-1   For all c 005  000\006 015 k-1 do  begin difference += \( c.support ñ mean \ 2 If \( c.support = max_val \en Pop c from 000\006 015 k-1  Insert c.support into HIGH_SUPPORT Else If \( c.support = min_val \Then Pop c from 000\006 015 k-1    Insert c.support into MEDIUM_SUPPORT End standard_deviation 000  000Ü\000ã\000à\000à\000á\000î\000á\000ê\000Ö\000 000\027\000ë\000ñ\000É\000é\000\003\000\027\000î\000É\000ê\000ï\000É\000Ö\000ñ\000ã\000ë\000ê\000 003  standard_difference = 2*standard_deviation  lean 003 003s\022:\000Ö\001 000ï\000ó\000í\000í\000ë\000î\000 015O 007I\007A\007=\007J\022  003 003s\022:\000Ö\001 000ï\000ó\000í\000í\000ë\000î\000 015P 007I\007A\007=\007J\022   Total number of Transactions c 005  000\006 015 k-1 and lean is a measure of Skewness While \(\(AVG\(HIGH_SUPPORT\VG\(MEDIUM_SUPPORT 000\003  015R  standard_difference\ND MEDIUM_SUPPORT 015M\005  Condition for Bimodality If 000\006 015 k-1 000\003\015M 005 hen Pop c from 000\006 015 k-1   Insert c.support into HIGH_SUPPORT Else Pop c from MEDIUM_SUPPORT  Insert c.support into HIGH_SUPPORT End while loop Pop the last support value inserted into HIGH_SUPPORT If \( HIGH = NULL\ Then If \(\(lean < 0 lean = 0 AND \(min-minsup 015R max_val-mean\Then HIGH_SUPPORT 000\006 015 k-1   ElseIf\(\(lean > 0 lean = 0 AND \(min-minsup < max_val-mean\hen HIGH_SUPPORT = NULL  End If MEDIUM_SUPPORT 000\006 015 k-1 HIGH_SUPPORT Update MEDIUM_SUPPORT to contain only distinct values For c 005  000\006 015 k-1 do begin For m 005 MEDIUM_SUPPORT do begin  If \( c.support = m.support \Then  Insert c.itemset, c.support into 000\020\000\010\000\007\000\014\000\030\000\020 015 015 015 015 015 015 015 015 015 015 015 015  End End The data sets HIGH_SUPPORT and MEDIUM_SUPPORT may contain the same value multiple times. This is because more than one itemsets may have same support. This function also keeps track of skewness in the pdf MEDIUM_SUPPORT contains all the support values present in 000\006 015 k-1 but absent in HIGH_SUPPORT 000\020\000\010\000\007\000\014\000\030\000\020 015 015 015 015 015 015 015 015 015 015 015 015 is constructed from MEDIUM_SUPPORT 000\020\000\010\000\007\000\014\000\030\000\020 015 015 015 015 015 015 015 015 015 015 015 015 contains all those itemsets present in 000\006 015 k-1 whose support is present in MEDIUM_SUPPORT  4.4 Candidate Generation The apriori_gen is exactly similar to the candidate generation function used in apriori algorithm. It involves two steps to generate k-candidate sets from lexicographic combination of \(k-1\itemsets. The first step is the join step It is followed by prune step. The first step uses all possible lexicographic combinations of L k-1 to generate new kcandidate sets. The prune step removes a candidate set if any one of its immediate subset is not present in L k-1  
405 


apriori_gen \( L k-1  Join step Insert into C k New  select p.item 1 p.item 2 p.item k-1 q.item k-1  from L k-1 p, L k-1 q where p.item 1 q.item 1 p.item 2 q.item 2  p.item k-1 q.item k-1  Prune step For all itemsets c 005 C k New do begin For all \(k-1\ubsets s of c do If \(s 005  L k-1 hen  delete c from C k New  End End 4.5 Generating low probable combinations of \(k-1 itemsets In this function, all pairs of lexicographically similar \(k-1 itemsets 005 MEDIUM\cessed to identify those pairs which will generate k-candidate set with support lesser than minimum support. Recursive medians are used to identify such candidate sets. These low support k-candidate sets are stored in the data set LOW. Pruning these unsuccessful kcandidate sets at an earlier stage results in increased performance of the algorithm. The average case and the worst case complexity for insertion as well as searching in IVMST is O\(log n\. Hence, IVMST presents a very efficient method to implement the recursive medians concept The first field of data set MEDIUM is the \(k-1\mset itself and the second is a list of transaction ids in which that itemset is present. We denote the end limits of the overlapped region for transactions lists of the two lexicographically similar \(k-1\itemsets as START and FINISH respectively. Thus, START and FINISH are the boundary limits which denotes the region where the two \(k1\-itemsets may have ìsameî transaction ids. The ìsame transaction ids refer to transactions that contain both the itemsets The identification of the ìsameî transaction ids is performed in two stages. In the first stage, we check whether the range of the boundary limits \(represented by FINISH-START\at least as large as minimum support. If the two itemsets fail in this test, the resulting k-candidate set is immediately placed in LOW and we proceed with the next pair of \(k-1\itemsets If the pair of itemsets passes this test, then we move to the second stage of testing. The second stage is implemented using the concept of recursive medians and IVMST. IVMST is created for each of the two itemsets. All possible medians for the list of transaction ids are represented as nodes in IVMST. Using the fact that successive medians have only one element in between, we predict the maximum number of sameî transaction ids that can be present in the range START, FINISH}. We again check whether the number of sameî transaction ids will be at least equal to the minimum support. If they fail to fulfill this criterion, then the resulting candidate set is pushed into LOW For all possible lexicographic k-candidate combinations among \(k-1\itemsets in MEDIUM do Let A and B be any two random lexicographically similar k-1\msets in MEDIUM Let [AB be k ca n d idate s e t obtain e d f r o m le x i cog r aphic association of A and B START 005Z MAXIMUM \(Min_Tid A Min_Tid B  FINISH 005Z MINIMUM \(Max_Tid A Max_Tid B  If \(FINISH-START < minsup\Then Insert [A O W  Else Repeat for both A and B IVMST X  005Z Inverted V Median Search Tree \(X Depth of nodes on left subtree are taken negative Depth of nodes on right subtree are taken positive H LX  005Z node_depth; node 005 IVMST X and node_value 015Q START Value LX 005Z node_value \(node_depth = H LX  H RX  005Z node_depth; node 005 IVMST X and node_value 015R FINISH Value RX 005Z node_value \(node_depth = H RX  For all nodes 005 IVMST X do begin If \(node_value 005 list AND node_value 005 RANGE{START, FINISH} \ Then X X  005Z X X 1 End If \(Value LX  015M START AND Value LX 005 list\hen X X  005Z X X 1 If \(Value RX  015M FINISH AND Value RX 005 list\hen  X X  005Z X X 1 Elements X  005Z ABSOLUTE\(H RX H LX  X  End Repeat Maximum_same_list=MINIMUM \(Elements A Elements B  If \(Maximum_same_list < minsup\hen Insert [A O W End of If Else Statement End of for loop  4.6 Generating list of transaction ids for newly generated candidates This function generates the list of transaction ids for each of the k-candidate set present in C k New We use the property that all possible \(k-1\ubsets for the k-candidate set are already present in C k-1 as k-candidate sets with missing \(k-1\ubsets were already pruned in the apriori_gen function. The list for 
406 


each of the k-candidate set is generated by intersection of the list \(of transaction ids\f all its \(k-1\ubsets. The kcandidate set along with the list is pushed into the data set C k List This is followed by pruning of all those k-candidate sets that do not have support at least equal to minimum support. Thus large k-itemsets are obtained gen_candidates_with_list \( C k-1 C k New  For all c 005 C k New do begin For all possible \(k-1\ubsets s of c do Insert { itemset,list } into C k List  where itemset = c.itemset AND list 006 000 013i 013g\013@\0135 i list s 005 C k-1  End End 5. Performance Study To confirm the time efficient approach of the algorithm, we implemented the proposed algorithm and the apriori algorithm on a 512 MB RAM Memory and 1.67 GHz Intel processor. We carried out a substantial performance evaluation and compared it with results obtained for apriori algorithm Table 3 compares performance of the proposed algorithm with the apriori algorithm for different minimum supports against different number of transactions. As minimum support decreases, the performance of the proposed algorithm increases and after a certain point, it performs better than the apriori algorithm. The relative performance also increases with increase in number of transactions. These results are also demonstrated in Figure 1. Total number of data items and average number of items per transaction are 30 and 15 respectively Table 4 compares the number of items in MEDIUM and LOW corresponding to the data values shown in Table 3 The execution time is shown to compare the increase in performance with increase in number of candidate sets in LOW Figure 2 compares the number of elements in MEDIUM and LOW against different number of initial data items for a given database and minimum support. For this experiment number of transactions in the database is 2K and minimum support is 20%.The proposed algorithm performs better with increase in the number of candidate sets present in LOW Figure 2 shows that the number of elements in MEDIUM and LOW increases with increase in number of data items Thus, the proposed algorithm performs better for large number of data items in the database 5.1 Result Analysis The performance of the proposed algorithm is directly proportional to the number of passes over the algorithm number of elements in LOW and data items in the database During initial passes, size of L k C k and 007 004 k is more than the database. Hence, initial passes are time consuming. But during the latter passes, these data sets become significantly smaller than the database. Larger minimum support generates less number of candidate sets and involves lesser passes over the algorithm. Thus, Figure 1 shows a dip in performance of the proposed algorithm for larger minimum support values. But, it performs better than the apriori algorithm as minimum support decreases  Table 3: Execution time in seconds   Table 4: Execution time v/s size of MEDIUM and LOW  6. Conclusion and future directions The proposed algorithm presents a probabilistic approach to apriori algorithm. The proposed algorithm creates exactly the same number of rules as the apriori algorithm but in lesser time. The results confirm that the discovery of association rules in larger databases is faster in the proposed algorithm than the apriori algorithm Integration of probabilistic approach to data mining in distributed databases shall produce amazing results [9 Hence, the effectiveness of the proposed algorithm needs to be checked on distributed databases. This algorithm was implemented on a synthetic database. Its application on real 
407 


world databases presents a future direction for further research  Figure 1: Execution time v/s Minimum Support  References 1 R. A g r a w a l a nd R. Srik a n t F a s t A l g o rithm s  f o r Mining  Association Rules,î In Proc. of the 20th VLDB Conference  Santiago, pp. 487-499, Chile, 1994 2 R. A g ra w a l T  Im i e linsk i, a nd A. Swami. ìMining Association Rules between Sets of Items in Large Databases.î In Proceedings of ACM SIGMOD pages 207-216, May 1993 3 R. A g ra wa l, R. Srik a n t M in i n g se que ntia l pa t t e r ns In proc. of the 11th International Conference on Data Engineering \(ICDE'95  pages 3-14, March 1995 4 M Blu m R.W  Flo y d   V   P r att, R.L  Riv e st an d R E. T a rjan  Time bounds for selection,î J. Comput. Syst. Sci. 7\(1973\, pp 448-461  Total number of data items Number of elements in MEDIUM Number of elements in LOW 10 139 0 20 3238 127 30 21899 1923 Figure 2: Size of MEDIUM and LOW v/s number of data items 5 Fe r e nc B odo n  A f a s t  A P R I O R I i m plem e n ta tion I n pr oc o f  IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'03 Melbourne, Florida, USA, 2003 6 Feren c Bo d o n   A T r i e b as ed A P RIORI Im p l e m en tatio n f o r  Mining Frequent Item Sequences.î In ACM SIGKDD Workshop on Open Source Data Mining Workshop \(OSDMí05 pages 56-65 Chicago, IL,USA, 2005  M  C h en  J Han an d  P  S   Yu  Dat a M i n i n g  A n o v ervi e w  from a Database Perspective IEEE Transactions on Knowledge and Data Engineering vol. 8, no. 6, pp. 866-883, Dec. 1996 8 A r on Culotta A ndre w Mc Ca llum Jona tha n Be tz  I nte g ra tin g probabilistic extraction models a nd data mining to discover relations and patterns in text,î In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics p.296-303, June 04-09, 2006, New York  M M S u fy an Be g P aral l e l an d Di st ri b u t ed Di sco v er y o f  Association Rules In Artifical Intelligence Application Book  Fadzilah Siraj, Eds\ersity Utara Malaysia 10 X ita o Fa n k os Fe ls v  l y i Ste phe n A  Siv o  M onte C a r l o   SASÆ for Monte Carlo studies: a guide for quantitative researchers 11 U  Fa y y e d  G  P i a t e t s k y Sha p iro P. Sm y t h a nd R. U t h u ra s a my  eds.\. ìAdvances in Knowledge Discovery and Data Mining AAAI Press / The MIT Press, 1996 1 W  J F r aw l e y  G  P i at et sk y S h ap i r o an d C M a t h eu s   Knowledge ìDiscovery In Databases: An Overview. In Knowledge Discovery In Databases eds. G. Piatetsky-Shapiro, and W. J Frawley, AAAI Press/MIT Press, Cambridge, MA., 1991, pp. 1-30 13 V  K u m a r  a nd M. J o s h i T utor ia l o n H i g h P e r f or m a nc e D a t a  Mining,î In proc. of International Conference on High Performance Computing \(HiPC-98 Dec. 1998  a v i d L  O l s o n a n d D e s h e n g Wu   D eci s i on  m a k i ng  with uncertainity and data mining.î In X. Li, S. Wang and Z.Y Dong\(Eds Lecture notes in Artificial Intelligence pp. 1-9 Berlin: Springer\(2005 15  P  W r ig ht. K now le dg e D i s c ov e r y I n D a ta ba s e s  T ools a n d  Techniques ACM  Crossroads Winter 1998   
408 


 7. Reference    Fetzer C,Hagstedt, K,Felb er P  Autom atic Deteciton  and Masking of Non-Atomic Exception Handling International Conference On Dependable Systems and Networks, \(DSN2003\10-116    Y e n S J, Lee Y S. ìMining Interesting  Associatio n  R u l es  and Sequential Patternsî. International Journal of Fuzzy Systems, 2004-6 \(4   Alasf f ar A  H, Deogun J S. ìConcept-b a sed Retr iev a l with Minimal Term Setsî. Foundations of Intelligent Systems: 11th Intíl Symposium, Springer, Poland, 2004 114- 122   Qiu Y ong gang,Frei H P  Concept B a sed Quer y   SIGIRí03,2003:16 0-169   Saltom G  W ong A, Y a ng C  S. ìA V ector Sp ace Model for Automation Indexingî. Communications of the ACM 2005, 18\(5\-620   Agrawal R, Srikant R. ìFast Algorithm f or Mining Association Rules in Large Databases.î Proceedings of the 20th International Conference on Very Large DataBases Santiago , Chile , 2004   Park J S. ìUsing A Hash-Based Method with Transaction Trimming forMining Association Rules.î IEEE Transactions on Knowledge and Data Engineering, 2007   Savasere A, Omiecinski E Navathe S  An Ef ficient Algorithm for Mining Association Rules in Large Databases.î Proceedings of the 21st International Conference on Very large Database, Switzerland, 2002  


              


   


                        





