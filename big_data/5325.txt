Organic Streams: Data Aggregation and Integration Based on Individual Needs  Xiaokang Zhou, Qun Jin, Bo Wu, Wei Wang Graduate School of Human Sciences Waseda University Tokorozawa, Japan xkzhou@ruri., jin@, wubo@ruri., ougi@ruri.}waseda.jp Julong Pan, Wenbin Zheng College of Information Engineering China Jiliang University Hangzhou, China pjl, zhengwb}@cjlu.edu.cn  Abstract With the high accessibility of the social media more and more people have been accustomed to sharing their personal contents across the social networks, which results in an explosive increase of data scale. In this study, in order to support information and knowledge discovery in big data, we propose an approach to aggregation and integration of personal big data from life logs in accordance with individual needs, which can benefit the sustainable information utilization process. In details the organic stream, which is designed as an extensible data carrier, is introduced and developed to formulize and organize the personal big data, in order to extract dynamical individual needs from the tremendous amount of data posted through social media, and further aggregate and integrate the related data in a meaningful way, which can also facilitate the personalized information retrieval and reuse process. The architecture of the system with the foundational modules is given, and the experiment result is presented to demonstrate the usability and effectiveness of our approach Keywords—Big Data; Life Log; Data Aggregation; Data Integration; Individual  Need I   I NTRODUCTION  With the continuous development of social media \(such as Twitter, Facebook\, more and more populations have been involved into this social networking revolution, which leads to a tremendous increase of data scale, ranging from the daily text data to multimedia data that describes different aspects of people’s life. For instance, every month, Facebook deals with 570 billion page views, stores three billion new photos, and manages 25 billion pieces of contents [1  B i g d a t a  w h i c h  includes data sets with sizes beyond the ability of current technology, method and theory to capture, manage, and process the data within a tolerable elapsed time” [2   ar e h ig h v o lu m e   high-velocity, and/or high-variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization  T h a t  is, it has become a big challenge to process such massive amounts of these complex big data, which has attracted a lot of attentions from academia, industry and government as well Life log, a kind of personal big data, has also attracted increasing attentions in recent years. Life logs include a variety of data, such as text, location, sound, images and videos, which are dynamically produced from multiple sources and with different data structures or even no data structures Consequently, it is difficult for an individual \(end-user\ to utilize the useful information hidden in these records, such as user experience, knowledge, by simply processing the raw data from life logs. Therefore, it is necessary to find a way to effectively integrate and mine this kind of personal big data that records people’s information behaviors and social activities in both cyber space and real world, in order to provide people with valuable individualized service In our previous study, we have introduced and defined the so-called organic stream with the formal description, which can discover and represent the potential relations among data, in order to organize the raw stream data into the methodically preprepared form [4  In th is s t u d y  in  o r d e r  t o f a ci lit ate  t h e sustainable data utilization process, we propose and develop an effective way to aggregate and integrate the personal big data in accordance with individual needs, which can further benefit the information fusion and knowledge discovery process. We extend and refine the definition of organic stream to be an extensible data carrier to formulize and organize the personal big data from life logs in a meaningful and flexible way, which can assist the transformation from data to information, and from information to knowledge, and finally from knowledge to asset iteratively The rest of this paper is organized as follows. We give a brief overview on the related issues and works in Section II. In Section III, we introduce and extend the concept of organic stream into the data level, and further propose the mechanism to aggregate and integrate the personal big data in accordance with individual needs. We present the architecture of a data aggregation and integration system, and show empirical analysis results in Section IV. We conclude this study and give some promising perspectives on future works in Section V  II  R ELATED W ORK  Big data has drawn more and more researchers these years 5-10  A l s u ba ie e e t al   5  h a v e bu i lt a pa ral l el d a ta b a s e s y s t em  called ASTERIX, which tried to combine time-tested principles from parallel database systems with those of the Web-scale computing community, in order to deal with the “Big Data management challenges. Berkovich et al. [6 v e de ve l o p e d a tool to cluster diverse information items in a data stream mode which can enhance the available information processing resources for on-the-fly clusterization from diverse sources Hoi et al ve prop o s e d  a fea t ure se le c t i o n al gor i t h m  i n  535 


order to solve the online feature selection \(OFS\ problem Zhang et al ha ve  pr e s ent e d  a  p a r a l l e l  ro ugh se t  b a sed approach using MapReduce for knowledge acquisition on the basis of the characteristics of data, in order to mine knowledge from big data. Menon et al. [9 f o c u sed o n da t a  w a r e ho usi ng and analytics platform of Facebook to provide support for batch-oriented analytics applications. Han et al  i n t r od uce d  a big data model which used social network data for information recommendation related to a variety of social behaviors There are many analyses, as well as applications, which focus on life logs [11-16 Y a m a gi w a e t a l 11 ha ve pr opose d  a system to achieve an ecological lifestyle at home, in which sensors measure the social life log by the temperature humidity, intensity of illumination related to human action and living environment closely. Hori et al. [1 ve lop e d  a context-based video retrieval system which utilized various sensor data to provide video browsing and retrieval functions for life log applications. Hwang et al. [13 d e vel ope d a machine learning method for life log management, in which a probabilistic network model was employed to summarize and manage the human experiences by analyzing various kinds of log data. Kang et al  d e fi ned m e t a d a t a t o s a ve a nd search life log media, in order to deal with those problems such as high-capacity memory space and long search time cost Shimojo et al ha ve  r e e ngi ne e r ed t h e  l i f e l o g co m m o n  data model \(LLCDM\ and life log mashup API \(LLAPI\ with the relational database MySQL and the Web services, which can help access the standardized data, in order to support the integration of heterogeneous life log services. Nakamura et al 16 p r o pos e d  a m e th od t o i n f e r u s ers  tem p o r a l  pr ef er en ce  according to their interests by analyzing the web browsing logs As for data aggregation [17-22 z de m i r   17 h a s  presented a reliable data aggregation and transmission protocol based on the concept of functional reputation, which can improve the reliability of data aggregation and transmission Jung et al pr opo sed a nd d e v e l o ped  t w o  da t a a ggre ga t i o n  mechanisms based on hybrid clustering: the combined clustering-based data aggregation mechanism and the adaptive clustering-based data aggregation, in order to improve both data aggregation and energy efficiency. Iftikhar et al. [1 h a v e  presented a rule-based tool to maintain data at different levels of granularity, which features automatic gradual data aggregation. Rahman et al. [2 p r o pos ed a p r iv a c y pres e rv i n g data aggregation scheme named REBIVE \(reliable private data aggregation scheme\, which considers both data accuracy maintenance and privacy protection, in order to provide privacy preservation technique to maintain data accuracy for realistic environments. Wei et al. [2 ha v e prop o s e d t h r e e  prediction-based data aggregation approaches: grey-modelbased data aggregation \(GMDA\ kalman-filter-based data aggregation \(KFDA\, and combined grey model and kalman filter data aggregation \(CoGKDA\hich can help reduce redundant data communications. Ren et al  p r op o s ed  a n  attribute-aware data aggregation \(ADA\cheme which consists of a packet-driven timing algorithm and a special dynamic routing protocol, to improve the data aggregation efficiency and further benefit the data redundancy elimination Research works have also been tried on data integration  23 fi ned a sy st e m fo r o n l i n e da t a i n t e gr at i o n ba sed  o n  the binary operations of relational algebra. Atkinson et al. [24  have proposed a framework of data mining, access and integration, to deal with the scale-up issue of data integration and data mining across heterogeneous and distributed data resources and data mining services. Sarma et al h a ve described the approximation algorithms to solve the costminimization and maximum-coverage problems for data integration over a large number of dependent sources. Tran et al ha v e  d e vel ope d a pl a t f o r m for  d i st r i b u t e d da t a  integration and mining, in which data are processed as streams by processing elements connected together into workflows Gong et al. [27 in t r odu ce d th e ar ch ite ctu re an d  pr ot ot y p e implementation to provide a dynamic solution for integration of heterogeneous data sources  III  D ATA A GGREGATION AND I NTEGRATION  In this section, based on the extension of organic stream which is employed as a self-adaptive data carrier to meaningfully formulize and organize the personal big data from life logs, an integrated mechanism is proposed and developed for the user need-based data aggregation and integration A  Organic Streams: Extension and Refinement In our previous study e ha ve  i n t r od uc ed a n d de fi ned the concept of organic stream, in order to benefit the meaningful organization of social streams. In this study, we extend the organic stream as an effective means to formulize and organize personal big data from life logs based on individual needs, which can assist the data aggregation and integration process The following definitions are given for the analysis and collection of the data that fit individual needs Heuristic Magnet The Heuristic Magnet is the data that is aggregated to describe an individual’s continuous changing need, concern and interest Associative Entity The Associative Entity is used to describe the related data entity that is aggregated in a data collection in accordance with individual needs, which can further be employed to describe the relationships among data Collective Body The Collective Body is the meaningfully aggregated and integrated data collection based on individual need, concern and interest We discover and mine individual needs by analyzing and aggregating his/her related data, which are represented by Heuristic Magnets, and will further become the aggregating center. The data related to this center will be aggregated as the Associative Entity. Finally, the Collective Body is generated to describe the relationships among data according to individual need, concern and interest Based on these discussed above, we extend the concept of organic stream as a data carrier in the individual need-based data aggregation and integration process 536 


   1 where H m  H m u 1 t 1   H m u 2 t 2    H m u x t y  To  r e pr es en t a  nonempty set of Heuristic Magnet where H m u i t j  de no t e s t h e Heuristic Magnet that indicates the need for a specific individual u i in a specific time period t j   Ae  Ae 1  Ae 2  Ae n To represent a set of Associative Entities based on inherent or potential relations to the individual needs R To describe the relationships among Heuristic Magnets and Associative Entities in organic stream These relationships can be defined as follows Heuristic Magnet × Associative Entity: The basic relationship between Heuristic Magnet and Associative Entity in organic stream, which is used to describe relationships between aggregated data and individual needs Heuristic Magnet × Heuristic Magnet: The relationship among Heuristic Magnets in organic stream, which is used to describe the similarities among different individual needs Associative Entity × Associative Entity: The relationship among Associative Entities in organic stream, which is used to describe the relationships among aggregated data in a Collective Body Following the definitions given above, in organic stream  the raw data can be aggregated according to the Heuristic Magnet × Associative Entity relation, and further be integrated based on Heuristic Magnet × Heuristic Magnet relation and Associative Entity × Associative Entity relation, which can be re-organized in an iterative and self-adaptive way  B  Data Aggregation and  Integration Based on Individual Needs Based on the discussions above, in order to capture an individual’s time-changing needs, the whole time period should be divided into several time slices \(e.g. one day, one week\ in which the developed TFIDF method can be employed to extract the data to represent individual needs, concerns and interests within this specific time slice. After that, the data related to the extracted individual need can be selected and aggregated according to the Heuristic Magnet × Associative Entity relation in a hierarchical way. Based on these, the Collective Body can be generated, in which the selected data shall be integrated and organized according to the Heuristic Magnet × Heuristic Magnet relation and Associative Entity Associative Entity relation. All the Collective Bodies compose the organic stream in an extensible way. The procedure for the data aggregation and integration process is shown as follows Step 1 For the whole time period T divide it into a series of time slices t 1 t 2 t n accordingly, divide the  data set M into several sub-set M t in each time slice t i  Step 2 For each in the time slice t i calculate the weight based on the TFIDF method, in order to extract the Heuristic Magnet set H m  H m u 1 t 1  H m u 2 t 2     H m u x t y   Step 3 For each H m u i t j  cording t o the Heuristic Magnet Associative Entity relation, select the related data into the Associative Entity set Ae  Ae 1  Ae 2  Ae n   Step 4 Use the data in set H m and Ae to generate the Collective Body as set Cb   Step 5 Calculate the Heuristic Magnet × Heuristic Magnet relations and Associative Entity × Associative Entity relations in each Collective Body Cb k record all the relationships in the relation set R   Step 6 Return the Heuristic Magnet set H m and Associative Entity set  Ae with the relation set R to form the integrated data set M    IV  E XPERIMENT AND A NALYSIS  In this section, after the introduction of the architecture of the basic functional modules for data aggregation and integration, we show and discuss experiment analysis results to illuminate the feasibility of our proposed method A  Functional Modules The architecture for the individual need-based data aggregation and integration is shown in Fig. 1, which consists of five major components: Data Collector, User Need Extractor Data Aggregator, Data Integrator, and Data Relation Analyzer   Fig. 1  Functional modules for data aggregation and integration As shown in Fig. 1, the Data Collector, which is the fundamental function module in this system, is used to collect the raw data from individuals and connect with the major database which saves the life logs of all users. The User Need Extractor is employed to extract individual needs, concerns and interests from his/her life logs to generate the heuristic magnets In addition, the Data Aggregator works for aggregating the data related to the extracted individual needs, while Data Integrator is responsible for integrating the heuristic magnets with the Associative Entities. Finally, Data Relation Analyzer analyzes the major relations among these data and further organizes them to generate the collective bodies 53 7 


 B  Experiment Analysis The Twitter data has been utilized to conduct our experiment in order to demonstrate the feasibility of our proposed method   Fig. 2  An example of experiment analysis result for data aggregation  Fig. 3  Conceptual image of data integration  We obtained the Twitter dataset from a list of Twitter users selected from a famous Twitter list named “twitter”, as well as some of their followers. The majority of the tweets that are collected for our experiment are published in April 2013 Finally, a total of 320,000 tweets were collected. In order to illuminate our method, in this case, during the time period April 4 to 16, the keyword, “Boston”, is extracted as the Heuristic Magnet using the TFIDF method, which can indicate the interest or need within this group of users. And then those data that is related to “Boston” is aggregated to form the 538 


Associative Entities. An example of experiment analysis result is shown in Fig. 2 We further integrate the aggregated data according to the relation R discussed above. As shown in Fig. 3, the selected Heuristic Magnet became the aggregating center, and the related data converged to it as the Associative Entity According to the relation, Heuristic Magnet × Associative Entity, the distance from Associative Entity to the center namely the Heuristic Magnet, describes the relevance between them, that is, the more related Associative Entity would be closer to the center. Moreover, according to the relation Associative Entity × Associative Entity, the Associative Entities which have the same relevance to the Heuristic Magnet will distribute in the same layer. For instance, in Fig. 3, all Associative Entities distribute in four layers, while in each layer, the more related data will stay closer to each other Finally, the Heuristic Magnet with these related Associative Entities will form the Collective Body  V  C ONCLUSION  In this paper, we have introduced and extended the organic stream to aggregate and integrate personal big data from life logs according to individual needs, in order to benefit the sustainable information utilization process We developed and refined the organic stream as an extensible data carrier, in which we defined Heuristic Magnet Associative Entity, and Collective Body to describe the individual need, related data, and integrated data set respectively. The major relations: Heuristic Magnet Associative Entity, Heuristic Magnet × Heuristic Magnet, and Associative Entity × Associative Entity, were proposed and studied in order to organize the aggregated data in a meaningful way. Based on these, we developed a mechanism to aggregate and integrate the personal big data in accordance with individuals’ time-changing needs, which can further support the information fusion and knowledge discovery process. Finally, we gave the system architecture with major functional modules and the experiment analysis results to show the feasibility and effectiveness of our proposed method As for our future work, we will develop and complete our system with the improved mechanism to realize the data aggregation and integration process in real world situations We will further quantify the relations in organic stream. The classification of the aggregated data, and the data quality assurance will also be considered, in order to deal with heterogeneous life log data from multiple sources. Moreover, a unified physical storage mechanism in regard to these diverse data will be developed, and we will try to utilize the organic stream in the information fusion and knowledge discovery process, in order to realize the sustainable utilization  A CKNOWLEDGMENT  The work has been partly supported by 2012 and 2013 Waseda University Grants for Special Research Project No 2012B-215 and No. 2013B-207  R EFERENCES  1  C.Q. Ji, Y. Li, W.M. Qiu, U. Awada, and K.Q. Li, “Big Data Processing in Cloud Computing Environments,” in Proc. 12th International Symposium on Pervasive Systems, Algorithms and Networks \(ISPAN Dec. 13-15, 2012, pp.17-23 2  Big data: science in the petabyte era,” Nature 455 \(7209\:1, 2008 3  Douglas and Laney, “The importance of ‘big data’: A definition,”2008 4  X.K. Zhou, J. Chen, Q. Jin and T.K. Shih, “Organic Stream Meaningfully Organized Social Stream for Individualized Information Seeking and Knowledge Mining,” Proc. The 5th IET International Conference on Ubi- Media Computing \(U-Media2012\ Xining, China Aug. 16-18, 2012 5  S.Alsubaiee, Y. Altowim, H. Altwaijry, A. Behm, V. Borkar, Y. Bu, M Carey, R. Grover, Z. Heilbron, Y. Kim, C. Li, N. Onose, P. Pirzadeh, R Vernica, and J. Wen, “ASTERIX: an open source system for "Big Data management and analysis \(demo\” Proc. VLDB Endow, vol. 5, no. 12 August , 2012, pp. 1898-1901 6  S. Berkovich and D. Liao, “On clusterization of "big data" streams,” in Proc. the 3rd International Conference on Computing for Geospatial Research and Applications \(COM.Geo '12\, ACM, New York, NY USA, Article 26 , 6 pages 7  S.C.H. Hoi, J. Wang, P. Zhao, and R. Jin, “Online feature selection for mining big data,” in Proc. the 1st International Workshop on Big Data Streams and Heterogeneous Source Mining: Algorithms, Systems Programming Models and Applications \(BigMine '12\, ACM, New York, NY, USA, pp. 93-100 8  J. Zhang, T. Li, and Y. Pan, “Parallel rough set based knowledge acquisition using MapReduce from big data,” in Proc. the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications \(BigMine '12\. ACM, New York, NY, USA, pp. 20-27 9  A. Menon, “Big data @ facebook,” in Proc. the 2012 workshop on Management of big data systems \(MBDS '12\. ACM, New York, USA pp. 31-32   X. Han, L. Tian, M. Yoon, and M. Lee, “A Big Data Model Supporting Information Recommendation in Social Networks,” in Proc. the Second International Conference on Cloud and Green Computing \(CGC\ Nov 1-3, 2012, pp. 810-813   M. Yamagiwa, M. Uehara, and M. Murakami, “Applied System of the Social Life Log for Ecological Lifestyle in the Home,” in Proc International Conference on Network-Based Information Systems \(NBIS 09\, Aug. 19-21, 2009, pp. 457-462   T. Hori, and K. Aizawa, “Capturing life-log and retrieval based on contexts,” in Proc. IEEE International Conference on Multimedia and Expo \(ICME '04\, Jun. 27-30, 2004, pp. 301-304   K.S. Hwang, and S.B. Cho, “Life log management based on machine learning technique,” in Proc. IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems \(MFI\, Aug 20-22, 2008, pp.691-696   H.H. Kang, C. H. Song, Y.C. Kim, S.J. Yoo, D. Han, and H.G. Kim Metadata for efficient storage and retrieval of life log media,” in Proc IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems \(MFI\ Aug. 20-22, 2008, pp. 687-690   A. Shimojo, S. Matsumoto, and M. Nakamura, “Implementing and evaluating life-log mashup platform using RDB and web services,” in Proc. the 13th International Conference on Information Integration and Web-based Applications and Services \(iiWAS '11\, ACM, New York USA, pp. 503-506   A. Nakamura and N. Nishio, “User profile generation reflecting user's temporal preference through web life-log,” in Proc. the 2012 ACM Conference on Ubiquitous Computing \(UbiComp '12\, ACM, New York, USA, pp. 615-616   S. Ozdemir, “Functional reputation based reliable data aggregation and transmission for wireless sensor networks,” Comput. Commun. vol. 31 no. 17, pp. 3941-3953, November 2008 53 9 


  W.S. Jung, K.W. Lim, Y.B. Ko, and S.J. Park, “Efficient clusteringbased data aggregation techniques for wireless sensor networks,” Wirel Netw. Vol. 17, no. 5, pp. 1387-1400, July 2011   N. Iftikhar and T. B. Pedersen, “A rule-based tool for gradual granular data aggregation,” in Proc. the ACM 14th international workshop on Data Warehousing and OLAP \(DOLAP '11\, ACM, New York, USA, pp 1-8   F. Rahman, E. Hoque, and S.I. Ahamed, “Preserving privacy in wireless sensor networks using reliable data aggregation,” ACM SIGAPP Applied Computing Review, vol. 11, no. 3, pp.  52-62, August 2011   G. Wei, Y. Ling, B. Guo, B. Xiao, and A.V. Vasilakos, “Predictionbased data aggregation in wireless sensor networks: Combining grey model and Kalman Filter,” Comput. Commun. vol. 34, no. 6, pp. 793802, May 2011   F. Ren, J. Zhang, Y. Wu, T. He, C. Chen, and C. Lin, “Attribute-Aware Data Aggregation Using Potential-Based Dynamic Routing in Wireless Sensor Networks,” IEEE Trans. Parallel and Distributed Systems, vol 24, no. 5, pp. 881-892, May 2013   J.R. Getta, “Optimization of online data integration,” in Proc. 7th International Baltic Conference on Databases and Information Systems 2006, pp.91-97   M.P. Atkinson, J.I. Hemert, L. Han, A. Hume, and C.S. Liew, “A distributed architecture for data mining and integration,” in Proc. the second international workshop on Data-aware distributed computing DADC '09\, ACM, New York, USA, pp. 11-20   A.D. Sarma, X.L. Dong, and A. Halevy, “Data integration with dependent sources,” in Proc. the 14th International Conference on Extending Database Technology \(EDBT/ICDT '11\, Anastasia Ailamaki Sihem Amer-Yahia, Jignesh Pate, Tore Risch, Pierre Senellart, and Julia Stoyanovich \(Eds.\. ACM, New York, USA, pp. 40-412   V. Tran, O. Habala, B. Simo, and L. Hluchy, “Distributed data integration and mining,” in Proc. the 13th International Conference on Information Integration and Web-based Applications and Services iiWAS '11\, ACM, New York, USA, pp. 435-438   P. Gong, I. Gorton, and D.D. Feng, “Dynamic adapter generation for data integration middleware,” in Proc. the 5th international workshop on Software engineering and middleware \(SEM '05\, ACM, New York USA, pp. 9-16   540 





0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Bytes Read GB a Transferred data 0 20 40 60 80 100 120 140 160 180 200 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Repair Time seconds b Time network-critical cluster 0 20 40 60 80 100 120 140 160 180 200 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Repair Time seconds c Time computation-critical cluster Figure 6 Comparing the repair performance of HDFS-RAID HDFS-RAID-Optimized and CORE using two different sets of coding parameters 9,6,3 and 14,12,5 inspired respectively by the code length and storage overheads of Googles GFS and Microsoft Azure In these schemes the overhead of COREs extra parities are 1  3=33 and 1  5  20 accordingly In each case two different failure patterns were enforced a one-failure pattern represented by X and a two-failures pattern represented by XX  For the two-failures pattern both are set to happen in the same object i.e on the row The reason for this setting is two-fold i it favors the HDFSRAID since at almost the same cost it can repair two failures instead of one ii if two failures happen on different rows the experiment will be in effect a variation of the onefailure pattern From the results shown in Figure 6 we can draw several conclusions  For single failure the overhead of CORE is less than 50 of HDFS-RAID This is quite signi“cant since in real-world clusters e.g in the Facebook cluster single f ailures per stripe are by far the most common type of failures This improvement results from two inherent advantages of CORE i single failure can be repaired vertically using far fewer blocks and ii it uses a much cheaper XOR operation instead of expensive decoding/re-encoding this is particularly signi“cant in the computation-critical cluster  The impact of our rst HDFS-RAID optimization Opt1 in Section VI-B can be seen in the results the difference between the 2nd and the 3rd chart bars As explained before this optimization is targeted speci“cally for the clusters in which the network is a scarce resource part b in Figure 6 The improvements are particularly pronounced in cases where the number of avoided block retrievals are higher e.g one failure in the scheme 9,6,3  The gains from our second HDFS-RAID optimization Op2 in Section VI-B are also noticeable the 5th and the 6th chart bars in all setups  Growth in the CORE matrix size from 9,6,3 to 14,12,5 results in even higher gains especially in clusters where computation power is scarce B Repair Scheduling Algorithms In this set of experiments the three repair scheduling algorithms of Section V-C were compared using the Step and Plus failure patterns HDFS-RAID has neither a notion of repair scheduling  it treats objects independently  nor can it fully recover from the Plus failure pattern so it was not considered in the following experiments These experiments were run for CORE matrix of size 14,12,5 The results are shown in Figure 7 and as expected the data part of this gure part a  mirrors the analytical results presented in Table II Moreover the completion time numbers parts b and c  are also to large extent in-line with the data results The only two discrepancies are explained below  Completion time of the Column-First algorithm on the Plus pattern in the network-critical cluster part b  is longer than expected This is caused by the last repair which uses two other freshly-repaired blocks Accessing those blocks is delayed until NameNodes heartbeat-driven mapping tables are updated  Completion time of the RGS algorithm in the computationcritical cluster part c  is only slightly better than that of Column-First despite applying one vertical repair less see Table II for the schedules This is due to the fact that for these patterns the RGS and Column-First apply the same number of horizontal repairs and these are the main driving factor of the cost in the computation-critical cluster VIII C ONCLUSIONS AND F UTURE W ORK In this paper we demonstrated that some simple and standard techniques and thus easy to implement and organically 253 


0 0.5 1 1.5 2 2.5 3 Step Plus Row First Column First RGS B ytes R ea d GB a Transferred data 0 50 100 150 200 250 300 Step Plus Row First Column First RGS Repair Time seconds b Time network-critical cluster 0 50 100 150 200 250 300 Ste p Plus Row First Column First RGS Repair Time seconds c Time computation-critical cluster Figure 7 Performances of the repair scheduling algorithms on two different failure patterns integrate can provide signi“cant data repair and access boost in erasure coded distributed storage systems Specifically we studied our approach of introducing cross-object coding on top of normal erasure coding The ideas were implemented and integrated with HDFS-RAID available at  and benchmark ed o v er a proprietary cluster and EC2 Experiments with the implementation as well as accompanying analytical studies comparing the approach with not only MDS codes but also with the very recently proposed Local Reconstruction Codes used in Azure demonstrate the superior performance of CORE over state-of-the-art techniques for data reads and repairs While naive solutions can be readily used in future we will like to explore the CORE code properties to achieve better performance also during data insertion/updates The current evaluations are static based on snapshots of the system state We speculate that COREs better repair properties will yield a system in a better state over time We will thus carry out trace driven experiments to study the systems dynamics better R EFERENCES  P  Elias Error Free Coding  Transactions on Information Theory  vol 4 no 14 1954  HDFS-RAID http://wiki.apache.or g/hadoop/HDFSRAID  C Huang et al Erasure Coding in W indo ws Azure Storage  in USENIX ATC  2012  A Datta et al Redundantly Grouped Cross-object Coding for Repairable Storage in Proc APSys  2012  P  Gopalan et al On the locality of code w ord symbols  Information Theory IEEE Transactions on  vol 58 no 11 pp 6925…6934 2012  K S Esmaili et al The CORE Storage Primiti v e  CrossObject Redundancy for Ef“cient Data Repair and Access in Erasure Coded Storage CoRR  vol abs/1302.5192 2013  CORE http://sands.sce.ntu.edu.sg/StorageCORE  H W eatherspoon et al Erasure Coding vs Replication A Auantitative Comparison in Proc IPTPS  2002  J K ubiato wicz et al OceanStore An Architecture for Global-Scale Persistent Storage in Proc ASPLOS  2000  R Bhagw an et al T otal Recall System Support for Automated Availability Management in NSDI  2004  S Plank The RAID-6 Liber8T ion Code  Intl Journal of High Performance Computing Applications  vol 23 no 3 2009  A P atterson et al A Case for Redundant Arrays of Ine xpensive Disks RAID SIGMOD Records  vol 17 no 3 1988  O Khan et al Rethinking Erasure Codes for Cloud File Systems Minimizing I/O for Recovery and Degraded Reads in USENIX FAST  2012  B F a n e t al DiskReduce Replication as a Prelude to Erasure Coding in Data-Intensive Scalable Computing CMU Tech Rep CMU-PDL-11-112 2011  B Calder et al W indo ws Azure Storage A Highly A v ailable Cloud Storage Service with Strong Consistency in ACM SOSP  2011  A Thusoo et al Data W arehousing and Analytics Infrastructure at Facebook in ACM SIGMOD  2010  C Huang et al Pyramid Codes Fle xible Schemes to T rade Space for Access Ef“ciency in Reliable Data Storage Systems in IEEE NCA  2007  F  Oggier et al Coding T echniques for Repairability in Networked Distributed Storage Systems FnT in Communications and Information Theory  vol 9 no 4 2013  A Dimakis et al A Surv e y on Netw ork Codes for Distributed Storage The Proc of IEEE  vol 99 2011  A Duminuco et al Hierarchical Codes Ho w t o Mak e Erasure Codes Attractive for Peer-to-Peer Storage Systems in Proc P2P  2008  M Li et al GRID Codes Strip-Based Erasure Codes with High Fault Tolerance for Storage Systems ACM Trans on Storage  vol 4 2009  A K ermarrec et al Repairing Multiple F ailures with Coor dinated and Adaptive Regenerating Codes in Proc NetCod  2011  K W  Shum Cooperati v e Re generating Codes for Distributed Storage Systems in Proc ICC  2011  F  Oggier et al Self-Repairing Homomorphic Codes for Distributed Storage Systems in Proc INFOCOM  2011  F  Oggier et al Self-Repairing Codes for Distrib uted Storage A Projective Geometric Construction in Proc ITW  2011 26 D Papailiopoulos et al Locally Repairable Codes in Proc ISIT  2012  M S et al Xoring elephants No v e l erasure codes for big data Proceedings of the VLDB13 To appear  2013  L P amies-Juarez et al Data Insertion  Archi ving in Erasure-coding Based Large-scale Storage Systems in Proc ICDCIT  2013  L P amies-Juarez et al RapidRAID Pipelined Erasure Codes for Fast Data Archival in Distributed Storage Systems in Proc INFOCOM  2013  Y  Hu NCFS On the Practicality and Extensibility of a Network-Coding-Based Distributed File System in Proc NetCod  2011  R Li et al CORE Augmenting Re generating-coding-based Recovery for Single and Concurrent Failures in Distributed Storage Systems in Proceedings of IEEE MSST13  2013  J I Hall Notes on coding theory  Citeseer 2003  K V  Rashmi and others A Solution to the Netw ork Challenges of Data Recovery in Erasure-coded Distributed Storage Systems A Study on the Facebook Warehouse Cluster in Proceedings of USENIX HotStorage13  2013 254 


 L Kaufman and P  Rousseeuw  Clustering by means of medoids Technische Hogeschool Delft Netherlands Department of Mathematics and Informatics Tech Rep 1987  S Deerwester  S Dumais G Furnas T  Landauer  and R Harshman Indexing by latent semantic analysis  vol 41 no 6 pp 391…407 1990  C Boutsidis J Sun and N Anerousis Clustered subset selection and its applications on it service metrics in  2008 pp 599…608  C Boutsidis M W  Mahone y  and P  Drineas  An impro v ed approximation algorithm for the column subset selection problem in  2009 pp 968…977  C Boutsidis P  Drineas and M Magdon-Ismail Near optimal column-based matrix reconstruction in  2011 pp 305 314  J Dean and S Ghema w at MapReduce Simpli“ed data processing on large clusters  vol 51 no 1 pp 107…113 2008  T  White  1st ed OReilly Media Inc 2009  A Frieze R Kannan and S V empala F ast Monte-Carlo algorithms for nding low-rank approximations in  1998 pp 370 378  P  Drineas A Frieze R Kannan S V empala and V  V inay  Clustering large graphs via the singular value decomposition  vol 56 no 1-3 pp 9…33 2004  P  Drineas R Kannan and M Mahone y  F ast Monte Carlo algorithms for matrices II Computing a low-rank approximation to a matrix  vol 36 no 1 pp 158…183 2007  P  Drineas M Mahone y  and S Muthukrishnan Subspace sampling and relative-error matrix approximation Column-based methods in  Springer Berlin  Heidelberg 2006 pp 316…326  A Deshpande L Rademacher  S V empala and G W ang Matrix approximation and projective clustering via volume sampling  vol 2 no 1 pp 225…247 2006  A C  i vril and M Magdon-Ismail Column subset selection via sparse approximation of SVD  vol 421 no 0 pp 1  14 2012  A K F arahat A Ghodsi and M S Kamel  An ef cient greedy method for unsupervised feature selection in  2011 pp 161 170   Ef cient greedy feature selection for unsupervised learning  vol 35 no 2 pp 285…310 2013  T  Elsayed J Lin and D W  Oard P airwise document similarity in large collections with MapReduce in  2008 pp 265…268  A Ene S Im and B Mosele y  F ast clustering using MapReduce in  2011 pp 681…689  H Karlof f S Suri and S V assilvitskii A model of computation for MapReduce in  2010 pp 938…948  S Dasgupta and A Gupta An elementary proof of a theorem of Johnson and Lindenstrauss  vol 22 no 1 pp 60…65 2003  D Achlioptas Database-friendly random projections Johnson-Lindenstrauss with binary coins  vol 66 no 4 pp 671…687 2003  P  Li T  J Hastie and K W  Church V ery sparse random projections in  2006 pp 287…296  G Golub and C V an Loan  3rd ed Johns Hopkins Univ Pr 1996  A Deshpande and L Rademacher  Ef cient v olume sampling for row/column subset selection in  2010 pp 329 338  V  Gurusw ami and A K Sinop Optimal column-based lo wrank matrix reconstruction in  2012 pp 1207…1214  D D Le wis Y  Y ang T  G Rose and F  Li Rcv1 A ne w benchmark collection for text categorization research  vol 5 pp 361…397 2004  W Y  Chen Y  Song H Bai C.-J Lin and E Chang Parallel spectral clustering in distributed systems  vol 33 no 3 pp 568 586 2011  A T orralba R Fer gus and W  Freeman 80 million tin y images A large data set for nonparametric object and scene recognition  vol 30 no 11 pp 1958…1970 2008  N Halk o P G Martinsson Y  Shk olnisk y  and M T ygert An algorithm for the principal component analysis of large data sets  vol 33 no 5 pp 2580…2594 2011 
Journal of the American Society for Information Science and Technology Proceedings of the Seventeenth ACM Conference on Information and Knowledge Management CIKM08 Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms SODA09 Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science FOCS11 Communications of the ACM Hadoop The De“nitive Guide Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science FOCS98 Machine Learning SIAM Journal on Computing Approximation Randomization and Combinatorial Optimization Algorithms and Techniques Theory of Computing Theoretical Computer Science Proceedings of the Eleventh IEEE International Conference on Data Mining ICDM11 Knowledge and Information Systems Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies Short Papers HLT08 Proceedings of the Seventeenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD11 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODA10 Random Structures and Algorithms Journal of computer and System Sciences Proceedings of the Twelfth ACM SIGKDD international conference on Knowledge Discovery and Data Mining KDD06 Matrix Computations Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science FOCS10 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODA12 The Journal of Machine Learning Research Pattern Analysis and Machine Intelligence IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE Transactions on SIAM Journal on Scienti“c Computing 
180 


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even goodŽ partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity … the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the clouds elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPs synchronous barrier between supersteps offers a window for dynamic scaleout and …in at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an oracleŽ approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workers time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440…442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


