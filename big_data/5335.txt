Classification of Closed Frequent Patterns Improved by Feature Space Transformation   Cheng Hao Jin 1 Gouchol Pok 2 Hi-Seok Kim 3 Eun-Jong Cha 4 Keun Ho Ryu 1 000\015  1 Database/Bioinformatics Laboratory, Chungbuk National University, South Korea 2 Department of Computer Science, Yanbian Un iversity of Science and Technology, China 3 School of Electronics & Information Engineering, Cheongju University, South Korea 4 Department of Mechanical Engineering Chungbuk National University, South Korea kimsunghoyust@gmail.com  gcpok2000@gmail.com  khs8391@cju.ac.kr  ejcha@chungbuk.ac.kr  khryu@dblab.chungbuk.ac.kr    Abstract  In some real-world applications, the predefined features are not discriminative enough to represent well the distinctiveness of different classe s. Therefore, building a more well-defined feature space becomes an urgent task. The main goal of feature space transformation is to map a set of features defined in a space into a new more powerful feature space so that the classification based on the transformed data can achieve performance gain compared to the performance in the original space. In this paper, we introduce a feature transformation method in which the feature transformation is conducted using the closed frequent patterns. Experiments on real-world datasets show that the transformed features obtained from combining the closed frequent patterns and the original features are superior in terms of classification accuracy than the approach based solely on closed frequent patterns Keywords- Closed Frequent Pattern, Feature Space Transformation, Classification Accuracy I   I NTRODUCTION  000\003 Classification such as decision trees [1, 17  Su pp o r t  Vector Machine [2  n e u r al n etw ork  3  B a y e sia n  networks k-ne ar e s t  ne i g h b or 31   c a se b ased reasoning[32 et c h a s b e en r e ce iv ed c o n s i d e r a b l e  att enti on  in various communities. Classification is a supervised learning task that can predict the class labels of unseen instances. If the predefined features represent well the distinctiveness of different classes, the classification could achieve high classification performance. However, in some cases, original single features could not satisfactorily capture the intrinsic structure of each class and thus cannot provide useful information to the classifier. Thus, in order to improve the classification performance, there is an emerging need for constructing a good feature space for the complex structural data. Feature space transformation, which is used to transform the original single feature space to another new more powerful feature space, can solve this problem One of attempts to improve classification accuracy is to apply frequent patterns in the classification task. In this research area, frequent patterns are used as features which   000\015\000\003  Corresponding author make classification models more accurate and easier to understand. Frequent pattern mining \(also association mining\ was first proposed by Agrawal, Imielinski and Swami [4 and w i dely stu d ie d in th e pas t  de ca des   A f r e q u en t  pattern is generated from the given database with its frequency no less than user-specified minimum support count. Frequent patterns contain much more underlying semantics than single features since a frequent pattern is the combination of single features. However, usually a large number of frequent patterns can be generated at a given minimum threshold so that it is impracticable to use the whole frequent patterns as features and becomes difficult to interpret. This led the researchers to study closed patterns which can extract the whole frequent patterns even their exact frequencies without any information loss. Also the number of closed frequent patter ns is much smaller than that of frequent patterns. Thus, it is a good choice to use closed frequent patterns as features instead of the whole frequent patterns The contributions of this paper can be summarized as follows: 1\ We introduce closed frequent pattern-based feature space transformation, 2\ the extensive experimental results on real-world datasets show that the transformed feature space obtained from combing the closed frequent patterns and single features could achieve better classification accuracy than th at solely on closed frequent patterns The rest of the paper is organized as follows. In Section II, we review related work. The basic terminology and the problem definition are presented in Section III. The extensive experimental evaluations on real-world datasets are reported in Section IV and our conclusion and future work are given in Section V II  R ELATED WORK  In this paper, we use closed frequent patterns as features to represent the data and then classification method is applied on this transformed feature space. Thus, our research belongs to pattern-based classification task. Associative classification [5, 6, 7, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25 26, 27  w h ich in teg r ates cl ass i f i c ati on an d ass o ci ati o n  mining, is related to our research topic. During last decades 978-0-7695-4108-2/10 $26.00 © 2010 IEEE DOI 10.1109/CIT.2010.235 1306 2010 10th IEEE International Conference on Computer and Information Technology \(CIT 2010 978-0-7695-4108-2/10 $26.00 © 2010 IEEE DOI 10.1109/CIT.2010.235 1306 2010 10th IEEE International Conference on Computer and Information Technology \(CIT 2010 978-0-7695-4108-2/10 $26.00 © 2010 IEEE DOI 10.1109/CIT.2010.235 1306 


there are a large number of publications about associative classification, however, here we list a few of them CBA\(Classification based on Associations\ proposed by Liu, Hsu and Ma is the first frequent pattern-based classification method [5  A t first it g e ne rat e s a ll th e cl ass  association rules and ranks them in the descending order of confidence and support. Rule selection is performed on the ranked rule list with a sequential coverage paradigm and prediction is based on whether the rule condition satisfies the test instance. If there is no rule to match the test instance, the takes the default class value like C4.5 CMAR\(Classification based on Multiple Association Rules\ another pattern-based classification method proposed by Li, Han and Pei [6  To av oi d c lass i f i cati on  bias  an d overfitting caused by the single rule in classification, CMAR predicts the class label with multiple rules and the classification task uses a weighted 2 on multiple high confidence rules. To predict a test instance, all rules matching the test instance are collected. If all of them have the same class label, assigns this class label to test instance However, if they have different class labels, CMAR divides the rules into groups according to class labels and compare the effects of the groups and then assign the class label of the strongest group CPAR\(Classification based on Predictive Association Rules\ proposed by Yin and Han [7 u s es th e b a sic i d ea  o f  FOIL[18 in  ru le g e n e r ati on   CP A R u s es a g r e e d y alg or ith m  to generate rules directly from the training data and selects multiple literals and builds multiple rules simultaneously To avoid overfitting, it uses expected accuracy to evaluate rules and choose the best k rules in prediction Top-k rule mining[13  d i sco v er s t o p k co ve ri ng r u l e  groups for each row of gene expression profiles for patternbased classification. It uses several pruning steps, such as top-k pruning based on support and confidence upper bounds and backward pruning, to filter the uninteresting rules. Then RCBT classifier is build from these top-k covering rule groups and prediction is performed based on the classification score which combines the support and confidence measures of the rules Almost all of associative classification methods have a drawback that should manipulate a large number of association rules. Thus, it is impracticable to use frequent patterns as features in classification tasks. To overcome this problem, we can use closed frequent patterns in patternbased classification instead of the whole frequent patterns because the number of closed frequent patterns is much smaller than that of frequent patterns The more recent works that most relate to our research are [8, 9  A ll of th em u s e cl os ed f r e q u e nt p att ern s as  features for choosing the most discriminative patterns however, any of them do not mention how the transformed feature space affects the classification accuracy with respect to different minimum support thresholds III  B ASIC T ERMINOLOGY AND PROBLEM DEFINITION  Given a database D and a user-specified minimum support threshold 0  1  F = {f 1 f 2 f m  is the set of all categorical features and C = {c 1 c 2 c k  is the set of class labels. Database D contains n transactions D = {x i  y i  n i 1 where x i  F is a set of features and y i  C is a class label. A subset of F is called an itemset. The support of an itemset X is defined as the number of instances that contain the itemset X and denoted as support\(X For numeric features, the continuous values are discretized first Definition 1  Frequent Pattern For a database D an itemset X is frequent if support\(X  n where n is the number of database transactions  Example Given a transaction database TDB in Fig. 1 \(a and  0.4, then n 5 and minimum support count = 5 * 0.4 2. The generated frequent patterns are shown in Fig 1 \(b   Figure 1. \(a\Transaction database and \(b\equent patterns Definition 2  Closed Frequent Pattern A frequent itemset X is closed if there does not exist an itemset Y such that X Y and support\(X\ = support\(Y   Example Still take Fig. 1 \(a\ as an example, where n 5 and minimum support count = 2. According to the property of closed frequent pattern, the generated closed frequent patterns are shown in Fig. 2 \(b\. From Fig. 2 \(b\, we can see that the number of closed frequent patterns is smaller than that of frequent patterns in Fig. 1 \(b   Figure 2. \(a\Transaction database and \(b\losed frequent patterns 
1307 
1307 
1307 


Definition 3  Feature Space Transformation Assume P = {p 1 p 2 p p  be the set of closed frequent patterns generated from the given database D at a given  Therefore, the feature space transformation can be expressed as follows D = {x i y i  n i 1  Dí = {z i y i  n i 1 where z i  P is a set of generated closed frequent patterns  Example Still take Fig. 1 \(a\ as an example and the generated closed frequent patterns are illustrated in Fig. 2 \(b Then the transformed feature space is shown in Fig. 3 \(b   Figure 3. \(a\ original transaction database and \(b\ansformed feature space IV  E XPERIMENTAL RESULTS  We use several datasets from UCI Machine Learning Repository [10 to test th e  ef f e ct of  cl ose d  f r e q u e nt pat t e r n based feature space transformation for the classification accuracy. In these experiments, we compare classification results of two transformed feature space. One feature space is from solely on closed frequent patterns and the other is from combining closed frequent patterns and single features. We show the results at a wide range of s. A summary of these datasets is shown in Table I TABLE I  D ATASET USED IN EXPERIMENTS  Dataset Instances Features Classes Diabetes 768 8 2 Glass 214 9 7 Heart-statlog 270 13 2 Iris 150 4 3 Waveform 5000 40 3 Wine 178 13 3 Zoo 101 16 7  First, all the continuous features in these datasets are discretized into binary ones with entropy-based method [11  and discretizations are performed with Weka version 3.6.0 1 T h e dis c r eti ze d d atas ets  a r e  sh ow n  in T a bl e I I   TABLE II  DISCRETIZED  D ATASET  Dataset Instances n  Features F  Classes C  Diabetes 768 15 2 Glass 214 20 7 Heart-statlog 270 18 2 Iris 150 12 3 Waveform 5000 109 3 Wine 178 37 3 Zoo 101 34 7  The state-of-the-art C4.5 [1 and SM O in Weka an d  C5.0 in Clementine are chosen as classification models Notice that, classification accuracy is the primary evaluation criterion for the experiments and 10-fold cross validation is used. The results of classification accuracy from the transformed feature space are shown in Table III, IV, V, VI VII, VIII and IX. In these tables P is the set of closed frequent patterns and F P is the set of features union of original single features and closed frequent features. From different maximum value of listed in each table, we can see that the stop point of generating closed frequent is different in each dataset respectively. These tables also give average classification accuracy. From these tables, we conclude that at most cases the transformed feature space from F P achieves much higher classification accuracy than features only from P  TABLE III  C LASSIFICATION RESULTS ON DIABETS     C4.5 SMO C5.0 P F P P F P P F P 5 77.99 77.99 76.30 76.30 79.95 79.95 10 77.86 77.86 77.08 77.08 79.30 79.30 15 76.56 76.56 74.09 74.09 78.65 78.65 20 76.17 75.78 75.39 75.00 79.30 79.56 25 75.91 75.91 75.65 74.09 77.73 79.56 30 71.22 76.04 70.70 75.13 70.96 78.65 35 71.22 75.91 67.84 76.69 70.96 78.39 40 69.40 76.04 68.49 75.78 70.31 78.39 45 69.40 76.17 68.10 75.91 70.44 78.52 50 69.40 75.91 68.62 74.74 70.31 78.39 55 69.14 75.78 68.23 77.08 70.05 78.65 60 69.14 75.78 68.23 77.08 70.05 78.65 65 69.14 75.78 68.23 77.08 70.05 78.65 70 70.05 75.78 69.14 77.08 70.05 78.65 75 67.84 75.78 67.84 76.95 67.84 78.65 Average 72.03 76.21 70.93 76.01 73.06 78.84 Single Feature 75.78 76.82 78.65 TABLE IV  C LASSIFICATION RESULTS ON GLASS    C4.5 SMO C5.0 P F P P F P P F P 5 77.10 77.10 80.37 80.37 79.91 79.91 10 76.17 76.17 78.97 78.97 82.24 82.24 
1308 
1308 
1308 


15 76.64 76.64 77.10 77.10 80.37 80.37 20 72.90 76.64 77.10 78.04 77.57 80.37 25 72.43 76.17 77.57 78.50 78.04 80.37 30 72.90 76.64 77.57 78.97 78.50 80.37 35 71.03 74.77 76.17 78.50 77.57 79.44 40 69.63 74.77 71.03 79.44 75.23 79.44 45 68.69 74.77 69.63 74.30 71.96 79.44 50 68.69 72.90 70.09 75.70 71.96 79.44 55 65.42 73.83 62.62 76.17 66.82 79.44 60 64.49 74.77 63.08 73.83 66.82 79.44 65 50.47 74.77 48.60 73.83 50.00 79.44 70 50.47 74.77 48.60 75.23 50.00 79.44 75 49.07 74.77 49.07 76.17 47.20 79.44 80 49.07 74.77 45.79 74.77 47.20 79.44 85 47.20 74.30 47.20 75.70 47.20 79.44 Average 64.84 75.21 65.92 76.80 65.76 79.85 Single Feature 74.77 73.36  79.44 TABLE V  C LASSIFICATION RESULTS ON HEART STATLOG    C4.5 SMO C5.0 P F P P F P P F P 5 82.96 82.96 80.37 80.37 90.00 90.00 10 85.93 85.93 80.37 80.37 90.37 90.37 15 83.33 83.33 81.48 81.48 88.89 88.89 20 81.11 81.11 81.11 81.11 88.89 88.89 25 79.63 79.63 83.33 83.33 87.78 87.78 30 83.33 83.33 78.89 78.89 88.89 88.89 35 81.85 81.85 80.74 80.74 87.41 87.41 40 80.74 80.74 83.33 82.96 87.04 87.04 45 82.96 82.96 83.70 81.48 87.04 87.04 50 82.59 82.59 83.70 82.59 86.67 86.67 55 81.11 80.37 82.22 84.07 83.70 87.04 60 72.96 81.48 72.96 83.33 74.07 87.41 65 72.96 81.48 72.96 83.33 74.07 87.41 70 70.00 81.48 70.00 84.44 70.00 87.41 75 70.00 81.48 70.00 84.44 70.00 87.41 Average 79.43 82.05 79.01 82.20 83.65 87.98 Single Feature 81.85 84.07 87.41 TABLE VI  C LASSIFICATION RESULTS ON IRIS   C4.5 SMO C5.0 P F P P F P P F P 5 95.33 95.33 94.67 94.67 96.00 96.00 10 95.33 95.33 94.67 94.67 96.00 96.00 15 94.00 94.00 94.67 94.67 96.00 96.00 20 94.00 94.00 94.67 94.67 96.00 96.00 25 94.00 94.00 94.00 94.00 96.00 96.00 30 94.00 94.00 94.67 94.67 96.00 96.00 35 94.67 94.67 95.33 94.67 96.00 96.00 Average 94.48 94.48  94.67 94.57  96.00 96.00 Single Feature 94.00 94.00 96.00 TABLE VII  C LASSIFICATION RESULTS ON WAVEFORM    C4.5 SMO C5.0 P F P P F P P F P 5 73.06 73.04 78.68 78.66 84.38 84.38 10 74.66 75.58 85.36 85.58 86.26 86.16 15 74.24 76.16 82.86 85.42 82.06 85.80 20 73.06 75.66 77.78 85.84 78.30 87.20 25 62.94 76.06 64.30 85.92 66.44 87.06 30 52.44 76.10 51.18 85.96 52.90 87.18 35 40.50 76.24 41.10 85.92 41.10 87.18 40 40.50 76.24 41.10 85.92 41.10 87.18 Average 61.43 75.64 65.30 94.90 66.57 86.82 Single Feature 76.44 85.96 87.32 TABLE VIII  C LASSIFICATION RESULTS ON WINE    C4.5 SMO C5.0 P F P P F P P F P 5 94.94 94.94 97.75 97.75 98.88 98.88 10 94.38 94.38 98.88 98.88 98.88 98.88 15 95.51 95.51 98.31 98.31 98.88 98.88 20 96.07 96.07 97.75 97.19 99.44 99.44 25 95.51 94.94 96.63 97.75 98.88 98.88 30 95.51 94.38 94.38 98.31 99.44 98.31 35 92.13 93.26 94.38 98.88 97.19 98.31 40 94.94 96.63 94.38 98.88 98.88 97.75 45 95.51 96.63 93.82 99.44 97.19 98.88 50 95.51 96.63 95.51 98.88 97.19 98.88 55 96.07 96.07 94.94 98.88 96.63 98.88 60 82.02 95.51 81.46 98.88 86.52 100 65 73.60 95.51 71.91 98.31 76.97 100 70 74.72 95.51 72.47 98.31 76.40 98.88 75 44.38 95.51 44.38 98.31 44.38 98.88 80 44.38 95.51 44.38 98.31 44.38 98.88 85 44.38 95.51 44.38 98.31 44.38 98.88 Average 82.91 95.44 83.28 98.45 85.56 98.91 Single Feature 95.51 98.31 98.88 TABLE IX  C LASSIFICATION RESULTS ON ZOO    C4.5 SMO C5.0 P F P P F P P F P 5 93.07 93.07 96.04 96.04 100 100 10 88.12 88.12 96.04 96.04 100 100 15 91.09 91.09 96.04 96.04 100 100 20 90.10 90.10 96.04 96.04 100 100 25 89.11 89.11 92.08 96.04 98.02 100 30 90.10 90.10 92.08 96.04 98.02 100 35 89.11 89.11 93.07 97.03 97.03 99.01 40 91.09 91.09 93.07 96.04 97.03 99.01 45 91.09 91.09 93.07 96.04 97.03 99.01 50 90.10 90.10 93.07 96.04 97.03 99.01 55 91.09 91.09 93.07 96.04 97.03 99.01 
1309 
1309 
1309 


60 86.14 91.09 87.13 95.05 89.11 99.01 65 87.13 93.07 85.15 96.04 89.11 99.01 70 87.13 93.07 86.14 96.04 89.11 99.01 75 89.11 93.07 87.13 97.03 89.11 99.01 80 81.19 93.07 79.21 97.03 79.21 99.01 85 40.59 93.07 40.59 96.04 40.59 99.01 Average 86.20 91.21 88.18 96.16 91.61 99.36 Single Feature 92.08  97.03 99.01  V  C ONCLUSION  AND FUTURE WORK  In this paper, we introduce closed frequent pattern-based feature space transformation and the experimental results on real-world datasets show that the transformed feature space from union of closed frequent patterns and original features achieves much higher classification accuracy than that from only the closed frequent patterns with respect to various s In the future, I plan to extend my research to mining the most discriminative closed frequent patterns efficiently from a large number of frequent patterns, to address the real-world application challenges and further improve the learning performance  A CKNOWLEDGMENT  This research was financially supported by the Ministry of Education, Science Technology \(MEST\ and Korea Industrial Technology Foundation \(KOTEF\ through the Human Resource Training Project for Regional Innovation and also supported by Basic Science Research Program through the National Research Foundation of Korea \(NRF funded by the Ministry of Education, Science and Technology \(NRF No. 2010-0001732  R EFERENCES   1  J. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann 1993 2  Corinna Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20:273-297, 1995 3  R. Duda, P. Hart, and D. Stork. Pattern Classification. Wiley Interscience, 2nd edition, 2000 4  R. Agrawal, T. Imielinski, and A. Swami  Mining association rules between sets of items in large databases  Proc. SIGMOD, 1993, pp 207-216 5  B. Liu, W. Hsu, and Y. Ma  Integrating classification and association rule mining  Proc. Fourth International conference on Knowledge Discovery and Data Mining\(KDD\, 1998, pp. 80-86 6  W. Li, J. Han, and J. Pei  CMAR: Accurate and efficient classification based on multiple class-association rules  Proc International Conference on Data Mining\(ICDM\, 2001, pp. 369-376 7  X. Yin and J. Han  CPAR: Classification based on predictive association rules  Proc. SIAM International Conference on Data Mining \(SDM'03\, 2003, pp. 331-335 8  H. Cheng, X. Yan, J. Han, and C.-W. Hsu  Discriminative frequent pattern analysis for effective classification  Proc. International Conference on Data Engineering\(ICDE 07\, Turkey, 2007, pp. 716  725 9  W. Fan, K. Zhang, H. Cheng, J. Gao. X. Yan, J. Han, P. S. Yu O Verscheure  Direct Mining of Discriminative and Essential Frequent Patterns via Model-based Search Tree  Proc 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 2008, pp. 230-238   D. Newman, S. Hettich, C. Blake,C. Merz  UCI Repository of Machine Learning Databases  1998   U. Fayyad, K. Irani  Multi-interval Discretization of Continousvalued Attributes for Classification Learning  Proc. IJCAI 1993, pp 1022  1027   I. Witten, E. Frank  Data Mining: Practical Machine Learning Tools and Techniques  Morgan Kaufmann, San Francisco, 2nd edition 2005   G. Cong, K. Tan, A. Tung, and X. Xu  Mining top-k covering rule groups for gene expression data  Proc. ACM International Conference on Management of Data \(SIGMOD\, 2005, pp. 670-681    J. Wang and G. Karypis  HARMONY: Efficiently mining the best rules for classification  Proc. SIAM international conference on Data Mining Proceedings \(SDM'05\ 2005, pp. 205-216   F. Thabtah., P. Cowling and Y. Peng MCAR: Multi class Classification based on Association Rule Proc. IEEE International Conference on Computer Systems and Applications ,2005, pp. 127133    A. Zimmermann and L. De Raedt  Corclass : Correlated association rule mining for classification  In Discovery Science, volume 3245/2004 of Lecture Notes in Computer Science, 2004, pp. 60  72     L. Breiman, J. Friedman, R. Olshen and C. Stone  Classification and regression trees  Wadsworth Intl., 1984   J. R. Quinlan and R. M. Cameron-Jones  FOIL: A midterm report   Proc. European Conf. Machine Learning, Vienna, Austria, 1993, pp 3-20   D. Lo, H. Cheng, J. Han, S. Khoo, and C. Sun Classification of software behaviors for failure detection: a discriminative pattern mining approach  Proc. 15th ACM SIGKDD International Conference on Knowledge Discovery and Data mining, 2009, pp 557  566   G. Dong, X. Zhang, L. Wong, and J. Li CAEP:classi fication by aggregating emerging patterns  Proc. of The Second International Conference on Discovery Science \(DS'99\, 1999, pp. 43-55   J. Li, G. Dong, K. Ramamohanarao, and L. Wong  DeEPs: a new instance-based lazy discovery and classification system  Machine Learning, 2002, 54\(2\99-124   F. Thabtah, P. Cowling, and Y. Peng MCAR: Multi class Classification based on Association Rule IEEE International Conference on Computer Systems and Applications ,2005, pp. 127133   F. Thabtah, P. Cowling, and Y. Peng MMAC: A new multi class multi-label associative classification approach  Proc. Fourth IEEE International Conference on Data Mining\(ICDM\, 2004, pp. 217-224   Z. Tang and Q. Liao  A New Class Based Associative Classification Algorithm  Proc. International Multiconference of Engineers and Computer Scientists, 2007, 685-689   G. Chen, H. Liu et al, ìA new approach to classification based on  association rule mining Decision Support Systems 42, 2006, pp  674-689   R. Thonangi and V. Pudi  ACME: An associative classifier based on maximum entropy principle  Proc. 16th International Conference on Algorithmic Learning Theory \(ALT\, Singapore, Springer, 2005 122  134   X  Li., D  Qin., C. Yu  ACCF: Associative Classification Based on Closed Frequent Itemsets  Proc. Fifth International Conference on Fuzzy Systems and Knowledge Discovery \(FSKD\, 2008, pp. 380384   A. Veloso, W. Meira Jr., and M. Zaki  Lazy associative classification  Proc. Sixth International Conference on Data Mining\(ICDM\, 2006, pp. 645-654 
1310 
1310 
1310 


  B. Arunasalam and S. Chawla  Cccs: a top-down associative classifier for imbalanced class distribution  Proc  the 12th ACM SIGKDD international conference on Knowledge Discovery and Data Mining\(ICDM  2006, pp  517  522   Pearl, J. & Russel, S  Bayesian networks. Report \(R-277  In Proc Handbook of Brain Theory and Neural Networks, . Arbib, ed, MIT Press, Cambridge, 2000, pp. 157  160   D. Aha, D. Kibler and M. Albert  Instancebased learning algorithms   Machine Learning., 1991, pp. 37  66   I. Watson and F. Marir  Case-based reasoning: A review  The Knowledge Engineering Review, 1994                   
1311 
1311 
1311 


Figure 3 Precision-recall curve in ROI-250 image dataset 5.3 Experiment 3 Mammograms-1080 image dataset This experiment employed a dataset composed by 1080 mammograms images collected in the Clinical Hospital of University of Sao Paulo at Ribeiro Preto The dataset was previously classi\002ed into 4 levels of breast tissue density 0501\051 mostly fatty 050362 images\051 0502\051 partly fatty 050446 images\051 0503\051 partly dense 050200 images\051 and 0504\051 mostly dense 05072 images\051 Figure 4 Precision-recall curve in Mammography-1080 image dataset Breast density is an important risk factor in the development of breast cancer In this experiment the images are represented by the feature set proposed in b uilding a vector of 85 features including shape and size of the breast the conditions of the breast contour nipple position and the distribution of 002broglandular tissue This dataset was divided in training set and test set The training set is composed of 720 images and test set is composed of 360 images Figure 4 shows the P&R curves over test dataset and also the number of features selected in each method Again the proposed methods reached the highest values of precision and select the smallest number of features 050a\051 050b\051 050c\051 050d\051 Figure 5 Queries in Mammography dataset 050a\051 226 is the query image 050b\051 using the features selected through 002tness function F cB  050c\051 using the features selected through classi\002cation error of C4.5 and 050d\051 using the all features extracted Results for the retrieval of the 5 most similar images from a query image are also provided in this experiment as illustrated in Figure 5 The image 5.\050a\051 is the query image taken from the mostly fatty image class Images shown in 5.\050b\051 are the 5 most similar images retrieved for the proposed 002tness function F cB  The row 050c\051 shows the results for C 4  5 classi\002er whereas 050d\051 illustrate the images resulting from all features 050no selection applied\051 In Figure 5 the images surrounded by dashed lines are false positives 050not relevant images\051 For this query the proposed method achieved the highest precision 050100%\051 when compared the results of C4.5 and the original feature vector 050precision of 40%\051 


6 Conclusions This work proposed a novel genetic feature selection framework for CBIRs It employs a wrapper strategy that searches for the best reduced feature set while optimizing 050or preserving\051 the quality of the solution From a ranking evaluation function three new 002tness functions namely F cA  F cB and F c have been proposed and evaluated in three experiments The proposed genetic feature selection approach which encompasses F cA  F cB and F c  has been compared with 050a\051 traditional methods found in the literature 050b\051 the StARMiner feature selector and 050c\051 the whole feature vector and signi\002cantly outperformed them The proposed approach has been able to optimize the accuracy of similarity queries while selecting a signi\002catively reduced number of features Additionally the proposal of combining the quality of the query results with the criterion of minimizing the number of selected features F cA and F cB  led to high accurate query answers while reducing the number of features more than the 002tness function F c  Therefore the 002nal processing cost of the queries is also reduced References  P  M d Aze v edo-Marques N A Rosa A J M T raina C Traina-Jr S K Kinoshita and R M Rangayyan Reducing the semantic gap in content-based image retrieval in mammography with relevance feedback and inclusion of expert knowledge International Journal of Computer Assisted Radiology and Surgery  3\0501-2\051:123\226130 June 2008  R Baeza-Y ates and B Ribeiro-Neto Modern Information Retrieval  Addison-Wesley Essex UK 1999  B Bartell G Cottrell and R Bele w  Optimizing similar ity using multi-query relevance Journal of the American Society for Information Science  49:742\226761 1998  O Cord 264 on E Herrera-Viedma C L 264 opez-Puljalte M Luque and C Zarco A review on the application of evolutionary computation to information retrieval International Journal of Approximate Reasoning  34:241\226264 July 2003  J G Dy  C E Brodle y  A Kak L S Broderick and A M Aisen Unsupervised feature selection applied to content-based retrieval of lung images IEEE Transactions on Pattern Analysis and Machine Intelligence  25\0503\051:373\226 378 March 2003  W  F an E A F ox P  P athak and H W u The ef fects of 002tness functions on genetic programming-based ranking discovery for web search Journal of the American Society for Information Science and Technology  55\0507\051:628\226636 2004  W  F an P  P athak and M Zhou Genet ic-based approaches in ranking function discovery and optimization in information retrieval a framework Decision Support Systems  2009  D E Golber g Genetic algorithms in search optimization and machine learning  Addison Wesley 1989  R L Haupt and S E Haupt Practical Genetic Algorithms  John Wiley  Sons New Jersey United States second edition edition 2004  J Horng and C Y eh Applying genetic algorit hms to query optimization in document retrieval Information Processing  Management  36:737\226759 2000  S K Kinoshita P  M d Aze v edo-Ma rques R R PereiraJr J A H Rodrigues and R M Rangayyan Contentbased retrieval of mammograms using visual features related to breast density patterns Journal of Digital Imaging  20\0502\051:172\226190 June 2007  F  K orn B P agel and C F aloutsos On the  dimensionality curse and the self-similarity blessing IEEE Trans on Knowledge and Data Engineering  13\0501\051:96\226111 2001  H Liu and L Y u T o w ard inte grating feature select ion algorithms for classi\002cation and clustering IEEE Transactions on Knowledge and Data Enginnering  17\0504\051:491\226502 April 2005  M X Ribeiro A J M T raina C T raina-Jr  and P  M Azevedo-Marques An association rule-based method to support medical image diagnosis with ef\002ciency IEEE Transactions on Multimedia  10\0502\051:277\226285 2008  U S Cancer Statistics W orking Group United states cancer statistics 1999-2005 incidence and mortality webbased report atlanta 050ga\051 Department of health and human services centers for disease control and prevention and national cancer institute 2009 Available in http://apps.nccd.cdc.gov/uscs   L T amine C C and M Boughanem Multiple query evaluation based on an enhanced geneticnext term algorithm Information Processing  Management  39\0502\051:215\226 231 2003  R S T orres A X  F alc 230 ao M A Gonc\270alves J P Papa Z B W Fan and E A Fox A genetic programming framework for content-based image retrieval Journal of the American Society for Information Science and Technology  42\0502\051:283\226292 2009  A Tsymbal P  Cunningham M P echenizkiy  and S Puuronen Search strategies for ensemble feature selection in medical diagnostics In Proceedings of the 16th IEEE Symposium on Computer-Based Medical Systems  pages 124\226 129 June 2003  A Tsymbal M Pechenizkiy  and P  Cunningham Sequential genetic search for ensemble feature selection In Proceedings of the International Joint Conferences on Arti\002cial Intelligence  pages 877\226882 August 2005  C.-M W ang a and Y F  Huang Ev olutionary-based feature selection approaches with new criteria for data mining A case study of credit approval data Expert Systems with Applications  36\0503 Part 2\051:5900\2265908 2009  H Y an J Zheng Y  Jiang C Peng and S Xiao Selecting critical clinical features for heart diseases diagnosis with a real-coded genetic algorithm Applied Soft Computing  8:1105\2261111 2008  T  Zhao J Lu Y  Zhang and Q Xiao Feature selection based on genetic algorithm for cbir In IEEE Congress on Image and Signal Processing  volume 2 pages 495\226499 2008 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnêt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnêt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnêt create sound clips  A5.1 We donêt need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIês   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





