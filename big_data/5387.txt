Abstract 
Chunming Rong Department of Science and Technology University of Stavanger Stavanger, Norway chunming.rong@uis.no Zhou Quan School of Mathematics and Information Science Guangzhou University Guangzhou, China zhouqq@gzhu.edu.cn Antorweep Chakravorty Department of Science and Technology University of Stavanger Stavanger, Norway antorweep.chakravorty@uis.no 
Hadoop is a distributed Big Data storage and 
On Access Control Schemes for Hadoop Data Storage 
processing framework hugely ad opted in different sectors from online media, education, gov ernment and social media to handle the enormous growth of information in their respective domains. However, the core arch itecture of the solution is based on a trusted cluster It lacks native methods for protecting sensitive data that cross over enterprises and are exposed or accessed illegally. In this paper, propose an access control scheme for data stored in Hadoop, based on concepts from BitTorrent and the secure sharing storage over the cloud The proposed schemes can im pose access control for data owners, and prevent unauthorized access and illegal authorization Keywordsaccess control; Hadoop; authorization; BitTorrent 
secure sharing storage 
I I NTRODUCTION Research in newer ways of st oring and processing data has led to the development of various No-SQL based  These newer t echnologies enable shift in paradigm from traditional relat ional solutions. Unlike traditional DBMS/RDBMS solutions they offer capabilities of storing and processing en ormous amounts of data. They enable aggregation of different data sources from structured semi-structured and unstructur ed. With distributed & parallel architectures, they offer competencies of processing large historical datasets \(often in TeraBytes 
continuously gather data Among others, the Hadoop platform has emerged as a forerunner for big data processing and storage ganizations such as Facebook Yahoo, IBM and Microsoft as well as other government and educational institutions have widely accepted it as key part of their data architecture. Amazon & Oracle wrapped Hadoop as one of their key bu sinesses by elevating it to the cloudî and offering ìPlatform  r  the core architecture of the Hadoop framework was conceived for a trusted cluster environment, with a small elite set of advanced programmers as its user base. Until recently, security aspects of the Hadoop framework remained wide 
However with its rapid acceptance in different sectors and its use for handling critical data, the need for proper data protection and access mechanisms have become quite prevalent In the paper, we propose two novel access control schemes for the Hadoop data storage, addressing unauthorized access and illegal authorization attacks on data stored in the different nodes th roughout the Hadoop cluster The first scheme is based on virtual piece concepts from  signed with the secure sharing  e thodology The remainder of this paper is organized as follows Section 2 provides a background on the Hadoop framework its architecture, process flow and security risks. We also 
Access Control Schemes for Ha doop data storage A Hadoop Architecture 
introduce concepts from the BitTorrent and secure sharing storage over the cloud. Next we introduce our in Section 3 Section 4 concludes the paper w ith discussion of future work II B ACKGROUND The following subsection sections, provides an overview of the Hadoop architecture, its process flows associated security risks, overview of the BitTorrent protocol for peer-to-peer file sharing and the secure sharing storage over the cloud methodology Hadoop is an open source framework for distributed 
storage and data-intensive processing, first developed by Yahoo. It offers a file system called Hadoop Distributed File System \(HDFS\ [7 and computing paradigm MapReduce 9 HDFS is a distri but ed file system that splits and stores data on nodes throughout a cluster, with a number of replicas. It provides an ex tremely reliable, fault tolerant consistent, efficient and cost-effective way to store large amounts data. The MapReduce pa radigm provides a model for computation on huge am ounts of data spread across a cluster. It consists of two key functions: Mapper and Reducer. The Mapper processes input data splits in parallel 
through different map tasks and sends sorted, shuffled outputs to the Reducers that in turn groups and processes them using reduce tasks for each group The HDFS stores the data sp lits into multiple nodes called DataNodes. The namespace & meta-data of each file and its blocks are stored in a master node called NameNode The MapReduce model enables distributed processing of data stored throughout the HDFS. A JobTracker node schedules & coordinates a number of map & reduce tasks on multiple data processing nodes called Task Trackers. The 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2829-3/13 $26.00 © 2013 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.82 644 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2829-3/13 $26.00 © 2013 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.82 641 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2830-9/14 $31.00 © 2014 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.82 641 


002 002 002 002 002 
architecture and process flow is given in Figure 1. The TaskTrackers are always deployed in the DataNodes Figure 1 Hadoop Architecture and Process Flow There are two types of proc ess flow for accessing processing data in the Hadoop framework: one being direct operations on the HDFS and other being processing data using the MapReduce paradigm HDFS Operations Process flows of HDFS include the read and write operations. HDFS read operations on a data file, from Hadoop clients need locations of its data blocks stored across different nodes. An RPC request to the NameNode returns the addresses \(metadata\Nodes that has a block of the requested file. The returned addresses of the DataNodes are used directly by Hadoop clients to read the data blocks. Write operations from Hadoop clients, sends a request to the NameNode for writing a new file. The NameNode performs checks to ensure that the file does not already exist and that the client has the right permissions. It next makes a record of the new file and returns a list of suitable DataNodes to store the file blocks. The Hadoop client then, splits an d writes the data blocks to the respective DataNodes MapReduce Jobs MapReduce jobs are scheduled and coordinated by the JobTracker. Hadoop clients submit jobs to the JobTracker. It in-turn consults the NameNode to learn the addresses of DataNodes that have the blocks of the input files for a job The JobTracker in itiates the TaskTrackers running near or on those DataNodes that contains the blocks of the input files. The TaskTrackers start a Map task and monitor its progress and report its status to the JobTracker. Each node stores the intermediate data on local disk after completion of a Map task. Simultaneously, the JobTracker also starts Reduce tasks on TaskTrackers. All intermediate data from Map tasks that have same output keys are sent to be respective Reduce tasks for final compu tation. Outputs are written back into the HDFS The Hadoop framework is designed for a trusted environment. Its operations ar e based on assumptions that clusters would consist of coop erating, trusted machines used by trusted users in a trusted environment. There arenít any native Hadoop mechanisms to provide authentication encryption, authorization an  creates serious security vulnerabilities as malicious users can read, corrupt or even delete data by bypassing access controlís rules. All users have direct access to all data stored in DataNodes without any fine-grain authorized access control mechanisms. A job to the JobTracker could be executed without proper validation or authentication Without encryption, sensitive data in the DataNodes may be leaked. Yahoo proposed addressing these concerns using  ver, it lacks requirem ents of role-base d access control, attribute-based access control and active directory [11, 13 and is unable to provide a comprehensive solution. Bello w some of the key security risks and challenges are summarized Authentication A typical Hadoop cluster is based on a trusted environment. Such a setti ng does not require any authentication of users or servi ces.  Hadoop integrated with Kerberos can provide some form of authentication mechanism. The NameNode and JobTracker can ensure that any HDFS or MapReduce requests are being executed with the required authentication This configuration, realizes service-to-service and user-to-service authentications However, Kerberos fails to control operations and permission of user submit ting jobs. Further it does not provide any security while directly accessing the data blocks store in the DataNodes Authorization The core authorization mechan ism in Hadoop is through its file-system. The HDFS enables authorization mechanisms based upon access control lists \(ACL\ilar to that used in POSIX file-systems. However, the au thorization mechanisms in Hadoop are still in their in fancy. Increase in the number of users and data files, makes it di fficult to control and manage the access control lists and permissions [13 Hadoop clients may directly read or write a data block bypassing the NameNode as long as it can supply an appropriate block id  Hado op clients may register themselves as another TaskTracker or DataNodes leading to data corruption, task interr uptions and other malicious activities Confidentiality Hadoop natively doesnít impl ement any cryptographic framework to support encryption decryption of files in HDFS. This enables users with administrative rights read data that are stored in the DataNodes. Moreover, RPC communication protocols between the Hadoop services 
B Process flows C Security Risks 
645 
642 
642 


D BitTorrent E Secure Sharing Storage over the Cloud k a k a G k b k b G k c k c G m m e m e m r k c G t G r t G k b G t c G r b k b G r c k c G r k c G t G r c r b r c G t c G r b G m e m c m e k c r c G t c G m c m c m b m c k b r b G m m b m BitTorrent protocol data encryption sharing storage over the cloud A Access Control Scheme with BitTorrent and encryption 
transports data over an unprotected netwo rk vulnerable to leaks and eavesdropping Audit The audit mechanisms of Had oop are the similar to that in Unix, where in it is difficult to keep track of audit events Data owners do not have a mechanism to track access's histories to their data BitTorrent is a popular peer-t o-peer transport protocol commonly used for sharing l BitTorrent splits  the data files into virtual pieces that are shared between peers nodes participating in shari ng of a file\maintains a torrent seed file with indexes and hashes for each virtual piece of a file. The torrent file also includes other metainformation such as addresses of the Trackers \(servers that assists in the communication between peers\unication ports and block sizes. The tracker maintains and provides a list of peers involved in sharing a file. Peer acts as a client as well as a server. Once it receives a complete data block, it acts as a server for that block. Each peer discovers other peers via one or more trackers from the torrent file. When the multiple peers download the sa me file concurrently, they upload to each other making it possible to support very a large number of downloaders with only a modest increase in load. The advantage of the BitTorrent is fast distribution and transportation of large files. There have been multiple-use cases for sharing data using the BitTorrent protocol as surveyed in A secure sharing storage scheme based on elliptic curve cryptography over the cloud was propos  scheme can imperatively impose the access control policies of data owners, preventing the cl oud storage providers from unauthorized access and making illegal authorization to access the data. The advantage of this scheme is that only the data owner has control of who has access to his data In this scheme, assuming that Alice has the private key and the public key Bob has the private key and the public key and the Cloud Storage Pr ovider has the private key and the public key Alice stores a message encrypted as in the cloud. The encrypted message      where and are random numbers is a point that satisfies so me conditions on the elliptic curve. In order for Bob to access the message, he sends a request to Alice with his public key Alice computes        where and are also some random numbers. Alice then sends   to the cloud storage provider and  to Bob. The cloud storage provider re-encrypts the data via the formula      Finally, Bob receives from the cloud storage provider and decrypts with the formula     to gain the message    III N OVEL ACCESS CONTROL SCHEMES In the section, we propo se two novel access control schemes for Hadoop data st orage. One is based upon concepts from and The other is derived using the secure methodology BitTorrent uses virtual pieces to distribute a data file Users download data files w ith their torrent seeds. The notion of virtual pieces, allows creation of access tokens for the metadata information such as addresses of blocks stored on DataNodes. Only users with right access tokens can access block addresses, and the respective blocks stored in the DataNodes. This scheme includes four phases: creating and distributing access token, gain access token and access blocks. Figure 2 shows the process of creating and distributing the access token over a Web server Figure 2 Creating and Distributing Access Token over a Web Server 
002 
646 
643 
643 


Step 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 
B Access Control Scheme with Secure Sharing Storage Access Control Scheme with BitTorrent and Encryption 
002 002 002 002 
Creating and Distributing Access Token A Hadoop client that is owner of a data file can create and distribute the access token for the file. The steps involved are as follows Request the NameNode: When a Hadoop client uploads or accesses its data file in HDFS, it sends a request to the NameNode Return the metadata: After receiving a request the NameNode returns the metadata of the requested data file. The metadata includes block ids and the address of DataNodes that store the blocks. The Hadoop client creates a record of metadata for each block Generate the metadata file: The Hadoop client generates a metadata file with the records of metadata returned by the NameNode Create the access token data: The Hadoop client encrypts each record of metadata. If a block is authorized to a user, this block s record in the metadata file is encrypted with a shared key with the Hadoop client and the user. If a block is authorized to a group, this blockís the record is encrypted with a sh ared key with the Hadoop client and the group Create the access token file: The Hadoop client uses the access token data and other file information, such as file head, timestamp, size and so on, to create an access token file of the data file Create the torrent file: The Hadoop client uses a BitTorrent tool, to split the virtual pieces of the access token file, create indexes and hash for each virtual piece to generate a torrent file Distribute the access token file and torrent file The Hadoop client uploads the torrent file to a Web server to distribute the access token seed. The access token file is also uploaded to the server of announ ce list in the .torrent file Gain Access Token When a user wants to access the blocks of a data file, it downloads its respectiv e torrent file from the Web server. It also uses a BitTorrent tool, to download the access token file. Then, it decrypts the access token data with its shared key to gain the authorized access token of blocks. Without the shared key, a user cannot decrypt the access token data and is not able to gain the author ized access token, thus unable to identify addresses of blocks in the DataNodes Access Blocks The authorized access token of blocks, allows access to the metadata of the blocks and their addresses on the DataNodes In the earlier scheme, a Hadoop client authorizes users access to the blocks stored in DataNodes via an access token and shared key. Although only users with a shared key may decrypt the access token data to gain the access token, data owners do not gain knowledge about who downloads the access token file and accesses the blocks. A secure sharing storage over the cloud addresses this concern. This scheme also includes four phases: creating and distributing access token, gain access token and access blocks. Figure 3 shows the steps of the creating and d istributing access token over the cloud Figure 3 Creating and Distri buting Access Token over the Cloud Creating and Distributing Access Token The steps 1 to 5 are same as discussed in The additional steps involved are given bellow 
647 
644 
644 


002 002 
Step 6 Step 7 
Encrypt the access token file: The Hadoop client uses the secure sharing scheme over the cloud to encrypt the access token file Distribute the encrypted the access token file The Hadoop client distributes the encrypted access token file to the cloud storage provider Gain Access Token A user needing to access to blocks of a data file requests the Hadoop client about the owner of the data file The Hadoop client sends a response to the user as well as the cloud storage provider according to concepts from the secure sharing storage scheme over the cloud. The user downloads the re-encrypted to ken file from the cloud storage provider and decrypts it. Finally, the user decrypts the access token data to gain the authorized access token Access Blocks When a user gains the authorized access token, they know the metadata of the blocks authorized by the data owner, including addresses of the blocks stored in the DataNodes. The user may access the data blocks stored in the DataNodes via their metadata information IV C ONCLUSION In our work, we identified some of the security risks of Hadoop system and proposed two novel access control schemes for storing data. It aimed at allowing the data owners to control and audit access to their data. However there could be a burden on data owners to update the access token and torrent files when the metadata of their file blocks changes. A flexible key sha ring key management solution for the access control would be investigated in the future We also aim to implement th e complete solution, evaluate its performance and application for access control A CKNOWLEDGMENT This work has been supported by the Province Natural Science Foundation thro ugh the Guangdong S2012040007370 and S2012010009950\he Key National Science and Technology through the China 2011BAD21B01 Natural Science Foundation through the China 11271003\Science and Technology Program through Guangdong \(2011B 020313023 projects R EFERENCES  Watson R T, Lind M, Haraldson S. The Em e r gence of Sustainability as the New Dominant Logic: Implications for Information System   Wei W Du J, Yu T, et al. Secu re mr: A service integrity assurance  framework for mapr  puter Secur ity Applications  Conference, 2009. ACSAC'09. Annual IEEE, 2009: 73-82  Lal K Mahanti N C. A Novel Data Mining Algorith m for Sem antic  Web Based Data Cloud onal Journal o f Co m puter Science 2  Faruk Berksˆz, htt p://zh.scribd.co m  doc/124 221 312 ha doop   Mahesh So m a ni, BitTorrent for Package Distri bution in the  Enterprise, http://www.ebaytechblog.com/2012/01/31/bittorrent-forpackage-distribution-in-the-enterprise  Zhao G, Rong C, Li J, et al Trusted data sharing over untrusted cloud  mputing Technology and Science CloudCom\, 2010 IEEE Second International Conference on. IEEE 2010: 97-103  To m  White, Had oop: The Definit ive Guide \(second edition[M OíReilly Press, 2010:41-72  Brad Helund, Understanding Had oop Cluster and the network http://bradhedlund.com/2011/09/10/understanding-hadoop-clustersand-the-network  To m  White, Had oop: The Definit ive Guide \(second edition[M OíReilly Press, 2010:15-38  Jeffrey Dean and S a njay Ghe m a w at MapReduce: Si m p lified Data Processing on Large Clusters, http://labs.google.com/papers mapreduce.html  Andr ew Becherer  Hadoop Secur ity  Design Just Add Ker ber os  Really? http://media.blackhat.co m/bh-us-10/whitepapers/Becherer BlackHat-USA-2010-Becherer Andrew-Hadoop-Security-wp.pdf  Owen OíMalley  Kan Zhang, Sanj ay Radia and el at Hadoop Security Design, https://issues.apache.org/jira/secure/attachment 12428537/ security-design.pdf  Hadoop Poses a Big Data Secu r ity Risk: 10 R easons W hy http://www.eweek.com/security/slideshows/hadoop-poses-a-big-datasecurity-risk-10-reasons-why  Andrew Brus, The Odd Couple: Hadoo p and Data Security http://www.zdnet.com/the-odd-couple-hadoop-and-datasecurity-7000018468  Yu L, Susilo W Safavi-Naini R X2bt trusted reputat ion sy stem a robust mechanism for p2p networ  ptology and Network  Security. Springer Berlin Heidelberg, 2006: 364-380  Secur ing Big Data: Secur ity  Reco m m endations for Hadoop an d NoSQL Environments, https://securosis.com/assets/library/reports SecuringBigData_FINAL.pdf  Z hou Quan Deqin Xiao, Daixian et al TSHC: Tr usted Schem e for Hadoop Cluster[C The 4-th In ternational Conference on  Emerging Intelligent Data and Web Technologies, EIDWT-2013:344-349 
648 
645 
645 


 Chun Liu, Zheng Zheng, KaiYuan Ca i Shichao Z h ang Distr ibuted Frequent Closed itemsets Mining,î Signal-Image Technologies and Internet-Based System, 2007. SITIS 07. Third International IEEE Conference on,2007,43-50  Gui mei Liu Hong jun Lu, ìAscending Frequency Ordered Prefix-tree Efficient Mining of Frequent Patterns,î Database Systems for Advanced Applications, 2003. \(DASFAA 2003\Eighth International Confer ence on,2003,65-72  Jie Xu Yun Li,Bo Liu, ì A Parallel Frequent Itemsets Mining Algorithm Based on Vertical FP-tree,î Computer and Digital engineering,vol.40 2012,12-16  I BM Almaden Research Center, "Synthetic Data Generation Code for Associations and Sequential Patterns,"URL:http://www.almaden.ibm.com/ software/quest/, 2006.4 
445 
445 
445 
445 
445 


  
the çcost rate based strategyé and çlocal-optimisation based strategyé can smartly choose to store or delete the datasets in one cloud storage service \(as shown in Table I\, thereby largely reducing the cost rate for storing datasets with one cloud service provider. If more cloud storage services are available as shown in Figure 4, the simulation of çT-CSB algorithm with two additional storage servicesé demonstrates further reduction of the cost rate by taking bandwidth cost into account. Table I shows the number of datasets transferred and smartly stored in two representative cloud storage services with our T-CSB algorithm. Furthermore, how much cost can be reduced depends on the price of available storage services. In the simulation of çT-CSB algorithm with additional Haylix storageé, although some datasets are transferred to Haylix for storage \(as shown in Table I\, the cost rate only drops slightly comparing to the çlocal-optimisation based strategyé \(as shown in Figure 4\. This is because the price of Haylix is not much cheaper than Amazon S3 cloud. In contrast, in the simulation of T-CSB algorithm with additional Glacier storageé, our T-CSB algorithm significantly reduces the cost rate \(as shown in Figure 4\ by transferring datasets to Glacier 13 for storage \(as shown in Table I From the above simulation, we can see that for different price models of cloud storage services, our T-CSB algorithm can always store the datasets accordingly, even in the situation that the price difference is minor \(e.g. the simulation of çTCSB algorithm with additional Haylix storageé\. Hence our TCSB algorithm is very effective in reducing the cost \(i.e. costeffective\ for storing generated application datasets with multiple service providers in the cloud VI R ELATED W ORK  Today, research on scientific applications in the cloud becomes popular [1    30 Co m p ar i ng t o  t h e  traditional computing systems, e.g. cluster, grid and HPC systems, a cloud computing system has cost benefits in various aspects [4   Wi th A m azon c l ou ds  c o s t m ode l an d  BO I N C  volunteer computing middleware, the work in [1 an aly s es  th e cost benefits of cloud computing versus grid computing. The work by Deelman et al. [11 a l s o  a p p lies Am azon  cl ou ds   c o s t  model and demonstrates that cloud computing offers a costeffective way to deploy scientific applications. The work mentioned above mainly focuses on the comparison of cloud computing systems and the traditional distributed computing paradigms, which shows that applications running in the cloud have cost benefits. However, our work focuses on reducing cost for running application in the cloud This paper is mainly inspired by the research in the area of scheduling, in which much work focuses on reducing various costsé for applications [2 sy st e m s [3 1 o r d a t a ce ntr e networks T h e d i ffer e n c e i s t h a t  sc h e d u l i ng a i m s  at  improving resource utilisation whilst our work investigates the trade-off among computation, storage and bandwidth costs which is a unique issue in cloud computing due to the pay-asyou-go model. Another important foundation for our work is the research on data provenance. Due to the importance of data    13 Data stored in Glacier usually need 3 to 5 hours to become available when users retrieve them. As analysed in Section II.B, usersê delay tolerance is out of the scope of this paper. Hence we only focus on the cost in the simulation provenance in scientific applications, many works about recording data provenance of the system have been done [9   Recently, research on data provenance in cloud computing systems has also appeared M o r e spe c ific a lly  Ost e r w e il e t  al e s e n t ho w t o ge ner a t e a da ta  de r i v a t i o n g r ap h  fo r execution of a scientific workflow. Foster et al. [1 pr o p o s e  the concept of virtual data in the Chimera system, which enables the automatic regeneration of datasets when needed Our DDG is based on data provenance, which depicts the dependency relationships of all the generated datasets in the cloud. With DDG, we can manage where the datasets are stored or how to regenerate them As the trade-off among computation, storage and bandwidth is an important issue in the cloud, much research has already embarked on this issue to a certain extent. First plenty of research has been done with regard to the trade-off between computation and storage. The Nectar system [15 is  designed for automatic management of data and computation in data centres, where obsolete datasets are deleted and regenerated whenever reused in order to improve resource utilisation. In [11  De elm an et al  p r es en t th at st o r in g so m e  popular intermediate data can save the cost in comparison to always regenerating them from the input data. In [2 A d am s e t  al. propose a model to represent the trade-off of computation cost and storage cost. In [33   th e au th o r s  pr o p o s e th e C T T SP algorithm that can find the best trade-off between computation and storage in the cloud, based on which a highly cost-effective and practical strategy is developed for storing datasets with one cloud service provider [3  How e v e r th e a b ov e w o rk  di d n o t  consider bandwidth cost into the trade-off model. In [6 B a li g a  et al. investigate the trade-off among computation, storage and bandwidth in the infrastructure level of cloud systems, where reducing energy consumption is the main research goal. In [3   Agarwala et al. transform application data to certain formats and store them with different cloud services in order to reduce storage cost in the cloud, but data dependency and the option of data regeneration are not considered in their work. In this paper we propose the T-CSB algorithm which can find the best tradeoff among computation, storage and bandwidth costs for storing datasets of linear DDG in the cloud. This algorithm can be utilised for cost-effectively storing generated application datasets with multiple service providers in the cloud VII C ONCLUSIONS AND F UTURE W ORK  In this paper, we have investigated the unique features of storing large volume of generated scientific datasets with multiple cloud service providers in the cloud. Towards achieving the cost-effectiveness, we have proposed a T-CSB Trade-off among Computation, Storage and Bandwidth algorithm to find the minimum cost storage strategy for datasets of linear DDG, which also represents the best trade-off among three key factors \(computation, storage and bandwidth for the cost of data storage in the cloud.  This algorithm can be utilised for cost-effectively storing generated application datasets with multiple service providers in the cloud. General simulations indicate that our T-CSB algorithm is very effective in reducing cost for cloud storage In our current work, we assume that the storage of one cloud service provider have a unified price. However, in the 
291 


pp. 1-5 2009 3 S  Ag a r wa la  D Ja d a v  a n d L  A B a t h e n   i C o st a l e  Ad a p t i v e  C o st  Optimization for Storage Clouds," in pp. 436-443, 2011 4 M  Ar mb ru st  A Fo x   R  Gri f fi t h  A D  J o s e p h   R  Ka t z  A Kon wi n sk i   G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, "A View of Cloud Computing vol. 53, pp. 50-58, 2010 5 M   S  Av i l a Ga rc i a  X  Xi on g A E   T r e f e t h e n   C   C r i c h t on  A T s u i   a n d  P. Hu, "A Virtual Research Environment for Cancer Imaging Research in pp. 1-6 2011   J  Ba l i g a R W   A y re  K   H int o n and R  S   T u ck er  G reen c lo u d  computing: Balancing energy in processing, storage, and transport vol. 99, pp. 149-167, 2011   R. B o s e and J   F r ew   L in e a ge R e tr i e va l  for S c i e n tific Data Pro c es s ing  A  Survey vol. 37, pp. 1-28, 2005   A   Bu r t o n and A  T r elo a r P ub l i s h M y D a ta  A  C o m p os iti o n of  Serv i c e s  from ANDS and ARCS," in pp. 164-170, 2009   P. Ch e n  B Pl a l e, and M S A k tas  T em por a l  re pres e n t a ti o n  fo r s c i e n tific data provenance," in pp. 1-8, 2012   Y  Cui, H  W a ng and X   Ch e n g C hann el A llocati o n  in W i reles s  Data Center Networks," in pp. 1395-1403, 2011   E  Deelm an  G  S i n g h M L i v n y  B  B e rr im an, and J   G o o d  T h e C o s t  of  Doing Science on the Cloud: the Montage Example," in pp. 1-12, 2008   I  F o s t er J   Vo ck ler M. W i l d e   and Z. Yo n g  C him er a  A V ir tu a l  D a ta System for Representing, Querying, and Automating Data Derivation," in pp. 37-46, 2002   I  F o s t er, Z. Y o n g I   Ra icu and S  L u  C lo ud C o m putin g and G r id  Computing 360-Degree Compared," in pp. 1-10, 2008   S K  G a rg   R  Bu y y a and H   J   Si egel  T i m e and C os t T r ad e Off  Management for Scheduling Parallel Applications on Utility Grids vol. 26, pp. 1344-1355, 2010   P. K  G unda L   R a v indr an ath C A  T h ekkath, Y  Y u   and L  Z huan g  Nectar: Automatic Management of Data and Computation in Datacenters," in pp. 1-14, 2010   X. H u an g  Z  L u o and B  Yan C y b er infr a s t r uctu re  and e-S ci e n c e  Application Practices in Chinese Academy of Sciences," in pp. 348-354 2011   M  H u m p h r ey  N  B e e k w i l d er J   L   G ood a l l  and M B E r c a n Calibration of watershed models using cloud computing," in pp. 1-8, 2012   G  J u ve E. Deelm an, K  V a hi  and G   M e h t a, "D ata S h a r in g O p ti o n s  for  Scientific Workflows on Amazon EC2," in pp. 1-9, 2010   D. K ond o   B. J a v a di  P Ma lec o t, F   Capp ello and D  P  A n d e r s o n   C o s t Benefit Analysis of Cloud Computing versus Desktop Grids," in pp. 1-12, 2009 20  X  L i u  Z  Ni  D  Yu a n  Y  J i a n g Z   Wu  J  C h en   a n d  Y  Ya n g   A N o v e l Statistical Time-Series Pattern based Interval Forecasting Strategy for Activity Durations in Workflow Systems vol. 84, pp. 354-376, 2011   B  L uda s c h e r   I   A ltint as  C B e r k ley  D H i gg in s  E J a eger, M  J o n e s  and E. A. Lee, "Scientific Workflow Management and the Kepler System pp. 1039Ö1065 2005   K  K  Munis w am y R e dd y  P  Mack o  and M. Selt zer   P rove nanc e  for th e  Cloud," in pp. 197-210, 2010   H Ng u y en and D A b r a m s o n  W ork W a y s   I n t e r acti ve W o r k flow b a s e d  Science Gateways," in pp. 1-8, 2012   L  J  Os t e rw ei l  L  A   C l a r k e A   M E llis o n   R  Po d o roz hn y A  W i s e   E   Boose, and J. Hadley, "Experience in Using A Process Language to Define Scientific Workflow and Generate Dataset Provenance," in pp. 319-329, 2008   J  Q iu, J  Ekana y ak e  T  G una r a thn e   J   Y Ch o i, S  H  Ba e, H  L i  B   Zhang, Y. Ryan, S. Ekanayake, T.-L. Wu, A. Hughes, and G. Fox Hybrid Cloud and Cluster Computing Paradigms for Life Science Applications vol. 11, 2010 26  X  Su  Y M a   H  Ya n g   X C h a n g  K  N a n  J  Xu  a n d  K N i n g   An Op en Source Collaboration Environment for Metagenomics Research," in pp. 7-14, 2011 27  A   S  S z al ay a n d J   G r ay    S c i e n ce i n a n  Ex po ne n t i a l  W o r l d    vol 440, pp. 23-24, 2006   S. Toor  M. S a b e s a n, S  H o lm gre n and T   R i s c h, "A S c a l ab le A r chit ecture for e-Science Data Management," in pp. 210-217, 2011   D W a r n e k e and O  K a o  E x p loiting Dy na m i c R e s o u r c e A llocati o n  for  Efficient Parallel Data Processing in the Cloud vol. 22, pp. 985-997, 2011   Y. Y a n g K  L i u, J   Ch e n X  L i u, D   Yuan and H   J i n   A n A l gorithm  in SwinDeW-C for Scheduling Transaction-Intensive Cost-Constrained Cloud Workflows," in pp. 374-375, 2008  L   Yo un g Ch oo n and A  Y Z o m a y a   E n e rgy C o ns ci o u s Sch e du l in g for  Distributed Computing Systems under Different Operating Conditions vol. 22, pp. 13741381, 2011   D Y u an  Y   Y a n g  X   L i u  and J  Ch e n   A C o st-Effecti ve S t r a t e gy  for Intermediate Data Storage in Scientific Cloud Workflows," in  pp. 1-12, 2010   D Y u an Y   Y a n g   X   L i u and J  Ch e n  O nd e m a nd Minim um C os t  Benchmarking for Intermediate Datasets Storage in Scientific Cloud Workflow Systems vol 71, pp. 316-332, 2011 34  D  Yu a n   Y  Ya n g X L i u  W  L i  L   C u i   M  Xu  a n d  J   C h en    A Hi gh l y Practical Approach towards Achieving Minimum Datasets Storage Cost in the Cloud vol 24, pp. 1234-1244, 2012 35  D  Yu a n   Y   Ya n g  X L i u  G Z h a n g  a n d  J  C h e n    A Da t a D e p e n d e n c y  Based Strategy for Intermediate Data Storage in Scientific Cloud Workflow Systems vol. 24, pp. 956-976, 2012   M. Z a h a r i a, A  K o n w ins ki A  D   J o s e ph R. K a t z and I  S t o ica  Improving MapReduce Performance in Heterogeneous Environments," in pp. 29-42, 2008  
real world, the price of cloud storage is different according to different usages. In the future, we will incorporate more complex pricing models in our datasets storage cost model Furthermore, methods for forecasting dataset usage frequency can be further studied, with which our T-CSB algorithm can be adapted to different types of applications more easily A CKNOWLEDGMENT  The research work reported here is partly supported by Australian Research Council under DP110101340 and LP130100324, Shanghai Knowledge Service Platform Project No. ZF1213. We are also grateful for the discussions on Finite Element Modelling application with Dr. S. Xu from Faculty of Engineering and Industrial Sciences, Swinburne University of Technology R EFERENCES    A m a zo n C l o ud S e rv ic es    http://aws.amazon.com  2  I  A d a m s  D  D  E  L o n g  E  L   M i l l e r   S  P a s u p a t h y  a n d  M  W  S t o r e r   Maximizing Efficiency by Trading Storage for Computation," in 
Workshop on Hot Topics in Cloud Computing \(HotCloud'09 IEEE International Conference on Cloud Computing \(CLOUD2011 Communication of the ACM 7th International Conference on E-Science \(e-Science2011 Proceedings of the IEEE ACM Computing Surveys 5th International Conference on E-Science \(eScience'09 8th International Conference on E-Science \(eScience2012 IEEE INFOCOM 2011 ACM/IEEE Conference on Supercomputing \(SC'08 14th International Conference on Scientific and Statistical Database Management, \(SSDBM'02 Grid Computing Environments Workshop \(GCE'08 Future Generation Computer Systems 9th Symposium on Operating Systems Design and Implementation \(OSDI'2010 7th International Conference on E-Science \(e-Science2011 8th International Conference on E-Science \(e-Science2012 ACM/IEEE Conference on Supercomputing \(SC'10 23th IEEE International  Parallel & Distributed Processing Symposium IPDPS'09 Journal of Systems and Software Concurrency and Computation: Practice and Experience 8th USENIX Conference on File and Storage Technology  FAS T'10 8th International Conference on E-Science \(eScience2012 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering Journal of BMC Bioinformatics 7th International Conference on E-Science \(e-Science2011 Nature 7th International Conference on EScience \(e-Science2011 IEEE Transactions on Parallel and Distributed Systems 4th International Conference on E-Science \(eScience2008 IEEE Transactions on Parallel and Distributed Systems 24th IEEE International Parallel & Distributed Processing Symposium \(IPDPS'10 Journal of Parallel and Distributed Computing IEEE Transactions on Parallel and Distributed Systems Concurrency and Computation: Practice and Experience 8th USENIX Symposium on Operating Systems Design and Implementation \(OSDI'2008 
292 


Jorda Polo, David Carrera, Yolanda Becerra, Malgorzata Steinder  and Ian Whalley. Performance-driven task co-scheduling for  mapreduce environments. In Network Operations and Management  Symposium \(NOMS\2010 IEEE, pages 373 Ö380, 19-23 2010 12 K. Kc and K. Anyanwu, çScheduling hadoop jobs to meet deadlines  in 2nd IEEE International Conference on Cloud Computing  Technology and Science \(CloudCom\, 2010, pp. 388 Ö392 13 Xicheng Dong, Ying Wang, Huaming Liao çScheduling Mixed Real time and Non-real-time Applications in MapReduce Environment  In the proceeding of 17th International Conference on Parallel and  Distributed Systems. 2011, pp. 9 Ö 16 14 Xuan Lin, Ying Lu, J. Deogun, and S. Goddard. Real-time divisible  load scheduling for cluster computing. In Real Time and Embedded  Technology and Applications Symposium, 2007. RTAS ê07. 13th  IEEE pages 303 Ö314, 3-6 2007 15 HDFS  http://hadoop.apache.org/common/docs/current/hdfsdesign.html  16 Chen He, Ying Lu, David Swanson. çMatchmaking : A New  MapReduce Scheduling Techniqueé. In the proceeding of 2011  CloudCom, Athens, Greece, 2011, pp. 40 Ö 47 17 Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma and Khaled  Elmeleegy, Scott Shenker, and Ion Stoica, çDelay scheduling: a  simple technique for achieving locality and fairness in cluster  schedulingé. In the proceedings of the 5th European conference on  Computer systems, 2010.  pp 265-278 18 Zhuo Tang, Junqing Zhou, Kenli Li, and Ruixuan Li "A MapReduce  task scheduling algorithm for deadline constraints.", Cluster  Computing, Vol. 15,  2012 19 Eunji Hwang, and Kyong Hoon Kim. "Minimizing Cost of Virtual  Machines for Deadline-Constrained MapReduce Applications in the  Cloud." Grid Computing \(GRID\, 2012 ACM/IEEE 13th  International Conference on. IEEE, 2012 20 Micheal Mattess, Rodrigo N. Calheiros, and Rajkumar Buyya  Scaling MapReduce Applications across Hybrid Clouds to Meet Soft  Deadlines." Technical Report CLOUDS-TR-2012-5, Cloud  Computing and Distributed Systems Laboratory, the University of  Melbourne, August 15, 2012 21 
 
11 
                
Chen He, Ying Lu, David Swanson. çReal-Time Application Scheduling in Heterogeneous MapReduce Environments Technical Report TR-UNL-CSE2012-0004, University  of Nebraska-Lincoln, 2012 Available: http://cse apps.unl.edu/facdb/publications/TR-UNL-CSE20120004.pdf 22 T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, K  Elmleegy, and R. Sears. çMapreduce Onlineé. In NSDI 2010 23 A. D. Ferguson, P. BodÌk, S. Kandula, E. Boutin, and R  Fonseca. çJockey: Guaranteed Job Latency in Data Parallel Clusters. In EuroSys, 2012 24 G. Wang, A. R. Butt, P. Pandey, and K. Gupta. çA Simulation Approach to Evaluating Design Decisions in MapReduce Setupsé. In MASCOTS 2009 25 H. Herodotou and S. Babu. Profiling, çWhat-if Analysis and Cost-based Optimization of MapReduce Programs In VLDB 2011 26 H. Herodotou, F. Dong, and S. Babu. çNo One \(Cluster Size Fits All: Automatic Cluster Sizing for Dataintensive Analyticsé. In SoCC 2011  
1544 
1544 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


