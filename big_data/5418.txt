  The authors wish to thank the 085 Innovation Projects Foundation of Shanghai Education Committee for contract J20902 under which the present work was possible Research On Decision Of air cargo strategy based on Data Mining  LI Yuewen School  of Management, Shanghai University of Engineering Science Shanghai, China   Abstract This article mines the air cargo customer information and establishes the decision tree of the ID3 association rules from the ID3 algorithm. It finds frequent customer information using Apriori algorithm and finally gets the best combination of customer information. The combination has some significance on 
the development of air cargo  Keywords-ID3 algorithm Apriori algorithm customer information I  I NTRODUCTION  During the recent years, great achievements of the research on data mining application have been achieved, whose broad application prospect have attracted many researchers and business organizations. A batch of data mining systems have been developed, and have been applied in commerce economy, financial management and other fields among which the most representative systems are the Quest System researched by R. Agrewal of the IBM Almaden and the data mining prototype system DB Miner developed by the team led by the famous international data mining professor Han Jiawei 
who comes from Canada, Simon Fraster university. The successful application of the commercial softwareís greatly arouse the enthusiasm for data mining technology in the academic circle and promote the application of data mining technology in industry, such as ERP system and various management information system, geographic information system, etc However, as for domestic airlines, they have already introduced some advanced management experience, but still in the beginning stages. They havenít applied it yet, but with the increasingly fierce competition, to use the data mining technologies like ID3 algorithm and Apriori algorithm in airline industry will definitely become a trend of the air freight 
company development  II  D ECISION T REE A LGORITHM  Decision tree is the most widely used method for data classification. Decision tree classification is a method to deduce the classification rules of the decision tree expression forms from disorder and irregular sample sets. It adopts the top-down recursive method, which first make comparisons of the property values between the inner nodes, then judge the branch followed and finally draw conclusions at the leave node. Therefore, there is a choice rule to correspond to the path from the root to the leave node, and thus a set of classification 
rules to correspond to the decision tree Among all the decision tree classification algorithms, the most influential one is ID3 algorithm, the main ideas of which are as follows Every non-leave node corresponds to a non-category property, whose value is represented by the branch. A leave node represents the category that corresponds to the path between the root to the leave, and itís property value is recorded by the leave In the decision tree, every non-leave node associates with the most informative non-category property Entropy is a term to measure the amount of information that a non-leave possesses Here is the definition for Information Entropy The entropy of discrete random variables: It reflects 
expected value of the variable. In general, there are two kinds of entropy: unconditional entropy and conditional entropy Unconditional entropy, entropy for short, can be expressed in the following formula  000    n i i i x p x p X H 1 2   log       1 In this formula 000\003\007T 013 i = 1,2 ..., n\ stands for X value, and n stands for the number of values 007L\022:\007T 013 022 
expresses the Probability of 007:\015L\007T 013 9 in which 2 is the substrate of the logarithmic function because entropy is encoded in binary. H \(X\ reflects the expectation of X with a certain value, that is to say, it stands for the uncertainty of X The conditional entropy of the discrete random variable X to a given discrete random variable Y can be referred to as the conditional entropy X to Y and recorded as H \(X|Y\  which can be expressed using the following formula 000¶\000    n i m j j i j i y x p y x p Y 
X H 11 2    log       2 In this formula 007L\022:\007T 013 001·\007U 013 022 is Joint probability of j i y Y x X     j i y x  stands for Conditional probability of 000\034\015L\000 013h 001·\000\033 015L 000 013g and n, m, respectively stand for the values 
2010 International Conference on Electrical and Control Engineering 978-0-7695-4031-3/10 $26.00 © 2010 IEEE DOI 10.1109/iCECE.2010.993 4085 
2010 International Conference on Electrical and Control Engineering 978-0-7695-4031-3/10 $26.00 © 2010 IEEE DOI 10.1109/iCECE.2010.993 4085 


  number of X, Y.  Similarly  2 is the substrate of the logarithmic function because entropy is encoded in binary H \(X|Y\ reflects the uncertainty of variable X under certain conditions. It can be proved that X can effectively lower its uncertainty if certain conditions are added. Therefore H\(X|Y 000 H\(X\                                   \(3 So generally we use the conditional entropy value of X so as to lower its uncertainty Through the characteristics showed in the data collected we can find an accurate description or model for each category Whatís more, we can use the category description it generates to classify the future test data. Therefore, we will have a better understanding of each category in the data. In other words, we obtain some knowledge about this category  III.     ASSOCIATION RULES A  Mining Algorithm Definition 1: Mining Association Rules data sets recorded for the D \(Generally, it is a transaction database D 022 007P 0135 001·\007P 0136 001·\001Â\001·\007P 013 001·\001Â\001·\007P 013 022  007P 013 015L 022 007E 0135 001·\007E 0136 001·\001Â\001·\007E 013 001·\001Â\001·\007E 013Á\013 022 000\003\000\003  007P 013ﬁ\000\003 022 k=1,2,Ö,n is called Transaction, and 000 013k 022 m=1,2, Öp 9 is called item Definition 2: Suppose I 9  000 0135 001·\000 0136 001·\001Â\001·\000 013k is a collection made up of all the items in D, therefore X , any subset of I, is called an itemset of D, and 000\003 002\001 000\033 002\001 015L\000 means collection X is a k itemset. Suppose 000 013i and X respectively are Transaction and Itemset in D. If 000\003 exists Transaction 000 013i will contain itemset X. Each Transaction has a unique identifier, called TID Definition 3: The number of itemsets X contained in data set D is called the supporting number of itemset X, recorded as 000\003\002P 013v The degree of supporting of itemsets X is recorded as support \(X  support\(X 000\003 014\031 014 002\001\013Ω\002\001 100 9  9 or support\(X 000\003 014\031 014 002\001\013Ω\002\001 9  In this formula, The number of Transaction in the data sets is 002\001\000\007\002\001 If support \(X\is not less than the minimum support specified by users, X is called frequent itemset \(or ìLarge Itemsetî\ otherwise, X is non-frequent itemset \(or ìSmall Itemset Definition 4: If X, Y are Itemsets, and 000\033\005Í\000\034\015L\005 the implication type 000\003\000\033 005 000\034 is called the association rule, in which X, Y are respectively known as association rule premise and conclusion of the association rule. The support degree of itemsets is called the degree of association rules, recorded as support 000\033\005ú\000\034  support 000\033\005ú\000\034 support 000\033\005Î\000\034 Association rules 000\033\005ú\000\034 Confidence level of the association rules is recorded as confidence 000\033\005ú\000\034  9 confidence 015l\000\003\007 005 007;\015p 015L 013Ê\013Ë\013 013„\013‚\013Â\013Á\022:\013—\005Î\013“\022 013Ê\013Ë\013 013„\013‚\013Â\013Á\022:\013—\022 015H 003s\003r\003r Usually the minimum confidence level specified by usersí mining need is recorded as minconfidence Support and confidence levels are two important concepts which describe association rules; the former is for measuring the importance of the association rules in the entire statistical data, and the latter is used to measure the credibility of the association rules. Generally speaking, only the association rules of high support and confidence levels can be attractive and useful to users The task of mining association rule is to dig out all of the strong rules in D. Strong rules 000\033\005ú\000\034 of the corresponding itemsets 9 000\033\005Î\000\034  9 must be the ìFrequency Itemsets Association rules 000\033\005ú\000\034 of the confidence level which is derived by ìFrequency Itemsets 9  000\033\005Î\000\034  9 can be calculated by the level support of ìFrequency Itemsetsî X and 9 000\033\005Î\000\034  9 Therefore, mining association rules can be divided into the following two sub-questions 1\According to the smallest degree of support, we can find out all ìFrequency Itemsetsî in data sets of D 2\ According to ìFrequency Itemsetsî and the minimum Confidence Levelî, we can generate association rules The basic model of mining Association Rules is shown in Figure 1.In this figure, D is data sets; Algorithm-1 is search algorithm of the ìFrequent Itemsetsî; Algorithm-2 is the Generated algorithm of the ìAssociation Rulesî; R is the mined association rules set. Users by specifying minsupport minconfidence separately with the algorithm Algorithm-1 Algorithm-2 interaction, and through the results of interaction with R excavation interpretation and evaluation   Figure 1. The basic model of mining association rules B  Discovery Algorithm of Association Rule In order to find out all the full level of support and confidence of association rules, we can divide this question into the following two sub-questions 000z  Looking for all combinations of items which are from those services support more than minimum support these combinations are called large itemsets, while other combinations are called small itemsets 000z  The necessary rules which are generated by the large itemsets. The general idea: If ABCD and AB are large itemsets, we can calculate the ratio of r which is equal to the number of supporting for \(ABCD\ the number k X t 002 XY 000 
4086 
4086 


  of supporting for \(AB\ and determine whether the rules are strong or not. Only r minconfidence, it is strong rules. Attention to this rule, it has the minimum level of support, because ABCD is the largest  IV.    CASE ANALYSIS Suppose here is the customer information of a certain air cargo company \(See Table 1 Table 1 The information of the customers of an air cargo company Customer Type Customer value Customer satisfaction Lost or not Regular cargo owner Low High No Individual cargo owner Low High No Individual cargo owner High High No Regular cargo owner Low Average No Regular cargo owner High Average No Individual cargo owner High Average Yes Individual cargo owner Low Average Yes Regular cargo owner Low Low Yes Individual cargo owner High Low Yes Individual cargo owner Low Low Yes  Now suppose 1  D1={A,B},A stands for regular cargo owner, and B for Individual cargo owner; D2={C,D,E,F,G},C 001 D 001 E 001 F 001 G stands for Customer category 1 001 Customer category 2 001 Customer category 3 001 Customer category 4 001 and Customer category 5 respectively\(See Table 2  Table 2 Customer classification Customer category Type Meaning Category 1 Potential customers Patronize one or two times or even no patronage Category 2 Disloyal customers Patronage occasionally Category 3 General customers Patronize at a comparatively high frequency Category 4 Loyal customers Patronize at an extremely high frequency Category 5 Disconnected customers Used to patronize at a high frequency but no patronage now D3={H,I,J},H stands for Door-to-door delivery service, I for expedited service and J for booking service 2\ There are 20 Services in the database,namely, |D|=20 3\ The smallest support is 3, and minconfidence is 60 9  A.  The application of ID3 algorithm 1\ Calculation process a\ Calculate the initial entropy Let lost or not be the category property, therefore there are only two categories: Yes or No, that is to say, m=2.The number of the sample sets N=10, thus the number of the No category N 9 5 and the number of the Yes category N2=5. According to the previous formula H \(Loss 015F 0139 0135\0134 015H\007H\007K\007C 0136 003w\002\000\003s\003r 015F 0139 0135\0134 015H\007H\007K\007C 0136 003w\002\000\003s\003r 015L 003s  b\ Calculate the entropies of the non-category properties respectively 0017 Calculate property A, namely Customer type There are 4 regular cargo owners, so n11=4, and the number of the No category n11\(1\=3 and that of the Yes category n11\(2 \= 1 There are 6 individual cargo owners, so n12=6, and the number of the No category n12\(1\=2 and that of the Yes category n12\(2\=4 Now Aís conditional entropy can be calculated H\(Loss|A 0138 0135\0134 015H\015F 0137 0138 015H\007H\007K\007C 0136 0137 0138 015E 0138 0135\0134 015H\015F 0135 0138 015H\007H\007K\007C 0136 022 0135 0138 022A\015E 013 0135\0134 015H 015F 0136 013 015H\007H\007K\007C 0136 022 0136 013 022A\015E 013 0135\0134 015H\015F 0138 013 015H\007H\007K\007C 0136 022 0138 013 022A 015L 003r\001‰\003z\003y\003x  0018 Calculate property B, namely Customer value There are 6 low customer values, so n2=6, and the number of the No category n21\(1\=3 and that of the Yes category n21\(2\=3 There are 4 high customer values, so n22=4, and the number of the No category n22\(1\=2 and that of the Yes category n22\(2 \=2 Now such conclusion H\(Loss |B\ can be drawn using the same method  0019 Calculate property C, namely Customer satisfaction There are 3 high customer satisfaction, so n31=3, and the number of the No category n31\(1\=3 and that of the Yes category n31\(2\=0 There are 4 average customer satisfaction, so n32=4, and the number of the No category n32 \(1\=2 and that of the Yes category n32 \(2\=2 There are 3 low customer satisfaction, so n33=3, and the number of the No category n33 \(1\0 and that of the Yes category n33 \(2\=3 So H\(Loss |C\=0.4 003 
4087 
4087 


  c\ Calculate the increased information value of each property, and select the highest as the root node of the decision tree G\(A\=H\(Loss\H\(Loss |A\1-0.876=0.124 G\(B\=H\(Loss\H\(Loss |B\1-1=0 Loss\ H\(Loss |C\1-0.4= 0.6 Obviously, what causes the entropy to reduce the most is the attribute ìCustomer satisfactionî, therefore this property is made the root of the decision tree B.   The application of Apriori algorithm 1\ TID list in the database A,C,I,H},{A,C,H},{A,C,I},{A,D,I},{A,E,I},{A,E,H},{A,F,I J},{A,F,J},{A,F,J,H},{A,G,I},{B,C,H},{B,C,I},{B,D,I},{B,E,I B,E,J},{B,E,I,J},{B,F,H},{B,F,H,I},{B,F,J},{B,G,I In order to get 007 0136 by 007 0135 we can apply Apriori algorithm The specific process is as follows 000z  Connect 000z  Prun 000z  Compare the candidate support count and minimum support count, and delete the designate option which does not meet minimum support count, and we can get 007 0136  007 0136 9 A,F },{ A,H },{ A,I },{ A,J },{ B,E B,F },{ B,I },{ B,J },{ C,H },{ E,I This step is repeated until 007 0136 cannot get 007 0137 and stop mining association rules Once frequent itemsets are obtained in the transactions of database D, it is straightforward to generate strong association rules by them \(Strong association rules satisfy minimum support and minimum confidence\. Confidence level can be calculated from Definition 4. The following will be generated by the association rules Each frequent itemset  L generates its all nonempty sets For each of Lís non-empty subset s, if confidence \(A B min, conf, ìs l-s\ is the output rule, in which min_conf stands for minconfidence The frequent item set deduced above is L={A,F}, therefore A},{F} are Lís non-empty subsets. Thereby, the following connection rule can be derived from L A F, confidence =3/10=30 F A, confidence =3/6=50 The minconfidence is 60%, so no rule can be deduced A H, confidence =5/10=50 H A, confidence =5/7=71 The minconfidence is 60%, so only the second rule can be deduced Similarly the results shown in the following Table are obtained  Table 3  Conclusions drawn by connection rule algorithm  Rules obtained Measures taken services offered HA Generally those who want door-to-door delivery service are regular cargo owners Offer door-to-door delivery service to the regular cargo owners, but only to high value customers because not all of them can bring high profits EB General customers are individual cargo owners No measures taken because it has nothing to do with the subject BI Individual cargo owners usually demand expedited service The profits individual cargo owners bring are not high therefore offer expedited service to loyal or high value individual cargo owners CH Potential customers usually demand door-to-door delivery service Offer door-to-door delivery service to high value potential customers only EI General customers usually demand expedited service The profits general customers bring are low, therefore no expedited service offered  V.   CONCLUSIONS Now that decision tree for the customers of a certain air cargo company, we can see from it 000z  It is more likely to keep the customers with high customer satisfaction and lose those with low customer satisfaction 000z  If the customer satisfaction is average, it is more likely to keep the regular cargo owners and lose individual cargo owners According to the decision tree knowledge and Table 3, we can obtain the specific and necessary measures to solve different problems we are face with. For example, individual cargo owners generally will require expedited services, and we can provide expedited services for the customers with average customer satisfaction to keep those who are liable to lose R EFERENCES  1  R. Agrawal, R. Srikant. ìFast algorithms for mining association rules Proceeding of the 20th International Conference on Very Large Databases 9 1994, pp.487-499 9  2  Jiawei Han, Micheline Kamber, translated by Ming Fan, Xiaofeng Meng. ìData mining concepts and skills.î Beijing: Machinery Industry Press.  August 2001 3  Zhijing Liu. ìA new itemset expression method.î Computer Engineering and Design, vol23, 2002,pp.42-44 000 003 000 000 000 000 000 000 000 000 000 000 
4088 
4088 


4  Yuming Qu. ìOne method to improve the efficiency of Apriori algorithm.î  Computer Engineering and Design, vol25, 2004 5  R. Agrawal. ìDatabase Ming: A Performance Prospective.î IEEE Transaction on knowledge and data engineering, May 1993,pp.914-925    
4089 
4089 


N 005 002 005 200 400 600 800 1000 2 0.22 0.44 0.67 0.88 1.11 3 0.33 0.66 0.97 1.32 1.65 5 0.55 1.08 1.64 2.14 2.75 10 1.2 2.2 3.2 4.3 5.5 20 2.1 4.3 6.5 8.7 10.8 TABLE I T IME S ECONDS  USED BY ALL PARTIES FOR DATA ENCRYPTION 003 200 400 600 800 1000 Time 0.15 0.32 0.48 0.63 0.8 TABLE II T IME S ECONDS  USED BY EACH MODERATOR that these computational costs do not include the overhead of key generation and computing two parameters 032 and 033  However generating these parameters belongs to the preparation period of the mining process Therefore it can be implemented before the protocol is executed without affecting the computation time of the protocol For evaluating the efìciency of the protocol in practice we build an experiment on the privacy preserving frequency mining in C environment which runs on a laptop with CPU Pentium M 1.8 GHz and 1GB memory The used cryptographic functions are derived from Open SSL Library To measure the computation cost of the frequency mining protocol in worst case we assume that all parties involve in the protocol except the miner are the moderators We measure the computation cost of the frequency mining protocol for 10 parties Before executing the protocol we generate a pair of keys for each party with the size of public keys set at 512 bits Table 1 illustrates our measurements of all partiesês computation time in the submission phase it is in regard to 010 and 010 017 010  for a typical scenario where 010 003 004\005\005\005  010 017 010 003\002\005 The computation time of all parties is about 10.8 seconds Table 2 illustrates our measurements of a moderatorês computation time it is linear in 010 and does not depend on  and 010 017 010 For a typical scenario where 010 003 004\005\005\005  the computation time of a moderator is about 0.8 seconds The minerês computation time it is linear in 010  010 017 010  and   However it is very small it only is 021\020 0126 when 010 003 004\005\005\005\005  010 017 010 003\002\005  and  003\004\005  V P RIVACY PRESERVING FOR CLASSIFICATION RULES LEARNING IN TWO DIMENSION DISTRIBUTED SETTING A Privacy preserving association rules mining 1 Association rules and frequent itemset The association rules mining problem can be formally stated in Let  003  002 006 005 006 007\007\007\006  002 003 be the set of all items Let 002\003 a transaction database where each transaction 020 is a set of items such that 020 012   Associated with each transaction is a unique identiìer denoted by 020&\002  We say that a transaction 020 contains  a set of some items in  if  012 020  The problem is to nd the association rules that have an implication of the form  016  014 6\006  015  where  012    012   and  017  003 7  The support 6 and the conìdence  of the rule  016  are deìned as 6 003 014 006  020  007\003 020 006  020  007 010 002\003 010  003 014 006  010  007\003 020 006  020  007 020 006  007 Where 020 006  007 stands for the number of transactions containing the set  in 002\003 and 010 002\003 010 denotes the total number of transactions in 002\003  The strong association rules are required to meet a minimum support  6 021\003\014  and a minimum conìdence   021\003\014  deìned by the miner A set of items is referred as an itemset An itemset that contains 016 items is a 016 itemset The support count of an itemset is the number of transactions containing the itemset The minimum support count is deìned as 6 021\003\014 010 002\003 010 An itemset is frequent if its support count is not less than the minimum support count Association rule mining is a twostep process 1 Finding all frequent itemsets 2 Generating strong association rules from the frequent itemsets Agrawal et al 2 presented the Apriori algorithm to efìciently identify frequent itemsets for boolean association rules The name of the algorithm is based on the fact that the algorithm uses the Apriori property i.e all nonempty subsets of a frequent itemset must also be frequent 2 Finding a frequent itemset Assume that the transactions set 002\003 is two-dimension distributed into 011\012 parties as in Section 002 007\005  Each party 014 003\004  015 003\004 006 007\007\011\006 016 003\004 006 007\007\006 012 wns 002\003 003\004 that contains information about certain attribute set  003 004 004 and certain records Given a candidate set  of the 016 items the parties wish to cooperatively nd whether or not the candidate set is frequent from the joint transaction set  002\003  without disclosing each partyês individual transactions and even the local frequent itemsets Assume that  is partitioned into parts  004  where 016 011 017  S 012\002 004 006 007\007\006 012 003  Each  004 consists of items 011  003 004 004  Note that if  is frequent in 002\003  it is frequent in at least one horizontal partition 002\003 003  where 002\003 003 003 002\003 003 002 020 007\007\007 020 002\003 003\021  In addition if  004 is frequent in 002\003 003 every  004  016 011 017  is frequent in 002\003 003\004  Considering a map from each 002\003 003\004 to a binary number that is done by each 014 003\004 as follows 8 003\004 003 002 004 006 if  004 is frequent in 002\003 003\004  005 006 otherwise Thus 002\003 is mapped to the binary matrix 011 004 012    Hence  is frequent in at least one horizontal partition 0029 003 as long as at least a row 015 in  with all elements are 004  As the result 8 003 003 003 004 004 010 8 003\004 013\010 017 010 003\005  Clearly using frequency mining can allow the miner to nd a random permutation of  031 036 002   031 036 012  Therefore the miner can identify whether  is frequent or not without knowing  be frequent in which 002\003 003  3 Finding all frequent itemsets and their support counts In the classic Apriori algorithm The k e y issue is computing the support of an itemset To nd out if a particular itemset is frequent we count the number of records where the values for all the attributes in the itemset are 1 Thus the 
101 


problem is to compute the frequency of values tuples that all values in the tuple are 004  The privacy preserving protocol for nding frequent itemsets and support counts follows Apriori algorithm as below 1 002 Finding an item 004 Itemsets is frequent 003 2 022 002 003  3 The miner sets  002 003 021 4 for each  011 022 002 do 5 The miner uses the frequency mining protocol to identify whether or not  is frequent 6 if  is frequent then 7 The miner does  002 003  002 020  8 Let  011  003 004 004  the miner broadcasts the requirement for computing 017\024++;\036 006  007 to all 014 003\004  015 011\002 004 006 007\007\007\006 011 003  9 All parties involve in frequency mining protocol to compute 017\024++;\036 006  007 10 end if 11 end for 12 for 002  003\002   037 005 002 006 003 021  016\016 003 do 13 The miner does 022 037 Approri-gen  037 005 002  14 for each  011 022 037 do 15 The miner uses the frequency mining protocol to identify whether or not  is frequent 16 if  is frequent then 17 The miner does  037 003  037 020  18 Let  consists of items partitioned into the sets  004  where 016 011 017  017 007\002 004 006 007\007\007\006 012 003  the miner broadcasts the requirement for computing 017\024++;\036 006  007 to all 014 003\004  016 011 017  19 The miner and the parties involve in frequency mining protocol to compute 017\024++;\036 006  007 20 end if 21 end for 22 end for 4 Analysis of protocol Statement 1 Correctness If all participants follow the protocol then the minerês result is the frequent itemsets and the support count of each frequent itemset Proof Candidate itemsets are generated by the Apriorigen procedure The correctness of that procedure has proved  The 022 037 sets are generated correctly as long as the input to the procedure is correct We show by induction that the  037 sets are generated correctly At steps 004 013 022 with  003\004   002 is correctly generated by the frequency mining protocol Assume that  037 005 002 has been correctly generated then 022 037 is correctly generated by Apriorigen procedure Since frequency ming protocol is correct the support count of each  011 022 004 005 002 is computed correctly Hence  037 is generated correctly from  037 005 002  The entire frequent itemsets and the support counts gives correct results Statement 2 Privacy The protocol preserves the privacy of the honest users against the miner and up to 011\012 013 002 corrupted parties as long as there is at least an honest be the moderator Proof Since all support count computations and frequent itemsets identiìcation are done independently using frequency mining This statement follows immediately from Theorem 2 5 Evaluation of complexity The communication analysis critically depends on the number of frequency computations called We incur the cost of privacy preserving frequency mining for each call Let 036 be the maximum size of a frequent itemset and let 022 003  015 003\004 006 007\007\007\006 036  and  003  015 003\004 006 007\007\007\006 036  represent the number of candidate itemsets and the found number of frequent itemsets at each round the total communication consists of cost of nding frequent and the cost of the support counts computation that is 022 003 003 012 003 006\002 006\020  016\002 022 003 007 0114 016\006\020  016\002  003 007 0104 bits Similarity the computational complexity is 5 006 003 012 003 006\002 006\006  016 022 003 007 011 016\006  016  003 007 010 007 modular exponentiations B Privacy preserving learning of ID3 tree in two-dimension distributed data Using the primitive of proposed privacy-preserving frequency mining we can learn ID3 trees in two-dimension distributed data without loss of accuracy The minerês algorithm has the same complexity as the original ID3 tree algorithm except for an additional linear overhead factor Which has a value determined by the number of times frequency mining protocol using to compute gain 1 ID3 decision tree learning we rstly present a brief review of ID3 decision trees An ID3 tree is a rooted tree containing nodes and edges Each internal node is a test node and corresponds to an attribute The edges going out of a node correspond to the possible values of that attribute The ID3 algorithm works as follows The tree is constructed top-down in a recursive fashion At the root each attribute is tested to determine how well it alone classiìes the samples The best attribute is then chosen and the samples are partitioned according to this attribute The ID3 algorithm is then recursively called for each child of this node using the corresponding subset of data Thus major problem of the algorithm is choosing the best attribute that can achieve the maximum information gain at each node Clearly the problem of choosing the best attribute can be reduced to computing entropies that require computation of the frequency of tuples of va 2 Protocol of privacy-preserving ID3 tree learning Let 002\003 be a data set that has the set of non-class attributes 005 003 002 005 002 006 007\007\006 005  003 and  the class attribute Without loss of generality we assume that all attributes have the same domain size d 002 037 002 006 007\007\007\006 037 002 003  002\003 is two-dimension distributed into 011\012 parties as in Section 002 007\005  Each party 014 003\004  015 003 004 006 007\007\011\006 016 003\004 006 007\007\006 012 wns 002\003 003\004  There are 011 parties 014 003\021  015 003\004 006 007\007\011 007 holding the classiìcation attribute   The parties wish to cooperatively build the 002 023 decision tree classiìer from the joint data set of all parties without disclosing each partyês individual transactions and even the number of the local records Assume that parties have set a system as the computation model described in Section 023  In this section we use frequency protocol as the primitive to design the privacy protocol for building decision tree following the ID3 
102 


Algorithm PrivacyPreservingID3 006 005\006 006 002\003 007 1 If 005 is empty return a leaf-node with the class value assigned to the most of all transactions in 002\003  2 Use the privacy preserving method to count the number of records with each class label If 002\003 consists of records which have the same class label 8  return a leaf node with 8  3 Otherwise Determine the best attribute 005 003 for 002\003 using the privacy-preserving method For 005 003 003 002 037 002 006 007\007\007\006 037 002 003 let 002\003 006 037 002 007  002\003 006 037 002 007 be a partition of 002\003 that every record in 002\003 006 037 005 007 has attribute value 037 005  Return a tree whose root is labeled 005 003  the root has outgoing edges labeled 037 002  037 002 s.t each edge 037 005 goes to the tree 014 036\0158\037"\033\014 036\6\\0368\015\011\031&\002 023\006 005 013 005 003 006\(\006\002\003 006 037 005 007\007 007 3 Analysis of protocol The communication/computation depends on the number of records number of vertically partition number of attributes number of attribute values per attribute number of classes and complexity of the tree For a rough analysis the cost of computation involves in terms of the time number of frequency mining protocol called to build the tree Assume that there are 036 nodes in nal classiìcation tree In total each node needs  006\004 016 004 007 the calls of frequency mining protocol All node of the tree need 036 006\004 016 004 007 the frequency computation Therefore in total the entire classiìcation process will require O 036\004<\010 006  016 010 017 010 007  encryptions and O 036\004<\010 006  016 010 017 010 007 4  bits communication VI C ONCLUSION In this paper we proposed a method for privacy-preserving classiìcation learning in two-dimension distributed data which has not been investigated previously Basically the proposed method is based on the ElGamal encryption scheme and it ensures strong privacy without loss of accuracy We illustrated the applicability of the method by applying it to design the privacy preserving protocol for some learning methods such as association rules mining decision tree learning We conducted experiments to evaluate the complexity of the protocols The experimental results showed that the protocols are efìcient and practical R EFERENCES  A Evmie vski R Srikant R Agra w al and J Gehrk e Pri v ac y preserving mining of association rules In Proc of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ACM Press pp 217-228 2002  C C Aggarw al P S Y u Eds Pri v ac y-Preserving Data Mining Models and Algorithms Series Advances in Database Systems Springer Vol 34 2008  D Agra w al and C Aggarw al On the design and quantiìcation of pri v ac y preserving data mining algorithms In Proc ACM SIGMOD pp 247-255 2001  D Boneh The decision Dif fe-Hellman problem In ANTS-III V o l 1423 of LNCS pp 48-63 1998  F  W u J Liu and S Zhong An ef cient protocol for pri v ate and accurate mining of support counts Pattern Recognition Letters Vol 30 Issue 1 1 pp 80-86 2009  O Goldreich F oundations of Cryptography  B asic T ools V o l 1 Cambridge University Press 2001  H Martin and S Kazue Ef cient receipt-free v oting based on homomor phic encryption In Proc of Advances in Cryptology-Eurocrypt 2000  J Benaloh and D T uinstra Receipt-free secretballot elections e xtended abstract In Proc of the 26th Annual ACM Symposium on Theory of Computing ACM Press pp 544-553 1994  J V aidya and C Clifton Pri v ac y preserving nai v e Bayes classiìer for vertically partitioned data In Proc of the 2004 SIAM Conference on Data Mining 2004  J V aidya and C Clifton Pri v ac y preserving association rule mining in vertically partitioned data In Proc of the eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining pp 639-644 2002  Luong T D Ho T B 2010 Pri v ac y Preserving Frequenc y M ining in 2-Part Fully Distributed Setting IEICE Trans Information Systems to appear  M Kantarcoglu and J V aidya Pri v ac y preserving nai v e Bayes classiìer for horizontally partitioned data In IEEE ICDM Workshop on Privacy Preserving Data Mining pp 3-9 2003  M.Freedman K.Nissim and B.Pinkas Ef cient pri v ate matching and set intersection In Proc of Eurocrypt Vol 3027 of LNCS Springer-Verlag pp 1-19 2004  R Agra w a l a nd R Srikant Pri v ac y preserving data mining In Proc of ACM SIGMOD Conference on Management of Data pp 439-450 2000  R Agra w al R Srikant and D Thomas Pri v ac y preserving OLAP  I n Proc of the 2005 ACM SIGMOD International Conference on Management of Data SIGMOD 05 ACM pp 251-262 2005  R Agra w a l a nd R Srikant Pri v ac y-preserving data mining In Proc of the ACM SIGMOD Conference on Management of Data ACM Press pp 439-450 2000  R.Agra w al T  Imielinski and A.Sw ami Mining association rules between sets of items in large databases In Proc of the 1993 ACM SIGMOD international Conference on Management of Data 207-216 1993  S Zhong Z Y a ng and T  Chen k-Anon ymous data collection Journal of Information Sciences Vol 179 Issue 17 pp 2948-2963 2009  V S V e rykios E Bertino I.N F o vino L.P  Pro v e nza Y  Saygin and Y Theodoridis State-of-the-art in privacy preserving data mining ACM SIGMOD Record Vol 3 No 1 pp 50-57 2004  Y  Lindell B Pinkas Pri v ac y preserving data mining In Adv ances in Cryptology Crypto2000 Vol 1880 of LNCS Springer-Verlag pp 36-53 2000  Y  Tsiounis and M Y ung On the security of ElGamal-based encryption In Public Key Cryptographyê98 Vol 1431 of LNCS pp 117-134 1998  Z Y a ng S Zhong R.N Wright Pri v ac y-preserving classiìcation of customer data without loss of accuracy In Proc of the 2005 SIAM International Conference on Data Mining SDM pp 21-23 2005  W  Du and Z Zhan Using randomized response techniques for pri v ac y preserving data mining In Proc of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ACM Press pp 505510 2003  W  Du and Z Zhan Building decision tree classiìer on pri v ate data In Proc of IEEE International Confonference on Privacy Security and Data Mining pp 1-8 2002 
103 


              


   


                        





