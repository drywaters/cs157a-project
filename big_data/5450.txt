May 12 2010 17:39 RPS  Trim Size 8.50in x 11.00in I EEE icfcc 2010-lineupúvol-3 F1655 Mining Maximal Patterns Based on Im proved FP-tree and Array Technique Huajin Wang ,Chunan Hu School of Information Engineering Jiangxi University of Science and Technology Ganzhou, China wanghj128@163.com Yuhuan Chen Center for Educational Technology Gannan Normal University Ganzhou, China chenyuhuan1979@163.com Abstract Mining frequent itemsets is very important for mining association rules. However, because of the inherent complexity, mining complete frequent patterns from a dense 
database could be impractical, and the quantity of the mined patterns is usually very large. It is hard to understand and make use of them. Maximal frequent patterns contain and compress all frequent patterns, and the memory needed for saving them is much smaller than that needed for saving complete patterns. Thus it is greatly valuable to mine maximal frequent patterns. In this paper, the structure of a traditional FP-tree is improved and an efficient algorithm for mining maximal frequent patterns based on improved FP-tree and array technique, called IAFP-max, is presented. By introducing the concept of postfix sub-tree, the presented algorithm neednt generate the candidate of maximal frequent 
patterns in mining process and therefore greatly reduces the memory consume, and it also uses an array-based technique to reduce the traverse time to the improved FP-tree. The experimental evaluation shows that this algorithm outperforms most exiting algorithms MAFIA, GenMax and FPmax   data mining; maximal frequent pattern; improved FP-tree array technique I I NTRODUCTION Mining association rules is an important research field on KDD presented firstly by those people, e.g. R. Agrawal Rules represent the interesting associations or relations between item sets in database. Mining frequent patterns is the fundamental and essential part in many data mining applications, such as the discovery of association rules 
sequential patterns, and so on. Apriori algorithm  and its improved ones are mostly adopted to mine frequent item sets in the past researches. All thes e algorithms make use of the character all subsets of frequent pattern are frequentŽ, but generate plenty of candidate item sets in mining process, and need to scan the original da tabase many times, thus the mining efficiency is cut down. Therefore, people such as Jiawei Han presented an algorithm without candidate generation„FP-growth  which only need to scan database twice:  frequent 1-item sets are gained in the first scanning database, and in the second time non-frequent item sets in original database are filtered with frequent 1-item sets and the FP-tree is built, then the complete frequent patterns are 
mined by performing recursively mining method in FP-tree Experiments show that FP-growth is about an order of magnitude faster than Apriori The drawback of mining complete frequent itemsets is that if there is a large frequent itemsets with size l then almost all 2 l candidate subsets of the items might be generated. However, since frequent itemset are upward closed, it is sufficient to discover only all maximal frequent itemsets. In addition, there are some mining applications which only need to discover maximal frequent patterns, not to discover complete ones, thus it is greatly significant to mine maximal frequent patterns. The previously presented representative algorithms for mining maximal frequent patterns are MaxMiner 
3 DepthProject 4 MAFIA 5  GenMax 6 and FPmax  etc MaxMiner extends Apriori to mine only longŽ patterns maximal frequent itemsets\. To reduce the search space, it performs not only subset infrequency pruning such that a candidate itemsets that has an infrequent subset will not be considered, but also a lookaheadŽ to do superset frequency pruning. Though superset frequency pruning reduces the search time dramatically, it still needs many passes to get all long patterns. DepthProject performs a mixed depthfirst/breadth-first traversal method and also use both subset infrequency pruning and superset frequency pruning. In the algorithm, the database is repr esented as a bitmap. Each row 
in the bitmap is a bitvector corresponding to a transaction each column corresponding to an item. Experiments show that it outperforms MaxMiner by at least an order of magnitude. MAFIA is similar to DepthProject, also uses a bitmap representation, where the count of an itemset is based on the column in the bitmap called vertical bitmapŽ\. To get the bitvectors for any itemset, the bitvector and operation r need to be applied on the b itvectors of the items for any itemset. Besides subset infrequency pruning and superset frequency pruning, the pruning technique called Parent Equivalence Pruning is also used in MAFIA. GenMax also uses a vertical representation of the database. However, for each itemset, it stores a t ransaction identifier 
or TIS, rather than a bitvector. The algorithm takes a novel technique called progressing focusing to maximality testing. This technique maintains a set of local maximal frequent itemsets LMFIs. The newly found FI is firstly compared with itemsets in LMFI. Most non-maximal FIs can be detected by this step, thus reducing the number of subset tests FPmax* extends the earlier algorithm FPmax, it only scans all FP-tee once by using arra y technique and uses MFI-tree to store all already discovered MFIs, adopts efficient method for subset testing. Experimental evaluation shows V3-660 978-1-4244-5824-0/$26.00 c  2010 IEEE 


May 12 2010 17:39 RPS  Trim Size 8.50in x 11.00in I EEE icfcc 2010-lineupúvol-3 F1655 that FPmax* outperforms MAFIA and GenMax in many cases, especially for datasets with short average transaction length and long average pattern length In this paper, an efficient algorithm, called IAFP-max for mining maximal frequent patterns based on improved FPtree and array technique is proposed, the algorithm improves the conventional FP-tree and by introducing the concept of the postfix sub-tree, avoids generating the maximal frequent candidate patterns in mining process and therefore greatly reduces the memory consume, it also uses an array-based technique to reduce the traverse time to the improved FPtree. So it greatly improves the mining efficiency and saves the cost in time and space II PROBLEM DESCRIPTION Let I = {i 1 i 2  i m be a set of literals, called items. Let database T = {t 1 t 2  t n be a set of transactions, where each transaction t j j=1,2 n\ is a set of items such that t j  I Each transaction is associated with a unique identifier, called TID. Let P be a set of items \(or a pattern\, a transaction t is said to contain P if P  t. The support of P is the number or the percentage of transactions in the database that contain P P is frequent if the support of P is no less than a user defined minimum support threshold \(min_sup Definition 1 Let I be a frequent pattern, if there is not another frequent J which J  I, I is called a maximal frequent pattern A Improved FP-tree FP-tree and conditional FP-tree built by FP-growth algorithm need to generate in a top-down order, but the mining process of frequent patterns employs the bottom-up strategy. Because of the recursively generation of conditional FP-tree, both FP-tree and conditional FP-tree need to be able to traverse in two directions, the nodes in these trees require plenty of pointer, thus a great deal of memory is required for saving FP-tree and conditional FPtree Improved FP-tree \(IFP-tree\s similar with FP-tree and each node in IFP-tree consists of four fields item, count ahead and next Where item registers which item this node represents count registers the number of transactions represented by the portion of the path reaching this node ahead links to the left child or the parent of the node, and next links to the right brother of the node or the next node in IFP-tree carrying the same item or null if there is none. We also define two arrays nodecnt and link and link item  registers a pointer which points to the first node in the IFPtree carrying this item  nodecnt item r e gist e r s the suppor t  count sum of those nodes in IFP-tree which carry the same item  In comparison with FP-tree, IFP-tree doesnt contain the path from root to leaf-node, contains fewer pointer than FPtree in mining process, so may greatly save cost in memory The construction method of IFP-tree is similar with that of FP-tree, the difference from FP-tree exits in the process of inserting frequent itemsets in each transaction into IFP-tree In this paper, we dont adopt the method of recursively performing the procedure, insert_tree \([p|P t   bu t em pl oy a  dynamic pointer to complete it. The algorithm for constructing IFP-tree as follows Procedure  FP-tree_construct \(T, min_sup 1 Scan T and count the support of each item, derive a frequent item set \(F\d a list \(L\of frequent items in which items are ordered in frequency-descending order 2 The root of IFP-tree is created and labeled with root 3 For  each transaction t T  do Frequent item set It= t F, in which items are listed to St according to the order of L, define a dynamic pointer \(p_current\ which points to root For each item k S t do If  there is a child\(u\f p_current, and u.item k.item Then  u.count =u.count +1 Else  create a new child \(u\of p_current and u.item= k.item, u.count = 1 4 Traverse IFP-tree in a root-first order and transfer the pointers of ahead and next, count the sum of nodes support carrying the same item and then list together On the step 3\, the ahead points to the left child of nodes and the next points to the right brother of nodes. After the step 4\, the ahead points the parent of nodes and the next points to the next node in IFP-tree carrying the same item  Then in IFP-tree, there is no path from a node to its children or to its brothers For example, let transaction database T be illustrated by TABLE I, and the minimum support \(min_sup\be 4, then we can get the list \(L\ of frequent items, L = {\(c,6 f,6 a,5 b,4 m,4 p,4\}, then IFP-tree is illustrated by Fig. 1 TABLE I TRANSACTION DATABASE  T TID Item Set S t t 1  a b c f m o c f a b m t 2 a c f q d c f a t 3 a f h m b f a b m t 4 b c k s p c b p t 5 f a c d i m p c f a m p t 6 f h j r f t 7 a f c e l p m n c f a m p t 8 d c g b k p c b p B Postfix Sub-tree Firstly let a order be the order of the list L, that is the support descending order of frequent items. Let the letters \(i,j k\ denote items in database, then i is called the minimum item and k is called the maximum item if i j k Volume 3 2010 2nd International Conference on Future Computer and Communication V3-661 


May 12 2010 17:39 RPS  Trim Size 8.50in x 11.00in I EEE icfcc 2010-lineupúvol-3 F1655 Figure 1 IFP-tree of transaction T Definition 2  Let the letters \(i k i 1 k   e n o te  items and i k i 1 P be a path from root to the node N in IFP-tree. If there exits a child node N of the node N and the items \(i k i 1 ear the sub-path from N to N in order that is, the item i k corresponds to the node N and i 1 corresponds to N then P is called the path with the postfix of the item sets {i k  i 1 the support count of the node N also is called the base count of P Definition 3 All paths with the postfix of the item sets i k  i 1 in IFP-tree contain  root, accordingly these paths form a sub-tree ,called the sub-tree with the postfix {i k  i 1  and labeled with PT\(i k  i 1  Let M be a node in PT\(i k  i 1 the sum of  base counts of those paths with the postfix  {i k  i 1 which pass through the node M is called the support count of the node M. In this paper we define an integer array PT\(i k  i 1 nodecnt o  register the support count of all nodes in PT\(i k  i 1 arrying the same item Lemma 1 Let M be a node of PT\(i k  i 1 nd M 1  M j j 1\ be the children of M in the sub-tree PT\(i k  i 1 then the support count of M equals to the sum of support count of M 1  M j  Lemma 2  If the item i m is a node of PT\(i k  i 1 he support count of the pattern { i k i 1 i m equals to the value of PT i k i 1 nodecnt[i m    The two mentioned lemmas can be proved according to the definition2, definition3 and the description of IFP-tree Due to the lack of spac e, we omit this part Let k be an item in frequent item sets F, the postfix subtree PT\(k\ can be built by traversing IFP-tree in bottom-up order. The detailed description of this method is the following: All paths from root to node k in IFP-tree form a sub-tree, in which the support count of each leaf-node equals to that of corresponding node in IFP-tree, and the support count of each inner node \(except root\equals to the sum of support count of its children, then we delete all leaf-nodes thus achieve the sub-tree PT\(k The process of building PT\(m\ is the following firstly, each node in IFP-tree whose value of item is m is retained in PT\(m\ the support count of each inner node except root\ is initialized to be zero. Secondly, for each node, we summate the support count of its children. For example, the support count 3 of the node \(a,3\ equals to the sum of support counts of its children: \(b,1\nd \(m,2 TABLE II shows the result. Finally delete all leaf-nodes. The PT\(m\ is illustrated by Fig. 2 TABLE II SUPPORT COUNTS OF PT\(M CHILDREN item nodecnt c 3 f4 a4 b2 Figure 2 The Example of PT\(m III MINING  MAXIMAL PATTERNS A An Array Technique The main work done in the mining process is traversing the postfix sub-tree to count the support of itemsets and constructing new postfix sub-tree, Recall that for each item i of conditional PT\(x\, two traversals of PT\(x\ are needed for constructing the new sub-tree PT\(k,i\. The first traversal finds all frequent items and their counts of support, the second traversal constructs the new sub-tree PT\(k,i\. In this paper, we use the array technique presented by reference [7   The following example will explain the idea. In TABLE I supposing that the minimum suppoet is 4, after the first scan of the original database, we sort the frequent items as c:6,f:6,a:5,b;4,m:4,p:4. During the second scan of the database we will construct PT and an array A this array will store the counts of all 2-itemsets. All cells in the array are initialized as 0 In A each cell is a counter of a 2-itemset, such as A b,c  is the c o unte r f o r  ite m s et {b  c   Du r i ng the s e c o n d  scan for constructing the tree PT for each transaction , first all frequent items in the trans action are extracted. Suppose these items form itemset I To insert I into PT the items in I are sorted according to the order in the header of PT When we insert I into PT at the same time A i,j is inc r em ented by 1 if {i,j} is contained in I After the second scan , array  V3-662 2010 2nd International Conference on Future Computer and Communication Volume 3 


May 12 2010 17:39 RPS  Trim Size 8.50in x 11.00in I EEE icfcc 2010-lineupúvol-3 F1655 A keeps the counts of all pairs of  frequent items, as shown in table \(a\ of Fig. 3 Therefore ,for each item i in PT the array A makes the first traversal of PT unnecessary, and PT\(i\ can be initialized directly from A For the same reason, from a subtree PT\(x\ when we construct a new PT\(x,i\ for an item i, a new A X i is calculated. During the construction of the new PT\(x,i\, the array A X i is filled. For example, the cells of array A b is shown in table \(b\ of Fig. 3 Figure 3 Two Array Examples B Algorithm Description As shown in Fig. 2, we can mine the local maximal frequent pattern \(LMFP m in which m is the maximal item that is {\(f,a,m\,4}, by examining whether each item of PT m nodecnt[ite i n t h e T A B L E I I is no les s t h an  min_sup or not. The support count of LMFP m is the minimum of that of all items in LMFP m  The idea of IAFP-max is introduced as follows: for each item i in L, we build PT\(i\ and mine LMFP i then combine all local maximal frequent patterns to gain the ultimate result. According to the correctness of FP-growth method we can conclude that IAFP-max returns all and only the maximal frequent patterns in the given dataset. The following is the main procedure of algorithm Procedure  IAFP-max \(IFP-tree, L, min_sup MFPs   MFPs is the set of all maximal frequent patterns For each  i L do  PT\(i\struct \( IFP-tree, i Mine LMFP i from PT\(i MFPs = MFPs LMFP i    In the step of MFPs LMFP i if LMFP i  MFPs MFPs keep changeless, else insert LMFP i into MFPs, then performs subset-checking if there is a pattern I  MFPs and I  LMFP i then delete I from MFPs Let the items in L be ranked into 1 k, and 1  i  k. The following is the procedure of constructing PT\(i Procedure  PT_construct \(IFP-tree, i for j = 1 to i-1 do link[j null and no d ecn t j  0   for each node N in link  basecnt = N.count basecnt is the base count let M be the parent of node N while  \(M.item  1  if   M isnt in link[M.item  Then insert M into link[M.item a n d  M.count = basecnt else M.count = M.count + basecnt nodecnt[M.item   n odecn tp[M.item     basecnt let M be its new parent again    By using the algorithm, the set of maximal frequent patterns is as follows: { { \(c,f,a\, 4 }, {\(b\,4 }, { \(f,a,m\4 c,p\ 4 C Algorithm Optimization The construction of postfix sub-tree PT\(i\ is the key step of mining process, decides the mining efficiency of IAFP-max. There are two optional traversal orders while i F\: the original order of L or the reverse order of L. In this paper, we adopt the former, that is, the items with large suppor t count are built earlier than those with small support count If the postfix sub-trees are built in the original order of L, because i LMFP i i  MFP, LMFP i  MFPs, so in the step of MFPs LMFP i we can omit testing whether LMFP i  MFPs or not and directly insert LMFP i into MFPs then go to the step of subset-checking If the reverse order of L is adopted, it is likely to exist LMFP i  MFPs, so we neednt insert LMFP i into MFPs. However, in many practical mining applications, Let i,j F, i j, the probability of LMFP i  LMFPj is not very large. For example, as shown by Figure 1, a b, but LMFP a  c,f,a\MFP b b\.4}, LMFP a  LMFP b  Otherwise, if the original order of L is adopted to construct PT\(i\, the construction process doesnt transform the structure of IFP-tree, therefore the storage of IFP-tree and the construction of PT\(i\ can be carried out in a same kind of data structure and avoid repeatedly constructing and deleting PT\(i\n the mining process. Consequently, avoid the waste of the efficiency in time and space and greatly improve the performance of the proposed algorithm IV E XPERIMENTAL EVALUATION The experiments were conducted on 2.1 Ghz Pentium with 512 MB of memory running Microsoft Windows XP All codes was compiled using Microsoft Visual C++ 6.0. We used the dense dataset Connect-4 and the sparse dataset  Volume 3 2010 2nd International Conference on Future Computer and Communication V3-663 


May 12 2010 17:39 RPS  Trim Size 8.50in x 11.00in I EEE icfcc 2010-lineupúvol-3 F1655 downloaded form a websit and compared the algorithm IAFP-max with GenMax MAFIA and FPmax Fig. 4 shows the experimental results for the datasets Connect-4 and As shown in the part \(a\, we can know that IAFP-max outperforms greatly GenMax and MAFIA for high levels of minimum support.  IAFP-max is about one to two orders of ma gnitude faster than GenMax and MAFIA for all levels of minimum support The part \(b\ of Fig. 4 gives the experimental results for the dataset The results show that IAFP-max outperforms others algorithms and is the fasterst one. IAFPmax is about one to two orders of magnitude faster than GenMax and MAFIA for all levels of minimum support Figure 4 a\omparison in Connect-4    \(b\comparison in V CONCLUSIONS In this paper, an efficient algorithm, called IAFP-max for mining maximal frequent patterns based on improved FP-tree and array technique is proposed, the algorithm improves the conventional FPtree and by introducing the concept of the postfix subtree, avoids generating the maximal frequent candidate patterns in mining process and therefore greatly reduces the memory consume. It also uses an array-based technique to reduce the traverse time to the improved FP-tree. Therefore it greatly improves the mining efficiency in time and space scalability. Experimental results show that this algorithm is indeed very effective A CKNOWLEDGMENT This research is supported by th the  Youth Science Foundation of Department of Education  of Jiangxi Province, NO. GJJ09522 R EFERENCES 1 R A g ra w a l  T I m i e li n s k i and A S w a m i  Min in g a s s o c i a t i o n ru les  between sets of items in large database,Ž Proceedings of the ACM SIGMOD International Conference Management of Date Washington, 1993, pp.  207-216 2 J H a n  J P e i an d Y Y i n   M i n ing f r e q u e n t p a tte r n s w itho u t  ca nd id ate  generation,Ž Proceedings of  Special Interest Group on Management of Data, Dallas, May 2000, pp. 1-12 3 R J B a y a r d o   E f f i cie n tl y m i ni ng l o ng p a t t e r ns f r o m  dat a b a s e s    Proceedings of special Interest Group on Management of Data Seattle, WA, June 1998, pp. 85-93 4 R  C A g g a r w a l C C A g ar w a l an d V V V P r as ad  D e p t h F i r s t  Generation of Long Patterns,Ž Proceedings of the 6 th ACM SIGMOD International Conference on Knowledge Discovery & Data Mining Boston, MA, USA, August 2000 5 B ur di ck D o ug   Cal i m l im  Ma nue l   an d G e hr ke J o ha n n e s  A Max i m a l  Frequent Itemset Algorithm for Transactional Database Proceedings of the 17 th International Conference on Data Engineering, Heidelberg, Germany, April 2001,pp . 443-452 6 K  G o uda a n d  M J Z a ki  E f f i c i e n tl y Min i ng M a x i m a l F r e que nt  Itemsets,Ž 1 st IEEE International Conference on Data Mining, San Jose,  pp . 163-170, November 2001 7  G  G r a h n e an d J Zhu  E f f i c i ent l y U s i n g P r ef i x t r ees i n  M i ni n g  Frequent Itemsets,Ž First Workshop on Frequent itemset Mining Implementation\(FIMI03\, Melbourne, FL, 2003 8 E  B e rti n o I  N F o vi n o  a n d L  P Prove n z a   A fra me work for evaluating privacy preserving data mining algorithms,Ž Data Min Knowl. Discov., vol. 11\(2\, 2005, pp. 121-154, doi:10.1007/s10618005-0006-6      a        b   V3-664 2010 2nd International Conference on Future Computer and Communication Volume 3 


Journal of Experimental Child Psychology 1997, pp.317 337 14 K  D a nie l, D  H i r s h l e i f e r  a nd A  S ubr a h m a ny am  I nv e s tor  psychology and s ecurity market unde r- and ove rreactions  Journal of Finance 1998, pp.1839…1885 15 D  G a rc i a F. Sa ng iorg i, a n d B   U r os e v ic  O ve rc onf ide nc e  and market efficiency with heterogeneous agents Economic Theory 2007, pp.313…339 16 E. H o e l z l a n d A  Rus tic hin i O v e rc onf ide nt: D o y ou put  your money on it Economic Journal 2005, pp.305…318 17 E. H a z e ltine a nd P  A p a r ic io C onf ig ura l re s pons e le a r ning   The acquisition of a nonpredictive motor skill Journal of Experimental Psychology: Human Perception and Performance 2007, pp.1451-1467 18 A  B a ndur a   S e l f ef f i c a c y  tow a r d a unif y ing the or y of  behavioral change Psychological Review 1997, pp. 191215 19 A  B a ndur a   S e l f ef f i c acy  m e cha nis m in hum a n a g e n cy    American Psychologist 1982, pp.122-147 2 M  S h erer an d J E  M a dd u x  T h e sel f e ff i cac y scal e  construction and validation Psychological Reports 1982 pp. 663-671 21 S. B  R o bbi ns  K  L a uv e r H  L e D  D a v i s  R  L a ng ley a n d  A, Carlstrom, Do Psychosocial and Study Skill Factors Predict College Outcomes?, A Meta-Analysis Psychological Bulletin 2004, pp.261…288 2 B S a nd ers F act o r s aff ect i n g reversal an d no nreversal sh i f t s  in rats and children", Journal of comparative and Physiological Psychology, 1971, pp.192-202 23 A  J W ills, M. No ury N. J. Mobe rly a nd M. Ne w port  Formation of category representations Memory and Cognition 2006, pp.17…27 24 D  A  Mo ore  a n d  P  J  H e a l y   T he T r ouble W ith  Overconfidence Psychological Review 2008, pp.502…517 25 L  Bre nne r D  G r iff i n, a n d D  J   K o e h le r M ode li ng  pa tte rns  of probability calibration with random support theory Diagnosing case-based judgment Organizational Behavior and Human Decision Processes 2005, pp. 64…81 26 S. L i c h te ns te in a n d B   Fis c h h o f f   D o t hos e w ho k now m o re  also know more about how much they know Organizational Behavior and Human Decision Processes  1977, pp.159…183 27 K. A  Burson, R. P. L a rric k a nd J. Kla y m a n S k ille d or unskilled, but still unaware of it: How perceptions of difficulty drive miscalibration in relative comparisons Journal of Personality and Social Psychology 2006, pp.60 77 28 D  W  G r i f f i n, a nd A  T v e r sky  T he  w e ig hing of e v ide nc e and the determinants of confidence Cognitive Psychology  1992, pp.411…435 29 A  K o r i a t L  She f f e r a nd H  Ma  a y a n C om pa r i ng  Objective and Subjective Learning Curves: Judgments of Learning Exhibit Increased Underconfidence With Practice Journal of Experimental Psychology: General 2002 pp.147…162 30 B  F i n n a n d J  M e t c a l f e   T h e R o l e o f M e m o r y f o r P a s t T e s t  in the Underconfidence With Practice Effect Journal of Experimental Psychology: Learning Memory, and Cognition  2007, pp.238-244 
88 
86 


Output? The complete set of frequent patterns Method:  FP-growth\(FP-tree, null Procedure FP-growth\(Tree  1 single prefix-path FP-tree 2 3 274 4 node replaced by a null root 5 denoted as the path P do 6 minimum support of nodes in 7 P generated 8 9 Mining multipath FP-tree 10 ai .support 11 conditional FP-tree Tree 12 13 Tree 14 Q generated 15 freq pattern set\(P Q freq pattern set\(P Q Using the FP-growth algorithm, getting the frequent item sets F of the database D A1:6},{B1:4},{B2:2},{B3:2},{C3:2},{D4:2},{A1,B1 2},{A1,B2:2},{A1,B3:2},{B1,C3:2},{B1,D4:2},{C3,D4:2 B1,C3,D4:2 C. The process of association rules 1 above ,continuing the  association rule mining If A ? B = ?  and A ? ? B is also a frequent set .Then calculate the path degree of credibility P = [support \(AUB A 2 


B and store it 3 association rules Else return to 1 The interest in the site combined with the previous degree of experience, setting the minconf as 0.30, the final rules by the association is A1=>B1;B1=>C3;C3=>D4; B1=>D4; \(B1,C3 Association rules can be drawn from the above the correlation between pages, understanding the user interface on the site who is interested in, mining the user preferred path to provide more valuable data to the construction site IV.  CONCLUSIONS In this paper, using the mining association rules those based on FP-Grow algorithm, combining with interest measure and website users to view the topology of the user the best access time, all the data mining are based on the web use log data source, analysising the users behavior to find the user s browsing mode to provide valuable data for the optimization of the site REFERENCES  1] Jiawei Han and Micheline Kamber Data Mining: Concepts and Techniques \(2nd edition 2]  Bamshad Mobasher .Web Usage Mining .Springer Berlin Heidelberg 2007, 449-483 3]  Rskesh Agrawal, Ramakrishnan Strikant .Fast Algorithms for Mining Association Rules.Visting from the Department of Computer Science ,University of Wisconsin,Madison.1-32 4]  Jaideep Srivastava and Robert Cooley U?sage Mining: Discovery and Applications of Usage Patterns from Web Data. ACM SIGKDD Explorations  Newsletter .2000,1\(2 5]  Jiawei Han ,Jian Pei,Yiwen Yin ,Runying Mao .Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach.2004,53-87  275 


The third phase extracts performance signatures by capturing frequently observed correlations among metrics from the historical dataset. Many metrics will exhibit strong correlations under normal operation. For example, medium request arrival rate would lead to medium usage of system processing power, and medium throughput. Thus, one signature of frequently observed correlation could be {Arrival Rate Medium, CPU utilization = Medium, Throughput = Medium To extract metric correlations, we use two data mining concepts: frequent item set and association rule. A frequent item set describes a set of metrics that appear together frequently. In this paper, we use the Apriori algorithm [5] to discover the frequent item sets. Association rules can be derived from a frequent item set. For example, Figure 6 shows one of the three association rules that can be derived from the frequent item set in the previous example. An association rule has a premise and a consequent. The rule predicts the occurrence of the consequent based on occurrences of the premise. The Apriori uses support and confidence to reduce the number of candidate rules generated Support is defined as the frequency at which all items in an association rule are observed together. Low support means that the rule occurs simply due to chance and should not be included in our analysis Confidence measures the probability that the rules premise leads to the consequent. For example, if the rule in Figure 6 has a confidence value close to 1, it means that when arrival rate and throughput are both medium, there is a high tendency that medium CPU utilization will be observed We apply the association rules extracted from the historical dataset to the new test and flag metrics in the rules that have significant change in confidence, as defined in eqn. \(1    1     2 


3 Confb and Confn represent the confidence of a rule in the historical dataset and the new test respectively. Confidence change is defined in terms of cosine distance, which measures the similarity between two vectors. Since Confb and Confn are scalar values, we must convert them into vector form with eqn 2 and 3 in order for the cosine distance to be calculated The confidence change for a rule will have a value between 0 and 1. A value of 0 means the confidence for a particular rule has not changed in the new test; value of 1 means the confidence of a rule is completely different in the new test. If the confidence change for a rule is higher than a specified threshold, we can conclude that the behavior described by the rule has changed significantly in the new test and the metrics in the rules consequent are flagged. For example, if the rule in Figure 6 drops in confidence from 0.9 to 0.2 in the new test, it indicates that medium arrival rate and throughput would no longer be associated with medium CPU utilization for the majority of the time. As a result, CPU utilization exhibits a significant change of behavior and should be investigated 4. Report generation In the last phase, we generate a report of the flagged metrics that highlights the association rules that the metrics violate. To further help a performance analyst to prioritize her time, we rank the metrics by level of severity \(eqn. 3 the report lists the violated rules ordered by confidence change eqn. 1 3  Severity represents the fraction of time in the new test that contains the flagged metric. Severity ranges between 0 and 1. If there are only a few instances where the metric is observed to be problematic, the severity will have a value close to 0. On the other hand, if the metrics are violated many times, severity will have a value close to 1 \(Figure 1a flagged in the report, we can conclude that the new test  Figure 6. Example of an Association Rule For each metric High = All values above the medium level Medium = Median +/- 1 standard deviation Low = All values below the medium level 


Figure 5. Definition of Metric Discretization Levels 36 performance has no performance regression and can be included in the historical dataset for analysis of future tests V. CASE STUDY We conducted three case studies on two open source ecommerce applications and a large enterprise system. In each case study, we wanted to verify that our approach can reduce the amount of data a performance analyst must analyze and the subjectivity involved, by automatically reporting a list of potential problematic metrics We manually injected faults into the test scenarios of the two open source e-commerce systems. This allows us to assess our approach using the precision \(eqn. 4 eqn. 5 evaluation metrics 4 5 High precision and recall mean that our approach can accurately detect most performance problems. Performance analysts can reduce the effort required for an analysis by investigating the flagged metrics. Note that false positives are metrics that are incorrectly flagged \(they do not lead to a performance regression For the large enterprise system, we use the existing performance metrics collected by the Performance Engineering team as the input of our technique. We seek to compare the results generated by our approach against the performance analysts observations. In cases where our approach flagged more metrics than the performance analysts noted, we verify the additional problematic metrics with the organizations Performance Engineering team to determine if the metrics truly represent performance regressions. Since we do not know the actual number of performance problems, we can only provide the precision of our approach We use the average precision and recall to show the overall performance of our approach across all test scenarios for each system. Average precision and recall combine the precision eqn. 6 eqn. 7 t1,t2 , ,tk performance of our approach in each case study   


 6    7  Research Prototype: Our research prototype is implemented in Java and uses the Weka package [20] to perform various data-mining operations. The graphs in the performance analysis reports are generated with R [4 A. Studied System: Dell DVD Store System description: The Dell DVD Store \(DS2 application [3] is an open source simulation of an online ecommerce website. It is designed for benchmarking Dell hardware. DS2 includes basic e-commerce functionalities such as user registrations, user login, product search and purchase DS2 consists of a back-end database component, a Web application component, and driver programs. DS2 has multiple distributions to support different languages such as PHP, JSP or ASP and databases such as MySQL, Microsoft SQL server and Oracle. The load driver can be configured to deliver different mixes of workload. For example, we can specify the average number of searches and items per purchase In this case study, we have chosen to use the JSP distribution and a MySQL database. The JSP code runs in a Tomcat container. Our load consists of a mix of use cases including user registration, product search, and purchases Data collection: We collected 19 metrics as summarized in table 2. The data is discretized into 2-minute intervals. We ran 4 one-hour performance regression tests. The same load is used in tests A, B, and C. Our performance signatures are derived from Test A during which normal performance is assumed. For tests C and D, we manually inject faults into either the JSP code or the load driver settings to simulate implementation defects and performance analysts mistakes. The types of faults we injected are commonly used in other studies [13]. Prior to the case study, we derive a list of metrics that are expected to show performance problems, as summarized in Table 3. The Recall of our approach is calculated based on the metrics listed in Table 3  


TABLE II.     SUMMARY OF METRICS COLLECTED FOR DS2 Load Generator Processor Time Orders/minute Network Bytes Sent/sec Network Bytes Received/Sec Tomcat Processor Time Threads Virtual Bytes Private Bytes MySQL Processor Time Private Bytes Bytes written to disk/sec Context Switches/sec Page Reads/sec Page Writes/sec Committed Bytes In Use Disk Reads/sec Disk Writes/sec I/O Reads Bytes/sec I/O Writes Bytes/sec TABLE I.   AVERAGE PRECISION AND RECALL of Test Scenarios Duration per Test hours Size of Data per Test Avg Precision Avg Recall DS2 4 1 360 KB 100% 52 JPetStore 2 0.5 92 KB 75% 67 Enterprise System 13 8 4.5 MB 93% N/A  


37  Analysis of Test B: The goal of this experiment is to show that the rules generated by our approach are stable under normal system operation. Since Test B shares the same configuration and same load as Test A, ideally our approach should not flag any metric Our prototype did not report any problematic metric in Test B. The output is as expected, since Test B uses the same configuration as Test A and no performance bug was injected Analysis of Test C: In test C, we injected a databaserelated bug to simulate the effect of an implementation error This bug affects the product browsing logic in DS2. Every time a customer performs a search on the website, the same query will be repeated numerous times, causing extra workload for the backend database and Tomcat server Our approach flagged a database related metric \(# Disk Reads/sec Threads and # private bytes signaling that the metrics are violated during the whole test The result agrees with the nature of the injected fault: each browsing action generates additional queries to the database As a result, an increase in database transaction leads to an increase of # Disk Reads/sec. When the result of the query returns, the application server uses additional memory to extract the results. Furthermore, since each request would take longer to complete due to the extra queries, more threads are created in the Tomcat server to handle the otherwise normal workload. Since 3 out of 6 expected problematic metrics are detected, the precision and recall of our approach in Test C are 100% and 50% respectively Analysis of Test D: We injected a configuration bug into the load driver to simulate that a wrongly configured workload is delivered to the system. This type of fault can either be caused by a malfunctioning load generator or by a performance analyst when preparing for a performance regression test [14 In the case where a faulty load is used to test a new version of the system, the assessment derived by the performance analyst may not depict the actual performance of the system under test In Test D, we double the visitor arrival rate in the load driver. Furthermore, each visitor is set to perform additional browsing for each purchase. Figure 7 below shows the violated 


metrics reported by our prototype. The result is consistent with the nature of the fault. Additional threads and memory are required in the Tomcat server to handle the increased demand Furthermore, the additional browsing and purchases lead to an increase in the number of database reads and writes. The extra demand on the database leads to additional CPU utilization Because of the extra connections made to the database caused by the increased number of visitors, we would expect the # context switch metric in the database to be high throughout the test. To investigate the reason for the low severity of a databases context switch rate \(0.03 examined the rules flagged the # context switch metric. We found that the premises of most rules that flagged the context switch metric also contain other metrics that were flagged with high severity. Consequently, the premises of the rules that flagged # context switch are seldom satisfied resulting in the low detection rates of the # context switch metrics. Since 7 out of 13 expected metrics are detected, the precision and recall of our approach in this test are 100% and 54% respectively B. Studied System: JPetStore System description: JPetStore [1] is a larger and more complex e-commerce application than DS2. JPetStore is a reimplementation of Sun's original J2EE Pet Store and shares the same functionality as DS2. Since JPetStore does not ship with a load generator, we use a web testing tool to record and replay a scenario of a user logging in and browsing items on the site Data collection: In this case study, we have conducted two one-hour performance regression tests \(A and B performance signatures are extracted from Test A during which caches are enabled. Test B is injected with a configuration bug in MySQL. Unlike the DS2 case study where the configuration bug is injected in the load generator, the bug used in Test B simulates a performance analysts mistake to accidentally disable all caching features in the MySQL database. Because of the nature of the fault, we expect the following metrics of the database machine to be affected: CPU utilization, # threads context switches, # private bytes, and # I/O read and write bytes/sec Analysis of Test B: Our approach detected a decrease in memory footprint \(# private bytes sec in the database, and increase in # disk reads/sec and 


threads in the database. The I/O metrics include reading and writing data to network, file, and device. These observations align with the injected fault: Since the caching feature is turned off in the database, less memory is used during the execution of the test. In exchange, the database needs to read from the disk for every query submitted. The extra workload in the database  Figure 7. Performance Regression Report for DS2 Test 4 \(Increased Load TABLE III.     SUMMARY OF INJECTED FAULTS FOR DS2 Test Fault Injected Expected Problematic metric A No fault N/A B No fault No problem should be observed C Busy loop injected in the code responsible for displaying  item search results Increase in # I/O reads bytes /sec, and disk read/sec in database Increase in # threads, # private and virtual bytes, and CPU utilization in the Tomcat server D Heavier load applied to simulate error in load test configuration Increase in CPU utilization, # threads private and virtual bytes in the Tomcat server Increase in database CPU utilization disk reads, writes and I/O read bytes per second, and # context switches Increase in # orders/minute and network activities in the load generator 38 translates to a delay between when a query is received and the result is sent back, leading to a decrease in # IO write bytes/sec to the network Instead of an increase, an unexpected drop of the # threads was detected in the database. Upon verifying with the raw data for both tests, we found that the thread count in Test A \(with cache without cache 


and 21 respectively. Upon inspecting the data manually, we do not find that the decrease of one in thread count constitutes a performance problem and this is therefore a false positive Finally, throughout the test, there is no significant degradation in the average response time. Since 4 out of 6 expected problems are detected, our performance regression report has a precision of 75% and recall of 67 C. Studied System: A Large Enterprise System System description: Our third case study is conducted on a large distributed enterprise system. This system is designed to support thousands of concurrent requests. Thus, performance of this system is a top priority for the organization. For each build of the software, performance analysts must conduct a series of performance regression tests to uncover performance regressions and to file bug reports accordingly. Each test is run with the same workload, and usually spans from a few hours to a few days. After the test, a performance analyst will upload the metric data to an internal website to generate a time series plot for each metric. This internal site also serves the purpose of storing the test data for future reference. Performance analysts then manually evaluate each plot to uncover performance issues. To ensure correctness, a reviewer must sign off the performance analysts analysis before the test can be concluded. Unfortunately, we are bounded by a NonDisclosure Agreement and cannot give more details about the commercial system Data collection: In this case study, we selected thirteen 8hour performance regression tests from the organizations performance regression testing repository. These tests were conducted for a minor maintenance release of the software. The same workload was applied to all tests. In each test, over 2000 metrics were collected Out of the pool of 13 tests, 10 tests have received a pass status from the performance analysts and are used to derive performance signatures. We evaluated the performance of the 3 remaining tests \(A, B and C the performance analysts assessment \(summarized in table 4 In the following sections, we will discuss our analysis on each target test \(A, B and C Analysis of Test A: Using the history of 10 tests, our approach flagged all throughput and arrival rate metrics in the system. The rules produced in the report imply that throughputs 


and arrival rates should fall under the same range. For example component A and B should have similar request rate and throughput. However, our report indicates that half of the arrival rates and throughput metrics are high, while the other half is low. Our approach has successfully uncovered problems associated with the arrival rate and throughput in Test A that were not mentioned in the performance analysts report. We have verified our finding with a performance analyst. Our performance regression report has a precision of 100 Analysis of Test B: Our approach flagged two arrival rate metrics, two job queue metrics \(each represents one subprocess consulting with the time-series plots for each flagged metric as well as the historic range, we found that the # database scans/sec metric has three spikes during the test. These spikes are likely the cause of the rule violations. Upon discussing with a performance analyst, we find that the spikes are caused by the systems periodic maintenance and do not constitute a performance problem. Therefore, the # database scans/sec metric is a false positive. Our performance analysis report has a precision of 80 Analysis of Test C: Our approach did not flag any rule violation for this test. Upon inspection of the historical value for the metrics noted by the performance analyst, we notice that the increase of # database transactions/sec observed in Test C actually falls within the metric historical value range. Upon discussing with the Performance Engineering team, we conclude that the increase does not represent a performance problem. In this test, we show that our approach of using a historical dataset of prior tests is more resistant to fluctuations of metric values. Our approach achieves a precision of 100 The case studies show that our approach is able to detect problems in metrics when the faults are present in the systems Our approach detects problematic metrics with high precisions in all three case studies. In our case studies with the two open source systems, our approach is able to cover 50% and 67% of the expected problematic metrics VI. DISCUSSION AND FUTURE WORK A. Quantitive Techniques Although there are existing techniques [10, 11] to correlate anomalies with performance metrics by mining the raw performance data without discretization, these techniques 


usually assume the presence of Service Level Objectives \(SLO that can be used to determine precisely when an anomaly occurs. As a result, classifiers that predict the state of SLO can be induced from the raw performance data augmented with the SLO state information. Unfortunately, SLOs rarely exist during development. Furthermore, automated assignment of SLO states by analyzing metric deviations is also challenging as there could be phase shifts in the performance tests, e.g., the spikes do not align. These limitations prevent us from using classifier based techniques to detect performance regression TABLE IV.      SUMMARY OF ANALYSIS FOR THE ENTERPRISE SYSTEM Test Performance Analysts Report Our Findings A No performance problem found Our approach identified abnormal behaviors in system arrival rate and throughput metrics B Arrival rates from two load generators differ significantly Abnormally high database transaction rate High spikes in job queue Our approach flagged the same metrics as the performance analysts analysis with one false positive C Slight elevation of database transactions/sec. No metric flagged  39 B. Sampling period and Metric Discretization We choose the size of time interval for metric discretization based on how often the original data is sampled. For example an interval of 200 seconds is used to discretize data of the enterprise system, which was originally sampled approximately every 3 minutes. The extra 20 second gap is used because there was a mismatch in sampling frequencies for some metrics. We also experimented with different interval lengths. We found that less metrics are flagged as the length of the interval increases, while precision is not affected 


In our case studies, we found that the false negatives metrics that were expected to show performance regressions but were not detected by our approach no rule containing the problematic metrics was extracted by the Apriori algorithm. This was caused by our discretization technique sometimes putting all values of a metric that had large standard deviation into a single level. Candidate rules containing those metrics would exhibit low confidence and were thus pruned. In the future, we will experiment on other discretization techniques, such as Equal Width Interval Binning C. Performance Regression Testing Our approach is limited to detecting performance regressions. Functional failures that do not have noticeable effect on the performance of the system will not be detected Furthermore, problems that span across the historical dataset and the new test will not be detected by our approach. For example, no problem will be detected if both the historical dataset and the new test show the same memory leak. Our approach will only register when the memory leak worsens or improves D. Passed Tests The historical dataset from which the association rules are generated should contain tests that have the same workload configuration, preferably same hardware, and exhibit correct behavior. Using tests that contain performance problems will decrease the number of frequent item sets extracted, making our approach less effective in detecting problems in the new test. In our case study with the enterprise system, we applied the following measure to avoid adding problematic tests to our historical dataset We selected a list of tests from the repository that have received a pass status from the performance analyst We manually examined the performance metrics that are normally used by a performance analyst in each test from the list of past test to ensure no abnormal behavior was found E. System Evolution and Size of Training Data The system is often updated to support new environments or requirements. These updates may lead to changes in performance. A large variability in metric values will negatively affect the confidence of association rules generated 


in our approach. Therefore, it is necessary to update the set of tests included in the historical dataset. We are currently studying the effect of using a sliding window to select prior tests to include in the historical dataset. A sliding window allows us to automatically discard outdated tests that no longer reflect the current systems performance. However, the optimal size of the sliding window will likely be project-dependent since each project has different release frequency Alternatively, the historical dataset can also be derived from within the run. For example, the first hour of the current test can be used to derive performance signatures. Assuming that the system runs correctly during the first hour, the performance signature generated from this historical dataset will be useful to assess the stability of the system F. Hardware Differences In practice, performance regression tests of a system can be carried out on different hardware. Furthermore, third party components may change in between tests. In the future, we plan to improve our learning algorithm so that, given a new test, our tool will automatically select the tests from the repository with similar configurations G. Automated Diagnosis Our approach automatically flags metrics by using association rules that show high deviations in confidence between the new tests and the historical dataset. These deviations represent possible performance regressions or improvements and are valuable to performance analysts in assessing the system under test. Performance analysts can adjust the deviation threshold to restrict the number of rules used and, thus, limit the number of metrics flagged. Alongside with the flagged metrics, our tool also displays the list of rules that the metric violated. Performance analysts can inspect these rules to understand the relations among metrics. From our case study, we notice that some of the rules produced are highly similar. In the future, we will research for ways to merge similar rules to further condense information for performance analysts to analyze The association rules presented in our performance regression report represent metric correlations rather than causality. Performance analysts can make use of these correlations to manually derive the cause of a given problem VII. RELATED WORK 


Our goal in this work is to detect performance problems in a new test using historical data. Existing approaches monitor or analyze a system through one of two sources of historical data execution logs and performance metrics A. Analyzing Execution Logs Reynolds el al. [18] and Aguilera et al. [22] developed various algorithms for performance debugging on distributed systems. Their approach analyzes message trace of system components to infer the dominant causal paths and identify the components that account for a significant fraction of the systems latency. Unfortunately, the accuracy of the inferred paths decreases as the degree of parallelism increases, leading to low precision in identifying problematic components. Our approach is different from Reynoldss and Aguileras in that we pinpoint performance issues on the metric level rather than locating the system components that contribute significantly to system latency. Jiang et al. introduce a technique [14] to 40 identify functional problems in a load test from execution logs The authors extended this approach to analyze performance in scenarios as well as in the steps of each scenario [13]. Chen et al. proposed Pinpoint [21] to locate the subset of system components that are likely to be the cause of failures. Our work is different from Pinpoint in that Pinpoint focuses on identifying system fault rather than performance regression which can occur even when the system functions correctly In contrast to the above studies, which analyze execution logs, our approach analyzes performance metrics to identify performance problems B. Analyzing Performance Metrics Bondi [9] presented a technique to automatically identify warm-up and cool-down transients from measurements of a load test. While Bondis technique can be used to determine if a system ever reaches a stable state in the test, our approach can detect performance problems at the metric level Cohen et al. [11, 12] applied supervised machine learning techniques to induce models on performance metrics that are likely to correlate with observed faults. Bodik et al. improved Cohens work [8] by using logistic regression. Our approach is different from the above work as we do not require knowledge of violations of Service Level Objectives Jiang et al. proposed an approach [16] for fault detection 


using correlations of two system metrics. A fault is suspected when the portion of all derived models that report outliers exceeds a predefined threshold. Our approach is based on frequent item sets that can output correlations of more than two metrics. Performance analysts can leverage these metric correlations to better understand the cause of a fault. Jiang et al 15] proposed an approach to identify clusters of correlated metrics with Normalized Mutual Information as similarity measure. The authors were able to detect 77% of the injected faults and the faulty subsystems, without any false positives While the approach in [15] can output only the faulty subsystems, our approach can detect and report details about performance problems, including metrics that deviate from the expected behaviors VIII. CONCLUSIONS It is difficult for performance analysts to manually analyze performance regression testing results due to time pressure large volumes of data, and undocumented baselines Furthermore, subjectivity of individual analysts may lead to incorrect performance regressions being filed. In this paper, we explored the use of performance regression testing repositories to support performance regression analysis. Our approach automatically compares new performance regression tests to a set of association rules extracted from past tests. Potential performance regressions of system metrics are presented in a performance regression report ordered by severity. Our case studies shows that our approach is easy to adopt and can scale well to large enterprise system high precision ACKNOWLEDGMENT We are grateful to Research In Motion \(RIM access to the enterprise application used in our case study. The findings and opinions expressed in this paper are those of the authors and do not necessarily represent or reflect those of RIM and/or its subsidiaries and affiliates. Moreover, our results do not in any way reflect the quality of RIMs products REFERENCES 1] iBATIS JPetStore, http://sourceforge.net/projects/ibatisjpetstore 2] MMB3, http://technet.microsoft.com/enus/library/cc164328%28EXCHG.65%29.aspx 3] The Dell DVD Store, http://linux.dell.com/dvdstore 4] The R Project for Statistical Computing. http://www.r-project.org 5] R. Agrawal, R.Srikant, Fast Algorithms for Mining Association Rules 


in Large Databases, Proc. of 20th Intl Conf. Very Large Data Bases 1994 6] A. Avritzer and B. Larson, Load testing software using deterministic state testing, Proc. of Intl Symp. on Software Testing and Analysis 1993 7] A. Avritzer, E. J. Weyuker, The automatic generation of load test suites and the assessment of the resulting software, IEEE Trans. Softw. Eng 21\(9 8] P. Bodik, M. Goldszmidt, A. Fox, HiLighter: Automatically Building Robust Signatures of Performance Behavior for Small- and Large-Scale Systems, Proc. of the  3rd SysML, Dec 2007 9] A. B. Bondi, Automating the Analysis of Load Test Results to Assess the Scalability and Stability of a Component, Proc. of 33rd Intl CMG Conf., San Diego, CA, USA, Dec. 2-7, 2007 10] L.  Bulej, T.  Kalibera, P. Tuma, Regression Benchmarking with Simple Middleware Benchmarks,  Proc. of the 2004 IPCCC, 2004 11] I. Cohen, M. Goldszmidt, T. Kelly, J. Symons, J. S. Chase, Correlating instrumentation data to system states: A building block for automated diagnosis and control, Proc. of 6th OSDI, Dec. 2004 12] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly, A. Fox Capturing, indexing, clustering, and retrieving system history Proc. of the 20th ACM Symp. on Operating Systems principles, 2005 13] Z. M. Jiang, A. E. Hassan, G. Hamann, P. Flora, Automated Performance Analysis of Load Tests, Proc. of the 25th ICSM, Sept 09 14] Z. M. Jiang, A. E. Hassan, P. Flora, G. Hamann, Automatic Identification of Load Testing Problems, Proc. of the 24th Intl Conf on Softw. Maintenance, Sept 2008 15] M. Jiang, M. A. Munawar, T.  Reidemeister, P A.S. Ward, Automatic Fault Detection and Diagnosis in Complex Software Systems by Information-Theoretic Monitoring, Proc. DSN, Jun 2009 16] M. Jiang, M. A. Munawar, T. Reidemeister, P. A. S. Ward System Monitoring with Metric-Correlation Models: Problems and Solutions Proc. of the 6th Intl Conf. on Autonomic Computing, 2009 17] T. Kalibera, L. Bulej, P. Tuma, Automated Detection of Performance Regressions: The Mono Experience, 13th MASCOTS, 2005 18]  P. Reynolds, J. L. Wiener, J.C. Mogul, M. K. Aguilera, A. Vahdat WAP5: Black-box Performance Debugging for Wide-Area Systems Proc. of the 15th Intl World Wide Web Conf.s, 2006 19] E. J. Weyuker, F. I. Vokolos, Experience with performance testing of software systems: Issues, an approach, andcase study, IEEE Trans Softw. Eng., 26\(12 20] I. H. Witten, E. Frank, Data Mining: Practical Machine Learning Tools 


and Techniques, Morgan Kaufmann, June 2005 21] M. Y. Chen , E. Kiciman , E. Fratkin , A. Fox , E. Brewer, Pinpoint Problem Determination in Large, Dynamic Internet Services, Proc. of the 2002 Intl Conf. on Dependable Systems and Networks, June  2002 22] M. K. Aguilera , J. C. Mogul , J. L. Wiener , P. Reynolds , A Muthitacharoen, Performance debugging for distributed systems of black boxes, Proc. of the 19th ACM Symp. on Operating systems principles, Oct 2003 41 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


