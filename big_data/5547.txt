Classification rules are the interest of most data miners to summarize the discrimination ability of classes present in data. A classification rule is an assertion which discriminates the concepts of one class from other classes. The most classification rules mining algorithm aims to providing a single solution where multiple 
Department of CSE, United International University, Dhaka, Bangladesh School of Electrical and Computer Engineering, RMIT University, Melbourne, Australia smr@cse.uiu.ac.bd, rokib_kotwal@yahoo.com, x.yu@rmit.edu.au 
 
201 201 
 
Abstract 
Mining Classification Rules via an Apriori Approach S M Monzurur Rahman, Mohamme d Rokibul Alam Kotwal, Xinghuo Yu 
The data mining has been a new and exciting research field receiving an increasing attention.  The first task of data mining is to construct a model that represents a huge data set of interest. One common representation of data is by means of rules.  There are mainly three kinds 
I 
solutions exist. Moreover, it does not guarantee the optimal solution and user has not any control over the classification error rate. In this paper, we addressed these problems inherent in mostly used classification algorithms. A solution has been proposed to solve these problems and it has been tested with experimental data 
Data Mining, Association Rules, Classification Rules, Characteristic Rules, Classification Algorithm, Apriori 
 
 
 
Keywords INTRODUCTION 
The acquisition of classification rules is of interest in this paper [1 f i catio n  is ab o u t  classifying given data into categories or groups which have some common features. Typical examples of mining classification rules include, for example, target mailing, franchises location, credit approval, treatment, appropriateness determination etc [1 f i catio n i s a  well-studied problem in AI and Mining classification rules have been studied since data mining is introduced in [1h ere are s ev eral m o dels f o r clas s i f i ca tion e g   Neural Networks t i cal m odels 6], Gen etic 
 
association rules characteristic rules 
of rules 
classification rules 
and 
models i on trees  8 10], am ong  w h ic h deci sion trees and neural networks appear to be used very often A decision tree is a directed graph consisting of nodes and directed arcs. The nodes frequently correspond to a question or test on an attribute of the given data set. The arcs contain values of attributes. Classes are at the leaf level. The most popular ID3 [9o rithm can be  used for generating decision trees. This algorithm partitions the given data set into smaller and smaller subsets by growing the tree. Decision trees have many advantages over other classifiers such as Neural Networks in that they are easy to construct, relatively fast, easy to be converted into SQL queries and can handle nonnumerical data. The rule extraction using decision trees is simple as well. However, its classification perfor 
mance may be compromised in the situation when the data set has complicated \(nonlinear\ains. Since the mining goal of classification problem is to assign each example in the data set to one of many categories, the decision tree approach has to generate many branches for each node, and may result in large mining errors. On the other hand, Neural Networks can learn the classification rules by many passes over the training data set which takes a longer time for learning. The classification functions generated by Neural Networks are buried in the weights which make the extraction of classification rules difficult as well. Some attempts have been made in this ecis i on fun ctions ca n be  
made of many forms e.g. equations, trees or production rules. Then each class can be classified or partitioned by the decision function associated to it. Efficient functions can be acquired by using, for example, the information theory. The weakness of this approach is that it is unable to deliver all the solutions i.e. how many decision functions can be constructed for each class and which one is best on the basis of compactness. Another problem associated with mostly used classification methods is that they do not allow relaxation and tolerance in the mining process. Consequently these methods may end up with decision functions with larger classification errors. The present paper attempts to address the aforementioned problems. Some new measures such as glob 
al and local measures have been introduced to study classification rules in this paper. Furthermore, it has provided a method to mine classification rules from real data based on well-known data mining algorithm First, we will illustrate common problems associated 
II 
PROBLEM STATEMENT 
apriori 
 
The proposed algorithm is also tested with real data and result is reported in this paper This paper is organized as follows. Section II discusses the data-mining problem with examples and formalizes it. Section III describes our proposed method to mine sets of classification rules from submitted data. The proposed method is tested and results are reported in Section IV. Finally, Section V gives some concluding remarks 
with most currently available classifiers by considering a known animal wo T h ere are th ree clas s e s   Bird, Hunter and Peaceful animals, which are assumed to have been identified already. We choose the mostly used classifier C4.5 from Quinlan a t y pical clas sifier. The result of the classification using C4.5 is shown in Fig. 1. It is found in the experiments that this classifier C4.5  and other as well only gives a single solution for the classification problem as shown in Fig Proceedings of 13th International Conference on Computer and Information\ Technology \(ICCIT 2010 23-25 December, 2010, Dhaka, Ban 
g ladesh 978-1-4244-8494-2/10/$26.00 \2512010 IEEE 388 


classes be a set of 004 i as the set of possible values for attribute 002 1 regardless of whether other solutions may exist. If we look at the data carefully, we can find that there is an alternative solution with the same size of tree as shown in another part of the figure, which cannot be obtained using C4.5. The downside of this single solution may have some consequence. For example, consider a test data of an imaginary animal, which has hooves, 2-legs can fly and also has feather. If we want to determine the class of this animal using the tree from C4.5, we will be given the answer that this animal belongs to Peaceful animal. But we will be more convinced if it is classified as a Bird since it has most characteristics of a bird. In this situation, it may be helpful if the classifier can give multiple solutions to choose from denotes the member index of the class The conventional classification problem formulation is given a large data set or database then it is said that the example is a member of   and  21 denotes the class information and class and non-class and it consists of classification rules of the  form denotes i a n n 004\004 If a new example fires a classification rule and i i adomv III and and   Before proceeding further, we first introduce the following measures, which will be used later and and that support the rule and Definition 1 whose member classification rule is of the form in conjunctive normal form of class MINING CLASSIFICATION RULES USING APRIORI APPROACH Fig. 1. Decisions tree and function for animal classifier produced and does not produce by C4.5 Another limitation is that the classifiers such as C4.5 do not allow enough relaxation in producing rules. This limitation restricts the classification ability of most classifiers as real data sets do contain noise and fuzziness and certain relaxation may help to produce more convincing rules. For example, some members of one class may have overlaps with members of other classes. This may incur a classification error during constructing the classification model. This problem has been addressed to some extent in the existing classifiers, for example C4.5 tries to keep this error as low as possible. When classification is not well defined by data given to C4.5 C4.5 constructs simple trees at the cost of complying an amount of classification. Note that C4.5 works according to Information Theory in the construction of the root node to leaf nodes and users have no influence in guiding the construction. Obviously it will be more desirable if users can have certain control in order to obtain their desired results. To demonstrate this point, let us consider again the animal world data. If a user agrees to accept the classification error of 1%, a good classification solution may be given as follows, in comparison with the solutions in Fig. 1 Decision Function 3 IF Animal Size_Is \(Small\HEN Class = Bird ELSE IF Animal Has \(Hooves\hen Class = Peaceful IF Animal Has_Not \(Hooves\Then Class = Hunter ENDIF We now look at the problem formulation. The classification problem can be considered as finding the set of classification rules that partition the given data into disjoint sets or groups. The problem of inferring classification rules from a set of given examples \(data\as been formalized in many literature  T h e f o r m ulation ca n  be stated as follows Let th attribute in the class  and The mining task is to derive a set of classification rules for each class as  It is obvious that the member classification rules in is the AND operator. Support of a classification rule 006 002 002 003 from given data with decidability and classification accuracy   be a set of For example, from the data we have 0.1 7 th member subset   rule sets e.g C m m CCC A n n aaa i a D n n vvv R ki i Caar l nimk j lj n li R ii r C jj r C j i ji r r i r i C i j i j i G j i i G k j i Gaa nji j i i G i C l k l i i a C i k C i r k C k D k i k ki N n Drs i k n k C i r k N k C i r k N k C  002  006\004 n and 389 7  tuple of the form the number of examples in 003\003 1 004 then it is said that this example is a member of tuple classification rule is expressed in the form of 002 002 is rule subset. For example, for the attributes each example of which is a Rule Class Support: An   1   where 002  where 004 002 002 are disjoint, i.e. if  bird DBird feather Hass jimk 004  We will explain these two terms in the following sections.  If a new example fires a classification rule from  Obtain an appropriate set of classification rules  then  where    005 005 006 007 b t 007 002 003\003 partition data between the i adom 21 Define  respectively. The membership of set where and that support the rule  21 where  1 where is defined in sub example space class. The problem is to obtain an appropriate set of classification rules, denoted as is the number of examples in the number of examples in 


 an d th e res u lti n g p redicates  w ould be of th e form 1  where For example, from the data, consider the classification rule set foe the class bird as Rule Set Class Support: Class support of a rule set is defined as  1 DBird feather Hass DBirdflyDoess 212 Bird DBirdflyDoess j k R j i j v i v is flydoes medium sizeR Bird is as All is the is the number of examples of all classes in D. For example, from the data we have 44.0 16 7  its rule set class confidence is then calculated as 6875.0\6875.0,8125.0 1 for Definition 2 Definition 4 Definition 5 Definition 6 Defenition 3 It is obvious a higher value of is the number of examples of all class Before using our mining algorithm, data needs to be transformed into an appropriate format so that it can be processed. This transformation can be done by constructing logic predicates from attributes, which may be continuous or discrete. We pr oduce rules in logical predicates from attributes and then truth-values of these  212 is the total number of examples of all classes. For example in Table I 0 100.1 1 where n is the total number of tuples in the set, r is the cardinality of the rule set 1 Here 002 002  th discrete value of attribute Continuous attribute values can be discredited via the existing approaches such as 003  327  supporting the rule and and In our algorithm, we will allow users to control the accuracy of the classification rule mining solution. There may be two types of errors: one is the indecisive error due to less rules set support, the other the misclassification error due to rule set confidence. If the rule set confidence is chosen too high, obviously the classification error will be small. However this may result in no solution at all. On the other hand, keeping the class rule set support we can get the closer picture of the class in terms of rules but it declines the prediction accuracy as discussed in the previous section. The other notable feature of our solution is that a rule cosseting of same logical predicates may appear for different classes. It does not mean that they do not have any discrimination power. They will have different confidence measure and from that we can have the idea it is more likely to be appeared in which class and at fires the rule and and    and   where   and  Table I. Classification Example the number of tuples classified by rule As in our case we are able to generate different set of classification rules, so we can find out the best set i.e. compact with that measure In the following we shall develop our algorithm for mining classification rules. This algorithm consists of three parts where where 1 are discrete values found, for example from 002 002 1 where r 004 Compactness of Classification Rule Sets The compactness of a classification rule set can be measured as follows   8125.0 16 3 0.1 th classification rule set of class   l i i d i l i i k j k drsMaxDRS is the subset of examples of the class 005 as well as are 002 Pattern Construction  212  002 BirdBird medium Sizec  1 BirdRS bird  002 002    85.0 7 6  002 BirdRS bird  i r k C N n Drs i k i i k n k C i N j k C j k i r R k l i i d D i d D k i d N n Drc ki ki ki n k DD k D i r N li j k i r R i t i i C i E r i C m j i i a v j i v j i a j i i j i vav      huntdoes smallsizeR Bird Their rule set class support is then 0.1 1 1  1  huntdoes medium size R Bird Rule Set Class Confidence: The rule set class confidence is defined as  Chi-merge Mining classification rules Construction of Classifications Rule Sets  D excluding Chi Chi the number of conditions in the antecedent part of rule  r i i i Cn t r E 3.1 Pattern Construction 002 013   f   002   38.0 16 6  mutually exclusive. For example from the data we have two classification rule sets for the class bird as follows bird\\(bird 1 bird\\(bird 2 85.0 7 06  2 Rule Class Confidence: The rule class confidence is defined as bird\\(bird 1       1 and 002 Min RC bird Rule Class Global Support: A gklobal support of classification rule in the rule set, and krcMinRC i j k of class indicates the goodness of the class ruleset. In the algorithm to be developed below, we will minimize the value of 2. After that, all patterns will be further constructed with two parts the 390 An example can only be considered once in the rule level support calculation i.e 002 attributes along with the class information to make the so-called patterns for next step of processing.  If the attribute is discrete then the predicate is constructed directly as  003 002 BirdBird swimDoes c is the number of examples from examples of 


and predicate set predicate and class information, from all examples. We now illustrate the above idea via examples given in Table 1. From Table 1, we have three attributes Age, Salary and Sex where Age and Salary are continuous but Sex is discrete.  Arbitrarily we discretize Age and Salary in three ranges, that is  40[\40,30[\30 empty 2.Attribute normalization starts at here. For all attributes 1.Set 1 Salary p MaleSexP MaleSexP J J JpJp ppp Jpp ppp p Jppp Jppp JpppJppp JpppJppp 002 Construct is ordinal then discretize its domain using Chi2 meth d g e t it s dis c rete ra n g e s De f i n e  ranges as is the number of ranges of ith attribute  017    Jpp 1 73 P P 002   1   1,0.75 1   1.0,1.0 1   25  0  0   1 8 7 6 5  If we set the minimum rule class support to be larger than zero and minimum rule class confidence level to be 0.5, we then have the classification rules for 1-itemset rules as  1 2 1 3 1 7 6   75  0  33  0   1 73  After 2-itemset rules we have still the three groups of conditions  632  1  , 1 7 6  1 73 1 1 763 762 p J  The classification rules from 2-itemset is 1 1 1 63 72 62 Jpp 017 017 item classification rules n 002 017  1 002 002 predicates in and   7    In this example we have three classes which are denoted as Programmer=C1, Secretary=C2 and Accountant=C3. The resulting corresponding patterns are given in Table II Table II. Classification rules produced by C4.5 and X2r rule generator The algorithm for normalizing attributes is given below from three attributes having a support higher than 0. We now create 3-itemset rules as  1  33  0   1  1  66  0   1 763 762  40 3  3000  4  5000 3000 5 and 5000  6  The other two predicates for Sex are constructed as   7 and   8  Next, we can construct 2-itemset rules from the three groups of conditions   632 and  from three attributes having a support higher than 0 in the data set and they are  1  1   1  1  1   1  1  1   1 63 72 62  The next step is to mine 4-itemset rules and as we have only three group of conditions we have no rule in the 4itemset rules and hence it concludes the iteration. We now summarize the final result of classification rules for the programmer is 1 2 logical predicates of the form  1 and add them to the predicate set where and the classification rules are mined as  1 1 763 762 JppJppJpp r 212\r  We can construct sets of classification rules from the above result as follows considering class set support 0.95 and confidence 0.75 by combining one rule with other rules as long as the criteria is not met. The sets are 391 002 002 JpJp Age p 002 JppJppJpp j 017 Jp Jp Jp Jp r r\212   Agep p    Salary p r 212\r 002 002 002 002 002 002 002 002 002 002 002 002 to the examples and get their truth-value \(1 or 0\ordingly b.Get the class member index of the example c.Construct the pattern for the example by concatenating   and add them to the predicate set  30 2     016 017 017\017 017\017 017\017 017\017 017\017 017\017 002 002 002 002  017 017 1 002 002 002 002 002\002 002 is discrete then find its discrete values and set them as 40 002 002 1 do the following steps a.If 1 Construct 017 017 attributes. The idea our algorithm is as follows. We start from 1-item classification rules \(one condition in antecedent part\irst, and then graduatelly to reach for all examples in the data set do the following steps a.Apply all predicates in Salary p  002 p 017 JppJppJpp               1 1 1 1 12 11 1 1 1 and construct the predicates as  30 1 logical predicates of the form truth-values of predicates to the class member index e.g 1\item classification rules. The rule of thumb is the present set of rules must have all of its subset rules in the previous iteration and there should not be conflict conditions for the same attribute in any rule as conditions of an attribute are disjoint We use the data in Table I and II to explain it. In order to mine classification rules for C1 \(Programmer class the 1-itemset-classification rules with their associated rule class support and rule class confidence levels can be obtained as  1   0.33,0.5 1    0.66,0.75 1   75  0  0   1 4 3 2 1 017 002 conditions in antecedent part\he novelty of our method is each classification rule is derived using the aprio ri algorithm with their rule class support and rule class confidence levels given This will enable us to discriminate those which are unlikely to get the desirable level of support and confidence of all classes. The main idea of the apriori algorithm lies in constructing n-item classification rules from 002 is the number of attributes and Jp Jp Jp Jp p 1 1 1 63 72 62 where 1 0 002 002 002 002 P n aa i a i l i i v v i l jva j i i i a i l j i i j i vav i lj p n P P p classvavvavvavvav m n m n m m n m m n m m i n n n n n Age i l i i v v  5000[\5000,3000[\3000  b.If  3.Pattern construction starts at here. Given We now mine classification rules from constructed patterns obtained in a\using the popular data mining algorithm s s u m e t h at pattern s h a v e u p to 3.2 Mining Classification Rules 


of class is empty I.Increment which has user defined class rule set support The class rule set mset rules as Apriori algorithm as well as taking account that at most one condition may appear in a rule from each attribute II.Calculate rule class support, global support and rule class confidence of all rules found in step I III.Remove those rules which has class support lower than user defined support from  21 and some rules from which are at least present once in the example set of class and do the followings until and lets say they are and construct IV  e t 2   1 7 of the form but does not obey any other rule from 1, 0.75  1 Set 5  1 763 762   Any set of the above has the discrimination ability to partition examples of the programmer from the rest. The algorithm for mining classification rules his given as below. The algorithm for Mining Classification Rule sets is given as below Input: Examples of IRIS data consists of three classes k then we conclude that the test pattern belong to the class  1 72  of each class gives us the whole picture of the class and we can partition the entire data set by has the higher confidence than all rules fired from taking account of different stop criteria. As this paper is concerned about extracting classification rules from discretized attributes, details of the discretization algorithm used in this experiment is omitted here. However, any discretization algorithm can be used. The predicates for classification rules were constructed as shown in Table IV. Three experiments were set with high value for user defined class error and lower value for support.  The set support was kept the same for two experiments. The first experiment gave few rules as shown in Table V, VI and VII but with strong class support and confidence. We constructed the sets of rules from these rules and every set had the discrimination power to partition data from the associated class to the rest. The second experiment was conducted with less support and confidence value and it produced many rules and sets which have also the discrimination power It should be noted that our result provides multiple solutions without any default rule Table IV. Classification rules produced proposed method with the parameters Min class support 80%, Min class confidence 99% \(experiment 1 p J 002 j j from for from 017 017 017\017 017\017 n n n n n n p J m m j j j m j j j bbbB B j i j b g Bb j i j G k F k k k F k F j G j i j G j i j i j i kj i k j i kj i k j i k i itemset rules as supersets of 002 002 002 002 Jpp Jpp JpppJppp n \006 n \006 002 \016 1 0.75   1 Set 3  62 classes, user defined class confidence and user defined class rule support and set support Output: Number of rule sets obtained in examples for each class Steps For class index  to F To study the performance of the proposed method for mining classification rules, we used the IRIS data set in 16  I t ha s fo ur n u m e r ic c o nti nuo us a ttr ib ute s  It has 150 examples in total and each class has 50 examples.  Table III contains some available results about the data in [17 as w e ll a s t h e res u lt ab o u t  class support and classification error added. It should be noted that only single solution was given in [17  m o r e over, the introduced default rules do not present the discrimination characteristics of the class Table III. Intervals of attributes for IRIS data to the solution set which has class confidence and support equal or higher than user defined class confidence and support do the followings 1.Find the distinct attributes of class  5.Set 1 The identification of an example pattern is straightforward. If the pattern obeys any rule from Now we report the results using our method. First, each attribute was discretized. We used a method similar to EXPERIMENT RESULTS F sepal-width, petal-length and petal-width     2.Construct all 1-item rules 1  3.Calculate rule class support, global support and rule class confidence of all rules found in step \(b 4.Add all classification rules of 1  IV.Add all classification rules of to the solution set k-1 sepallength 6 1 Set 1  n Setosa, versicolor and virginica  002 002 002 Chi2  then it is obvious that the pattern is belong to the class j.  Since we are considering control parameters class confidence and support, the test pattern has the chance to fire some rules from  In this case we will consider the rule level confidence to determine the membership of the test pattern and if any rule fired by the test pattern from  Table V. Classification rule sets from experiment 1 Min set support 95%, Min set confidence 99 392 which has class confidence equal or higher than user defined class confidence 6.Finally, construct all rule sets 


Table VI. Intervals Classification rules produced proposed method with the parameters Min class support 50%, Min class confidence 90% \(experiment 2 Table VII. Classification rule sets produced from experiment 2 Min set support 95%, Min set confidence 90 V The classification problem in the context of data mining should be solved taking account that it should provide as many as solutions possible by meeting user-defined criteria. The more solutions means more classification rule sets which increase the discrimination, prediction and characteristics exhibition of classes found in the data set.  In the paper we have demonstrated multiple solutions construction with our algorithm to mine classification rules. Our algorithm can be tuned with many user-defined controls such as intervals, class support class confidence, set support, set confidence etc. It results different solutions as well as it also guarantees the discrimination ability. Our method has been tested with widely used Iris data set for testing machine learning algorithms. The result has been reported which shows the effectiveness of our proposal in the classification rule-mining field. We would like to do more research at this field in near future   REFERENCES 1 CONCLUSION Agrawal, R., Imielinski T., and Swami A., \215Database Mining: A Performance Perspective\216, IEEE Trans. Knowledge and data Eng., vol. 5, no. 6, Dec 1993 2 Weiss S. M., and Kuilikowski S. M., \215Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems\216, Morgan Kaufman 1991 3 Chou P. A., \215Application of Information Theory to Pattern Recognition and the Design of Decision Trees and trellises\216, Ph.D. Thesis, Stanford University, California, June 1988 4 Lippmann R. P., \215An introduction to Computing with Neural Nets\216, IEEE ASSP Magazine, PP 4-22 April 1987 5 Lubinsky D. J., \215Discovery from databases: A Review of AI and Statistical Techniques\216, IJCAI-89 Workshop on Knowledge Discover in Databases pp. 204-218, Detroit, August 1989 6 James M., \215Classification Algorithms\216, Wiley 1985 7 Goldberg D. E., \215Genetic Algorithms in Search Optimization and Machine learning\216, Morgan Kaufmann, 1989 8 Breiman L., Friedman J. H., Olshen R. A., and Stone C. J., \215Classification and Regression Trees\216 Wadsworth, Belmont, 1984 9 Quinlan J. R., \215Induction of Decision Trees\216, Machine Learning, 1:81-106,1986 10 Quinlan J. R., \215C4.5 Programs for Machine Learning\216, Morgan Kaufman, 1993 11 Lu, H., Setiono, R., and Liu, H., \215Effective data mining\216, IEEE Transactions on Knowledge Discovery and Data Engineering, Vol. 8, No. 6, pp 957-961, December 1996 12 Kohonen T., \215Self-Organizing Maps\216, pp 116 Springer Series in Information Sciences, 1995 13 Agrawal R., et al., \215An Interval Classifier for Database Mining Applications\216, Proceedings of the 18th VLDB Conference Vancouver, British Columbia Canada, 1992 14 Yen S. J., Chen A., \215An Efficient Algorithm Deriving Compact Rules From Databases\216, In Proceedings of the Fourth International Conference on Database Systems for Advanced Applications, Singapore, April, 1985 15 Liu, H., and Setiono, R., \215Discretization of Ordinal Attributes and Feature Selection\216, Publication from Department of Information Systems and Computer Science, National University, Singapore 16 Public domain IRIS data set at http://www.ncc.up.pt/liacc/ML/statlog/datasets.htm 17 Liu, H., and Setiono, R., \215Dimension Reduction via Discretization\216, Publication from Department of Information Systems and Computer Science, National University, Singapore 393 


Capacity for analysis and synthesisé, çResearch skills Capacity to learn 4   Interpersonal skills Leadershipé, çInterpersonal skillsé, çAbility to communicate with non-expertsé, çOral and written communications in native languageé, çCritical and self-critical capabilityé, çTeamworking 5   Personal skills Ability to work autonomously Problem solvingé, çCapacity to adapt to new situations Knowledge of a second languageé, çConcern for qualityé, çWill to succeedé, çElementary computing skillsé, çCapacity for applying knowledge in practice Decision makingé, çProject design and management Information management skills Table 7 shows the mean importance of each gr oup of competences by stakeholder group for the First Cycle Degree respondents. For the employers the personal skills are the most importance group followed by professional skills and interpersonal skills. The table confirms that employers rank internationalisation and entrepreneurship lowest. Academics agree with the order of the competence groups but rate all of them more strongly important than employers. Students also rank the competence groups in the same order but rate internationalisation slightly higher than employers and entrepreneurship very slightly lower Analysis of the level of development of the competenceas shows that there is good agreement that Professional skills are the best developed of the skill groups followed by Personal skills and Interpersonal skills. As with importance the Entrepreneurship and Internationalisation skill groups are developed the least. There is clearly a difference between the rated importance and level of development of these groups in that the Personal skills group is rated most important but the Professional skills group is developed the most. Other than that the general structure of the supply demand balance of the generic competences is quite well aligned V  CONCLUSIONS AND RECOMMENDATIONS  The objective of this study was to apply the Tuning Methodology to the EIE discipline set to test the alignment between the views of the importance and level of development of sets of competences between students, academics and employers. The results of this study show that the Tuning Methodology is a useful tool for assessing alignment in these subjects  TABLE VII  MEAN IMPORTANCE OF EACH GROUP BY STAKEHOLDER  Competence Group Academic Employer Student Internationalization 2.86 2.63 2.79 Entrepreneurship 3.07 3.05 3.02 Interpersonal skills 3.23 3.09 3.06 Professional skills 3.36 3.29 3.13 Personal skills 3.44 3.30 3.29  In total 3,275 questionnaires have been collected from the stakeholder groups from a range of European countries. The number of responses from each c ountry is variable and a full by country analysis is not possible with the responses currently available, that said a range of analyses have been carried out Tests of the homogeneity of the responses across all countries show that there are country differences in some analyses and some of these are explored, others merit further investigation. Many of the analyses presented in this paper are aggregated results and therefore potentially suffer clustering problems. This too is a topic of further investigation The following is a summary of the key conclusions drawn from the analyses presented Consistent top of importance of the generic competences for all stakeholders is çProblem solvingé. Second in the ranking for students is çElementary computing skillsé. Comparatively students under rate the importance of this skill, perhaps it is taken for granted in students than in academics and employers The results show employers value it more than students and this message could be communicated to students A number of gaps exist between the importance and level of development between the stakeholders. The largest gap is Knowledge of a second languageé and the evidence from the languages section suggests this view is strongly aimed at English The generic competences group into 5 sets with çPersonal skillsé rated consistently as the most important set. This is followed, in descending order of importance, by çProfessional skillsé, çInterpersonal skillsé, çInternationalisationé and Entrepreneurshipé. The smallest mean çInternationalisation is just over midway between çweaké and çconsiderableé. Given the European Unionês desire to see greater student and employee mobility across Europe, it is clear there is scope for scope for improvement in the value placed in this skill set by curriculum designers Curriculum designers and academics can take comfort in the finding that çProfessional skillsé are the best developed of the skill gr oups followed by çPersonal sk illsé and Interpersonal skillsé. This not only aligns with the views of employers but aligns with anecdotal evidence on the real purpose of EIE education programmes. That said there is a trend in a number of countries across Europe away from large firm employment towards a Small to Medium Sized Enterprise culture. Curriculum designers may wish to reflect on the fact that entrepreneurial skills are very low in the list and perhaps merit more attention and emphasis in the curricula In general the different stakeholders rate the importance and level of development on average differently. This difference has been taken into account in the conclusions drawn. The general unevenness in ranking reflects different perspectives and is, in itself not considered a major issue, of concern are the relative positions of competences and the relative gaps. In general and even allowing for this employers and academics tend to rate competences higher in importance than students and graduates a number of specific instances of differences are drawn out in section 8 73 


After the text edit has been completed, the paper is ready for the template. Duplicate the template file by using the Save As command, and use the naming convention prescribed by your conference for the name of your paper. In this newly created file, highlight all of the contents and import your prepared text file. You are now ready to style your paper; use the scroll down window on the left of the MS Word Formatting toolbar A CKNOWLEDGMENT  The authors acknowledge the funding provided by the European Union for this project and to the members of the project team for help in collecting questionnaires R EFERENCES   1  European Association for Education in Electrical and Information Enginering http://www.eaeeie.org  2  EIE-Surveyor project http://www.eie-surveyor.org  3  Tuning Report http://tuning.unideusto.org/tuningeu/index.php?option=content&task=vi ew&id=172&Itemid=205  4  Overview of the Bologna Process Ö Implementation in Europe in Electrical and Informaiton Engineetringé, ISBN 2-9516740-3-1, pp 542  74 


              


   


                        





