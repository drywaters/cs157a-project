U sing the Real Gentle and Modest AdaBoost Learning Algorithms to Investigate the Computerised Associations between Coronal Mass Ejections and Filaments R Qahwaji M AI-Omari T Colak and S Ipson School of Infonnatics Department of Electronic Imaging and Media Communications University of Bradford Bradford BD7 IDP UK e-mails:r.s.r.qahwaji@brad.ac.uk.m.h.al-omari@brad.ac.uk.t.colak@brad.\ac.uk.s.s.ipson@brad.ac.uk 
AbstractSpace weather forecasting is a very challenging task and 
investigating the associations between properties i.e shape scale location of the related solar features appearing in solar images are usually complicated because of the variation in their physical and visual properties Establishing the correlations among the occurrences of solar activities and solar features is a long-standing problem in solar imaging This work is an attempt to shed more light on the driving forces behind the initiations of Coronal Mass Ejections CMEs This 
is still a big mystery in this field and in this work we have analysed years of data relating to one particular feature filaments to determine if an association between filaments and the eruptions of CMEs can be drawn The resulting association set has been fed to a powerful machine learning algorithm to determine 
CMEs can be predicted solely based on filaments Our learning algorithm AdaBoost is used because of robust 
if 
and accurate performance Three of the most common versions of the Adaboost algorithm are used in this work which are the Gentle AdaBoost the Real AdaBoost and the Modest AdaBoost Index Terms-Data 
mining machine learning information retrieval AdaBoost Solar Imaging space weather I Introduction The term space weather refers to conditions on the Sun in the solar wind and in the Earth's magnetosphere ionosphere and thermosphere that may affect space-borne 
or ground-based technological systems and endanger human health or life 1 The importance of space weather is increasing as more human activities take place in space and as more reliance is placed on communications and power systems Solar flares and Coronal Mass Ejections CMEs are the most dramatic solar events affecting the terrestrial environment The Earth environment and geomagnetic activity are affected by the ionized solar plasma known as the solar wind Solar wind flows outward from the sun to form the 
heliosphere and it is affected by solar activity and carries with it the magnetic field of the Sun 2 This interplanetary magnetic field IMF creates magnetic storms by injecting plasma into the Earth's magnetosphere 3 4 Geomagnetic storms are correlated with CMEs 5 and predicting CMEs can be useful to forecasting space weather 6 Previous CMEs research 7-9 reported that most CME events are associated to some extent with eruptive filaments/prominences and/or solar flares Currently five major 
CME eruptions models exist 10-13 These are the Thermal Blast the Dynamo the Mass Loading the Tether Release and the Tether Straining Models The last three are storage and release type models where a slow build-up of magnetic stress occurs before eruption begins 14 The model which is directly related to this work is the Mass loading model This can be manifested during the pre\255 eruption phase of a CME in the fonn of a growing quiescent or eruptive filament Mass loading can 
be associated with prominences which are extremely dense contained in a compact volume and of chromospheric temperature Prominences are thought to play a major role in CME eruptions because of their coincident starting according to observations reported in 15 16 And the mass of the prominence is considered as a crucial criterion for CMEs eruptions 17 18 The exact degree of association is not clear because most of the available studies have been 
MIC-CCA 2008 Page 37 


example of an associated CME-Filament pair is depicted in Figure 1 filament and Fi re 2 CME Figure 1 H-alpha 6563 A full disk image taken on 19 Jul 2001 at 09:49 UT showing a filament with centroid located in S20W59 Figure 2 LASCO C2 image taken on 19 Jul 2001 at 10:30 UT showing a Partial Halo CME with central position angle of 275\260 The work presented here is an extension of previous work described in references 21-23 which designed computer vision systems to analyse the associations between solar flares and sunspots and also between flares and CMEs This work has been extended by carrying out a larger-scale analysis of associations for years of solar data contained in CMEs and filaments catalogues In addition the knowledge extraction introduced in this paper uses the AdaBoost algorithm for the automated prediction of CMEs based on features extracted from filament catalogues The data used in this study are from the publicly available NGDC filament catalogue 24 and the SOHOILASCO CME catalogue 25 This paper is organised as follows Section 2 explores the AdaBoost algorithm Section 3 describes the creation of our training set and our Computer Platfonn for CME Prediction The practical implementation and evaluation of the learning system is discussed in Section 4 Concluding remarks and recommendations for future work are presented in Section 5 II Adaptive Boosting Algorithm In some real-time situations experts identify rules of thumb that they can use to predict the outcomes of certain situations on the basis of their accumulated experience To maximise the advantages associated with the use of these rules of thumb they need to be represented using computerised rules However this process faces two problems Firstly how to choose the collections of data presented to the experts in order to extract a representative set of rules of thumb Secondly how to combine the selected rules of thumb into a single and accurate prediction rule The principle of boosting could provide a solution to the problems highlighted above According to 26 Yi  Boosting  m  Ym where each x represents the associated class which is usually refers to a general and provably effective method of producing a very accurate prediction rule by combining rough and moderately inaccurate rules of thumb in a manner similar to that suggested above Boosting enhances the perfonnance of a weak learning algorithm which perfonns slightly better than random guessing into an arbitrary strong learning algorithm The AdaBoost procedure short for Adaptive Boosting algorithm was introduced in reference 26 and a pseudo code description is shown in Figure 3 As shown in this figure the algorithm takes as input a training set carried out on only a few years of data or on limited cases and using physics-based modelling In some cases contradictory findings have been reported For example not all researchers agree that strong relationships exist between CMEs and filament/prominence eruptions In reference 19 for example it was reported that the association rate was about only 30 On the other hand it was reported in 20 that more than 94 of the CMEs studies were associated with eruptive prominences/filaments This means that the exact degree of association is one of the long standing uncertainties in solar research and it is the aim of this paper to reduce this uncertainty Xl Yl represents a vector of extracted features from the input space and each label MIC-CCA 2008 Page 38 An Xi 


Xt filaments and 313 60 The training starts with all the weights set equally On each round the weights of incorrectly classified examples are increased to force the weak learner to focus on the hard examples in the training set The error is measured with respect to the distribution addition the Central Position this work all the CME and filament data in the period from January 1996 until the end of December 2001 were processed analyzing data relating to 5449 CMEs and 7332 eruptive filaments/prominences By applying the association algorithm an associated data set consisting of 522 filaments with 209 40 this work we introduce a computer platform that analyses all the available years of data in the filament and CME catalogues and defines learning rules to provide automated predictions for CMEs A C platfonn was created to automatically associate CMEs in the SORa/LASCO catalogue with filaments in the NGDC solar filaments catalogue The association is determined based on timing and location information the date and time for every CME is compared with the date and time for every filament is a normalization factor Output the final hypothesis 1 vith error Xi H\(x The error associated with vhere t U d  D   T  NA appropriate for represents the weight of this distribution on training example Fort=O,\267\267\267,T E tn on which the weak learner was trained The weak learner finds a weak hypothesis        ht\(Xi 1 Et  NA Yl Dt\(i i:ht Xi EX I Yl 267fh either lor 1 As described in 26 a weak or base learning algorithm is called repeatedly in a series of rounds A ht:X In l Ot In In t+1 is found as follows III.Association Algorithms and the Prediction System filaments was created Properties such as start time end time location type and extent of the Figure 4 Location-based CME-Filament association Using these criteria we automatically associate the filament of Figure 1 and the CME of Figure 2 as shown in Figure 5 After finding all the associations a numerical dataset was created for the learning algorithms using all the associated and not-associated filaments Angle CPA for every CME is compared with the angular polar coordinate of the centroid of every filament An associated filament is labelled t Z AdaBoost maintains a distribution or set of weights over the training set Dt\(i D\(i as explained below 225 If a CME is not recorded within 60 minutes before or after the time a filament disappears then this filament is labelled PrirvDi tel t if Y MIG-GGA 2008 Page 39 267Train veak learner using distribution t 225 t Zt A 27 D D D h 1    I 1 ft  Yi Figure 3 The AdaBoost algorithm CMEsinthis region are not associated with the shown filament Figure 5 CME-Filament association example  1,+1 Initialize D 1  267Get weak hypothesis 267Choose X m  Yrn Yi eOt Yi t sign\(L,;=l Otht\(x t 225 vhere NA 225 Otherwise if the CPA for the recorded CME is located within 26130\260 of the centroid of the filament as shown in Figure 4 then this filament is labelled E  pate X Given:\(Xl A t i h t and a not-associated filament is labelled and round  ht\(Xi 


filaments can be extracted from the NGDC filaments catalogue We wanted to include more features but unfortunately a large number of the associated filaments do not have location details given for both ends As it was not clear which properties are more important for machine learning and for the prediction of CMEs it was decided to carry out extensive experiments in order to detennine the significance of each property for this application The properties selected are shown in Table 1 Table 1 Groups of properties that are used as input nodes in the AdaBoost algorithm Group Inputs 4 Timing duration type extent 3 Timing duration type 2 Timing duration 3a Timing duration extent 3b Timing type extent 2a Timing type 2b Timing extent IV Practical Implementation and Results 0.42593 0.53704 0.42593 TN 0.74342 0.88158 0.73684 Acc 0.67308 0.70769 0.66923 Spec 0.74342 0.88158 0.73684 Sense 0.57407 0.46296 0.57407 The TP and FP values obtained from all the experiments on the three types of AdaBoost algorithm are plotted in Figure 6 and are used to detennine the input group providing the best perfonnance The best perfonnance shown in a ROC curve is the one furthest away from the random guessing diagonal line in the northwest The CME prediction perfonnance was evaluated using the Receiver Operating Characteristic ROC analysis technique 33 Intensive training totalling 5 learning experiments with 1000 iterations for each experiment were carried out for the optimum group of input features which is proven to be three in reference 34 Each experiment involved training on a random set of associated sets followed by a testing phase on the remaining sets As explained previously these sets were obtained using the Jack-Knife technique and were quite different for each experiment The ROC perfonnance indicators were found for every experiment These indicators are TP FP FN TN accuracy specificity and sensitivity all calculated as explained in reference 33 The averages of these indicators over all the experiments and for the three AdaBoost algorithms are shown in Table 2 Table 2 Average ROC performance indicators for different input combinations Real Gentle Modedst Algorithm AdaBoost AdaBoost AdaBoost TP 0.57407 0.46296 0.57407 FP 0.25658 0.11842 0.26316 For this work we have implemented three different boosting algorithms Real AdaBoost Gentle AdaBoost and Modest AdaBoost Real AdaBoost is the boosting algorithm reported in 28 which is a generalisation of the basic AdaBoost algorithm introduced by 29 The Gentle AdaBoost introduced in 30 is a more robust and stable version of the Real AdaBoost algorithm and perfonns slightly better than the latter on regular data and considerably better on noisy data 30 Modest AdaBoost described in 31 can provide better generalization capability and higher resistance to over fitting compared to the alternative AdaBoosts In addition Modest AdaBoost in certain cases can provide good perfonnance in tenns of test error The three AdaBoost algorithms have been implemented using the GML AdaBoost matlab toolbox which is publically available at http://research.raphiconru achine-Iearning gml\255 adaboost-matlab-toolbox.html All the machine leaming!training and testing experiments have been carried out with the aid of the Jack-knife technique 32 This technique is used to provide a correct statistical evaluation of the perfonnance of a classifier when implemented on a limited number of samples The technique divides the total number of samples into two random sets a training set and a testing set 32 In this work all the experiments were carried out using 80 randomly selected samples for training and the remaining 20 for testing MIG-GGA 2008 Page 40 FN IJJ:A The Learning Algorithm IJJ:B The Practical Implementation 


In this work we provide an objective and computerised representation of the degree of association between solar filaments and CMEs based on historical solar data and represent it using learning rules This work has the potential to be used to measure the accuracy of the existing CMEs models which exhibit high degrees of conflict It can shed more light on the pre-conditions associated with CMEs eruptions The creation of automated and reliable prediction systems that can predict the extremes of space weather depends on the creation of machine learning-based representations of these associations this work we are investigating the mass loading model of CME eruptions which assumes that filaments are associated with this phenomena Hence in this work we only focus on the one factor to study its significance for CME eruptions using machine learning The resulting prediction rate will enable us to measure the significance and accuracy of this model As shown in Table 2 the Adaboost algorithms provides good accuracy compared to our previous study with Radial Basis Functions 34 but still not very high values because of the physical nature of CMEs as explained above Compared to Radial Basis Functions they provide lower TP rates but also higher TN rates The best prediction performance measured in terms of the accuracy of predictions is 70.8 and is provided by Gentle AdaBoost However the high accuracy value is caused mainly by the high TN value which is 88.1  not by the TP value which is 46.3 This TP value is very low and even a random guesser would perfonn better than this Hence a major conclusion of this study is that the Gentle AdaBoost can provide reliable performance if used as a rejection classifier to predict when CMEs are not likely to occur It is less efficient if used as a positive classifier tool The Real and Modest AdaBoosts provide higher TP rates but their accuracy and TN rates are lower It is worth noting that the TP rates provided by the Real and Modest AdaBoosts are not as high as the TP rates provided by Radial Basis Functions used in our previous work 34 Hence it is not very feasible to use them within the context of this problem V Conclusions and Future Research As indicated in the introduction the prediction of CMEs is a long standing problem in solar physics It is well agreed that CMEs eruptions are associated with other phenomena such as filaments and flares the near future we will investigate the association of CMEs with flares and compare our findings with the reported associations with filaments VI References 1 H Koskinen E Tanskanen R Pirjola A Pulkkinen C Dyer D Rodgers P Cannon J.-C Mandeville and D.Boscher Space Weather Effects Catalogue FMI QinetiQ RAL Consortium 27 2001 2 M Pick C Lathui llere  and J Lilensten Ground Based Measurements Alcatel-LPCE Consortium 2001 3 L S Yevlashin and Y P Maltsev Relation between Coronal Mass Ejections Solar Flares Certain Parameters of the Magnetosphere and Different Auroras during Great Magnetic Stonns 225\225\225 225\225     0.4 Discussion of Results   1 1 1 C In In In    0.25 OJ Figure 6 The ROC graph for the AdaBoost learning experiments 225\225 225  0.5  IV      035    Geomagnetism and Aeronomy this paper we propose a novel machine\255 learning based system that analyses the available data in the NGDC filament catalogues and the SOHO/LASCO CME catalogues Our system associates CMEs with filaments and represents these associations numerically in training vectors which are input to the Real Gentle and Modest AdaBoost learning algorithms The AdaBoost algorithms are used because of their advantages highlighted in Section 2 One of the maj or findings of this work is that the Gentle AdaBoost algorithm can provide accurate performance if used as a rejection classifier to predict when a CME is not likely to occur vol 43,pp.291-297,2003 direction 33 This was used to detennine the optimal values shown in Table 2 o 55 MIG-GGA 2008 Page 41 


vol 110 2005 10 J A Klimchuk Theory of Coronal Mass Ejections Space Weather Geophysical Monograph 125 ed P Song H Singer G Siscoe Washington Am Geophys Un vol 125 p 143,2001 11 B C Low Magnetic Energy and Helicity in Open Systems in vol 248 pp 471 483 2008 24 NGDC The National Geophysical Data Centre flo ftp.ngdc.noaa.gov/STP/SOLAR OAT A 2007 25 SOHO/LASCO SOHO LASCO CME Catalogue,''http://cdaw.gsfc.nasa.gov/CME list!,2007 26 Y Freund and R E Schapire A decision-theoretic generalization of on-line learning and an application to boosting vol 37 pp 297-336 1999 29 Y Freund and R E Schapire Game theory on-line prediction and boosting in Encyclopedia of Astronomy and Astrophysics Solar Physics Palma de Mallorca Spain 2008 P Murdin Ed Bristol Institute of Physics Publishing 2001 14 M J Aschwanden SolarTerrestrial Magnetic Activity and Space Environment Proceedings of the COSPAR Colloquium held in the NAOC Graphicon,2005 32 K Fukunaga vol 32 pp 1965-1970 2003 5 R M Wilson and E Hildner Are interplanetary magnetic clouds manifestations of coronal transients at 1 AU vol 61 pp 201-215 1979 8 A vol 69 pp 169-175 1981 9 S Yashiro N Gopalswamy S Akiyama G Michalek and R A Howard Visibility of Coronal Mass Ejections as a Function of Flare Location and Intensity vol 600 pp 1043-1051 2004 19 G Yang and H Wang Statistical Studies of Filament Disappearances and CMEs in Nantucket Massachusetts USA 1999 pp 109-114 17 B C Low B Fong and Y Fan The Mass ofa Solar Quiescent Prominence The solar wind nine conference The Astrophysical Journal The Astrophysical Journal vol 241 pp 195\255 211 2007 23 R Qahwaji T Colak M AI-amari and S Ipson Prediction ofCMEs Using Automated Machine Learning of CME-Flare Associations http://archives.njit.edu/vol01/etd/2000s/2005/njit\255 etd2005-0 73/njit-etd2005-0 Solar Physics Solar Physics Solar Physics Topical Issue on Solar Imaging I Journal of Computer and System Sciences 4 V Yurchyshyn H Wang and V Abramenko How Directions and Helicity of Erupted Solar Magnetic Fields Define Geoeffectiveness of Coronal Mass Ejections Poland R A Howard M J Koomen D J Michels and N R Sheeley Coronal transients near sunspot maximum Solar Physics 111 M R Brown R C Canfield and A A Pevtsov Eds Washington DC USA The American Geophysical Union 1999 p 25 12 B C Low Coronal mass ejections magnetic flux ropes and solar magnetism Machine Learning Chichester UK Praxis Publishing 2004 15 B C Low Solar activity and the corona Modest AdaBoost\255 teaching AdaBoost to generalize better Introduction to Statistical Pattern Recognition Journal of Geophysical Research Journal of Geophysical Research pdf lASTED International Conference on Visualization Imaging and Image Processing VIIP 2008 vol 91 pp 169-180 1984 6 D F Webb Understanding CMEs and their source regions vol 397 pp 1057-1067,2003 21 T Colak and R Qahwaji Automated McIntosh\255 Based Classification of Sunspot Groups Using MDI Images vol 38 pp 337-374,2000 31 A Vezhnevets and V Vezhnevets Astronomy and Astrophysics Solar Physics Solar Physics Poland and C L Ross The association of coronal mass ejection transients with other foons of solar activity 2007 22 R Qahwaji and T Colak Automatic Short-Term Solar Flare Prediction Using Machine Learning and Sunspot Associations Magnetic Helicity in Space and Laboratory Plasmas Geophysical Monograph vol 62 pp 1415-1426,2000 7 R H Munro J T Gosling E Hildner R M MacQueen A vol 167 pp 217-265 1996 16 B C Low Coronal Mass Ejections flares and prominences in vol 594,pp.l060-1067,2003 18 M Zhang and B C Low Magnetic Energy Storage in the Two Hydromagnetic Types of Solar Prominences vol 27 pp 861-874,2006 34 R Qahwaji M AI-amari T Colak and S Ipson Computerised Representation of the Association between Solar Features and Activities using Radial Basis Functions in New York Academic Press 1990 33 T Fawcett An introduction to ROC analysis Advances in Space Research Proceedings of the Ninth Annual Conference on Computational Learning Theory Pattern Recognition Letters 73 The Annals of Statistics MIC-CCA 2008 Page 42 1996 pp 325-332 30 J Friedman T Hastie and R Tibshirani Additive logistic regression A statistical view of boosting vol 106 pp 25141-25164,2001 13 B C Low Solar Coronal Mass Ejection Theory in vol 55,pp 119-139,1997 27 J Jing Dynamics of Filaments Flares and Coronal Mass Ejections CMEs in Physics of the Solar Corona An Introduction I thesis PhD 2005 28 R E Schapire and Y Singer Improved boosting algorithms using confidence-rated predictions Journal of Atmospheric and Solar-Terrestrial Physics  Beijing China 2002 p 113 20 G Zhou J Wang and Z Cao Correlation Between Halo Coronal Mass Ejections and Solar Surface Activity 


  vol. 26, no. 2, pp. 028 902+, 2009. [Onlin a b l e http://dx.doi.org/10.1088/0256-307X/26/2/028902 11  D. J. Watts and S. H. Strogatz, "Collective dynamics of `small-world networks Nature vol. 393, no. 6684, pp. 440-442, June 1998  http://dx.do i.org 10.1038/30 918  12  Y. Fernandess and D. Malkhi, "O n spreading recommendations via social gossip," in SPAA '08: Proceedings of the twentieth annual symposium on Parallelism in algorithms and architectures  New York, NY, USA: ACM, 2008, pp. 91-97. [Onlin l e  http://dx.doi.org/10.1145/1378533.1378547 13  Reka Albert and Albert-L´aszl´o Barabási. Statistical mechanics of complex networks. Review of Modern Physics, 74\(1\7–97, 2002 14  E. M. O’Grady, M. Rouleau, M. Tsvetovat. Network Fracture: How Conflict Cascades Regulate Network Density Agent 2007 Chicago IL, 2007 15  Barry Wellman, with Wenhong Chen and Dong Weizhen Networking Guanxi. Social Networks in China: Institutions, Culture and the Changing Nature of Guanxi. Cambridge University Press 2001 16  Banks, J., J. S. Carson, B. L Nelson, and D. M. Nicol. “Discreteevent system simulation”. 4th ed Upper Saddle River, New Jersey Prentice-Hall, Inc. 2005 17  Satnam Alag. Collective Intelligence in Action. Manning Publications Co. 2009 18  Brynn M. Evans, Ed H. Chi. Towards a Model of Understanding Social Search. CSCW’08, San Diego, California, USA. November 8 12, 2008 19  China Internet Network Information Center \(CNNIC\24rd Statistical Survey Report on the Internet Development in China. June. 2009 Online Available:http://research.cnnic.cn/img/h000/h11/attach200907161306 340.pdf 20  J. Howe, "Wired 14.06: The rise of crowdsourcing." [O Available: http://www.wired.com/wired/archive/14.06/crowds.htm 21  L.-H. Liu, F. Fu and L. Wang. Information propagation and collective consensus in blogosphere: a game-theoretical approach. eprint arXiv:physics/0701316 22  Crowdsourcing online, August 2009. [Onlin v ailable  http://en.wikipedia.org/wiki/Crowdsourcing    88 


The interactions between roles are also modeled. In the figure BookPicShow is realized by three roles PicShowControl  ImageFetch and ImageContainer  ImageContainer is an image container for picture visualization ImageFetch is to fetch image data, and PicShowControl takes the responsibility of controlling image fetching and showing. So, we can see that ImageFetch is involved by PicShowControl which means the former is activated by the latter, and PicShowControl accesses ImageContainer to set the image. They are the interactions between roles from a same feature. On the other hand PicShowControl is involved by BookSearch to take effect. It’s the interaction between roles from different features. In particular, the role BorrowProcess controls the execution process, so it involves all the roles mapped from the sub-actions in the composition action Borrow  We also model the role interactions related to abstract roles \(see in section 3.1.3\This kind of interactions usually crosscut multiple parts of a system For example, in figure 3 Log will be informed to activate by all the events of readers, e.g AddReaderRec  DelReaderRec etc. So, abstract role ReaderEvent is modeled to generalize the two roles and to simplify the interactions towards them Roles are instantiated by different kinds of assets in the bottom level. For instance, the role SetBookAmt is instantiated by the setAmount interface in the component Book The role Penalty Param corresponds to the penalty segment in a configuration file which is assessed by the Calculate Penalty role. The role ImageContainer as a resource role, is instantiated by a component Canvas as well as a code fragment to be used for its initialization in UI. The two RewardPoint Calculation Type roles are optional roles and they are realized by two methods which are not included in any component but only one of the methods will be composed into the base programs. The property Minimal Storage can be instantiated into an interface accessing the related variable in component Book The role log is instantiated as a log method to perform its function as well as a schema to create the log table in database for storing operation records         Figure 3. The traceability model on library management domain 
217 


5. Discussion about th e feature-oriented traceability model  In this section, we will briefly discuss the featureoriented traceability model: why we adopt the feature implementation model and ignore the architecture, how the model instructs the product derivation and SPL evolution phases. On the other hand, related tools are presented  5.1. Feature implementation model instead of SPL architecture  A product line architecture \(PLA\pecifies the architecture for a set of closely related software products u s u a l l y i n t e r m s of co m p o n en t s  connectors and configurations A P L A  f o cu s e s on modeling the variability for a domain and promotes the reuse in the SPL development. However, in the traceability context, feature tangling and scattering still exist between the feature model and the PLA, i.e several components may contribute to a single feature and a single component may contain implementations for several features. Therefore, it is difficult to explain how the functions of features are splitted as well as what is the intrinsic semantic of the feature interaction In our model, the feature implementation model is introduced to replace the complex many-to-many traceability between features and implementation artefacts with two sets of clear trace links. Roles that decompose features and the inter-relationships between the feature parts \(role interactions\o defined Role model provides the design decisions from the viewpoint of system designers, which are not concerned by the requirement analysers. It also captures the inner structure of a feature and records the semantic of feature implementation \(reason for splitting feature functions\ and feature interactions \(the intrinsic relationships between features\ a finer level Furthermore, roles are explicitly assigned to different implementations artefacts including components and other forms of implementations. Thus, the semantic for traceability from requirements to implementations is complemented and extended  5.2. Traceability-based product derivation and SPL evolution  Traceability is the basis of product derivation because we need to find out the variability-related program implementations according to the variable requirements and combine them with the base implementations [9  I n  o u r m ech an ism  r a th e r th a n  feature-driven customization and composition, we involve traceability-based role level customization and program-level composition. The first step is to decide whether the variable roles will be included in the final product. Some can be determined directly by the related features’ bounding status, others especially the internal variability-r elated roles should be customized by developers in role-level since they are unconcerned by clients. The next step is to select and configure the variability-related implementation artefacts according to the role instantiation traceability. In particular, role interaction, as an important kind of traceability, guides the program-level composition, which is to instruct what kinds of program implementations should be composed and how they can be composed using AOP mechanis   Traceability also plays an important role in SPL evolution. We are able to locate the features, roles and program implementations that are involved in the evolution through traceability and then make decisions on the evolution of the models, the implementations, as well as the traceability links. Usually evolution is driven by two factors. One is the requirement changes the other is bug fixing. Our mechanism is useful in these perspectives for it assists change impact analysis to identify the involved part of a SPL driven by a specific evolution request  5.3. Tool support  The tools OntoFeature  a n d FDAPD featuredriven and aspect-based product derivation tool   are developed to support the manual traceability capturing and visualizing. They are now separated and are used by different stakeholders \(requirement analyzer, system designer\n the SPL development process OntoFeature is a graphic feature modeling tool used to accomplish the ontology-based feature modeling which includes actions, facets and the relationships between these elements. FDAPD is a role modeling tool and is integrated with OntoFeature. It captures all the features and dependencies in the previous tool and provides graphic editing space for each of the features, where the roles, interactions and the corresponding implementation artefacts can be modeled. In practice, the two tools are cooperated, i.e OntoFeature for representing requirements and FDAPD for further design and implementation FDAPD is also developed to accomplish the role customization and program-level composition by invoking the AOP mechanism i.e. the variabilityrelated implementations are selected as aspects to be woven into the base programs according to the interaction types. The detailed composition process can be referred to  
218 


6. Conclusion and future work  In this paper, we focus on the visualized representation of the traceability in a software product line, and introduce a comprehensive feature-oriented traceability model. The model explicitly represents the product line artefacts in different abstraction levels. It also contains various kinds of traceability links explicit/implicit\mong the artefacts. Based on it, the traceability information from requirements to implementations is extended, described in finer-grained levels. In the whole model, the feature implementation model is innovative that it integrates the knowledge of both business logics and implementation techniques, i.e how the features are splitted and what is the intrinsic semantic of the feature interaction. This level helps to reduce the key problem of the mapping between the problem space and the solution space by providing evidences that the relationships between features and programs can be reasonably obtained through a third party ‘role’ rather than simply connecting the two sides As a result, the new concep t contributes to the ideal mechanism that the traceability of a system can be clearly presented However, the method we propose is far from mature in the following perspectives 1\The model is an informal representation model Now the models and the traceability links are manually identified and recorded by means of the developed tools. However, the automatic traceability capture deduction and validation relying on a formal basis are not available thus they are expected in the future work 2\When the product line grows larger, the traceability model will be farther complex and difficult to define. However, the problem cannot be avoided perfectly unless we make the model focus on the core part of a product line rather than the whole boundary This tailored model can be expected to describe the variable part in detail while the base program curtly. It will also be our future work that we think it will be helpful in the variability management domain especially tracing the variabil ity at different abstraction levels 3\The meta-model can be extended to other SPL development perspectives. For example, traceability related to testing is necessary in many development processes. Thus, we will complement the current traceability on design and implementation with the trace to testing artefacts in the future work  Acknowledgments This work is supported by National Natural Science Foundation of China under Grant No 60703092, 90818009, and National High Technology Development 863 Program of China under Grant No 2007AA01Z125  7. References  1 D  M  We is s a nd C  T. R   La i S of tw a r e Pr o duc t Line  Engineering: A Family-Based Software Development Process”, Addison-Wesley, 1999  te l a n d A.Fi nke lste i n  A n Ana l ysis of t h e  Requirements Traceability Problem”, in Proceeding of 1st International Conference on Requirement Engineering, 1994  Pe n g We ny un Zha o Y unjia o Xue a n d Yijia n W u   Ontology-Based Feature Modeling and ApplicationOriented Tailoring”, in Proceedings of International Conference on Software Reuse \(ICSR2006\, pp.87-100 4 n P e ng  L i wei S h en W e n y u n Z h ao F eat u r e Implementation Modeling based Product Derivation in Software Product Line”, in Proceedings of International Conference on Software Reuse\(ICSR2008  b isc h R.Brc i na  O pti m iz ing De sign f o r Va ria b ilit y  Using Traceability Links”, in Proceedings of International Conference on Engineering of Computer Based Systems ECBS2008\, pp.235-244 6  L a go  E  Ni emel a H Van Vl i e t  T o o l S u pp o r t f o r  Traceable Product Evolution”, in Proceedings of the Eighth European Conference on Software Maintenance and Reengineering \(CSMR2004\, pp.261-269 7 M.R i e b is c h  S u p p o r t i ng E v oluti ona r y D e ve lo pm e n t b y  Feature Models and Traceability Links”, in Proceedings of IEEE International Conference and Workshop on the Engineering of Computer-Based Systems \(ECBS2004\ pp 370-377  y sne i ros  J.Le ite   N onf un c tiona l Re q u ire m e n ts: Fr om  Elicitation to Conceptual Models”, in IEEE Transactions on Software Engineering, Vol. 30, No. 5, May, 2004 9 A  G  J  J a ns e n R  Sm e d i nga  J  va n G u r p a n d J  B o s c h   First class feature abstractions for product derivation”, in IEE Proc.-Softw., Vol. 151, No. 4, August 2004 10 W e i Z h ang  H o ng  M e i  H a i y an Z h ao   F eat u r ed r i v e n  requirement dependency analysis and high-level software design”, in Requirements Eng \(2006\ Vol.11, pp: 205–220  M  A l e k s y  T.H ile n b r a nd C  O b e r gf e ll, M  Sc hw i nd A  Pragmatic Approach to Traceability in Model-Driven Development”, in Proceedings of the Multikonferenz Wirtschaftsinformatik 2008 \(MKWI2008  B  R a m e s h  M.J a r k e   T ow a r ds r e f e r e nc e m ode ls f o r  requirements traceability”, in IEEE Transactions on Software Engineering, 2001, 27\(1\:58 14 J Bo sch Desi g n an d Use o f S o ft ware Arch i t ect u r es  Adopting and Evolving a Product Line Approach”, Pearson Education \(Addison-Wesley & ACM Press\, May 2000 15 N. M e dv i d o v i c R  N T a yl o r  A C l assi f i cat i o n and Comparison Framework for Software Architecture Description Languages”, in IEEE Transactions on Software Engineering, 2000. 26\(1\: p. 70-93 16 Yi j u n Yu  Yi q i ao W a ng  J M y l o p o u l o s  et al  Rev erse  Engineering Goal Models from Legacy Code”, in Proceedings of IEEE International Conference on Requirements Engineering \(RE2005\, pp.363-372  
219 


Automated Windowed Coefficient Trending Earlier we attempted to find explanatory models whose coefficients changed over time in a predictable way This would then let us predict future coefficient values for these models and apply these trended models towards future estimates The problem was that we were not able to find such stable models at least manually We automated our search for stable models by building a system we refer to as the Automated Coefficient Trend Search ACTS tool A patent is currently pending for the method implemented in this software Against the Far Out data set ACTS is designed to automatically permute fields run chronologically-windowed regressions with the chosen fields establish linear and nonlinear trends for the resulting coefficients and then use these trended coefficients to estimate a future set of data ACTS also automatically tries up to seven different transformations on any given dependant or independent variable The length of calibration windows and the increment between them are automatically varied ACTS establishes most promising coefficient trends and also rates the performance of predictions done using trended-coefficient models With so many parameters transformations windows length and intervals to evaluate ACTS needed to search a huge problem space Despite assembling a cluster of 8 computers to run ACTS if necessary for months the problem still required pruning Each run of ACTS was therefore restricted to producing models with a different number of variables between 3 and 15 We quickly found that the best models contained very few parameters due to over-fitting on wider models permitting us to pare down the problem Neural Netv 1.00 0.75 0.50 0.25 I  M The system produced literally thousands of forecasts for each project If the distribution of project costs and project costs estimates were known a confidence interval could be computed using the cost estimates and a method such as Bayesian model averaging used to integrate them 5 Not having a priori knowledge of this distribution we chose instead to use the inter-quartile range of the estimates that is we looked at whether the estimate fell between the first and third quartile of the estimates The top and bottom 25 were disregarded because there are always outliers and it was presumed the system could work well even when many of the individual estimates were not very good Given the very large number of estimates produced by the system literally thousands or tens of thousands depending on the settings we could afford to squander a few Results were best with only three variables per equation In eight out of 31 projects 26 of the projects the project's actual costs fell within the inter-quartile range of the forecasts of costs for that project produced by the ACTS models For 30 out of 31 projects the inter-quartile range of estimates was made up of over a thousand estimates while for the 3 1st project only 446 estimates were part of the inter-quartile range The significance of these numbers is revealed in the next paragraph When equations with four variables were used only one of the true project costs fell within the inter-quartile range of forecasts produced by ACTS When more than 4 variables were used none of the true project costs fell within the interquartile range From this we concluded that ACTS performed best when only 3 variables were included in each model Using more variables appears to produce over-fitted models Aork MRE By Lauch Month Chronologic 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Launch Month Since 1960 Figure 10 Neural network magnitude of relative error sorted chronologically 11 space 


incapable of accurate forecasts Of course the wider the inter-quartile range of forecasts the more likely it is that the true forecast will fit within that range But how parsimonious is the range As figure 11 shows in about 55\2600 of the inter-quartile ranges created by ACTS the 3rd quartile of estimates is 50%0 larger than the Ist quartile of estimates For about 90\2600 of the ranges created the difference between the I't and 3rd quartile is less than 75\2600 and for 97\2600 of the projects the top of the range is 100 or less than the bottom of the range 45\2600 of the true project costs falling into the new range When the top and bottom of the range were changed by 50\2600 the range estimates encompassed 77\2600 of the true project costs In the end however we felt that the adjusted ranges were too wide for practical use Adaboosting using binomial logit classifiers Short for adaptive boosting the Adaboost algorithm 6 is a type of machine learning algorithm where a set of classifiers are adjusted to favor previous misclassifications These classifiers are actually other estimating algorithms Percentage of Time 3rd Quartile no more than XO/o Greater than 1st Quartile 120 100 80 60 40 20 0 25 50 75 100 Figure 11 Variation in ACTS-produced estimates measured between 3rd and 1st quartile Percentage of time True Cost Falls in Range Produced by ACTS 3 variable ersion 90 8070 60 5040 30 20 10 l 0 Interquartile  or10  or25  or50  or75 Figure 12 The effect of widening the estimating range measured by  of estimates now lying within Of the eight projects that fell within the average percentage difference between the 1st and 3rd quartile was 55 with median of 52 This indicated that the inter-quartile range might be too restrictive We therefore proceeded to widen the range of cost estimates for each project multiplying the ISt quartile estimate by 1 X%0 and the 3rd quartile estimate by 1  X using various Xs In each instance we compared actual project costs to estimates produced and determined how frequently the actual fell into the range of estimates Results are summarized in figure 12 When the top and bottom of the range were changed by 10 32 of the actual estimates fell within the new range Extending each of the range boundaries by 25 resulted in Adaboost works by iteratively weighting observations so that poorly classified ones are given more weight during the next iteration With the algorithm focused on improving estimates for exceptional cases it is somewhat more sensitive to noisy data and outliers though less likely to over fit We chose to test this approach due to the latter property The boosting algorithm seemed structured particularly for binary decisions though we are seeking to estimate a continuous variable cost We therefore staged the method by estimating cost bands so that the estimate for each band reverted to a binary decision The algorithm would separately estimate whether an observation was 12 


our evaluation method tested potential models using either ordinary least squares least absolute above or below X dollars then Y dollars then Z such that instead namely that we were examining a proportion rather X  Y  Z than a binary outcome but this was impractical to use and we judged that the problem was not strongly biased towards The boosting algorithm still required a classifier and model either context For a classifier we implemented binary logit78 a discrete Adaboost MRE By COST Lu was similar to ACTS but with 0 1.00 0.75 0.50 0.25 0.00 0.25 0 50 0 75 1 00 1 25 1 50 1 75 2 00 Cost Figure 13 Adaboost magnitude of relative error sorted by actual cost Adaboost MRE CHRONOLOGICALLY Year Figure 14 Adaboost magnitude of relative error sorted chronologically choice model specifically intended for a binary dependant variable is the cost above or below this point There may have been some support for using the probit model 7 This was done by a statistical programmer with output checked against the statistics program STATA 8 we obtained from this first stage system We also attempted to use OLS as a classifier but our experiments showed no significant effect from adding relatively orthogonal i.e unrelated models Since using more than one model did not make a difference boosting using OLS was little more than a single OLS model with estimates divvied up in bands no dynamic re-weighting of classifiers and thus no boosting effect to speak of To obtain models some changes and improvements and again put were discarded and resolved by the latter 13 Lu 1.00 0.75 0.50 0.25 0.00 0 0.50 0.75 1 00 1.25 1 50 1 75 2 00 error which better discounts outliers and weighted least squares The models which we built another windowed test system that our computing cluster to use trying to find good ones from were then used to bootstrap the binary logit method although coefficients a sea of potential transformations This time 


This exercise required a number of decisions about how many classifiers to use how many bands to estimate and how many variables should be used in the models All these choices were made empirically Models with 3 variables were marginally more accurate than those with 4 More models seemed generally better to a point so we used 99 models We also used 22 cost bands checking whether an estimate was above or below 12M 35M 44M and so on through 519M It is interesting to note that there was no data scarcity-induced limit on the number of bands we could have used since each was estimated using all observations lying above and below and not just between bands As with the neural network training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 The trend in estimate deviation would again permit us to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 13 and chronologically in figure 14 Figure 13 shows that MREs are large and negative for missions at 56M actual mission cost and below Above this point results are mixed although not as poor as at the very low range There seems to be no degradation in predictive accuracy as projects increase in scale The correlation between estimates and actual values is approximately 0.66 Sorted chronologically in figure 14 no trend can be seen as estimates go farther out up to 11 years as was seen with the neural network Again this may have been due to the normalization process in which we mapped each variable to its percentage ranking based on future and past minimum and maximum values basically embedding the effect of time within the factors being input to the model It also may be due to a relatively stable era Weighted combinations of simple models A persistent problem in multivariate estimation methods which has been mentioned is that a model may be produced that over fits the observations used for calibration It would thus not be adequately generalized to estimate out of that sample Techniques such as jack-knifing and outlier removal can mitigate this We chose another approach developing models that were little more than simple rules of thumb and then combining these in a weighted manner so each would have a say in the estimate The weightings were formulated using Excel's Solver to minimize estimation error on a set observations lying 15 to 25 years before the test set We refer to this method as continuous boosting because as with the Adaboost method it too involves a mix of classifiers that are combined according to their performance against a set of test observations Unlike Adaboost which solves a binary problem the algorithms are here estimating on a continuous scale Also unlike Adaboost the models are re-weighted rather than the observations to maximize prediction Before presenting results it is worth noting that we ran a control test where all variables from the simple rules of thumb were combined into a single model and run on the same windows performance was nowhere near as good We chose the models by hand guided by the criteria that they should be relatively orthogonal rely on differing parameters so they could each contribute a unique influence to the combined result Figure 15 shows that MREs are mostly positive though for lower cost missions errors are regularly negative and large Continuous Boost MRE By COST M M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Cost Figure 15 Continuous boosting magnitude of relative error sorted by actual cost 14 


CONCLUSIONS Given that this project was intended to estimate missions lying 10-15 years out we structured it differently than one intended to estimate contemporary projects A variety of conventional techniques were not used as we felt they would over fit training observations and thus not be suitable for prediction We also were not sure which approach would work so we tried many We heavily favored ensemble methods where models are combined because we surmised that any one model could not be guaranteed to have the best view of the future However a single neural network ultimately yielded the most competitive results Another finding was that methods built upon simple models such as with three variables generally did work best not surprisingly because they were less likely to over fit calibration data A graph of the three best methods is shown in figure 18 with estimates for the same missions sorted by cost For the neural network calibration occurred until just before the results shown for continuous boosting calibration occurred no more recently than 15 years prior to the results and for Adaboost calibration also occurred up until the results shown A graph sorted by year is not shown we think it instructive that no results seemed to degrade over time As mentioned but obvious from figure 17 the neural network performs best followed by Adaboost and then continuous boosting It is interesting to note that the three cases in which the neural network goes haywire and predicts too low also correspond to worst performances for the Adaboost method pointing to exceptional data points We will be further investigating these regularly errant results and other outliers which may result in estimating improvements ACKNOWLEDGEMENTS This work was carried out under Small Business Innovation Research contract FS9453-05-C-0023 with the Air Force Research Lab The authors wish to gratefully acknowledge Judy Fennelly and later Ross Wainwright our technical points of contact at AFRL for their continuous support and encouragement Our contract officer Timothy Provencio also provided invaluable assistance Our critical seed stock of data was provided by Joseph Hamaker who previously was Director of NASA Headquarters Cost Analysis Division and now is Senior Cost Analyst with SAIC Ainsley Chong and Dale Martin USAF Ret also lent considerable assistance with data gathering APPENDIX A MISSIONS COLLECTED Active Cavity Radiometer Irradiance Monitor Satellite Active Magnetospheric Particle Tracer Explorer Adeos Advanced Communications Technology Satellite Advanced Composition Explorer Alexis Amos-I AMSC-1 Anik El Anik E2 Applications Technology Satellite-I Applications Technology Satellite-2 Applications Technology Satellite-5 All MREs Lu 1.00 0.75 l 0.50l 0.25 l 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75-2.00oi v 1   Cost o N X  X~~~~~Cl Figure 16 Comparison of estimating error for three best methods 15 


Applications Technology Satellite-6 Aqua Argos Atmospheric Explorer AURA Aurora 2 Calipso Cassini Cassini Spacecraft  Huygens Probe Chandra X Ray Observatory CHIPSat Clark Clementine CloudSat COBE Columbia 5 Contour CRRES DART Dawn DBS-1 Deep Impact Flyby Spacecraft  Impactor Deep Space 1 Deep Space 2 Defense Meteorological Satellite Program-5D Defense Meteorological Satellite Program-5D3 Defense Support Program DSCS 3 FIO DSCS 3 F7 DSCS I DSCS-II DSCS-IIIA DSCS-IIIB Dynamics Explorer-I Dynamics Explorer-2 Earth Observing Satellite 1 Earth Radiation Budget Experiment EchoStar 5 Extreme Ultraviolet Explorer Far Ultraviolet Spectroscopic Explorer FAST FLTSATCOM 6 Galaxy 5 Galaxy 11 Galaxy Evolution Explorer Galileo Orbiter  Probe Gamma Ray Large Area Space Telescope GE 1 GE 5 Genesis GFO 1 Globalstar 8 Glomr GOES 3 GOES 9 GOES N GPS-1 GPS-IIR GPSMYP GRACE Gravity Probe-B GRO/Compton Gamma Ray Observatory GStar4 Hayabusa HEAO-1 HEAO-2 HEAO-3 HESSI-II High Energy Transient Explorer-II HETE HST ICESat Ikonos IMAGE IMP-H Inmarsat 3-F5 Intelsat K INTELSAT-II INTELSAT-IV International Ultraviolet Explorer Iridium James Webb Space Telescope Jason 1 JAWSAT KEPLER KOMPSAT LANDSAT1 LANDSAT-4 LANDSAT-7 Lewis Lunar Orbiter Lunar Prospector Magellan Magsat Mariner-4 Mariner-6 Mariner-8 Mariner1 0 MARISAT Mars Exploration Rover Mars Express/Beagle 2 Mars Global Surveyor Mars Observer Mars Odyssey Mars Surveyor 2001 Orbiter Mars Pathfinder  Sojourner Rovers Mars Polar Lander Mars Reconnaissance Orbiter Mars Telecommunication Orbiter Mars Climate Orbiter Messenger Meteor Mid-course Space Experiment MightySat Milstar 3  Adv EHF Model-35 Morelos NATO III Near Earth Asteroid Rendezvous NEAR Shoemaker New Horizons 16 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


