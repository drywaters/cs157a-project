Big Data Analytics Frameworks    Parth Chandarana  V.E.S.I.T, Chembur  Mumbai, India  chandaranaparthk@gmail.com  M. Vijayalakshmi  Department of Information Technology  V.E.S.I.T, Chembur  Mumbai, India  Viji.murli@gmail.com   Abstract   Big Data concerns massiv e heterogeneous autonomous sources with distributed and decentralized control. These characteristics make it an extreme challenge for organizations using traditional data management mechanism to store and process these huge datasets It is required to de fine a new paradigm and re evaluate current system to manage and process Big Data. In this paper, the important characteristics issues and challenges related to Big Data management has been explored Various open source Big Data analytics frameworks that deal with Big Data analytics workloads have been discussed Comparative study between the given frameworks and suitability of the same has been proposed   Keywords  Big Data Analytics  Big Data Issues and Challenges   Apache Hadoop   Apache Drill; Project  St orm  I   I NTRODUCTION  Digital universe is flooded with huge amount of data generated by number of users worldwide These data are of diverse in nature come from various sources and in many forms. To keep with the desire to store and analyze ever larger volume s of complex data relational databases vendors have delivered specialized analytical platforms that come in many shapes and sizes from software only to analytical services that run in third party hosted environments In addition new technologies have emer ged to address exploding volumes of complex data, including web traffic, social media content and machine generated data including sensor data global positioning system data. New non relational database vendors combine text indexing and natural language p rocessing techniques with traditional database technologies to optimize ad hoc queries against semi structured data Number of analytical platform are available in the market for analysis of complex structured and unstructured data each of which is design ed to handle specific type of data/workload In this paper we will discuss three open source Big Data Analytics frameworks suitable for different types of workload  This paper is organized as follows Section 2 we will discuss characteristics of Big Data  motivation for adapting Big Data and analytics platform in organization Section 3 will discuss issues and challenges organization facing with Big Data and Analytics. Section 4 discusses three open source Big Data analytics frameworks Comparison between these 3 frameworks and suitability of the framework is suggested in section 5  II  BIG  DATA  AND  ANALYTICS  CHARACTESISTICS   The term Big Data covers diverse  technologies same as cloud computing  Input to Big Data systems comes from web server logs social netwo rks satellite imagery traffic flow sensors broadcast audio sensors, ba nking transaction etc. This data is called  Big Data To identify data as Big Data should be analyzed from different dimensions  A  Characteristics of Big Data  Big Data can be characteriz ed by different aspects The commonly used as pects are Volume Velocity and V ariety Veracity and Value are also used to characterize Big Data They are helpful lens through which we can understand the nature of Big Data and the platform available to explo it them   Volume   As infrastructure becomes increasingly available and affordable data generated by different sources is very huge in size; petabytes or zettabytes. This huge amount of data is called Big Data   Velocity   The sheer velocity at which we a re creating data is huge cause of Big Data Digital universe expands from 130 million to 40 trillion in 8 years 2005 2013 The data generated from various sources range from Batch to Real time       Variety   The representation of data generated by various sources are diverse in nature for example ecommerce web sites deal with structured data 12    w eb  s e r v er  lo g s  d e al w ith  semi structured data 13   a n d  s o cial w eb s it es d ea l w ith  unstructured data  data can be categorized into structured, unstructured and semi structured types and digital universe  deals with combination of all  Veracity  Duo to sheer velocity of some data we cannot spend time in cleans the da ta before using it. Compiling multisource data and use it for decision making for business requires 
2014 International Conference on Circuits, Systems, Communication and Information Technology Applications \(CSCITA 
978-1-4799-2494-3/14/$31.00 ©2014 IEEE 
430 


mechanism that deals with imprecise data. Hence combination of precise, imprecise, accurate, data can be called big data  Value  By processing huge volume high velocity, variety and veracity of data presents a new dimension for analyzing big  putting them all together in order to extract hidden knowledge for business and getting competitive advant age from it represents value of big data    Fig. 1  Characteristics of Big Data  11   B  Motivaton for Big Data and Analytics  Statistics  18   shows that rate of data generated on digital universe  is increasing exponentially Current tools and technologies are not up t o the mark to store and process  huge amount of data They are also unable to extract value from these data which is most important  When an enterprise can leverage all the information available with large data rather than just a subset of its data then it has a powerful advantage over the market competitors Big Data can help to gain insights and make better decisions  In order to handle big data modified paradigms are required Following are some areas where Big Data can play important role   Big Data Ana lytics and Health Care  Medical practitioners store huge amount of data about  amount of data are being stored by drug manufacturing company  These data are very complex in nature and sometimes pr actitioners cannot correlate with other information thus results in important information remains hidden. By applying advance analytics techniques, this  hidden information can be extracted  which results in personalized medication Advance analytics techn iques can also gain insight into genetic and environmental causes of diseases   Big Data Analytics and intelligence agencies  Intelligence agencies collect huge amount of data from different sources like satellite imagery signal intercepts and publicly av ailable sources. Connecting dots, b y linking all the information possible threats can be found   thefts can be prevented or detected All of these requires robust analytics technique th at handles large amount of complex data   Big Data Analytics and Enviro nment  Understanding environment requires huge amount of data collected from various sources like sensors monitoring air and water quality, metrological conditions, proportion of CO 2  and other gas in air etc By linking all information together important tr ending such as increased CO 2 emission increase or decrease of greenhouse effect can be found out  All above example shows that adaption of new frameworks tools and technologies result into extraction of valuable information which remains hidden previousl y  III  BIG DATA ANALYTICS I SSUES AND CHALLENGES  Organization dealing with Big Data facing n umerous challenges. System work ing  with Big Data need to understand the need of technology and need of user. Meeting challenges presented by Big Data will be difficult volume of data increasing every day velocity of its generation is increasing faster than ever; variety of data is also expanding   Current tools technologies architecture management and analysis approaches are unable to cop up with complexity of data presented. Some challenges are presented below  Privacy, Security and Trust  Organization using Big Data committed to protect the privacy security of its users and should ensure that the organization must comply all privacy and security related act to e nhance the protection of and set clear boundaries for usage of personal information  Trust in the organization needs to be maintained as the volume of data holding increases The trust that users have in these agencies and their abilities to securely hold information of a personal can easily be affected by leakage of data or information into public domain  Data Management and Sharing  Agencies realize that for data to have any value, it needs to be discoverable, accessible and usable. Agencies must achieve  these requirements but still adhering to privacy laws. Current trends towards open data has seen a focus on making datasets available to the public Agencies must put focus on making data available open and standardize within and between agencies in such  a way that allows agencies to use and collaborate to the extent made possible by privacy laws  Technology and Analytical skills  Big Data and Analytics put lot of stress on ICT providers for developing new tools and technology to handle complex data Cur rent tools and technologies are unable to store, process and analyze massive amount of diverse data Vendors and developers of Big Data systems and solutions including open source software are developing more capable tools to simplify the challenges of Big  Data Analytics  
2014 International Conference on Circuits, Systems, Communication and Information Technology Applications \(CSCITA 
431 


Some specific challenges related to Big Data and Analytics are  Data Storage and Retrieval  Current available technologies are able to handle data entry and data storage But the tools designed for transaction processing which will add u pdate search for small to huge amount of data is not be able to handle big data How to handle semi or unstructured data for processing it is yet unknown [2   Quality vs quantity  When dealing with huge amount of data, sometime it is difficult to decide     Which data is inappropriate and how do we select most appropriate data    How do ensure authenticity of the data    How to estimate the value of data  Data Growth and Expansion   As the organizations increases their services their data is also expected to g row Few organization also consider data expansion because of data grow in richness, data evolved with new techniques [2    Speed and scale   When volume of data grows, it is difficult to gain insight into data within time period. Gaining insight into data  is more important than processing complete set of data Processing near real time data will always require processing interval in order to produce satisfactory output [2    Structured and unstructured data   Transition between structured data  stored in w ell defined tables and unstructured data \(images, videos, text\ required for analysis will affect end to end processing of data Invention of new non relational technologies will provide some flexibility in data representation and processing  2   Data own ership   Very huge amount of data resides in the servers of social media service providers These data is not really owned by them but they store data of their users. Actual owner of the page is one who has created the page or account This is ongoing and big challenge in area of social media [2   IV  BIG  DATA  ANALYTICS  FRAMEWORKS  Different  types of data when we consider Big Data  Different types of framework required to run different types of analytics  A variety of workloads present in la rge scale data proces sing enterprise In order to achieve a business goal, we often see a combination of said workloads deployed    Batch oriented  processing for example Map Reduce based frameworks like Hadoop for recurring tasks such as large s cale data mining or aggregation  8     OLTP such as user facing e commerce transactions with Apache HBase  14    Stream  processing to handle stream sources such as social media feeds or sensor data, with Storm being a representative framework  9     Interactive  ad hoc query and analysis wi th Apache Drill  5   A  Apache Hadoop  Apache Hadoop is open source software library which includes framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It has a variety of option s ranging from single computer to thousands of computers each of which offering local computation and storage  Instead of depending on hardware, library itself designed to detect and handle failure and assure high availability at application layer [7    A pache Hadoop include following modules  a  Hadoop core Common utilities that support other modules  b  Hadoop distributed file system  Provide high throughput access to application data  c  Hadoop YARN  Framework for job scheduling and resource management  d  Hadoop M ap Reduce Framework for parallel processing of large data set   Fig. 2  Data store and retrival in  Apache Hadoop  system   15   Data management with  Apache Hadoop is shown  in Fig.2  Here query is submitted by user to Hadoop Engine which will take input data from HDF S Data is spread across number of data nodes. There is one name node or Job Tracker which will take care of assigning  the work among data nodes and producing  the result and responding back to user Architecture of Apache Hadoop is very robust and fault to lerant Job Tracker is continuously tracing the status of data node and if data node remains silent for more than predefined time, task of that data node is given to another data node  B  Project Storm  Hadoop and related technologies have made it possible to store and process data at scales previously unthinkable  Unfortunately these data processing technologies are not real time systems However real time data processing at massive scale is becoming more and more of a requirement for businesses Storm expo ses a set of primitives for doing real time computation Like how Map Reduce greatly eases the 
2014 International Conference on Circuits, Systems, Communication and Information Technology Applications \(CSCITA 
432 


en-IN writing of parallel batch processing, Storm's primitives greatly ease the writing of pa rallel real time computation [9   Fig. 3  Architecture  of  Storm  cluster   16   A rchitecture of storm cluster is shown in Fig 3 A Storm cluster is superficially similar to a Hadoop cluster. Whereas on Hadoop you run Map Reduce jobs on Storm you run topologies Jobs and topologies themselves are very different  one key diffe rence is that a Map Reduce job eventually finishes whereas a topology processes messages forever \(or until you kill it  9   There are two kinds of nodes on a Storm cluster: the master node and the worker nodes The master node runs a daemon called Nimbu s that is similar to Hadoop's Job Tracker Nimbus is responsible for distributing code around the cluster assigning tasks to machines, and monitoring for failures. Each worker node runs a daemon called the Supervisor The supervisor listens for work assigned to its machine and starts and stops worker processes as necessary based on what Nimbus has assigned to it Each worker process executes a subset of a topology a running topology consists of many worker processes spread across many machines All c oordination between Nimbus and the Supervisors is done through a Zookeeper   17   cluster  a coordinating service in distributed environment which will take care of naming configuration management synchronization etc Additionally the Nimbus daemon and Su pervisor daemons are fail fast and stateless all state is kept in Zookeeper or on local disk This means you can kill 9 Nimbus or the Supervisors and they'll start back up like nothing happened. This design leads to Storm clusters being incredibly stable   C  Apache Drill  Apache Drill is a distributed system for interactive ad hoc analysis of large scale datasets Designed to handle up to petabytes of data spread across thousands of servers the goal of Drill is to respond to ad hoc queries in a low latency m anner  Many a times it happens that human sits in front of business application and need to execute ad hoc queries as per business needs Query should not need more then few seconds to execute even at scale; some time user do not know which query to fire i n advance also user need to react to changing circumstances Apache drill will provide the solution for all above issues [5   Fig. 4  Architecture block diagram of Apache Drill   5   High level architecture of Apache Drill is shown in Fig 4 At high level Apache D  layers  User   providing interfaces such as a command line interface CLI a REST interface JDBC/ODBC etc for human or application driven interaction  Processing  allowing for pluggable query languages as well as the query planner, execution, and storage engines  Data sources  pluggable data sources either local or in a cluster setup, providing in situ data processing  Apache Drill is not a database but rather a query layer that works with a number of underlyin g data sources. It is primarily designed to do full table scans of relevant data as opposed to say maintaining indices Apache Drill provides for a flexible query execution framework enabling a number of use cases from quick aggregation of statistics to  explorative data analysis  The workers in Apache Drill, suitably called drill bits, run on each processing node in order to maximize data locality The coordination of the drill bits, the query planning, as well as the optimization, scheduling, and execut ion are performed and distributed  V  CONCLUSION  In this work a detailed study of Big Data and analytics has been performed and comparison between different frameworks is given below   T ABLE 1  COMPARISION BETWEEN BIG DATA ANALYTICS F RAM EWORKS  Features  Apache Hadoop  Project Storm  Apache Drill  Owner  Community  Community  Community  Workload  Batch processing  Real time computation  stream analysis  Interactive and Ad hoc analysis  Source code  Open  Open  Open  Low Latency  No  Yes  Yes  Comple xity  Easy  Easy  Complex   
2014 International Conference on Circuits, Systems, Communication and Information Technology Applications \(CSCITA 
433 


As shown in above table Apache Hadoop is suited for workload where time is not critical factor whereas Project storm is well suited for data stream analysis in which analysis performed is real time and Apache drill is best for int eractive and ad hoc analysis. Following points related to Big Data and Analytics are worth noted    There is a requirement of Big Data Analytics frameworks for the organization that deal with different types of Big Data workloads In addition a middleware ar chitecture is also required to integrate and process all Big Data related workloads    Organization dealing with Big Data and Analytics need to deal with challenges like privacy security data management and sharing, technology skills and other specific ch allenges related to workload present in the organization  R EFERENCES  1    Computing \(IC3\ 2013   2  Stephen K Frank A J A   Conference on System Sciences, 2013  3   Conference on Communication Information  Co mputing Technology ICCICT\,Oct. 19 20, 2012  4     5   hoc interactive analysis at   6  Sergey M, Andrey G, Jing Jing L, Geoffrey R, Shiva S, Matt T, Theo V    7    ymposium on Operating System Design and Implementation, San Francisco, CA, December, 2004  8     http://hadoop.apache.org/#What+Is+Apache+Hadoop%3F  9    http://storm project.net  10  Apache   https://cwiki.apache.org/confluence display/DRILL/Apache+Drill+Wiki  11  Characteristics of Big Data  http://www.datatechnocrats.com/tag/big data  12   http://www.webopedia.com/TERM/S/structured_data.html  13     http://en.wikipedia.org/wiki/Semi structured_data  14  Ap ache HBase  http://hbase.apache.org  15  Storing and querying data Big Data in HDFS  http://ecomcanada.wordpress.com/2012/11/14/storing and querying big data in hadoop hdfs  16  Storm cluster  https://github.com/nathanmarz/storm/wiki/Tutorial  17  Apache Zookeeper  http://zookeeper.apache.org  18  Big Data statistics  wikibon.org/blog/big data statistics      
2014 International Conference on Circuits, Systems, Communication and Information Technology Applications \(CSCITA 
434 


002 002 002 002 002 
010 
002 002 002 
covered alive sleepy covered  return throw covered 
A Subject Programs 
 and 3.2 Library of mathematics and statistics components HSQLDB   Relational database engine with inmemory and disk-based tables JFreeChart Library to display professional quality charts in applications JTopas    Suite for parsing arbitrary text data Scimark2    Benchmark for scientiìc and numerical computing Weka r5178   and r1004 Collection of machine learning algorithms for data mining tasks XML-Security   Library to implement XML signature and encryption standards with a small memory footprint Thus these three programs perform data manipulation tasks Finally JFreeChart is a graphics library and Commons-Lang is a library that perform tasks related to string manipulation concurrency creation and serialization of objects among others All programs are either open-source or publicly available The third criterion is related to the size of the programs To assess the scalability of BA we deìned three ranges of systems small medium and large Small systems are those ranging from 150 to 2,000 LOCs medium size systems vary from 2 KLOCs up to 30 KLOCs and large systems were those with more than 30KLOCs These ranges were deìned by estimating the number of engineers working in the system A small program can be easily coded by a single engineer a medium size program can be coded by a single engineer in the bottom of the range 2 KLOCs but more help will be needed when its size reaches the top of the range 30 KLOCs Large systems will hardly preclude the need of a group of engineers Table IV describes the characteristics of the selected programs It contains the number of lines of code LOC the number of methods and classes the number of methods with more than 64 duas with more than 32 up to 64 duas with up to 32 duas and with zero duas it also contains the total number of duas and of edges The percentage numbers in braces mean the coverage achieved by running the program test suite For Commons-lang 99.30 of classes and 93.03 methods were covered as well as 88.05 of duas and 92 of edges The percentage of methods with a particular property e.g requiring more than 64 duas with respect to the total number of methods is represented by the percentage number without braces For example Commons-lang has 1.58 of its methods with more than 64 duas required Control-îow coverages were obtained with JaCoCo and data-îow coverage with BA-DUA The lines of code were measure with JavaNCSS 6  Two programsÑJTopas and Scimark2Ñare classiìed as small systems Commons-Lang Commons-Math-2.1 and XML-Security are medium size programs and ve programs Commons-Math 3.2 HSQLDB JFreeChart Weka r5172 and r10042Ñare large systems With the exception of the programs 6 http://www.kclee.de/clemens/java/javancss 
At the beginning of the method code instructions are added to initialize the new local variables   and  with the empty value Because is a local variable it is not sensitive to recursive calls nor threads However its scope is limited to the method call To keep the coverage between method calls each class has a reference to a global array of covered duas BA-DUA implements the global array as one array of longs   per class A method can be associated with more than an entry for instance if a method have more than 64 duas Just before a or a command instructions are added to update the correspondent entries in the global array of covered duas As mention before the local variable is not thread sensitive however the global array entries are Therefore race conditions may occur when entries of the global array are updated As a result the coverage obtained can be slightly lower than the real coverage for programs with multiple threads Strategies for dealing with race conditions are beyond the scope of this paper VI BA E VALUATION An experiment to evaluate the penalties imposed by BA was carried out The experiment was designed to answer the following research questions RQ1 How do the penalties imposed by BA impact on an instrumented program RQ2 How much more expensive is BA monitoring in comparison with edge monitoring Two kinds of penalties were assessed execution overhead given by the extra time needed to execute the instrumentation code added and memory overhead represented by the growth of the object code due to the insertion of instrumentation code BA-DUA was used to assess BA performance whereas JaCoCo was utilized to assess edge monitoring This tool was chosen because it has been used to obtain control-îow instruction and edge coverage of large systems Both BADUA and JaCoCo require limited extra memory at run-time thus the program growth provides an estimate of the extra memory needed to run both instrumentation approaches Next the evaluation carried out and its results are described Table III contains the description of the programs utilized in our evaluation We utilized several criteria to select them The rst criterion was to select programs that were previously utilized in data-îow testing instrumentation papers The idea was to allow a rough comparison between instrumentation techniques These programs are indicated in Table III by a star   Within this same criterion we added those programs utilized in the simulations contained in The y are referred to by a diamond   The second selection criterion was the characteristics of the systems The goal was to have a diverse selection of programs Five programsÑCommons-Math 2.1 and 3.2 Scimark2 and Weka r5178 and r10042Ñperform mathematical calculations JTopas parses arbitrary texts and XML-Security parse and manipulate XML les and HSQLDB is a relational database TABLE III D ESCRIPTION OF THE PROGRAMS SELECTED FOR EVALUATION  Program Description Commons-Lang Library with utilities classes for Java Commons-Math 2.1  
002 
86 


Baseline JaCoCo BA-DUA 
TABLE IV C HARACTERISTICS OF THE SELECTED PROGRAMS  Program LOC Classes Methods Methods w Methods w Methods w Methods w Duas Edges duas 
003 003 
real time 
64 64 duas 32 32 dua 0 duas=0 Commons-Lang 13,743 143 2,281 36 129 907 1,209 20,594 7,395 99.30 93.03 1.58 5.66 39.76 53 88.05 92 Commons-Math 2.1 26,347 428 3,995 125 207 1,099 2,564 42,901 8,745 99.53 86.43 3.13 5.18 27.51 64.18 84.78 86 Commons-Math 3.2 52,731 845 6,886 276 370 1,745 4,495 89,678 18,576 95.03 85.42 4.01 5.37 25.34 65.28 77.08 85 HSQLDB 116,618 439 8,365 414 587 3,090 4,274 122,987 34,722 66.29 48.44 4.95 7.02 36.94 51.09 35.40 35 JFreeChart 66,895 520 8,313 274 353 2,001 5,685 77,957 21,165 82.88 60.60 3.30 4.25 24.07 68.39 44.76 45 JTopas 6184 41 475 11 17 152 295 3,773 1,233 78.05 75.37 2.32 3.58 32.00 62.11 65.52 62 Scimark2 672 10 61 5 8 23 25 1220 218 90.00 62.30 8.20 13.11 37.70 40.98 58.11 59 Weka r5178 209,482 2,068 20,727 1,142 1,297 5,477 12,811 314,369 69,420 37.86 35.95 5.51 6.26 26.42 61.81 36.15 36 Weka r10042 184,454 2,257 19,694 982 1,151 5,196 12,365 271,443 64,760 28.93 28.49 4.99 5.84 26.38 62.79 25.96 26 XML-Security 17,929 317 2,353 38 82 745 1,488 16,821 6,273 74.13 61.24 1.61 3.48 31.66 63.24 56.10 53 for which we selected two versions the others are the latest version as of September 2013 
The treatments were deìned taking into account the coverage tool utilized if any Below they are deìned  This treatment regards an uninstrumented version of the program  This treatment regards a JaCoCo instrumentated program tracking instructions and edges at run-time  This treatment regards a BA-DUA instrumentated program tracking duas at run-time Our data collection utilized two procedures One to collect execution and other to obtain memory overhead The execution overhead was determined by executing the test suites accompanying each program Excepting HSQLDB and JTopas for which failing test cases were removed the test suites executed were those contained in the repository of the program The memory overhead was obtained by the ratio of the total size of all instrumented classes by the size of all compiled classes of each program For each treatment the execution time of the subject program was obtained by averaging the time of ten executions It was collected using the option of the Unix command since we were interested in obtaining the wall clock time Exceptionally for programs whose random input data generated outliers in the time measures we executed each program one hundred times Only one program needed this procedure which was XML-Security For two methods in Commons-Math 3.2 and two in HSLQDB the size of the instrumented method was larger than 64Kbytes which hampered their execution in the JVM Our procedure was not to instrument the classes of these methods Thus the coverage of these classes was not obtained for both tools JaCoCo and BA-DUA were run in off-line mode that is the instrumentation was performed before the execution of the programs All data was collected using an Intel\(R Core\(TM Duo CPU E4500@2.20GHz 2,063,668 kBytes of RAM running Debian GNU Linux 6.0.2 and Java HotSpot\(TM 32-Bit Client VM 1.7.0 40 The execution overhead results are presented in Table V The table contains the absolute averaged execution times for each treatment In addition it comprises the overhead imposed by JaCoCo and BA-DUA in percentage terms In the second row the data regarding Commons-lang is presented The uninstrumented version of the program Baseline executed the test suite in 20.02 seconds in average the JaCoCo instrumented version executed in 20.25 seconds and BA-DUA version in 20.57 seconds The overhead imposed by JaCoCo was 1.15 and BA-DUA overhead was 2.75 In Figure 4 the ratios Baseline to JaCoCo and Baseline to BA-DUA are presented The value 1 in the chart means that there is no overhead between the uninstrumented program and the one instrumented by the testing tool On the other hand a ratio equals to 1.5 means that there is 50 overhead when the instrumented version is executed Figure 5 presents JaCoCo and BA-DUA memory overhead with respect to the Baseline it describes the ratio between the size of all compiled classes of the uninstrumented program to the size of all JaCoCo instrumented classes and to all BA-DUA instrumented classes Analogously a value 1 means that there is no memory overhead a value 2 means 100 of overhead 
B Treatments C Procedure D Results 
   
87 


000 000\002\003 004 004\002\003 005 005\002\003 006\007\010\011\010\011 012\013\014\015\016\013 
TABLE V E XECUTION OVERHEAD OF THE SELECTED PROGRAMS  Program Baseline s JaCoCo s BA-DUA s Commons-lang 20.02 20.25 20.57 1.15 2.75 Commons-Math 2.1 22.79 25.03 31.48 9.83 38.13 Commons-Math 3.2 179.57 214.27 487.8 19.46 171.65 HSQLDB 24.39 26.99 28.64 10.66 17.43 JFreeChart 5.24 5.29 5.36 0.95 2.29 JTopas 50.24 148.7 68.59 195.98 36.52 Scimark2 27.31 30.47 27.08 11.57 0.84 Weka r5178 497.39 554.87 890.78 11.56 79.09 Weka r10042 106.85 130.27 139.13 21.92 30.21 XML-Security 51.41 51.47 50.96 0.12 0.88 Fig 4 JaCoCo:Baseline and BA-DUA:Baseline execution overhead ratios VII D ISCUSSION Our discussion of the results is guided by the research questions The section is concluded with the threats to validity of the experiment Fig 5 JaCoCo:Baseline and BA-DUA:Baseline memory overhead ratios over 50 Thus our data suggests that by using BA DF testing can be applied to larger class of programs with affordable execution overhead Misurda et al.ês demand-driven approach obtained an average of 127 overhead the values varying from 4 to 279 Hassan and Andrew report that DUA-FORENSICS imposed an overhead of 2078 160.4 and 86.9 for JTopas Scimark2 and XML-Security respectively Although a comparison with previous approaches should be carried out with caution due to differences in the DF testing implementation e.g interor intra-procedural testing and JVM and hardware utilized the results indicate that BA overhead is signiìcantly smaller This difference is more signiìcant when comparing the size of the program utilized in experiments Previously only small to medium size programs were utilized This is so because both approaches have to keep data structures that encompass all duas of the programs On other hand BA performance is governed by the number of duas in each method and by how long the execution remains in particular methods BA-DUA bit vectors are implemented as a 64-bit long thus there is no difference in tracking one or 64 duas However a method with 65 duas will require another long and more code to update the extra long As a result the execution overhead increases Table IV shows that the number of methods with more than 64 duas is small at most 8.2 and as low as 1.58 of all methods Even though a small number of methods with high concentration of duas can hurt performance For example Commons-Math 3.2 has 276 methods with more that 64 duas 24 of which with more than 320 duas 5 longs and 4 with more than 1000 duas 16 longs Thus depending on how long the execution remains in these methods the BA overhead will increase Particularly Commons-Math 3.2 has a high method coverage 85.42 which means that they are very likely to be executed by the test suite BA memory overhead is essentially the bit vectors created  
A BA Penalties 
covered live cursleepy 
The execution overhead imposed by BA varied from 0.88 for XML-Security to 171.65 for Commons-Math 3.2 being the average overhead 37.64 More interestingly though is to analyze the data presented in Table V and in Figure 4 For four of the subject programs the overhead was less than 3 which is negligible taking into account the number of duas tracked The overhead was between 10 and 50 for four subjects and only two subjects had overhead   and  and the extra code 
000 000\002\003 004 004\002\003 005 005\002\003 006 006\002\003 007\010\011\012\011\012 013\014\015\016\017\014 
88 


inserted 7  Figure 5 presents the program growth caused in BADUA instrumented programs in comparison to uninstrumented programs With the exception of Scimark2 the memory overhead is around 50 Although that can be considered a sizeable memory footprint a BA-DUA instrumented program can run in the majority of hardware platforms with the possible exception of embedded systems with very tight memory We purposely decided to implement BA in BA-DUA following its description in The idea w as to assess the original proposition of the algorithm However the program growth can be reduced by instrumenting not nodes as in BA-DUA but edges as in JaCoCo This simple strategy will reduce the size of the instrumented program and additionally the execution overhead A comparison with edge monitoring implemented in tools like JaCoCo is valid since dua coverage provided by BA-DUA can be approximately inferred from edge coverage In terms of execution overhead both tools have a similar performance for 7 out of 10 programs The exceptions are Commons-Math 3.2 and Weka r5178 for which JaCoCo is signiìcantly better For three programs BA-DUA performed better Scimark2 XMLSecurity and JTopas BA-DUA instrumented Scimark2 outperforms even the uninstrumented program this performance is possibly due to how the JVM Just-In-Time compiler optimizes register allocations and cache effectiveness for the BA-DUA instrumented code For XML-Security the performance are very similar and BA-DUA advantage is possibly due to the random input data JaCoCo JTopas instrumentated version is hampered by the fact that a method without duas is called a huge amount of times This method is not instrumented by BA-DUA thus it does incur in such an overhead The decision of not to instrument zero-duas methods is because dua coverage does not subsume edge coverage in the presence of infeasible paths  Thus one will not prescind a control-îo w tool if method edge and node coverage are needed The results suggest though that for several programs the execution overhead is similar In applications in which DF testing is most indicated namely security and critical applications approximate intraprocedural dua coverage will hardly be useful The approximate dua coverage can overshoot the precise coverage in up to 30 In these scenarios one is interested in kno wing which paths were not tested thus a precise dua coverage is needed It can be collected by using BA at affordable overhead BA-DUA utilizes a bit to represent a dua whereas JaCoCo uses a boolean to represent an edge node coverage is inferred from edge coverage JaCoCo memory overhead varies from 12 HSLQDB to 19 Scimark2 which can be observed by the almost constant ratio in Figure 5 BA-DUA incurs in higher memory overhead but its growth should be analyzed by the number of entities tracked at run-time Scimark2 which is the worst case with memory overhead of 118 for BA-DUA requires around 6 times more duas to be tracked than edges Thus the memory overhead grows 6 times 7 This is an estimate since we are not taking into account the size of the global array of covered duas created at run-time for each loaded class The external validity of the experiment is arguable on the grounds that our large programs are not large enough One may argue that a 30 KLOC program is not large or even a 200 KLOC program Although systems with millions of lines of code are rather common they are not monolithically coded On the contrary they are divided into manageable components of smaller size for which test suites are developed Our goal was to identify programs that could be part of a system of more than 1 MLOC In this sense the medium size programs are also candidates to be a component of such systems Our data suggests that BA-DUA can be applied to collect dua coverage of these components However it does not allow us to infer that BA-DUA can support integration testing of these huge systems The internal validity of our experiment concerns the results provided by JaCoCo and BA-DUA JaCoCo is widely used in industry which is an indication that its results are trustworthy BA-DUA has been validated with small programs and its results were compared with those provided by JaBUTi BA-DUA performs a more accurate data-îow analysis of bytecodes but the results were similar Regarding conclusion and construction validities the comparison among treatments was straightforward and a modest hardware was utilized to conduct the experiment Regarding this latter point the idea was to better observe the relationship between treatments Had a top hardware conìguration been utilized the results could be more compelling especially for the subjects with execution overhead above 50 VIII C ONCLUSION The goal of this paper was to show that Data-îow DF testing supporting tools can be implemented to tackle large programs DF testing requires tests that traverse a path in which the deìnition of a variable and its subsequent use i.e a deìnition-use association dua is exercised To achieve such a goal a tool called BA-DUA Bitwise Algorithm-powered Deìnition-use Association coverage was implemented BA-DUA utilizes a strategy to track duas at runtime called Bitwise Algorithm BA for intra-procedural dua monitoring BA was recently proposed and was only assessed by means of simulations It is based on a simple idea which consists of encoding a solution for the reaching deìnitions data-îow problem into an object code In this paper  we ha v e described BA and how it is mapped into bytecodes BA-DUA implements such a mapping An experiment was conducted to assess the penalties imposed by BA to track intra-procedural duas at run-time Two kinds of penalties were assessed execution overhead given by the extra time needed to execute the instrumentation code added and memory overhead represented by the growth of the object code due to the insertion of instrumentation code Ten programs with size varying from 600 LOC and 200 KLOC were utilized in the experiment Two programs were classiìed as small less than 2 KLOC three as medium size programs more than 2 KLOC and less than 30 KLOC 
B BA-DUA Costs versus JaCoCo Costs C Threats to Validity 
89 


and ve were categorized as large more than 30 KLOC The subject programs perform mathematical functions data manipulation tasks graphical functions and utilities functions for Java BA-DUA execution overhead varied from negligible to 172 being the average 38 However only two subjects in ten programs had their performance impacted more than 50 for four programs the overhead was less than 3 In terms of program growth the BA memory overhead ranges from 33 to 118 being the average 61 BA-DUA performance was also compared with that of the JaCoCo toolÑa widely used control-îow testing supporting tool In terms of execution overhead both tools have a similar performance for 7 out of 10 programs JaCoCo has a smaller memory footprint but tracks fewer entities at run-time We purposely implemented BA in BA-DUA following its original proposition The idea was to assess BA as it is The results suggest that by using BA to track duas the overheads especially the execution overhead are signiìcantly reduced As a result a larger class of program are able to be assessed by intra-procedural DF testing BA-DUA is available for download and use at https://github.com/saeg/ba-dua Nevertheless there is room for improvement The program growth and slowdown can be reduced by instrumenting edges instead of nodes However the BA main drawback are methods with a high concentration of duas since they require more code to be inserted causing higher execution and memory overheads We intend to investigate how the subsumption relationship among duas can be utilized to determine a minimal set of duas to be tracked at run-time We conjecture that in methods with many duas this minimal set is relatively small Other avenues of research encompass the extension of the BA algorithm to track inter-procedural duas and to tackle programs with multiple threads As a concluding remark we hope that the results presented in this paper encourage vendors to consider including DF testing in their set of testing tools A CKNOWLEDGMENT The authors acknowledge the support granted by the CAPES FAPESP to the INCT-SEC National Institute of Science and Technology  Critical Embedded Systems Brazil processes 573963/2008-9 and 08/57870-9 and by the Research Support Center on Open Source Software NAPSoL of the University of Sao Paulo R EFERENCES  D G Feitelson E Frachtenber g and K L Beck De v elopment and deployment at facebook 
 vol 17 no 4 pp 8Ö17 August 2013  S Grimm F acebook engineering What kind of automated testing does facebook do On A v ailable http://www quora.com/F acebookEngineering What-kind-of-automated-testing-does-Facebook-do  S Rapps and E J W e yuk er  Selecting softw are test data using data ow information  vol 11 no 4 pp 367Ö375 Apr 1985  K Moir  Releng of the nerds Open source release engineering sDK code coverage with JaCoCo  A v ailable http://relengofthenerds.blogspot.com.br/2011/03 sdk-code-coverage-with-jacoco.html  M Hutchins H F oster  T  Goradia and T  Ostrand Experiments of the effectiveness of dataîowand controlîow-based test adequacy criteria in  ser ICSE 94 1994 pp 191Ö200  P  G Frankl and O Iak ounenk o Further empirical studies of test efìctiveness in  ser FSE 98 1998 pp 153Ö162  T B Dao and E Shibayama Security sensiti v e data o w co v erage criterion for automatic security testing of web applications in  ser ESSoS 2011 pp 101Ö113  R Santelices J A Jones Y  Y u and M J Harrold Lightweight fault-localization using multiple coverage types in  ser ICSE 09 2009 pp 56Ö66  J Misurda J A Clause J L Reed B R Childers and M L Sof f a Demand-driven structural testing with dynamic instrumentation in  ser ICSE 05 2005 pp 156Ö165  R Santelices and M J Harrold Ef ciently monitoring data-îo w test coverage in  ser ASE 07 2007 pp 343Ö352  M L Chaim and R P  A de Araujo  An ef cient bitwise algorithm for intra-procedural data-îow testing coverage  vol 113 no 8 pp 293Ö300 2013  P  G Frankl The use of data o w information for the selection and evaluation of software test data Ph.D dissertation New York University New York October 1987  T  J Ostrand and E J W e yuk er  Data o w-based test adequac y analysis for languages with pointers in  ser TAV4 1991 pp 74Ö86  J R Hor gan and S London  A data o w co v erage testing tool for C  in  1992 pp 2Ö10  P  G Frankl and E J W e yuk er   An applicable f amily of data o w testing criteria  vol 14 no 10 pp 1483Ö1498 Oct 1988  M L Chaim POKE-T OOL  A tool to support structural program testing based on data ow analysis Masterês thesis School of Electrical and Computer Engineering State University of Campinas Campinas SP Brazil 1991 in Portuguese  A M R V incenzi J C Maldonado W  E W ong and M E Delamaro Coverage testing of java programs and components  vol 56 no 1-2 pp 211Ö230 Apr 2005  J Misurda J Clause J Reed B R Childers and M L Sof f a Jazz a tool for demand-driven structural testing in  ser CCê05 2005 pp 242Ö245  I Bluemk e and A Arthur Rembisze wski Dataîo w approach to testing java programs in  ser DEPCOSRELCOMEX 09 2009 pp 69Ö76  Q Y ang J J Li and D M W eiss  A surv e y of co v erage-based testing tools  vol 52 no 5 pp 589Ö597 2009  M M Hassan and J H Andre ws Comparing multi-point stride co v er age and dataîow coverage in  ser ICSE 13 2013 pp 172Ö181  A V  Aho M S Lam R Sethi and J D Ullman  2nd ed Boston Pearson AddisonWesley 2007  M L Chaim and R P  A de Araujo Proof of correctness of the bitwise algorithm for intra-procedural data-îow testing coverage PPgSI-001/2013 School of Arts Sciences and Humanities University of Sao Paulo Tech Rep 2013 available at http://ppgsi.each.usp.br relatorios-tecnicos-2013  M Marr  e and A Bertolino Using spanning sets for coverage testing  vol 29 no 11 pp 974Ö984 2003 
IEEE Internet Computing IEEE Trans on Software Eng Proc of the 16th International Conference on Software Engineering Proc of the ACM SIGSOFT Foundations of Software Engineering Conference Engineering Secure Software and Systems Proc of the 31st International Conference on Software Engineering Proc of the 27th International Conference on Software Engineering Proc of the 22nd IEEE/ACM International Conference on Automated Software Engineering Inf Process Lett Proc of the Symposium on Testing Analysis and Veriìcation Proc of Symposium on Assessment of Quality Software Development Tools IEEE Trans on Software Eng Science of Computer Programming Proc of the 14th International Conference on Compiler Construction Proc of the 2009 IEEE Fourth International Conference on Dependability of Computer Systems Comput J Proc of 35th International Conference on Software Engineering Compiler s principles techniques and tools IEEE Trans Software Eng 
90 


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


