Abstract 
High-Performance RDMA-based Design of Hadoop MapReduce over InìniBand 
  
Md Wasi-ur-Rahman Nusrat Sharmin Islam Xiaoyi Lu Jithin Jose Hari Subramoni Hao Wang and Dhabaleswar K DK Panda 
Department of Computer Science and Engineering The Ohio State University 
MapReduce is a very popular programming model used to handle large datasets in enterprise data centers and clouds Although various implementations of MapReduce exist Hadoop MapReduce is the most widely used in large 
rahmanmd islamn luxi jose subramon wangh panda cse.ohio-state.edu 
data centers like Facebook Yahoo and Amazon due to its portability and fault tolerance Network performance plays a key role in determining the performance of data intensive applications using Hadoop MapReduce as data required by the map and reduce processes can be distributed across the cluster In this context data center designers have been looking at high performance interconnects such as InìniBand to enhance the performance of their Hadoop MapReduce based applications However achieving better performance through usage of high performance interconnects like InìniBand is a signiìcant task It requires a careful redesign of communication framework 
inside MapReduce Several assumptions made for current socket based communication in the current framework do not hold true for high performance interconnects In this paper we propose the design of an RDMA-based Hadoop MapReduce over InìniBand and several design elements data shufîe over InìniBand in-memory merge mechanism for the Reducer and pre-fetch data for the Mapper We perform our experiments on native InìniBand using Remote Direct Memory Access RDMA and compare our results with that of Hadoop-A and default Hadoop over different interconnects and protocols For all these experiments we perform network level parameter 
tuning and use optimum values for each Hadoop design Our performance results show that for a 100 GB TeraSort running on an eight node cluster we achieve a performance improvement of 32 over IP-over InìniBand IPoIB and 21 over Hadoop-A With multiple disks per node this beneìt rises up to 39 over IPoIB and 31 over Hadoop-A 
I I NTRODUCTION Hadoop is one of the most popular open-source frameworks to handle big data analytic applications It provides support for MapReduce programming model and Hadoop Distributed FileSystem HDFS for big data 
storage Hadoop is being used in many research projects and many companies such as Facebook Yahoo etc due to its scalability fault-tolerance and productivity The Hadoop MapReduce framework provides application developers several critical functions such as key-based This research is supported in part by National Science Foundation grants CCF-0937842 OCI-0926691 OCI-1148371 and CCF-1213084 sorting multi-way merging data shufîing etc When the MapTasks nish the user deìned map operation and set keyvalue pairs into context for the ReduceTasks the framework executes the shufîe operation which sends key-value pairs from the MapTasks to the appropriate ReduceTask It also 
performs the merge of the key-value pairs from multiple mappers on the reducer side As the size of the dataset increases the entire operation becomes very communicationintensive As big data applications begin scaling to tera bytes and peta bytes of data the socket based communication model which is the default implementation in Hadoop MapReduce demonstrates performance bottleneck To avoid this potential bottleneck designers of high end enterprise data centers and clouds have been looking towards high performance interconnects such as InìniBand to allow the unimpeded scaling of their big data applications The Greenplum Analytics Workbench a 1000 node InìniBand based 
cluster is one of the latest in a series of clusters being deployed to design develop and test InìniBand based solutions for the Hadoop ecosystem Although such systems are being used the current Hadoop middleware components do not leverage the high performance communication features offered by InìniBand Without high performance InìniBand support in the vanilla Hadoop system the big data analytic applications using Hadoop cannot get the best performance on such systems Recent research works 1 analyze on the huge performance improvements possible for different cloud computing middlewares using InìniBand networks The research project Hadoop-A pro vides nati v e IB v erbs support for 
data shufîe in Hadoop MapReduce on InìniBand network However it does not exploit all the potential performance beneìts of high performance networks As a result it is necessary to rethink the Hadoop system design when a high performance network is available This leads to the following research challenges 1 How do we re-design Hadoop MapReduce to take advantage of high performance networks such as InìniBand and exploit advanced features such as RDMA 2 What will be the performance improvement of 
2013 IEEE 27th International Symposium on Parallel & Distributed Processing Workshops and PhD Forum 978-0-7695-4979-8/13 $26.00 © 2013 IEEE DOI 10.1109/IPDPSW.2013.238 1908 


MapReduce applications with the new RDMA based design in Hadoop MapReduce over InìniBand 3 Can the new design lead to consistent performance improvement on different hardware conìgurations such as with multiple HDD or SSD on the compute node of Hadoop cluster In this paper we address these research challenges First we present a detailed RDMA-based design of MapReduce shufîe engine to provide faster data communication We use Uniìed Communication Runtime UCR a light-weight communication library to provide faster availability of the data Second we design and implement an efìcient keyvalue pair pre-fetching and caching mechanism inside the TaskTracker of Hadoop which is responsible for fast data shufîe This mechanism helps reduce the overhead in the reducer side when the shufîe and the merge procedures run in an overlapping manner Finally we investigate the advantage of increasing disk bandwidth for our design using multiple HDD and SSD conìgurations in Hadoop DataNodes We perform extensive performance evaluation with popular benchmarks used in Hadoop literature We compare our performance with that of 1 GigE IP-over-InìniBand IPoIB 10 GigE and Hadoop-A These e v aluations show that for TeraSort benchmark our implementation of Hadoop MapReduce over InìniBand achieves 21 and 32 beneìt in execution time over Hadoop-A and IPoIB 32Gbps respectively for 100GB sort With multiple disks deployed in our design improvement increases up to 31 over Hadoop-A and 39 over IPoIB Also it outperforms Hadoop-A by 32 in regular Sort benchmark with 40GB sort size The rest of the paper is organized into sections in section II we present background about the key components involved in our design in section III we provide the detailed design for both default MapReduce and RDMA-based approach and in section IV we describe our experiments and evaluations Related works are discussed in section V and in section VI we present conclusions and future work II B ACKGROUND Hadoop is a popular open source implementation of the MapReduce programming model The Hadoop Distributed File System HDFS 2 is the primary storage for Hadoop cluster An HDFS cluster consists of two types of nodes NameNode and DataNode The NameNode manages the le system namespace and the DataNodes store the actual data Figure 1\(a shows a typical MapReduce cluster The NameNode has a JobTracker process and all the DataNodes can run one or more TaskTracker processes These processes together act as a master-slave architecture for a MapReduce job A MapReduce job usually consists of three basic stages map shufîe/merge/sort and reduce Figure 1\(b shows these stages in details JobTracker is the service within Hadoop that farms out tasks to speciìc nodes in the cluster A single JobTracker and a number of TaskTrackers are responsible for successful completion of a MapReduce job Each TaskTracker can launch several MapTasks one per split of data The map function converts the original records into intermediate results and stores them onto the local le system Each of these les are sorted into many data partitions one per ReduceTask The JobTracker then launches the ReduceTasks as soon as the map outputs are available from the MapTasks TaskTracker can spawn several concurrent ReduceTasks Each ReduceTask starts fetching the map outputs from the map output locations that are already completed This stage is the shufîe/merge period where the data from various map output locations are sent and received via HTTP requests and responses While receiving these data from various locations a merge algorithm is run to merge these data to be used as an input for the reduce operation Then each ReduceTask loads and processes the merged outputs using the user deìned reduce function The nal result is then stored into HDFS Some popular MapReduce benchmarks from Apache Hadoop community are mentioned here TeraSort is probably the most wellknown Hadoop benchmark It is a benchmark that combines testing the HDFS and MapReduce layers of a Hadoop cluster The input data for TeraSort can be generated by TeraGen tool which writes the desired number of ro ws of data in the input directory By default the key and value size is xed for this benchmark at 100 bytes The output of the TeraSort can be validated by a TeraValidate tool which is also implemented in Hadoop trunk The Sort benchmark uses the MapReduce framework to sort the input directory into the output directory The input of this benchmark can be generated by RandomWriter which writes random-sized k e y v alue pairs in HDFS This benchmark is a very useful tool to measure the performance efìciency of MapReduce cluster In this section we present an overview of different networking technologies that can be utilized in a data center for high-performance communication During the past decade the eld of High-Performance Computing HPC has been witnessing a transition to commodity clusters with modern interconnects such as InìniBand and 10 Gigabit Ethernet InìniBand is an industry standard switched fabric that is designed for interconnecting nodes in HPC clusters It is a high-speed general purpose I/O interconnect that is widely used by scientiìc computing centers world-wide The recently released TOP500 rankings 
A Hadoop MapReduce and Benchmarks 1 TeraSort 2 Sort B High Performance Networks 1 InìniBand 
1909 


ib0 ib1 
InìniBand Verbs Layer InìniBand IP Layer 
 The layer is the lowest access layer to InìniBand Verbs are used to transfer data and are completely OS-bypassed i.e there are no intermediate copies in the OS The verbs interface follows the Queue Pair or communication end-points model Upperlevel software using verbs can place a work request on a queue pair The work request is then processed by the Host Channel Adapter HCA When work is completed a completion notiìcation is placed on the completion queue Upper level software can detect completion by polling the completion queue or by asynchronous events interrupts The OpenFabrics interface is the most popular v erbs access layer due to its applicability to various InìniBand vendors b InìniBand software stacks such as OpenFabrics pro vide dri v er for implementing the IP layer This makes it possible to use the InìniBand device as just another network interface available from the system with an IP address Such IB devices are presented as and so on just like other Ethernet IP interfaces Although the verbs layer in InìniBand provides OS-bypass the IP layer does not provide so This layer is often called IP-over-IB or IPoIB for short We use this terminology in this paper In data center environments to achieve better performance with respect to higher bandwidth 10 Gigabit Ethernet is typically used It is also realized that traditional sockets interface may not be able to support high communication rates T o w ards that ef fort iW ARP Internet Wide Area RDMA Protocol standard was introduced for performing RDMA over TCP/IP In f act the OpenFabrics netw ork stack pro vides a uniìed interf ace for both iWARP and InìniBand In addition to iWARP there are also hardware accelerated versions of TCP/IP available These are called TCP Ofîoad Engines TOE which use hardware ofîoad Solid State Drives have amassed a lot of attention over the recent past owing to signiìcant data-throughput and efìciency gains over traditional spinning-disks Although it is a mere physical array of fast ash-memory packages the core-intelligence of an SSD can be attributed to its Flash Translation Layer FTL which plays a vital role in the adoption of this technology Some of major functionality of an SSD such as Wear leveling Garbage collection and Logical Block Mapping is packed into the FTL High bandwidth as well as low latency makes SSD an ideal candidate to be used in the DataNodes of the Hadoop cluster in order to lessen the I/O bottlenecks for MapReduce applications The U niìed C ommunication R untime UCR is a light-weight high performance communication runtime designed and developed at The Ohio State University It aims to unify the communication runtime requirements of scientiìc parallel programming models such as MPI and Partitioned 
002\003\004\005\006\007\010\011\012\006 005\007\013\011\005\006\007\010\011\012\006 005\007\013\011\005\006\007\010\011\012\006 005\007\013\011\005\006\007\010\011\012\006 005\007\013\011\005\006\007\010\011\012\006 014\007\015\012\014\003\016\012 017\007\020\007\014\003\016\012 017\007\020\007\014\003\016\012 017\007\020\007\014\003\016\012 017\007\020\007\014\003\016\012 021\022\023\012\024\020 
025\026\022\023\020\027\030 015\007\026 025\003\006\020 025\026\022\023\020\027\031 015\007\026 025\003\006\020 025\026\022\023\020\027\032 015\007\026 025\003\006\020 033\007\006\020\027\030 006\012\016\034\010\012 015\012\006\035\012 033\007\006\020\027\031 006\012\016\034\010\012 015\012\006\035\012 025\036\034\037\037\022\012 024\026\034\020\027 017"\025 034\020\026\034\020\027 017"\025 017"\025\027 012\026\022\023\010\007\020\023\003\024 017"\025\027 012\026\022\023\010\007\020\023\003\024 
verbs 2 10 Gigabit Ethernet C Solid State Drive SSD Overview D Uniìed Communication Runtime UCR 
a High-level overview of Hadoop MapReduce architecture b MapReduce data ow with multiple map and reduce tasks Figure 1 Overview of MapReduce and its components in November 2012 indicate that more than 44 of the computing systems use InìniBand as their primary interconnect One of the main features of InìniBand is Remote Direct Memory Access RDMA This feature allows software to remotely read or update memory contents of another remote process without any software involvement at the remote side This feature is very powerful and can be used to implement high-performance communication protocols InìniBand offers various software layers through which it can be accessed described below a 
1910 


Global Address Space PGAS along with those of datacenter middle-ware such as Memcached HBase 20 MapReduce etc UCR is designed as a native library to extract high performance from advanced network technologies The design of UCR has evolved from MVAPICH and MVAPICH2 software stack MVAPICH2 and MVAPICH2-X are popular implementations of MPI-2 speciìcation with initial support for MPI-3 These libraries are being used by more than 2,000 organizations in 70 countries and also distributed by popular InìniBand software stacks and Linux distributions like RedHat and SUSE III E XISTING AND P ROPOSED D ESIGN In this section we rst discuss the default design of Hadoop MapReduce and its various components Then we propose our design for MapReduce that takes the beneìts of RDMA over InìniBand We discuss the key components of this design and describe the related challenges and our approach to nd the solution We also contrast our design with that of Hadoop-A in Section III-C Figure 2 shows a hybrid version of MapReduce consisting of both the default vanilla design and our proposed RDMAbased design Here we keep the existing design and add our RDMA-based communication features as a conìgurable option with a selection parameter  With this parameter set to true user can enable RDMAbased features and vice versa Figure 2 shows the hybrid version of MapReduce which has both the existing design and our proposed RDMAbased approach As discussed in Section II-A in the default MapReduce workîow shown in Figure 1\(b map outputs are kept in les residing in local disks which can be accessed by TaskTrackers During shufîe reduce processes that are running in ReduceTasks ask TaskTrackers to provide these map outputs over HTTP All the related components that are responsible for these operations are shown in the left side of both ReduceTask and TaskTracker in Figure 2 Some of these components are described here  HTTP Servlets run in TaskTracker to handle incoming requests from the ReduceTasks Upon HTTP request the servlets get the appropriate map output le from local disk and send the output in an HTTP response message  Inside the ReduceTask the copiers are responsible for requesting appropriate map outputs from TaskTracker in an HTTP request message Upon arrival of the data it keeps the data in memory if a sufìcient amount of memory is available or in a local disk otherwise  Inside ReduceTask there are two level of Merge operations If the data received during shufîe can be held in memory the in-memory merger merges these data from memory and then keeps the merged output to local disk Otherwise only the Local FS Merger is used  Local FS Merger gets the data from local disk which are kept either by Copier or In-Memory Merger It merges and stores these data in local disk in an iterative manner minimizing the total number of merged output les in local disk each time  Inside ReduceTask a Map Completion Fetcher keeps track of currently completed map tasks After a map completion event occurs it signals the Copiers to start copying the data for this map output by sending out HTTP request In the existing implementation of MapReduce TaskTracker and ReduceTask exchange their information over Java Sockets Each HTTP Servlet sends an acknowledgment to the appropriate reducer with the meta information for the data requested and then begins sending data packets one after another ReduceTask after receiving the acknowledgment extracts the meta information and sets appropriate parameters and buffers for receiving the data packets next We introduce the major components in our RDMA based MapReduce design and then explain our design details We rst discuss our updated Shufîe design followed by the Merge design In the shufîe stage both TaskTracker and ReduceTask are modiìed to achieve the beneìts of RDMA We have added the following new components in the TaskTracker side  Each TaskTracker initiates an RDMAListener during its startup RDMAListener in TaskTracker waits for incoming connection requests from the ReduceTask side adds the connection to a pre-established queue and starts an RDMAReceiver if necessary We use UCR as the library for communication which is an end-point based library An end-point is analogous to socket connection RDMAListener provides end-point for UCR communication and allocates an appropriate buffer for each end-point  Each RDMAReceiver is responsible for receiving requests from ReduceTasks RDMAReceiver gets the end-point list that are currently being used and receives request from those end-points After receiving the request it places the request in DataRequestQueue 
mapred.rdma.enabled 
A Existing Design HTTP Servlet Copier In-Memory Merger Local FS Merger Map Completion Fetcher B Proposed Design 1  RDMAListener RDMAReceiver 
RDMA based Shufîe 
1911 


Faster Merge 
DataRequestQueue RDMAResponder RDMACopier 2  
 DataRequestQueue is used to hold all the requests from ReduceTasks It gets these requests from RDMAReceiver and stores it until one of the RDMAResponders take it for further processing  RDMAResponder belongs to a pool of threads that wait on DataRequestQueue for incoming requests Whenever a new request gets inserted in Data Request Queue one of the RDMAResponders responds to that request It is a very light-weight thread and after sending the response it immediately goes to wait state if no other requests are available The ReduceTask side is also enhanced by adding RDMACopier  In the default design of MapReduce the copier threads are responsible for requesting data to TaskTracker and storing data for Merge Likewise RDMACopier sends requests to TaskTrackers and upon arrival of data it stores data to DataToMergeQueue for merge operation Both TaskTracker and ReduceTask have which enables the Java code to make use of UCR library functions implemented in native C for communication Initially RDMACopier sends end point information to RDMAListener in TaskTracker to establish the connection From each ReduceTask one RDMACopier sends such information to all available TaskTrackers After a successful map completion event detection by Map Completion Fetcher in the ReduceTask side RDMACopier sends data request to the TaskTracker for this mapês output RDMAReceiver gets this request and puts the request into DataRequestQueue RDMAResponder then responds with this request and extracts the required data from PrefetchCache It then sends the data to the ReduceTask using the same end point For successful and reliable transmission of data each request and response messages consist of various identiìcation and control parameters such as map id reduce id job id number of key value pairs sent etc All these data in shufîe is transferred using RDMA over InìniBand to leverage the beneìts of high performance interconnect in terms of latency and throughput In the default design of MapReduce each HTTP response consists of the entire map output le dividing it into packets of the default packet size 64 KB With the use of high performance networks the communication time reduces signiìcantly which in turn creates the opportunity to transfer one map output le in multiple communication steps instead of one By doing this we can start the merge process as soon as some key-value pairs from all map output les reach at the reducer side For this reason in our design RDMAResponder in TaskTracker 
002\007%\007\027\025\003\010\011\012\020\013\027 \024\020\012\006\037\007\010\012 031&\023\035'\(\031\030&\023\035'\027\014\012\020 002\014 \027*\016\007\026\020\023%\012\027 \024\020\012\006\037\007\010\012 021 027-\012\006\004\013 002\007%\007\027\025\003\010\011\012\020\013\027 \024\020\012\006\037\007\010\012 031&\023\035'\(\031\030&\023\035'\027\014\012\020 002\014 \027*\016\007\026\020\023%\012\027 \024\020\012\006\037\007\010\012 021 027-\012\006\004\013 005\007\013\011\005\006\007\010\011\012\006 012\016\034\010\012\005\007\013\011 005\005\033\027\025\012\006%\022\012\020 007\026\027#\034\020\026\034\020\027 033\006\012\037\012\020\010\036\012\006 017.*\027 012\013\026\003\024\016\012\006 017.*\027 023\013\020\012\024\012\006 017.*\027 012\010\012\023%\012\006 007\026\027\021\003\015\026\0220\027 012\020\010\036\012\006 021\003\026\023\012\006 007\026\027\021\003\015\026\0220\027 012\020\010\036\012\006 017.*\027 021\003\026\023\012\006 012\016\034\010\012 012\016\034\010\012 024\027 012\015\003\0061\027 012\006\035\012\006 024\027 012\015\003\0061\027 012\006\035\012\006 003\010\007\022\027"\025\027 012\006\035\012\006 033\006\012\037\012\020\010\036\027 021\007\010\036\012 017\007\020\007\027$\0122\034\012\013\020\027 3\034\012\034\012 024\020\012\006\015\012\016\023\007\020\012\027\017\007\020\007\027\017\023\006 003\010\007\022\027\017\007\020\007\027\017\023\006 003\010\007\020\023\003\024\0273\034\012\034\012 003\010\007\020\023\003\024\0273\034\012\034\012 017\007\020\007\005\003.\012\006\035\012\027 3\034\012\034\012 017\007\020\007\005\003$\012\016\034\010\012\027 3\034\012\034\012 
JNI Adaptive Interface 
Figure 2 RDMA-based MapReduce architecture and communication characteristics 
1912 


side sends a certain number of key-value pairs starting from the beginning of map output le instead of sending the entire map output le at once While receiving these key-value pairs from all map locations a ReduceTask now merges all these data to build up a Priority Queue It then keeps extracting the key-value pairs from the Priority Queue in sorted order and puts these data in a rst in rst out structure named as DataToReduceQueue As each map output le is already sorted in the mapper side the merger in the ReduceTask can only extract the data from Priority Queue until the point when the number of key-value pairs from a particular map decreases to zero At that point it needs to get next set of key-value pairs from that particular map task to resume extracting from Priority Queue For faster response in TaskTracker side we propose an efìcient caching mechanism for the intermediate data residing in map output les stored in local disk User can enable caching in TaskTracker side through a conìguration parameter  The following is a new component in the TaskTracker side for caching  MapOutputPrefetcher is a daemon threadpool which caches intermediate map output as soon as it gets available After nishing a map task one of the daemons starts to fetch the data from this map output and caches it in PrefetchCache The novel feature for this cache is that it can adjust caching based on data availability and necessity It can also prioritize which data to cache more frequently based on the demand from the ReduceTasks Depending on heap size availability it can limit the amount of data to be cached in PrefetchCache Even with caching cache misses may occur as ReduceTasks requests may arrive faster than caching In that case TaskTracker fetches data directly from disk itself without waiting for caching But after disk fetch it requests MapOutputPrefetcher to cache this particular map output data with more priority so that successive requests for this output le can be served from the cache Intermediate data pre-fetching and caching plays an important role for achieving better performance with respect to job execution time For large number of ReduceTasks more requests for map output les arrive to a single TaskTracker which can swamp out the I/O bandwidth in TaskTracker side So an efìcient cache implemented here can alleviate such bottleneck and provide better performance In Section IV-D we show how our caching mechanism provides signiìcant performance improvement Figure 3 shows our design which enhances performance by efìciently overlapping shufîe merge and reduce operations in ReduceTask In the default design the merge process starts immediately with shufîe However there exists an implicit barrier for the start of reduce operation the reduce operations can only be started after all the merge operations have completed In our design we start reduce operation as soon as the rst merge completes In this way we can achieve maximum beneìt by introducing pipelining between merge and reduce stages Here we distinguish our proposed RDMA-based design for MapReduce with that of Hadoop-A Some major differences are as follows 1  with RDMA-based communication in the shufîe stage we redesign the merge mechanism to get more overlapping of the reduce stage with the shufîe and merge stages In order to respond to the key-value pairs requested from the reducer as soon as possible we design the intermediate data pre-fetching and caching mechanism in Hadoop TaskTrackers On the other hand although Hadoop-A has proposed a DataEngine tool for intermediate data access on the disk of the mapper side DataEngine doesnêt provide data caching to decrease the disk access based on the paper In section IV we compare our design with Hadoop-A The better performance of our design is a result of the data pre-fetching and caching mechanism 2  HadoopA has implemented a separate merge algorithm and the DataEngine tool in native C and has provided a plug-in based approach to provide RDMA operations on both TaskTracker and ReduceTask side However in order to provide minimal code changes in the existing Hadoop codebase our implementation for merge algorithm still makes use of the existing Merge operation with minimal code changes And the data pre-fetching and caching mechanism is implemented as a daemon in TaskTracker side which does not require any separate tools 3  Between these two RDMA based implementations our design has more user level exibility in terms of conìguration and tuning We provide a number of conìguration parameters such as RDMA packet size enabling of caching number of key value pairs transmitted in each packet etc to give user the exibility and options of choosing the best conìguration for a particular workload Tuning of these parameters can also play a major role on achieving better performance in terms of job execution time Section IV-C shows one of these examples where better parameter tuning achieves better beneìts On the other hand we donêt nd the similar interfaces in Hadoop-A implementation 
3  MapOutputPrefetcher 4  C Differences with respect to Hadoop-A Data Pre-fetching and Caching JAVA Implementation vs C Implementation Conìguration and Tuning Interfaces 
Intermediate Data Pre-fetching and Caching Overlap of Shufîe Merge and Reduce 
mapred.local.caching.enabled 
1913 


017\012\037\007\034\022\020\027.\007\026$\012\016\034\010\012\027 017.*4\004\007\013\012\016\027.\007\026$\012\016\034\010\012\027 
We have used an Intel Westmere cluster for our evaluations This cluster consists of compute nodes with Intel Westmere series of processors using Xeon Dual quad-core processor nodes operating at 2.67 GHz with 12GB RAM and 160GB HDD Each node is equipped with MT26428 QDR ConnectX HCAs 32 Gbps data rate with PCIEx Gen2 interfaces The nodes are interconnected using a Mellanox QDR switch Each node runs Red Hat Enterprise Linux Server release 6.1 Santiago at kernel version 2.6.32131 with OpenFabrics version 1.5.3 This cluster also has dedicated storage nodes with the same conìguration but with 24GB of RAM each Additionally eight of the storage nodes are equipped with two 1TB HDD each Four of the storage nodes also have Chelsio T320 10GbE Dual Port Adapters with TCP Ofîoad capabilities In the gures presented in this section we have mentioned OSU-IB to indicate our RDMA-based design of MapReduce and Hadoop-A to indicate the design in 32 Gbps indicates InìniBand QDR card speed We have performed this experiment in 10 GigE IPoIB 32 Gbps with vanilla Hadoop and Hadoop-A and compared the results with our design For this experiment we have found that the optimal HDFS block-size for 10 GigE IPoIB 32 Gbps and our design is 256 MB whereas it is 128 MB for Hadoop-A We have used TeraGen to generate the input data for TeraSort Figure 4 shows the job execution times of the TeraSort benchmark in a four-DataNode cluster In the experiments for Figure 4\(a we show performance results with single and dual HDDs for each interconnect For single HDD 30 GB sort size our design reduces the job execution time by 9 over Hadoop-A 32 Gbps 35 over IPoIB 32 Gbps and 38 over 10 GigE Compared with IPoIB and 10 GigE our design uses native IB verbs communication for the data shufîe which is much better than the socket based communication on IPoIB and 10 GigE Although HadoopA also uses native IB verbs communication the data prefetching and caching mechanism of our design improves the performance On the other hand if two HDDs are used per node our design improves the execution time by 13 over HadoopA 32 Gbps 38 over IPoIB 32 Gbps and 43 over 10 GigE for the same sort size For 40 GB sort size our design achieves an improvement of 17 48 and 51 over Hadoop-A 32 Gbps IPoIB 32 Gbps and 10 GigE respectively When multiple HDDs are used per node the performance bottleneck of the local disk read and write bandwidth is alleviated Compared to Hadoop-A our design can utilize the improved bandwidth more efìciently to overlap the data shufîe merge and reduce further as illustrated in section III-B We have performed similar experiments with the TeraSort benchmark using eight DataNodes In this case we varied the sort size from 60 GB to 100 GB As shown in Figure 4\(b our design reduces the job execution time 21 over Hadoop-A 32 Gbps for 100 GB sort size with single 
Figure 3 Overlapping of different processes in MapReduce workîow IV P ERFORMANCE E VALUATION In this section we present the detailed performance evaluations of our RDMA-based design of Hadoop MapReduce and its impact on different Hadoop benchmarks We compare the performance of our design with socket based interconnects 10 GigE and IPoIB and Hadoop-A W e ha v e performed the experiments on different storage platforms single/multiple HDDs or SSD per node in order to illustrate the effect of I/O on our design In this study we perform the following set of experiments 1 Evaluation with the TeraSort benchmark and 2 Evaluation with the Sort benchmark These benchmarks are described in section II For each of the benchmarks we have identiìed the optimal values of HDFS block-size for different interconnects as well as for Hadoop-A and our design Additionally in our experimental setup we have also determined that four is the maximum number of map and reduce tasks that can be run simultaneously to achieve the optimal performance by a TaskTracker In all our experiments we have used Hadoop 0.20.2 Hadoop-A and JDK 1.7.0 
002\003\004 005\006\007\010\010\011\012 002\012\013\014\012 015\012\016\007\017\012 020\021\004\011\022\017\022\023\024\025\003\013\013\022\012\013 002\003\004 005\006\007\010\010\011\012 002\012\013\014\012 015\012\016\007\017\012 020\021\004\011\022\017\022\023\024\025\003\013\013\022\012\013 
A Experimental Setup B Evaluation with the TeraSort Benchmark 
1914 


      
0 500 1,000 1,500 2,000 2,500 40 30 20 Job Executiion Time \(sec Sort Size \(GB 10GigEä1disk 10GigEä2disks IPoIBä1disk \(32Gbps IPoIBä2disks \(32Gbps HadoopAäIBä1disk \(32Gbps HadoopAäIBä2disks \(32Gbps OSUäIBä1disk \(32Gbps OSUäIBä2disks \(32Gbps 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 100 80 60 Job Executiion Time \(sec Sort Size \(GB 1GigEä1disk 1GigEä2disks IPoIBä1disk \(32Gbps IPoIBä2disks \(32Gbps HadoopAäIBä1disk \(32Gbps HadoopAäIBä2disks \(32Gbps OSUäIBä1disk \(32Gbps OSUäIBä2disks \(32Gbps 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 200GBä24nodes 100GBä12nodes Job Executiion Time \(sec Sort Size 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 50 100 150 200 250 300 350 400 450 500 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 
C Evaluation with the Sort Benchmark 
a Total job execution times in 4 nodes cluster          b Total job execution times in 8 nodes cluster Figure 4 TeraSort benchmark evaluation HDD From Figure 4\(b we observe that with two HDDs per node our design achieves an improvement of 31 over Hadoop-A 32 Gbps          Figure 5 TeraSort benchmark evaluation with larger sort size We have also evaluated TeraSort with larger sized cluster In this case we have varied the sort size from 100 GB to 200 GB with 12 and 24 compute nodes in the cluster respectively Figure 5 shows these results For 100 GB sort size we achieve 41 beneìt over IPoIB 32Gbps and 7 beneìt over Hadoop-A 32Gbps For 200 GB sort size also we achieve similar beneìts From Figure 4 and Figure 5 we observe that Hadoop-A performs better with respect to IPoIB in a bigger cluster with the same sort size whereas our implementation achieves better performance in both cases compared to Hadoop-A As in our setup storage nodes have twice as much memory as compute nodes our implementation has more beneìts in storage nodes compared to those in compute nodes This clearly depicts the efìciency of the caching mechanism implemented in our design Figure 7 Sort benchmark evaluation with SSD Figure 6\(a shows the performance results of the Sort benchmark using four DataNodes For this we have used four compute nodes in our cluster Each DataNode has single HDD per node In this case our design reduces the job execution time by 26 over IPoIB 32 Gbps and 38 over Hadoop-A for 20 GB sort From Figure 6\(b we observe that with eight DataNodes our design can achieve an improvement of 27 over IPoIB 32 Gbps and 32 over Hadoop-A Compared with the TeraSort benchmark the difference in the Sort benchmark is the variable size of the key-value pairs In Sort the combined length of key-value pairs can be as large as 20,000 bytes From the results we can observe that Hadoop-A performs worse than IPoIB even after conìguring all the tunable parameters with optimum values as mentioned in Hadoop-A release It re v eals that only substituting the socket based communication with the native IB verbs some applications such as the Sort benchmark cannot get better performance due to the inefìciency in number of keyvalue pairs transferred each time that also affects proper overlapping between all the stages Our design with the efìcient caching mechanism can get better performance in 
We perform the regular Sort benchmark in 1 GigE IPoIB 32 Gbps with vanilla Hadoop and Hadoop-A and compare the results with our design For this experiment the optimal value of HDFS block-size is 64 MB We have used RandomWriter to generate the input data for the Sort benchmark            
1915 


        
0 200 400 600 800 1,000 1,200 1,400 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 200 400 600 800 1,000 1,200 1,400 40 35 30 25 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 100 200 300 400 500 600 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB IPoIB OSUäIB \(Without Caching Enabled OSUäIB \(With Caching Enabled 
D Beneìts of Caching 
a Total job execution times in 4 nodes cluster         b Total job execution times in 8 nodes cluster Figure 6 Sort benchmark evaluation both these cases as it considers the size of the key-value pair before the transfer We also evaluate Sort benchmark using SSD as HDFS data stores Figure 7 shows the comparison for this evaluation In this case our design achieves a beneìt of 22 over Hadoop-A 32Gbps and 46 over IPoIB 32Gbps in job execution time for 15 GB sort Figure 8 Effect of Caching Mechanism Figure 8 shows the performance comparison between caching enabled and caching disabled in our RDMA-based design for Sort benchmark We perform this experiment using SSD as HDFS data store In this case enabling caching with our design can enhance performance by 18.39 over caching disabled in the same design for 20 GB sort size It reveals that for big workload as in sort an efìcient caching mechanism can signiìcantly improve the performance for an RDMA-based design using a high performance interconnect V R ELATED W ORK Many studies have paid attention to improve the performance of MapReduce in recent years As mentioned before the most analogous one is Hadoop-A which pro vides a new merge method to Hadoop MapReduce framework by utilizing RDMA over InìniBand Our work has some major enhancements and differences with respect to their work along pre-fetching caching codebase modiìcation etc We have discussed these detail in section III-C In the authors ha v e proposed techniques of prefetching and pre-shufîing into MapReduce From their results the techniques can improve the overall performance of Hadoop We have focused on implementing an efìcient key-value pairs pre-fetching and caching mechanism inside TaskTracker which is responsible for fast data shufîe when the mappers are yet to be completed Such a design can help reduce the overhead in the reducer side when the shufîe and merge procedures run in an overlapped manner The research has demonstrated that there is an impressi v e space for performance improvement in Hadoop MapReduce compared with traditional HPC technologies such as MPI Our earlier work 7 re v ealed that SSD can reduce the I/O cost and make the overheads involved in datatransmission over the network prominent In this paper we have conducted experiments with multiple HDDs and SSDs per node to lessen the I/O bottleneck when studying the effect of communication over different interconnects VI C ONCLUSION In this paper we have presented an RDMA-based design of MapReduce over InìniBand We have also proposed efìcient pre-fetching and caching mechanisms for retrieving of the intermediate data Our performance evaluation shows that we achieve 21 beneìt in terms of execution time over Hadoop-A for 100 GB TeraSort For regular Sort benchmark our design outperforms Hadoop-A by 32 for 40 GB sort We also observe an improvement of 22 over Hadoop-A for the same sort size using SSD In future we plan to extend 
In our design we have implemented efìcient map output pre-fetching and caching mechanism We have also provided a conìguration parameter to enable/disable the caching In this experiment we have evaluated the performance improvement that we can get through caching enabled        
1916 


our design to handle faster recovery in case of task failures We will also evaluate our design on larger clusters with a range of applications R EFERENCES  Y  W ang X Que W  Y u D Goldenber g and D Sehgal Hadoop Acceleration through Network Levitated Merge in  ser SC 11 2011  Apache Hadoop http://hadoop.apache.or g  J Dean and S Ghema w at MapReduce Simpliìed Data Processing on Large Clusters in  2004  K Shv achk o H K uang S Radia and R Chansler  The Hadoop Distributed File System in  2010  J Appa v oo A W aterland D Da Silv a V  Uhlig B Rosenburg E Van Hensbergen J Stoess R Wisniewski and U Steinberg Providing A Cloud Network Infrastructure on A Supercomputer in  ser HPDC 10 New York NY USA ACM 2010 pp 385Ö394  Greenplum Analytics W orkbench http://www.greenplum.com/news/greenplum-analyticsworkbench  S Sur  H W ang J Huang X Ouyang and D K Panda Can High Performance Interconnects Beneìt Hadoop Distributed File System in  Atlanta GA 2010  J Jose H Subramoni M Luo M Zhang J Huang M W Rahman N S Islam X Ouyang H Wang S Sur and D K Panda Memcached Design on High Performance RDMA Capable Interconnects in  Sept 2011  J Huang X Ouyang J Jose M W  Rahman H W ang M Luo H Subramoni C Murthy and D K Panda High-Performance Design of HBase with RDMA over InìniBand in   N S Islam M W  Rahman J Jose R Rajachandrasekar H Wang H Subramoni C Murthy and D K Panda High Performance RDMA-based Design of HDFS over InìniBand in  November 2012  J Jose M Luo S Sur  and D K P a nda Unifying UPC and MPI Runtimes Experience with MVAPICH in  Oct 2010  Hadoop Map Reduce The Apache Hadoop Project  http://hadoop.apache.org/mapreduce  Sort http://wiki.apache.or g/hadoop/Sort  RandomWriter http://wiki.apache.or g/hadoop RandomWriter  Inìniband T rade Association http://www inìnibandta org  OpenF abrics Alliance http://www openf abrics.or g  P  Balaji H V  Shah and D K P anda Sockets vs RDMA Interface over 10-Gigabit Networks An In-depth analysis of the Memory Trafìc Bottleneck in  2004  RDMA Consortium  Architectural Speciìcations for RDMA over TCP/IP http://www.rdmaconsortium.org  B Fitzpatrick Distrib uted Caching with Memcached   vol 2004 pp 5 August 2004  A v ailable http://portal.acm.or g/citation.cfm id=1012889.1012894  Apache HBase The Apache Hadoop Project  http://hbase.apache.org  MV APICH2 MPI o v er InìniBand 10GigE/iW ARP and RoCE http://mvapich.cse.ohio-state.edu  Mellanox T echnologies Unstructured Data Accelerator http://www.mellanox.com/page/products dyn product family=144  S Seo I Jang K W oo I Kim J.-S Kim and S Maeng HPMR Prefetching and Pre-shufîing in Shared MapReduce Computation Environment in  Sep 2009 pp 1Ö8  X Lu B W ang L Zha and Z Xu Can MPI Beneìt Hadoop and MapReduce Applications in  2011 
Proceedings of 2011 International Conference for High Performance Computing Networking Storage and Analysis Operating Systems Design and Implementation OSDI IEEE 26th Symposium on Mass Storage Systems and Technologies MSST Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing Workshop on Micro Architectural Support for Virtualization Data Center Computing and Clouds in Conjunction with MICRO 2010 International Conference on Parallel Processing ICPP IEEE International Parallel and Distributed Processing Symposium IPDPSê12 The International Conference for High Performance Computing Networking Storage and Analysis SC Fourth Conference on Partitioned Global Address Space Programming Model PGAS Workshop on Remote Direct Memory Access RDMA Applications Implementations and Technologies RAIT in conjunction with IEEE Cluster Linux Journal Cluster Computing and Workshops 2009 CLUSTER 09 IEEE International Conference on IEEE 40th International Conference on Parallel Processing Workshops ICPPW 
1917 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





