The Experience of NoSQL Database in Telecommunication Enterprise Tuncay Yigit Department of Computer Engineering Suleyman Demirel University Isparta, Turkey tuncayyigit@sdu.edu.tr  Mehmet Akif Cakar Department of Computer Engineering Suleyman Demirel University Isparta, Turkey mehmetakif.cakar@petadata.com.tr  Asim Sinan Yuksel Department of Computer Engineering Suleyman Demirel University Isparta, Turkey asimyuksel@sdu.edu.tr   Abstract  The rapid evolution in communication technology facilitates the Internet access, and makes it faster and cheaper The advancement of Internet accessibility changes the paradigm of application usage methodologies. As a result, data amount in application database reaches unprecedented sizes. The increase in the data amount and user load concretizes the scalability requirements for software components, especially for database technologies. This work aims to provide an example solution experienced in centralized enterprise application. We examine how NoSQL based document store can be adapted according to the requirements of telecommunication enterprise  Index Terms  Big Data, Cloud Computing Document Store Telecommunication, NoSQL I  I NTRODUCTION  The rapid evolution in communication technology facilitates the Internet access, and makes it faster and cheaper. The advancement of Internet accessibility changes the paradigm of application usage methodologies. The new paradigm implies the access of massive number of users to the same cloud application As a result, data amount in application database reaches unprecedented sizes. Data-incentive cloud applications also have to deal with millions of re quests every day. Good examples of such cloud applications are social platforms like Linked-in and Facebook [1 The increase in the data amount and user load concretizes the scalability requirements for softwa re components, especially for database technologies. Among all software components scalability in data layer has been a popular topic in research community. Most relational database management systems RDBMS\re not at desired level [2 in t e rm s of s o lv in g t h e  problems against both the increase of data amount in services and the load intensity. It is seen that NoSQL databases have been popular among the solutions that meet the scalability needs and big data processing.  Database solutions such as Cassandra [3   CouchBase [4 C o uchDB 5  Dynamo  Bi g T ab le [7 MangoDB [8 a r e su ccess f u l  e x am ples of  appl ica ti on s in t e r m s  of horizontal scalability Telecommunication enterprise consists of both data and load insensitive applications. Telecommunication enterprise has to rely on centralized systems that comprises of several sub systems. This study describes the architectural process of a centralized telecommunication system in context of managing enterprise work force, customer requests, call management and decision support II  S UB S YSTEMS  Developed telecommunication application relies on the feed of six basic subsystems. The first system that is to be handled is work force management system. Work force management system stores its data on Microsoft SQL Server RDBMS. Data contains personnelís daily shift plans, holidays, permits health reports etc. Work Force Management \(WFM\data tables contain millions of records. However, RDBMS is still suitable for this sub system since less than a hundred management staff actively uses it Call management system uses hybrid storage solution to persist application data. Hybrid solution consists of file system and Oracle RDBMS. File system plays a buffering role to analyze and validate the real-time data before storing it in RDBMS Customer Support and Staff Rating \(CSSR\ system is built on top of IBM DB2 RDBMS. It is the data provider sub system that needs the least amount of data in our telecommunication application Call Rating System \(CRS\ uses a custom file system. It contains unstructured call records and call quality rating result associated with call record file Decision Support System \(DSS\relies on a custom data warehouse solution. It provides de-normalized data, collected from several subsystems using Extract, Transform, and Load ETL\olution Staff Tracking System \(STS\ provides personnelís turnstile transaction logs. It is a distributed system running on each location of company III  DATA MODEL  The system presented in this study is responsible for business decisions related to information collected from sub systems. It collects data, analyzes, merges and delivers to decision phase Additionally, it provides real-time requirement analysis for each team & department. It handles gigabytes of daily data and high volume of requests from thousands of active users ETL component of the system gathers data from more than five hundred relational tables. In this study, we will cover pre 


selected part of the data model. Key entities of subsystemís data model \(see Figure 1\ is composed of seven entities such as Staff Shift, Permit, Work Activity, Service, Turnstile Transaction Department   Staff StaffId PK  ManagerId  Shift ShiftId PK  StaffId  1..1   1..n Permit PermitId PK  StaffId  1..n WorkActivity WorkActivityId PK  StaffId  TurnstileTransaction TTransactionId PK  StaffId  1..n Department DepartmentId PK  ServiceId  1..n Service ServiceId PK  DepartmentId  1..n  Fig. 1  Data model of key entities The Staff entity contains telecom  s personnel information Many attributes such as work type, manager info, average performance value that affects the decision takes part in this entity The Shift entity maps shift plans of whole department into each staffís shift plans Permit table holds the real-time and previously planned permissions that belong to staff Work activity is the most intensive table in terms of data size All daily activities of personnel are instantly recorded to this table In service table, location information and type of service that the personnel provide are stored. In turnstile transaction table check-in and check-out information of personnel from all turnstile locations within the corporation are stored. Department table stores the team, division and hierarchy information between them For each personnel information request from the system, all of the related entities like staff, permit, shift and work activity tables has to be scanned. Although there are temporary solutions such as caching for permit and shift tables that are assumed to change daily, caching is not an effective solution for the work activity table that is updated continuously Furthermore, feeding the same table with different sources may cause data redundancy. Previous attempts tried to solve redundancy problem by re-scanning whole table to prevent reinserting same data. However this solution made the systems unresponsive In our solution we decided to use document model. For this purpose we transformed relational data structure to document model \(see Figure 2\. In this context Shift, Permit, Work Activity, Turnstile Transaction tables are combined into a document structure called A ctivityPackage. Each activity package is designed in a way that it contains all the activities that the personnel has done or should do in the same day. Data that comes from different sources are first stored in RDBMS based Activity Queue Table and then synchronization service transforms relational data to ActivityPackage document   Fig. 2  Document model of Activity Package IV  D ATA S TORES  Developed system has to deal with millions of requests every day. Previous RDBMS solutions with powerful and expensive hardware are not capable of handling daily workload since they rely on vertical scalability. However, common NoSQL solutions have already proven to perform better in terms of their horizontal scalability performance. It is easy to increase load capacity by adding new machines to NoSQL database cluster In this study, we use three types of data stores: Document Store, RDBMS, and File System  A  Document Store Activity Packages are the primary sources of document store. Documents are persisted in JSON format, which are the data structures capable of holding arrays and other complex information. JSON documents are flexible and information-rich structures that enable developers to model objects as individual documents. Activity packages are the most frequently used data type in our system. That is the major reason why we chose the document store for activity packages In terms of optimistic concurrency, we use check-and-set token \(CAS\ associated with document provided by document store. Document store checks a CAS value before changing the data; it effectively prevents data loss without having to lock records. Document store prevents a document from being altered by an operation if another process alters the document and its CAS value, in the meantime. Additionally, all update operations of Activity Package assigned to only one background service daemon service  B  RDBMS Real-time stream listener service captures multiple CMS records at once. To handle multi-record transactions itís decided to use RDBMS in Activity queues  


C  File System File system is preferred to store unstructured files and realtime buffering records  V  D ISTRIBUTION O F W ORKLOAD  Sources of workload in the system are covered under two main titles  A  Regular Background Tasks To generate live analysis results, system needs continuous updates from its sub systems. To maintain this requirement, we designed several background services. All relational data gathering tasks are assigned to ETL service. ETL periodically checks for new records from subsystems and takes them in associated store Real-time stream capturing tasks are assigned to real-time listener \(RTL\rvice. RTL continuously buffers stream data to file system Analyzing and parsing tasks are assigned to real-time parser RTP\ervice. The RTP service checks for buffer size generated by RTL, validates and then parses the buffer package to save in activity queue Queue processing requirement is handled by real-time synchronization \(RTS\ervice. RTS consists of several sub synchronization services. All of the sub services check for their associated queue store to handle their tasks. Queuing is the key mechanism in terms of providing system stabilization. To prevent size overflow in queue stores, each RTS sub service has right to pick additional thread from thread pool, and then leave it back to the pool, if the item size comes to the pre-defined threshold. Service workflow is presented in figure 3                                 Data 1 Data 2 Data 3       Real-time Stream RTL CMS    File System    RTP    Activity Queue RTS                                    WFM    ETL       Activity Package  Fig. 3  Service workflow System needs to generate warnings to notify users and admin staff. Common warning types have deadlines to take actions Once their deadline is passed, they become abandoned records In early attempts, we stored warnings and other disposable information in RDBMS and developed another service to delete inactive records. In our final system design all of disposable data are stored in document store, since it provides time-to-live ttl\ attribute for each document TTL is an expiration time for a document specified in seconds. Document store automatically deletes values during regular maintenance, if the ttl for a document has expired A failure in any service will not affect other services since all of them run in independent app domains. With regular background task architecture, the most part of workload over the whole system became measurable and scalable A sample workload distribution of regular background tasks is shown in figure 4. When the user workload is in its lowest rate, system starts running daily ETL tasks. For this reason background task workload peaks at around 03:00am. In day time background tasks mainly focus on processing real-time data taken from CMS. As long as amount of captured real-time data decreases the workload of background tasks will also drop off   Fig. 4  Workload distribution of regular background tasks B  User Workload Thousands of users actively use the system in their daily work. All of the software patterns are developed according to cloud architecture [9   W h e n th e w o rk l o a d in c r e a ses  i t is  distributed by automatically increasing the number of application servers behind the load balancer Figure 5 shows one of the daily workload distribution of user requests 


 Fig. 5  Daily user workload distribution VI   C ONCLUSION  This work aimed to provide an example solution experienced in centralized enterprise application. We examined how NoSQL based document store could be adapted according to the requirements of telecommunication enterprise Some solution providers attempted to solve this problem However they all failed because of several reasons. Since previous solutions didnít run properly, w e were not able to capture performance results to compare with our solution Our performance results and horizontal scalability requirements were the key factors of storage decision. Different from the previous studies [1  NoS Q L so lu ti on u s e d in  ou r  project has significant performance advantage over the competitive RDBMS solutions. In this project we determined to use both NoSQL and RDBMS solutions for data storage  We conclude that when a system needs to provide a high level of scalability, high performance, and flexibility in data structure, a N oSQL solution is well suited for the project. If itís required to handle multi-record transactions or needed to perform rollback operations, a traditional RDBMS may be a better decision to use in a project        R EFERENCES   1  F. Cruz, P. Gomes, R. Oliveira and J. Pereira, "Assessing NoSQL Databases for Telecom Applications CEC '11 Proceedings of the 2011 IEEE 13th Conference on Commerce and Enterprise Computing pp. 267 270 2011 2 J. Pokorny, "NoSQL databases: a step to database scalability in web environment," in iiWAS '11 Proceedings of the 13th International Conference on Information Integration and Web based Applications and Services New York, NY, USA, 2011 3 A. Lakshman and P. Malik, "Cassandra: a decentralized structured storage system ACM SIGOPS Operating Systems Review vol. 44, no. 2, pp. 35-40, 2010 4 Couchbase, Inc, "Couchbase Developer's Guide 2.0," 20 June 2013. [Onlin v ai lable  http://www.couchbase.co m/docs/couchbasedevguide2.0/index.html. [Accessed 21 June  5 J. Lennon, Beginning CouchDB, Berkely, CA, USA Apress, 2009 6 G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati A. Lakshman, A. Pilchin, S. Sivasubramanian, P Vosshall and W. Vogels, "Dynamo: amazon's highly available key value store Proceedings of twentyfirst ACM SIGOPS symposium on Operating systems principles pp. 205-220, 2007 7 F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A Wallach, M. Burrows, T. Chandra, A. Fikes and R. E Gruber, "Bigtable: a distributed storage system for structured data OSDI '06 Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation vol. 7, p. 15, 2006 8 10gen, Inc, "MangoDB," [Onlin ailab le  http://www.mongodb.org 9 G. Shroff, Enterprise Cloud Computing: Technology Architecture, Applications, Cambridge University Press 2010   A. Floratou, N. Teletia, D. J DeWitt, J. M. Patel and D Zhang, "Can the Elephants  Handle the NoSQL Onslaught Proceedings of the VLDB Endowment vol 5, no. 2, pp. 1712-1723, 2012       


0 0,2 0,4 0,6 0,8 1 1,2 1,4 2 4 8 16 32 first-touch opt-coarse opt-fine 
original optimized 
As mentioned and shown in Figure 2 data clustering provides us three results with different granularities i.e single memory address individual virtual page and the complete data set The rst result enables extremely negrained optimization i.e to allocate each data item to the corresponding processor node However such a granularity introduces high overhead especially for the case of runtime optimization Therefore we performed the ne-grained page-level optimization using single pages as an allocation unit and the coarse-grained optimization of allocating the whole data set on a single node The optimization was enabled by using the annotations provided by the simulation platform to explicitly specify the best location in the source code In the case of page-level optimization each virtual page is speciìcally allocated on its dominating node while with the coarse-grained optimization a single node is speciìed for the entire working load A dominating node is the computing node that performs the most accesses on the virtual page The baseline for both optimization versions is the transparent data placement i.e all data are placed on the host node on which the job is submitted We also addressed the rst-touch scheme which is usually applied to e v aluate the memory optimization on systems with a distributed shared memory 31 First-touch allocates data on the node that rst accesses it This scheme tends to behave better than other data distribution policies because the node that rst accesses a page is usually the node that mostly accesses it Our rst experiment aims at studying the direct impact of the optimizations For this we simulated all applications with rst-touch node-level optimization opt-coarse pagelevel optimization opt-ìne and the conventional host-node data placement We also executed the application using different numbers of processors in order to examine the scalability of our locality optimization approach According to the requirement of some applications the number of processors is speciìcally chosen as a power of two For each test the execution time of all applications was measured and the speedup was calculated by dividing the time needed for running the code with the conventional data placement policy via the execution time of an optimized version i.e  The L1 cache is speciìed as a two-way associative cache with a size of 16 KB while the L2 cache is four-way associative with a size of 512 KB The local memory access latency is speciìed as 100 CPU cycles and remote access latency 1,000 cycles based on the conìguration of modern commodity processors For the following tests we use the most strict communication policy which allows only one node to access one remote memory at the same time During this delay no other nodes can perform remote accesses 
S T T 
B Locality Optimization C Optimization with Exclusive Data Access 
 
Figure 3 Improvement in execution time with the LU application Figure 3 to 7 depict the experimental results with different applications The x-axis of these gures shows the number of processors we used for the experiments while the y-axis depicts the speedup of the optimized code version against the transparent version in absolute execution time The gures show the data with the two optimization schemes and the allocation policy rst-touch Observing the result with LU as illustrated in Figure 3 it can be seen that the rst-touch scheme has no speedup to the basic policy where the y-axis shows a value of 1 in speedup for all tests meaning that this scheme results in the same execution time as the default version This also indicates that rst-touch brings the same runtime data layout as the host-node scheme that allocates all data sets on the same node i.e the host node The reason may lie on the data initialization which is usually done by the host and the rst-touch scheme hence puts all shared data on the host node that rst accesses the data The gure also depicts that the node-level optimization has a similar behavior where only on two-processor systems a slight speedup in execution time is observed Page-level optimization on the other hand introduces a speedup with the LU application and the speedup arises with the number of processors For example a speedup of factor 1.04 is achieved with twoprocessor systems while on 32 processors a factor of 1.23 is obtained This renders that the page-level optimization achieves a good scalability with LU FFT shows a different behavior As illustrated in Figure 4 all three data allocation policies perform better than the conventional data placement Nevertheless both rst-touch scheme and coarse-grained optimization present a poor scalability because the improvement goes down as the number of processors increases The same behavior can also be seen with the ne-grained page-level optimization on small systems However using page-level optimization the performance achievement arises and goes up starting with 16-node 
Number of processors Speedup \(opt vs. basis 
1101 
1101 


Figure 4 Improvement in execution time with the FFT application systems The speedup on a 32-node system for example is still lower than that on the two-node system but higher than the case with four-node systems Further observation of the trend on larger systems would be interesting Unfortunately the simulation platform is limited to 32 processors Figure 5 Reduction rate of remote accesses on different system scales The improved runtime behavior with page-level optimization directly contributes to the better data locality For a deeper insight into this issue we have measured the number of remote accesses for both page-level optimization and the transparent data placement We then computed the reduction rate which is deìned as the percentage of reduced remote accesses achieved by the optimized version to the total remote accesses introduced by the transparent version Figure 5 presents the results with all tested applications running on systems with 2 to 32 processing nodes Observing the curves in Figure 5 it can be seen that for most applications the reduction rate is higher on smaller systems than on larger ones for example LU WATER OCEAN and BARNES This is not surprising because on smaller systems a data page is requested by few processor nodes However on large systems the references to a single page are distributed across a set of nodes which possibly all require the page frequently This means that the optimization of exclusively putting the data on the dominating node can only remove the remote accesses from this node but not of the others Nevertheless if an application has a lower shared degree a data page would be potentially only dominantly accessed by a single processor In this case the reduction rate of remote accesses can still be high on larger systems FFT and RADIX are such examples With the FFT application we achieved a reduction of as high as 47 in remote memory accesses on 32-processor systems while RADIX shows a constant reduction rate of 40 from 4 to 32 processors Figure 6 Improvement in execution time with the RADIX application The high reduction rate of RADIX directly results in the performance gain in terms of execution time As demonstrated in Figure 6 page-level locality optimization introduces an exciting speedup in execution time and this improvement increases drastically with the number of processors On a 32-node system for instance we achieved a speedup of as high as a factor of two The scalability achieved with RADIX has to be contributed by the reduction in remote memory accesses As shown in the previous gure the reduction rate with RADIX is similar with systems of different scales which also means a similar reduction in the remote access penalty In this case the speedup on larger systems is higher because the execution time is smaller The application WATER however does not present good results with the optimization As shown in Figure 7 even the page-level locality tuning is not effective This is caused by its speciìc access pattern which will be explained later In summary the applications demonstrate different behavior with our locality optimization This distinction lies directly on the access pattern of each individual program Using the proposed approach we found that for the LU application nearly half of the total data pages is accessed equally by many processors while the other half has several dominating nodes Only 6 of the pages is accessed mainly by a single processor We have placed each page to its best position suggested by our data classiìer However for most pages other nodes also require them Hence still many 
Number of processors Reduction rate of remote accesses 
0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 2 4 8 16 32 lu fft radix water ocean barnes 
Number of processors Speedup \(opt vs. basis Number of processors Speedup \(opt vs. basis 
0 0,2 0,4 0,6 0,8 1 1,2 1,4 1,6 1,8 2 2 4 8 16 32 first-touch opt-coarse opt-fine 0 0,5 1 1,5 2 2,5 2 4 8 16 32 first-touch opt-coarse opt-fine 
1102 
1102 


Proceedings of the 6th IASTED International Conference on European Power and Energy Systems EuroPES Journal of Supercomputing Proceedings of the International Conference on Intelligent Networking and Collaborative Systems INCoS Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures Proceedings of the 6th European Congress on Intelligent Techniques and Soft Computing EUFIT Classiìcation and Regression Trees 
0 0,2 0,4 0,6 0,8 1 1,2 2 4 8 16 32 first-touch opt-coarse opt-fine 
Figure 7 Improvement in execution time with the WATER application remote accesses exist after the location tuning Similarly for WATER although 36 of the data pages is dominantly accessed by a single node the number of accesses by this node is only 2 of the total remote accesses The others are shared by all or almost all processors and these processors perform equally accesses to an individual page For BARNES the whole shared data is accessed by all nodes Even though some nodes do not frequently request a page there exists no dominating node for any data page This means that at least two processors equally access the same page Therefore it is difìcult to optimize this code With FFT nevertheless 96 of the data pages has a dominating node and 33 of them are exclusively accessed by this node RADIX is even better with more than a half of the data pages accessed by a single node For OCEAN nearly 70 of the pages are exclusively used by one node and 2/3 of the rest has a clear dominating processor Therefore the optimization with these three applications leads to a considerable performance gain As observed in the gures the coarse-grained optimization generally does not work well The reason is as following The global dominating node for the whole data set is discovered based on the total remote accesses each node performed at the runtime Actually most pages are shared by many processors The page-level optimization improves the performance because of the existence of a dominating node for each individual page However we note that in most cases this dominating node varies from page to page Therefore node-level optimization could improve the data locality in smaller systems For example on a two-node machine there are only two candidates for a page hence placing all data on the global dominating node can achieve speedup However on larger systems the best position for a virtual page can be any of the processors therefore only a ne tuning with the ability of placing each page on a speciìc node is able to improve the memory access behavior V C ONCLUSIONS This paper describes our research work of applying generic data analysis techniques for memory locality optimization on systems with a distributed memory The goal of this work is to validate the feasibility of the data classiìcation algorithms in the task of detecting performance bottlenecks and proposing an efìcient solution This is actually the rst step towards our nal target of establishing self-optimizing parallel systems with respect to different interacting performance metrics For this research goal we deploy data clustering to evaluate the runtime performance data because this technique provides the required properties such as analysis accuracy overhead and association of multiple optimization targets The proposed approach has shown its ability in nding access patterns and bottlenecks The data analysis results have guided us to achieve signiìcant improvement in both execution time and scalability of parallel programs With this initial experience we will further address other performance metrics like the cache performance processor utility energy consumption etc A more exciting future work is to optimize the system with all these metrics taken into account A CKNOWLEDGEMENTS Dr Lizhe Wangês work is supported by the National Natural Science Foundation of China 61361120098 R EFERENCES  C Athanasopoulou V  Chatziathanasiou M K omninou and Z Petkani Applying Knowledge Engineering and Data Mining for Optimization of Control Monitoring of Power Plants In 
 2006  D H Baile y  FFTs in External or Hierarchical Memory   4\(1 March 1990  N Bessis S Sotiriadis V  Cristea and F  Pop Modelling Requirements for Enabling Meta-scheduling in Inter-Clouds and Inter-Enterprises In  pages 149Ö156 2011  G E Blelloch C E Leiserson B M Maggs C G Plaxton S J Smith and M Zagha A Comparison of Sorting Algorithms for the Connection Machine CM-2 In  pages 3Ö16 July 1991  Christian Bor gelt A Decision Tree Plug-In for DataEngine In  volume 2 pages 1299Ö1303 Aachen Germany 1998 Verlag Mainz  L Breiman J Friedman R Olshen and C Stone  Chapman  Hall 1993 
Number of processors Speedup \(opt vs. basis 
1103 
1103 


 S Bro wne J Dongarra N Garner  G Ho and P  Mucci Portable Programming Interface for Performance Evaluation on Modern Processors  14\(3 2000  H Brunst D Hack enber g G Juck eland and H Rohling Comprehensive Performance Tracking with Vampir 7 In M.S Mller M.M Resch A Schulz and W.E Nagel editors  pages 17Ö29 Springer 2009  B R Buck and J K Hollingsw orth Data Centric Cache Measurement on the Intel Itanium 2 Processor In  November 2004  CERN LHC  The Lar ge Hadron Collider  W eb P age http://lhc-new-homepage.web.cern.ch/lhc-new-homepage  The LSST Corporation Lar ge Synoptic Surv e y T elescope Web Page http://www.lsst.org/lsst  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters  51\(1 2008  C M Dinu F  Pop and V  Cristea P attern Detection Model for Monitoring Distributed Systems In  pages 268 275 2011  Gordon at san die go supercomputing center  http://www.sdsc.edu/us/resources/gordon  J R Hammond S Krishnamoorthy  S Shende N A Romero and A D Malony Performance Characterization of Global Address Space Applications A Case Study with NWChem  24\(2 2012  M Hermanns S Krishnamoorthy  and F  W olf A scalable infrastructure for the performance analysis of passive target synchronization  39\(3 March 2013  T on y He y  Ste w art T ansle y  and Kristin T olle editors  Microsoft Research Redmond Washington 2009  Intel Intel VT une Ampliìer XE 2013 Performance and Thread Proìler http://software.intel.com/en-us/intel-vtuneampliìer-xe  A K usiak and Z Song Comb ustion Ef cienc y Optimization and Virtual Testing A Data-Mining Approach  2\(3 August 2006  Sylv ain Letourneau F azel F amili and Stan Matwin Data Mining to Predict Aircraft Component Replacement  14\(6 1999  H L  of M Nord  en and S Holmgren Improving Geographical Locality of Data for Shared Memory Implementations of PDE Solvers In  volume 3037 of  pages 9Ö16 2004  A D Malon y  S Biersdorf f S Shende H Jagode S T omo v  G Juckeland R Dietrich D Poole and C Lamb Parallel Performance Measurement of Heterogeneous Parallel Systems with GPUs In  pages 176Ö185 September 2011  J Marathe F  Mueller  and B de Supinski A Hybrid Hardware/Software Approach to Efìciently Determine Cache Coherence Bottlenecks In  pages 21Ö30 June 2005  C Olaru P  Geurts and L W ehenk el Data mining tools and applications in power system engineering In  1999  B Quaing J T ao and W  Karl Y A CO A User Conducted Visualization Tool for Supporting Cache Optimization In  volume 3726 of Lecture Notes in Computer Science pages 694Ö703 Sorrento Italy September 2005  J Ross Quinlan  Morgan Kaufmann Publishers Inc San Francisco USA 1993  S Shende A Malon y  W  Spear  and K Schuchardt Characterizing I/O Performance Using the TAU Performance System In   S Shende and A D Malon y  The T A U P arallel Performance System  20\(2 2006  H T akashi O Hiroshi I T akayoshi and D Henry  Automatic Data Distribution Method Using First Touch Control for Distributed Shared Memory Multiprocessors In  volume 2624 of  pages 147Ö161 2001  J T ao M Schulz and W  Karl A Simulation Tool for Evaluating Shared Memory Systems In  pages 335Ö342 Orlando Florida April 2003  M M T ikir and J K Hollingsw orth Using Hardw are Counters to Automatically Improve Memory Performance In  2004  F  W olf Scalasca In  pages 1775Ö1785 Springer October 2011  S C W oo M Ohara E T orrie J P  Singh and A Gupta The SPLASH-2 Programs Characterization and Methodological Considerations In  pages 24Ö36 June 1995  J Zhao J T ao L W ang and A W irooks A T oolchain For Proìling Virtual Machines In  pages 497Ö503 Aalesund Norway May 2013 
The International Journal of High Performance Computing Applications Tools for High Performance Computing Proceedings of SuperComputing Commun ACM Proceedings of the International Symposium on Symbolic and Numeric Algorithms for Scientiìc Computing SYNASC Concurrency and Computation Practice and Experience Parallel Computing The Fourth Paradigm Data-Intensive Scientiìc Discovery IEEE Transactions on Industrial Informatics IEEE Intelligent Systems Computational Science ICCS 2004 Lecture Notes in Computer Science Proceedings of International Conference on Parallel Processing Proceedings of the International Conference on Supercomputing Proceedings of the Power Systems Computation Conference PSCC High Performance Computing and Communcations First International Conference HPCC 2005 Proceedings C4.5 Programs for Machine Learning Proceedings of the ICPP Parco 2011 conference Exascale Mini-symposium International Journal of High Performance Computing Applications Languages and compilers for parallel computing International workshop Lecture Notes in Computer Science Proceedings of the 36th Annual Simulation Symposium Proceedings of the 2004 ACM/IEEE conference on Supercomputing Encyclopedia of Parallel Computing Proceedings of the 22nd Annual International Symposium on Computer Architecture Proceedings of the 27th European Conference on Modelling and Simulation ECMS 2013 
1104 
1104 


boards of several journals including IEEE Transactions on Service Computing and the Journal of Performance Evaluation   Zhen has given keynotes and distinguished lectures in various conferences and universities. He was an adjunct professor at University of Science and Technology of China and Beijing University of Post and Telecommunications While he was in France Zhen was  also an adjunct professor of the University of Paris VI \(University of Pierre & Marie Curie\and the University of Nice  Sophia Antipolis, France   His areas of expertise include mobile computing, mobile services, cloud computing, stream processing re al time analytics performance modeling stochastic optimization service oriented architecture and semantic Web      
lxxxi 


en-US Keynote VI I I  en-US GreenCom iThings CPSCom 2013   Towards Carrier Cloud   Dr. Tarik Taleb  Senior Researcher and 3GPP Standards Expert  NEC Europe Ltd, Heidelberg, Germany  Email tarik.taleb@nw.neclab.eu    Abstract   Mobile operators are in need of means to cope with the ever increasing mobile data traffic, introducing minimal additional capital expenditures on existing infrastructures, principally due to the modest Average Revenues per User ARPU Network virtualizat ion and cloud computing techniques along with the principles of the latter in terms of service elasticity on demand and pay per use could be important enablers for various mobile network enhancements and cost reduction This talk discusses the recent tr ends the mobile telecommunications market is experiencing showcasing some of the emerging consumer products and services that are facilitating such trends. The talk also discusses the challenges these trends are representing to mobile network operators. T he talk also demonstrates the possibility of extending cloud computing beyond data centers towards the mobile end user providing end to end mobile connectivity as a cloud service. The talk introduces a set of technologies and methods for the on demand pro vision of a decentralized and elastic mobile network as a cloud service over a distributed network of cloud computing data centers; federated cloud. The concept of Follow Me Cloud whereby not only data but also mobile services are intelligently following t heir respective users is also introduced. The novel business opportunities behind the envisioned carrier cloud architecture and service are also discussed, considering various multi stakeholder scenarios   Bio   Tarik Taleb is currently working as Senior Researcher and 3GPP Standards Expert at NEC Europe Ltd Heidelberg, Germany. Prior to his current position and till Mar. 2009, he worked as assistant professor at the Graduate School of Information Sciences, Tohoku University, Japan, in a lab fully funded by KDDI, the second largest network operator in Japan From Oct 2005 till Mar 2006 he was working as research fellow with the Intelligent Cosmos Research Institute Se ndai Japan He received his B E   degree in Information Engineering with distinction M.Sc and Ph.D degrees in Information Sciences from GSIS Tohoku Univ., in 2001, 2003, and 2005, respectively   Dr Taleb  s research interests lie in the field of architectural enhancements to mobile core networks particularly 3GPP  s mobile cloud net working mobile multimedia streaming congestion control protocols handoff and mobility management inter vehicular communications and social media networking Dr Taleb has been also directly engaged in the development and standardization of the Evolved  Packet System as a member of 3GPP  s System Architecture working group. Dr. Taleb is a board member of the  IEEE Communications Society Standardization Program Development Board  As an attempt to bridge the gap between academia and industry Dr Taleb has f ounded and has been the     Dr Taleb  is/was on the editorial board of the IEEE Wireless Communications Magazine IEEE Transactions on Vehicular Technology, IEEE Communications Surveys & Tutorials, and a number of Wiley journals. He is serving as vice chair of the Wireless Communications Tech nical Committee, the largest in IEEE ComSoC He also served as Secretary and then as Vice Chair of the Satellite and Space Communications Technical Committee of IEEE ComSoc 2006  2010 He has been on the technical   
lxxxii 


program committee  of different IEEE c onferences including Globecom, ICC and WCNC and chaired some of their symposia   Dr Taleb is the recipient of the 2009 IEEE ComSoc Asia Pacific Best Young Researcher award Jun 2009 the 2008 TELECOM System Technology Award from the Telecommunicati ons Advancement Foundation Mar 2008 the 2007 Funai Foundation Science Promotion Award Apr 2007 the 2006 IEEE Computer Society Japan Chapter Young Author Award Dec 2006 the NiwaYasujirou Memorial Award Feb 2005 and the Young Researcher's Enc ouragement Award from the Japan chapter of the IEEE Vehicular Technology Society \(VTS\\(Oct. 2003\ Some of Dr. Taleb  s research work has been also awarded best paper awards at prestigious conferences. Dr. Taleb is a senior IEEE member      
lxxxiii 


en-US Keynote I X  en-US GreenCom iThings CPSCom 2013   How Densely Should the Data Base Stations  B e Deployed in Hyper Cellular Networks   Professor Zhisheng Niu  Tsinghua National Lab for Information Science and Technology  Tsinghua University, Beijing 100084, China  E mail niuzhs@tsinghua.edu.cn    Abstract   One of the key approaches to make the mobile communication networks more GREEN Globally Resource optimized and Energy Efficient Networks\is to have the cellular architecture and radio resource allocation more adaptive to the environment and traffic varia tions including making some lightly loaded base stations \(BSs\go to sleep. This is the concept of so called TANGO \(Traffic Aware Network planning and Green Operation and CHORUS Collaborative and Harmonized Open Radio Ubiquitous Systems published by th e author earlier. To realize this, a new cellular framework, named hyper cellular networks HCN has been proposed in which the coverage of control signals is decoupled from the coverage of data signals so that the data coverage can be more elastic in ac cordance with the dynamics of traffic characteristics and QoS requirements. Specifically, the data base stations \(DBSs\in HCN can be densely deployed during peak traffic time in order to satisfy the capacity requirement, while a portion of DBSs can be swi tched off or go to sleep mode if the traffic load is lower than a threshold in order to save energy. A fundamental question then arises how densely should the DBSs be deployed in order to balance the QoS requirements and the energy consumption in hyper ce llular networks     In this talk, we characterize the optimal DBS density for both homogeneous and heterogeneous hyper cellular networks to minimize network cost with stochastic geometry theory For homogeneous cases both upper and lower bounds of the optimal DBS density are derived For heterogeneous cases our analysis reveals the best type of DBSs to be deployed for capacity extension or to be switched off for energy saving. Specifically, if the ratio between the micro DBS cost and the macro DBS cost  is lower than a threshold which is a function of path loss and their transmit power then the optimal strategy is to deploy micro DBSs for capacity extension or to switch off macro DBSs \(if possible\for energy saving with higher priority Otherwise the  optimal strategy is the opposite Based on the parameters from EARTH numerical results show that in the dense urban scenario compared to the traditional macro only homogeneous cellular network with no DBS sleeping deploying micro DBSs can reduce about 40 of the total energy cost, and further reduce about 20% with DBS sleeping capability   Bio   Zhisheng Niu graduated from Northern Jiaotong University currently Beijing Jiaotong University Beijing China in 1985 and got his M.E and D.E degrees fr om Toyohashi University of Technology Toyohashi, Japan, in 1989 and 1992, respectively. After spending two years at Fujitsu Laboratories Ltd Kawasaki, Japan, he joined with Tsinghua University, Beijing, China, in 1994, where he is now a professor at the  Department of Electronic Engineering and the deputy dean of the School of Information Science and Technology. His major research interests include queueing theory, traffic engineering, mobile Internet radio resource management of wireless networks, and g reen communication and networks   Dr Niu has been an active volunteer for various academic societies including council member of Chinese Institute of Electronics 2006 10 vice chair of the Information and Communication Network Committee of Chinese In stitute of Communications 2008 12 Councilor of IEICE Japan 2009 11 and membership development coordinator of IEEE Region 10 \(2009 10\ In particular, in IEEE Communication 
lxxxiv 


Society, he has been serving as an editor of IEEE Wireless Communication Magaz ine \(2009 12\ director of Asia Pacific Region \(2008 09\ director for Conference Publications \(2010 11\ chair of Beijing Chapter 2001 08 and members of Award Committee 2011 13 Emerging Technologies Committee 2010 12 On line Content Committee 20 10 12 and Strategy Planning Committee He has also been serving as general co   co    chairs o f    Prof. Niu is a co recipient of the Best Paper Awards from the 13th and 15th Asia Pacific Conference on Communication APCC in 2007 and 2009 respectively and received Outstanding Young Researcher Award from Natural Science Foundati on of China in 2009 He is now the Chief Scientist of the National  Energy and Resource Optimized Hyper Cellular Mobile Communication System 2012 2016 which is the first national project green communications in China He is the fellow of IEEE and IEICE and a distinguished lecturer of IEEE Communication Society \(2012 2013  
lxxxv 


