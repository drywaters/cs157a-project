University of Waterloo Waterloo Ontario Canada N2L 3G1 Email afarahat aelgohary aghodsib mkamel uwaterloo.ca 
Abstract 
Distributed Column Subset Selection on MapReduce 
Ahmed K Farahat Ahmed Elgohary Ali Ghodsi Mohamed S Kamel 
Given a very large data set distributed over a cluster of several nodes this paper addresses the problem of selecting a few data instances that best represent the entire data set The solution to this problem is of a crucial importance in the big data era as it enables data analysts to understand 
  
the insights of the data and explore its hidden structure The selected instances can also be used for data preprocessing tasks such as learning a low-dimensional embedding of the data points or computing a low-rank approximation of the corresponding matrix The paper rst formulates the problem as the selection of a few representative columns from a matrix whose columns are massively distributed and it then proposes a MapReduce algorithm for selecting those representatives The algorithm rst learns a concise representation of all columns using random projection and it then solves a generalized column subset selection problem at each machine in which a subset of columns are selected from the sub-matrix on that 
machine such that the reconstruction error of the concise representation is minimized The paper then demonstrates the effectiveness and efìciency of the proposed algorithm through an empirical evaluation on benchmark data sets 
Column Subset Selection Greedy Algorithms Distributed Computing Big Data MapReduce 
Keywords 
I I NTRODUCTION Recent years have witnessed the rise of the big data era in computing and storage systems With the great advances in information and communication technology hundreds of petabytes of data are generated transferred processed and stored every day The availability of this overwhelming amount of structured and unstructured data creates an acute 
need to develop fast and accurate algorithms to discover useful information that is hidden in the big data One of the crucial problems in the big data era is the ability to represent the data and its underlying information in a succinct format Although different algorithms for clustering and dimension reduction can be used to summarize big data these algorithms tend to learn representatives whose meanings are difìcult to interpret For instance the traditional clustering algorithms such as 
k 
means tend to produce centroids which encode information about thousands of data instances The meanings of these centroids are hard to interpret Even clustering methods that use data instances as prototypes 
such as 
k 
medoid learn only one representati v e for each cluster which is usually not enough to capture the insights of the data instances in that cluster In addition using medoids as representatives implicitly assumes that the data points are distributed as clusters and that the number of those clusters are known ahead of time This assumption is not true for many data sets On the other hand traditional dimension reduction algorithms such as Latent Semantic Analysis LSA tend to learn a fe w latent concepts in the feature space Each of these concepts is represented by a dense vector which combines thousands of features with positive and negative weights This makes it difìcult for the 
data analyst to understand the meaning of these concepts Even if the goal of representative selection is to learn a low-dimensional embedding of data instances learning dimensions whose meanings are easy to interpret allows the understanding of the results of the data mining algorithms such as understanding the meanings of data clusters in the low-dimensional space The acute need to summarize big data to a format that appeals to data analysts motivates the development of different algorithms to directly select a few representative data instances and/or features This problem can be generally formulated as the selection of a subset of columns from a data matrix which is formally known as the Column Subset 
Selection CSS problem 5 6 Although man y algorithms have been proposed for tackling the CSS problem most of these algorithms focus on randomly selecting a subset of columns with the goal of using these columns to obtain a low-rank approximation of the data matrix In this case these algorithms tend to select a relatively large number of columns When the goal is to select a very few columns to be directly presented to a data analyst or indirectly used to interpret the results of other algorithms the randomized CSS methods are not going to produce a meaningful subset of columns One the other hand deterministic algorithms for CSS although more accurate do not scale to work on big 
matrices with massively distributed columns This paper addresses the aforementioned problem by presenting a fast and accurate algorithm for selecting a very few columns from a big data matrix with massively distributed columns The algorithm starts by learning a concise representation of the data matrix using random projection Each machine then independently solves a generalized column subset selection problem in which a subset of columns is selected from the current sub-matrix such that the reconstruction error of the concise representation is minimized A further selection step is then applied to 
2013 IEEE 13th International Conference on Data Mining 1550-4786/13 $31.00 © 2013 IEEE DOI 10.1109/ICDM.2013.155 171 


f g x x x x x 
m n A B i i i 002 A A i j A A i A A j A A A A A A A A A A A A l A l m n A l l A P A  P m m A A A P A A l l A P A 
the columns selected at different machines to select the required number of columns The proposed algorithm is designed to be executed efìciently over massive amounts of data stored on a cluster of several commodity nodes In such settings of infrastructure ensuring the scalability and the fault tolerance of data processing jobs is not a trivial task In order to alleviate these problems MapReduce  w as introduced to simplify lar ge-scale data analytics over a distributed environment of commodity machines Currently MapReduce and its open source implementation Hadoop is considered the most successful and widelyused framework for managing big data processing jobs The approach proposed in this paper considers the different aspects of developing MapReduce-efìcient algorithms The contributions of the paper can be summarized as follows The paper proposes an algorithm for distributed Column Subset Selection CSS which rst learns a concise representation of the data matrix and then selects columns from distributed sub-matrices that approximate this concise representation To facilitate CSS from different sub-matrices a fast and accurate algorithm for generalized CSS is proposed This algorithm greedily selects a subset of columns from a source matrix which approximates the columns of a target matrix A MapReduce-efìcient algorithm is proposed for learning a concise representation using random projection The paper also presents a MapReduce algorithm for distributed CSS which only requires two passes over the data with a very low communication overhead Large-scale experiments have been conducted on benchmark data sets in which different methods for CSS are compared The rest of the paper is organized as follows Section II describes the notations used throughout the paper Section III gives a brief background on the CSS problem Section IV describes a centralized greedy algorithm for CSS which is the core of the distributed algorithm presented in this paper Section V gives a necessary background on the framework of MapReduce The proposed MapReduce algorithm for distributed CSS is described in details in Section VI Section VII reviews the state-of-the-art CSS methods and their applicability to distributed data In Section VIII an empirical evaluation of the proposed method is described Finally Section IX concludes the paper II N OTATIONS The following notations are used throughout the paper unless otherwise indicated Scalars are denoted by small letters e.g   sets are denoted in script letters e.g   vectors are denoted by small bold italic letters e.g   and matrices are denoted by capital letters e.g   The subscript indicates that the variable corresponds to the th block of data in the distributed environment In addition the following notations are used For a set  the cardinality of the set For a vector  th element of  the Euclidean norm  norm of  For a matrix  th entry of  th row of  th column of  the sub-matrix of which consists of the set of columns the transpose of  the Frobenius norm of   a low rank approximation of  a rankapproximation of based on the set of columns where  III C OLUMN S UBSET S ELECTION CSS The Column Subset Selection CSS problem can be generally deìned as the selection of the most representative columns of a data matrix 5 6 The CSS problem generalizes the problem of selecting representative data instances as well as the unsupervised feature selection problem Both are crucial tasks that can be directly used for data analysis or as pre-processing steps for developing fast and accurate algorithms in data mining and machine learning Although different criteria for column subset selection can be deìned a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns 10 11 12  4 5 6 14 Most of the recent w ork either develops CSS algorithms that directly optimize this criterion or uses this criterion to assess the quality of the proposed CSS algorithms In the present work the CSS problem is formally deìned as Problem 1 Column Subset Selection Given an matrix and an integer  nd a subset of columns such that and where is an projection matrix which projects the columns of onto the span of the candidate columns  The criterion F represents the sum of squared errors between the original data matrix and its rankcolumn-based approximation where  1 
     S S S S S S S S S 
002 
     002 037 037   argmin    037  
2    2   2      2   
m i m n ij i j T F F i,j ij F F 
S R S S 002 003 003 002 S 003 003 003 003 S S  L L L 003  003  S 003  003 S 
R R 
172 


E A A P P A A A A  A A A A A A T A A T Q A Q W Q A A W O n mnl E  E A P A E E E t p p i  t G n n E G E E t p p G G G E E E A A t t G G  G  A A A  E G G G i A A 003  g  
   037  argmin      037  037  037 argmin   1  argmax   037 1    002     f   g   2 002   003    003  
In other words the criterion calculates the Frobenius norm of the residual matrix  Other types of matrix norms can also be used to quantify the reconstruction error Some of the recent work on the CSS problem 5  deri v es theoretical bounds for both the Frobenius and spectral norms of the residual matrix The present work however focuses on developing algorithms that minimize the Frobenius norm of the residual matrix The projection matrix can be calculated as 2 where is the sub-matrix of which consists of the columns corresponding to  It should be noted that if is known the term is the closed-form solution of least-squares problem  The set of selected columns i.e data instances or features can be directly presented to a data analyst to learn about the insights of the data or they can be used to preprocess the data for further analysis For instance the selected columns can be used to obtain a low-dimensional representation of all columns into the subspace of selected ones This representation can be obtained by calculating an orthogonal basis for the selected columns and then embedding all columns of into the subspace of as  The selected columns can also be used to calculate a column-based low-rank approximation of  Moreover the leading singular values and vectors of the lowdimensional embedding can be used to approximate those of the data matrix IV G REEDY CSS The column subset selection criterion presented in Section III measures the reconstruction error of a data matrix based on the subset of selected columns The minimization of this criterion is a combinatorial optimization problem whose optimal solution can be obtained in  This section brieîy describes a deterministic greedy algorithm for optimizing this criterion which extends the greedy method for unsupervised feature selection recently proposed by Farahat et al 16 A brief description of this method is included in this section for completeness The reader is referred to for the proofs of the dif ferent formulas presented in this section The greedy CSS is based the follo wing recursi v e formula for the CSS criterion Given a set of columns  For any  where  and is the low-rank approximation of based on the subset of columns See 16 Theorem The term represents the decrease in reconstruction error achieved by adding the subset of columns to  This recursive formula allows the development of an efìcient greedy algorithm that approximates the optimal solution of the column subset selection problem At iteration  the goal is to nd column such that 3 where is the set of columns selected during the rst iterations Let be an matrix which represents the innerproducts over the columns of the residual matrix  i.e  The greedy selection problem can be simpliìed to See 16 Section At iteration  nd column such that where  and is the set of columns selected during the rst iterations For iteration  deìne 002 and 003  The vector 002 can be calculated in terms of and previous 003 s as 002 4 The numerator and denominator of the selection criterion at each iteration can be calculated in an efìcient manner without explicitly calculating or using the following theorem Let f and g be the numerator and denominator of the criterion function for column respectively f  and g  Then f where represents the Hadamard product operator See 16 Theorem Algorithm 1 shows the complete greedy CSS algorithm The distributed CSS algorithm presented in this paper introduces a generalized variant of the greedy CSS algorithm in which a subset of columns is selected from a source matrix such that the reconstruction error of a target matrix is minimized The distributed CSS method uses the greedy generalized CSS algorithm as the core method for selecting columns at different machines as well as in the nal selection step 
Theorem 1 Proof Problem 2 Greedy Column Subset Selection Theorem 2 Proof 
F F F F 
T T T T T F T l F F i T i i ii T p p pp p t t T p t r r p 003 r i i i ii i i n i i n t T t r r t t t 
S S S S S S  S S S S  S 002 S R P R R S     
003 004 003 004 003 004 005 005 002 006 007 f 007 003 007 007 003 010 003  r  010\010 010 007 g 010 
       1     1   2 2   2  2        1 1 003      2 1 1   2 1   T 003 2  1    1 
S  S S 003  003 S P\004S S P 003 003  R S\\P 003 003 R P S\005  S   003 003  S   003 003  006  003 003 003 006 003  006 003 006 
173 


   1 1 argmax         
A l A A A A i n t l p  p A A  b b A A A i A P A  A c A 
Algorithm 1 Input Output 
f g f g 002 003 003 003 002 002 f g 
1 Initialize 2 Initialize 3 Repeat 5 6 7 Update 
Greedy Column Subset Selection Data matrix  Number of columns Selected subset of columns  for  4  s s Theorem 2 V M AP R EDUCE P ARADIGM MapReduce w as presented as a programming model to simplify large-scale data analytics over a distributed environment of commodity machines The rationale behind MapReduce is to impose a set of constraints on data access at each individual machine and communication between different machines to ensure both the scalability and faulttolerance of the analytical tasks Currently MapReduce is considered the de-facto solution for many data analytics tasks over large distributed clusters 18 A MapReduce job is executed in two phases of userdeìned data transformation functions namely map and reduce phases The input data is split into physical blocks distributed among the nodes Each block is viewed as a list of key-value pairs In the rst phase the key-value pairs of each input block are processed by a single map function running independently on the node where the block is stored The key-value pairs are provided one-by-one to the map function The output of the map function is another set of intermediate key-value pairs The values associated with the same key across all nodes are grouped together and provided as an input to the reduce function in the second phase Different groups of values are processed in parallel on different machines The output of each reduce function is a third set of key-value pairs and collectively considered the output of the job It is important to note that the set of the intermediate key-value pairs is moved across the network between the nodes which incurs signiìcant additional execution time when much data are to be moved For complex analytical tasks multiple jobs are typically chained together and/or man y rounds of the same job are executed on the input data set In addition to the programming model constraints Karloff et al deìned a set of computational constraints that ensure the scalability and the efìciency of MapReducebased analytical tasks These computational constraints limit the used memory size at each machine the output size of both the map and reduce functions and the number of rounds used to complete a certain tasks The MapReduce algorithms presented in this paper adhere to both the programming model constraints and the computational constraints The proposed algorithm aims also at minimizing the overall running time of the distributed column subset selection task to facilitate interactive data analytics VI D ISTRIBUTED CSS ON M AP R EDUCE This section describes a MapReduce algorithm for the distributed column subset selection problem Given a big data matrix whose columns are distributed across different machines the goal is to select a subset of columns from such that the CSS criterion F is minimized One na  ve approach to perform distributed column subset selection is to select different subsets of columns from the sub-matrices stored on different machines The selected subsets are then sent to a centralized machine where an additional selection step is optionally performed to lter out irrelevant or redundant columns Let be the submatrix stored at machine  the na  ve approach optimizes the following function 5 where is the set of columns selected from and is the number of physical blocks of data The resulting set of columns is the union of the sets selected from different submatrices  The set can further be reduced by invoking another selection process in which a smaller subset of columns is selected from  The na  ve approach however simple is prone to missing relevant columns This is because the selection at each machine is based on approximating a local sub-matrix and accordingly there is no way to determine whether the selected columns are globally relevant or not For instance suppose the extreme case where all the truly representative columns happen to be loaded on a single machine In this case the algorithm will select a less-than-required number of columns from that machine and many irrelevant columns from other machines In order to alleviate this problem the different machines have to select columns that best approximate a common representation of the data matrix To achieve that the proposed algorithm rst learns a concise representation of the span of the big data matrix This concise representation is relatively small and it can be sent over to all machines After that each machine can select columns from its submatrix that approximate this concise representation The proposed algorithm uses random projection to learn this concise representation and proposes a generalized Column Subset Selection CSS method to select columns from different machines The details of the proposed methods are explained in the rest of this section 
i T i i T i i i t i t i t T p t r r p r t t t p i i i c i i i F i i c i i 
0  2 0          1 1             1     2     1    
011 002 006 012 012 012 012 012 012 
S S  003 003 007 S S\005   S S  L L 005 L L 
 L  i  L 
174 


   003  003\010\003  003 010 003  003 S 003  003 011\003  003 003  003 003  003  L L  L 003  003  S L  012 013 014 015 012 013 014 012 013 012 013 012 013   012 013 S  
B A n r X m n Y X X X 003 X X X X 003 X X  003 A P A A P A A A A P A A P A  A P A A P A B A m n A i l l B P B  B A n r A i.i.d A i A A i A B A B A A r B A  i j B i A    B B A j m j B j 013 B  B   B B B j B A A m r O nmr n m reducers B B m r A O cmr c emit key value B P A A A A B A B P B 012 012 
A Random Projection Problem 3 map in-memory B Generalized CSS 
in-memory combiner 
Distributed Column Subset Selection Algorithm 2 Input Output foreach v v v v v v for to foreach 
1 map 2 3 4 Generate 5 6 7 emit 8 reduce 9 10 11 emit summation can also be replaced by a MapReduce  
The rst step of the proposed algorithm is to learn a concise representation for a distributed data matrix  In the proposed approach a random projection method is employed Random projection   is a well-kno wn technique for dealing with the curse-of-the-dimensionality problem Let be a random projection matrix of size  and given a data matrix of size  the random projection can be calculated as  It has been shown that applying random projection to preserves the pairwise distances between vectors in the row space of with a high probability 6 where is an arbitrarily small factor Since the CSS criterion measures the reconstruction error between the big data matrix and its low-rank approximation  it essentially measures the sum of the distances between the original rows and their approximations This means that when applying random projection to both and  the reconstruction error of the original data matrix will be approximately equal to that of when both are approximated using the subset of selected columns 7 So instead of optimizing  the distributed CSS can approximately optimize  Let  the distributed column subset selection problem can be formally deìned as Given an sub-matrix which is stored at node and an integer  nd a subset of columns such that and where  is an random projection matrix is the set of the indices of the candidate columns and is the set of the indices of the selected columns from  A key observation here is that random projection matrices whose entries are sampled from some univariate distribution can be exploited to compute random projection on MapReduce in a very efìcient manner Examples of such matrices are Gaussian random matrices uniform random sign   matrices and sparse random sign matrices In order to implement random projection on MapReduce the data matrix is distributed in a column-wise fashion and viewed as pairs of where is the th column of  Recall that can be rewritten as 8 Fast Random Projection on MapReduce Data matrix  Univariate distribution  Number of dimensions Concise representation   and since the function is provided one columns of at a time one does not need to worry about pre-computing the full matrix  In fact for each input column a new vector needs to be sampled from  So each input column generates a matrix of size which means that data should be moved across the network to sum the generated matrices at independent each summing a row to obtain  To minimize that network cost an summation can be carried out over the generated matrices at each mapper This can be done incrementally after processing each column of  That optimization reduces the network cost to  where is the number of physical blocks of the matrix 1  Algorithm 2 outlines the proposed random projection algorithm The term is used to refer to outputting new pairs from a mapper or a reducer This section presents the generalized column subset selection algorithm which will be used to perform the selection of columns at different machines While Problem 1 is concerned with the selection of a subset of columns from a data matrix which best represent other columns of the same matrix Problem 3 selects a subset of columns from a source matrix which best represent the columns of a different target matrix The objective function of Problem 3 represents the reconstruction error of the target matrix based on the selected columns from the source matrix and the term is the projection matrix which projects the columns of onto the subspace of the columns selected from  In order to optimize this new criterion a greedy algorithm can be introduced Let be the 1 The 
S S S S S S S S  S S 003 S S 004  S S 
i j i j i j F F F F i i i i i i i F i i i i n 006 i i i ij m r i r j i j j j c j j c i i j j i i j T T F 
F  F 
            2   2   2   2                 2       1    1 2   1  2     014  1             1    2 
003  003 003 1  003 003 1     003 003 003 003 003  003  argmin  003 003 004 1  003  003 004  003 003 004  0   004     1            011    003 003 004        012 012 
175 


Theorem 3 Proof Problem 4 Greedy Generalized CSS Theorem 4 
      037  037   037  037  037 003 037 003 037 003 037 003 003 003 003  003   003  003  003 003 003 003             2      037    argmin    arg max 037   1 1 argmax     002  004   037     argmax     1    004     f   g   2 002   003    003  
distributed CSS criterion the following theorem derives a recursive formula for  Given a set of columns  For any  where  and is the low-rank approximation of based on the subset of columns of  Using the recursive formula for the low-rank approximation of   and multiplying both sides with gives Low-rank approximations can be written in terms of projection matrices as Using  Let  The matrix is the residual after approximating using the set of columns This means that Substituting in gives Using gives Using the relation between Frobenius norm and trace trace trace trace Using and proves the theorem Using the recursive formula for allows the development of a greedy algorithm which at iteration optimizes 9 Algorithm 3 Greedy Generalized Column Subset Selection Input Source matrix  Target matrix  Number of columns Selected subset of columns  g for  3   005 s g s Theorem 4 Let and  the objective function of this optimization problem can be simpliìed as follows trace 10 This allows the deìnition of the following generalized CSS problem At iteration  nd column such that where    and is the set of columns selected during the rst iterations For iteration  deìne 004 and 005  The vector 004 can be calculated in terms of  and previous 003 s and 005 s as 004  Similarly the numerator and denominator of the selection criterion at each iteration can be calculated in an efìcient manner using the following theorem Let f and g be the numerator and denominator of the greedy criterion function for column respectively f  and g  Then f where represents the Hadamard product operator As outlined in Section VI-A the algorithmês distribution strategy is based on sharing the concise representation of the data among all mappers Then independent columns 
1 Initialize f 2 Repeat 4 002 5 004 6 003 7 Update f 
012 012 012 012 012 012 007 010 012 012 012 012 012 012 012 012 012 012 012 012 012 012 012 012 015 007 010 007 010 016 007 010 007 010 012 012 012 012 012 012 012 012 012 012 012 012 011 011 002 002 002 002 012 012 012 012 012 012 012 012 012 003 004 012 012 012 007 003 004 010 012 012 012 012 005 005 002 011 007 f 007 003 007 007 005 010 003  r  010\010 010 007 g 010 
 F  F  F  F  F  F  F  F  F  F 
2                               2     2     2             2   2 2   2 0  2 0          1 1 003        1 1 003                 2    1  2    1   2    2  2            1 1 003      2 1 1   2 1   2  1    1   
F F F F T T T T T T F F F i i i F i T i i T i i i f t i t i t T p t r r p 003 r t T p t r r p 005 r t t t p t t t p i i T T i F i T i i T i F T i T i i T i T i T i i i ii i i ii T T p p pp p t t T p t r r p 005 r i i i ii i i n i i n t T t r r T 005 t t t b 
R P R P S P R S P R S P R S P R P P P S P R S P R P R R R R R R R R R R         S S     
S S P\004S S P   R S\\P  P    S  S    S  S     003 003  P 003 003 S\005  S\005  S 003 003 007 S S\005    003 003 003 003   S   003 003  006  003 005 003 006 003  006 003 006 
F  F B P B F F E A P A A A A E A A E  P A P A R E  B A P B P B R E  F E F B F E A P A A P A B P B P B P B R F B P B B P B R F F B P B F R F F R F F R F F F F R F F R R F F F F R F F R F F F R F i t p i F A B l Output B A A A i n t l p  g p A A B A   G E E H F E F E E E E F F E E E E F F E E E H G  t p p H G H F E G E E F B P B E A P A t t H H  G  A B B A H G i A B 005  g  B l 
176 


map reduce 
A m n B l C A i A A A A A B,l j  A A  A   A A A  A   A A B l j  A l c l B l b l l l/c b   c l k l k k A l k k l k k 
Algorithm 4 Input Output foreach foreach in foreach in 
1 2 3 4 5 7 emit 8 9 For all values 10 11 13 emit 
1 2  c  1 2  c  020 
Distributed CSS on MapReduce Matrix of size  Concise representation  Number of columns Selected columns GeneralizedCSS  6 GeneralizedCSS     12 from each mapper are selected using the generalized CSS algorithm A second phase of selection is run over the 011 where is the number of input blocks columns to nd the best columns to represent  Different ways can be used to set for each input block  In the context of this paper the set of is assigned uniform values for all blocks i.e  Other methods are to be considered in future extensions Algorithm 4 sketches the MapReduce implementation of the distributed CSS algorithm It should be emphasized that the proposed MapReduce algorithm requires only two passes over the data set and its moves a very few amount of the data across the network VII R ELATED W ORK Different approaches have been proposed for selecting a subset of representative columns from a data matrix This section focuses on brieîy describing these approaches and their applicability to massively distributed data matrices The Column Subset Selection CSS methods can be generally categorized into randomized deterministic and hybrid The randomized methods sample a subset of columns from the original matrix using carefully chosen sampling probabilities Frieze et al w as the rst to suggest the idea of randomly sampling columns from a matrix and using these columns to calculate a rankapproximation of the matrix where  That work of Frieze et al was followed by different papers 11 that enhanced the algorithm by proposing different sampling probabilities Drineas et al proposed a subspace sampling method which samples columns using probabilities proportional to the norms of the rows of the top right singular vectors of  Deshpande et al proposed an adapti v e sampling method which updates the sampling probabilities based on the columns selected so far Column subset selection with uniform sampling can be easily implemented on MapReduce For non-uniform sampling the efìciency of implementing the selection on MapReduce is determined by how easy are the calculations of the sampling probabilities The calculations of probabilities that depend on calculating the leading singular values and vectors are time-consuming on MapReduce On the other hand adaptive sampling methods are computationally very complex as they depend on calculating the residual of the whole data matrix after each iteration The second category of methods employs a deterministic algorithm for selecting columns such that some criterion function is minimized This criterion function usually quantiìes the reconstruction error of the data matrix based on the subset of selected columns The deterministic methods are slower but more accurate than the randomized ones In the area of numerical linear algebra the column pivoting method exploited by the QR decomposition permutes the columns of the matrix based on their norms to enhance the numerical stability of the QR decomposition algorithm The rst columns of the permuted matrix can be directly selected as representative columns Besides methods based on QR decomposition different recent methods have been proposed for directly selecting a subset of columns from the data matrix Boutsidis et al proposed a deterministic column subset selection method which rst groups columns into clusters and then selects a subset of columns from each cluster C ivril and Magdon-Ismail presented a deterministic algorithm which greedily selects columns from the data matrix that best represent the right leading singular values of the matrix Recently Boutsidis et al presented a column subset selection algorithm which rst calculates the topright singular values of the data matrix where is the target rank and then uses deterministic sparsiìcation methods to select columns from the data matrix Besides other deterministic algorithms have been proposed for selecting columns based on the volume deìned by them and the origin 25 The deterministic algorithms are more complex to implement on MapReduce For instance it is time-consuming to calculate the leading singular values and vectors of a massively distributed matrix or to cluster their columns using means It is also computationally complex to calculate QR decomposition with pivoting Moreover the recently proposed algorithms for volume sampling are more complex than other CSS algorithms as well as the one presented in this paper and they are infeasible for large data sets A third category of CSS techniques is the hybrid methods which combine the beneìts of both the randomized and deterministic methods In these methods a large subset of columns is randomly sampled from the columns of the data matrix and then a deterministic step is employed to reduce 
S S S S S S 
               1   2       0 1   2       0 0  1         
      0          017        0    1 2 
b i b b i b b b j c c j c b b b b b 
 012 013 S S 012 013   S S 012 013 016 017\015 002 020 020 
177 


RCV1-200K TinyImages-1M RCV1-200K TinyImages-1M key-value 
O l l l l l A A A A A A A A  A l A l A l l l l 
columns based on probabilities calculated using the leading right singular vectors and then employs a deterministic algorithm to select exactly columns from the columns sampled in the rst stage However the algorithm depends on calculating the leading right singular vectors which is time-consuming for large data sets The hybrid algorithms for CSS can be easily implemented on MapReduce if the randomized selection step is MapReduce-efìcient and the deterministic selection step can be implemented on a single machine This is usually true if the number of columns selected by the randomized step is relatively small In comparison to other CSS methods the algorithm proposed in this paper is designed to be MapReduce-efìcient In the distributed selection step representative columns are selected based on a common representation The common representation proposed in this work is based on random projection This is more efìcient than the work of C ivril and Magdon-Ismail which selects columns based on the leading singular vectors In comparison to other deterministic methods the proposed algorithm is speciìcally designed to be parallelized which makes it applicable to big data matrices whose columns are massively distributed On the other hand the two-step of distributed then centralized selection is similar to that of the hybrid CSS methods The proposed algorithm however employs a deterministic algorithm at the distributed selection phase which is more accurate than the randomized selection employed by hybrid methods in the rst phase VIII E XPERIMENTS Experiments have been conducted on two big data sets to evaluate the efìciency and effectiveness of the proposed distributed CSS algorithm on MapReduce The properties of the data sets are described in Table I The is a subset of the RCV1 data set which has been prepared and used by Chen et al to e v aluate parallel spectral clustering algorithms The data set contains 1 million images that were sampled from the 80 million tiny images data set and con v erted to grayscale Similar to previous work on CSS the different methods are evaluated according to their ability to minimize the reconstruction error of the data matrix based on the subset of selected columns In order to quantify the reconstruction error across different data sets a relative accuracy measure is deìned as Relative Accuracy where is the rankapproximation of the data matrix based on a random subset of columns is the rankapproximation of the data matrix based on the subset of columns and is the best rankapproximation of the data matrix calculated using the Singular Value Decomposition SVD This measure compares different methods relative to the uniform sampling as a baseline with higher values indicating better performance The experiments were conducted on Amazon EC2 2 clusters which consist of 10 instances for the data set and 20 instances for the data set Each instance has a 7.5 GB of memory and a two-cores processor All instances are running Debian 6.0.5 and Hadoop version 1.0.3 The data sets were converted into a binary format in the form of a sequence of pairs Each pair consisted of a column index as the key and a vector of the column entries That is the standard format used in Mahout 3 for storing distributed matrices The distributed CSS method has been compared with different state-of-the-art methods It should be noted that most of these methods were not designed with the goal of applying them to massively-distributed data and hence their implementation on MapReduce is not straightforward However the designed experiments used the best practices for implementing the different steps of these methods on MapReduce to the best of the authors knowledge In speciìc the following distributed CSS algorithms were compared  is uniform sampling of columns without replacement This is usually the worst performing method in terms on approximation error and it will be used as a baseline to evaluate methods across different data sets  and  are different distributed variants of the hybrid CSS algorithm which can be implemented efìciently on MapReduce In the randomized phase the three methods use probabilities calculated based on uniform sampling column norms and the norms of the leading singular vectors rows respectively The number of selected columns in the randomized phase is set to  In the deterministic phase the centralized greedy CSS is employed to select exactly columns from the randomly sampled columns  is an extension of the centralized algorithm for sparse approximation of Singular Value Decomposition SVD The distrib uted CSS algorithm presented in this paper Algorithm 4 is used 2 Amazon Elastic Compute Cloud EC2 http://aws.amazon.com/ec2 3 Mahout is an Apache project for implementing Machine Learning algorithms on Hadoop See http://mahout.apache.org 
Table I T HE PROPERTIES OF THE DATA SETS USED TO EVALUATE THE DISTRIBUTED CSS METHOD  RCV1-200K Documents 193,844 47,236 TinyImages-1M Images 1 million 1,024 the number of selected columns to the desired rank For instance Boutsidis et al proposed a tw o-stage hybrid CSS algorithm which rst samples 
003  003 003  003 003  003 003  003  U S 
Data set Type  Instances  Features 
UniNoRep HybirdUni HybirdCol HybirdSVD DistApproxSVD 
F F F l F l 
U S U U S    
 log   037 037 037 037 100 037 037 037 log   
178 


B U 
DistGreedyCSS 
 The use of the distributed CSS algorithm extends the original algorithm proposed by C ivril and Magdon-Ismail to work on distributed matrices In order to allow efìcient implementation on MapReduce the number of leading singular vectors is set of   is the distributed column subset selection method described in Algorithm 4 For all experiments the dimension of the random projection matrix is set to  This makes the size of the concise representation the same as the DistApproxSVD method Two types of random matrices are used for random projection 1 a dense Gaussian random matrix rnd and 2 a sparse random sign matrix ssgn For the methods that require the calculations of Singular Value Decomposition SVD the Stochastic SVD SSVD algorithm is used to approximate the leading singular values and vectors of the data matrix The use of SSVD signiìcantly reduces the run time of the original SVDbased algorithms while achieving comparable accuracy In the conducted experiments the SSVD implementation of Mahout was used Table II shows the run times and relative accuracies for different CSS methods It can be observed from the table that for the data set the DistGreedyCSS methods with random Gaussian and sparse random sing matrices outperforms all other methods in terms of relative accuracies In addition the run times of both of them are relatively small compared to the DistApproxSVD method which achieves accuracies that are close to the DistGreedyCSS method Both the DistApproxSVD and DistGreedyCSS methods achieve very good approximation accuracies compared to randomized and hybrid methods It should also be noted that using a sparse random sign matrix for random projection takes much less time than a dense Gaussian matrix while achieving comparable approximation accuracies Based on this observation the sparse random matrix has been used with the data set For the data set although the DistApproxSVD achieves slightly higher approximation accuracies than DistGreedyCSS with sparse random sign matrix the DistGreedyCSS selects columns in almost one-third of the time The reason why the DistApproxSVD outperforms DistGreedyCSS for this data set is that its rank is relatively small less than 1024 This means that using the leading 100 singular values to represent the concise representation of the data matrix captures most of the information in the matrix and accordingly is more accurate than random projection The DistGreedyCSS however still selects a very good subset of columns in a relatively small time IX C ONCLUSION This paper proposes an accurate and efìcient MapReduce algorithm for selecting a subset of columns from a massively distributed matrix The algorithm starts by learning a concise representation of the data matrix using random projection It then selects columns from each sub-matrix that best approximate this concise approximation A centralized selection step is then performed on the columns selected from different sub-matrices In order to facilitate the implementation of the proposed method a novel algorithm for greedy generalized CSS is proposed to perform the selection from different submatrices In addition the different steps of the algorithms are carefully designed to be MapReduce-efìcient Experiments on big data sets demonstrate the effectiveness and efìciency of the proposed algorithm in comparison to other CSS methods when implemented on distributed data R EFERENCES  A K Jain and R C Dubes 
Table II T HE RUN TIMES AND RELATIVE ACCURACIES OF DIFFERENT CSS METHODS T HE BEST PERFORMING METHOD FOR EACH 0.6 0.6 0.5 0.00 0.00 0.00 0.8 0.8 2.9 2.37 1.28 4.49 1.6 1.5 3.7 4.54 0.81 6.60 1.3 1.4 3.6 9.00 12.10 18.43 16.6 16.7 18.8 41.50 57.19 63.10 5.8 6.2 7.9 61.92 67.75 2.2 2.9 5.1 40.30 1.3 1.3 1.3 0.00 0.00 0.00 1.5 1.7 8.3 19.99 6.85 6.50 3.3 3.4 9.4 17.28 3.57 7.80 52.4 52.5 59.4 3.59 8.57 10.82 71.0 70.8 75.2 22.1 23.6 24.2 67.58 25.18 20.74 to select columns that best approximate the leading singular vectors by setting 
Run time minutes Relative accuracy  RCV1 200K Uniform Baseline Hybird Uniform Hybird Column Norms Hybird SVD-based Distributed Approx SVD Distributed Greedy CSS rnd 51.76 Distributed Greedy CSS ssgn 62.41 67.91 Tiny Images 1M Uniform Baseline Hybird Uniform Hybird Column Norms Hybird SVD-based Distributed Approx SVD 70.02 31.05 24.49 Distributed Greedy CSS ssgn 
Algorithms for Clustering Data 
 002 100 100 
l l l l l l l 
10  100  500 10  100  500 
 Upper Saddle River NJ USA Prentice-Hall Inc 1988 
 
k k 
RCV1-200K TinyImages-1M TinyImages-1M 
IS HIGHLIGHTED IN BOLD  AND THE SECOND BEST METHOD IS UNDERLINED N EGATIVE MEASURES INDICATE METHODS THAT PERFORM WORSE THAN UNIFORM SAMPLING  Methods 
179 


 L Kaufman and P  Rousseeuw  Clustering by means of medoids Technische Hogeschool Delft Netherlands Department of Mathematics and Informatics Tech Rep 1987  S Deerwester  S Dumais G Furnas T  Landauer  and R Harshman Indexing by latent semantic analysis  vol 41 no 6 pp 391Ö407 1990  C Boutsidis J Sun and N Anerousis Clustered subset selection and its applications on it service metrics in  2008 pp 599Ö608  C Boutsidis M W  Mahone y  and P  Drineas  An impro v ed approximation algorithm for the column subset selection problem in  2009 pp 968Ö977  C Boutsidis P  Drineas and M Magdon-Ismail Near optimal column-based matrix reconstruction in  2011 pp 305 314  J Dean and S Ghema w at MapReduce Simpliìed data processing on large clusters  vol 51 no 1 pp 107Ö113 2008  T  White  1st ed OêReilly Media Inc 2009  A Frieze R Kannan and S V empala F ast Monte-Carlo algorithms for nding low-rank approximations in  1998 pp 370 378  P  Drineas A Frieze R Kannan S V empala and V  V inay  Clustering large graphs via the singular value decomposition  vol 56 no 1-3 pp 9Ö33 2004  P  Drineas R Kannan and M Mahone y  F ast Monte Carlo algorithms for matrices II Computing a low-rank approximation to a matrix  vol 36 no 1 pp 158Ö183 2007  P  Drineas M Mahone y  and S Muthukrishnan Subspace sampling and relative-error matrix approximation Column-based methods in  Springer Berlin  Heidelberg 2006 pp 316Ö326  A Deshpande L Rademacher  S V empala and G W ang Matrix approximation and projective clustering via volume sampling  vol 2 no 1 pp 225Ö247 2006  A C  i vril and M Magdon-Ismail Column subset selection via sparse approximation of SVD  vol 421 no 0 pp 1  14 2012  A K F arahat A Ghodsi and M S Kamel  An ef cient greedy method for unsupervised feature selection in  2011 pp 161 170   Ef cient greedy feature selection for unsupervised learning  vol 35 no 2 pp 285Ö310 2013  T  Elsayed J Lin and D W  Oard P airwise document similarity in large collections with MapReduce in  2008 pp 265Ö268  A Ene S Im and B Mosele y  F ast clustering using MapReduce in  2011 pp 681Ö689  H Karlof f S Suri and S V assilvitskii A model of computation for MapReduce in  2010 pp 938Ö948  S Dasgupta and A Gupta An elementary proof of a theorem of Johnson and Lindenstrauss  vol 22 no 1 pp 60Ö65 2003  D Achlioptas Database-friendly random projections Johnson-Lindenstrauss with binary coins  vol 66 no 4 pp 671Ö687 2003  P  Li T  J Hastie and K W  Church V ery sparse random projections in  2006 pp 287Ö296  G Golub and C V an Loan  3rd ed Johns Hopkins Univ Pr 1996  A Deshpande and L Rademacher  Ef cient v olume sampling for row/column subset selection in  2010 pp 329 338  V  Gurusw ami and A K Sinop Optimal column-based lo wrank matrix reconstruction in  2012 pp 1207Ö1214  D D Le wis Y  Y ang T  G Rose and F  Li Rcv1 A ne w benchmark collection for text categorization research  vol 5 pp 361Ö397 2004  W Y  Chen Y  Song H Bai C.-J Lin and E Chang Parallel spectral clustering in distributed systems  vol 33 no 3 pp 568 586 2011  A T orralba R Fer gus and W  Freeman 80 million tin y images A large data set for nonparametric object and scene recognition  vol 30 no 11 pp 1958Ö1970 2008  N Halk o P G Martinsson Y  Shk olnisk y  and M T ygert An algorithm for the principal component analysis of large data sets  vol 33 no 5 pp 2580Ö2594 2011 
Journal of the American Society for Information Science and Technology Proceedings of the Seventeenth ACM Conference on Information and Knowledge Management CIKMê08 Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms SODAê09 Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science FOCSê11 Communications of the ACM Hadoop The Deìnitive Guide Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science FOCSê98 Machine Learning SIAM Journal on Computing Approximation Randomization and Combinatorial Optimization Algorithms and Techniques Theory of Computing Theoretical Computer Science Proceedings of the Eleventh IEEE International Conference on Data Mining ICDMê11 Knowledge and Information Systems Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies Short Papers HLTê08 Proceedings of the Seventeenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDDê11 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODAê10 Random Structures and Algorithms Journal of computer and System Sciences Proceedings of the Twelfth ACM SIGKDD international conference on Knowledge Discovery and Data Mining KDDê06 Matrix Computations Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science FOCSê10 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODAê12 The Journal of Machine Learning Research Pattern Analysis and Machine Intelligence IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE Transactions on SIAM Journal on Scientiìc Computing 
180 


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even çgoodé partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity Ö the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the cloudês elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPês synchronous barrier between supersteps offers a window for dynamic scaleout and Öin at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an çoracleé approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workerês time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440Ö442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


