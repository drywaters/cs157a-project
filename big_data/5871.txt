Incremental Mining Frequent Itemsets Based on the Trie Structure and the Pre-Large Itemsets  Thien-Phuong Le Faculty of Technology Pacific Ocean University Nha Trang City, VietNam e-mail phuonglt@pou.edu.vn  Bay Vo Department of Computer Science University of Technology Ho Chi Minh City, VietNam e-mail vdbay@hcmhutech.edu.vn  Tzung-Pei Hong Department of CSIE National University of Kaohsiung Kaohsiung City, Taiwan, R.O.C e-mail tphong@nuk.edu.tw  Bac Le Department of Computer Science University of Science Ho Chi Minh City, Vietnam e-mail lhbac@fit.hcmus.edu.vn    Abstract Incremental data mining has been discussed widely in recent years since it has many real applications. In this paper, we propos e a Pre-FUT algorithm \(Fast-Update algorithm using Trie data structure and the concept of pre-large itemsets\whic h does not only build and update the trie structure when new transactions are inserted, but also mine all frequent itemsets easily from the tree. Experimental results show the good performance of the proposed algorithm Keywords-data mining; trie structure; prel arge itemset incrementa l mining I  I NTRODUCTION  Many algorithms for mining association rules from transactions were proposed, most of which were based on the Apriori algorithm whic h gene rate-a nd-t est candidates in each level. This might need scanning databases iteratively and cause hi gh computational cost. An important data structure used in the Apriori alg orithm is the hash-tree [2 200 3, Bodo n an d Ron yai [1 1  ad op ted t he trie data structure to profitably substitute hash-trees. In addition to Apriori-based algorithms, Han et al 4  proposed a new approach of mining frequent itemsets \(FIs They proposed the Freque nt-Pattern tree \(FP-tree\ure for efficiently mining FIs without candidate generation Both the Apriori and the FP-tree mining approaches must process all the transactions in a batch way In real-world applications, new transactions are usually inserted into databases incremen tally. The first incremental mining algorithm was the Fast-UPdated algorithm  called FUP\P al gorithm could indeed improve mining performance for incrementally growing databases, original databases still n eed be scanned when it is necessary. Hong et al then proposed the concept of prelarge itemsets to further reduce the need for rescanning original databa Since rescanning the database spent much computation time, the maintenance cost could thus be reduced in the pre-large itemsets algorithm. Some algorithms based on the concept of pre-large itemsets have been developed [6, 7, 8   In this paper, we propose the Pre-FUT algorithm for handling newly inserted transactions based on the pre-large itemsets concept and the trie  trie we can find FIs directly from the tree-building process Experimental results show that the proposed algorithm has a good performance for incrementally handling inserted transactions II R EVIEW OF R ELATED WORKS  In this section, some related researches to the proposed approach are briefly reviewed. They are the trie data structure and the pre-large itemsets algorithm A The trie data structure             Figure 1 A trie data structure The trie data structure was origin ally introduced to store and retrieve words of a di suitable to store and retriev e not only words, but any finite ordered sets. In 2003, Bodon and Ronyai  extended the  trie structure to mine FIs with good performance. In their approach, the trie structure stored not on ly candidates, but  0  1  7  AK C E M 2  4  10  D G M 8  3  5  6  9  L N 369 2011 IEEE International Conference on Granular Computin g 978-1-4577-0371-3/11/$26.00 ©2011 IEEE 


also FIs as well. Bodon and Ronyai showed that their approach could improve the per formance of generating candidates and determining supports. Fig. 1 presents a trie  that stores the candidate 3-itemsets A  C  D  A  E  G   A  E  L  A  E  M and K  M  N  B The pre-large itemsets algorithm The pre-large itemsets algor ithm was proposed by Hong et al  safety threshold f to reduce the need of rescanning original databases for efficiently maintaining association rules. The safety number f of inserted records was derived as follows    1 ul u SSd f S       where S u is the upper threshold S l is the lower threshold and d is the number of original transactions. When the number of new transactions is smaller than f the algorithm did not need to rescan the original database. A summary of the nine cases and their results are given in Table I TABLE I N INE CASES AND THEIR RESULTS 5 Cases: Original … New Results Case 1: Large … Large Always large Case 2: Large … Pre-large Large or pre-large, determined from existing information Case 3: Large … Small Large or pre-large or small determined from existing information Case 4: Pre-large … Large Pre-large or large, determined from existing information Case 5: Pre-large … Pre-large Always pre-large Case 6: Pre-large … Small Pre-large or small, determined from existing information Case 7: Small … Large Pre-large or small, when the number of transactions is small Case 8: Small … Pre-large Small or pre-large Case 9: Small … Small Always small III T HE P RE FUT A LGORITHM  The notation used in the proposed Pre-FUT maintenance algorithm is first described below A Notation D  the original database T  the set of new transactions U  the entire upd ated database, i.e D    T  d  the number of transactions in D  t  the number of transactions in T  S l  the lower support thresho ld for pre-large itemsets S u the upper support threshold for large itemsets S u  S l  X an itemset Tr D  a trie storing the set of pre-large and large itemsets from D  Tr U  a trie storing the set of pre-large and large itemsets from U  D   T   T count X  the number of occurrences of X in T   D Tr count X  the number of occurrences of X in Tr D   U Tr X count the number of occurrences of X in Tr U  B The Pre-FUT algorithm INPUT A lower support threshold S l an upper threshold S u a trie  Tr D storing large itemsets and pre-large itemsets derived from the origi nal database consisting of  d  c nsactions, and a set of t new transactions OUTPUT A trie Tr U storing large and pre-large itemsets, and rescan itemsets from U  The rescan itemsets in the trie are small in the original database but pre-large or large in the new transactions. They are stored in the trie but not used to generate new candidates BEGIN 1  1 ul u SSd f S       2 k 1 3 Tr U is initially root node 4 add k itemsets from the newly inserted transactions to Tr U  5 do  6  scan T to determine   U Tr T count X count X     X  Tr U with X  k  7  for each itemset X  Tr U with X  k  8  if  X does not exist in Tr D  9  if  U Tr l count X S t 0 63 41 52   Cases 7, 8, 9 in Table I 10 mark X as a rescan itemset in Tr U 11  else remove X from Tr U  12 for each itemset X  Tr D with X  k    Cases 1, 2, 3, 4, 5, 6 in Table I 13 if  X exists in Tr U  14  if   DU Tr Tr l count X count X S dtc  0  63 41 52  15     UUD Tr Tr Tr count X count X count X  16 else remove X from Tr U 17   else if   D Tr l count X S dtc 0  63 41 52  18 add itemset X to Tr U with   UD Tr Tr count X count X   19 end for 20  if  t  c  f  21  rescan the original database to determine whether the rescan itemsets are large or pre-large 22  k  k 1 23 while  Trie_gen Tr U  k  9  Trie_gen generates new candidate k itemsets from Tr U and  stores them in Tr U  24 if  t  c  f   25  d  d  t  c 26  c 0  27 else c  c  t  END   370 


C An example In this section, an example is given to illustrate the proposed incremental data mi ning algorithm. Assume the initial data set includes the 8 transactions shown in Table II For S l 30% and S u 50%, the sets of large itemsets and pre-large itemsets for the give n data are showed in Tables III and IV, respectively TABLE II A N EXAMPLE OF ORIGINAL DATABASE  Original database TID Items 1 ACE 2 ABDE 3 BCDE 4 ACE 5 ACE 6 ABC 7 BDE 8 ABCE TABLE III T HE LARGE ITEMSETS DERIVED FROM THE ORIGINAL DATABSE  Large itemsets 1 item Count 2 items Count 3 items Count A 6 AC 5 ACE 4 B 5 AE 5   C 6 BE 4   E 7 CE 5   TABLE IV T HE PRE LARGE ITEMSETS DERIVED FROM THE ORIGINAL DATABSE  Pre-large itemsets 1 item Count 2 items Count 3 items Count D 3 AB 3 BDE 3 BC 3  BD 3  DE 3   Firstly, a trie Tr D storing the set of pre-large and large itemsets in the original database is shown in Fig. 2                Figure 2 The trie  Tr D storing the set of pre-large and large itemsets in the original database  Assume the two new transactions shown in Table V are inserted after the trie  Tr D is built. The Pre-FUT algorithm proceeds as follows. The variable c is initially set at 0 TABLE V T WO NEW TRANSACTIONS  New transactions TID Items 9 ABCD 10 CEF  From Line 1 of the algorithm, we have   0.5 0.3\8 3 110.5 ul u SSd f S          From Lines 4 to 6, two newly inserted transactions are first scanned to get their 1itemsets and counts. A trie Tr U is then built from them, and the results are shown in Fig. 3       Figure 3 The trie  Tr U constructed Lines 7 to 11 mark the 1-itemse ts which are small in the original database. They are the 1-itemsets in Tr U which do not exist in Tr D but pre-large or large itemsets in the new transactions. In this example only the 1-itemset {F:1} is small in the original databa se. But it is pre-large in the new transactions and is thus marked as a rescan 1-itemset. The results are show n in Fig. 4       Figure 4 The trie Tr U after the rescan 1-itemset is marked Lines 12 to 19 then process the 1-itemsets which are prelarge or large in th e original database In this example, the large or pre-large 1-itemsets are {A:6, B:5, C:6, D:3, E:7 Since they also exist in Tr U their total counts are calculated using    UUD Tr Tr Tr count X count X count X   Table VI shows the results TABLE VI T HE TOTAL COUNTS OF A},{B},{C},{D},{E Items Count A 7 B 6 C 8 D 4 E 8   0  1 6 11 3 7 4 2 5 3 6 10 3 4 7 A B C D E B C E C D E E E E 12 3 5 5 6 5 1 3 8 5 14 3 9 4 15 3 E 0  1 1 2 1 3 2 4 1 5 1 A B C D E 6 1 F 0   1 1 2 1 3 2 4 1 5 1 A B C D E F 1 6 371 


The new support ratios of {A}, {B}, {C}, {D} and {E are then calculated. For example, the new support ratio of A} is 7/\(8+2+0\larger than 0.3. {A} is thus a pre-large itemset at least. In this example, the support ratios of {A}, {B}, {C}, {D} and {E} are greater than 0.3 such that their counts will be updated in the trie The results are shown in Fig. 5       Figure 5 The Trie Tr U after line 19 Since t  c 2 + 0 = 2 f rescanning the original database is unnecessary \(from lines 20 to 21\nothing is done. Then at line 22 k is set at 2. In this algorithm, the subfunction Trie_gen  Tr U  k rates candidate k itemsets with k 2 from trie  Tr U In this example Trie_gen  Tr U  k  9 with k 2 such that the algorithm is repeated. Note that the marked nodes will not be used to generate new candidates. The results of the trie  Tr U after the new candidate 2-itemsets are generated with their counts from the new transactions are shown in Fig. 6            Figure 6 The Trie Tr U after line 23 Similar to the first iteration, the same procedure is executed for itemsets with more items IV E XPERIMENTAL R ESULTS  All the algorithms in the experiments were coded by C++. The PC Config uration includes CPU Intel Core 2 Duo 2.1 GHz, RAM 2GB, and Windows 7. The well-known T40I10D100K \(with 100000 transactions\d in the experiments We compared the performance of the proposed Pre-FUT algorithm with that of the pre-large itemsets algorithm which used the hash-tree data structure. We do not compare our algorithm with the Pre-FUFP algorithm by Hong et al   p uted the time for building and updating the FUFP tree when the new transactions were inserted. In our experiments, we compute the time for mining all FIs of the two algorithms The first 90000 transactions wer e extracted from the T40I10D100K database for offline mining. Each next 1000 transactions were then sequentially used each time as new transactions for the experiments. The upper and the lower support thresholds were set at 5% and 2 respectively. In the experiments with the T40I10D100K database, the safety number was calculated as f  90000*\(0.05-0.02\/\(1-0.05\ = 2842. Fig. 7 shows the execution time required by the two algorithms. It can be seen from the Fig. 7 that the Pre-FUT maintenance algorithm ran faster than the pre-large itemsets algorithm even in the case when the number of inserted transactions exceeded the safety threshold   Figure 7 The comparison of the execution times for sequentially inserted transactions V C ONCLUSION  This paper has adopted an alternative data structure for incremental data mining. The Pre-FUT algorithm has been proposed, which is based on the concept of pre-large itemsets to reduce the number of database scans. It uses two user-specified upper and lower su pport thresholds to avoid small items becoming large in the updated database when transactions are inserted. All the tasks are processed and stored in a trie With these strategies, the proposed approach can thus spend less execution time than the pre-large itemsets algorithm which is coded by the hash-tree data structure R EFERENCES    R. Agrawal, T. I m ielinksi, and A. Swami, ŽMining association rules between sets of items in large database The ACM SIGMOD conference 1993, pp. 207…216, doi:10.1145/170035.170072   R. Agrawal, and R. Srikant, Fast algor ith m for mining  association rules The international  conference on very large data bases 1994, pp. 487…499  D. W. Cheung, J  Han, V T. Ng, and C. Y  Wong Maintenance of discovered association rules in  large databases An incremental updating approach The twelfth IEEE international conference on data engineering 1996, pp. 106…114   J. Han, J. Pei, and Y. Yin, Mining frequ ent p a tterns withou t candidate generation The 2000 ACM SIGMOD international conference on management of data 2000, pp. 1…12, doi 10.1145/342009.335372   T. P. Hong, C   Y. Wang and Y  H  Tao A new incr emental data mining algorithm using pre-large itemsets Intelligent Data Analysis Vol. 5, No. 2, 2001, pp. 111…129   0  1 7 2 6 3 8 4 4 5 8 A B C D E F 1 6 0  1 7 2 6 3 8 4 4 5 8 A B C D E B C D C D D E 15 1 11 1 7 1 8 1 9 1 E 12 1 13 0 E 14 1 16 0 E 1 6 F 10 0 372 


  T. P. Hong, C  W. Lin and  Y L. Wu, Incrementally fast updated frequent pattern trees In Expert Systems with Applications Vol. 34, No. 4, 2008, pp. 2424-2435, doi 10.1016/j.eswa.2007.04.009   T. P. Hong, C   W. Lin  and Y  L. Wu, Maintenance of fas t updated frequent pattern trees for record deletion Computational Statistics and Data Analysis Vol. 53, No. 7, 2009, pp. 2485-2499 doi:10.1016/j.csda.2009.01.015   T. P  Hong, an d C. Y. Wang An efficien t and ef fectiv e association-rule maintenance algorithm for record modification Expert  Systems with Applications Vol. 37, No. 1, 2010, pp. 618626, doi: 10.1016/j.eswa.2009.06.019   D. E. Knuth  T he art of computer progr ammingŽ, Vol. 3  Addison-Wesley 1968   C. W. Lin T P. Hong, and W. H. Lu, The Pre-FUFP algorithm for incremental mining Expert Systems  with Applications Vol. 36, No. 5, 2009, pp. 9498-9505 doi:10.1016/j.eswa.2008.03.014   F. Bodon and L. Ron y ai T rie: An Alter nativ e data structure for data mining algorithms Mathematical and Computer Modeling Vol. 38, No. 7-9, 2003, pp. 739-751, doi:10.1016/08957177\(03               373 


For that, the service will check based on its properties, rules and preferences defined and the resources needed whether it can or should instantiate it. If the execution is properly done, it will continue executing the services of the same level of the tree. It a service cannot be executed or an error occurred, the execution will go down a level in the structure, in order to process the subordinated services \(Service 2.1, Service 2.2 etc subordinated, the next service level to be executed will be the corresponding to the directly upper level, that is, Service 3 III. NFC SCENARIOS CREATOR With the purpose of testing the functionality and utility of the interaction model described, a tool named NFC Scenarios Creator \(NFCSC developed in Java for the scenario definition under the model described and that integrates the use of them to be understood by a MIDlet that will be executed in a NFC device NFCSC allows, therefore, the definition of the scenario rules and user interaction preferences; it also has the capability both of recording the Tags associated to the objects in the scenario as of generating the contents needed for a proper performance of the MIDlet Although NFCSC is still in development stage, the system allows to establish the hierarchy of functionalities associated to each object and to define restrictions and preferences NFCSC \(Fig. 4 scenarios or projects. For each project a set of identifying attributes are defined, as well as the objects that compose the scenario, the services associated to the objects, the general and user interactions preferences and the resources used on it Each scenario is described by a XML document, as shown in the DTD file from Chart I  CHART I. DTD FOR THE SCENARIO DEFINITION xml version=1.0 encoding=UTF-8 ELEMENT XMLProject \(XMLDescription, XMLPath XMLResource*, XMLTag ATTLIST XMLProject ID CDATA #REQUIRED name CDATA #REQUIRED ELEMENT XMLTag \(XMLDescription, XMLResource XMLService 


ATTLIST XMLTag ID CDATA #REQUIRED IDScenario CDATA #REQUIRED name CDATA #REQUIRED ELEMENT XMLService \(XMLUri?, XMLMethod?, XMLOS XMLResource*, XMLService ATTLIST XMLService ID CDATA #REQUIRED activated \(on|off execution \(manual|automatic iterative \(on|off ELEMENT XMLResource \(XMLContent?, XMLPath ATTLIST XMLResource type CDATA #REQUIRED state \(on|off ELEMENT XMLUri \(XMLContent ATTLIST XMLUri type CDATA #REQUIRED ELEMENT XMLPath \(#PCDATA ELEMENT XMLDescription \(#PCDATA ELEMENT XMLContent \(#PCDATA  11 In order to make the devices that interact with the user able to answer properly to the different scenarios, the MIDlet installed must have initial knowledge of its definition. So when an object is detected the services to be offered can be defined and also the way to execute them. Herewith, together with the installation or deployment of an application, the initial XML file of the project is also stored in the device being the processing of that file by the MIDlet the one that really defines the interaction The user can modify the XML file making use of the MIDlet, adjusting the parameters that define the interaction model \(properties and resources a user that in the scenario definition decided that when touching a certain object an action started automatically, could modify that decision converting it to manual afterwards, or activate/deactivate the vibration when touching a Tag, simply by changing the values of the corresponding properties in the XML file As can be seen in Chart I, each project is defined by a 


unique name and identifier, a description, and a set of Tags assigned which characterize the scenario together with a set of resources. The Tag identifier includes the scenario identifier which allows the MIDlet of the device to reach the XML document that has to been used in order to guide the interaction For each Tag one or several services could be defined and one or several resources could be associated to them. The services could be defined in a recursively way, which allows specifying the execution hierarchy aforementioned. For each service a set of attributes is defined, those attributes define their activation and execution mode The resources used in the interaction should also be stored in the device, so, together with the definition and, pending of the type it is possible to: define its content \(XMLContent specify the path where it is stored in the device \(SMLPath Figure 4. NFC Scenario Creator main interface As can be seen in Fig. 4, the NFCSC interface is divided in some sections. The left sidebar presents the different projects scenarios of the projects represented as the root of the tree, while the Tags assigned to each scenario are added as if they were their leaves. The system allows to edit the information of the elements from each scenario, both the Tags and the own project Pending of which element is selected among the project tree, the content of the central panel would be different. For each project the system allows to introduce a general description, as well as to assign the general resources used If a Tag is chosen, the possibilities offered by the system change, allowing: a management, c associated to the services, and d among the execution hierarchy established NFCSC has an event viewer \(frame placed under the main pane the system during the interaction with the user. Each entry in the event viewer is composed by the time when that event happened and a description that allow the user to know the action that has been performed. Thanks to this log the edition or step back in the scenario building is easier IV. AN EASY EXAMPLE. EMERGENCIES AND ASSISTANCE 


SYSTEM With the aim of showing the characteristics of the proposed model and the tool built, its application to a very simple example will be described with the purpose of not including an extension and complexity not needed  Figure 5. Emergencies and assistance example The basic idea of this scenario is that any user in urgency could ask for help in an easy, quick and intuitive way, just by touching with their NFC device an image with a Tag associated. Chart II shows a fragment with the definition of the scenario described As shown in Fig. 5, the scenario is formed by a frame where, in order to represent the emergency, a picture of an ambulance has been placed. Behind the ambulance there is a Tag. Where the device is got close to the Tag placed in the frame: a the MIDlet retrieves and decode the content in order to identify the scenario, c XML file and its resources, d corresponding action and services and, e ends up the application The full definition of the XML file that specifies the interaction in this simple example contains two services: a phone call to the emergencies number, and b message to two relatives, both making use of multiple device 12 resources. However, and for keeping it short, Chart II only shows a partial view of the example, showing only the definition of the general properties of the Tag as well as the first two services and some examples of resource use As shown in Chart II, the Tag has defined the use of two resources. First one is associated to the vibration of the device while the second is about an information text that will be displayed to the user when starting the interaction CHART II. SCENARIO INTERACTION DEFINITION. XML FILE  XMLTag ID="T1245132"IDScenario="S4514152 name="Ambulance Picture XMLDescripcion>This is the tag that will be associated to the ambulance picture XMLDescripcion 


XMLResource type="vibration" state="on XMLResource XMLResource type="InfoText" state="on XMLContent>S.O.S. Emergency application XMLContent XMLResource XMLService ID="S3292378" activated="on execution="automatic iterative="off XMLUri type="tel XMLContent>112</XMLContent XMLUri XMLResource type="Audio" state="on XMLPath>E:/scenarios.midlet/S4514152 T1245132/sound/HelpCall.wav XMLPath XMLResource XMLService XMLService ID="S3292379" activated="on execution="automatic iterative="off XMLUri type="sms XMLContent>+34667406220 </XMLContent XMLUri XMLResource type="Image" state="on XMLPath>E:/scenarios.midlet/S4514152 T1245132/pictures/ambulancia.gif</X MLPath XMLResource XMLService XMLTag   The definition of the first service is characterized by a set of parameters as its identifier, whether the execution would be manual or interactive and an URI that defines the action to be performed. In this case is a tel URI. This service makes use of an audio resource whose content it stored on the device storage. In future versions it will be implemented with a voice synthesizer Notice how the following service definition, corresponding to sending a sms, is indented from the previous one. This 


means that the service is in the same branch, but at a lower level in the hierarchy of services execution. Therefore, this service will only be conducted if an error has occurred in the execution of the previous one. As shown in chart II the latter service uses an image resource in charge of showing some information at the MIDlet display and whose content is also stored on the device Besides, the MIDlet allows in a simple way to modify all those parameters that define the execution of the services and their use. As commented before, a user could not want that a service is executed automatically and requires manual confirmation. For that, he/she only would need to change the value of the execution parameter of the service into manual. In the same way, the user can activate or deactivate the use of resources just by modifying the value of the state attribute into off. Even more, it is possible to add new resources, if, together with their definition some content needed for their execution is included Therefore, it can be checked as, even when the user define an initial interaction for a scenario and even when the same deployment is done in different devices, each user could easily adapt and customize the interaction to its needs and preferences afterwards Equally, the user could activate or deactivate a service changing the execution order and modifying the hierarchy initially defined for them. Herewith, the same scenario could be customized and act in a different way depending of the person that interacts with it V. DISCUSSION IoT is not a reality yet, but a prospective view of a series of technologies that, combined, could sharply modify the interaction mode with our environment and the working of our society in the following years. In order to establish the IoT philosophy, it would be needed an advance of the development and dissemination of elements as sensors, new communication technologies \(as well as their intercommunication mobile devices able to integrate them, a proper infrastructure and above all software applications that supports them and are able to integrate all those elements, as well as to fulfills the needs and requirements of the users Those new systems must also be adapted to the different interaction scenarios and, mainly, to the different 


characteristics, requirements, preferences and needs of the people that interact with the environment. In this paper we present a model that provides a solution to this issue The model is based in augment different objects among a scenario with Tags, by the assignment of services and resources defined in an abstracted hierarchy that allows configuring a customized interaction model. Thus, for the same scenario, the model is able to define different interaction contexts. So, a context is defined as the aggregation of: a certain scenario formed by a set of intelligent objects offering a set of services, b characteristics, preferences and needs, c and d Under the proposed model a tool that allows the definition and scenario deployment has been built. A neuter MIDlet installed in the NFC device is the one in charge of guiding the 13 interaction through the definition of the scenarios stored in a XML file. This architecture allows that the same MIDlet could be used with any scenario and that the user could adapt the interaction to his preferences Currently we are working in the completeness of the model and the complex interaction scenarios generalization, being the latter based in rules systems that allows adapting the different interaction contexts to the users ACKNOWLEDGMENT This work was supported by the Ministry of Science and Innovation of Spain \(MICINN Project TIN2009-07184 REFERENCES 1] M. Weiser. The Computer for the Twenty-First Century. Scientific American, 1991, pp. 91-104 2] G. Broll, S . Siorpaes, E. Rukzio, M. Paolucci, J. Hamard, M. Wagner and A. Schmidt. Supporting Mobile Service Usage through Physical Mobile Interaction, 5th Annual IEEE International Conference on Pervasive Computing and Communications, White Plains,NY, USA 2007 3] C. Floerkemeier, M. Langheinrich, E. Fleisch, F. Mattern and S.E Sarma. The Internet of Things. Proceedings of First International Conference, IOT 2008, Zurich, Switzerland. Lecture Notes in Computer Science, vol. 4952, 2008 4] Revising Europes ICT Strategy, ftp://ftp.cordis.europa.eu/pub/ist/docs 


istag-revising-europesict-strategy-final-version_en.pdf 5] E. Rukzio, G. Broll, K. Leichtenstern, A. Schmidt. Mobile Interaction with the Real World: An Evaluation and Comparison of Physical Mobile Interaction Technique, Ambient Intelligence. Lecture Notes in Computer Science, vol. 4794, 2007, pp. 1-18 6] J. Hong and E. Suh, S. Kim. Context-aware systems: A literature review and classification, Expert Systems with Applications. 2009 Elsevier 7] A.K. Dey and G.D. Abowd. Towards a better understanding of context and context-awareness, Proceedings of the Workshop on the What Who, Where, When and How of Context-Awareness, ACM Press, New York. 2000 8] H. Chen, T. Finin and A. Joshi.An Intelligent Broker for ContextAware Systems, Adjunct Proceedings of Ubicomp 2003 9] H. Chen, F. Perich, T. Finin, A. Joshi. SOUPA: standard ontology for ubiquitous and pervasive applications, The First Annual International Conference on Mobile and Ubiquitous Systems: Networking and Services, 2004, MOBIQUITOUS 2004 10] M. Roman, C. Hess, R. Cerqueira, A. Ranganathan, R.H. Campbell and K. Nahrstedt. GAIA: A middleware infrastructure for active spaces IEEE Pervasive Computing, vol. 1, no. 4, 2002, pp. 74-83 11] Service Platform for Innovative Communication Environment \(SPICE An Integrated Project in European Unions IST 6th Framework Programme, http://www.ist-spice.org 12] Open Platform for User-centric service Creation and Execution OPUCE Framework Programme, http://www.opuce.tid.es 13] MobiLife, an Integrated Project in European Unions IST 6th Framework Programme, http://www.ist-mobilife.org 14] L. Lamorte. A platform for enabling context aware telecommunication services, Third Workshop on Context Awareness for Proactive Systems. 2007 15] C. Venezia, C.A. Licciardi Improve ubiquitous Web applications with Context Awareness, 11th ICIN 2007 16] J.J. Chen, C. Adams. Short-range wireless technologies with mobile payments systems, Proc. ICEC 04, ACM Press, 2004, pp. 649 656 17] K. Cheverst. Experiences of developing and deploying a context-aware tourist guide: The GUIDE project, 6th International Conference on Mobile Computing and Networking, Boston, August 2000, pp. 2031 18] D.J. Cooka, J.C. Augusto, V.R. Jakkula, Ambient intelligence Technologies, applications, and opportunitie, Pervasive and Mobile Computing. 2009 


19] J. Hong, E. Suh, S. Kim. Context-aware systems: A literature review and classification, Expert Systems with Applications. Elsevier. 2008 20] G. Matas Miraz, I. Luque Ruiz, M.A. Gmez-Nieto. How NFC can be used for the Compliance of European Higher Education Area Guidelines in European Universities. Proceedings 1st International IEEE Workshop on Near Field Communication. 3-8. 2009   14 


dataset contains a stream of TCP connection records from two weeks of LAN traffic over MIT Lincoln Labs. It consists of 42 attributes that usually characterize network traffic behavior, both categorical attributes and quantitative attributes such as duration of the connections, protocol type etc. Attribute src_byte denoting the number of data bytes from source to destination and attribute dst_bytes inverse are selected in this experiment.  They are both quantitative attributes The user specified parameters are set as follows 5, 0.3, 0.03, 0.01,W min_sup preMinsup T 0.5, 0.4, 30.min_confidence max_MFB min_num_triples And the number of transactions in each time slot is 250 It is assumed that there are no more than three fuzzy sets or intervals in the datasets i.e. 3F Four different approaches to mine association rules are compared using the following notations: Fuzzy+MFB: the approach that use both fuzzy method and MFB_measure with 1 3.0? = and 2 0 .5 2 0.5? = , Fuzzy+P: the approach using fuzzy method with 1 3.0? = and 2 0.5? = also but repressing the first part of the MFB_measure that ignores the changed rate of the membership function, Discrete1: the approach using discrete method with 1 2 1.5? ?= =  and Discrete2 : the approach using discrete method with 1 2 2.5 TABLE I. RESULT OF EXPERIMENT ONE NUM_C NUM_FI NUM_RULE TIME\(s 1 \(50,50 2 \(60,60 3 \(70,70 4 \(80,80 5 \(90,90 6 \(100,100 7 \(110,110 8 \(120,120 9 \(130,130 10 \(140,140 Volume 4] 2010 2nd International Conference on Computer Engineering and Technology V4-157 0 0.5 1 1.5 2 x 104 


0 200 400 600 800 1000 1200 1400 Size of Databases\(250 Ex ec u tio n Ti m e\(s ec  Fuzzy+MFB Discrete1 Discrete2 Fuzzy+P Figure 3. Comparison of Execution Time Fig. 3 shows the execution time of the four approaches The runtimes of them grow linearly as the data stream grows which confirms that they are scalable with respect to the size of data stream, and it is mainly because of the usage of sliding window model. Fuzzy+P uses the least time Fuzzy+MFB has similar execution time to Discrete2, and Discrete1 has the most execution time. The difference of runtimes between them is mainly influenced by the clustering operations they use. The more clustering operations were executed, the more runtime it was Fig. 4 and Fig. 5 show the number of frequent itemsets and interesting rules found with the data stream increased Fuzzy+P and Fuzzy+MFB used less clustering operations than Discrete1 and Discrete2. Fuzzy+MFB nearly finds the most number of frequent fuzzy sets and interesting rules with the second least of clustering operations. Sometimes Discrete2 returns nearly the same number of fuzzy sets and interesting rules but with more clustering operations and the 


semantics of Discrete2 are meaningless as discussed in experiment one. Furthermore, the number of interesting rules found by Discrete1 is even less than Fuzzy+P that used the clustering operations least which illustrates the superiority of the method using fuzzy sets V. CONCLUSIONS In this paper, a novel fuzzy ARM algorithm called FFI_Stream is presented to tackle quantitative attributes in data streams and some techniques are proposed in the algorithm. Both synthetic and real datasets are used to evaluate the performance of the proposed algorithm. The experimental results show both the effectiveness and efficiency of the proposed algorithm.  In comparison with the discrete method, the proposed algorithm using fuzzy sets and MFB_measure gets a trade-off between the number of interesting rules and efficiency ACKNOWLEDGMENT This work is supported by The National High Technology Research and Development Program of China 863 Program 2008AA042902 Technology Research and Development Program of China 863 Program 2009AA04Z162 Project \(B07031 0 500 1000 1500 2000 2500 3000 3500 4000 0 2000 4000 6000 8000 10000 12000 Size of Databases\(250 N um be r o f F re qu e n t I 


te m se ts Fuzzy+MFB Discrete2 Discrete1 Fuzzy+P Clustering Operation Figure 4. Number of Frequent Itemsets 0 500 1000 1500 2000 2500 3000 3500 4000 0 1000 2000 3000 4000 5000 6000 7000 8000 Size of Databases\(250 Nu m be r o f I n te re st in g Ru le s Fuzzy+MFB Discrete2 Discrete1 Fuzzy+P Clustering Operation Figure 5. Number of Interesting Rules REFERENCE 1] R. Srikant and R. Agrawal, Mining Quantitative Association Rules 


in Large Relational Talbes, Proc. ACM SIGMOD, 1996, pp. 1-12 2] A.W. Fu et al. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes, In Proceedings of the First International Symposium on Intelligent Data Engineering and Learning \(IDEAL'98 3] C. M. Kuok, A. Fu and M. H . Wong, Fuzzy Association Rules in Large Databases with Quantitative Attributes, In ACM SIGMOD Records, vol. 27, 1998, pp. 41-46 4] X. Dang, V. Lee, W. K. Ng and K.L Ong, Incremental and Adaptive Clustering Stream Data over Sliding Window, Database and Expert Systems Applications, vol. 5690, 2009, pp. 660-674 5] M. Kaya,?R. Alhajj, F. Polat, and A. Arslan, Efficient Automated Mining of Fuzzy Association Rules, Database and Expert System Applicaton, vol. 2453, 2002, pp.133-142 6] http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html 7] S. Guha, A. N. Mishra, R. Motwani, L. OCallaghan, Clustering Data Streams: Theory and Practice,  Proc. IEEE Transactions on Knowledge and Data Engineering, vol. 15, May/Jun. 2003, pp. 515528 8] C. Aggarwal, J. Han, J. Wang, P. Yu, A Framework for Clustering Evolving Data Streams,  Proc. VLDB Conference, 2003,  pp. 81-92 9] C. K. S. Leung, B. Y. Hao, Mining of Frequent Itemsets from Streams of Uncertain Data,  Proc. IEEE International Conference on Data Engineering \(ICDE 09 10] C. C. Aggarwal, Y. Li, J. Y. Wang, and J. Wang, Frequent Pattern Mining with Uncertain Data, Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Jun. 2009, pp 29-37 11] P. M. Tsai, Mining Frequent Itemsets in Data Streams using the Weighted Sliding Window Model, Expert Systems and Applications vol. 36, Nov. 2009, pp.11617-11625 V4-158 2010 2nd International Conference on Computer Engineering and Technology [Volume 4 


In all charts reported in this section, the X-axis is k, which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k x, where x is a ?xed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain kx data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size Figure 1 shows the result from our strati?ed sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment the ?xed value x is set to be 300, which means the smallest sample size at k = 1 is 300, and the largest sample size at k 10 is 3000. Figure 1 a the ?ve sampling procedures. Figure 1 b cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of 330 6DPSOLQJ9DULDQFH            9D UL DQ FH R I V WL PD WL RQ  


9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW           6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF            5 


 9DU 9DU 9DU 5DQG c Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset estimation and sampling cost for the sampling procedures var7 var5, var3, and rand are normalized by the corresponding values of Full Var. Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c sampling procedures From Figure 1 a pared with sampling procedures Var7, Var5 and Var3, Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7, Var5, and Var3, we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k = 10, the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40 compared with sampling procedure Full Var. The experiment shows that our method decreases the sampling cost ef?ciently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k = 10, we can see that the AER of sampling procedure Var3 is increased by 20 compared with sampling procedure Full Var. However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy In addition, compared with sampling procedure of Full Var Var7, Var5, and Var3, sampling procedure Random, has higher estimation of variance, sampling cost and lower estimation accuracy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web Figure 2 shows the experiment result of our proposed strati?ed 


sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the ?xed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var, the sampling cost of Random is 2% smaller than Full Var. This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low ?, which denotes the probability of containing data records under the space of A = a, resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7, Var5, Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random. Furthermore, Random has smaller sampling accuracy than Full Var, Var7 and Var5, but has larger sampling accuracy than Var3. This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent To summarize, our results shows that our proposed strati?ed sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to tradeoff variance of estimation and sampling accuracy to some extent while achieving a large reduction in sampling costs B. Differential Rule Mining In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a ? D1\(t t categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and Random, have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are reprocessed 100 times using 100 different \(pilot sample, sample iterations 5000 runs First, we evaluated the performance of these procedures on 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


