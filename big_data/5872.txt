ACAC Associative Classiìcation Based on All-Conìdence Zaixiang Huang Zhongmei Zhou Tianzhong He and Xuejun Wang Lab of Granular Computing Zhangzhou Normal University Zhangzhou 363000 China Email huangzaixiang@126.com Abstract Classiìcation and association rule mining are important data mining tasks Associative classiìcation integrates association rule mining and classiìcation Previous studies show that associative classiìcation achieves high classiìcation accuracy and strong exibility However it often generates a huge set of rules when the minimum support is set too low Therefore it is time consuming to select high quality rules To deal with this problem we propose a new associative classiìcation approach called Associative Classiìcation based on All-Conìdence ACAC We use support and all-conìdence to mine not only frequent but also mutual associated itemsets for classiìcation Therefore ACAC generates a small set of high-quality rules Then we directly use these rules to classify new objects without pruning any rules ACAC uses average information entropy and the number of rules to measure the combined effect of group rules Experiment results on the Mushroom data set show that ACAC is not only efìcient but also high accurate Keywords data mining classiìcation associative classiìcation mutual association information entropy I I NTRODUCTION Building accurate and efìcient classiìers for large databases is one of the essential tasks of data mining Previous studies have developed many techniques for building classiìers such as decision trees KNN 2 3 4 and rough set approaches 6 7 8 9 In 1998 Liu et al proposed associati v e classiìcation which integrates association rule mining and classiìcation Associative classiìcation uses association rule mining algorithm such as Apriori or FPgro wth 12 to generate the complete set of association rules Then it selects a small set of high quality rules and uses this rule set for classiìcation Liu et al and Li et al 13 sho w that this approach achieves higher accuracy than traditional classiìcation approaches such as C4.5 Since then associati v e classiìcation became one of hot topics in data mining  16 17 Most existing associative classiìcation algorithms are based on the support-conìdence framework However these algorithms suffer from efìciency for two reasons 1 they often generate a large number of rules if the minimum support threshold is too low and 2 they also take efforts to select high quality rules from among them To deal with this problem we propose a new approach called Associative Classiìcation based on All-Conìdence ACAC All-conìdence is proposed by Omiecinski It is an appropriate measurement of the degree of mutual association in an itemset All-conìdence has be used to efìciently mine mutually associated patterns 20 21  W e use both all-conìdence and support measures to mine not only frequent but also mutual associated itemsets which contribute to classiìcation From our experiments we can see that ACAC generates a small set of high-quality rules Furthermore ACAC do not need prune any rules and directly uses these rules to classiìcation As a result ACAC is much more efìcient Moreover we use the average information entropy and the number of rules to measure the combined effect of group rules From our experiments we also can see that we achieved high classiìcation accuracy The remaining of the paper is arranged as follows Section II reviews the general ideas of associative classiìcation Section III describes the rule generation process and discusses how to classify a new data object The experimental results on classiìcation accuracy and the performance study on efìciency are presented in section IV We conclude the study in section V II A SSOCIATIVE C LASSIFICATION In associative classiìcation the training data set T has m distinct attributes A 1 A 2 A m and a list of classes Attribute can be categorical or continuous For a categorical attribute all the possible values are mapped to a set of consecutive positive integers For a continuous attribute its value range is discretized into intervals and the intervals are also mapped to consecutive positive integers In general we call each attribute-value pair an item An itemset X  a i 1 a i k is a set of values of different attributes A data object is said to match itemset X  a i 1 a i k  if and only if for 1  j  k the object has value a i j in attribute A i j  The number of data objects in T matching itemset X is called the support of X  denoted as sup  X   All-conìdence of itemset X  a i 1 a i k is deìned as follows allconf  X  sup  X   max sup  a i 1  sup  a i k  1 It represents the minimum conìdence of all association rules extracted from an itemset We use both support and allconìdence to mine frequent and mutual associated itemsets Given a training data set T  let c is a class label A ruleitem is of the form 012 condset c   where condset is an itemset Each ruleitem basically represents a rule 289 2011 IEEE International Conference on Granular Computin g 978-1-4577-0371-3/11/$26.00 ©2011 IEEE 


condset  c  Ruleitem whose condset has k items is called k ruleitem The support count of the condset called condsup is the number of objects in T that match the condset The support count of the rule called rulesup is the number of objects in T that match the condset and belong to c  The ruleês support is  rulesup  T    100  where  T  is the size of the dataset and the ruleês conìdence is  rulesup/condsup   100  Most associative classiìcation algorithms work in three phases 1  Generating the set of rules from the training data set In this phase association rules of the form condset  c are discovered by using association rule mining algorithm such as Apriori or FPgrowth These algorithms often generate large number of rules especially when support threshold is too low We use both support and all-conìdence measures to select not only frequent but also mutual associated itemsets for classiìcation Thus our approach generates a small set of rules 2  Pruning the set of discovered rules In this phase those rules that may be overìtting or redundant are pruned However pruning techniques is a challenging task because of the huge number of rules generated by association rule mining We do not need prune any rules and directly uses these rules to classiìcation because we only select these mutual associated itemsets that contribute to classiìcation 3  Classifying new objects Some algorithms use the rst matching rule and others use multiple rules 13 to predict a new objects class label There are different ways to combine the rules that could classify a new object Some algorithms average the conìdences for each category while others compute a weighted chi-square for each cate gory  We use the average information entropy and the number of rules to measure the combined effect of group rules From our experiments we can see that we achieved high classiìcation accuracy III ACAC A SSOCIATIVE C LASSIFICATION BASED ON A LL C ONFIDENCE In this section we develop a new associative classiìcation method called Associative Classiìcation based on AllConìdence ACAC It consists of two phases 1 rule generation Called ACAC-RG which is based on algorithm Apriori and 2 classiìcation based on multiple rules Subsection III-A discusses rule generation and subsection III-B describes classiìcation A Generating Rules for Classiìcation The ACAC-RG algorithm is based on Apriori To make mining efìcient ACAC adds all-conìdence measure to support-conìdence framework Both support and allconìdence measures have downward closure property If a ruleitem passes given support and all-conìdence threshold it is selected as candidate rule If a candidate rule passes given conìdence threshold it is select as a classiìcation rule The general idea of generating rules in ACAC is shown in the following example Example 1  Given a training data set T as shown in Table I Let the support threshold be 2 all-conìdence threshold 50 and conìdence threshold 100 According to Equation 1 all-conìdence values of 1 itemsets are 100 Table I A TRAINING DATA SET  A B C D class x 1 32 55 80 83 90 x 2 33 52 80 85 89 x 3 33 52 80 85 89 x 4 33 55 79 82 90 x 5 34 55 79 82 89 x 6 34 55 77 82 89 x 7 32 55 80 88 90 x 8 33 55 79 82 90 ACAC selects those ruleitems whose condsup and allconìdence pass corresponding thresholds to push into the set of candidate 1-ruleitems F 1  Then from F 1  it selects those ruleitems whose conìdence pass the given conìdence threshold to push into the set of rules R 1  Once a rule is pushed into R 1  it will be deleted from F 1  The results are shown in table II and table III Table II T HE SET OF CANDIDATE 1RULEITEMS F 1 itemset class condsup rulesup allconf conf 33 89 4 2 100 50 33 90 4 2 100 50 55 89 6 2 100 67 55 90 6 4 100 33 79 89 3 1 100 33 79 90 3 2 100 67 80 89 4 2 100 50 80 90 4 2 100 50 82 89 4 2 100 50 82 90 4 2 100 50 Table III T HESETOFRULE R 1 itemset class condsup rulesup allconf conf 32 90 2 2 100 100 34 89 2 2 100 100 52 89 2 2 100 100 85 89 2 2 100 100 ACAC generates candidate 2-ruleitems by self-jointing F 1 just as Apriori.The difference is that ACAC is required to compute the all-conìdence of ruleitems just as follows For example there are two ruleitems in table II 290 


 Algorithm 1 ACAC-RG T Input  Training data set Minimum Support minsup Minimum conìdence mincof Minimum all-conìdence minallconf Output  A set of rules for predicting class labels for objects 1 C 1  init  pass  T   2 ruleSelection  C 1 F 1 R   3 for  k 2 F k  1     k  do 4 C k  candidateGen  F k  1   5 supportCount  C k   6 ruleSelection  C k F k R   7 end for 8 return R Algorithm 2 ruleSelection  C k F k R  1 for all candidate c  C k do 2 for all class[i do 3 compute c.allconf c.conf i 4 if c.condsup  T  minsup and c.allconf  i   minallconf then 5 if c.conf  i   minconf then 6 push c   into R 7 else 8 push c into F k  9 end if 10 end if 11 end for 12 end for Ruleitem 012 55  90   rulesup is 4 Ruleitem 012 79  90   rulesup is 2 By jointing these two ruleitems it will generate ruleitem 012 55  79  90   rulesup is 2 All-conìdence of ruleitem  012 55  79  90  s2 max 4 2 50 The results are shown in table IV and table V Table IV T HE SET OF CANDIDATE 2RULEITEMS F 2 itemset class condsup rulesup allconf conf 55 79 90 3 2 50 67 55 82 89 4 2 50 50 55 82 90 4 2 100 50 79 82 90 3 2 100 67 Table V T HESETOFRULE R 2 itemset class condsup rulesup allconf conf 33 55 90 2 2 100 100 33 79 90 2 2 100 100 33 80 89 2 2 100 100 33 82 90 2 2 100 100 55 80 90 2 2 100 100 This process is iterated until the set of candidate kruleitems is empty The algorithm ACAC-RG is presented in Algorithm 1 In this algorithm C k is the set of candidate k-itemsets F k is the set of frequent k-itemsets R is the set of rules Line 1-2 represents the rst pass of the algorithm It counts the condsup and rulesup of individual item at the same time Then the function ruleSelection is executed also done in each subsequent pass line 6 For each subsequent pass the algorithm performs 3 major operations First the frequent itemsets F k  1 found in the k-1 pass are used to generate the candidate itemsets C k using the condidateGen function line 4 which is similar to the function apriori-gen in algorithm Apriori Second supportCount function scans the data set and counts the condsup and rulesup of the candidates in C k line 5 Finally Function ruleSelection is executed The Function ruleSelection is shown in Algorithm 2 The function ruleSelection computes all-conìdence of each candidate itemset in each class and conìdence of each candidate rule line 1 to 3 If candidate rules pass the given support all-conìdence and conìdence thresholds it is pushed into R If candidate rules only pass the given support and all-conìdence thresholds it is pushed into F k line 410 When a candidate rule is selected its condset will not be extended in next subsequent pass any more B Classiìcation based on Multiple Rules In this section we discuss how to determine the class label of new objects ACAC collects the subset of rules matching the new object from the set of rules If all the rules matching the new object have the same class label ACAC just simply assigns that label to the new object If the rules are not consistent in class labels ACAC divides the rules into groups according to class labels All rules in a group share the same class label and each group has a distinct label Firstly we use information entropy of a ruleês condset X to evaluate its classiìcation power which is deìned as follows Info  X   1 log 2 k k  i 1 p  C i  X og 2 p  C i  X  2 where k is the number of classes and p  C i  X  is the probability that an object matching X belongs to C i  We use the Laplace expected error estimate to estimate this probability which is deìne as follows p  C i  X  rulesup 1 condsup  k 3 291 


where condsup is the total number of objects matching X among which rulesup objects belong to C i  the predicted class of the rule Secondly we measure the combined effect of a group rule R  r 1 r n  by compute its strength\(called s  R  s follows s  R   9  1   sup  X i   Info  X i    sup  X i    1  n n tot 4 where X i is the condset of r i  n tot is the total number of rules whose condset matches the new object and n is the number of rules in the group R  The strength of a group combines the average information entropy with the number of rules in the group We consider the information entropy contribution is larger so we give a high weight to it Finally ACAC assigns the class label of the group with maximum strength to the new object IV E XPERIMENTS We tested our algorithm on the Mushroom data set All the experiments are performed on a 3.2GHz Core i3 PC with 2G main memory running Microsoft Windows XP To evaluate the efìciency and accuracy of ACAC we designed two groups of experiments One is performed with all-conìdence measure allconf and the other without all-conìdence w/o allconf A 10-fold cross validation is performed The results are given as average of the accuracy number of rules and run time obtained for each fold All reports of the runtime only include the runtime in rule generation In table VI we set the minimum support threshold minsup to 0.01 conìdence threshold minconf to 100 the all-conìdence threshold minallconf to 10 From table VI the runtime of ACAC with all-conìdence is nearly 16 percent of that without all-conìdence The number of rules generated by ACAC with all-conìdence are almost 36 percent of that without all-conìdence ACAC with and without all-conìdence both achieve high accuracy Table VI P ERFORMANCES OF ACAC runtime\(sec  rules accuracy allconf 278 292 100 w/o allconf 1750 813 99.98 In table VII the minimum support threshold is xed at 0.01 the all-conìdence threshold minallconf xed at 10 From table VII we can see that ACAC with all-conìdence is consistently superior to that without allconìdence on performances when the conìdence threshold decreases from 100 to 97 In table VIII the minimum conìdence threshold is xed at 100 the all-conìdence threshold minallconf xed at 10 From table VIII we can see that ACAC with all-conìdence consistently outperforms that without allconìdence when the minimum support threshold increases from 0.01 to 0.05 Table VII P ERFORMANCES OF ACAC IN DIFFERENT CONFIDENCE THRESHOLD minconf runtime\(sec  rules accuracy w/o allconf allconf w/o allconf allconf w/o allconf allconf 100 1750 278 813 292 99.98 100 99 1708 273 803 290 99.93 100 98 1474 257 751 280 99.85 99.90 97 1180 219 677 260 99.93 99.87 Table VIII P ERFORMANCES OF ACAC IN DIFFERENT SUPPORT THRESHOLD minsup runtime\(sec  rules accuracy w/o allconf allconf w/o allconf allconf w/o allconf allconf 0.01 1750 278 813 292 99.98 100 0.02 1753 276 810 292 99.98 100 0.03 1750 276 797 292 99.98 100 0.04 1750 276 797 292 99.98 100 0.05 1742 276 788 291 99.98 100 To sum up above arguments ACAC with all-conìdence is much more efìcient than that without all-conìdence when support threshold is set very low and conìdence threshold is high ACAC also achieves high accuracy The all-conìdence threshold is an important parameter in ACAC to control the number of rules selected for classiìcation Thus we need to test the sensitivities of the allconìdence threshold w.r.t classiìcation accuracy In table IX the support threshold is set to 0 and the conìdence threshold to 100 From table IX we can see that the accuracy is not very sensitive to the all-conìdence threshold when it increases from 5 to 30 Table IX T HE EFFECT OF ALL CONFIDENCE THRESHOLD minallconf runtime\(sec  rules accuracy 5 448 415 99.98 10 279 292 100 15 218 222 99.98 20 172 169 99.96 30 136 98 99.92 40 120 54 99.76 50 116 33 84.42 V C ONCLUSIONS Associative classiìcation has high classiìcation accuracy and strong exibility However this approach also suffers from efìciency at handling huge number of association rules In this paper we proposed a new associative classiìcation approach called Associative Classiìcation based on All-Conìdence ACAC ACAC employs support and 292 


all-conìdence measures to select both frequent and mutual associated itemsets which contribute to classiìcation From our experiments we can see that ACAC generates a small set of rules and is much more efìcient ACAC also achieves high accuracy In our future work we will focus on enhancing the accuracy and scalability of this approach and compare it with other well-established classiìcation schemes A CKNOWLEDGEMENTS This work is in part supported by National Science Foundation of China under Grant Nos 10971186 61170129 60873077 a grant from education ministry of Fujian of China JA10202 and the Natural Science Foundation of Fujian Province China under Grant No 2011J01374 R EFERENCES  J R Quinlan C4.5 programs for machine learning  Morgan Kaufmann 1993  T  Co v e r and P  Hart Nearest neighbor pattern classiìcation IEEE Transactions on Information Theory  vol 13 pp 21Ö27 1967  B Dasarathy  Nearest Neighbor Pattern Classiìcation Techniques IEEE Computer Society Press 1990  P  Nesk o vic and L N Cooper  Impro ving nearest neighbor rule with a simple adaptive distance measure Pattern Recognition  vol 28 no 2 pp 21Ö27 2007  W  Zhu and F  W ang Reduction and axiomization of co v e r ing generalized rough sets Information Sciences  vol 152 no 1 pp 217Ö230 2003  W  Zhu Generalized rough sets based on relations  Information Sciences  vol 177 no 22 pp 4997Ö5011 2007  F  Min Q Liu and C F ang Rough sets approach to symbolic value partition International Journal of Approximate Reasoning  vol 49 pp 689Ö700 2008  W  Zhu Relationship among basic concepts in co v eringbased rough sets Information Sciences  vol 179 no 14 pp 2478Ö2486 2009  F  Min and Q Liu  A hierarchical model for test-costsensitive decision systems Information Sciences  vol 179 no 14 pp 2442Ö2452 2009  B Liu W  Hsu and Y  Ma Inte grating classiìcation and association rule mining in KDD  1998 pp 80Ö86  R Agra w a l and R Srikant F ast algorithms for mining association rules in VLDB  1994 pp 487Ö499  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in SIGMOD  2000 pp 1Ö12  W  Li J Han and J Pei CMAR accurate and ef cient classiìcation based on multiple class-association rules in ICDM  2001 pp 369Ö376  X Y i n and J Han CP AR classiìcation based on predicti v e association rules in SDM  2003 pp 331Ö335  G Cong K T an A T ung and X Xu Mining top-k co v ering rule groups for gene expression data in SIGMOD  2005 pp 670Ö681  J W ang and G Karypis HARMONY ef ciently mining the best rules for classiìcation in SDM  2005 pp 205Ö216  A V eloso W  M Jr  and M Zaki Lazy associati v e classiìcation in ICDM  2006 pp 645Ö654  E Omiecinski  Alternati v e interest measures for mining associations in IEEE TKDE  2003  Y  K Lee W  Y  Kim Y  D Cai and J Han CoMine efìcient mining of correlated patterns in ICDM  2003 pp 581Ö584  H Xiong P  T an and V  K umar  Ming strong af nity association patterns in data sets with skewed support distribution in ICDM  2003 pp 387Ö394  Z Zhou Z W u  C  W ang and Y  Feng Ef ciently mining maximal frequent mutually associated patterns in ADMA  2006 pp 110Ö117  L Duan and W  N  Street Finding maximal fully-correlated itemsets in large databases in ICDM  2009 pp 770Ö775  P  Clark and R Boswell Rule induction with CN2 some recent improvements in EWSL  1991 pp 151Ö163 293 


21 21 22  2121 ,cos WbWa WbWa ba ab WWWWsim 2 Cosine law in essence is using the geometry angles cosine between vector  1W  and  2W . Larger the distance of two vectors is, smaller the cosine value is; on the contrary the similarity is lower 3 Initial Optimization Center based on "Min-Max" Principle Here K-Means clustering algorithm based on the min-max" principle was adopted to choose initial centers The "min-max" principle can ensure the distance between the initial centers far enough. Firstly choose two domain concepts with the minimum similarity from the similarity matrix as the initial centers. Secondly for each non-center domain concept, obtain the ones which have the maximum similarity with the existed centers. Finally choose the domain concepts with the minimum similarity from those maximum similarities as new centers. Suppose the number of chosen centers is m, then recurrence formula of the m+1 2,1],,,2,1 max{min[ mjmixxd ij LL ?=      \(3 According to the rule that the distance between adjacent centers should grow stable, then determine mutation point k. The number of centers should be less than the square of the number of clustering data, that means nk < . The concept center depth is calculated as the following  min min min min idistidistidistidistiDepth ?++??=   \(4 The center domain concepts prior to the max Depth\(i chosen as center, and the following ones become non- center 


K-Means clustering algorithm is as the follows Step1: Divide the non- center domain concepts into the clusters of nearest center Step2: Choose new centers from the clusters where similarity of domain concepts must exceed the appointed value Repeat step1, 2 until the obtained centers or cluster set become stable C. Extraction of Non-taxonomy Relations Compared to the extraction of taxonomy relations, the extraction of non-taxonomy is more difficult. Extracting the related concept pairs is the key. The method based on association rules mining is adopted to extract related concept pairs. The algorithm of extracting related concept pairs is as follows Output: the set of related concept pairs 117  Step1: Construct a concept pair set by gathering every two different concepts Step2: For each concept pair in the set, calculate the support and the confidence based on association rules mining algorithm If both support and confidence satisfy MinSup and MinConf, the concept pair is related; else the pair must be deleted V. IMPLEMETATION OF A SEMI-AUTOMATIC DOMAIN ONTOLOGY CONSTRUCTION SYSTEM This section mainly shows the operation of each module in this system A. Implemetation of Domain Concepts Extraction Module Users can check the selected concept words and then delete the illegal ones in this interface. The output is compound and common words. The interface of concept extraction module is shown in Figure.3  Figure 3.  The interface of concept extraction module B. Implemetation of Taxonomy Relations Extraction Module An example is shown in Figure. 4. The left edit box is the set of edited domain concepts, which are the data source of the taxonomy and non-taxonomy relations extraction module 


The three boxes in the middle, respectively implement extraction of taxonomy relation based on clustering and generalized suffix tree and extraction of non-taxonomy relations based on association rules mining. The right edit box shows the extraction results  Figure 4.  Example of extraction of relations among domain concepts VI. CONCLUSIONS AND FUTURE WORK Nowadays it is also quite difficult to construct a common and mature tool used for constructing ontology semi-automatically. The methods in this paper adopted accelerate the construction of ontology to some extent, but there are still many defects. \(1 manual .This reduces the automatic degree and efficiency 2 characteristics of language. \(3 different grammatical components of sentence to conclude the relations among concepts can be tried REFERENCES 1] Gruber T R. A Translation Approach to Portable Ontology Specifications Knowledge Acquisition. 1993,5:199-200 2] Studer, Benjamins V R. FenselD Knowledge Engineering. Principles and Methods. Data and Knowledge Engineering. 1998,25\(2 3] Zhang Li, Jiang Hao.Research and realization of semi-automatic domain concept construction. WISA2009 academic conference essays 4] Jian Zhang, Jianfen Gao, Ming Zhou. Extraction of Chinese Compound Words-An Experimental Study on a Very Large Corpus The second Chinese Language Processing Workshop attached to ACL2000. Hong Kong, October 8,2000 5] Lee-Feng Chien. PAT-tree-based adaptive keyphrase extraction for intelligent Chinese information retrieval. Information Processing and Management. Elsevier Press, 1998 6] A. Maedche, V.Pekar, S.Staab. Ontology Learning Part One-On Discoverying Taxonomic Relations from the Web. In Web Intelligence, Springer,2002 7] A. Maedche, S. Staab. Discovering conceptual relations from text.. In ECAI-2000-European Conference on Artificial Intelligence Proceedings of the 13th European Conference on Artificial Intelligence. IOS Press, Amsterdam, 2000:321-324 8] Generalized Suffix Trees http://www.msci.memphis.edu/~giri/compbio/f00/Wally/Wally.html 


9] J.Han. M.Kamber. Data Mining: Concepts and Techniques, 2nd edition, Morgan Kaufmann, 2006 10] Blaschke, C. and A.Valencia\(2002 from the literature, Genome Informatics. 13,201-213  118 


And put forward that we could use confidence, category homoplasy and relevancy strength to improve the quality of feature extension modes. We also verified that confidence category homoplasy and relevancy strength are effective through our experiments. In the same time we have drawn the following conclusions: \(1 relationships for short-text can improve their classification performance; \(2 effectiveness of information in the feature extension mode library we should choose the suitable thresholds; \(3 information is too small to meet the demand of short-text feature extension. So we should find out a perfect method which can increase information coverage in the feature extension mode library for short-text classification; \(4 extension library for short-text extension effectively, i.e., choosing a perfect feature extension strategy is also our further work ACKNOWLEDGMENT The research is supported in part by the National Natural Science Foundation of China under grant number 60703010 the Nature Science Foundation of Chongqing province in China under grant number CSTC, 2009BB2079, and the Scientific Research Foundation for the Returned Overseas Chinese Scholars of Ministry of Education of China under grant number [2007] 1109 REFERENCES 1] Fabrizio Sebastiani.Machine Learning in Automated Text Categorization, A.ACM Computing Surveys, C.2002.34\(1 2] Fan Xing-hua,Wang peng. Chinese Short-Text Classification in TwoStep, J.Journal of DaLian Maritime Universtiy, 2008,11\(2 3] Zelikovitz S. and Hirsh H. Improving Short Text Classification Using Unlabeled Background Knowledge to Assess Document Similarity C. In: Proceedings of ICML-2002, 2002, 1183-1190 4] Wang Xi-wei,Fan Xing-hua and Zhao Jun. A Method for Chinese Short Text Classification Based on Feature Extension, J.Journal of Computer Applications,2009,29\(3 5] JIAWEI HAN,JIAN PEI ,YIWEN YIN, BUNYING MAO.Ming Frequent Patterns without Candidate Generation:A Frequent-Pattern Tree.Data Mining and Knowledge Discovery,2004,8:53-87 6] Liu Fei. Huang Xuan-qing and Wu Li-de.Approach for Extracting Thematic Terms Based on Association Rule, J.Computer Engineering,2008\(4 7] Xinhua Fan, Jianyun Nie. Link Distribution Dependency Model for 


Document Retrieval, C.Journal of Information and Computational Science6:3\(2009  90 


shows that proposed post mining of association rule mining technique for missing sensor data estimation is an area worth to explore REFERENCES 1] Agrawal, R., & Imielinski, T., & Swami, A., "Mining association rules between sets of items in massive databases", International Conference on Management of Data, 1993 2] Austin, F. I., "Austin Freeway ITS Data Archive", Retrieved January 2003 from http://austindata.tamu.eduidefauIt.asp 3] Bastide, Y., & Pasquier, N., & Taouil, R, & Stumme, G., & Lakhal L., "Mining minimal non-redundant association rules using frequent closed itemsets", First International Conference on Computational Logic, 2000 4] Cool, A. L., "A review of methods for dealing with missing data The Annual Meeting of the Southwest Educational Research Association, 2000 5] Deshpande, A., & Guestrin C., & Madden, S., "Using probabilistic models for data management in acquisitional environments", The Conference on Innovative Data Systems Research, 2005 6] Halatchev, M., & Gruenwald, L., "Estimating missing values in related sensor data streams", International Conference on Management of Data, 2005 7] Iannacchione, V. G., "Weighted sequential hot deck imputation macros", Proceedings of the SAS Users Group International Conference, 1982 8] Nan Jiang, "Discovering Association Rules in Data Streams Based On Closed Pattern Mining", SIGMOD Ph.D. Workshop on Innovative Database Research, 2007 9] Li, Y., & Liu, Z. T., & Chen, L., & Cheng, W., & Xie, C.H Extracting minimal non-redundant association rules from QCIL The 4th International Conference on Computer and Information Technology, 2004 10] Little, R 1. A., & Rubin, D. B., "Statistical analysis with missing data", New York: John Wiley and Sons, 1987 II] McLachlan, G., & Thriyambakam, K., "The EM algorithm and extensions", New York: John Wiley & Sons, 1997 12] Mitchell, T., "Machine Learning", McGraw Hill, 1997 13] Papadimitriou, S., & Sun, 1., & Faloutsos, C., "Streaming pattern discovery in multiple time-series", The International Conference on Very Large Databases, 2005 14] Rubin, D., "Multiple imputations for nonresponce in surveys", New York: John Wiley & Sons, 1987 


15] Shafer, 1., "Model-Based Imputations of Census Short-Form Items In Proceedings of the Annual Research Conference, 1995 16] Taouil, R., & Pasquier, N., & Bastide, Y., & Lakhal, L., "Mining bases for association rules using closed sets", International Conference on Data Engineering, 2000 17] Wilkinson & The AP A Task Force on Statistical Inference, 1999 18] Zaki, M. 1., Hsiao, C. 1., "Efficient algorithms for mining closed itemsets and their lattice structure", IEEE Transactions on Knowledge and Data Engineering, 2005 V5-106 


General Chair f!!\f  Organizing Chairs  f!!\f  f$% \f!!\f  Organizing Co-chairs f    f  f\f   f\f\f   f*!\f!\f.\f  f f  Program Committee Chairs  f\f\f   f!!\f  Publication Chair 0   


200 250 300  The size of dataset/10,000 R es po ns e tim e S    a 0 50 100 150 200  The size of dataset/10,000 R es po ns e tim e S    b 0 10 20 30 40 50 


60  The size of dataset/30,000 R es po ns e tim e S    c Fig. 9 The scalability of our algorithm compared with FP-growth  Paper [12] proposed a way to reduce times of scanning transaction database to reduce the cost of I/O IV. CONCLUSIONS AND FUTURE WORK This paper first discusses the theory of foundations and association rules and presents an association rules mining algorithm, namely, FP-growth algorithm. And then we propose an improved algorithm IFP-growth based on many association rules mining algorithms. At last we implement the algorithm we propose and compare it with algorithm FPgrowth algorithm. The experimental evaluation demonstrates its scalability is much better than algorithm FP-growth 177 Now, lets forecast something we want to do someday Firstly, we would parallelize our algorithm, because data mining needs massive computation, and a parallelable environment could high improve the performance of the algorithm; Secondly, we would apply our algorithm on much more datasets and study the run performance; At last, we would study the performance when the algorithm deal with other kinds of association rules  REFERENCES 1] S. Sumathi and S. N. Sivanandam. Introduction to Data Mining and its Applications, Springer, 2006 2] V. J. Hodge, J. Austin, A survey of outlier detection 


methodologies, Artificial Intelligence Review, 2004, 22 85-126 3] Han, J. and M. Kamber. Data Mining: Concepts and Techniques. Morgan Kaufmann, San. Francisco, 2000 4] Jianchao Han, Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases, Journal of Advanced Computational Intelligence and Intelligent Informatics 2006, 10\(3 5] Jiuyong Li, Hong Shen, Rodney Topor. Mining Informative Rule Set for Prediction. Journal of Intelligent Information Systems, 2004, 22\(2 6] Jianchao Han, and Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases. Journal of Advanced Computational Intelligence, 2006, 10\(3 7] Doug Burdick, Manuel Calimlim, Jason Flannick Johannes Gehrke, Tomi Yiu. MAFIA: A Maximal Frequent Itemset Algorithm. IEEE Transactions on Knowledge and Data Engineering, 2005, 17\(11 1504 8] Assaf Schuster, Ran Wolff, Dan Trock. A highperformance distributed algorithm for mining association rules. Knowledge and Information Systems, 2005, 7\(4 458-475 9] Mohammed J. Zaki. Mining Non-Redundant Association Rules. 2004, 9\(3 10] J.Han, J.Pei, Y.Yin, Mining frequent patterns without candidate generation, Proceedings ACM SIGMOD 2000 Dallas, TX, May 2000: 1-12 11] P.Viola, M.Jones. Rapid Object Detection Using A Boosted Cascade of Simple Features. Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2001 12] Anthony K. H. Tung, Hongjun Lu, Jiawei Han, Ling FengJan. Efficient Mining of Intertransaction Association Rules. 2003, 154\(1 178 


For each vertex b in g form j forests body\(a, g, i s.t. bodyAnt\(a, g, i a, g, i with itemsets Ant\(b b and each subset of itemsets Ant\(b b in P\(a, g, j Assign to each leaf l of trees bodyAnt\(a, g, i bodyCons\(a, g, i a fresh variable Vm,M, m, M = size\(itemset\(l Assign to each leaf l of tree headAnt\(a, g, j the variable assigned to itemset l in some leaf of some tree bodyCons\(a, g, i TABLE II.  EXPERIMENTAL DATA Conf. #rules #pruned #dftrs PtC 0.5 6604 2985 1114 0.6 2697 2081 25 0.75 1867 1606 10 0.8 1266 1176 0 0.95 892 866 1 0.98 705 699 1 DSP 0.5 2473 1168 268 0.6 1696 869 64 0.75 1509 844 89 0.8 1290 1030 29 0.95 1032 889 15 0.98 759 723 1 Arry 0.5 770 492 82 0.6 520 353 60 0.75 472 327 39 0.8 408 287 22 0.95 361 255 25 0.98 314 243 30  Our induction algorithm has been launched for each combination of thresholds. Our scheme eliminates all redundant rules in the sense of [25, 31], i.e. those association rules that are not in the covers. All the meta-rule deductive schemes implicitly included in [25] and [31] are induced by our method. The percentage of pruning, thus, outperforms [25 


The results produced for k=3, support 0.25 and confidences between 0.7 and 0.99 are shown in Fig. 3, in terms of pruning percentage \(vertical axis when applied to low confidences \(from 0.7 to 0.9 The percentage of pruning achieved diminishes as the confidence is superior to 0.9. Nevertheless, the pruning is effective with confidence of 0.99 in the majority of cases Pruning at Support = 0.25 0,00 5,00 10,00 15,00 20,00 25,00 30,00 35,00 40,00 45,00 50,00 0,7 0,8 0,9 0,95 0,99 Confidence P ru n in g L e v e l Case 1 Case 2 Case 3  Figure 3.  Pruning experiences at support 0.25  V. DISCUSSION AND CHALLENGES It is important to discuss the technique presented here with focus on the purpose the technique pursues:  to produce semantic recommendation The reader should have noticed that the algorithm presented 


relies strongly on "choice". For instance, the algorithm chooses ears in the graph to form an order for elimination, and the choice is arbitrary. This strategy is essential to maintain low complexity \(polynomial practical. Nevertheless, a warned reader may conclude that this arbitrary choice implies that there are many compactions to produce and therefore the approach as a whole does not show to produce an optimal solution. And the reader is right in this conclusion. Since the goal is compaction, the search for an optimal solution can be bypassed provided a substantial level of pruning is achieved To complete the whole view, we describe how web service descriptions are complemented with the association rules as recommendations. In effect, under our scheme, the document describing the web service is augmented with a set of OWL/RDF/S triples that only incorporate the non-pruned rules with the format of Example 1, that is, the set ARmin of the compaction program obtained by our algorithm, together with the thresholds applied to the mining process and a registered URI of a registered description service. The assumptions and defeaters are not added to the web service description. If the associations encoded in the triples are not sufficient for the client \(a search engine, for instance widening of the response to the description service identified by the given URI, and then the assumptions and defeaters are produced. The reasoning task required for deriving all the implicitly published rules is client responsibility Notice that, under this scheme, the actual rules that appear as members of the set initial ARmin set are irrelevant; the only important issue is the size of the set The developed scheme also supports an extension of the algorithm that admits the assignment of priorities to rules and to itemsets, in order to allow the user to produce a more controlled program as output. Nonetheless, the importance of the extension has not been already tested, and therefore it is beyond the subject of the present paper It would be also interesting to design a scheme that supports queries where the client provides an itemset class and values for support and confidence and the engine produces a maximal class of inferred associated itemsets as a response. This scheme is also under development, so we have not discussed this aspect here 


VI. CONCLUSION In this paper, we have presented a defeasible logic framework for managing associations that helps in reducing the number of rules found in a set of discovered associations. We have presented an induction algorithm for inducing programs in our logic, made of assumption schemas, a reduced set of association rules and a set of counter-arguments to conclusions called defeaters, guaranteeing that every pruned rule can be effectively inferred from the output. Our approach outperform those of [17], because all reduction compactions presented there can be expressed and induced in our framework, and several other patterns, particular to the given datasets, can also be found. In addition, since a set of definite clauses can be obtained from the induced programs, the knowledge obtained can be modularly inserted in a richer inference engine Abduction can be also attempted, asking for justifications that explain the presence of certain association in the dataset The framework presented can be extended in several ways Admitting defeaters to appear in the head of assumption, to define user interest Admitting arithmetic expressions within assumptions for adjustment in pruning Admitting set formation patterns as itemset constants Extending the scope, to cover temporal association rules REFERENCES 1]  R. Agrawal, and R. Srikant: Fast algorithms for mining association rules In Proc. Intl Conf. Very Large Databases. \(1994 2]  A. V. Aho, J. E. Hopcroft, J. Ullman. The design and analysis of computer algorithms, Addison-Wesley, 1974 3]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher, A. Rock: A Family of Defeasible Reasoning Logics and its Implementation. ECAI 2000: 459-463 4]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher: Representation results for defeasible logic. ACM Trans. Comput. Log. 2\(2 2001 5]  A. Basel, A. Mahafzah, M. Al-Badarneh: A new sampling technique for association rule mining, Journal of Information Science, Vol. 35, No. 3 358-376 \(2009 6]  R. Bayardo and R. Agrawal: Mining the Most Interesting Rules. In Proc of the Fifth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 145-154, \(1999 


7]  R. Bayardo, R. Agrawal, and D. Gunopulos: Constraint-based Rule Mining in Large, Dense Databases. Data Mining and Knowledge Discovery Journal, Vol. 4, Num-bers 2/3, 217-240. \(2000 8]  A. Berrado, G. Runger: Using metarules to organize and group discovered association rules. Data Mining and Knowledge Discovery Vol 14, Issue 3. \(2007 9]  S. Brin, R. Motwani, J. Ullman, and S. Tsur: Dynamic itemset counting and implication rules for market basket analysis. In Proc. ACMSIGMOD Intl Conf. Management of Data. \(1997 10] L. Cristofor and D.Simovici: Generating an nformative Cover for Association Rules. In ICDM 2002, Maebashi City, Japan. \(2002 11] Y. Fu and J. Han: Meta-rule Guided Mining of association rules in relational databases. In Proc. Intl Workshop on Knowledge Discovery and Deductive and Object-Oriented Databases. \(1995 12] B. Goethals, E. Hoekx, J. Van den Bussche: Mining tree queries in a graph. KDD: 61-69. \(2005 13] G. Governatori, D. H. Pham, S. Raboczi, A. Newman and S. Takur: On Extending RuleML for Modal Defeasible Logic. RuleML, LNCS 5321 89-103. \(2008  14] G. Governatori and A. Stranieri. Towards the application of association rules for defeasible rules discovery In Legal Knowledge and Information Systems, JURIX, IOS Press, 63-75. \(2001 15] J. Han, J. Pei and Y. Yin: Mining frequent patterns without candidate generation. In Proc. ACM-SIGMOD Intl Conf. Management of Data 2000 16] C. Hbert, B. Crmilleux: Optimized Rule Mining Through a Unified Framework for Interestingness Measures. DaWaK: LNCS 4081, 238247. \(2006 17] E. Hoekx, J. Van den Bussche: Mining for Tree-Query Associations in a Graph. ICDM 2006: 254-264 18] R. Huebner: Diversity-Based Interestingness Measures For Association Rule Mining. Proceedings of ASBBS Volume 16 Number 1, \(2009 19] B. Johnston, Guido Governatori: An algorithm for the induction of defeasible logic theories from databases. Proceedings of the 14th Australasian Database Conference, 75-83. \(2003 20] P. Kazienko: Mining Indirect Association Rules For Web Recommendation. Int. J. Appl. Math. Comput. Sci., Vol. 19, No. 1, 165 186. \(2009 21] M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A Verkamo: Finding interesting rules from large sets of discovered association rules. In Proc. 3rd Intl Conf. on Information and Knowledge 


Management. \(1994 22] M. J. Maher, A. Rock, G. Antoniou, D. Billington, T. Miller: Efficient Defeasible Reasoning Systems. International Journal on Artificial Intelligence Tools 10\(4 2001 23] C. Marinica, F. Guillet, and H. Briand: Post-Processing of Discovered Association Rules Using Ontologies. The Second International Workshop on Domain Driven Data Mining, Pisa, Italy \(2008 24] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal: Closed sets based discovery of small covers for association rules. In Proc. BDA'99 Conference, 361-381 \(1999 25] N. Pasquier, R. Taouil, I. Bastide, G. Stume, and  L. Lakhal: Generating a Condensed Representation for Association Rules. In Journal of Intelligent Information Systems, 24:1, 29-60 \(2005 26] P. Pothipruk, G. Governatori: ALE Defeasible Description Logic Australian Conference on Artificial Intelligence.  110-119 \(2006 27] J. Sandvig, B. Mobasher Robustness of collaborative recommendation based on association rule mining, Proceedings of the ACM Conference on Recommender Systems \(2007 28] W. Shen, K. Ong, B. Mitbander, and C. Zaniolo: Metaqueries for data mining. In Fayaad, U. et al. Eds. Advances in Knowledge Discovery and Data Mining. \(1996 29] I. Song, G. Governatori: Nested Rules in Defeasible Logic. RuleML LNCS 3791, 204-208 \(2005 30] H. Toivonen, M. Klemettinen, P. Ronkainer, K. Hatonen, and H Mannila: Pruning and grouping discovered association rules. In ECML Workshop on Statistics, Machine Learning and KDD. \(1995 31] M. Zaki: Generating Non-Redundant Association Rules. In Proc. of the Sixth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 34-43, \(2000 32] w3c. OWL Ontology Web Language Reference. In http://www.w3.org/TR/2004/REC-owl-ref-20040210 33] w3c. RDF/XML Syntax Specification. In: http://www.w3.org/TR/rdfsyntax-grammar 34] w3c. RDF Schema. In: http://www.w3.org/TR/rdf-schema      


 8   2  3\f            8  D    F  \b 1 8 & #J      b 1  1  4    2  


4 1    9  E 1  2 4 1    9 1   4      8 2  8 1  D 1        1 1  b 


     b b b b b  K            8          2 D 9   F  \b 1 8 ,+J  9 


     b 1     1 2  9 1  12 L 1   9  8       1  2      2   


     b b b b b  K            2  0 \b f  b\f      9       


  8 2   E 1   1     M13 31L 1    b  8E 1   1 #3\b?### 1  1     E 1   1 \b?###3        


1   1   b 1  2 2 18 2     8              1    2 \b 1    2  


    2          2   1 L 2 1   1   L 2 2    2 1  2        


    8  2H D \b A             2  2H D \b A 2 \f 3%\f  f   4%\f f !  , \f\b  C    2    2 


 6    3 1      253 6   1 L 2    6   1         f\b3\f       


               1     1     8 2    E       2  1   


     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


