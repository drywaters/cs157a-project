DataCenter 2020: Near-memory Accelerat ion for Data-oriented Applications  Edward Doller, Ameen Akel, Jeffrey Wang, Ken Curewitz, and Sean Eilert  Memory Systems Architecture Research and Development, Micron Technology Inc., Folsom, CA, USA emdoller, aakel, jeffreywang, kmcurewitz, sseilert}@micron.com  Abstract  In the years between now and 2020, we should expect continued exponential data growth  n u m b er of  ongoing advances in storage the transition to solid-state drives \(SSDs\, the scaling of NAND flash capacity, and advanced silicon packaging techniques will dramatically increase the capacity of storage subsystems over the same timeframe.  This will significantly reduce the ratio of storage bandwidth to storage density.  Consequently, the majority of 
data in 2020 will either be cold or will require near-memory acceleration to pull rich information out of the sea of big data  We argue that, increasingly over time, value lies not merely in the size of the data, but rather in what one can do with it  Introduction  Non-volatile memories \(NVMs\e NAND flash in solidstate drives \(SSDs\ have changed the landscape of modern computing and storage subsystems; they enable significant performance improvements over incumbent technologies like rotating disk drives.  SSDs leverage relatively low-latency NAND flash memory to provide 10-1000X faster sector access than rotating disk.  From the systems perspective, the addition of NAND flash provides significant benefits. From the memory perspective, though, systems designed for 
rotating media leave memory bandwidth underutilized and add latency overhead for data access    In big data problems, a system must separate the sparsely stored interesting data from one or more enormous datasets  th i s ca s e th e proces s o r sh ou lders  th e b u rden  of  filtering the incoming data down to an information-rich subset of interest.  Likewise, the storage system must transfer the mostly unused\ata across the storage link, resulting in unnecessary utilization, congestion [13 an d p o w er  consumption [9  1 0  To transform data into useful information, an architecture enabling near-memory acceleration through parallel filtering and preprocessing leverages high internal bandwidth, massive parallelism, and low latencies nearer to the physical location 
of stored data to perform routine data-oriented operations Many previous studies posit similar tradeoffs with disks   ou g h  w e ai m to prov e th e co n cept\222s  u s e f uln e s s  in SSDs.  We hypothesize that a solid-state storage system equipped with this capability will enable systems designers to dramatically improve system efficiency. Systems can offload tedious \(but not computationally expensive\ data manipulation tasks to the massively-parallel storage subsystem, which will provide information-rich datasets to expensive, currently underutilized, central processors    Micron is researching this con cept. We have coined the term ScaleIN to refer to storage and memory devices that are aware of the structure of their contents such that they can pre-filter and enable basic computation on the content at the lowest 
possible level.  We elaborate a few of the potential advantages of this approach   Capacity & Physical Size Today\222s datacenters scale out at the rate of ~petabytes per rack due to the capacity limitations of rotating disks and the relatively high cost of SSDs.  For those with deep pockets storage devices with capacities on the order of 1 PB/U are available today [14  T h o ugh  fo l l o w i n g  m o d e r n N A N D  scaling trends and advanced stacking techniques, we expect that in-package capacities will increase by more than 50X \(by 16X via scaling and by 4x via stacking\y 2020.  This will enable straightforward deployment of exabyte-scale racks Surprisingly, the major problem with this scenario is not the cost.  At this density, system bandwidth becomes a major issue.  Even at 1 TB/s, it would take almost 2 weeks to traverse the data in one rack.  This is perhaps sufficient if the 
problem to be solved involves archival and retrieval of old photos of friends\222 kids\222 little league practices, family vacations, grandma\222s birthday and Fido pestering a frog.  If the problem to be solved involves data manipulation, then system bandwidth must increase on the order of 1,000,000X We demonstrate our vision for near-memory filtering and preprocessing in today\222s systems and highlight the benefits of providing solely information-rich data to high-performance processors  Bandwidth  Consider a representative 2.5\224 6 Gb/s SATA SSD: Its internal architecture contains 64 die organized as 8 die per channel on 8 channels enumerated in Table I and II TABLE I READ BANDWIDTH  Interface Type Bandwidth Relative Increase 
Host 0.6 GB/s 1 X Channel 333 MB/s/ch * 8 ch 2.6 GB/s 4.3 X Total Die 150 MB/s/die * 64 die 9.6 GB/s 16 X  TABLE II BURST WRITE BANDWIDTH   Interface Type Bandwidth Relative Increase Host 0.6 GB/s 1 X Channel \(Not Limited\ 2 X Total Die 20 MB/s/die * 8 die/ch * 8 ch 1.2 GB/s 2 X Note that this analysis igno res write amplification, block erasure, read/write collisions, non-uniform channel requests, and other factors.  It is intended to demonstrate the potential benefit of 
storage devices architected to minimize the impact of these factors  With todays flash technology, our representative 2.5\224 SSD contains 1 TB of flash.  With flash scaling and improved stacking technology the 2020 version of this SSD should scale to 64 TB with roughly 4X higher internal bandwidth The host interface bandwidth will also increase, but the overall internal/external bandwidth disparity in 2020 will grow to approx. 16X-32X\227a 4X increase!  In other words systems designers will have an increased need for nearmemory acceleration in the foreseeable future 2014 Symposium on VLSI Circuits Digest of Technical Papers 
978-1-4799-3328-0/14/$31.00 \2512014 IEEE 


Before jumping to the conclusion that one could push all computation to NAND and immediately reap enormous benefits, we must point out that modern NAND devices can operate with bit error rates on the order of 1%.  While there are some applications that can tolerate high error rates, for general-purpose applications, one should assume that one must operate on corrected data.  Since the correction algorithms and hardware are relatively complex, integration of these functions below the channel level requires significant changes to the memory devices themselves.  One could however, architect a controller with many more channels and fewer devices per channel to achieve a higher bandwidth to capacity ratio.  Though, this change wouldn\222t make sense in the current environment as any benefits would be squandered at the host interface but the rules differ considering nearmemory acceleration For large systems, the bandwidth restriction is even more pronounced as many drives in a RAID or JBOD enclosure are all accessed through a single host bus adapter.  The degree of bandwidth limitation scales with the cost of the storage interface and is likely to amount to a cumulative bandwidth of less than 10 disks; though, the subsystem may have on the order of 100 disks.  Thus, we should count another 10-100X factor in bandwidth inefficiency at the rack level  Latency  There are numerous latencies involved in accessing data from storage devices.  We can view the IO latency components as two major factors: software and storage hardware.  Software latencies refer to the time a storage request spends solely on the host processor, while storage hardware latencies refer to the time a request takes in the storage hardware \(usually attached via an external PCIExpress or SATA interconnect\ When driving a slow storage technology, like rotating disk, software latencies often appear insignificant; however, when driving a much lower-latency medium, like a flash-based SSD, the software latencies become a much more significant latency factor [4   Furthermore, system-level transfer times between the storage hardware and the underlying host processor can add noticeable latency to an IO request It is interesting to note that system-level features like OSlevel IO caches, added to compensate for slow storage represent a significant part of the software latency component These features frequently get in the way of achieving optimal performance from SSDs.  The industry is gradually refactoring systems to leverage the true value of SSDs IO latency factors on a Linux-based OS have been measured in T h e au th ors  m eas u r e a n onopti m ized IO read request latency of 21.5us for a low-latency SSD.  The software latency component totaled 13.2us and the hardware transfer time required 0.93us/KB.  Compared to a 1-10ms rotating disk latency, the software and hardware transfer latencies are insignificant.  The additional overhead perceived for the software layers is far outweighed by the benefit of the bundled caching mechanisms.  Though, for a low-latency SSD with a 10-100us hardware latency, the additional 13.2us software latency adds significant overhead and can represent sizable performance degradation In the ScaleIN architecture, accesses to the storage subsystem from the host system occur at a much higher level of abstraction.  As such, accesses are much less frequent and the impact of latency penalties for each access represents a much smaller overall factor.  In a conventional architecture traversal of a data-dependent data structure\227like a B-treebased index\227involves numerous dependent, serialized sector reads with each suffering the system level latency penalty.  In contrast, the same function in the ScaleIN architecture involves only a single access to the drive [e.g. tree_find\(key root, device n application perceiv e s th e s y s t e m l e v el latency penalty only once for the entire operation Additionally, in decoupling the lower-level IO requests from the host system, designers may more-easily optimize request latencies for each memory type  Energy  The topic of energy consumption in complex compute and storage systems is not simple.  Results depend heavily on the specifics of the workload and hardware chosen.  Let\222s consider some major categories of energy consumption and some generalities around them A. Compute In the presence of an information-rich data source and a compute intensive algorithm, modern multi-core processors are very efficient at performing low-energy computation In many cases today big data can be easily equated to sparse information In other words, large amounts of data must be processed in a trivial fashion to generate an information rich dataset that is intensively processed.  For this case, the central processor spends much of its time waiting on IO.  On receipt of the IO result, in the majority of cases the data doesn\222t contain the desired information and is thus discarded.  Pushing these trivial operations to storage allows the processor to be used in a more efficient manner for computation or to go to sleep and save energy  B. Aggregation In many cases today, server-like systems are being constructed for the simple task of aggregating results from a number of simple storage devices.  It is arguable that many of these architectures will change to a more JBOD-like topology with the advent of advanced SSDs that can reduce data sets to a more manageable form  C. Interface As discussed in the previous section, transmission of data from an SSD ASIC through the host interface and HBA and into the processor core induces latency relative to the latency required to read the data into the SSD ASIC in the first place This movement of data to the host also represents an energy cost.  The specifics depend greatly on the system architecture  MySQL Measurements  To demonstrate some of these concepts, we experimented with custom firmware versions of the Micron M500, a modern commercially available SSD.  The firmware was modified to perform list and tree searches with on-drive resources.  We created a host-side ScaleIN  library that provides application programmers with a simple object-based API \(e.g. list and tree creation, management, and traversal functions\ while abstracting away the management of these structures across multiple underlying SSDs At the application level above the library, we created a custom MySQL storage engine.  This storage engine uses the ScaleIN library to perform all table operations necessary for management of its tables and indices.  Additionally, we 2014 Symposium on VLSI Circuits Digest of Technical Papers 


Number of M500s InnoDB 5000 2 4 6 8 2 3 4 5 2 3 4 5 6 7 8 10000 15000 20000 25000 30000 5 20 25 b\ 4KB Records Speedup Number of M500s 10 Number of M500s a\0B Records MyISAM 0 1 7 1 0 0 Transactions/Second c\ Indexed Records 10 15 modified the MySQL kernel to pass conditionals and indices to the storage engine We conducted experiments in two stages.  In the first stage we limited the acceleration to list-search, effectively allowing the SSD to autonomously search through all records of a nonindexed table for those matching a specific criteria.  In the second stage, we built a system with 24 total drives and added B+-tree searching capability to the firmware.  This allowed us to experiment with searches of indexed tables  A. System Overview In stage one of our experiment, we employed 8 M500 SSDs attached to an Intel i5-2300 system with 8GB of DDR3 DRAM.  The motherboard in our base system allowed connection of up to six SATA drives.  In order to connect the full eight drives, we added an LSI 9212-4i4e PCI-Express 2.0 SAS cont n ected four SSDs to both the native controller and the LSI controller as a JBOD.  This left a native motherboard port for the host OS HDD.  We used Ubuntu Linux 10.04 LTS and test using MySQL 5.5.27 In stage two, we increased the total to 24 drives with an external 2U SAS expansion enclosure connected to the same Intel i5-2300 system through an LSI 9207-8e PCI-Express 3.0 6Gb/s SAS controller.  This allowed access all 24 drives with up to 2.2GB/s bandwidth  B. The ScaleIN Distributed Storage Engine We focused our changes to the base system configuration on MySQL server and the Micron M500 SSD on-board firmware.  Because of the limited firmware space we optimized for the subsystem in MySQL that allowed for the greatest increase in parallelism and the greatest decrease in data movement with the least amount of complexity: the pluggable storage engine For this system, we limited our study to read-only transactions, though we may extend our implementation to include write-based transactions The ScaleIN distributed storage engine consists of two subsystems: the storage engine and the workers.  The storage engine subsystem resides on the host machine, while the workers reside in the firmware of the SSDs.  We spread database records equally across each worker in the system The storage engine is responsible for distributing queries among available workers.  Instead of retrieving each individual record or a collectio n of records from a drive for processing on the host CPU, the engine instructs workers to locally search their portions of the database and return results When a worker exhausts its local buffer space or completes its search, it returns the results back to the engine for aggregation and transmission back to the kernel.  Upon completion by all workers, the aggregated result is returned to the kernel from the storage engine and query results are subsequently returned to the client.  As an interesting side note, the kernel still checks records to ensure that they match the search criteria but in this case it receives 100% hit rate of records matching the search criteria because mismatches were filtered by the workers.  The effect of this is that workers need not be perfect in their filtering of the mismatching data So long as they return all records that match, the search will be functional regardless of what fraction of the mismatching records they are capable of filtering For non-indexed searches, workers manage a list or set of lists contained within their storage.  For indexed searches workers manage a B+-tree contained within their storage.  In both cases, workers accept commands from the storage engine directed at an on-disk structure that contains field definitions in its root sector.  Each command spawns a local search thread in the worker.  When a worker finds a record that matches the current query, it copies the fields of interest to a local buffer for that query.  Once the buffer fills, or the search completes, the worker indicates that results are ready for host-side transfer.  In the current implementation, each worker allows up to three concurrent queries  ScaleIN Performance  This section explores the performance of the ScaleIN system running a simple MySQL benchmark A. MySQL Non-Indexed Query Performance  First, we explore performance searching non-indexed table field.  This search requires every record in the table to be sequentially examined, an O\(n\ operation.  We consider 32 cases of 10 threads accessing a non-indexed, read-only table with 10 million records: 40 or 4K Byte record sizes; stored in either the ScaleIN or MyISAM storage engine; and partitioned across 1 to 8 drives \(Figure 1 \(a\d \(b In the case of 40B records, MyISAM performs better than ScaleIN for cases where partitioning is across less than three drives \(3.2X for one partition or drive\This is expected, as the Intel core that runs MyISAM is much more capable for low-thread-count performance than the small core in the M500.  For cases where we partition across more than three drives, ScaleIN outperforms MyISAM by 1.24-2.28X.  For 4KB records, MyISAM outperforms the ScaleIN system with less than four drives \(2.7X for 1 drive\tperforms MyISAM with more than four \(1.4X for 8 drives Regardless of the record size, host CPU utilization varies vastly between the two systems: MyISAM uses 100% and ScaleIN uses less than 1%.  This significantly impacts energy per operation.  More importantly, this factor leads to seamless linear performance scaling with the number of ScaleIN partitions in the 40B case To combat increasing database storage requirements database designers frequently resort to techniques like sharding \(which dedicates multiple database instances to small portions of a larger table\The ScaleIN approach Fig. 1 Performance comparison of both non-indexed \(a\\(b\ and indexed \(c\ searches between the ScaleIN, MyISAM, and InnoDB stora ge engines.  The results in \(a\d \(b\ormalized to single-disk M500 results, while the results in \(c\ measured valu es  2014 Symposium on VLSI Circuits Digest of Technical Papers ScaleIN 6 8 


Number of M500s 100 Query Latency \(ms 0.1 1000 10000 Scale IN Scale IN MyISAM Scale I N MyISAM MyISAM 20 0 compliments sharding, as one can use multiple mysqld instances with the ScaleIN architecture B. MySQL Indexed Query Performance Having demonstrated performance improvement on an O\(n\on-indexed search by distributing the problem among many workers, we next analyze an O\(log\(n\dexed search We expect less significant gains here, as fewer storage accesses are required in an indexed search.  We show query bandwidth in Figure 1 \(c\ in transactions per second versus number of drives.  Here, we observe a roughly 60 performance improvement for 24-drives In addition to a throughput improvement, we observed improved quality of service with indexed searches \(Figure 2 b\d \(c\.  While all engines perform better with more drives, the ScaleIN engine performs best in all cases except the trivial case of only one drive.  Note that these results are presented on a log scale  C. Energy Comparison  ScaleIN also provides significant energy savings over conventional approaches.  We measured power consumption in each of the query cases benchmarked.  The data show that the linear search benchmark on MyISAM consumes 140W on average, while ScaleIN only consumes 81W due to reduced CPU utilization.  We measured high CPU utilization for all indexed benchmarks, which led to similar power measurements.  This implies that even complex functions pushed to the SSD do not increase power.  Figure 3 shows measured operations per unit energy.  When compared to MyISAM, we observe an energy reduction for indexed workloads of 1.57-1.72X for 7+ threads \(1.13-3.96X for 2 threaded non-indexed workloads\.  This shows the energy efficiency of the highly parallel ScaleIN architecture  Conclusion  Decision makers in the storage industry have begun to use terms like IO bottlenecks  memory bottlenecks  the memory wall and others.  With some basic projections on scaling of storage densities and bandwidth, we showed that these problems should be expected to get worse over time, not to improve.  Near-memory acceleration changes the rules and thus allows us to think about these problems differently Near-memory acceleration can provide orders of magnitude increases in bandwidth to the enormous data sets of the future to compensate for the ever-increasing disparity between storage capacity and bandwidth.  This paper has demonstrated an example case where acceleratio n primitives provided real system value through improved scalability that may be realized through cost reductions, performance improvements or energy reductions. With predictable advances in the storage industry, we will be able to store copious amounts of data.  With realizable advances in near-memory acceleration though, it will be possible to turn this big data into useful information-rich data sets; a much more exciting proposition   References  1  http://www.micron.com/products/solid-state-storage/clientssd/m500-ssd 2  http://www.lsi.com/products/host-bus-adapters/pages/lsi-sas9212-4i4e.aspx 3  http://www.storagereview.com/micron_m500_enterprise_ssd_r eview 4  A. M. Caulfield et.al., Moneta: A high-performance storage array architecture for next-generation, non-volatile memories In Proceedings of The 43rd A nnual IEEE/ACM International Symposium on Microarchitecture, 2010 5  K. Keeton, D. Patterson, and J. Hellerstein, \223A Case for Intelligent Disks \(IDISKs\ SIGMOD, September 1998 6  A. Ailamaki, D. DeWitt, M. Hill, and D. Wood, \223DBMSs On A Modern Processor: Where Does Time Go?\224  VLDB, 1999 7  N. Hardavellas et. al., \223Database Servers on Chip Multiprocessors: Limitations and Opportunities,\224 3 rd Biennial Conference on Innovative Data Systems Research, January 2007 8  A. De, M. Gokhale, R. Gupta, and S. Swanson, \223Minerva Accelerating Data Analysis in Next-Generation SSDs,\224 21 st  annual International Symposium on Field-Programmable Custom Computing Machines, pp. 9-16, 2013 9  M. Horowitz, \223Computing\222s Energy Problem \(and what we can do about it\ IEEE International Solid-State Circuits Conference, pp. 10-14, February 2014 10  K. Malladi et. al, \223Towards Energy-Proportional Datacenter Memory with Mobile DRAM,\224 The 39 th Annual International Symposium on Computer Architecture, pp. 37-48, June 2012 11  A. Caulfield et. al., \223Understanding the Impact of Emerging Non-Volatile Memories on High-Performance, IO-Intensive Computing\224, Supercomputing, 2010 12  E. Riedel, G. Gibson, and C. Faloutsos, \223Active Storage For Large-Scale Data Mining and Multimedia,\224 24 th International Conference on Very Large Databases, August 1998 13  J. Piernas, J. Nieplocha, and E. Felix, \223Evaluation of Active Storage Strategies for the Lustre Parallel File System,\224 The 2007 ACM/IEEE conference on Supercomputing, 2007 14  http://www.skyera.com/products/skyeagle/overview 15  M. Hilbert and P. Lopez, \223The World\222s Technological Capacity to Store, Communicate, and Compute Information,\224 Science, pp 60-65, April 2011 16  J. Manyika et. al., \223Big data: The next frontier for innovation competition, and productivity,\224 May 2011  2014 Symposium on VLSI Circuits Digest of Technical Papers 0 0 0 MyI SA M b\rives InnoDB 10 10 15 25 a a\rive 1 2 3 4 20 30 40 50 60 5 15 20 25 5 b c Fig. 3 Operations per unit energy comparison for each benchmark a\, \(b\ for non-indexed and \(c\ for indexed transactions  Normalized Queries/Unit Energy 10 c\rives Fig. 2 A comparison of query response latencies by storage engine and number of drives  ScaleIN 1 0.01 10 InnoDB InnoDB InnoDB 


       


      


         


8 case Since the process of exibly determining  is done after TR algorithm We also nd that FTR algorithm can save more regeneration time than TR algorithm when d grows B Effect of the bandwidth variance In order to show the impact of network bandwidth variance we run simulations with 5 different link capacity distributions U 1 0  3  120 Mbps U 2 3  120 Mbps U 3 30  120 Mbps U 4 60  120 Mbps U 5 90  120 Mbps The number of providers d is set to 10  Results are shown in Fig 8 In general the performance of our algorithms is better when the variance of network bandwidth is large For uniform distribution U 1 0  3  120 Mbps both TR and FTR can reduce 90 of regeneration time compared with the traditional STAR scheme When the variance of network bandwidth becomes small for example at U 4 60  120 Mbps and U 5 90  120 Mbps TR has the same regeneration time as STAR but FTR still reduces the regeneration time by 10  20   0.3,120]Mbps 3,120]Mbps 30,120]Mbps 60,120]Mbps 90,120]Mbps 0 0.2 0.4 0.6 0.8 1 Regeneration time percentage MSR, n = 20, k = 5, d = 10, file size = 1GB   RCTREE/STAR TR/STAR FTR/STAR Fig 8 Regeneration time of RCTREE TR and FTR normalized by STAR for different bandwidth The parameters are n 20 k 5 d 10 and M 1 GB C Effect of the storage capacity per node  Our previous tests mainly focus on the MSR point which achieves the optimal storage efìciency However as shown by Dimakis et al  the repair bandwidth can be reduced by storing more data on each storage node Speciìcally the MBR point achieves the minimum repair bandwidth We vary the storage  from the MSR point to the MBR point to test its impact on our algorithms Fig 9 shows the regeneration time for different   We can see that every regeneration schemes repair faster as  grows to the minimum repair bandwidth point and the tendencies of different repair schemes are the same Compared with STAR we nd that the ratio of reduced regeneration time does not change much for different values of   This implies that our previous simulation results and conclusions for the MSR case also apply to different storage   205 211 221 236 256 0 2 4 6 8 10 12 14 16 18 Storage per node MB Regeneration time\(s n = 20, k = 5,d = 10, file size = 1GB, bandwidth = [10,120]Mbps   STAR RCTREE TR FTR MSR MBR Fig 9 Regeneration time of STAR RCTREE TR and FTR vs storage capacity of each node    where  vary from MSR point to MBR point The parameters are n 20 k 5 d 10 and M 1 GB VI R ELATED W ORK Li et al  rst considered the heterogeneity of netw ork bandwidth in data regeneration process and proposed a treestructured regeneration scheme to reduce the regeneration time They also proposed a scheme of building parallel regeneration trees to further reduce the regeneration time in the network with asymmetric links Ho we v e r  the y only discussed the case that the regeneration scheme requires k providers which means the minimal regeneration trafìc is equal to the size of original le M  To further reduce the regeneration time they considered the regenerating codes in the tree-structured regeneration scheme and proposed RCTREE in The y emplo y a minimum-storage re generating MSR codes in RCTREE which means the minimal regeneration trafìc is d k  d  k 1 M bytes i.e  in a regeneration with d providers each provider sends  d  k 1 blocks to its parent node To make sure that the newcomer has enough information to restore  blocks it has to receive data directly from at least d  k 1 providers The details of how to construct an optimal regeneration tree can be found in Algorithm 1 in their paper  Although the y h a v e done this in their algorithm to ensure that the degree of newcomer is at least d  k 1  the MDS property can not be preserved after data regeneration Sun et al  considered the scenario of repairing multiple data losses and proposed two algorithms based on treestructured regeneration to reduce the regeneration time However they assumed the amount of data transferred between providers and newcomer is designed to be the same as  in regenerating codes According to our analysis their regeneration schemes can not preserve the MDS property either Some researches such as 17 considered the heterogeneity of nodes availability and optimized the erasure code deployment to reduce the data redundancy Some researches such as jointly considered the repair cost and heterogeneity of communication\(download cost on each links They exibly determine the amount of data to minimize the total repair cost which is different from the regeneration time IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 1885 


9 VII C ONCLUSION We reconsider the problem of how to reduce the regeneration time in networks with heterogeneous link capacities We analyze the minimum amount of data to be transmitted on each link of the regeneration tree and prove that the problem of building optimal regeneration tree is NP-complete We propose a heuristic algorithm to construct a near-optimal regeneration tree and further reduce the regeneration time by allowing non-uniform end-to-end repair trafìcs Simulation results show that our regeneration schemes are able to maintain the MDS property and reduce the regeneration time by 10  90 compared with traditional star-structured regenerating codes With the non-uniform end-to-end repair trafìcs we can exibly determine the amount of coded data generated by each provider The proposed Flexible Tree-structured Regeneration scheme performs even better than RCTREE R EFERENCES  J Li S Y ang X W ang and B Li T ree-structured Data Regeneration in Distributed Storage Systems with Regenerating Codes in Proceedings of IEEE Conference on Computer Communications INFOCOM  2010  S Ghema w at H Gobiof f and S.-T  Leung The google le system in ACM SIGOPS Operating Systems Review  vol 37 no 5 ACM 2003 pp 29Ö43  R Bhagw an K T ati Y C Cheng S Sa v age and G M Voelker Total recall system support for automated availability management in Proceedings of the 1st conference on Symposium on Networked Systems Design and Implementation NSDI  2004  J K ubiato wicz D Bindel Y  Chen S Czerwinski P Eaton D Geels R Gummadi S Rhea H Weatherspoon W Weimer et al  Oceanstore An architecture for global-scale persistent storage ACM Sigplan Notices  vol 35 no 11 pp 190Ö201 2000  W uala-Secure Cloud Storage Online A v ailable http://www.wuala.com  A G Dimakis P  B Godfre y  M J W  Y  W u  and K Ramchandran Network Coding for Distributed Storage System IEEE Transactions on Information Theory  vol 56 no 9 pp 4539Ö4551 2010  B Gast  on J Pujol and M Villanueva A realistic distributed storage system the rack model arXiv preprint arXiv:1302.5657  2013  T  Benson A Ak ella and D A Maltz Netw ork traf c characteristics of data centers in the wild in Proceedings of the 10th ACM SIGCOMM conference on Internet measurement  ser IMC 10 ACM 2010 pp 267Ö280  Google Datacenters Online A v ailable http://www.google.com/about/datacenters/inside/datasecurity/index.html  P  P  C L Y uchong Hu Henry C H Chen and Y  T ang Nccloud applying network coding for the storage repair in a cloud-of-clouds in Proceedings of USSENIX Conference on File and Storage Technologies\(FAST  2012  J Li S Y ang and X W ang Building parallel re generation trees in distributed storage systems with asymmetric links in 2010 6th International Conference on Collaborative Computing Networking Applications and Worksharing CollaborateCom  2010  S.-J Lee P  Sharma S Banerjee S Basu and R F onseca Measuring bandwidth between planetlab nodes in Passive and Active Network Measurement  Springer 2005 pp 292Ö305  A Duminuco and E Biersack  A practical study of regenerating codes for peer-to-peer backup systems in 29th IEEE International Conference on Distributed Computing Systems\(ICDCSê09  IEEE 2009  Prims algorithm Online A v ailable http://en.wikipedia.org/wiki/Primsalgorithm  J Li S Y ang X W ang X Xue and B Li T reestructured data regeneration with network coding in distributed storage systems in Proceedings of 17th International Workshop on Quality of Service\(IWQoS  2009  W  Sun Y  W ang and X Pei T ree-structured parallel regeneration for multiple data losses in distributed storage systems based on erasure codes Communications China  vol 10 no 4 pp 113Ö125 2013  L P amies-Juarez P  Garcia-Lopez and M SanchezArtigas Heterogeneity-aware erasure codes for peer-topeer storage systems in International Conference on Parallel Processing ICPP  2009  G Xu S Lin G W ang X Liu K Shi and H Zhang Hero Heterogeneity-aware erasure coded redundancy optimal allocation for reliable storage in distributed networks in International Performance Computing and Communications Conference IPCCC  2012  S Akhlaghi A Kiani and M R Ghana v ati Costbandwidth Tradeoff in Distributed Storage Systems Computer Communications  vol 33 no 17 pp 2105 2115 2010  M Gerami M Xiao and M Sk oglund Optimal-cost Repair in Multi-hop Distributed Storage Systems in Proc of IEEE International Symposium on Information Theory ISIT  2011 pp 1437Ö1441  S Akhlaghi A Kiani and M Ghana v ati  A fundamental trade-off between the download cost and repair bandwidth in distributed storage systems in Proceedings of IEEE International Symposium on Network Coding NetCod  2010  C Armstrong and A V ardy  Distrib uted storage with communication costs in Proceedings of Annual Allerton Conference on Communication Control and Computing Allerton  2011  N Shah K V  Rashmi and P  K umar   A  e xible class of regenerating codes for distributed storage in Proceedings of IEEE International Symposium on Information Theory Proceedings ISIT  2010 IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 1886 


       


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


