Sentence Completion Task using Web scale Data   Kyusong Lee  Department of Computer Science and Engineering  Pohang University of Science and Technology  Pohang, Korea  kyusonglee@postech.ac.kr  Gary Geunbae Lee  Department of Computer Science and Engineering  Pohang University of Science and Technology  Pohang, Korea  gblee@postech.ac.kr   Abstract 227  We propose a method  to automatically answer SAT style sentence completion questions using web scale data Web scale da ta have been used in many language studies and  have been  found to be a very useful resource for improving accuracy in  sentence completion task Our method e 
mploys assorted N gram probability information for  each candidate word We also  proposed back off strategy was used to remove zero probabilities. We found that the accuracy of our proposed method  improved by 52 87% over the current state of the art  Keywor ds 227 N gram;sentence completion Web cale Data Lexical  Disambiguation  I   I NTRODUCTION  Recently, standardize d  examinations  such as the Test of   English as  a  Foreign Language \(TOEFL\, Graduate Record Exam \(GRE\ and SAT  Reasoning Test  have been used for assessing  in  natural  
language processing  NLP  tasks  1    2   Microsoft Research published sentence completion challenge SCC\  data for the problem of sentence level semantic coherence  3   The challenge was designed as a benchmark for semantic models and consists of SAT style sentence completion problems. Given 1,040 sentences, each  of which is missing a word, the task is to select the correct word from among  the candidates provided for each sentence P revious method s have used  Ensemble learning with both local lexical and global coherence, such as latent semantic allocation    4    We b s cale data have  b een successfully used in many research area s such as lexical disambiguation, machine 
translation and grammar error correction \(Bergsma, Lin Goebel, 2009 Most NLP systems resolve ambiguit ies  with the help of a large corpus of text  1  The s ystem tried to decide {among, between} the two confusable words  The larger the corpus, the more accurate the disambiguation  5  Many systems incorporate the web count into their selection process. For the above example \(1\, a typ ical web based system would query a search engine with the sequences 223decide among the\224 and 223 decide between the\224 and select the candidate that returns the most hits This approach would fail when more context is required for disambiguation Bergsma  2009  s uggested  using contexts o f various lengths and positions 
and t his method wa s successfully applied in lexical disambiguation. Web scale data were used with its count information  specified  as features  6  The re sults of previous research show  that web scale data is a very useful resource for language rese arch. This paper shows that using a  probability feature rather than count  feature  can  significantly improve accuracy on SCC. Additionally, rather than using a single sequence of n gram probability applying  context of various lengths and positions from 2 g ram to 5 gram improved accuracy. Moreover rather than calculating one word\222s probability given n words such as  
our model calculates  the probability of  m words given  an n word sequence  such as  m = 3, n = 1  m = 2, n = 2  m = 1, n = 3    The p roposed back off strategy was used to avoid case 
s  in which  every distractor has the same probability  of 0  II  D EFINE THE P ROBLEM  A s entence completion task  pres ent s  a sentence with one blank  which is   options for each blank  Stem: We took no _____ to hide it  Distractors   a\ fault   b\ instructions   c\ permission   d\ pains   e\ fidelity  Figure 1 sample MSR sentence completion challenge data  All possible answers except 
the one true solution  pains  result in a nonsensical  sentence. The system select s  the answer by calculating coherence. Among 1,040 questions, we explore how many questions the system can correctly answer  III  D ATA S ET  W e used the Google  Web1T  N gram  Count  corpus 1  which contain s  1 trillion words of running text and the counts for all 1 billion five word sequences that appear at least 40 times A fter words that appear fewer than 200 times are discarded there are    1  Available from the LDC as LDC2006T13  173 


13 million unique words  The MS SCC  data were used for test data 2 Th e test data includes 1,040 questions, and each equation has five distractors, with the correct answer labeled   IV  M ETHOD  We constructed  a probabilistic language model using the Google Web1T N gram Count corpus. Zero probability    2  http://research.microsoft.com/en us/projects/scc   problems are in evitable when considering high order N gram probability Thus, various N gram smoothing technique s  w ere  studied, such as Good Turing interpolation, and back off  7    8    9  However  retaining  every count sequence is impossible in web scale data because the quantity of data will be too large to handle. T he Google N gram co unt corpus also cannot be used to build previous smoothing methods because of the frequency cut offs, which impl y that N grams appearing  less  than 40 times were removed   most smoothing m et hods require low order word counts   The p revious method  has low perf ormance at best an  accuracy of  52%\ for the sentence completion challenge \(Table 2 Only half of  the  questions in the SCC data could be correctly answered by the previous method  which implies that it is a challenging task  T he task is to select the high est probability sentence among the five sentences as shown  in Figure 1   S 1 We took no fault to hide it.                  P\(S 1   S 2 We took no instructions to hide it.       P\(S 2   S 3 e  took no permission to hide it.        P\(S 3   S 4 We took no pains to hide it.                 P\(S 4   S 5 We took no fidelity to hide it.              P\(S 5   The previous method use d  the count feature to distinguish the sensible  sentence among the non sens ical  sentences. We applied the probability information using web scale count information as in equation \(1    1  For smoothing, a proposed backoff model is used as in equation \(2        2   where            denotes distractors e.g   fault,  instructions permission, pains, fidelity Stem denotes the  prearranged  words in the question sentence \(e.g., Stem We took no _____ to hide it   denotes the answers the system selects. If the N gram probabilities are zero for  every distract or, we approximate them by backing off to the \(N 1 grams. We describe the notation of P N  in more detail in \(Table 1  Words immediately  preceding  and following the blank are already giv en in  the  sentence completion so we can co nsider both directions  from the blank  and various ranges  In some sentences words  following the blank  are good features for identifying  the question  position  I've never _____ a word about it yet to mortal man  Table 1 Proposed back off N gram  5 GRAM                                                                                               


In o ther sentences the preceding  words are more useful  features  His grandfather was a royal _____, and he himself has been to Eton and Oxford  However, we cannot predict  which direction will  be the crucial feature in selecting  the answer. App lying only forward N gram and applying only  backward N gram have  the same accuracy \(Table 3\. Therefore   o ur proposed  method  for  the  using N gram model consider s  both forward and backward N gram probabilit ies  and has  a  much reduced  data sparseness problem The 5 gram to 2 gram probability  information is  used to develop the proposed N gram model  Detailed algorithms for the proposed n gram, back off, and sentence completion tasks are presented in Figure 2. The Ngram  algorithm \(Figure  2 is exactly the same as in the Table 1  V  E XPERIMENT AND RESULT  We used the evaluation tool provided  in the MS SCC set  The previously established best result  of  52% was produced by a combination of  the  LSA total similarity and N gram models Zweig et al 201 2 Note that the result of  1\ \(2\ and the combination \(1\+\(2\ are from Zweig\222s  paper  The accuracy of our proposed method is 87.4 %, which is significantly improved  We used the unsupervised approach that does not require training data for the s entence completion task To compare the accuracy with Bergsma 222s unsupervised approach w e implemented the SUMLM method proposed for disambiguation problem Bergsma, Lin, & Goebel, 2009  For example, from  Figure  1, the following 5 gram patterns  are extract ed   We took no _____ to hide it  s> We took no D i  We took no D i  hide  took no  D i  hide it  no D i  hide it  D i  hide it . </s  Similarly, there are four 4 gram  patterns three 3 gram pattern s and  two 2 gram patterns spanning the target  The words that replace the target word are called pattern fillers; F f 1 f 2 205,f F F fault, instructions, permission,  pains fidelity}. With |F| fillers, there are 14|F| filled patterns with  a  relevant N gram count. A score for each filler is calcula ted by summing the log counts of all the context patterns  produced by using that filler. Add one smoothing is applied in the case of a 0 count. The SUMLM achieves 60% accuracy, which is higher than the current state of the art. From the results of the pro posed method \(87.4%\ and four gram probability with Laplace smoothing \(85.9%\, we found that using the probability feature offers considerably better accuracy than using the count feature. The 5 gram count will always be a more critical feature than the 2 gram count. However, if we just consider count information, the bi gram count is normally larger than the five gram count, which leads to mis predictions When applying the probability, the high order probability is normally larger than low order probabili ty  To explore the effect of using forward and backward n gram, we conducted additional experiments \(Table 3 Laplace Algorithm Ngram      sent  Require   ta rget index  1 f or  k  in range\(5,1 1   2 for  j  in range\(1 k 1   3 for  i  in range k j   4:              d1    j 1   k j 1   5:              d2    1 i    k j 1   6:              d3     j k    j    7:              d4     j k   i   8 M append        9 M append      10 return  M   Algorithm backoff     Require 5,40 matrix \(Return values from Ngram  algorithm on 5 sentences  1:  fivegram 5,1   T 0:20 T 20,1  2:  fourgram 5,1   T 20:32  T   12,1  3:  trigram 5,1   T 32:38  T   6,1  4:  bi gram 5,1   T 38:40  T   2,1  5  if  not sum\(fivegram 5,1 is 0  6 return  fivegram 5,1  7 else if  not sum\(fourgram 5,1 is 0  8 return  fourgram 5,1  9 else if  not sum\(trigram\ is 0  10 return  trigram 5,1  11 else if  not sum\(bigram 5,1 is 0   12 return  bigram 5,1  13 else  14 return  Uniform Distribution 5,1   Algorithm SentComp       List of sentences  Require  Target Index  1 f or  sent in   2:     M Ngram   sent    3 W append\(M  4: Result backoff   W   5 return  argmax\(Result  Figure 2 Detail ed  algorithms of Ngram, backoff and sentence completion denotes every element of the matrix is 1, Variable n,m denotes the size is n by m. T denotes matrix transformation. __ Underlined syntax is the same as the pytho n syntax   175 


smoothing the N gram model  10   avoids zero pr obability in equation 3       3  The accuracy is the same for forward s  and backward s  in every n gram. An interesting result was that the accuracy on 5 gram count is very low \(3.8 %\. Becaus e a ll N grams with counts lower than 40 were discarded most of the 5 gram s  count s  were  discarded. Thus, most five gram have 0  probability. The 4 gram model with Laplace smoothing has very high accuracy, which indicates that the web scale data are very useful for improving the accuracy of the SCC  task   Our proposed method ha s several advantages particularly well suited to  SCC F irst, data size is very important in the statistical approach, and the web scale Google 1T n gram corpus is enormous. Moreover  probability is more informative than the count feature. T o  treat the high order n gram as more important than the low order n gr am, we consider various cases of high order n gram sequence probability \(table 1\, then only back off when all the distractors have 0 probability  VI  C ONCLUSION  We found that using the web scale count is the crucial feature for sentence completion \(52 60%\. T he accuracy is improved when using probability rather than the count feature using web scale data \(60 85.9%\. We applied both backward and forward n gram sequences probability with various ranges 85.8  87.4%\. In the future, to address long dependency fea tures, we will explore the dependency N gram features to further improve accuracy   ACKNOWLEDGMENT  This research was supported by the MSIP\(The Ministry of Science, ICT and Future Planning\, Korea and Microsoft Research, under IT/SW Creative research prog ram supervised by the NIPA\(National IT Industry Promotion Agency\" \(NIPA 2013  H0503 13 1006   REFERENCES    E. Charniak, Y. Altun, R. d. S. Braz, B. Garrett, M Kosmala, T. Moscovich et al Reading comprehension programs in a statistical language processing class," in Proceedings of the 2000 ANLP/NAACL Workshop on Reading comprehension tests as evaluation for computer based language understanding sytems Volume 6 2000, pp. 1 5    T. K. Landauer and S. T. Dumais, "A solution to Plato's problem: The latent semantic anal ysis theory of acquisition, induction, and representation of knowledge Psychological review vol. 104, p. 211 1997    G. Zweig and C. J. Burges, "The Microsoft Research sentence completion challenge," Technical Report MSR TR 2011 129, Microsoft2011   4  G. Zweig, J. C. Platt, C. Meek, C. J. Burges, A. Yessenalina and Q. Liu, "Computational approaches to sentence completion," in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers Volume 1 2012, pp. 601 610    M. Banko and E. Brill, "Scaling to very very large corpora for natural language disambiguation," in Proceedings of the 39th Annual Meeting on Association for Computational Linguistics 2001, pp. 26 33    S. Bergsma, D. Lin, and R. Goebel, "Web s cale N gram models for lexical disambiguation," in Proceedings of IJCAI 2009, pp. 1 6    I. J. Good, "The population frequencies of species and the estimation of population parameters Biometrika vol 40, pp. 237 264, 1953    L. E. Baum, "An equalit y and associated maximization technique in statistical estimation for probabilistic functions of Markov processes Inequalities vol. 3 pp. 1 8, 1972    S. Katz, "Estimation of probabilities from sparse data for the language model component of a speech  recognizer Acoustics, Speech and Signal Processing IEEE Transactions on vol. 35, pp. 400 401, 1987    G. J. Lidstone, "Note on the general case of the Bayes Laplace formula for inductive or a posteriori probabilities Transactions of the Faculty o f Actuaries vol. 8, p. 13, 1920    Table 2   MSR sentence completion  challenge  SC C  performance with the proposed N gram method   MSR SC   Chance  20  1\ GT N gram LM  39  2\ LSA  Total Similarity  49  Combination \(1\ + \(2  52  SUMLM  60  Proposed Method         4 gram forward  85.8  4 gram backward  85 9  3 gram forward  55.8  3 gram backward  55.8  2 gram forward  32.7  2 gram backward  32.7   176 


5 for short is NP-complete To demonstrate this we start with the formal de“nition of the ORT problem De“nition 1 For a given overlay network G  V E  with link capacity c  u v    u v   E  the optimal regeneration tree problem is to nd a spanning tree T such that its regeneration time max  f  u,v   c  u,v    u v   T  is minimized where f  u v in  m u    and m u is the number of nodes in the sub-tree rooted at u  To study the complexity of ORT problem it is equivalent to restate this optimization problem as a decision problem which aims to determine whether the regeneration time of an optimal regeneration tree is no more than 1  The following theorem shows that this problem is NP-complete Theorem 2 The ORT problem is NP-complete Proof We rst show that ORT  NP Suppose we are given a graph G  V E   The certi“cate we choose is the optimal regeneration tree T  The veri“cation algorithm checks for each edge  u v   T  that f  u,v  c  u,v   1  This veri“cation can be performed in polynomial time We now prove that the ORT problem is NP-hard by reduction from the VERTEX-COVER problem which is known to be NP-complete In particular given an undirected graph G  V E  and an integer k  the VERTEX-COVER problem asks whether all edges can be covered by k nodes where node u  V can cover edge e  E only if they are adjacent For an instance of the VERTEX-COVER problem we construct a regeneration scenario in an overlay network G   V  E    such that the regeneration time is less than 1 if and only if G has a vertex cover of size k  We construct G  in the following way G  has four layers of nodes The rst layer has only one node that is the root t  The second layer has two nodes node a and node b  Both a and b are connected to the root The link capacity of edge  a t  is k   E  1 and the link capacity of edge  b t  is unlimited The nodes in the third layer correspond to the vertices in graph G  all of them are connected to both a and b  The link capacity of their edges connected to a is unlimited and the link capacity of edges connected to b is 1  The nodes in the last layer correspond to the edges in graph G  Each node in the last layer is connected to two nodes in the third layer which is connected by the corresponding edge in graph G  The link capacity of these edges which connect the nodes in the third layer to the nodes in the last layer is unlimited Links which are not mentioned in the construction are supposed to have zero capacity From the construction graph G  can be constructed from G in polynomial time Fig 6 shows an example of this reduction Fig 6\(a is an instance  G k 2 of the VERTEX-COVER problem Fig 6\(b is the graph G  constructed from  G k   We next show that this transformation of G into G  is a reduction First suppose that G has a vertex cover set V   V  where  V    k  We claim that we can nd a tree whose regeneration time is no more than 1  We construct this tree according to the vertex cover for each node e i of the fourth layer let its parent be v j which covers the edge e i in the vertex cover of G  For each node v i of the third layer let its parent a b Fig 6 Reducing VERTEX-COVER problem to Optimal Regeneration Tree problem a An undirected graph G  V E  with k 2  b The graph produced by the reduction procedure be a  if it belongs to the vertex cover V   Otherwise let its parent be b  Finally let the parent of a and b be the root It can be veri“ed the regeneration time is 1  Conversely suppose that G  has a tree whose regeneration time is no more than 1  Then we claim that the edges of G can be covered by no more than k nodes Let V   V be the set of nodes that correspond to children of node a in the regeneration tree First V  is a vertex cover of G since otherwise some nodes in the fourth layer will have to be connected to the root through b  causing some ow entering b larger than 1 and the regeneration time less than 1  Second  V   k because there are no more than k   E  1 nodes transferring data from node a to root t  and as all the  E  nodes in the fourth layer must transfer data through the link a to t  the number of third layer nodes with parent a is no more than k to ensure the ow on the link a to t is no more than k   E  1  C The heuristic algorithm In this section we propose a heuristic algorithm to solve the ORT problem mentioned in Sec III-B The algorithm is inspired by Prims algorithm for the maximum weighted spanning tree problem We start from a tree containing only the newcomer and iteratively add the remaining nodes to the regeneration tree until it spans all providers In each iteration we try all possible positions for each remaining provider and choose the best one and position to add it to the regeneration tree The details are shown in Algorithm 1 The most time consuming step is to test all possible positions for each provider which has  V  2 choices Each test takes O   V   to compute the regeneration time Thus the algorithm runs in polynomial time O   V  3   IV T REE STRUCTURED R EGENERATION WITH F LEXIBLE E ND TO E ND T RAFFICS In section III we assume that each provider has the same amount of data to be transmitted to the newcomer This assumption is actually not necessary As the available bandwidth varies for different providers we may repair faster if we allow providers to generate different number of blocks according to their bandwidths IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 1882 


6  Algorithm 1 Find a regeneration tree T for a given network G  V E   1 Input Network topology G  V E   link capacity c    2 Output Regeneration tree T 3 T  v 0 4 A  V  v 0  5 while A 012   do 6 for all v  A do 7 for all u  T do 8 Compute the regeneration time of tree T   v u   9 If the regeneration time is better than previous choices update  v  u     v u  10 end for 11 end for 12 T  T   v  u    13 A  A  v  14 end while In this section we propose the Flexible Tree-structured Regeneration FTR scheme to further reduce the regeneration time The idea can be illustrated by the example shown in Fig 1 Fig 1\(c shows a regeneration tree computed by algorithm 1 where each provider generates  80 Mb coded data to be transmitted to the newcomer We have computed that its regeneration time is 4s Note that the bottleneck link is  v 3 v 0  Ifwerequire provider v 1 v 2 v 3 v 4 to generate 90Mb 90Mb 60Mb 90Mb data respectively The actual data transmitted on each link will be f  v 4 v 1  f  v 2 v 0 0 Mb,f  v 3 v 0  60 Mb,f  v 1 v 0 in  90  90 Mb  240 Mb   and the regeneration time is further reduced to 3s which saves 25 of regeneration time than TR in this example Let  i denote the amount of information generated by the i th provider Note that if the providers of low-bandwidth generate less coded blocks the high-bandwidth providers will have to generate more coded blocks to make sure the MDS property is maintained So our rst task is to analyze the explicit restrictions on  i that ensures the MDS property For a regeneration tree and given  i i 1  2   d we require that the intermediate nodes perform coding on received blocks only when the number of blocks received from the child nodes plus the coded blocks generated by themselves is larger than   In this way the number of blocks transmitted on each link f  u v  will be min   v i  S  u   i   where S  u  denotes the set of nodes in the sub-tree rooted at u  The following theorem provides a suf“cient condition Theorem 3 The MDS property is maintained if in each regeneration we choose  i i 1  2   d that satisfy d  k  j  l 1  i l  min   d  k  j     j 1  2   k where i l is a permutation of 1  2   d such that  i 1   i 2     i d  Proof We need to prove that any min-cut  U  U  DC  U  has volume at least M  As the capacity from a storage node to the data collector DC is set to in“nity in the information ow graph we only need to consider the case that U contains at least k storage nodes Let v 1 v 2   v k be the rst k storage nodes of U in the topological order If it is an output node the storage link of capacity  will be included in the cut If it is an input node set of the links repairing this node will be included in the cut For the j th storage node the number of its provider not in U will be at least d  i 1  According to the way how we determine the ow on the regeneration tree the total capacity of the cutting repair links will be at least d  k  j  l 1  i l  min   d  k  j     Therefore the volume of the cut  U  U  will be no less than M  According to the analysis above we propose an algorithm to calculate the amount of blocks  i generated by each provider The algorithm is based on the regeneration tree constructed in the previous section In general the algorithm has two steps 1 calculate the available repair bandwidth b i i 1  2   d for each provider and rearrange the labels of providers such that b 1  b 2    b d  2 solve the following linear programming problem for  i  min  subject to    i b i   i 1  2   d  d  k  j l 1  l  min   d  k  j       j 1  2   k 0   1   2   d   1 Here the variable  is introduced for representing max 1  i  d   i b i   which is ensured by the rst d constraints The details are shown in Algorithm 2 Note that we must be careful calculating the available end-to-end bandwidth b i from each provider to newcomer as the provider-to-newcomer traf“cs may share a common link in the regeneration tree For a link shared by m providers we assume each provider occupies 1 m of the link bandwidth Lin e 3 11 In Algorithm 2 the estimated bandwidth b i can be computed in time O   V  3   The end-to-end traf“c  i can be computed by solving a linear programming problem of d 1 variables and 2 d  k 1 constraints which can be solved in polynomial time as well V E VALUATION In this section we present simulation results to verify the effectiveness of our proposed algorithms Tree-structured Regeneration TR and Flexible Tree-structured Regeneration FTR Our most concern is the regeneration time which is measured as the time that the newcomer spends on regenerating the coded blocks We ignore the the encoding time on IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 1883 


7  6 7 8 9 10 11 12 13 14 15 16 17 18 19 10 0 10 1 10 2 d \(# of providers Regeneration Time\(s MSR, n = 20, k = 5,  fileSize = 1GB, bandwidth = [10,120]Mbps   STAR RCTREE TR FTR a Regeneration time 6 7 8 9 10 11 12 13 14 15 16 17 18 19 0.25 0.3 0.35 0.4 0.45 0.5 d \(# of providers Regeneration time percentage MSR, n = 20, k = 5, file size = 1GB, bandwidth = [10,120]Mbps   RCTREE/STAR TR/STAR FTR/STAR b Regeneration time of RCTREE TR and FTR normalized by the regeneration time of STAR 6 7 8 9 10 11 12 13 14 15 16 17 18 19 10 20 30 40 50 60 70 80 d\(# of providers Bottleneck bandwidth \(Mbps MSR, n = 20, k = 5, file size = 1GB, bandwidth = [10,120]Mbps   STAR RCTREE TR\(FTR c Bottleneck bandwidth Fig 7 Performance of four regeneration schemes for different number of providers  d  The parameters are n 20 k 5 and M 1 GB Algorithm 2 Determine the amount of coded blocks that each provider should generate 1 Input n k d M  a regeneration tree T with root v 0  2 Output   1    d  3 for i 1 to d do 4 for all edge e from v i to v 0 do 5 a  the bandwidth of edge e 6 m  the number of nodes in the subtree rooted at edge e 7 s  the smallest value of a/m 8 end for 9 b i  s 10 end for 11 sort  b 1   b d  such that b 1  b 2  b d 12 solve the linear programming problem 1 for  i each provider and the decoding time on newcomer because the encoding and decoding operations can be performed simultaneously with the transmission For default settings we use the same experiment setup as  where redundant data is produced as an  n 20 k  5 MDS code The original le size is set to 1GB The link capacities between these nodes obey the uniform distribution U  according to the measurement of real world networks A Effect of the number of providers d The number of providers d is a key parameter in the distributed storage system According to the total bandwidth consumed in the regeneration process decreases as d grows Because of this the theoretical optimal value of d is n  1  But accessing a large number of providers will introduce extra communication overhead Thus all feasible values of d may appear in practice In the evaluation we vary d from k 1 to n  1  in order to nd out how the factor affect the performance of each regeneration scheme We consider the MSR point where each node stores M/k 1 GB  5  200 MB data The results are shown in Fig 7 The traditional regeneration schemes STAR proposed by Dimakis et al in is implemented as a benchmark W e also test the regeneration time of RCTREE proposed by Li et al in although their algorithm cannot preserv e the MDS property Fig 7\(a illustrates the regeneration time of these schemes We can see that the regeneration time of every schemes decreases when d increases because total repair bandwidth decreases and the total repair traf“c is dispersed to more providers In Fig 7\(b we divide the regeneration time of TR FTR and RCTREE by the regeneration time of STAR to show the relative improvement In general our algorithms reduce 50  70 of regeneration time compared with STAR As d grows the performances become better This is because the star topology has a large chance to include a low capacitated provider-to-newcomer link which can be bypassed by a treestructured regeneration scheme In previous sections we have illustrated that in RCTREE the amount of data transferred through the regeneration tree is not enough for the MDS property and we x this problem by proposing the TR scheme which transmits more data Thus one might intuitively regard the regeneration time of RCTREE as a lower bound of the regeneration time of TR However in Fig 7\(b we observe that TR algorithm performs even better than RCTREE when d is large The reason is that TR can nd a better regeneration tree than RCTREE To show this we also measure the bottleneck bandwidth of the regeneration tree computed by the three algorithms STAR RCTREE and TR The results are presented in Fig 7\(c The bottleneck bandwidth of RCTREE and TR is improved signi“cantly compared with STAR Since STAR always uses the d providers-to-newcomer links but RCTREE and TR can freely select a spanning tree with edges chosen from  d 1 2  links Moreover the bottleneck bandwidth of TR becomes better than RCTREE when d is large The main reason is that RCTREE has a particular restriction on the regeneration tree which requires d  k 1 providers directly connected to newcomer while TR has no such restrictions When d is large the regeneration tree constructed by RCTREE is more like a star-structured topology The performance of FTR is always better than TR in any IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 1884 


8 case Since the process of exibly determining  is done after TR algorithm We also nd that FTR algorithm can save more regeneration time than TR algorithm when d grows B Effect of the bandwidth variance In order to show the impact of network bandwidth variance we run simulations with 5 different link capacity distributions U 1 0  3  120 Mbps U 2 3  120 Mbps U 3 30  120 Mbps U 4 60  120 Mbps U 5 90  120 Mbps The number of providers d is set to 10  Results are shown in Fig 8 In general the performance of our algorithms is better when the variance of network bandwidth is large For uniform distribution U 1 0  3  120 Mbps both TR and FTR can reduce 90 of regeneration time compared with the traditional STAR scheme When the variance of network bandwidth becomes small for example at U 4 60  120 Mbps and U 5 90  120 Mbps TR has the same regeneration time as STAR but FTR still reduces the regeneration time by 10  20   0.3,120]Mbps 3,120]Mbps 30,120]Mbps 60,120]Mbps 90,120]Mbps 0 0.2 0.4 0.6 0.8 1 Regeneration time percentage MSR, n = 20, k = 5, d = 10, file size = 1GB   RCTREE/STAR TR/STAR FTR/STAR Fig 8 Regeneration time of RCTREE TR and FTR normalized by STAR for different bandwidth The parameters are n 20 k 5 d 10 and M 1 GB C Effect of the storage capacity per node  Our previous tests mainly focus on the MSR point which achieves the optimal storage ef“ciency However as shown by Dimakis et al  the repair bandwidth can be reduced by storing more data on each storage node Speci“cally the MBR point achieves the minimum repair bandwidth We vary the storage  from the MSR point to the MBR point to test its impact on our algorithms Fig 9 shows the regeneration time for different   We can see that every regeneration schemes repair faster as  grows to the minimum repair bandwidth point and the tendencies of different repair schemes are the same Compared with STAR we nd that the ratio of reduced regeneration time does not change much for different values of   This implies that our previous simulation results and conclusions for the MSR case also apply to different storage   205 211 221 236 256 0 2 4 6 8 10 12 14 16 18 Storage per node MB Regeneration time\(s n = 20, k = 5,d = 10, file size = 1GB, bandwidth = [10,120]Mbps   STAR RCTREE TR FTR MSR MBR Fig 9 Regeneration time of STAR RCTREE TR and FTR vs storage capacity of each node    where  vary from MSR point to MBR point The parameters are n 20 k 5 d 10 and M 1 GB VI R ELATED W ORK Li et al  rst considered the heterogeneity of netw ork bandwidth in data regeneration process and proposed a treestructured regeneration scheme to reduce the regeneration time They also proposed a scheme of building parallel regeneration trees to further reduce the regeneration time in the network with asymmetric links Ho we v e r  the y only discussed the case that the regeneration scheme requires k providers which means the minimal regeneration traf“c is equal to the size of original le M  To further reduce the regeneration time they considered the regenerating codes in the tree-structured regeneration scheme and proposed RCTREE in The y emplo y a minimum-storage re generating MSR codes in RCTREE which means the minimal regeneration traf“c is d k  d  k 1 M bytes i.e  in a regeneration with d providers each provider sends  d  k 1 blocks to its parent node To make sure that the newcomer has enough information to restore  blocks it has to receive data directly from at least d  k 1 providers The details of how to construct an optimal regeneration tree can be found in Algorithm 1 in their paper  Although the y h a v e done this in their algorithm to ensure that the degree of newcomer is at least d  k 1  the MDS property can not be preserved after data regeneration Sun et al  considered the scenario of repairing multiple data losses and proposed two algorithms based on treestructured regeneration to reduce the regeneration time However they assumed the amount of data transferred between providers and newcomer is designed to be the same as  in regenerating codes According to our analysis their regeneration schemes can not preserve the MDS property either Some researches such as 17 considered the heterogeneity of nodes availability and optimized the erasure code deployment to reduce the data redundancy Some researches such as jointly considered the repair cost and heterogeneity of communication\(download cost on each links They exibly determine the amount of data to minimize the total repair cost which is different from the regeneration time IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 1885 


9 VII C ONCLUSION We reconsider the problem of how to reduce the regeneration time in networks with heterogeneous link capacities We analyze the minimum amount of data to be transmitted on each link of the regeneration tree and prove that the problem of building optimal regeneration tree is NP-complete We propose a heuristic algorithm to construct a near-optimal regeneration tree and further reduce the regeneration time by allowing non-uniform end-to-end repair traf“cs Simulation results show that our regeneration schemes are able to maintain the MDS property and reduce the regeneration time by 10  90 compared with traditional star-structured regenerating codes With the non-uniform end-to-end repair traf“cs we can exibly determine the amount of coded data generated by each provider The proposed Flexible Tree-structured Regeneration scheme performs even better than RCTREE R EFERENCES  J Li S Y ang X W ang and B Li T ree-structured Data Regeneration in Distributed Storage Systems with Regenerating Codes in Proceedings of IEEE Conference on Computer Communications INFOCOM  2010  S Ghema w at H Gobiof f and S.-T  Leung The google le system in ACM SIGOPS Operating Systems Review  vol 37 no 5 ACM 2003 pp 29…43  R Bhagw an K T ati Y C Cheng S Sa v age and G M Voelker Total recall system support for automated availability management in Proceedings of the 1st conference on Symposium on Networked Systems Design and Implementation NSDI  2004  J K ubiato wicz D Bindel Y  Chen S Czerwinski P Eaton D Geels R Gummadi S Rhea H Weatherspoon W Weimer et al  Oceanstore An architecture for global-scale persistent storage ACM Sigplan Notices  vol 35 no 11 pp 190…201 2000  W uala-Secure Cloud Storage Online A v ailable http://www.wuala.com  A G Dimakis P  B Godfre y  M J W  Y  W u  and K Ramchandran Network Coding for Distributed Storage System IEEE Transactions on Information Theory  vol 56 no 9 pp 4539…4551 2010  B Gast  on J Pujol and M Villanueva A realistic distributed storage system the rack model arXiv preprint arXiv:1302.5657  2013  T  Benson A Ak ella and D A Maltz Netw ork traf c characteristics of data centers in the wild in Proceedings of the 10th ACM SIGCOMM conference on Internet measurement  ser IMC 10 ACM 2010 pp 267…280  Google Datacenters Online A v ailable http://www.google.com/about/datacenters/inside/datasecurity/index.html  P  P  C L Y uchong Hu Henry C H Chen and Y  T ang Nccloud applying network coding for the storage repair in a cloud-of-clouds in Proceedings of USSENIX Conference on File and Storage Technologies\(FAST  2012  J Li S Y ang and X W ang Building parallel re generation trees in distributed storage systems with asymmetric links in 2010 6th International Conference on Collaborative Computing Networking Applications and Worksharing CollaborateCom  2010  S.-J Lee P  Sharma S Banerjee S Basu and R F onseca Measuring bandwidth between planetlab nodes in Passive and Active Network Measurement  Springer 2005 pp 292…305  A Duminuco and E Biersack  A practical study of regenerating codes for peer-to-peer backup systems in 29th IEEE International Conference on Distributed Computing Systems\(ICDCS09  IEEE 2009  Prims algorithm Online A v ailable http://en.wikipedia.org/wiki/Primsalgorithm  J Li S Y ang X W ang X Xue and B Li T reestructured data regeneration with network coding in distributed storage systems in Proceedings of 17th International Workshop on Quality of Service\(IWQoS  2009  W  Sun Y  W ang and X Pei T ree-structured parallel regeneration for multiple data losses in distributed storage systems based on erasure codes Communications China  vol 10 no 4 pp 113…125 2013  L P amies-Juarez P  Garcia-Lopez and M SanchezArtigas Heterogeneity-aware erasure codes for peer-topeer storage systems in International Conference on Parallel Processing ICPP  2009  G Xu S Lin G W ang X Liu K Shi and H Zhang Hero Heterogeneity-aware erasure coded redundancy optimal allocation for reliable storage in distributed networks in International Performance Computing and Communications Conference IPCCC  2012  S Akhlaghi A Kiani and M R Ghana v ati Costbandwidth Tradeoff in Distributed Storage Systems Computer Communications  vol 33 no 17 pp 2105 2115 2010  M Gerami M Xiao and M Sk oglund Optimal-cost Repair in Multi-hop Distributed Storage Systems in Proc of IEEE International Symposium on Information Theory ISIT  2011 pp 1437…1441  S Akhlaghi A Kiani and M Ghana v ati  A fundamental trade-off between the download cost and repair bandwidth in distributed storage systems in Proceedings of IEEE International Symposium on Network Coding NetCod  2010  C Armstrong and A V ardy  Distrib uted storage with communication costs in Proceedings of Annual Allerton Conference on Communication Control and Computing Allerton  2011  N Shah K V  Rashmi and P  K umar   A  e xible class of regenerating codes for distributed storage in Proceedings of IEEE International Symposium on Information Theory Proceedings ISIT  2010 IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 1886 


       


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


