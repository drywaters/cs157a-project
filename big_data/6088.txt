Auto-scaling Techniques for Elastic Data Stream Processing Thomas Heinze 1  Valerio Pappalardo 1  Zbigniew Jerzak 1  Christof Fetzer 2 1 SAP AG 2 TU Dresden Systems Engineering Group C h e m n i t z e r S t r  4 8  0 1 0 6 9 D r e s d e n  G e r m 
a n y N o e t h n i t z e r S t r  4 6  0 1 1 8 7 D r e s d e n  G e r m a n y f firstname.lastname g  s a p  c o m c h r i s t o f 
 f e t z e r  t u d r e s d e n  d e Abstract 227An elastic data stream processing system is able to handle changes in workload by dynamically scaling out and scaling in This allows for handling of unexpected load spikes without the need for constant overprovisioning One of the major challenges for an elastic system is to 002nd the right point in time to scale in or to scale out Finding such a point is dif\002cult as it depends on constantly changing workload and system characteristics In this paper we investigate the application of different auto-scaling techniques for solving this problem Speci\002cally 1 we formulate basic requirements for an auto 
scaling technique used in an elastic data stream processing system 2 we use the formulated requirements to select the best auto scaling techniques and 3 we perform evaluation of the selected auto scaling techniques using the real world data Our experiments show that the auto scaling techniques used in existing elastic data stream processing systems are performing worse than the strategies used in our work I I NTRODUCTION Data stream processing systems are designed to process high velocity big data They are used in various scenarios including algorithmic trading or monitoring of k e y performance indicators in the manufacturing domain The processing of information using data streaming systems is achieved by executing a set of standing queries over unbounded streams of data Unlike in batch-oriented systems 4 query results 
in data stream processing systems are returned continuously as soon as they become available To ensure the required low latency and high throughput data streaming systems are typically implemented as distributed systems A classical distributed data stream processing system  uses a 002x ed number of hosts which is chosen to meet the expected maximal workload However due to the fact that peak loads only occur from time to time in average the system is mostly underutilized Ideally the system should automatically acquire new processing nodes or release existing nodes to match the workload Such systems are called elastic Elastic scaling systems are especially bene\002cial for the end user when running within a public cloud environment with a pay per use model Various authors studied the problem of designing elastic data stream processing systems Most of them focused on the underlying problems like an ef\002cient operator state manage 
ment or the coordination of lar ge processing clusters for an elastic streaming system 10 An important problem faced by an elastic streaming system is to decide when the system has to scale in or out Autoscaling techniques deri v e scaling decisions based on an algorithmic analysis of current workload parameters They try to avoid situations of unexpected overload and limit the number of scaling decisions needed to meet a given utilization target Many different auto-scaling techniques ha v e been proposed in the context of cloud databases or application layer scenarios However to the best of our knowledge no other work has yet investigated the usage of different autoscaling techniques within data stream processing systems Data streaming use cases create new challenges for auto-scaling techniques 1 an unpredictable event workload prohibits the usage of 002xed workload models 2 fast changing workload 
requires the auto-scaling technique to respond within seconds 3 due to inhomogenous load changes between hosts the system needs to decide for each host individually Therefore established auto-scaling techniques need to be carefully investigated and adapted in order to be used within an elastic data stream processing system For this paper we have implemented three auto-scaling strategies 1 global thresholds 2 local thresholds as well as 3 reinforcement learning They are based on different established algorithmic techniques used within non-streaming elastic cloud systems The selection of the above techniques is based on the ful\002llment of the requirements presented in Section III The chosen techniques were implemented on top of a state of the art elastic streaming system prototype presented in Section II Using real-world data in the evaluation see Section V we compare to which extent the selected 
techniques can ful\002ll the requirements of elastic data stream processing systems We show that reinforcement learning is the best alternative being robust and adaptive This is a clear contrast to existing related work relying on global or local thresholds II B ACKGROUND The presented auto-scaling techniques are implemented on top of an elastic data streaming processing engine FUGU 10 It consists of a centralized management component dynamically allocating a varying number of hosts FUGU is executed on top of a commercial state of the art distributed streaming engine 
978-1-4799-3481-2/14/$31.00 
002 
2014 IEEE ICDE Workshops 2014 296 


Fig 1 Architecture of FUGU The centralized management component see Figure 1 serves two major purposes 1 it derives placement decisions using an operator placement component and assigns operators to hosts including decisions to allocate new hosts or release existing hosts and 2 it coordinates the construction of the operator network within the distributed CEP engine For the operator placement FUGU constantly monitors all running operators in the system and measures CPU RAM and network consumption for each of them periodically Based on these measurements the used auto-scaling technique decides if a host/system is overor underloaded For overloaded hosts a subset of their operators is selected using a subset sum algorithm and re-assigned to not o v erloaded hosts The assignment of operators itself is done using a bin packing approach where hosts are modeled as bins and operators as items An operator can only be assigned to a host if its CPU load is less than the available capacity on the host As sub-constraints enough network and RAM capacity need to be available on the host The bin packing problem is solved using a First Fit heuristic It 002rst sorts the operators based on the CPU load and assigns them in decreasing order on any host with enough capacity The heuristic is enhanced with a priority for preferring certain hosts in case neighboring operators of the selected operator are placed there The placement is executed in an incremental way All operators marked for movement by the auto-scaling technique are used as input for the bin packing approach the position of the remaining operators is 002xed For each migration of a stateful operator its state needs to be transfered to the new host Therefore the st ate of the moved operator is extracted and mo v ed to the ne w destination of the operator Before starting the operator on the new host the state is replayed During the migration the predecessor operator is paused to avoid the loss of events A more detailed description of the used state migration protocol is given in Each host works autonomously and only knows the centralized manager If different operators of the same query are executed on different hosts each host deploys its operators individually and asks the centralized manager for the location of successor and predecessor operators As a response FUGU provides valid locations of the requested operators and thereby allows the construction of point to point connections between individual operators FUGU also informs the processing nodes about changes of the topology due to operator movements An auto-scaling technique is used as a decision kernel within FUGU  Periodically FUGU triggers the auto-scaling algorithm with up to date system information As input for the auto-scaling technique the current utilization of each host is used The output of the auto-scaling technique can be either scale out no action or scale in III R EQUIREMENTS FOR AN AUTO SCALING TECHNIQUE For selecting possible approaches from the set of available auto-scaling techniques we propose a set of requirements for an auto-scaling technique in an elastic data streaming engine Workload Independence the solution has to be independent from the workload characteristics we make no assumption on the input workload for example we do not want to adjust the solution to a speci\002c workload pattern or stochastic model of the workload Adaptivity the system has to be able to adapt online to changing conditions like different workload characteristic therefore the system should be learning using online feedback Con\002gurability the scaling strategy has to be easy to set up and con\002gure by an end user Computational feasibility the algorithm has to be computationally feasible in a soft real-time environment In contrast to a classical database or a webserver the workload for streaming system changes within seconds State of the art streaming systems 9 10 can scale out and in within seconds to accommodate such changes An auto-scaling strategy has to have a computational complexity small enough to be able to response in such small timescales Existing auto-scaling approaches can be categorized based on the underlying algorithmic techniques into 002ve major groups 1 thres hold-based approaches 2 time series analysis 3 reinforcement learning 4 queuing theory and 5 control theory Predicting workload using a time series analysis based on historical data or workload pattern is not feasible because usually event rates and the data distribution for a data stream processing system change in an unpredictable way Therefore we decided to exclude time series analysis algorithms from the paper A queuing theory approaches are based on a detailed system model and have a limited adaptivity T o 002nd such a model for an elastic streaming system is very complicated due to the unknown workload characteristics For the remaining three classes we selected either existing techniques for data stream processing systems or approaches used in other domains but ful\002lling the mentioned requirements Threshold-based approaches Threshold-based rules derive placement decisions using simple rules Therefore the user needs to de\002ne an upper as well as a lower bound for the system utilization The decisions can be taken very fast independent of the workload and they are con\002gurable by a user However the approach is not adaptive the thresholds 297 


Fig 2 Input/Output of the different auto-scaling techniques are 002xed for the complete runtime of the system We present a threshold-based approach as a baseline because it has been used by existing scale out data stream processing systems Guilsano et al de\002ned an upper threshold for the load variance between all hosts and Fernandez et al de\002ned an upper as well as lower threshold for each individual host Cervino et al pro vided an algorithm which determined the required number of virtual machines based on the current input rate and a maximal capacity of an individual host Reinforcement learning Reinforcement learning describes a set of approaches which decide based on the current state of the system for the action which is expected to yield the biggest improvement of the current system state As a response the data stream processing system provides a feedback describing the result of the used scaling action The feedback allows the algorithm to change its behavior and choose another action next time Control theory In general a controller is a good candidate for elastic data streaming it can be designed to be independent of the workload it responses very fast to a changing input and the system can become adaptive by using a feedback loop This approach could be an alternative to the reinforcement learning presented in this paper A comparison between both approaches is left for future work IV I MPLEMENTED A UTO SCALING TECHNIQUES In the following we describe the selected auto-scaling techniques The goal of the auto-scaling technique in our system is to maximize the system utilization and at the same time to guarantee a low end to end latency Both objectives are con\003icting if the utilization is maximized the end to end latency will increase due to more frequent migration decisions In contrast optimizing for a stable end to end latency will limit the achieved utilization by avoiding overload situations and too frequent migration decisions We have implemented three different variants of autoscaling mechanisms 1 a local threshold-based approach 2 a global threshold-based approach and 3 an adaptive policy using reinforcement learning All three auto-scaling techniques are con\002gured using only three parameters 1 a maximal host utilization 2 a minimal host utilization and 3 a target utilization All other parameters e.g a learning rate for the reinforcement learning are used with pre-de\002ned values if not mentioned differently A Local Threshold-based Approach When using a threshold-based approach as soon as the autoscaling technique is triggered the system utilization is checked against an upper and lower threshold In case the upper threshold is exceeded for a set of n successive measurements the system is marked as overloaded A host is marked as underloaded if the utilization drops below the lower threshold for n consecutive measurements In this situation FUGU tries to 002nd a new host for all currently running operators on the underloaded host In case it is successful the host is released In case not enough free capacity on the remaining hosts exists the release is canceled and no operator movement is done Certain actions are taken to avoid too frequent scaling decisions 1 after each scaling decision the affected host is not touched for a certain period of time called grace period 2 scaling decisions are only done after n consecutive violations of the threshold 3 scaling out decisions the load to move will be chosen larger so as to get done to the target utilization B Global Threshold-based Approach In contrast to the local thresholds the global thresholdbased approach is de\002ned based on the average load of all running hosts In case this average load exceeds an upper limit the system is marked as overloaded and for each individual host an amount of load to move is calculated In case the lower threshold is exceeded the host with the minimal load is released and all its operators are redistributed A global threshold will trigger less frequent scaling decisions This has the advantage of less operator movements but also results in more overload situations for individual hosts see Section V which in turn negatively impact the latency C Reinforcement learning The reinforcement learning approach b uilds up a lookup table for deciding which action to take based on historical feedback A table entry describes for a given utilization value the expected reward value for different actions Possible actions include scale out scale in and no action Based on the current state always the action with the highest reward is chosen The lookup table is updated using an online learning based on the reward received by the system Therefore the so-called Sarsa\(0 algorithm is used which updates the re w ard for the previous state and action Q  s i 000 1  based on an immediate reward r and the expected future reward for the current state Q  s i   Q  s i 000 1   1 000 013  s i 000 1  Q  s i 000 1   013  s i 000 1   r  f 001  Q  s i   where 013  s i 000 1  and f are learning factors describing the in\003uence of the immediate and the future reward respectively The immediate reward is calculated based on the utilization error which describes the difference between measured utilization and target utilization If by an action the utilization 298 


error between Q  s i 000 1  and Q  s i  decreases the immediate reward is larger than 1 Similarly if the error increases the immediate reward is smaller than 1 Within this work we use as learning rate 013  s i  a monotonic decreasing function which decreases with the number of visits of the state 013  s i   imp 001  dur  dur  visits  s i    where visits  s i  describes the number of learning steps the system has done for the current state imp is the impact of the learning and dur shows how long the system is learning The default values used in our system are imp  0  2 and dur  80 see Section V-C The learning of the system is accelerated by a soft monotonicity constraint which requires that the system behaves in a monotonic way If the system decides for the current threshold and latency combination to scale out it should not derive a scale in decision for an even larger utilization and latency value This monotonicity is enforced after each update of the reinforcement table We have extended this basic algorithm to meet the characteristics of an elastic data streaming engine 017 Local Policy within our prototype each host an individual lookup table is maintained which is created on the allocation of the host and deleted on the host release As a consequence for each host an own scaling strategy based on its own load characteristics is learned 017 Initialisation we initialize the lookup table based on a local elasticity policy Below the prede\002ned lower threshold the highest reward is assigned to scale in between the lower and the upper threshold the highest reward for no action and above the upper threshold for scale out 017 Learning algorithm certain wrong decisions of the reinforcement learning can not be enforced by the underlying bin packing e.g in case a host with a load of 0.8 should be released typically not enough resources are available on the remaining hosts to move the operators of this host In this situation the scaling decision are canceled and the system state is not changed However we modi\002ed the reinforcement to receive as immediate feedback on the next learning step a negative penalty to reduce the reward of the taken action 017 Grace period similar to the threshold-based approach we enforce a 002xed grace period The learning for a previous scaling decision is done using the stable utilization after the grace period 002nished The following example demonstrates the implementation of our reinforcement learning approach In Table Ia a part of the lookup table for an individual host after the initialization is shown using a target utilization of 0.6 A row represents the expected rewards for a given utilization value for the three possible actions Assuming FUGU now triggers the reinforcement learning with a current utilization of 0.7 the reinforcement learning decides to take no action NA because this action has the highest expected reward If the next measured utilization  Util ScaleIn NA ScaleOut 0.7 0.4 0.85 0.8 0.8 0.38 0.7 0.85 0.9 0.35 0.5 0.9 1.0 0.32 0.4 1.0  a Before Learning  Util ScaleIn NA ScaleOut 0.7 0.4 0 822 0.8 0.8 0.38 0.7 0.85 0.9 0.35 0.5 0.9 1.0 0.32 0.4 1.0  b After Learning TABLE I Example for reinforcement learning for this host is 0.9 it indicates that the scaling decision increased the utilization error from j 0  6 000 0  7  0  6 j  0  16 to j 0  6 000 0  9  0  6 j  0  5  As result the calculated immediate reward equals to r  1  0  0  16 000 0  5  0  66  The Sarsa learning algorithm updates the entry to 1  0 000 0  2 001 0  85  0  2 001 0  66  0  1 001 0  5  0  822 see Table Ib if it is the 002rst visit of this entry and 013  s 0  7   0  2 as well as f  0  1  As result the difference between the reward for no action and scale out shrinks and after the next learning steps scale out might become the action with the highest reward for a utilization of 0.7 V E VALUATION The goal of the evaluation is to show to which extent the different approaches are able to ful\002ll the requirements presented in Section III 017 Con\002gurability  we compare the performance of all three auto-scaling techniques using different con\002gurations 017 Workload independence  Using the best con\002guration we test the robustness of each approach for three different workloads 017 Adaptability  We show the in\003uence of the learning rate on the reinforcement learning approach In our evaluation we use three real-world data sets from the Frankfurt stock exchange The event rates for each data set change dynamically like presented in Figure 3 Each data set represents the tick load of one working day from the Frankfurt stock exchange The data rate are changing very fast which requires the auto-scaling technique to also derive decision very fast All used auto-scaling techniques have a small enough complexity to achieve this We run each experiment with a 002xed number 35 of continuous queries For evaluating our system we use the following query template  SELECT  FIRST price MIN price AVG price MAX price LAST price FROM tickStream WITHIN x SEC  GROUP  BY comp WHERE sector=y  The above query calculates the average minimal maximal 002rst and last price for each company within a certain sector The query workload is made variable by choosing the window size  x  and the sector  y  randomly FUGU is deployed on top of a private shared cloud using up to 10 processing nodes The system automatically allocates or deallocates hosts as instructed by the selected auto-scaling technique For each run we collect the data for about 700 measurement points Each experiment lasts for at least an hour We measure 299 


Fig 3 Financial workload from Frankfurt stock exchange both the utilization and the end to end latency averaged over all queries running in the system The minimal host load the maximal host load as well as the average host load are computed as average over all 700 measurement points For the end to end latency we present the median 10th percentile as well as the 90th percentile of all measurement A measurement point is calculated as the average query latency of all running queries The 10th percentile and median are indicators for the normal processing latency the 90th percentile shows the impact of overload as well as scaling decisions A Con\002gurability As 002rst experiment we run all three auto-scaling techniques with the same con\002guration We vary the parameters to study their in\003uence on the utilization and latency We use workload Day 1 and compare for each auto-scaling technique 002ve con\002gurations with different lower threshold t  and upper threshold t   017 Default t   0  3 and t   0  8 017 Decreased Lower Threshold t   0  2 and t   0  8 017 Increased Lower Threshold t   0  4 and t   0  8 017 Decreased Upper Threshold t   0  3 and t   0  7 017 Increased Upper Threshold t   0  3 and t   0  9 As target utilization we use 0.6 The results for using local thresholds are presented in Figure 4 For the default con\002guration the system achieves an average utilization of 0.52 and a 90th percentile latency of 1850 ms If the lower threshold is decreased also average utilization and average latency decrease An increased lower threshold shows the opposite behavior due to more frequent operator migrations For both varying upper threshold the latency is increasing signi\002cantly The upper threshold 0.7 is too close to the target utilization and thereby results in many operator migrations In contrast the upper threshold of 0.9 result in many overload situations which also increases the end to end latency The global threshold shows a comparable performance for the default con\002guration with an average utilization of 0.49 and a 90th percentile for latency of 2340 ms see Figure 5 However for all remaining con\002gurations it has a signi\002cantly worse latency compared to the local threshold approach Due to the inhomogeneous load of different hosts overloads of individual hosts can not be detected by a global threshold F i g  4  R e s u l t s f o r d i f f e r e n t c o n 002 g u r a t i o n s f o r L o c a l T h r e s h olds F i g  5  R e s u l t s f o r d i f f e r e n t c o n 002 g u r a t i o n s f o r G l o b a l T h r e s h olds and result in an increased latency For all con\002gurations the average maximal host utilization exceeds 0.7 where else for local thresholds it is always below this value The results for reinforcement learning are shown in Figure 6 The reinforcement learning is able to achieve for all con\002gurations similar utilization and latency values The utilization varies between 0.56 and 0.62 the average latency between 1300 and 2400 ms The achieved average utilization improves by at least 0.05 compared to the local threshold approach with a same or even smaller 90th percentile latency This improvement is substantial given the small number of hosts used and the discrete operator loads Assuming we rented our cloud infrastructure from Amazon EC2 a 5 300 


Fig 6 Results for different con\002gurations for Reinforcement Learning Fig 7 Results for different workload for Local Thresholds improvement in utilization would result in 500 saving per 10 host cluster of m1.medium instances within a period of one year This is caused by the decrease in the average number of used hosts for our scenario by 0.5 hosts In addition the achieved utilization improvement stems solely from the changes in the scaling strategy and not from the changes to the placement algorithm Speci\002cally an improved as compared to the currently used bin packing placement strategy could further increase the achievable system utilization There are two reasons for this good performance of the reinforcement learning 1 the thresholds are adapted based on the online learning and 2 negative penalty values prevent too frequent scale in decisions In addition in order to maximize the learning effect we use a coarse granularity reinforcement table We use a step size of 0.05 which speeds up the learning and due to the resulting rounding disambiguates the scaling decisions B Workload independence We evaluate the robustness of the local thresholds and the reinforcement learning by comparing with the default con\002guration used in the previous section Therefore we run the local thresholds with a lower threshold of 0.3 and an upper threshold of 0.8 for all three workloads Day 1  Day 2 and Day 3 see Figure 7 The achieved utilization as well as latency varies signifFig 8 Results for different workload for Reinforcement Learning icantly for the local threshold based policy Especially for Day 2 the achieved latency is for the same con\002guration nearly twice as high as for Day 1  In contrast the reinforcement learning shows more stable results especially in the achieved latency see Figure 8 The reinforcement learning adapts to the changed workload characteristics and thereby shows better performance values C Adaptivity Finally we tested the adaptivity of the reinforcement learning We in\003uence the learning of the system by varying the impact of the learning imp as well as the duration dur see Section IV-C In total we tested four different con\002gurations 017 No Learning imp  0  0 017 Default Learning imp  0  2 and dur  80 017 More intensive Learning imp  0  5 and dur  80 017 Longer Learning imp  0  2 and dur  160 As con\002guration we use a lower threshold of 0.3 and an upper threshold of 0.8 The results are presented in Figure 9 Our default learning con\002guration improves the utilization of the system compare to an disabled learning from 0.55 to 0.58 The latency decreases from 2100 to 1700 ms Both increasing the learning duration as well as the learning impact results in a worse utilization as well as latency The loss for an increased impact is larger than the loss for an increased learning duration Depending on the workload the learning will either arrive sooner or later in a stable state and show better or worse performance However it has to be noted that all different learning con\002gurations still show better performance than a local threshold approach D Discussion Based on the results of our evaluation we conclude that a global threshold-based solution is not really applicable for data stream processing system A correctly chosen local thresholds shows a good performance but for a wrongly con\002gured the system performance decreases rapidly The reinforcement learning is a good alternative which provides a robust and adaptive solution for the auto-scaling problem However the 301 


F i g  9  R e s u l t s f o r d i f f e r e n t l e a r n i n g c o n 002 g u r a t i o n s f o r R e i n f o r c e m e n t L e a r n i n g parameters in\003uencing the learning need to be carefully chosen to avoid too fast learning VI S UMMARY Ef\002cient scaling in and scaling out is a major challenge of modern elastic data stream processing systems Only using intelligent scaling strategies these systems can ensure stable latency as well as good system utilization at all time despite constantly changing workload In this paper we have formulated four requirements for using auto-scaling techniques within data stream processing systems We have also compared different approaches based on a scenario using real-world data From our evaluation we conclude that global thresholds are not usable for data stream processing systems and that an approach based on reinforcement learning shows the most robust and adaptive performance For the future work we would like to investigate the in\003uence of additional parameters on the auto-scaling technique Especially we would like to study how the end to end latency can be re\003ected more explicitly within our system Two different aspect could be touched 1 how the latency could be incorporated e.g into the reinforcement learning and 2 if the expected latency peak of a scaling decision can be estimated and used to derive better scaling decisions Other important factors could be the queue length of an operator or the current event rate In addition an investigation of controller-based techniques would be of interest Especially adaptive online learning controllers like e.g neuro fuzzy controller seem to be well suited for this task Finally we like to study the in\003uence of the used placement algorithm on the elastic scaling of our system Therefore we want to compare adapted versions of existing load balancing algorithms 19 for data stream processing systems with the used bin packing algorithm R EFERENCES 1  R Genc\270ay M Dacorogna U A Muller O Pictet and R Olsen An introduction to high-frequency 002nance  2001 2  Z Jerzak T Heinze M Fehr D Gr 250 ober R Hartung and N Stojanovic 223The DEBS 2012 Grand Challenge,\224 in DEBS 2012 6th ACM International Conference on Distributed Event-Based Systems  Berlin Germany ACM July 2012 3  J Dean and S Ghemawat 223MapReduce simpli\002ed data processing on large clusters,\224 Communications of the ACM  vol 51 no 1 pp 107\226113 2008 4  A Alexandrov D Battr 264 e S Ewen M Heimel F Hueske O Kao V Markl E Nijkamp and D Warneke 223Massively parallel data analysis with pacts on nephele,\224 PVLDB  vol 3 no 2 pp 1625\2261628 2010 5  D J Abadi Y Ahmad M Balazinska U Cetintemel M Cherniack J H Hwang W Lindner A S Maskey A Rasin E Ryvkina N Tatbul Y Xing and S Zdonik 223The design of the borealis stream processing engine,\224 in CIDR'05  2005 pp 277\226289 6  E Wu Y Diao and S Rizvi 223High-performance complex event processing over streams,\224 in SIGMOD 06 Proceedings of the 2006 ACM SIGMOD international conference on Management of data  New York NY USA ACM 2006 pp 407\226418 7  M Armbrust A Fox R Grif\002th A D Joseph R Katz A Konwinski G Lee D Patterson A Rabkin I Stoica and M Zaharia 223A view of cloud computing,\224 Communications of the ACM  vol 53 no 4 pp 50\22658 April 2010 8  R Castro Fernandez M Migliavacca E Kalyvianaki and P Pietzuch 223Integrating scale out and fault tolerance in stream processing using operator state management,\224 in Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data  2013 pp 725\226736 9  V Gulisano R Jimenez-Peris M Patino-Martinez C Soriente and P Valduriez 223Streamcloud An elastic and scalable data streaming system,\224 IEEE Transactions on Parallel and Distributed Systems  vol 23 no 12 pp 2351\2262365 2012 10  T Heinze Y Ji Y Pan F J Grueneberger Z Jerzak and C Fetzer 223Elastic complex event processing under varying query load,\224 in First International Workshop on Big Dynamic Distributed Data BD3  2013 pp 25\22631 11  T Lorido-Botr 264 an J Miguel-Alonso and J A Lozano 223Auto-scaling techniques for elastic applications in cloud environments,\224 Department of Computer Architecture and Technology University of Basque Country Tech Rep EHU-KAT-IK-09-12  2012 12  S Martello and P Toth Knapsack Problems Algorithms and Computer Implementations  John Wiley  Sons 1990 13  E G Coffman Jr M R Garey and D S Johnson 223Approximation algorithms for bin packing A survey,\224 in Approximation algorithms for NP-hard problems  PWS Publishing Co 1996 pp 46\22693 14  M A Shah J M Hellerstein S Chandrasekaran and M J Franklin 223Flux An adaptive partitioning operator for continuous query systems,\224 in Data Engineering 2003 Proceedings 19th International Conference on  IEEE 2003 pp 25\22636 15  R Das G Tesauro and W E Walsh 223Model-based and model-free approaches to autonomic resource allocation,\224 IBM Ressearch Report RC  vol 23802 2005 16  J Cervino E Kalyvianaki J Salvachua and P Pietzuch 223Adaptive provisioning of stream processing systems in the cloud,\224 in Data Engineering Workshops ICDEW 2012 IEEE 28th International Conference on  IEEE 2012 pp 295\226301 17  P Lama and X Zhou 223Autonomic provisioning with self-adaptive neural fuzzy control for end-to-end delay guarantee,\224 in Modeling Analysis  Simulation of Computer and Telecommunication Systems MASCOTS 2010 IEEE International Symposium on  IEEE 2010 pp 151\226160 18  Y Xing S Zdonik and J.-H Hwang 223Dynamic load distribution in the borealis stream processor,\224 in Data Engineering 2005 ICDE 2005 Proceedings 21st International Conference on  IEEE 2005 pp 791\226 802 19  J Wolf N Bansal K Hildrum S Parekh D Rajan R Wagle K.-L Wu and L Fleischer 223Soda An optimizing scheduler for large-scale stream-based distributed computer systems,\224 Middleware 2008  pp 306\226 325 2008 302 


       


9 VII C ONCLUSION We reconsider the problem of how to reduce the regeneration time in networks with heterogeneous link capacities We analyze the minimum amount of data to be transmitted on each link of the regeneration tree and prove that the problem of building optimal regeneration tree is NP-complete We propose a heuristic algorithm to construct a near-optimal regeneration tree and further reduce the regeneration time by allowing non-uniform end-to-end repair trafìcs Simulation results show that our regeneration schemes are able to maintain the MDS property and reduce the regeneration time by 10  90 compared with traditional star-structured regenerating codes With the non-uniform end-to-end repair trafìcs we can exibly determine the amount of coded data generated by each provider The proposed Flexible Tree-structured Regeneration scheme performs even better than RCTREE R EFERENCES  J Li S Y ang X W ang and B Li T ree-structured Data Regeneration in Distributed Storage Systems with Regenerating Codes in Proceedings of IEEE Conference on Computer Communications INFOCOM  2010  S Ghema w at H Gobiof f and S.-T  Leung The google le system in ACM SIGOPS Operating Systems Review  vol 37 no 5 ACM 2003 pp 29Ö43  R Bhagw an K T ati Y C Cheng S Sa v age and G M Voelker Total recall system support for automated availability management in Proceedings of the 1st conference on Symposium on Networked Systems Design and Implementation NSDI  2004  J K ubiato wicz D Bindel Y  Chen S Czerwinski P Eaton D Geels R Gummadi S Rhea H Weatherspoon W Weimer et al  Oceanstore An architecture for global-scale persistent storage ACM Sigplan Notices  vol 35 no 11 pp 190Ö201 2000  W uala-Secure Cloud Storage Online A v ailable http://www.wuala.com  A G Dimakis P  B Godfre y  M J W  Y  W u  and K Ramchandran Network Coding for Distributed Storage System IEEE Transactions on Information Theory  vol 56 no 9 pp 4539Ö4551 2010  B Gast  on J Pujol and M Villanueva A realistic distributed storage system the rack model arXiv preprint arXiv:1302.5657  2013  T  Benson A Ak ella and D A Maltz Netw ork traf c characteristics of data centers in the wild in Proceedings of the 10th ACM SIGCOMM conference on Internet measurement  ser IMC 10 ACM 2010 pp 267Ö280  Google Datacenters Online A v ailable http://www.google.com/about/datacenters/inside/datasecurity/index.html  P  P  C L Y uchong Hu Henry C H Chen and Y  T ang Nccloud applying network coding for the storage repair in a cloud-of-clouds in Proceedings of USSENIX Conference on File and Storage Technologies\(FAST  2012  J Li S Y ang and X W ang Building parallel re generation trees in distributed storage systems with asymmetric links in 2010 6th International Conference on Collaborative Computing Networking Applications and Worksharing CollaborateCom  2010  S.-J Lee P  Sharma S Banerjee S Basu and R F onseca Measuring bandwidth between planetlab nodes in Passive and Active Network Measurement  Springer 2005 pp 292Ö305  A Duminuco and E Biersack  A practical study of regenerating codes for peer-to-peer backup systems in 29th IEEE International Conference on Distributed Computing Systems\(ICDCSê09  IEEE 2009  Prims algorithm Online A v ailable http://en.wikipedia.org/wiki/Primsalgorithm  J Li S Y ang X W ang X Xue and B Li T reestructured data regeneration with network coding in distributed storage systems in Proceedings of 17th International Workshop on Quality of Service\(IWQoS  2009  W  Sun Y  W ang and X Pei T ree-structured parallel regeneration for multiple data losses in distributed storage systems based on erasure codes Communications China  vol 10 no 4 pp 113Ö125 2013  L P amies-Juarez P  Garcia-Lopez and M SanchezArtigas Heterogeneity-aware erasure codes for peer-topeer storage systems in International Conference on Parallel Processing ICPP  2009  G Xu S Lin G W ang X Liu K Shi and H Zhang Hero Heterogeneity-aware erasure coded redundancy optimal allocation for reliable storage in distributed networks in International Performance Computing and Communications Conference IPCCC  2012  S Akhlaghi A Kiani and M R Ghana v ati Costbandwidth Tradeoff in Distributed Storage Systems Computer Communications  vol 33 no 17 pp 2105 2115 2010  M Gerami M Xiao and M Sk oglund Optimal-cost Repair in Multi-hop Distributed Storage Systems in Proc of IEEE International Symposium on Information Theory ISIT  2011 pp 1437Ö1441  S Akhlaghi A Kiani and M Ghana v ati  A fundamental trade-off between the download cost and repair bandwidth in distributed storage systems in Proceedings of IEEE International Symposium on Network Coding NetCod  2010  C Armstrong and A V ardy  Distrib uted storage with communication costs in Proceedings of Annual Allerton Conference on Communication Control and Computing Allerton  2011  N Shah K V  Rashmi and P  K umar   A  e xible class of regenerating codes for distributed storage in Proceedings of IEEE International Symposium on Information Theory Proceedings ISIT  2010 IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 1886 


       


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


