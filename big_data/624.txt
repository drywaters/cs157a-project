Dynamic Financial Forecasting with Automatically Induced Fuzzy Associations Yazann Romahi and Qiang Shen  yazannr qiangs dai ed ac uk School of Artificial Intelligence Division of Informatics University of Edinburgh 80 South Bridge Edinburgh EH1 lHN UK Abstmct The past decade has witnessed significant growth in developing intelligent tools for financial fore casting Expert Systems were quickly shown to be inad equate for the tasks required in financial forecasting due to their static nature As a result interest started to move towards soft computing despite 
the fact that com prehensibility is often of paramount concern in financial forecasting. Merging.the domains of fuzzy logic and rule induction paved the way for the emergence of successful generalisation techniques with high comprehensibility In this paper we present a novel technique for financial forecasting derived from a fuzzy association induction algorithm allowing the development of an evolving rule based expert system In such a way changing market dynamics are continuously taken into account as time progresses and thus the rulebase does not become out dated Simulations carried out show promising results for this approach I INTRODUCTION A typical market tries 
to establish an equilibrium between the buying and selling forces This dynamic mechanism of trading has inspired traders to want to outperform the market by predicting what the future trend of the market would be The financial time series forecasting that results has posed some very interesting problems for researchers Artificial Neural Networks ANN have drawn a large amount of attention in this area As well as in financial forecasting 6 lo they have been applied to a wide range of problems in the financial domain from options pricing 7 
9 to portfolio management 13 Several disadvantages to ANN approaches are noted including the fact that there is no single configuration that is ad equate for all domains forcing topology determination to proceed by trial and error in a fairly ad hoc man ner Another issue where care must be taken is their susceptibility to over-fitting Most importantly how ever from the perspective of using them as a tool in the trading domain, they lack an important attribute comprehensibility Their user-opaque nature forces us to look 
elsewhere for an approach that could prove po tentially useful in financial forecasting and that can also provide a human-readable explanation of its reasoning Although rule extraction methods from neural net works do exist 3 considerable work remains before they can be considered mature for real applications This leads us to look towards fuzzy rule induction as a potential tool for financial time series forecasting The fuzzification of time series has profound implications for the design of trading models Based on fuzzified in puts and outputs trading rules can be identified 
that embed imprecise ideas Thus changes in market in dicators and relationships between changing indicators can be captured in general terms This paper presents a novel technique for financial forecasting using the fuzzy automatic pattern analysis and classification system FAPACS 4 5 FAPACS infers fuzzy association rules via an approach based on the 223adjusted difference\224 between observed and expec ted frequency counts As a fuzzy logic based algorithm FAPACS allows the use of linguistic terms to represent the revealed regularities and exceptions which makes the rules straightforward for a human-user to under stand This 
is particularly so because the approach used here is concerned with the direct use of predefined fuzzy descriptive sets 8 which may be obtained and augmented by a domain expert We introduce the notion of an evolving ruleset to fuzzy rule induction by training the system on the last n day data \(that is using a moving window of a size n to screen training data in order to make the prediction for day n  1 In such a way the ruleset continuously evolves 
as the oldest data is dropped from the training set while the latest is added to it The rest of this paper is arranged as follows Sec tion I1 provides an overview of the FAPACS approach showing the association induction process and how in ferred associations may be applied Section I11 presents a brief description of the application domain Section IV gives experimental results, demonstrating the poten tial of the present work Finally the paper is concluded in section V 0-7803-5877-5/00/$10.00 0 2000 IEEE 493 


11 FAPACS An association rule in FAPACS associates two attrib ute variables x and y such that Lpq  Ljk where Lpq and Ljk are hguistic terms respectively de scribing the two attributes with their underlying se mantics being represented by fuzzy sets Traditional algorithms decide on whether or not an association is interesting using two user-defined thresholds 2 ll 12 namely the support and the confidence Given two attributes x and y the support is defined as the percentage of records having both at tributes and the confidence is defined as the percent age of records having y given that they also have x Whether an association is deemed significant depends on whether both support and confidence are greater than the predefined thresholds This contains an inher ent weakness as it is difficult to ascertain where these thresholds should be FAPACS uses adjusted difference analysis 4 to identify associations amongst attributes As a result it does not require any user-defined thresholds and has a further advantage in that it is possible to infer both positive and negative associations Here a positive as sociation means that an attribute having a certain char acteristic defined by L implies that another attribute will have the characteristic Ljk In other words the existence of an antecedent implies the existence of a consequent. Similarly a negative result means that the presence of the former implies the absence of the latter A Mining Interesting Associations The process of association induction begins with the calculation of the sum of degrees to which records in a given training dataset are characterised by the lin guistic terms Each possible combination between a pair of attributes is considered in turn Given a record d E D where D refers to the dataset and linguistic terms Lpq,Ljk E L,p  j where L is the collection of linguistic terms which are represented by fuzzy sets e.g L and Ljk with membership functions p~~~\(u and phjk U the degree of association is accumulated across all records and is used to infer significance This is done via algorithm 1 as given in 4 where R stands for the collection of generated associations Note that the T-norm which reflects the degree of association between two membership functions is inter preted using the min operator However as for the S-norm across all the datasets, rather than using the mal the addition operator is adopted This is because it lends itself well for statistical analysis on which the definition of the interestingness function is based The interestingness is defined using adjusted differ ence 41 where Algorithm 1 Generation of Interesting Associations for all Lpq,Ljk E L,p  j do end for degLpqL jk   min\(pLpq 7 pL jk 1 fordlL,,,LjkEL,p#jdo if interesting\(L Ljk then end if R rulegen\(L Ljk end for LpqC j k dLpqLjk   d=GG with z~~~e being the standardised difference given by degLpqL j k  eLpqL j k LpqLjk  dcz where eLpqLjk is the sum of degrees to which records are expected to be characterised by L and Ljk and is calculated by and L~~c is the maximum likelihood of the variance of given by According to 4 if ldL,,LjiI  1.96 i.e the 95th percentile of the normal distribution the association is deemed significant or interesting A positive value of d signals a positive association while a negative value for d means that the existence of the antecedent implies the absence of the consequent If an association is deemed interesting the rvlegen function then generates a fuzzy association rule such that it is in the form l  Ljk[WCpqLjk where wLPqLjk denotes the weight of evidence and means that the existence of the antecedent implies the existence of the consequent to the degree of this weight The weight of evidence is defined by Thus the weight is determined by an estimate of the gain in information when a record is characterised by a particular fuzzy set of a given attribute L vs all fuzzy sets of the same attribute except L 494 


B Inference Process Using the inferred associations FAPACS can predict or classify previously unseen data Unlike most clas sification techniques which categorise records into dis tinct classes FAPACS generates a certainty measure for each result of rule-firing Consider a record which is characterised by values of n attributes al a2  a Without losing gener ality suppose that the value of a E 1,2  n is to be predicted and that the domain of ap is defined as the following set of linguistic terms 13  L,1 Lp2  Lps with their underlying fuzzy sets be The value of ap should of course be assigned ac cording to its underlying domain To predict this value FAPACS searches the association rules with L as con sequents If a certain attribute is characterised by a linguistic term in the antecedent of a rule which im plies Lpq it can be considered as providing evidence whether positive or negative for the value of a be ing assigned to Lpq This is repeated and the total evidence measure is calculated for each potential value of ap The linguistic term with the highest total evidence is then assigned to a More formally the method is presented in al gorithm 2 Algorithm 2 FAPACS Inference for all aj j  1,2  n do ing  7 E L2  5 for all Ljk  L do for all Ljk E Lj do end for wq  wLpqLj,,.pLjk\(aj end for end for Lp\(maz\(wq 111 FINANCIAL TIME SERIES Financial time series are difficult to analyse This is because they are typically non-stationary and thus require open or modifiable definitions of memberships and universes of discourse However there exists a large body of literature concerned with the conscious and de liberate study of market price history with a view to predicting the future price change and hence enhance trading profitability Technical Analysis involves in es sence a collection of indicators that indicate to a trader a change in the market trend The method adopted herein for preprocessing the data borrows heavily from this area as the raw data is preprocessed into technical indicators to be fed into the rule induction algorithm The indicators used in the experiments are outlined be low with each using a set of predefined descriptive fuzzy sets Price Change The first and obviously useful attrib ute considered for financial forecasting is the percentage price change over the day Comparative Moving Average This attribute reflects differences between shorter term moving averages and longer term ones Volatility This attribute gives a meawre of the intens ity of market activity high volatility is typically an indication of an impending major move in the market Volume Moving Average This is important because when the current volume rises above a long term volume moving average it is potentially a signal that there is increased market activity that will be affecting the price Price Channel Breakout This is the simplest attribute indicating that a long trade is placed when the market closes above a previous high and vice versa The Stochastic This attribute aims to identify trends based on the principle that as market price rises the market closes near the market high for the past few periods and vice versa Relative Strength Index RSI This attribute helps re veal the case of overbought/oversold which is very use ful for predicting price reversal points For all the above indicators the data fed merely provide their values The decision concerning what ac tion should be taken is left to the rule-firing process of the FAPACS to infer Real inputs are fuzzified As an example the fuzzy partitioning for the attribute RSI is given in figure 1 Concerning the training data however the classifica tion of each record is done by looking "into the future If the index price of day k 1 was 4 above the price on day k, this was considered a StrongBuy over 2% was considered a Buy Similarly if the index price of day k  1 was 4 less than the price on day k a StrongSell would be considered, less than 2 a Sell and otherwise Hold IV EXPERIMENTAL RESULTS A large number of experimental simulations were car ried out using real historical data typical results are presented below A Explanation Generation As mentioned previously one of the main attributes of this approach lay in its superior comprehensibility over previous approaches An example run by the sys tem which implements the associations induced resul ted in the following explanation Today's Volume-MovingAve 10:Day is This implies that Conclusion is "Buy definitely 1 Higher with certainty 0.567332 495 


Relative Strength Index Indicator RSI Underbought Market  Slightly Sli htly Underbought Over%ought Overbought Market 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Neutral Market Fig 1 Fuzzification for Relative Strength Index RSI Today's 20:day-Channel-Breakout:UpperBound is This implies that Conclusion is Buy slightly 0.425373 Def inite-Breakout  with certainty 0.488933 Today's 14:Day-RSI is definitely 1 Underbought-market  This implies that Conclusion is StrongBuy with certainty 0.479263 Considering the above my overall decision for today is Buy A closer examination of these results in fact indic ates that the rules inferred by FAPACS are in line with the general ideas of technical analysis This is very en couraging, given that the system was only lead in the correct direction by allowing the expert to define the fuzzification procedures based on technical analysis but the expert did not provide any of the rules themselves B Trading on the Equity Financial Markets The present application of FAPACS is to train a fore casting system using subsets of data over a certain win dow size say n days long Assuming that today is given by k the automated trading system is trained from k  n to k in order to predict what will happen on day k  1 The window then moves one step and FAPACS is retrained The trained system will give one of 5 signals upon classification of a newly presented record StrongBuy Buy Hold Sell and StrongSell A StrongBuy signal would suggest an action to buy the stock with half the cash currently held while Buy would cause one quarter of cash held to be used Similarly, StrongSell and Sell would cause an action to liquidate half and a quarter respectively of one's portfolio Using this approach the experiment was run during various market eras The era between 1973-1988 was first considered This era was rather volatile due to the existence of two crashes within that time period the oil crisis 1973 and Black Monday 1987 The results obtained from this run are shown in table I Window Size 50 100 150 200 250 300 Tracker Buv/Holdl Total Profit 2,739,653 1,948,397 1,865,373 1,869,520 1,680,440 1,553,327 2.951.363  Change t 173.4 t 94.8 t 86.5 f 86.9 f 68.0 t 295.1 t 55.3 TABLE I 1974  12 FEBRUARY 1988 TRADING THE DOW JONES INDUSTRIAL AVERAGE 7 NOVEMBER As is evident from the results the lower the mov ing window size the better the results Figure 2a demonstrates the system's performance during this time period with a window size of 50 and one of 250 It is clear that the use of the 50 day window offers quite a successful run in that throughout the time period it leads the index tracker Missing the last market run however caused it to fall under the index tracker At first it may feel counter intuitive that shorter window sizes performed better One would assume that longer window sizes should be desirable as it makes it possible to differentiate between noise and true system response Thus, the accuracy of the system should im prove as the training window is increased However this is only true if the current state of the system re mains constant During state changes or in the case of volatile mar kets as we saw above long training windows will result in slow system response Since much of the opportun ity for outperforming the market occurs early in the state change the overall effectiveness of a system with a long training window will be low The results obtained showed that smaller window sizes are more desirable during turbulent streaks in the market as the associ ation induction algorithm is allowed to react quickly to market changes 496 


 Y Fig 2 FAPACS based strategies vs an Index Tracker Buy/Hold strategy a 1973-1988 top and b 1928-1949 bottom If we then consider figure 2b which covers data from 1928-1949 we notice that this time the FAPACS de rived rules performed significantly better than the mar ket average The results obtained for this era are shown in table 11 Although in all cases FAPACS outper formed the market the best performer was indeed due to the use of again the 50 training day window This reflects an important feature of the present work in that it may suit tasks that require real-time prediction be cause smaller windows involve less computation while producing better results Attention is then moved to consider an era known for its fairly stable bull market The graph is shown in figur 3 In this case the performance of the system unfortunately did not do as well as was hoped How ever, considering the graph it can be seen that during the market dips FAPACS managed to have caused ac tions to sell the holdings in time not to be affected by market downturns Nevertheless it was unable to give advice to buy quickly enough to make full use of mar ket upturns The data for this run is shown in table Window Size 50 I 1,820,402 1 t 82 I Total Profit I  Change TABLE I1 TRADING THE DOW JONES INDUSTRIAL AVERAGE 25 OCTOBER 1928  17 JANUARY 1949 111 As is expected we notice this time that except for 50 the performance improved with increasing window sizes for the reasons outlined earlier Fig 3 Trading the Dow Jones Industrial Average: 1950-1973 Window Size I Total Profit 50 I 1,734,906 100 150 200 250 300 Tracker Buy/Hold 2,336,453 2,537,919 2,605,027 2,660,607 2,325,245 4,353,453  Change t 133.6 t 153.8 t 160.5 t 166 f 132.5 t 335.3 f 73.4 TABLE 111 TRADING THE DOW JONES INDUSTRIAL AVERAGE 8 FEBRUARY 1950  30 AUGUST 1973 497 


V CONCLUSION This paper has successfully demonstrated the poten tial for the use of fuzzy association induction in fin ancial forecasting The approach adopted using the fixed size training windows sliding through time showed some promise Continued and automatic adaptation to changes in market conditions is as a result implicitly taken into account by the fact that as time progresses the newest data is added to the training set while the oldest is discarded This thus marks a departure from the traditional static Expert System approach and yet managing to solve one of the biggest problems of typ ical soft computing approaches in financial forecasting e.g using a neural network comprehensibility The preliminary results developed here open the door for the analysis and consideration of a whole host of fuzzy rule induction based algorithms against financial forecasting benchmarks Other extensions may involve using different pre-processing techniques For example it would be interesting to use clustering methods on the data without human intervention in order to compare the performance of automatically generated fuzzy sets against that of descriptive sets Of course to derive the most out of the potential for explanation generation descriptive fuzzy sets are desirable ACKNOWLEDGEMENTS This work was partly supported by a UK-EPSRC grant The authors are grateful to Ilias Boussios Alex ios Chouchoulas Bavo Christiaens Chris Jones Jer oen Keppens, Javier Gomez Marin-Blazquez and Doris Young for useful discussions REFERENCES M Adya and F Collopy 223How Effective are Neural Networks at Forecasting and Prediction A Re view and Evaluation\224 Journal of Forecasting R Agrawal T Imierlinkski and A Swami 223Mining Association Rules Between Sets of Items in Large Databases\224 Proceedings of 1993 ACM SIGMOD International Conference on Management of Data 1993 R Andrews J Diedrich and A Tickle 223A Survey and Critique of Techniques for Extracting Rules from Trained Neural Networks\224 Technical Report NeuroComputing Research Centre 1995 W Au K Chan 223An Effective Algorithm for Dis covering Fuzzy Rules in Relational Databases\224 Proceedings of the 7th IEEE International Confer ence on Fuzzy Systems pp.1314-1319 1998 17\(5/6 1998 K Chan and A Wong. \223APACS A System for Auto matic Analysis and Classification of Conceptual Patterns\224 Computational Intelligence 6:119-131 1990 B Flower T Cripps M Jabri and A White 223An Artificial Neural Network Based Trade Forecasting System for Capital Markets\224 Proceedings of the 4th International Conference on Neural Networks in the Capital Markets 1995 R Gencay and R Garcia 223Pricing and Hedging De rivative Securities with Neural Networks and a Ho mogeneity Hint\224 Proceedings of the 5th Interna tional Conference on Neural Networks in the Cap ital Markets 1997 J Gomez-Marin-Blazquez Q Shen and A Gomez Skarmeta 223From Approximative to Descriptive Models\224 Proceedings of the 9th IEEE International Conference on Fuzzy Systems 2000 M Qi and G Maddala 223Option Pricing Using ANNs The Case of S&P 500 Index Call Options\224 Proceedings of the 4th International Conference on Neural Networks an the Capital Markets 1995 lo N Roehl and C Pedreira 223Online Learning for Multi-Layered Neural Network Applications to Time Series Prediction\224 Proceedings of the 4 th In ternational Conference on Neural Networks in the Capital Markets 1995 ll R Srikant and R Agrawal 223Mining Generalised Association Rules\224 Proceedings of the 21st VLDB Conference 1995 12 R Srikant and R Agrawal 223Mining Quantitat ive Association Rules in Large Relational Tables\224 Proceedings of 1996 ACM SIGMOD International Conference in Management of Data 1-12 1996 13 H Zimmermann and R Neuneier 223Active Portfolio Management with Neural Networks\224 Proceedings of the 6th International Conference on Computa tional Finance 1999 498 


          0 2 4 6 8 10 12 14 16 18 Minimum Support \(in T5.I2.D100K.data ApriHash ApriPref MINER\(I MINER\(II Time\(sec 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05          0 20 40 60 80 100 120 140 Minimum Support \(in T10.I4.D100K.data 0.25 0.15 0.45 0.40 0.35 0.20 0.30 0.60 0.50 0.55 Time\(sec ApriHash ApriPref MINER\(I MINER\(II          0 50 100 150 200 250 Minimum Support \(in T10.I6.D100K.data ApriHash ApriPref MINER\(I MINER\(II Time\(sec 0.60 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.55          0 50 100 150 200 250 300 350 400 450 500 Minimum Support \(in T20.I6.D100K.data ApriHash ApriPref MINER\(I MINER\(II 0.95 1.00 0.90 0.85 0.75 0.65 0.55 0.60 0.70 0.80 Time\(sec Figure 2 P erformance comparison Algorithm P AR INTER MINER  A p ar al lelization of INTRA MINER for i 1 to n at pro cessor P i do in parallel C inter  f c j c 022 cl iq  cl iq 2 Q  j c j\025 3 c is in ter-CPI all in tra-CPI subsets of c are in L intr a g  Let T i b e an empt y pre\014x-tree and for eac h cl iq 2 Q do instree T  c  Coun tSup T i  D i  and send T i to all other pro cessors Receiv e T j s from all other pro cessors P j s  Synchr onization p oint for j 2 to n do AccumSup T 1  T j   T 1 030 T n ar e in the same shap e Collect all frequen tin ter-CPI k itemsets for k 025 3to L inter b y a scan of T 1  L inter  L inter f x 2 L 2 j x is in ter-CPI g  Both algorithms ab o v e emplo y the same idea First eac h pro cessor P i for all i 2 f 1 030 n g indep enden tly inserts all in tra-CPI or in ter-CPI candidates in to a local pre\014x-tree T i  Next P i uses lo cal transaction data D i to obtain the lo cal supp orts for all candidates and sends the result tree T i to all other pro cessors Then it w aits for receiving all T j s from all other pro cessors P j s After all T 1 030 T n are a v ailable P i do es supp ort accum ulation for all candidates and store the global supp orts in to T 1  Finally  L intr a or L inter can b e generated easily at eac h P i  It is not hard to see that the ab o v e pro cedure is complete Th us w e can presen t the parallelization of MINER\(X as follo ws Algorithm P AR MINER\(X  X 2f I II g for i 1 to n at pro cessor P i do in parallel Coun t lo cal supp orts for all 1itemsets and 2-itemsets send the results to all other pro cessors receiv e lo cal supp orts from all other pro cessors accum ulate all lo cal supp orts to get global supp orts and then generate L 1 and L 2  GEN CLQ  Get Q  S maxcliqsize i 3 Q i if maxcliqsize 024 maxcansize then for i 1 to n at pro cessor P i do in parallel C all  f c j c 022 cl iq 2 Q j c j\025 3 g  Let T i b e an empt y pre\014x-tree and for eac h c 2 C all do instree T i  c  Coun tSup T i  D i  and send T i to all other pro cessors Receiv e T j s from all other pro cessors P j s  Synchr onization p oint for j 2 to n do AccumSup T 1  T j   T 1 030 T n ar e in the same shap e Collect all frequen t k itemsets for k 025 3in to L b y a scan of T i  L  L  L 1  L 2  else for i 1 to n at pro cessor P i do in parallel GEN CPI\(X  Get a CPI P AR INTRA MINER  L intr a gener ation at P 1 030 P n P AR INTER MINER  L inter gener ation at P i 030 P n for i 1 to n at pro cessor P i do in parallel L  L intr a  L inter  


It is easy to see that P AR MINER\(X is a natural parallelization of MINER\(X on a shared-nothing arc hitecture b y using our main idea men tioned b efore where X ma y be I or II This parallelization uses a simple principle of allo wing redundan t computations in parallel on otherwise idle pro cessors to a v oid comm unication whic h is also emplo y ed b y Count Distribution algorithm 3 Note that Count Distribution is the fastest parallelization of Apriori  Ho w ev er Count Distribution con tains uncertainly m ultiple sync hronization p oin ts due to the sum-reduction at the end of eac h pass to construct the global coun ts This fact greatly limits the dev elopmen t of parallelism Our P AR MINER\(X has o v ercome this dra wbac k b ecause it con tains at most 3 sync hronization p oin ts F urthermore di\013eren t from the metho ds in 11  all of our database partitions are non-o v erlapping whic h guaran tees there is no redundan t supp ort-coun ting op eration in our metho d Hence our metho d dev elops the parallelism more su\016cien tly than b oth the ab o v e existing metho ds 6 Conclusions W eha v e prop osed new algorithms for e\016cien t mining of asso ciation rules Di\013eren t from all existing algorithms w ein tro duce a concept of CPI Complete P artition of Items and divide all itemsets in to t w ot yp es in tra-CPI and in ter-CPI After obtaining all frequen t 1-itemsets and 2-itemsets w e generate and main tain a set Q of item cliques maximal p oten tially frequen t itemsets F urthermore w eha v e designed t w o metho ds for generating an e\013ectiv e CPI b y using Q  Then w e can use Q and our CPI to get a set C intr a of in traCPI candidates and coun t supp orts for them so that all frequen t in tra-CPI itemsets can be obtained Finally  w e use Q  our CPI and all frequen t in tra-CPI itemsets to generate a set C inter of in ter-CPI candidates and also coun t supp orts for them so that all frequen tin ter-CPI itemsets can b e obtained Our algorithms ha v e sev eral adv an tages First their I/O costs are quite limited b ecause they only require at most 3 scans o v er database Second they can mak e b oth sizes of C intr a and C inter reasonable so that their computation costs are also e\013ectiv ely con trolled Third they use a pre\014x-tree structure to store candidates whic h can also reduce computation cost As a result they app ear to be more e\016cien t than Apriori  one of the b est algorithms for asso ciation disco v ery  T o con\014rm that W e ha v e done the exp erimen ts to compare the p erformances of our algorithms together with Apriori  The test results sho w that our algorithms outp erform Apriori consisten tly  b y factors ranging from 2 to 4 in most cases Another adv an tage of our algorithms is that they are easy to be parallelized W e ha v e also presen ted a p ossible parallelization for our algorithms based on a shared-nothing arc hitecture W e observ e that the parallelism can b e dev elop ed more su\016cien tly in our parallelization than t w o of the b est existing parallel algorithms References 1 R Agra w al H Mannila R Srik an t H T oiv onen and A I V erk amo F ast Disco v ery of Association Rules A dvanc es in Know le dge Disc overy and Data Mining  Chapter 12 AAAI/MIT Press 1996 2 R Agra w al T Imielinski A Sw ami Mining Asso ciations bet w een Sets of Items in Massiv e Databases Pr o c of the A CM SIGMOD Int'l Confer enc e on Management of Data W ashington D.C Ma y 1993 207-216 3 R Agra w al J.C Shafer P arallel Mining of Association Rules IEEE T r ansactions on Know le dge and Data Engine ering  V ol 8 No 6 Decem ber 1996 4 D.W Cheung and Y Xiao E\013ect of Data Sk ewness in P arallel Mining of Asso ciation Rules Pr o c The Se c ond Paci\014c-Asia Conferenc e on Know le dge Disc overy and Data Mining P AKDD-98  Melb ourne Australia April 1998 48-60 5 Dao-I Lin and Zvi M Kedem Pincer-Searc h A New Algorithm for Disco v ering the Maxim um F requen t Set EDBT'98  Marc h 1998 6 Heikki Mannila Hann uT oiv onen and A Ink eri V erk amo E\016cien t algorithms for disco v ering asso ciation rules AAAI Workshop on Know le dge Disc overy in Datab ases KDD-94  pages 181 192 July 1994 7 G D Mulligan and D G Corneil Corrections to Bierstone's Algorithm for Generating Cliques J Asso ciation of Computing Machinery  19\(2 247 Apr 1972 8 Jong So o P ark Ming-Sy an Chen and Philip S Y u An E\013ectiv e Hash-Based Algorithm for Mining Asso ciation Rules Pr o c 1995 A CMSIGMOD Int Conf Management of Data  San Jose CA Ma y 1995 9 Jong So o P ark Ming-Sy an Chen and Philip S Y u E\016cien tP arallel Data Mining for Asso ciation Rules A CM CIKM 95  Baltimore MD USA 10 Ashok Sa v asere Edw ard Omiecinski Shamk an t Na v athe An E\016cien t Algorithm for Mining Asso ciation Rules in Large Databases Pr o c e e dings of the 21st VLDB Confer enc e  Zuric h Switzerland 1995 11 Mohammed Ja v eed Zaki Sriniv asan P arthasarath y  Mitsunori Ogihara W ei Li New P arallel Algorithms for F ast Disco v ery of Asso ciation Rules Data Mining and Know le dge Disc overy Sp e cial Issue on Sc alable High-Performanc e Computing for KDD  pp 343373 V ol 1 No 4 Decem b er 1997 12 Mohammed Ja v eed Zaki Sriniv asan P arthasarath y  Mitsunori Ogihara W ei Li New Algorithms for F ast Disco v ery of Asso ciation Rules T e chnic al R ep ort UR CS TR 651  Univ ersit yof Roc hester 1997 


                 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


