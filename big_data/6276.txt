The Fast Itemset Miner A Detailed Analysis of the Candidate Generation Stages Alexandru Archip and Mitic 013 a Craus Abstract 227 Association Rule Mining 050ARM\051 represents an important tool for Data Mining techniques A key step in ARM is to determine the frequent itemsets present within the analyzed data Recent algorithms addressing this problem focus on identifying frequent itemsets without candidate generation steps This paper details a new algorithm 226 Fast Itemset Miner 050FIM for short\051 226 that still relies on candidate generation stages but has a different approach from the standard Apriori algorithm This approach focuses on increasing size of the interval that candidates belong to rather than increasing the size of the candidates by one with each corresponding iteration By comparing the Apriori and FIM candidate generation stages we show that this second approach uses a faster and more ef\002cient method of determining candidate itemsets Moreover such an approach also favors a better support counting method which greatly impacts on the overall time response of the FIM algorithm Index Terms 227 Data Mining Association Rule Discovery Frequent Itemset Mining Candidate Based Algorithms I I NTRODUCTION Association Rule Mining 050ARM\051 refers the problem of identifying all relations between the elements of a given data set so that the presence of a given element/set of elements implies the presence of another element/set of elements 2 3 4 This process usually in v olv es tw o distinct stages the 002rst one involves determining all frequent itemsets present within the collection of transactions while the second stage aims to 002nd strong association rules based on the previously identi\002ed itemsets 5 The 002rst stage is generally considered to be the most important one with respect to the overall ef\002ciency of the ARM algorithms The 002rst notable algorithm to ef\002ciently solve the issue of frequent itemset mining is the Apriori algorithm 0501994\051  1 5 The algorithm performs multiple scans o v er the target transaction database as follows The 002rst such scan determines the frequent items with respect to the minimum support limit given by the user All subsequent scans involve two stages the 002rst one 226 the so called join stage 226 is to identify all possible k itemset candidates based on the frequent 050 k 000 1 051 itemsets found in the corresponding previous scan the support for each candidate is then computed in the second stage and all candidates that do not meet the minimum support value are removed 226 this stage is known as the pruning stage  The key aspect of this algorithm is A Archip is with The Faculty of Automatic Control and Computer Engineering The 223Gheorghe Asachi\223 Technical University of Ias\270i alexandru.archip@cs.tuiasi.ro M Craus is with The Faculty of Automatic Control and Computer Engineering The 223Gheorghe Asachi\223 Technical University of Ias\270i craus@tuiasi.ro the ef\002cient generation of candidates An optimal approach must consider the Apriori Property  If an itemset is frequent then all of its subsets must also be frequent  Should this restriction be met Agrawal et al state that the Apriori algorithm achieves a linear time complexity with respect to the size of the transaction database Further analyses performed using the Apriori algorithm have shown that its main advantage of counting the support value only for the determined candidates could also prove to be its greatest weakness The amount of candidate itemsets that is determined in the join step could be quite large and this could negatively in\003uence the overall performance  4 Moreo v er  in such cases methods that of fer an ef\002cient candidate selection may consume considerable more resources 5 Taking this into consideration ulterior attempts to develop better and faster algorithms follow two different approaches The 002rst one is to completely eliminate the candidate itemsets One such remarkable algorithm is FP-Growth 050\002rst presented in and re vised in 8]\051 The algorithm b uilds pre\002x trees from the target transaction database by performing exactly two scans of the database one to 002nd the frequent items and one to actually build the pre\002x tree In order to determine all frequent itemsets the pre\002x tree is traversed in a bottom-up manner 8 A much more recent algorithm 226 called And Code 050AC for short\051 226 is presented in The main idea behind AC is to binary code itemsets and transactions The second line of approaches to the itemset mining problem has been to improve the standard candidate generation techniques for the Apriori algorithm Attempts have been made to reduce the overall number of I/O operations involved  or to use dif ferent data structures in order to represent the itemsets and the candidates It can be noticed that almost no effort had been directed to investigating other means of generating candidate itemsets The present paper details a new method for identifying valid candidates for a relatively new algorithm 226 Fast Itemset Miner 050FIM for short\051 2 In our pre vious w ork 050papers  and 3]\051 little or no detail is gi v en for the candidate generation stages used in FIM The main goal of this paper is to show how simple and ef\002cient is this new method of determining valid candidates Section II describes the base sequential algorithm and the simple candidate generation stages for this algorithm This section also includes a proof of correctness for this particular method of identifying candidate itemsets Section III includes a presentation of the generalized form for both the algorithm and the candidate 


generation and a few details regarding the parallel version of the algorithm Section IV will present a 002rst set of comparative time results between FIM and Apriori while the last section of the paper is dedicated to some 002nal remarks and conclusions as well as a few details on how this new algorithm could evolve II T HE B ASE FIM A LGORITHM AND C ANDIDATE G ENERATION M ETHOD The fundamental idea behind FIM is to determine new frequent itemsets by gradually increasing the size of the interval that includes the items contained within an itemset  Let n be the total number of frequent items The algorithm begins with n intervals each such interval including a single frequent item At a given stage k  new candidates will be generated from items belonging to the interval  i  i  k  050having i 2 f 1  2   n 000 k g 051 while for the subsequent stage k  1 new candidates will be determined by combining items belonging to  i  i  k  1  050having i 2 f 1  2   n 000 k 000 1 g 051 The following notations will be used 2 F i  j 000 the set of all frequent itemsets that include items belonging to interval  i  j  C i  j 000 the set of all candidates that include items belonging to interval  i  j  0501\051 In it is sho wn that ne w frequent itemsets in F i  j contain both items i and j  As a direct consequence the new candidates for interval  i  j  must contain both items i and j  Therefore new candidates will be generated according to equation 0502\051 3 C i  j  f X  Y j 050 X 2 F i  j 000 1  i 2 X 051  050 Y 2 F i  1  j  j 2 X 051 g 0502\051 The base FIM algorithm is given in Algorithm 1  3 Algorithm 1 works as follows A preprocessing stage is required to determine all frequent items trim transactions by discarding non-frequent items and sort the new transaction dataset descending with respect to an items support threshold Assuming n frequent items in the 002rst stage of the algorithm new candidates will be generated based on the following subsets f 1  2 g  f 2  3 g   f n 000 1  n g  For the next stage candidates will be determined from items belonging to f 1  2  3 g  f 2  3  4 g   f n 000 2  n 000 1  n g  This same procedure continues until 002nally new candidate itemsets will include items from the interval f 1  2   n g  One of the key advantages of this approach is that the FIM algorithm does not need to scan the whole transaction database in order to determine the support for any given valid candidate In order to determine the support for all candidates included in C i  j  the algorithm must scan only the transactions that include item i  Since the FIM algorithm is a candidate based approach to the frequent itemset mining problem a key issue that needs to be solved is to ef\002ciently determine the new candidates for a given stage 050line 4 in Algorithm 1\051 In order to better  Algorithm 1 Fast Itemset Miner Base Algorithm  Require F i  j  set of frequent itemsets belonging to i Require D  transaction set t 032 D 1 for k  1 to n 000 1 do 2 for all i  j  i 2 1   n 000 k  j  i  k do 3 F i  j 040 F i  j 000 1  F i  1  j 4 C i  j  f X  Y j 050 X 2 F i  j 000 1  i 2 X 051  050 Y 2 F i  1  j  j 2 X 051 g 5 for all transactions t 032 D i do 6 f D i denotes the subset of transactions in D that include frequent item i g 7 C t 040 subset 050 C i  j  t 051 f determine all candidates in C i  j that are include in transaction t g 8 for all candidates c 2 C t do 9 c.support 10 end for 11 end for 12 F i  j  F i  j  f c 2 C i  j j c  support 025 minsupp g 13 F k 040 F k  F i  j 14 end for 15 end for  understand the solution we submit for this step of the base FIM algorithm the following notations will be used for an itemset X 2 F i  j  the maximal itemset N such that N 032 X and N 134 f i g  N 134 f j g  f 226 the core itemset of X 0503\051 su f f ix 050 X 051  X n f i g 226 the suf\002x itemset of X pre f ix 050 X 051  X n f j g 226 the pre\002x itemset of X  0504\051 Also our solution relies on the following lemma which is a direct consequence of the Apriori Property  Lemma 1 Any given itemset X is not frequent if its core itemset 050equation 0503\051\051 is not frequent Using Lemma 1 and considering line 4 in Algorithm 1 we can say that a candidate itemset is valid if its core is a frequent set Moreover each subset of a valid candidate T 2 C i  j that does not simultaneously contain items i and j must be a frequent itemset The new method we use to identify valid candidates is based on the following theoretical result Lemma 2 Let T be a candidate itemset in C i  j  T is a valid candidate if and only if it is obtained through joining two frequent itemsets X 2 F i  j 000 1 and Y 2 F i  1  j that simultaneously meet the following conditions i 2 X s\270i j 2 Y 0505\051 su f f ix 050 X 051  pre f ix 050 Y 051 0506\051 Proof Lets assume that there is a valid candidate T 2 C i  j that may be obtained from joining itemset X 2 F i  j 000 1 and Y 2 F i  1  j such that i 2 X s\270i j 2 Y 0507\051 su f f ix 050 X 051 6  pre f ix 050 Y 051 0508\051 and T  X  Y 0509\051 


According to equation 0503\051 the core itemset of T is N T  T n f i  j g 05010\051 According to equations 05010\051 0501\051 and 0508\051 candidate T may be written as T  f i g  N T  f j g 05011\051 If T is a valid candidate then is core N T must be a frequent itemset 050Lemma 1\051 Therefore considering Lemma 1 and the Apriori Property  it results that itemset X 0  f i g  N T is frequent 05012\051 itemset Y 0  N T  f j g is frequent 05013\051 Considering equations 05012\051 05013\051 and 0501\051 it results that X 0 2 F i  j 000 1 05014\051 Y 0 2 F i  1  j 05015\051 and that T  X 0  Y 0 05016\051 Furthermore considering equations 05012\051 05013\051 and 0503\051 it results that su f f ix 050 X 0 051  X 0 n f i g  N T  pre f ix 050 Y 0 051  Y 0 n f j g 05017\051 Equation 05017\051 contradicts the assumption of equation 0508\051 Reciprocal let X 2 F i  j 000 1  i 2 X and Y 2 F i  1  j  j 2 Y that conform to equation 0506\051 Let T  X  Y 2 C i  j be the candidate itemset that may be obtained by joining itemsets X and Y  T may be written as T  f i g  su f f ix 050 X 051  f j g  f i g  pre f ix 050 Y 051  f j g 05018\051 According to equations 0503\051 and 0504\051 the core itemset of T is N T  N T  su f f ix 050 X 051  pre f ix 050 Y 051 05019\051 Since X and Y are frequent itemsets then all their subsets are frequent itemsets 050 Apriori Principle  This implies that itemset N T is frequent Therefore all subsets of candidate T that do not simultaneously include items i and j are frequent Therefore T is a valid candidate Algorithm 2 depicts the new base method for generating all candidate itmesets in C i  j for the FIM base algorithm  Algorithm 2 The candidate generation method for the FIM base algorithm  Require F i  j 000 1  F i  1  j 1 for all itemset X 2 F i  j 000 1 do 2 for all itemset Y 2 F i  1  j do 3 if su f f ix 050 X 051  pre f ix 050 Y 051 then 4 candidate 040 X  Y 5 add candidate 210 021n C i  j 6 end if 7 end for 8 end for  In order to compare this new method against the standard Apiori method we must 002rst analyze the basic AprioriGen method given in Algorithm 3  Algorithm 3 The standard Apriori candidate generation method  Require L k 000 1 f the set of all frequent k 000 1-itemsets g 1 for all pairs 050s.a s.b\051 2 L k 000 1 002 L k 000 1 such that a  b do 2 candidate  s.a.b 3 if all k 000 1 subsets of candidate are frequent then 4 add candidate to candidate list 5 end if 6 end for  Lets assume that the current stage for the Apriori algorithm is stage k  5 i.e all valid candidates of size k  5 must be determined Lets assume the target candidate is f 1  2  3  4  5 g  According to Algorithm 3 this candidate is obtained by joining itemsets f 1  2  3  4 g and f 1  2  3  5 g  In order to validate this candidate all its k  4 subsets must be generated and all of them must be frequent 4 000 itemsets  Identifying all 4 000 itemsets include in f 1  2  3  4  5 g requires 5 steps Assuming n 4 to be the total number of frequent 4 000 itemsets  we can state that validating candidate f 1  2  3  4  5 g requires 050 5 001 log n 4 051 steps to complete When analyzing our method for generating the same candidate 050Algorithm 2\051 it can be observed that no supplemental steps are required for validating candidate f 1  2  3  4  5 g  This result is proven to be correct and is enforced by Lemma 2 This example clearly outlines the simplicity of our method when compared against the standard Apriori approach III T HE G ENERAL FIM A LGORITHM AND C ANDIDATE G ENERATION M ETHOD In the previous section the base FIM algorithm has been brie\003y presented As it can be observed from Algorithm 1 the simple model assumes that initial intervals include only a single frequent item If we consider n to be the total number of frequent items it is clear that FIM requires n stages to completely determine all frequent itemsets 3 This implies a total of n partial scans over the transaction database The general model of the FIM algorithm aims to reduce this number of scans In order to achieve this goal it is necessary to group the frequent items in m larger intervals 050obviously m  n 051 The 002rst stage of the generalized FIM algorithm implies applying Algorithm 1 over each interval of frequent items I i 0501 024 i 024 m 051 in order to indentify all frequent itemsets that belong to the target interval The following notations will be used F I i  I j 000 the set of all frequent itemsets in I i  j  I i  I i  1    I j C I i  I j 000 the set of all candidate itemsets in I i  j 05020\051 In it is sho wn that a frequent itemset X 2 F I i  I j must simultaneously include itemsets belonging to I i and I j  Therefore new candidates belonging to C I i  I j must simultaneously include itemsets belonging to I i and I j  This implies that new candidates are determined according to 05021\051 050a general 


approach to equation 0502\051\051 C I i  I j  f X  Y j 050 X 2 F I i  I j 000 1  I i 134 X 6  f 051  050 Y 2 F I i  1  I j  I j 134 Y 6  f 051 g 05021\051 The main difference from the base model is that the general FIM algorithm will consider for each stage a new interval I i 0501 024 i 024 m 051 rather than a new frequent item i 0501 024 i 024 n 051 Algorithm 4 presents this general approach  Algorithm 4 The general model for the FIM algorithm  Require F I i  I i  f set of all frequent itemsets in I i g Require D i  set of transactions grouped by intervals 050subsets of the original D transaction list that include items for interval I i 051 1 for k  1 to m 000 1 do 2 for all I i  i 2 1   m 000 k do 3 I j  I i  k 4 F I i  I j 040 F I i  I j 000 1  F I i  1  I j 5 C I i  I j  f X  Y j 050 X 2 F I i  I j 000 1  I i 134 X 6  f 051  050 Y 2 F I i  1  I j  I j 134 Y 6  f 051 g 6 for all transactions t 032 D i do 7 C t 040 subset 050 C I i  I j  t 051 f determine candidates in C I i  I j included in t g 8 for all candidates c 2 C t do 9 c.support 10 end for 11 end for 12 F I i  I j  F I i  I j  f c 2 C I i  I j j c  support 025 minsupp g 13 end for 14 end for  In order to ef\002ciently generate all valid candidates for a single stage we must adapt Algorithm 2 For a given itemset 050either frequent or candidate\051 X 2 F I i  I j  the following notations will be used 050the general form of equation 0504\051\051 su f f ix G 050 X 051  X n f k j k 2 I i g pre f ix G 050 X 051  X n f k 0 j k 0 2 I j g  05022\051 Using equation 05022\051 the corresponding method for determining valid candidates represents a generalization of Algorithm 2 and is presented in Algorithm 5  Algorithm 5 Generalized approach to the candidate generation stage for Algorithm 4  Require F I i  I j 000 1  F I i  1  I j 1 for all itemset X 2 F I i  I j 000 1 do 2 for all itemset Y 2 F I i  1  I j do 3 if su f f ix G 050 X 051  pre f ix G 050 Y 051 then 4 candidate 040 X  Y 5 add candidate in C I i  I j 6 end if 7 end for 8 end for  Lemma 3 proves that Algorithm 5 generates all possible valid candidates Lemma 3 Let T be a candidate itemset in C I i  I j  T is a valid candidate if and only if it is obtained through joining two frequent itemsets X 2 F I i  I j 000 1 and Y 2 F I i  1  I j that simultaneously meet the following conditions X n su f f ix G 050 X 051 032 I i 05023\051 Y n pre f ix G 050 Y 051 032 I j 05024\051 su f f ix G 050 X 051  pre f ix G 050 Y 051 05025\051 Lemma 3 represents the general form for Lemma 2 therefore its proof is similar The parallel model for the general FIM algorithm is straight forward since processing a set of intervals may be performed independently once the required values have been transfered accordingly The initial number of intervals is equal to the number of parallel working units 050either processes or threads depending on the underlying parallel architecture\051 Each working unit would apply Algorithm 1 for its given interval After local processing is 002nished it can easily be observed the the for all loop on line 2 in Algorithm 4 may be run in parallel A detailed presentation of this parallel model is given in The important thing to notice is that this approach does not affect at all the candidate generation method presented in Algorithm 5 Once each active working unit receives its corresponding new data no supplemental communications are required in order to determine all possible candidates and all computation is performed locally This is a major advantage for this new algorithm over almost all parallelizations of the Apriori based approaches IV P RELIMINARY T EST R ESULTS A preliminary series of tests have been performed in order to compare the overall performance of the FIM algorithm against the Apriori standard algorithm For both algorithms a binary approach was used in order to code transactions and itemsets Both algorithms were run on a synthetic dataset 050T10I4D100K\051 having 100000 transactions and 1000 individual items For the FIM algorithm the number of intervals has been modi\002ed such that both Algorithm 1 and Algorithm 4 could be tested 050with their corresponding candidate generation methods depicted in this paper\051 Table I presents the results we obtained for a minimum support limit of 500 transactions 050resulting in a total of 569 frequent items\051 The 002rst important note is that FIM outperformed Apriori every time even for the last test for FIM 0501 interval with 569 items\051 This last test is extremely relevant since in this particular case the FIM algorithm needed to fully scan the transaction database for each stage Since the standard Apriori algorithm behaves in a similar manner this last test outlines the ef\002ciency of the candidate generation methods that have been presented in this paper 050Algorithms 2 and 5\051 a difference of 800 seconds The second important note is that the minimum execution time for FIM is 517.19 seconds 05060 intervals with 9  10 items per interval 226 even split\051 226 more than 5 times faster than Apriori This result is explained 


TABLE I FIM VS  A PRIORI TRANSACTION SET T10I4D100K  MINIMUM SUPPORT THRESHOLD 500 TRANSACTIONS   Apriori Algorithm    Frequent items  Time 050s\051    569  2845.20     FIM Algorithm    Interval count  number of frequent items per interval  Time 050s\051    569  1  2029.24    300  2  999.91    200  3  730.19    150  4  642.68    100  6  539.54    90  7  533.90    80  8  524.44    70  9  520.34    60  10  517.19    50  12  556.67    40  15  581.98    30  19  590.57    20  29  634.84    10  57  1025.86    1  569  2026.04   by both our methods of generating candidates and by the reduction in database scans explained in V C ONCLUSIONS AND F URTHER W ORK This paper presented new and ef\002cient means of determining candidate itemsets for a particular algorithm 226 The Fast Itemset Miner 050FIM\051 These methods exploit both the Apriori Property and the particularities of FIM algorithm to avoid any unnecessary candidate validation steps and to avoid generating too many candidates Also these methods do not impose any supplemental restrictions on the parallelization of the base and general FIM algorithms Although limited tests have been performed until now their results underline the potential of this novel approach to the frequent itemset mining problem Our immediate priority is to properly test this new approach not only against Apriori but also against faster algorithms such as FP-Growth Favorable results would underline the fact that candidate based approaches should still be considered in addressing the frequent itemset mining problem If properly handled such approaches do have the advantage of eliminating unnecessary test cases A second line of tests must address the parallel FIM algorithm This new algorithm implies a simple straight forward parallelization that may be easily adapted to either shared memory parallel machines either message passing ones R EFERENCES  M J Zaki 223P arallel and distrib uted association mining A surv e y  224 IEEE Concurrency  vol 7 no 4 pp 14 226 25 1999  M Craus 223 A ne w algorithm for association rule disco v ery  224 in Proceedings of the 9th International Symposium on Automatic Control and Computer Science  2007  M Craus and A Archip 223 A generalized parallel algorithm for frequent itemset mining,\224 in ICCOMP'08 Proceedings of the 12th WSEAS international conference on Computers  World Scienti\002c and Engineering Academy and Society 050WSEAS\051 2008 pp 520\226523  H Zhou and J Liu 223 A no v el algorithm for association rule mining without candidate,\224 in Proceedings of the 2009 International Joint Conference on Arti\002cial Intelligence  ser JCAI 09 Washington DC USA IEEE Computer Society 2009 pp 116\226119  J Han and M Kamber  Data Mining Concepts and Techniques  Morgan Kaufman Publications 2006  R Agra w al and R Srikant 223F ast algorithms for mining association rules in large databases,\224 in Proceedings of the 20th International Conference on Very Large Data Bases  ser VLDB 94 San Francisco CA USA Morgan Kaufmann Publishers Inc 1994 pp 487\226499  J Han J Pei and Y  Y in 223Mining frequent patterns without candidate generation,\224 in SIGMOD 264 00 Proceedings of the 2000 ACM SIGMOD international conference on Management of data  2000  J Han J Pei Y  Y in and R Mao 223Mining frequent patterns without candidate generation A frequent-pattern tree approach,\224 Data Min Knowl Discov  vol 8 pp 53\22687 January 2004  C lap Y ip K K Loo B Kao D Cheung and C K Cheng 223Lgen 227 a lattice-based candidate set generation algorithm for ef\002cient association rule mining,\224 in Paci\002c-Asia Conference on Knowledge Discovery and Data Mining  1999 pp 54\22663  N Malhis A Ruttan and H H Ref ai 223 An ef 002cient approach for candidate set generation,\224 Journal of Information  Knowledge Management 050JIKM\051  vol 4 pp 287\226291 2005 


KOLABC.CH Technical Details\323 http://about-threats.trendmicro.com ArchiveGrayware.aspx?language=en KOLABC.CH 
Figure 4 Number of Downloads 2007-2010 A CKNOWLEDGEMENT We thank Mr Masashi Fujiwara at Hitachi Ltd for their useful suggestions R EFERENCES  R Agra w al T  Imielinski and A Sw ami 322Mining Association Rules between Sets of Items in Large Databases\323 Proc of ACM SIGMOD-93 pp 207-216 1993  J  Pei J  Han MA B ehzad and H Pinto 322Pre\336xSpan Mining Sequential Patterns Ef\336ciently by Pre\336x-Projected Pattern Growth\323 Proc of the 17th Int\325l Conf on Data Engineering pp 215-224 2001  M Ohrui H Kikuchiand M T erada 322Mining Association Rules Consisting of Download Servers from Distributed Honeypot Observation\323 The 13th Int\325l Conf on Network-Based Information Systems NBiS 2010 pp 541-545 2010  N R  R osyid M Ohrui H Kikuchi and P  Sooraksa M T er ada 322A Discovery of Sequential Attack Patterns of Malware in Botnets\323 The 2010 IEEE Int\325l Conf on Systems Man and Cybernetics SMC 2010 pp 2564-2570 2010  M Hatada Y  Nakatsuru M T erada and Y  Shinoda 322Dataset for Anti-Malware Research and Research Achievements Shared at the Workshop\323 IPSJ Malware Workshop 2009 MWS 2009 pp 1-8 2009 in Japanese  M Hatada Y  Nakatsuru M T erada and Y  Shinoda 322Datasets for Anti-Malware Research MWS 2010 Datasets\323 IPSJ Malware Workshop 2010 MWS 2010 pp 1-5 2010 in Japanese  K K uw abara H Kikuchi M T erada and M Fujiw ara 322Heuristics for Detecting Botnet Coordinated Attacks\323 The 4th Int\325l Workshop on Advances in Information Security WAIS 2010 pp 603-607 2010  T rend Micro Threat Enc yclopedia 323TSPY 
004 
name=TSPY 
10000 May-08 10 20000 Oct-08 
Figure 1 Number of Coordinated Attacks 2009-2010 Figure 2 Average Langth of Coordinations VI C ONCLUSIONS We have reported the characteristics and evolution of the coordinated attacks using the CCC DATAset for the past 3 years While the number of coordinated attacks decreased the number of distinct malware that used for the coordinated attacks has been increased Figure 3 Duration of Coordinated Attacks 
May-08 3 0 Jan-09 P3.7  Oct-07 Jul-08 Number of Downloads [DL/Week Date [3 years 
0 20000 30000 40000 50000 60000 Jul-08 Sep-08 Nov-08 Jan-09 Mar-09 May-08 Jul-09 Sep-09 Nov-09 jan-10 Mar-10 May-10 3.2 3.4 3.6 3.8 4 4.2 Jul-08 Sep-08 Nov-08 Jan-09 Mar-09 May-09 Jul-09 Sep-09 Nov-09 Jan-10 Mar-10 May-10 2 4 6 8 12 14 Feb-09 Mar-09 Apr-09 P3.10     P3.37    0 40000 60000 80000 100000 120000 140000 160000 Jan-08 Apr-08 Jan-09 Apr-09 Jul-09 Oct-09 Jan-10 Apr-10 
Number of Rules [Rules/Month Date [2 years  Number of Patterns [Patterns/Day Date [2 years  Frequency [Slots/Day Date [Day 
97 


are in DMCC In fact in the Bugzilla of ArgoUML the bug ID 1957 7 relates the two 036les Each label text is a few pixels too high for its component They should be positioned such that the label text is vertically aligned with the text in the labeled component In FreeBSD  We 036nd that ah-core.c and hpfs-alsubr.c are in approximate DMCC In the mailing list of FreeBSD the Message-ID 200906011106.n51B62Da020139@freefall.freebsd.org states that the two 036les are related in a lengthy the message from bugmaster  FreeBSD.org on June 1 2009 about Current problem reports In SIP  We 036nd that MuteDataSource.java and CallPeerActionMenuBar.java were changed systemically with one shift change period in 036ve years In fact These two 036les implement the same feature 8  Audio-Calls In XalanC  We 036nd that Cloneable.cpp and XLocator.cpp are in approximate DMCC In the XSLT syntax and semantic speci\036cation 9  these 036les are related A single template  can pull s tring values out of arbitrary locations in the source tree it can generate structures that are repeated according to the occurrence of elements In the following scenarios we summarise the usefulness of DMCCs reported by Macocha 1 Management of Development Teams If two classes are in approximate dephase macro co-change they should ideally be maintained by the same team of developers to minimise the risks of introducing bugs in the future The team of developers most likely possesses a wealth of unwritten knowledge about the design and implementation choices that they made for these classes which would help them to prevent introducing bugs 19  Consequently a team leader should rede\036ne the organisation of the maintenance team according to the DMCCs links among 036les so that her team does not introduce bugs because of the absence of info rmation or lack of communication among developers For example in ArgoUML when we analysed changes made in three 10 11 12 dephase macro co-changing 036les that have generated bugs we found that these changes have been made with one shift in time in their periods of change and by different developers  Thus such co-changes can not be detected by previous work Thanks to DMCCs a team leader should ensure that team who will maintain these 036les in each change period have the necessary knowledge to maintain the dependency among these 036les 2 Bug and Change Propagation Knowing that two 036les are in DMCCs implies the existence of hidden dependencies between these two 036les If these dependencies 7 http://argouml.tigris.org/issues/show bug.cgi?id=1957 8 http://www.jitsi.org/index.php/Main/Features 9 http://www.w3.org/TR/xslt 10 http://argouml.tigris.org/issues/show bug.cgi?id=1957 11 http://argouml.tigris.org/issues/show bug.cgi?id=2926 12 http://argouml.tigris.org/issues/show bug.cgi?id=4604 are not properly maintained they can introduce bugs in a program With our approach for each program studied we detected 036les in dephase macro co-changes By using external information we con\036rmed our observation and that these 036les indeed participate to bugs For example in SIP we detected seven bugs in relation with dephase macro cochanging 036les By applying the association rule approach described in 3  w e c annot 036 n d t hat t hes e 036 l es are c ochanging Thus by knowing 036les that are in DMMCs we could explain and possibly prevent bugs we plan to study in future work the bug prediction using approximate dephase macro co-changes 3 Traceability Analysis The change history represents one of sources of information available for recovering traceability links that are manually created and maintained by developers The version history may reveal hidden links that relate 036les and would be suf\036cient to attract the developers attention For example in SIP we detect traceability links between four approximate dephase macro co-changing 036les By applying the association rule approach described in we cannot 036nd that these 036les are co-changing Due to the distributed collaborative nature of open-source development version-control systems are the primary location of 036les and the primary means of coordination and archival The requirements of open-source programs are typically implied by communication among project participants and through test cases However such traces of requirements are lost in time Thus by knowing classes there are in approximate dephase macro co-change we could detect potentially traceability links between them which we plan to concretely study in future work V D ISCUSSIONS With our approach we detect 036les in MCCs or in DMCCs in four different programs belonging to different domains and with different sizes histories and programming languages However we do not detect MCCs and DMCCs with the same proportion in each program We observe that the numbers of MCCs and DMCCs found in the programs developed in Java ArgoUML and SIP are greater than the number of MCCs and DMCCs found in program developed in C or C see Table II We explain this 036nding by the fact that on the one hand the majority of FreeBSD 036les are idle and that on the other hand XalanC is the smallest program analysed Thus we also apply our approach to detect dephase macro co-changes on fewer C and C  036les than Java 036les less than 529 036les thus explaining the lower numbers of MCCs and DMCCs In future work we will conduct studies on other programs in these languages to con\036rm this observation and to assess the numbers of MCCs and DMCCs according to the programming languages A Threats to the Study Validity Some threats limit the validity of our empirical study 
331 


Construct Validity  Construct validity threats concern the relation between theory and obs ervations In this study they could be due to implementation errors They could also be due to a mistaken relation between changes in 036les We believe that this threat is mitigated by the facts that many authors discussed this relation that this relation seems rational and that the results of our analysis shows that indeed MCCs and DMCCs exist and are corroborated by external sources of information bug reports and others Actually we apply static analysis to detect MCCs and DMCCs because co-change analysis is known to be more useful when combined with static analysis 23  As previous work detected co-changes committed by the same author in a short time window relaxing these constraints may also lead to false positives The results of our empirical study show that Macocha improves precision and recall with respect to the state of the art in four different programs However we cannot claim that our approach will give similar results for any program Internal Validity  Internal validity is the validity of causal inferences in studies based on experiments The internal validity of our study is not threatened because we have not manipulate a variable the independent variable to see its effect on a second variable the dependent variable Reliability Validity  Reliability validity threats concern the possibility of replicating this study We attempted to provide all the necessary details to re-implement our approach and replicate our empirical study The change logs and the changed 036les of the four programs analysed with their pro\036les to obtain our observations are on-line at http://www.ptidej.net/downloads/experiments/wcre11b External Validity  We performed our study on four different real programs belonging to different domains and with different sizes histories programming languages Yet we cannot assert that our results and observations are generalisable to any other pr ograms and the fact that all the analysed programs are open source may reduce this generability future work includes replicating our study in other contexts and with other programs VI R ELATED W ORK The concepts of MCCs and DMCCs relate our work to that on 036le stability co-change and change propagation A File Stability Many approaches exist to group 036les based on their relative stability th roughout the software development life cycle For example Kpodjedo et al 20 propos ed t o i d ent i f y all 036les that do not change in the history of a program using an Error Tolerant Graph Matching algorithm They studied the evolution of the Mozilla class diagram by collecting 144 Mozilla snapshots over six years reverse-engineering their class diagrams and recovering traceability links between subsequent class diagrams Their approach identi\036ed evolving classes that maintain a stable structure of relations association inheritance and aggregation and thus that likely constitute the stable backbone of Mozilla As other example UMLDiff 11 compares and d etects the differences between the contents of two object-oriented program versions A fact extractor parses each version to extract models of their design Next a heuristic-differencing algorithm UMLDiff extracts the history of the program evolution in terms of the additions removals moves renamings and signature-changes of design entities such as packages classes interfaces and their 036elds and methods UMLDiff then assigns a stability to each class short-lived classes that exist only in a few versions of the program and then disappear idle classes that rarely undergo changes after their introduction in the program and active classes that keep being modi\036ed over their whole lifespan The Error Tolerant Graph Matching algorithm and UMLDiff take few hours to analyse 036le stability for the four programs analysed in this paper because they require parsing and comparing AST-like representations of the programs before performing their analyses Macocha computes stability in few minutes using the change periods of a program which depend on how the developers of the program organise their work and group changes through the life cycle of the program B Co-changing Files Ying et al 2 a nd Zi mmermann et al 3 a p p lied a sso ciation rules to identify co-changing 036les Their hypothesis is that past co-changed 036les can be used to recommend source code 036les potentially relevant to a change request An association-rule algorithm extracts frequently co-changing 036les of a transaction into sets that are regarded as change patterns to guide future changes Such algorithm uses co-change history in CVS and avoids the source code dependency parsing process However it only computes the frequency of co-changed 036les in the past and omits many other cases e.g  036les that co-change with always the same period of time between changes In Section IV we showed that approaches based on association rules cannot detect all occurrences of MCCs and any occurrences of DMCCs because by their very de\036nition they do not integrate the analysis of 036les that are maintained by different developers and–or with some shift in time which could lead to missed co-changing 036les German 7 u sed t h e in f o r m atio n i n t h e CVS t o v isu a lize what 036les are changed at the same time and who are the people who tend to modify certain 036les He presented SoftChange a tool that uses a heuristic based on a sliding window algorithm to rebuild the Modi\036cation Record MRs based on 036le revisions In Softchange a 036le revision is included in a given MR if all the 036le revisions in the MR and the candidate 036le revision were created by the same author and have the same log Thus Softchange can not detect co-changed 036le maintained in the same time by 
332 


different developers Ceccarelli et al 21 and C anfora et al 10 propos ed t h e u s e of a v ect or aut o re gres s i on model  a generalisation of univariate auto-regression models to capture the evolution and the inter-dependencies between multiple time series representing changes to 036les They used the bivariate Granger causality test to identify if the changes to some 036les are useful to forecasting the changes to other 036les They concluded that the Granger test is a viable approach to change impact anal ysis and that it complements existing approaches like association rules to capture cochanges If the authors integrate the analysis of 036les that are maintained by different developers in periods of time of more than few minutes their approach could then detect typical examples of MCCs and DMCCs Antoniol et al 8 p res e nt ed an approach t o det ect s i mi larities in evolutions of 036les starting from past maintenance notwithstanding their temporal distortions They applied the LPC/Cepstrum technique which models a time evolving signal as an ordered set of coef\036cients representing the signal spectral envelope to identify in version-control systems the 036les that evolved in the same or similar ways Their approach can 036nd 036les having very similar maintenance evolution history but they did not present a tool to detect MCCs and DMCCs It used cepstral distance to assess series similarity if two cepstra series are clos e the original signals have a similar evolution in time with which we can not distinguish between the occurrences of MCCs and DMCCs C Change Propagation The development and maintenance of a program involves handling a large number of 036les These 036les are logically related to each other and a change to one 036le may imply a large number of changes to various other 036les Change propagation analyses how changes made to one 036le propagate to others Law and Rothermel 22 presented an approach for change propagation analysis based on whole-path pro\036ling Path pro\036ling is a technique to capture and represent a program dynamic control 037ow Unlike other path-pro\036ling techniques which record intraprocedural or acyclic paths whole-path pro\036ling produces a single compact description of a program control 037ow including loops iteration and inter-procedural paths Law et al s approach builds a representation of a program behavior and estimates change propagation using three dependencybased change-propagation ana lysis techniques call graphbased analysis static program slicing and dynamic program slicing Hassan and Holt 23 in v e stig ated se v e r a l h eu r i stics to predict change propagation among source code 036les They de\036ned change propagation as the changes that a 036le must undergo to ensure the consistency of the program when another 036le changed They proposed a model of change propagation and several heuristics to generate the set of 036les that must change in response to a changed 036le Zhou et al 24 pres ent e d a change propagat i o n a nal y s i s b as ed on Bayesian networks that incorporates static source code dependencies as well as different features extracted from the history of a program such as change comments and author information They used the Evolizer system that retrieves all modi\036cation reports from a CVS and uses a sliding window algorithm to group them Canfora and Cerulo 25 proposed an approach to derive the set of 036les impacted by a proposed change request A user submits a new change request to a Bugzilla database The new change request is then assigned to a developer for resolution who must understand the request and determine the 036les of the source code that will be impacted by the requested change Their approach exploits information retrieval algorithms to link the change request descriptions and the set of historical source 036le revisions impacted by similar past change requests Theses approaches detect change propagation among 036les Their change-propagation model can be used to predict future change couplings and may involve several 036les that are in MCCs or in DMCCs but they do not allow to differentiate between these two concepts All these approaches grouped change couplings created by the same author and have the same log message thus they can not detect approximate MCCs and–or DMCCs Ambros et al 18 pres ented t he Ev olution R adar  a n approach to integrate and visualise module-level and 036lelevel logical couplings which is useful to answer questions about the evolution of a program the impact of changes at different levels of abstraction and the need for restructuring Beyer and Hassan 26 i nt roduced t h e e v o l u t i o n s t o ryboard a new concept for animated visualisations of historical information about the program structure and the storyboard panel which highlights structural differences between two versions of a program They also formulated guidelines for the usage of their visualisation by non-experts and to make their evaluations repeatable on other programs However Xing and Stroulia 27  r eport e d t hat t hes e visualisations are limited in their applicability because they assume a substantial interpretation effort of their users and they do not scale well they become unreadable for large systems with numerous components VII C ONCLUSION AND F UTURE W ORK We introduced the novel concepts of macro co-changes and dephase macro co-changes to describe that two 036les were changed by developers within same change periods with possible shifts in time We describe Macocha an approach to detect dephase macro co-changes using 036le pro\036les and their stability in time Macocha relates to 036le stability and co-changes We therefore performed two types of empirical studies Quantitatively we compared Macocha with UMLDiff 11 a nd an as sociation rules approach 3 b y a ppl yi ng and c ompari ng t h e results of the three approaches on four different programs 
333 


ArgoUML FreeBSD SIP and XalanC and showed that Macocha can identify the same idle/changed 036les as UMLDiff and that Macocha has a better precision and recall than the approach based on association rules Qualitatively we used external information provided by bugs reports mailing lists and requirement descriptions to show that detected MCCs and DMCCs explain real important evolution phenomena We also showed that dephase macro co-changes do exist and can help in explaining bugs managing development teams and traceability analysis We are currently 1 replicating our studies with other programs 2 performing a comprehensive study of the number of MCCs and DMCCs with varying values of t and s especially dependent on the analysed programs 3 identifying other scenarios in which dephase macro cochanges help and 4 relating MCCs and DMCCs with static analysis and external software characteristics such as change proneness Future work also includes a comparative study of the different sets computed by Macocha and associations rules with different value of con\036dence and support other than the values reported in 3  A CKNOWLEDGMENT This work has been partly funded by a FQRNT team grant the Canada Research Ch air in Software Patterns and Patterns of Software and the Tunisian Ministry of Higher Education and Scienti\036c Research We gratefully thank Massimiliano Di Penta and Daniel M German for their generous comments R EFERENCES  M  M  L ehm a n a nd L  Belady  E ds   Program evolution processes of software change  Academic Press Professional Inc 1985  A  T  T  Y ing G C Murphy  R  N g and M  C  C hu-Carroll Predicting source code changes by mining change history Transactions on Software Engineering  IEEE Computer Society Press 2004 vol 30 no 9 pp 574–586 3 T  Z im m e r m ann P  W e is ger b er  S  D iehl a nd A  Z e ller  M ining version histories to guide software changes in Proceedings of the 26th International Conference on Software Engineering  IEEE Computer Society 2004 pp 563–572  H  G all K  H a jek and M  J azayer i Detection of logical coupling based on product release history in Proceedings of the International Conference on Software Maintenance  IEEE Computer Society 1998 pp 190  A  M ockus  R  T  F ielding and J  D  H erbs leb T w o cas e s tudies of open source software development Apache and mozilla ACM Trans Softw Eng Methodol ACM July 2002 vol 11 pp 309 346  M  F is cher  M  P inzger  a nd H Gall Populating a releas e h is tory database from version control and bug tracking systems in Proceedings of the International Conference on Software Maintenance  IEEE Computer Society 2003 pp 23  D  M  G erm a n  A n e m p irical s t udy of 036ne-grained s oftw are m odi\036cations Empirical Softw Engg Kluwer Academic Publishers September 2006 vol 11  G  A ntoniol V  F  R ollo a nd G V e nturi L inear predicti v e coding and cepstrum coef\036cients for mining time variant information from software repositories in Proceedings of the International Workshop on Mining software repositories  ACM Press 2005 pp 1–5  S  B oukt i f  Y  G Gu  eh eneuc and G Antoniol Extracting changepatterns from cvs repositories in Proceedings of the 13th Working Conference on Reverse Engineering  IEEE Computer Society 2006 pp 221–230  G  Canf or a M  Ceccar elli L  Cer u lo a nd M  D i P e nta U s i ng multivariate time series and association rules to detect logical change coupling An empirical study in Proceedings of the 2010 IEEE International Conference on Software Maintenance  IEEE Computer Society Press pp 1–10 1 Z  X i ng and E  S tr oulia  A n alyz ing the evolutionary history of the logical design of object-oriented software Transactions on Software Engineering  IEEE Computer Society Press 2005 vol 31 pp 850 868 1 L  H a tton H o w accur a tely do engineer s p r e dict s o f t w a r e m a intenance tasks Computer  IEEE Computer Society Press 2007 vol 40  V  R Bas ili and D  M  W eis s   A m ethodology for c ollecting v alid software engineering data Software  IEEE Computer Society Press 1984 vol 10 no 6 pp 728–738  T  Z i m m e rm ann S Breu C  L indi g and B Livshits Mining additions of method calls in argouml in Proceedings of the International Workshop on Mining Software Repositories  ACM Press 2006  Z  X i ng and E  S tr oulia  U m ldif f  an algor ithm f or objecto r i ented design differencing in Proceedings of the 20th International Conference on Automated Software Engineering  ACM Press 2005 1 R A g r a w a l a nd R S r ikant F as t a lgor ithm s f o r m ining a s s ociation rules in large databases in Proceedings of the 20th International Conference on Very Large Data Bases  Morgan Kaufmann Publishers Inc 1994 1 A  V a n y a S  K l us ener  N  v an Rooijen and H  v an V liet Char acterizing evolutionary clusters in Proceedings of the 16th Working Conference on Reverse Engineering  IEEE Computer Society 2009  M D’Am bros  M  L anza and M  L ungu V is ualizing c o-change information with the evolution radar Transactions on Software Engineering  IEEE Computer Society Press 2009 vol 35 no 5 pp 720–735 1 B W  Rebecca W i r f s B r o ck and L  W iener  E d s   Designing ObjectOriented Software  Prentice Hall 1990  S  K podjedo F  Ricca P  G a linier  and G Antoniol Recovering the evolution stable part using an ecgm algorithm Is there a tunnel in mozilla in CSMR  2009 pp 179–188 2 M  Ceccar elli L  Cer u lo G  C anf o r a  a nd M  D i P e nta  A n eclectic approach for change impact analysis in Proceedings of the 32nd International Conference on Software Engineering ACMPress 2010 pp 163–166  J  L a w a nd G Rotherm e l W hole p rogram path-bas ed dynam i c impact analysis in Proceedings of the 25th International Conference on Software Engineering  IEEE Computer Society 2003 pp 308 318  A E  Ha ssa n a nd R  C  Hol t   P re dicting change propagation in software systems in Proceedings of the 20th IEEE International Conference on Software Maintenance  IEEE Computer Society 2004 pp 284–293  Y  Z h ou M  W  ursch E Giger H C Gall and J L  u A bayesian network based approach for change coupling prediction in Proceedings of the 15th Working Conference on Reverse Engineering  IEEE Computer Society 2008 pp 27–36  G Canfora a nd L  Cerulo  Im pact analys is by m i ning s o ftw a re and change request repositories in Proceedings of the 11th IEEE International Software Metrics Symposium  IEEE Computer Society Press 2005 p 29  D  Be yer a nd A  E  H a s s an  A n im ated vis u alization o f s of tw ar e history using evolution storyboards in Proceedings of the 13 th Working Conference on Reverse Engineering  IEEE Computer Society Press 2006  Z  Xing and E  S troulia  Bottom up des i gn e v olution c oncern discovery and analysis Tech Rep 2007 
334 


relate approximately the 34% of the events, which enables the forecast of new events that are the same type as those contained in the rules. In a similar way to the presented case we analyzed 62 different measurement points with a total of 6300 events occurred in the distribution network over 2 years TABLE VI ASSOCIATION RULES AND THEIR CONFIDENCE rule conf  1]? [1? 5] 0.750 9]? [9? 9] 0.5294 139 Fig. 2. Complete sequence and their frequent serial episodes TABLE VII COMMON RULES FOUND IN DIFFERENT MEASUREMENT POINTS rule number of measurement points 1] ? [1? 1] 16 1] ? [1? 5] 10 6] ? [6? 9] 8 9] ? [9? 9] 5 14] ? [14? 15] 11 15] ? [15? 15] 19 1? 1] ? [1? 1? 1] 18 The rules commonly found in most of the measurement points are shown in Table VII. The parameters used to extract the association rules are win = 5, min fr = 0.15 and min conf = 0.5. The summary showed in Table VII indicates that the ranges of occurrence of events in each measurement point presents different behaviours and, therefore, it is necessary to examine separately the sequences of each measurement point for a successful development of a valid prediction. The results show that it is possible to build rules to relate time spans between successive events which give an estimate of time lag for the appearance of a new event. Although it is not possible to relate all the events of the sequence, the knowledge of network behavior is improved, and takes advantage of power system information available in databases. Finally, different values of win min fr and min conf were tested to find frequent episodes 


and association rules. The result show that increasing win larger episodes can be found, while increasing min fr and min conf the amount of frequent episodes and rules will be minor, since the requirement for selection increases Similarly, it was found that using WINEPI methodology, the frequency of an episode is proportional to the width of the window. The appropriate values of min conf and min fr are defined taking into account criteria such as sequence length, type events frequency and the confidence threshold that the user wishes to assume. It is advisable to test different values of min conf and min fr V. CONCLUSIONS AND FUTURE WORKS The proposed methodology suggests a new approach for failure analysis in power distribution system. The aim of the solution is the forecast of faults, which is analyzed in the domain of temporal data mining and, in particular, the discovery of patterns starting from the registers of events The proposed solution is able to extract useful information about the behaviour and evolution of the faults in the electrical system, as a first step in the exploitation of event sequences recorded in power distribution systems, for the prediction of future failures Future work should continue with the search of episodes in sequences of events to discovery patterns related with failures in components and to exploit other information contained in the events recorded besides the elapse times between events e.g., the depth of voltage sags during the fault, etc. Next use this information from a predicting point of view, to anticipate future failures VI. ACKNOWLEDGMENTS This research has been developed within the eXiT, Control Engineering and Intelligent Systems, research group of the Institute of Informatics and Applications \(University of Girona Decision Support Systems \(AEDS with a consolidated distinction \(2009 SGR 523 2012 period in the Consolidated Research Group \(SGR project of the Generalitat de Catalunya The work has been supported by the research project Moniorizacion Inteligente de la Calidad de la Energ?a Electrica DPI2009-07891 vacion \(Spain 


scholarship \(2009FI-A00452 per a Universitats i Recerca del Departament dInnovacio Universitats i Empresa of the Generalitat de Catalunya and also the European Social Fund REFERENCES 1] Math H.J Bollen. Understanding power quality problems, voltage sags and interruptions. IEEE press series on power engineering, 1999 2] Carl L. Benner and B. Don Russell. Distribution incipient faults and abnormal events: Case studies from recorded field data. In 57th Annual Conference for Protective Relay Engineers, 2004 3] K. C. P Wong, H. M. Ryan, and J. Tindle. Power system fault prediction using artificial neural networks. In International Conference on Neural Information Processing, 1996 4] Bach Quoc Khanh, Dong-Jun Won, and Seung-Il Moon. Fault distribution modeling using stochastic bivariate models for prediction of voltage sag in distribution systems. IEEE Transactions on Power Delivery, 23:347354, 2008 5] J. A. Martinez-Velasco and J. Martin-Arnedo. Stochastic prediction of voltage dips using an electromagnetic transient program. In 14th PSCC, Sevilla, Spain, 2002 6] Heikki Mannila, Hannu Toitoven, and A. Inkeri Verkamo. Discovery of frequent episodes in event sequences. Data Mining and Knowledge Discovery, 1:259289, 1997 7] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Int. Conf. Very Large Data Bases VLDB94 8] Charles J. Kim, Lee Seung-Jae, and Kang Sang-Hee. Evaluation of feeder monitoring parameters for incipient fault detection using laplace trend statistic. IEEE Transactions on Industry Applications, 40:1718 1724, 2004 9] Z.W. Liao, G. Wang, Q.H. Ye, and Y.M. Sun. A novel fault diagnosis system for transmission line system based on sequence of events. In 6th International Conference on Advances in Power System Control Operation and Management APSCOM 2003, pages 440445, 2003 140 10] Srivatsan Laxman and P. Shanti Sastry. A survey of temporal data mining. SADHANA Academy Proceedings in Engineering Sciences 31:173198, 2006 11] Rakesh Agrawal and Ramakrishnan Srikant. Mining sequential patterns. In Int. Conf. Data Engineering \(ICDE95 12] Srivatsan Laxman, P. Shanti Sastry, and K. P. Unnikrishnan. Fast algorithms for frequent episode discovery in event sequences. Technical 


report, CL-2004-04/MSR, GM R&D Center, Warren, 2004 13] K. P. Unnikrishnan, Debprakash Patnaik, and P.S. Sastry. Discovering patterns in multi-neuronal spike trains using the frequent episode method. Technical report, General Motors R&D Center, Warren, 2007 14] Kuo-Yu Huang and Chia-Hui Chang. Efficient mining of frequent episodes from complex sequences. Information Systems, 33:96114 2008 15] Gregory Piatetsky-Shapiro and William Frawley. Knowledge Discovery in Databases. AAAI/MIT Press, 1991 141 


