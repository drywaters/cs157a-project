A New Classification Algorithm Based on Certainty Architecture  Liyan Dong 1 Mo Shi 2 Renbiao Wang 1 Han Yin 1 Mao rui 1 Le Huang 1  1 College of Computer Science and Technology Jilin University Changchun, 130012, China  dongliyan@gmail.com 2 Interation Exchange Department Jilin University of Finance and Economics 
Changchun, 130117, China yisi_1993@hotmail.com Abstract This paper proposed a kind of classification algorithm which brought in certainty factors of expert system as certainty theory proof based on certainty and support architecture aimed at the problem that classical association rules algorithm based on confidence and support architecture couldnt generates association rules accurately facing some questions on mathematical 
statistics. With experiment verified, this classification algorithm based on certainty and support architecture has a better performance improved the accuracy and perceptiveness of association rules effectively Keywords- association rules; certainty; confidence I  I NTRODUCTION  Association rules describes certain relationships of the set of attribute items in the database, which is not instructive and simple[1  
Sin ce th e cl as s i c A p ri o r i alg o r i t h m  w a s pr op os ed   the domestic and foreign scholars have focused on the efficiency of the association rules. For instance, Mannila, Toivonen and Verkamo introduced the technology of decision tree pruning to the classic association rules theory   Park, Chen and Yu proposed the association rules algorithm based on hash S a va ser e Omiecinski and Navathe also proposed the association rules algorithm based on partition [4  
Although the previous studies improved the mining efficiency of association rules to some  extent, and reduced the space and time complexity greatly of the association rules[5    However, it has not done much to improve the framework system of the association rules algorithm based on the support and confidence t h i s  pa pe r p r opo se s a n e w association rules algorithm with framework system based on certainty and support 
architecture. This algorithm introduces a certainty factor in expert system to measure the certainty of association rules. It accurately describes the relationship between the conditional probability and the prior probability The algorithm not only resolves the problem of missing association rules fundamentally, but also it improves the accuracy of association rules mined II.T HE PROBLEMS BASED ON CONFIDENCE ARCHITECTURE 
 Definition1 If the item sets U X 002 the total number of transaction of non-empty transaction database is N, the support degree of X is S/N, we denote it as sup\(X\, that is P\(X\. If sup\(X\min_sup which min_sup is the given minimum support threshold, we call X frequent item sets[7    Definition2 Supposed that Y X 002 is a 
association rule, which meet U X 002  U Y 002  003  004 Y X the conditional probability of Y is P\(Y/X\ we call it the confidence degree of Y X 002 we denote it as   Y X conf 002 8   
2011 Fourth International Conference on Intelligent Networks and Intelligent Systems 978-0-7695-4543-1/11 $25.00 © 2011 IEEE DOI 10.1109/ICINIS.2011.4 17 


Definition3 As for the association rules 005 002  sup Y X min_sup  005 002   Y X conf  min_conf we call Y X 002 as strong association rule [9  The confidence Y X 002 is called as conditional probability. Although the confidence degree interpreted by conditional probability reflects the relevance of between X and Y, it separates the relevance of the conditional probability P\(Y/X\ and the prior probability P\(Y\ which may lead to three problems are as following   When 005 002  sup Y X min_sup and min_conf 006 P\(Y|X 006 P\(Y\, the association rule generated is lack of credibility. Due to min_conf 006 P\(Y|X\tput Y X 002 will be strong association rule. But P\(Y|X 006 P\(Y\ shows that the probability of Y reduced under the premise of X. Instance X kept Instance Y from advancing. At this time, it is easy to return the strong association rules Y X 002 which is normal association rule in fact   When 005 002  sup Y X min_sup and min_conf 005 P\(Y|X 005 P\(Y\, we may miss some important association rules. Due to P\(Y|X 006 min_conf output Y X 002 would not be strong association rule. But P\(Y|X 005 P\(Y shows that the probability of Y increased under the premise of X.  Instance X deduced Instance Y As a result we ought to get the association rule Y X 002  as a useful association rule   When   Y X conf 002  P\(Y|X\=P\(Y we can easily get P\(YX\ = P\(X 000h P\(Y Instance Y  and X are independent. Therefore, the association rules generated was not accurate III  T HE THEORY OF CERTAINTY FACTOR  The theory of certainty factor is proposed by Shortliffe in 1975, which was a reasoning model The specific formulas are as follows         003 003 004 003 003 005 006    H P MAX H P H P E H P MAX MAX H P E H MB 0  1  1  0 1   001A          003 003 004 003 003 005 006     H P MIN H P H P E H P MIN MIN H P E H MD 0  1  1  0 1   001B     E H MD E H MB E H CF       001C  CF is the certainty factor based on some assumption H under the situation that the posterior evidence exists MB\(H,E is the increased confidence measure caused by the existence of E. MD\(H,E is the increased no-confidence measure caused by the existence of E The relationship between MB\(H,E and MD\(H,E can be expressed by table 1  TABLE I  T HE RELATIONSHIP OF VALUE  conclusion MB,MD,CF If  E H P 1 is ture MB=1,MD=0,CF=1 If  1   E H P is false MB=0,MD=1,CF=-1 Lack of evidence  H P E H P   MB=0,MD=0,CF=0   If CF\(Y, X\>0, it shows the appearance of X increases the probability of Y. The larger the value of CF\(Y, X\ is, the greater the increase probability Y is   If CF\(Y, X\<0, it shows the appearance of X decreases the probability of Y. The smaller the value of CF\(Y, X\ is, the greater the reduction probability Y is IV  A LGORITHM DESCRIPTION  In general, we should set certain threshold to fix on the conditions association rules met in advance, the threshold  1  0  007 010 The range of 010  based mainly on the association strength that the classification system required. When 1  010  the evidence of posterior support the hypothesis proposed fully. This paper take 0  010    Scan all k-item sets in the database, find all k-item sets that its support is greater than the minimum support by connecting operation 
18 


deleing operation, etc. Then generate k-frequent sets   Calculate the certain degree of k-frequent sets, find all k-frequent sets that meet the certain condition, derive the certain association rules Classification-SC algorithm description Input: data item sets k, minimum support threshold min_sup, minimum confidence threshold min_conf Output: all the item sets that meet the requirements Step 1 Initialize the database and scan all the item sets that the length of them is k according to the initial conditions, and the initial value of k is 1 Step 2 Calculate the support of each k-item set, remove the k-item set thats support is less than min_sup If the k-item set is empty, we finish the algorithm Step3 Connect the k-item set in accordance with Apriori. If the k+1-item set are empty, we finish the algorithm Step4 Calculate the support of each k+1-item set, remove the k-item set thats support is less than min_sup If the k-item set is empty, we finish the algorithm Step5 Calculate the certain of each k+1-item set, remove all k+1-item set thats certain is less than threshold 010 If k+1-item set are empty, we finish the algorithm Step6 Get the certain association rules according to the result of k+1-item certain frequent set, then k We use pseudo-language to describe the classification algorithm based on certain-support framework Input: DataSet 001 minsup Output: AruleSet // the association rules set of certain classification-SC\(AruleSet,DataSet,minsup,ce r_threshold Step1 FOR i=1 TO Length\(DataSet\ do Step2 FrequentSet[i 011 GenFrequentSet\(i,DataSet Step3 IF FrequentSet[i   000 THEN RETURN //if i-item data set is empty, finish the algorithm Step4 FOR j=1 TO Length\(FrequentSet[i   Step5 IF FrequentSet[i   m i ns up T H E N  Remove\(FrequentSet[i  Step6AruleSet 011 GenAruleSet\(FrequentSe t[i ce r_ th r e sh o l d  g en e r a t e iitem  ce rt ain  association rules Step7 DataSet 011 FrequentSet[i   th e i+1-item frequent set is got by i-item frequent set V  S IMULATED EXPERIMENT  Example 1: The value of min_sup is 30% and the value of min_conf is 60 TABLE II  T RADING STATISTICS   pens bought pens not bought total The pencils bought 4200 2800 7000 The pencils not bought 1800 1200 3000 Total 6000 4000 10000 The analysis of Apriori algorithm is as follows sup min_  42 10000 4200 sup    002  conf conf min_  70 6000 4200        pencil buy pen buy 002 012  is a strong association rules The analysis of classification-SC algorithm is as follows sup min_  42 10000 4200 sup     According to the certain formula    E H MD E H MB E H CF      000\003  1 0   B P 002 000\003    0       012 A B MD A B MB A B CF 000\003  010  012 A B CF  000\003    pencil buy pen buy 002 012  isnt a strong association rules 
19 


The percentage of customers who has bought pencil is 70% in all the customers who has bought pen. That is the average probability of customers bought pencil in all customers is the same with the conditional probability of customers has first bought pen and then has bought pencil. Therefore, we can see from the above analysis that the behavior of buying pen did not promote the behavior of buying pencil  Example 2 The value of min_sup is 20% and the value of min_conf is 20 TABLE III  T HE RELATIONSHIP AMONG THE PRODUCT PARTS   Defective products Qualified products total equipment from B 52 636 688 equipment from others 11 1800 1811 Total 63 2436 2499 The analysis of Apriori algorithm is as follows conf conf min_  56  7 688 52 sup min_  08  2 2499 52 sup       002     products defective result B from equipment produce  002   012  isnt a strong association rule 000\003 The analysis of classification-SC algorithm is as follows sup min_  08  2 2499 52 sup    000\003  1 0   B P 002 000\003    010        012  2  5 9748  0 02521  0 07558  0 1  B P B P A B P A B MB   0   012 A B MD 000\003 VI  C ONCLUSION  The strong association rules got from the classic association rules algorithm based on support-confident framework might go against the objective laws of reality. This is because the association rules based on confident-framework can not describe the relationship between the evidences and the assumptions accurately. In addition, we can not measure the influence which prior event had on the back event. The paper introduced certainty factors to verify the confidence degree, accordingly describing the relationships between the conditional probability and prior probability. Whats more, the algorithm resolved the problem that important association rules often were missed  Reference 1  Z hun Z h o u  B i ng r u Y a ng  Y u nf e n g Z h ao W e i H o u  Research on algorithms for association rules mining based on FP-treeŽ, Systems and Control in Aerospace and Astronautics, ISSCAA 2008, 2nd International Symposium on, pp. 1-5  2   R  Ag r a wa l  T  Im i e l i n s k i a n d A S w a m i   M i n i n g association rules between sets of items in large databasesŽ, Proceedings of the 1993 ACM SIGMOD international conference on Management of data ACM Press, 2003, pp.207-216 3 Hei k k i Man ni la  Hann u T o i von en a n d A   I n k e ri  Verkamo. Efficient Algorithms for Discovering Association RulesŽ, Proceedings of AAAI94 Workshop on Knowledge Discovery in Databases Seattle, Washington. AAAI Press , 1994, pp.181-192 4  P M eh m e t K a y a  P R ed a Alh a j j  Ut i li zi n g G e n e t i c  Algorithms to Optimize Membership Functions for Fuzzy Weighted Association Rules MiningŽ, Applied Intelligence, vol. 24, Feb. 2006, pp.7-15 5  A m iho o d A m ir Y o nat an A u m a n n  Ro ne n F i l d m a n  Moshe Fresko, Maximal Association Rules: A Tool for Mining Associations in TextŽ, Journal of Intelligent Information System, vol.25, Nov. 2005, pp.333-345 6 Dav y Jan sse n s  G e e r t W e t s T o m B r i j s K o e n Va n h o o f  Adapting the CBA algorithm by means of intensity of implicationŽ, Information Sciences, vol. 173, June 2005 pp. 305-318 7  Mu ha m m a d F a uz i Mo hd Z a i n  M d  N a z r ul  I s l a m   I r   Hassan Basri, An expert system for mix design of high performance concreteŽ, Advances in Engineering Software, vol.36, May.2005, pp.325-337  8  Dou S h en  Ji an T a o S u n  Q i an g Ya ng Zh en g C h en  A comparison of implicit and explicit links for web page classificationŽ, Proceedings of the 15th international conference on World Wide Web, May.2006 pp.643-650 
20 


35  3 rd IEEE International Conference on Adaptive Science and Technology \(ICAST 2011   Algorithm: X-Apriori function X-Apriori \(Dt : a transactional database, s: a support threshold\, returns a set of frequent itemsets S 1: if first_mining then 2: begin 3 k 1; \(Note that a k-itemset represents a set of k items 4 F   an empty set for holding the identified frequent itemsets 5: generate all candidate k itemsets from Dt  6: while \(candidate k itemsets exist\ do 7: determine support for candidate k itemsets from Dt  8: add frequent k itemsets into F  9: remove all candidate k itemsets that are not sufficiently supported to give frequent k itemsets 10: generate candidate k 1\-itemsets from frequent k itemsets using “downward closure property 11 k  k 1 12: end while 13 KnownFrequentItemsets    F  14: return KnownFrequentItemsets  15: end if first_mining 16: else 17: if NewTransactions 0.05* |Dt| then 18: Subsequent_mining 19: begin 20 NewFrequentItemsets 0 21: k:= 1 22: NewCandidateSet:= \(all 1-itemset 23 NewFrequentItemsets  FrequentItemsets in NewCandidateSet  24: While NewFrequentItemsets   0\ do 25: determine support for candidate k itemsets from NewTransactions  26: add frequent k itemsets into F  27: remove all candidate k itemsets that are not sufficiently supported to give frequent k itemsets 28: generate candidate k 1\-itemsets from frequent k itemsets using “downward closure property 29 NewFrequentItemsets    F  30 k  k 1 31: End While   32 NewFrequentItemsets Max KnownFrequentItemsets    NewFrequentItemsets  33 KnownFrequentItemsets  NewFrequentItemsets  34: Return all subsets of elements in KnownFrequentItemsets   The X-Apriori algorithm presented here does not have to scan the entire database when new transactions are added to the database. It only adaptively mines the incremental database whenever five percent additional transactions were added to the database. Information about the known frequent item sets is stored at any point in time. The union of this and the new frequent item sets generated from the added transactions gives the frequent item sets in the updated database at any point in time. For subsequent mining, the new frequent item sets generated from the updated database will always become the known frequent item sets. Performance of the system will tremendously improve with time as the system learns and stores relevant information for optimized movement of the integration agent.. Moreover, whenever a data site is down during the mining process. The ARMCA uses the results from the other sites for DKI Also, whenever a result site is down during the KI process, the RICA integrates the results of the last site visited with the next result site. If it is the first result site another instance of RICA is initiated by ARMCA to the next result site. If it is the last result site, the state of RICA in the second to the last result site is returned  IV  CONCLUSION AND FUTURE WORKS In this work, an adaptive architecture for mining association rules in distributed databases using mobile agents’ scenarios involving two state-of-the-art algorithms and an improved one is proposed. The proposed architecture is adaptive in nature and also system initiated. Efficient integration of mining results at the result sites will also be achieved. This will greatly save data communication costs by drastically reducing movement of massive data in the system. There is no need for the usual massive data movement in global knowledge integration as only the final results are migrated to the DARM server. The architecture ensures that the response time for DARM tasks is improved each time the mining is repeated. The architecture is system initiated and has the capacity of adjusting to current prevalent and unforeseen circumstances that may arise over time in the life of the system. The system AMAARMD will be implemented with Java and all agents will be created in JADE environment.  Agent characteristics, agent functionalities, protocols communication, coordination and co-operation will all be done under JADE, which is a standard agent based development platform that is robust, scalable and secure It is believed that the system will  help to discover new hidden, previously undiscovered patterns in these sites leading to a dynamic improvement in business yield and turnover. Future works will treat details about the adaptivity of the mining agent and the implementation of the architecture proposed in this work   REFERENCES 1   R. Ag ra w a l, T   Im ie lin sk i an d A  S w a m i, Mi n i n g  Association Rules between Sets of Items in Large Databases, in Proceedings of the ACM SIGMOD Conference on Management of Data, Washington D.C, 1993, pp. 207-216   M. C h e n J. H a n  an d P. Y u  1996  D at a Mi n i ng   An Overview from Database Perspective”, IEEE Transactions on Knowledge and Data Engineering 8\(6\1996, pp. 866-883 3  V. S. Rao an d S. Vid y av at h i Distrib u ted Data  Mining and Mining Muli-agent Data, International Journal on Computer Science and Engineering 2\(4\7 -1244, 2010   G. W  W e bb E f f i cien t Se arch  f o r A s s o ciation Rules,"Proc. Sixth ACM SIGKDD Int'l Conf Knowledge Discovery and Data Mining \(KDD 00 ACM Press, 2000, pp. 99-107 5  M  Z  A s h r a f i  D  T a n i a r  D a n d K  S m i t h   O D A M   An Optimized Distributed Association Rule Mining Algorithm”, IEEE Distributed Systems online 1541-4922 © 2004 published by the IEEE Computer Society, vol. 5, no. 3, March 2004  6 T  Ch ia, a n d S Ka n n a p a n    S trate g icall y Mo b ile  Agents”, in First Intl. Workshop on Mobile 


36  3 rd IEEE International Conference on Adaptive Science and Technology \(ICAST 2011    Agents MA ’97\, Berlin, Germany, SpringerVerlag LNCS 1219, 1997, pp. 149-161   H. Kargu p ta, H. Il k e r, a n d S  Brian  S calable  Distributed Data Mining-An Agent Architecture, In Heckerman e 21 1   M. B y rdan d C  Fran k e T h e State of Dis t ribu ted  Data Mining. [ECS265 Project Report U C D a vi s    Davis CA 95616, USA, 2007   S  K o t s i a n t i s an d D   K a n e l l opou l o s  A s s o ci at i on Rules Mining: A Recent Overview. GESTS International Transactions on Computer Science and Engineering, Vol.32 \(1\006, pp. 71-82   S  Pau l A n O p t i m i zed D i st ri bu t e d A s s o ci at i o n Rule Mining Algorithm In Parallel And Distributed Data Mining With Xml Data For Improved Response Time, International Journal of Computer Science and Information Technology, Volume 2 Number 2, April 2010. 10.5121/ijcsit.2010.2208 88   K  A   A l bas h i r i  F  C o e n en an d P.  L e ng   EMADS: An Extendible Multi-Agent Data Miner Knowledge-Based Systems \(KBS\rnal Volume 22, Issue 7, 2009, pp. 523-528  12  K  A  Al b a s h i r i  E M AD S  An Investigation into the Issues of Multi-Agent Data Mining. PhD Thesis The University of Liverpool, Ashton Building Ashton Street, Liverpool, 2010, UK    T  L e g l er, L  Wol f g a n g an d S  Jan  R obus t an d  Distributed TopN Frequent Pattern Mining With SAP BW Accelerator, ACM. VLDB ‘09, 2009 Lyon, France    A  O  O g u n d e O   F o l o ru ns o, A  S  S odi y a a n d G   O. Ogunleye, A Review of Some Issues and  Challenges in Current Agent-Based Distributed Association Rule Mining, Asian Journal of  Information Technology, Vol. 10, No. 02, 2011 pp. 84-95   R  G r a y D  K o t z G  Cy benk o, D  Ru s    M obi l e  agents: Motivations and state-of-the-art systems 2000, ftp://ftp.cs.dartmouth.edu/TR/TR2000365.ps.Z 16   J  Hip p  U. Gu n t zer a n d G. Na k h a e i zad eh  Algorithms for Association Rule Mining  A General Survey and Comparison SIGKDD Explorations, Volume 2, Issue 1., 2000    J. H a n  J Pei  an d Y  Y i n  Mi n i ng F r equ e nt  Patterns without Candidate Generation: A Frequent-Pattern  Tree Approach, In Proceedings of the ACM Special Interest Group on Management of Data \(SIGMOD\ternational Conference Management of Data \(SIGMOD 2004, pp. 53-87 18   M. W o o l d r id g e A n In tro d u ctio n to Mu lti Ag e n t  Systems. John Wiley and Sons, United Kingdom 2009  19   Ira Ru d o w s k y In tell ig e n t Ag e n ts Co m m u n i cat io n s  of the Association for Information Systems, Vol 14, 2004   E. I.  A r i w a, B. S  Moh a m e d a n d M. M. Moh a m e d Informatization and E-Business Model Application for Distributed Data Mining Using Mobile Agents International Conference WWW/Internet 2003   R. Agrawal. and R.  Srikant  \(1994\ast algorithm for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, Santiago de Chile, Chile, 1994,  pp. 487499   


E D C B  Table 2: Dynamic array \(N frequent items The possible maximal frequent itemsets \(PMFI obtained from this array are considered to the next level All the other itemsets are pruned. The number of possible large itemsets is less than or equal to number of frequent itemsets. From this array, PMFIs are obtained in two ways. We can take all columns and first row. \(Columns A B, C, D and row E Rows E D, C, B and Column A One column entry or one row entry is a single PMFI. For example the itemset \(x,y,z itemset if and only if there is an entry 1 in both cells N[x,y] and N[x,z]. For example ADC is a PMFI because of the values of both cells N [A, D] and N [A, C] is 1 Once all PMFIs are retrieved from the array, they are arranged in descending order with respect to their size First column entries produce the PMFI that includes the frequent item A \(ADC the PMFI includes the frequent item B \(B entries produce the PMFI that includes the frequent item C CDE the frequent item D \(D PMFI that includes the frequent item E \(EC are arranged in descending order with respect to their size So the Itemsets in PMFI are ADC, CDE, EC, B, D The First itemset ADC has no superset in MFI and it is frequent. So ADC is a MFI\(with support count 3 added to MFI The Second itemset CDE has no superset in MFI and it is a infrequent item set. so all \(n-1 includes C CDE are generated. 2-itemsets of CDE are CD and CE.\(DE is not taken for test. It does not include the item C ignored. CE is frequent and it has no supersets in MFI. So it is added to MFI. CE is a MFI\(with support count 3 obtained from CDE 


The Third itemset EC is already included in MFI and it is ignored The Forth item B has no superset in MFI and it is a frequent item and it is added to MFI The Fifth item D has a superset in MFI and it is ignored From above example, MFIs with support count 3 are ADC, CE, B The process will be continued till testing all possible maximal frequent itemsets. The pruning can be done while finding the MFI itself, but not after finding FI completely The pseudo code for proposed algorithm is given below in figure 1  Item  Tidset A  T1, T2, T3, T4 B  T1, T4, T5 C  T1, T2, T3, T5, T6 D  T1, T2, T3 E  T2, T4, T5, T6 F  T6, T5    Pseudo code Find All MFI\(PMFIs,min_sup all Maximal Frequent Itemsets Inputs i Possible Maximal Frequent itemsets ii for mining process Interfacing Functions Output i Find All MFI\(PMFIs, min_sup For each x  PMFIs if x has a superset in MFI continue else if x is frequent MFI=MFI U x else Find MFI by obtaining Permutations\(x,min_sup For End return MFI 


Find MFI by obtaining Permutations \(PMFI, min_sup Function to find Maximum Frequent Itemsets form Kitemsets of PMFI Inputs i ii defined for mining process Interfacing Functions Figure 1. Pseudo code for ABMFI Algorithm Output i Find MFI by obtaining Permutations \(PMFI, min_sup n=number of items in PMFI k=n-1; // k-itemsets Freq_item={}; S={}; do In_Freq=0; //to check infrequent itemsets C=generate k-itemsets that includes first item of PMFI; Foreach x C if x has a superset in MFI S=S U x; continue; else if x is frequent Freq_item=Freq_item U x; else In_Freq=1; For End; If PMFI==unique\(Freq_item U S MFI=MFI U Freq_item; return; End if; K--; while\(In_freq!=0 && k!=0 The proposed algorithm performs better because MFI is being calculated directly before computing FI completely The Pruning mechanism works effectively and counting is not performed for the subset of MFIs. So, the time taken to compute MFI is negligible. As we are following vertical data format, support also need not be calculated separately In this case, support is directly given by the number of transactions in the tidlist of each FI. The vertical representation has the following major advantages over the horizontal layout: Firstly, computing the support of itemsets is simpler and faster with the vertical layout since it involves only the intersections of tidsets. Secondly, with the vertical layout, there is an automatic reduction of the database before each scan in that only those itemsets that are relevant to the following scan of the mining process are accessed from disk Pruning The Possible Maximal Frequent itemsets\(Maximal 


Candidate Itemsets array. All Maximal frequent itemsets are obtained from only these PMFIs. Other itemsets are pruned automatically. The pruning can be done while finding the MFI itself, but not after finding FI completely. The proposed approach applies superset checking to eliminate the non maximal frequent itemsets. Once all PMFIs are generated, each PMFI is checked whether it is a subset of any maximal pattern. If so the itemset is eliminated entirely. Counting is not performed for this itemset and next PMFI is taken for test IV. RESULTS The testing of the proposed algorithm has been carried out on the real dataset \(containing long itemsets the number of candidate itemsets taken by the proposed algorithm to find MFIs and it is compared to Genmax algorithm for various values of minimum support. The support is varied from 75 to 95. The proposed algorithm had been compared with GenMax algorithm and results show the proposed approach generates less number of candidate itemsets to find all MFIs Figure 2 illustrates that, the proposed approach generates less number of candidate itemsets and produces all MFIs very quickly than GenMax algorithm. Support is taken as x axis and the number of candidate itemsets taken to find all MFI is taken as y axis For Mushroom, the improvement is best explained by how the MFI is computed at each level and found directly without waiting for FI completely. This leads to a much greater reduction in the overall search space, since the reductions is so great at highest levels  Figure 2. Number of Candidate itemsets taken by ABMFI and GenMax Algorithm from Mushroom dataset This approach will be working very efficiently for any sparse and dense dataset, when the size of maximal frequent itemsets is close to the number of frequent itemsets V. CONCLUSION In this paper we have investigated an array based approach and algorithm \(ABMFI itemsets. The algorithm is straight forward  basic steps are finding frequent items from the database, Dynamic 


array construction and Pruning infrequent itemset obtaining Possible Maximal Frequent itemsets from array and finding MFIs from PMFIs. Our algorithm had been compared with GenMax algorithm and obtained that the proposed algorithm generates less number of candidate itemsets to find all MFIs. The vertical data format representation of the database, Dynamic array construction and directly computing MFIs from PMFIs are the added advantages of this algorithm REFERENCES 1]. Agrawal, R., T. Imielinski and A. Swami, 1998. Mining association rules between sets of items in very large databases In the Proceedings of the ACM SIGMOD International Conference on Management of Data, May 25-28, Washington D.C., US, pp: 207-216. http://doi.acm.org 10.1145/170035.170072. [2]. Jiawei Han and Micheline Kamber, 2001. Data Mining: Concepts and Techniques. 1st Edn., Morgan Kaufmann pp: 500. ISBN-10: 1558604898. [3 Ganti, V., J. Gehrke and R. Ramakrishnan, 2000. DEMON mining and monitoring evolving data. ICDE 2000, San Diego CA., pp: 439-448. http://wwwdb.cs.wisc.edu/dbseminar/spring00 / talks/demon_paper.pdf [4 Holsheimer M., M. Kersten, H. Mannila and H. Toivonen, 1995 A perspective on databases and data mining. Proceeding of the 1st International Conference on Knowledge Discivery and Data Mining, Aug. 1995, AAAI Press, Montreal, Canada, pp 150-155, http://www.cs.helsinki.fi/ research fdk/datamining/pubs/kdd95.ps.gz [5]. Savasere, A., E Omiecinski and S. Navathe, 1995.An efficient algorithm for mining association rules in large databases. Proceedings of 21 st International VLDB Conference on Very Large Data Bases, Sep. 11-15, Morgan Kaufmann Publishers Inc. San Francisco, CA, USA ., pp:432-444 http://portal/acm.org/citation.cfm?id=673300 [6]. Ramesh C Agarwal, Charu C. Aggarwal and V.V.V. Prasad, 2001. A tree projection algorithm for generation of frequent itemsets. J Parallel Distribut. Comput., 61: 350-371. DOI: 10.1006 jpdc.2000.1693 [7]. Agrawal, R. H. Mannila, R. Srikant, H Toivonen and A.I. Verkamo, 1996. Fast Discovery of Association Rules. In: Advances in Knowledge Discovery and Data Mining, Usama Fayyad, M.G.P. Shapiro, P. Smyth, and R 


Uthurusamy,\(Eds pp: 307 -28 I. SBN:0-262-56097-6 [8]. Aggarwal, C.C. and P.S Yu, 1998. Mining large itemsets for association rules. Bull IEEE Comput. Soc. Technical Committee Data Eng.,: 23-31 http://citeseerx.ist.psu.edu/viewdoc/summary?doi 10.1.1.48.306 [9]. Aggarwal, C.C. and P.S. Yu, 1998. Online generation of association rules. In Proceedings of the 14th International Conference on Data Engineering, Feb. 23-27,IEEE Xplore, Orlando, FL, USA., pp: 402-411. DOI 10.1109/ICDE.1998.655803 [10]. Mohammed J. Zaki, 2000 Scalable algorithms for association mining. IEEE Trans. Knowl Data Eng., 12: 372 390. DOI: 10.1109/69.846291. [11]. Shenoy, P., J. Haritsa, S. Sudarshan, G Bhalotia, M. Bawa and D. Shah, 2000. Turbo-charging vertical mining of large databases. Proceeding of ACM SIGMOD International Conference on Management of Data, June 2000, Dallas, Texas USA,pp:22-33.http://doi.acm.org/10.1145/ 335191.335376 [12 Gunopulos, D., H. Mannila and S. Saluja, 1997. Discovering all most specific sentences by randomized algorithms. In Proceedings of the 6 th International Conference on Database Theory, Jan. 08-10, Springer-Verlag London, UK pp:215-229. http://portal.acm.org/citation.cfm? id=65 6097 [13]. Agrawal, R and R. Srikant, 1994. Fast algorithms for mining association rules Proceedings of the 20th International Conference on Very Large Databases Sep. 12-15, Santiago de chile, Chile, pp: 487-499. DOI: 10.1.1.40.7506. [14 Lin, D.I. and Z.M. Kedem, 1998. Pincer search: A new algorithm for discovering the maximum frequent sets. In Proceedings of the 6 th International Conference on Extending Database Technology, Mar. 23-27, Springer-Verlag London, UK.,pp:105-119. http://portal.acm.org/citation.cfm id=645338.6503 96. [15]. Park, J.S., M.S. Chen, P.S. Yu, 1995. An effective hash based algorithm for mining association rules. ACM SIGMOD Record 24: 175-186. http://doi.acm.org/10.1145/ 68271.223813 [16]. Rin Popescul and Lyle H. Ungar and Steve Lawrence and David M. Pennock 2002. Towards structural logistic regression: Combining relational and statistical learning. In Proceedings of KDD2002 Workshop on Multi-Relational Data Mining 02, ACM, Alberta, Canada, pp: 130-141 http://citeseerx.ist.psu.edu/ viewdoc/summary?doi=10.1.1.19.6235 [17 Taskar, B., E. Segal and D. Koller, 2001. Probabilistic classification and clustering in relational data. In Proceedings of the 17 


th International Joint Conference on Artificial Intelligence 01, Lawrence Erlbaum Associates Ltd USA.,pp:870-876 http://direct.bl.uk/bld/PlaceOrder.do?UIN=107907 71&ETOC=RN&from=searchengine 18]. Dunkel, B. and N. Soparkar, 1999. Dataorganization and access for effcient data mining. In the Proceedings of the 15th International Conference on Data Engineering, Mar. 23-26, IEEE Xplore, Sydney, NSW, Australia, pp 522-529. DOI: 10.1109/ICDE.1999.754968 [19]. Mohammed Zaki, J. and C.J Hsiao, 2002. CHARM: An efficient algorithm for closed itemset mining. In Proceedings of SDM02Conference http://citeseerx.ist.psu.edu/viewdoc /summary?doi=10.1.1.111.520 20]. Bastide, Y., R. Taouil, N. Pasquier, G. Stumme and L. Lakhal, 2000 Mining frequent patterns with counting inference. ACM SIGKDD Explorations Newsletter,2:66-75. http://doi.acm.org/10.1145 /380995 381017 [21]. Pasquier, N., Y. Bastide, R. Taouil and L. Lakhal, 1999 Discovering frequent closed itemsets for association rules. In Proceedings of the 7 th International Conference on Database Theory,Jan. 10-12 Springer-Verlag London, UK., pp:398-416. http://portal acm.org/citation.cfm?id=645503.656256 [22]. Getoor, L., N. Friedman, D Koller and B. Taskar, 2001. Learning probabilistic models of relational structure. In Proceedings of International Conference on Machine Learning ICML'01 177.http://direct.bl.uk/bld/PlaceOrder.do?UIN=100556 121&ETOC=RN&from=searchengine [23]. Zaki, M.J., S Parthasarathy, M. Ogihara and W. Li, 1997. New algorithms for fast discovery of association rules. In Proceeding of the 3 rd International Conference on Knowledge Discovery and Data Mining 97, AAAI Press, pp: 283-286 http://citeseerx.ist.psu.edu/viewdoc/summary?doi 10.1.1.42.5143 [24]. Zaki, M.J., 2000. Generating non-redundant association rules. In Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Aug. 20-23, Boston, Massachusetts, US., pp: 34-43 http://doi.acm.org/10.1145/347090.347101 [25]. Ganter B. and R. Wille, 1999. Formal Concept Analysis: Mathematical Foundations. 1st Edn., Springer-Verlag, USA., pp 284. ISBN-10: 3540627715 


26]. Gouda, K. and M.J. Zaki, 2001. Efficiently mining maximal frequent itemsets. In the Proceedings of International Conference on Data Mining, Nov 29-Dec. 02 2001, IEEE Computer Society Washington, DC, USA., pp 163-170. http://portal.acm.org/citation. cfm?id = 645496.6580 47&coll=GUIDE&dl=GUIDE [27]. Bayardo, R.J., 1998. Efficiently mining longpatterns from databases. In the Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data, Seattle, June 001-04 Washington, United States, pp: 85-93 http://doi.acm.org/10.1145/276304.27631 [28]. Gunopulos, D., H. Mannila and S. Saluja, 1997. Discovering all the most specific sentences by randomized algorithms. In Intenational Conference on Database Theory, Jan 08-10, Springer-Verlag London,UK.,pp:215-229 http://portal.acm.org/citation.cfm?id=645502.6560 97 [29]. Burdick D., M. Calimlim and J. Gehrke, 2001. MAFIA: A maximal frequent itemset algorithm for transactional databases. In International Conference on Data Engineering, Apr. 02-06, IEEE Computer Society Washington, DC, USA pp:443-452 http://portal.acm.org/citation.cfm?id=645484.6563 86&coll=GUIDE&dl=GUIDE. [30]. Agrawal, R., C. Aggarwal and V Prasad, 2000. Depth first generation of long patterns. In the Proceedings of the 6th ACM SIGKDD international Conference on Knowledge Discovery and Data Mining, Aug. 20-23, Boston, Massachusetts, United States, pp: 108-118. http://doi.acm.org/10.1145/347090.347114 31]. A.M.J. Md. Zubair Rahman and P. Balasubramanie,2008 Kongu Engineering College, Perundurai, Tamilnadu, India An Efficient Algorithm for Mining Maximal Frequent Item Sets [32]. Karam Gouda and Mohammed J. Zaki, 2005. GenMax: An Efficient Algorithm for Mining Maximal Frequent Itemsets. In the Proceedings of the Data Mining and Knowledge Discovery, 11, 120, 2005. [33]. Han, J., J. Pei and Y. Yin, 2000. Mining frequent patterns without candidate generation In the Proceedings of the 2000 ACM SIGMOD International Conference on Management o f Data, May 15-18, Dallas, Texas, United States, pp 1-12 http://doi.acm.org/10.1145/342009.335372 


experts can start their ore deposits estimations with much clearer data which is easier to deal with XII. CONCLUSION We have presented how data mining can be applied to the borehole data coming from active mine area. We are certain that data mining has a huge potential for other types of borehole data. One of the important steps in data mining is the preparation of data into useful form for various algorithms. Together with domain experts we have identified a way for transforming data to a form acceptable to k-NN classification and association rules mining algorithms Although we have shown how the k-NN classification and association rules mining techniques can be applied here this framework will open new possibilities to perform other data mining tasks to this type of data. Our experimental results are very promising in this regard as they show that we are not only able to match the accuracy of the results with IDW method used in the mining industry, but also exceed it \(93.1% accuracy was obtained by 3-NN method while IDW gave 88.5% accuracy rules as a separate analysis tool can improve not only k-NN classification results but also IDW interpolation results. This is particularly important for convincing mining companies 119 of the benefits of data mining, as with a little effort they can improve the method they are already using As we have shown in this paper, not only choosing right classification techniques can help us to improve interpolation, but more general analysis on data like association rules can contribute a lot. Thus, the more knowledge we have on the hidden relationships and patterns the more accurately you can construct an interpolation. Moreover, discovering hidden correlations not only important for this task, but also can contribute to understanding about complex geological processes that this specific area undergone. Therefore data mining, which can discover useful knowledge purely from data is of great importance and will be area of research for next generation of exploration and mining specialists Finally, we have proposed to use mathematical morphology for filtering the results of rock type interpolation. In the example given here, we have seen that it performed well in removing relatively small objects and filtering out large 


areas of interest from rock types XIII. FUTURE WORK This paper provides a framework for using data mining techniques on the borehole data. Using this framework we would like to investigate possibilities of using other classification techniques on this type of data. We mentioned that borehole data can contain more information about the area besides spatial coordinates, rock types and metal grades so classification that utilizes this extra information will be on our immediate research agenda Application of mathematical morphology provides possibilities for further research on domaining \(filtering out large areas of interest be given also to this ACKNOWLEDGEMENT I want to thank the Director of the WH Bryan Mining and Geology Research Centre, at the University of Queensland Professor Alan Bye and his PhD student Mr Younes Fadakar Alghalandis for contributing their expertise to this research This work is supported by the AuScope National Collaborative Research Infrastructure Strategy by the Australian Commonwealth, the Queensland State Government and The University of Queensland REFERENCES 1] A. G. Journel and C. J. Huijbregts, Mining geostatistics Academic Press, London, 1978 2] D. Shepard, A two-dimensional interpolation function for irregularly-spaced data, in Proc. ACM Annual Conference 1968, pp. 517524 3] C. Caruso and F. Quarta, Interpolation methods comparison Computers Math. Applications., vol. 35, pp. 109126, 1998 4] K. Gibert, M. S?nchez-Marre`, and I. Rodr?guez-Roda, Short communication: Gesconda: An intelligent data analysis system for knowledge discovery and management in environmental databases, Environ. Model. Softwares, vol. 21, no. 1 pp. 115120, 2006 5] A. R. Solow, Mapping by simple indicator kriging, Mathematical Geology, vol. 18, no. 3, pp. 335352, 1986 6] M. Armstrong, Problems with universal kriging, Mathematical Geology, vol. 16, no. 1, pp. 101108, 1984 7] N. Roussopoulos, S. Kelley, and F. Vincent, Nearest neighbor queries, in Proc. SIGMOD Conference, 1995, pp. 7179 


8] M. Ankerst, H.-P. Kriegel, and T. Seidl, A multistep approach for shape similarity search in image databases, IEEE Trans Knowl. Data Eng., vol. 10, no. 6, pp. 9961004, 1998 9] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft When is nearest neighbor meaningful? in Proc. of Database Theory - ICDT 99, 7th International Conference 1999, pp. 217235 10] P. Ciaccia and M. Patella, Pac nearest neighbor queries Approximate and controlled search in high-dimensional and metric spaces, in Proc. of ICDE, 2000, pp. 244255 11] X. Wu, V. Kumar, J. Quinlan, J. Ghosh, Q. Yang, H. Motoda G. McLachlan, A. Ng, B. Liu, P. Yu, Z.-H. Zhou, M. Steinbach, D. Hand, and D. Steinberg, Top 10 algorithms in data mining, Knowledge and Information Systems, vol. 14, no. 1 pp. 137, 2008 12] X. Cheng, R. Dolin, M. O. Neary, S. Prabhakar, K. Ravikanth D. Wu, D. Agrawal, A. El Abbadi, M. Freeston, A. K. Singh T. Smith, and J. Su, Scalable access within the context of digital libraries, in Proc of ADL, 1997, pp. 7081 13] C. Faloutsos, M. Ranganathan, and Y. Manolopoulos, Fast subsequence matching in time-series databases, in Proc. of SIGMOD Conference, 1994, pp. 419429 14] F. Korn, N. Sidiropoulos, N. Faloutsos, E. Siegel, and Z. Protopapas, Fast nearest neighbor search in medical image databases, in Proc. of VLDB, 1996, pp. 215226 15] T. Kahveci and A. K. Singh, Efficient index structures for string databases, in Proc. of VLDB, 2001, pp. 351360 16] M. Ankerst, G. Kastenmuller, H.-P. Kriegel, and T. Seidl Nearest neighbor classification in 3d protein databases, in Proc of International Conference on Intelligent Systems for Molecular Biology, ISMB, 1999, pp. 3443 17] R. Agrawal and R. Srikant, Fast algorithms for mining association rules in large databases, in Proc of VLDB, 1994 pp. 487499 18] B. Liu, W. Hsu, and Y. Ma, Integrating classification and association rule mining, in Proc of KDD, 1998, pp. 8086 19] J. Serra, Image Analysis and Mathematical Morphology Orlando, FL, USA: Academic Press, Inc., 1983 120 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


