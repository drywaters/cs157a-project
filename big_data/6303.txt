1 User Interestingness for Pre-fetching in Mobile Environment Roziyah Darus 1 Hamidah Ibrahim 2 Mohamed Othman 3 Lilly Suryani Affendey 4 Department of Computer Science Faculty of Computer Science and Information Technology Universiti Putra Malaysia, UPM 43400 Serdang, Selangor 1 roziyah@gmail.com 2 hamidah@fsktm.edu.my 3 mothman@fsktm.edu.my 4 suriani@fsktm.edu.my Abstract„In mobile environment, pre-fetching is used to prevent network congestion, delays and latency problems during disconnection. Lately, the pre-fetching strategies become more complicated in which to support new types of application especially in wireless environments. Due to this complication, new method is introduced where it requires data mining technique to improve the pre-fetching process.  Previously, the pre-fetched data item is filtered using an data driven measurement for data interestingness and the data generated are based on the structure of a query pattern and quantified using statistical methods. The measurement is not good enough to solve the rule quality problems as in predicting next query in mobile environment. In this paper, a new measurement method is proposed to generate new criterion of prefetched data items before sending to mobile users.  This method is improved by introducing user driven measurement for user interestingness where it will consider the subjectivity and the understandability of the users Keywords; User Interestingness, Subjective Measurement Prefetching I I NTRODUCTION In mobile environment pre-fetching is used to prevent network congestion, delays, and latency problems as in   and 16  Mobile ap plications store the predicted data item in their local cache and in the remote server for back up procedure so that mobile users can use in future. Pre-fetching is very effective technique in mobile environment in improving data availability especially during disconnection time. It prevents mobile users from any delay of ongoing job or terminate job due to the disconnection and they are free from needing any re-connection. User interestingness is an important criterion in determining the availability of data items where the expected data items will be useful and significant to mobile users. They will expect that the data that available during disconnection time will be useful and important in finishing their job successfully Interestingness in this case refers to the existence of power to attract users attention because of the exciting or the unexpectedness criterion there are two t y pe s o f  interestingness and measurement categories. The first type is called data interestingness where the data itself are having the criterion to attract users attention. The data items are generated from data driven measurement or objective measurement. The second type of interestingness is called user interestingness where the combination criterion of data and users are required to attract other users attention. In this case the data items are generated from user driven measurement or subjective measurement In this study for the pre-fetching process, not only the data interestingness to be considered but also the user interestingness will be considered. From previous researchers the pre-fetched data items were measured based on the data interestingness The predicted data items are depend on the structure of a query pattern and the data are quantified using statistical methods. The quantity and quality of the data items are controlled by support and confidence metrics. The interestingness is more concerned on the data driven measurement rather than user driven measurement. The measurement is not good enough to solve the rule quality problems as in answering user query in mobile environment   In this study a new prediction method is proposed to predict new criterion of data item before sending to mobile users. The new criterion of pre-fetched data items is not only measured based on data accuracy but also will be measured based on the subjectivity and understandability of previous users The additional metrics used for this measurement are un-expectedness of the pattern and action-ability of the data items. Table 1 shows the difference between the data driven measurement and the user driven measurement The difference between this study and the previous study is the criterion of the pre-fetched data items. The pre-fetched data items are generated from an un-expected pattern. The new items are unknown to the users or contradict to the users existing knowledge 14  The data items are available and can attract other mobile users interestingness in making decision even though they are on moving Proceedings of the 5th International Conference on IT & Multimedia at UNITEN \(ICIMU 2011 14 … 16 November 2011 978-1-4577-0989-0/11/$26.00 ©2011 IEEE 


2 Table 1: The Comparison between Data Driven Measurement and User Driven Measurement Type of Measurement and Description Data Driven Measurement User Driven Measurement It captures the statistical strength of a p attern and the underlying data used in the discovery process only. It is more generic and independent of the application domain. Can generate patterns that are objectively interesting but which are still of little interest to the user 4 Measurement is b ased on the domain knowledge, believes on preferences of the user. More effective at discovering truly novel or surprising knowledge to the user since it explicitly takes into account the users background knowledge Metric Used and Description Metric Used and Description Support Confidence Un-expectedness Action-ability Used for filtering out infrequent rules Support \(X   P \(X  Support is the probability of X and Y happen together, in which X and Y is the transaction set of user queries and the results of the requested data items respectively Measure the implication of relationship from a set of items to one another Confidence X   P \(X 012  Confidence is the probability of X and Y happen together with respect to probability of X in which X and Y are the transaction set of user queries and results of the requested data items respectively The data are unknown to the users or contradict to the users existing knowledge belief/expectations  Using Bayesian Approach In this approach, the degree of    012 015   defined as a conditional probability       holds, given some previous evidence  supporting that belief. Given new evidence E then the update degree of belief in 015    E   where P  E   E          P\(E          E           The interestingness measured for pattern p  I \(p, B   B|P  p   P    P       The users can do something/re-act on the data to their advantage. It is also defined as the extent to which a user can get benefit from the discovered patterns No specific method to measure action-ability Existing measures depend on the applications. For example, measured actionability as the cost of changing the customers current condition to match the objectives action-ability as the profit that an association rule can bring The action-ability of the data items occur when the mobile users can react to their advantage 14  Although the two metrics are independent of each other, majority of actionable patterns are un-expected and majority of un-expected patterns are actionable 3  Therefore, the un-expectedness is a good approximation for action-ability 14 and 2 In this study, we use un-expectedness as the metric to measure the user interestingness. By having this additional criterion of data items, mobile users can use them and proceed with their job without any problem especially during disconnection. The unexpected pre-fetched data items are always available and can avoid them from higher expenses on reconnecting to network and problem with un-finished job I RELATED WORK There are many pre-fetching strategies that have been introduced before. The trend in pre-fetching started with semiautomated hoarding set where they rely on user intervention to some extent 7 and 11 There were based on observing past accessed patterns of users and involve in maintaining the users profiles Another pre-fetching approach was designed and operated in specific environments and focused were on semi-structured data such as for location dependent web pages and location dependent data services   20  Lately, with the advancement of mobile computing technology the pre-fetching techniques in mobile systems become more complicated in which to support new types of applications such as in wireless environmen and  5 Due to this complication, researchers start to introduce new technique where it requires data mining technique. Existing automated of pre-fetching techniques did not focus on the granularity of mobile databases. They focused on the files such as web pages and  9    Th e pre-fetchin g  data item was being measured by the data driven measurement in which the data generated are based on the structure of a query pattern.  The quantity and the quality of generated data are controlled by the support and confidence metric only 


3 Other researchers concentrated on mining tuples in a mobile relational DBMS environment, however their approach can still be further improved to get optimal solutions and 15  They also used data driven measurement where the prefetched data items were computed based on support and confidence in which the pre-fetched data items are not semantically enough for the mobile users especially during disconnection time There are many pre-fetching strategies that have been introduced but from our knowledge, none of them are concerned on the user interestingness for the pre-fetching data items by using user driven measurement II METHODOLOGY In this research, we introduced two methods using user interestingness in belief system concepts called Interestingness Relative to One Global user value, \(IROG and Interestingness Relative to Set of Global user value, \(IRSG These methods not only consider the data interestingness but there will be consideration on the global user interestingness The confidence value is treated as the initial belief of the global users in determining the user interestingness values of the local users. IROG refers to only one confidence value which is the highest value, treated as the initial belief of global users. Whereas IRSG refers to all set of confidence values which are treated as the initial belief of global users. The confidence values and the interestingness values are used to compare between the data interestingness and the user interestingness A TPC-D data schema is used to perform the experiment The data is generated using random generator given by Transaction Processing Council, for Decision Making as given in TPC-D database schema. The data set consist of five regions, twenty five nations and we choose a sample of part brands. We treat the region data as global data set and the nation data as local data set Firstly, we make an assumption by assigning a priority index for each nation and region We assume that mobile users registered their application at local level. Then the nation is assigned as the local level priority index and the region will become the global level priority index For example in this case, users are chosen from China and India from Asia region. Results for data interestingness and user interestingness using IRSG are shown in Figure 1 and Figure 2 A Results Analysis From Figure 1, the confidence values show the expected order pattern and the interestingness values show the unexpected order pattern for part brands. Users from China will expect to order many part brands based on the most frequent ordered part brands before. In descending order of confidence value, after having order part brand B#2, users will expect to order part brands B#9, B#18 and B#10 and so on, i.e. in association rule form, B#2  then B#2  and then B#2   Whereas after taking into consideration user interestingness values, the pattern shows that un-expected ordering pattern of part brands. The initial expected part brands are no longer interested to order. In descending order of the interestingness values show that after having order part brand B#2, the users are un-expected to order other part brands, i.e. B#19 B#13 and B#7 and so on, in association rule form, B#2  then B#2  B#13 and then B#2  Summary of the expected and un-expected patterns for ordering part brands for users from China are shown in Table 2 Table 2: Summary of expected and un-expected pattern in ordering part brands for users from China Measurement Used Types of Interestingness First   part brand to be ordered after B#2 Second part brand to be ordered after B#2 Third  part brand to be ordered after B#2 Data Driven Measurement Data Interestingness B#2  B#2  B#2  User Driven Measurement User Interestingness B#2  B#2  B#2  Similar un-expected ordering patterns occur for users from India as in Figure 2. The confidence values show that there are many part brands that users will expect to order, i.e. after having order part brand B#2, the users will expect to order B#9, B#17, B#15 and so on, i.e in association rule form B#2  then B#2  B#17 and then B#2   Whereas after taking into consideration user interestingness values the pattern shows that the part brands that initially expected to order are no longer interested to order Again, after having order part brand B#2, the users are un-expected to order other part brands, i.e. B#5, B#20 and B#12 and so on, i.e. in association rules form B#2  then B#2  B#20 and then B#2  B#12. Summary of the expected and the un-expected patterns for ordering part brands users from India are shown in Table 3 Table 3: Summary of expected and un-expected pattern in ordering part brands for users from India Measurement Used Types of Interestingness First   part brand to be ordered after B#2 Second part brand to be ordered after B#2 Third  part brand to be ordered after B#2 Data Driven Measurement Data Interestingness B#2  B#2  B#2  User Driven Measurement User Interestingness B#2  B#2  B#2  


4 Figure 1:  Comparison of Data Interestingness and User Interestingness Local Users in China Figure 2:  Comparison of Data Interestingness and User Interestingness  Local Users in India The implementations are also applied to other users in other nations to see the interestingness. By using IROG, the total confidence values for data interestingness and total interestingness values for user interestingness are also measured and compared.  Figure 3 shows the result of the data interestingness and the user interestingness B Discussion From Figure 1 and Figure 2, every confidence value in the data driven measurement gave an expected pattern of data interestingness to local users. The values are controlled by the support and the confidence metrics of the global users. By introducing new method in user driven measurement, the interestingness value will give un-expected pattern to local users. By treating the confidence values as the initial degree of belief for local users, it will generate and produce a contradictory and un-expected pattern in ordering part brands as expected by to the global user belief Figure 3 shows the results for data interestingness and user interestingness for all local users in the region. The values of Total Confidence in descending order which are controlled by the support and the confidence metrics gave an expected interestingness pattern based on the most frequent nation for specific ordered part brand. By using these values, the requested part brand from China is the most interesting compared to other nations since users in China is the highest total confidence value After taking into consideration the global user beliefs, an un-expected pattern of nation interestingness is produced. The initial interestingness of users from China is no longer interesting compared to other nations The new pattern using user driven measurement in descending order are contradicted to the initial patterns using data driven measurement.  Summary of data interestingness and user interestingness for the expected and the un-expected patterns for nations in Asia are shown in Table 4 0 0.05 0.1 0.15 0 0.2 0.4 0.6 0.8 1 1.2 B#2->B#3 B#2->B#4 B#2->B#5 B#2->B#6 B#2->B#7 B#2->B#8 B#2->B#9 B#2->B#10 B#2->B#11 B#2->B#12 B#2->B#13 B#2->B#14 B#2->B#15 B#2->B#16 B#2->B#17 B#2->B#18 B#2->B#19 B#2->B#20 Interestingness Confidence Part Brands Confidence, Interestingness by Part Brands Confidence Interestingness 0 0.05 0.1 0.15 0 0.2 0.4 0.6 0.8 1 Interestingness Confidence Part Brands Confidence, Interestingness by Part Brands Confidence Interestingness 


5 Figure 3:  Comparison of   Data Interestingness and User Interestingness local users in Asia Table 4: Summary of expected and un-expected pattern for users in Asia region Measurement Used Types of Interestingness First interesting nation Second interesting nation Third interesting nation Data Driven Measurement  Data Interestingness China Indonesia Japan User Driven Measurement  User Interestingness Indonesia India Japan III C ONCLUSION  By taking into consideration user interestingness, we managed to generate an un-expected pattern for next interesting items request or next query request for local users The un-expected request data items or queries will be more interesting to the users especially if this technique is applied to mobile users. The data items will be pre-fetched during the connection time and will be updated or refreshed when the mobile users are reconnected. Further experiment will be carried out to show that, by taking into consideration the user interestingness values into pre-fetching process in mobile environment, a significant performance will be expected in terms of better cache hit ratios compared to pre-fetching without taking consideration of the user interestingness values IV ACKNOWLEDGEMENT We greatly appreciate the financial support and encouragement from The University of Fundamental Research Grants \(UPM R EFERENCES  Abhinav, A.V 2005 Data Stashing Strategies fo r Disc onnec ted an d Partially Connected Mobile Environments.Doctoral Thesis, RMIT University.http://citeseerx.ist.psu.edu/viewdoc/summary doi=10.1.1.112.308  Ahmed,S.A 2004 Su bjective Meas ures an d  Their Role in Da ta Mining Process. University of Delhi In Proceedings of the 6th International Conference on Cognitive Systems http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.90.8183  Avi, S  Alexa nder,T 1996 What Mak es P atterns Intere sting in Knowledge Discovery Systems IEEE Transactions on Knowledge and Data Engineering Vol.8, No. 6, pp. 970-974. doi: 10.1109/69.553165  Carl D T., Hui, L  Swarup, A., & Henry  C  199 5  I n t elligent File Hoarding for Mobile Computers In Proceedings of the Ist Annual International Conference on Mobile Computing and Networking Berkeley, USA pp 119-125 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.6487  Darus R Ibrahim H 2 010 Prediction Model fo r Pre-f etchi ng in Mobile Database. In Proceedings of the 12thInternational Conference onInformation Integration and Web-basedApplications & Services iiWAS2010 Paris, France, pp 938-942 doi>10.1145/1967486.1967653  Deborah R Alex,  A Nelson, E. \(2005 Evaluating the Correla tion Between Objective RuleInterestingness Measures and Real Human Interest In Proceedings of PKDD 2005, LNAI 3721 pp. 453-461 doi=10.1.1.102.1758  Ge of frey H., Kuenning Gerald, J  P 199 7 A utomated Hoard ing fo r Mobile Computers, SOSP-16 10197 Saint-Malo France ACM pp. 264275. doi: 10.1.1.15.9270  Ho,S.K. &Hwan,S.Y. \(2004 A ssociation Based Pr e-fetching Alg orithm in Mobile Environments Springer-Verlag Berlin Heidelberg pp. 243 250. http://www.springerlink.com/content/9ek2a8g65breu15k  Hui, S. & Gu o hon g, C 200 4 Miss I nit iated Prefetch in Mo bile Environments In Proceedings of the International Conference on Mobile Data Management, IEEE Vol. 28, Issue 7, pp. 741-753.doi  ieeecomputersociety.org/10.1109/MDM.2004.1263086  Hy eonch eol, K Eun,Y.K 2005 ormation-Based Pruning fo r Interesting Association Rule Mining in the Item Response Dataset KES LNAI 3681, © Springer-Verlag Berlin Heidelberg pp. 372-378. doi 10.1007/11552413_54  J ame s J K  Maha dev, S. \(1992 isconnected  O p eration in the Coda  File System ACM Transactions on Computer Systems Vol. 10, Issue 1 0.91 0.92 0.93 0.94 0.95 95 100 105 110 115 Vietnam China Japan Indonesia India Total Interestingness Total Confidence Total Confidence, Total Interestingness by Nations Total confidence \(OM Total Interestingness\(SM 


6 pp 3-25 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.448  L ing, C Chen T Ya ng Q  Chen J  2 002 Mining O ptimal Ac tions for Profitable CRM. In Proceedings of the 2002 IEEE International Conference on Data Mining, Maebashi City, Japan, pp. 767…770 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.8041  L i  Yi-jun L v Ying-jie 2007 Research on Diff erent Cu stomer Purchase Pattern Based on Subjective Interestingness In Proceedings of the International Conference on Management Science & Engineering ICMSE 2007 pp 38.http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4421816  L iu B H s u W., Ch en S  Ma Y 2000 Analy zing the Su bjective Interestingness of Association Rules In Intelligent Systems and their Applications, IEEE,Sep/Oct 2000 Pp.  47-55. doi  10.1109/5254.889106  Mariano, C.T.N A na, C.S 200 6 Hoard ing and Prefetching fo r Mobile Databases. In Proceedings of the 5th IEEE/ACIS International Conference on Computer and Information Science and 1st IEEE/ACIS International Workshop on Component-Based Software Engineering Software Architecture and Reuse IEEE, pp. 219-224 http://doi.ieeecomputersociety.org/10.1109/ICIS-COMSAR.2006.44  Mary  M. J   F I l a y araja N., &Nadaraja n R 2 009 ach e p re-f etch and replacement with dual valid scopes for location dependent data in mobile environments International Conference on Information Integration and web-based Applications and Services. In Proceedings of the 11th International Conference on Information Integration and Webbased Applications & Service s, pp 364-371 http://portal.acm.org/citation.cfm?id=1806404  Peter P W   200 8  Interesting  Association Rules using  Genetic Algorithm International Journal of Computing and ICT Research Vol. 2, No. 1, pp. 26-33.  http://www.ijcir.org/volume2number1/article4.pdf  Raab, F.\(199 5 T PC Benchmark  TM D\(decision support ard specification, revision 2.1. Transaction Processing Performance Council  Shi,M.H., Binshan,L  Qun,S.D. \(2005 ligent Cach e Management for Mobile Data Warehouse Systems Journal of Database Management Vol. 16, No. 2, pp. 46-65.   doi: 10.4018/jdm.2005040103  Sang, W.K J oon,M.G Jongw an K., Seokjin I Sangke un L  2008\ption-Based Semantic Pre-fetching Scheme for Data Management in Location-Based Services. Journal of Information Science and Engineering Vol. 24, Issue 6, pp. 1799-1820 http://www.iis.sinica.edu.tw/page/jise/2008/200811_12.pdf  Wang, K Zho u S    Han J 200 2 Pro fit Mining: Fro m Patte r ns to Actions. In Proceedings of the 8th Conference on Extending Database Technology, Prague, Czech Republic, © Springer-Verlag Berlin Heidelberg pp 70…87 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.7328  Yucel, S., O zgu  r.U  Ahmed, K E 2000 Association Rules fo r Supporting Hoarding in Mobile Computing Environments. In Proceedings of the 10th International Workshop on Research Issues in Data Engineering, IEEE Computing Society Press, pp. 389-401 doi:ieeecomputersociety.org/10.1109/RIDE.2000.836502 


pnewitemarray pnewitemarray=ptempitem>productid ptempitem=ptempitem->next cofnewitemarray=cofnewitemarray+1  else  newdbitemset=new dbitemset item item  customer_id customer_id customer_id  item item    item item  item item  item item   item item   itemsets for each y? { Tk-1 Tk-1} do  // Tk-1 is all frequent k-1-itemsets in t. Tk-1  Tk-1 is a natural join of Tk-1 and Tk-1 on the first k-2 items 


if? z| z=k-1 subset of y ? ?Hk-1.hassupport\(z then Hk.add\(y itemsets=itemsets? y end Dk= Dk? t   //such that t contains itemsets only in the set itemsets End Hk.prune\(min_sup IV. TEST CONTRAST The test data are sale data from a real company. There are 5581 affairs and 1559 different fields, in Fig.1 the data structure of improved PHP algorithm, candidates k-itemset Hash are in database Dk Fig.1 the structure of database Dk of improved PHP algorithm  The structure of the node customer_id is struct tid  short customerid struct tid *next struct dbitemset* pdbitemset  The structure of item struct dbitemset  short *pitem struct dbitemset *next  In the structure of dbitemset, pitem is point the field or item of fixed size array. For put out the itemset from array, should know the length k of the itemset , and take out k items combinations from array to gain a itemset. The size of tha array should be proper to fill the itemset, not too much nodes there, to save the memory size. The main program of candidate k-itemset ot database Dk is as follow newdbitemset=new dbitemset pnewitemarray=new short[SIZEOFPITEMARRAY ptempitem=pkitemset; // pkitemset point to the list of candidate k-itemset pnewitemarray=ptempitem->productid;     //product ID is the item of candidate k-itemset newdbitemset->pitem=pnewitemarray 


newdbitemset->next=NULL thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next while\(ptempitem!=NULL  if\(cofnewitemarray<SIZEOFP ITEMARRAY  pnewitemarray pnewitemarray=ptempitem>productid ptempitem=ptempitem->next cofnewitemarray=cofnewitemarray+1  else  newdbitemset=new dbitemset item item  customer_id customer_id customer_id  item item    item item  item item  item item   item item 


  pnewitemarray=new short[SIZEOFPITEMARRAY pnewitemarray=ptempitem>productid newdbitemset->pitem=pnewitemarray newdbitemset->next=NULL thisnewdbitemset->next=newdbitemset thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next   The test result is as Tab TABLE 1 ITEMSETS IN DIFFERENT MINIMUM SUPPORT COUNT The minimu m support count F1 | F| 2 | F| 3 F  4  F 5 Frequent set  7 1559 406 0 0 0 1965 6 1559 1960 0 0 0 3519 5 1559 8605 8 0 0 10172 4 1559 34172 129 1 4 2 35876 It can get the same frequent activities that PHP algorithm and improved PHP algorithm, in Tab.2 we can find that there is inverse ratio between frequent activity and minimum support count, when the minimum support counts decrease 


frequent activities increase, just as the real world. The cost of CPU time compared as Fig.2 of these two algorithms 213 124 89 73 57 53 52 65 0 40 80 120 160 200 240 4 5 6 7 Minimum Support Count T i m e  s e c o n d s  PHP Improved Fig.2 the compare CPU time cost of PHP and improved PHP From Fig.2, as support count decreasing, the CPU running time of these two algorithms gets increasing, but the PHP is much speeder than the other, especially when count in 6. so wen know the more frequent itemsets and their items, the more efficient of the algorithm V. THE APPLICATION After we got the frequent itemset by the improved PHP algorithm, we can generate the strong associate rule that satisfied minimum support and minimum confidence. The 


confidence can be computed by Confidence\(A ? B A | B A  B support_count\(A Support_count\(A  B support_count\(A And then we can use confidence threshold strengthen the association, we use improved PHP algorithm dig X company 2002 year sale data, minimum support is 5, 3-itemset, they are 83,549,915}, {485,558,1290}, {454,1097,1546 83,549,982}, {631,980,1490}, {454,1103,1546 810,1026,1469}, {360,830,1036}, and their none null subset as for frequent 3-item {83,549,915}, we can get 83 ? 549? 915,      confidence = 5/9 = 56 83 ? 915? 549,      confidence = 5/5 = 100 915 ? 549? 83,      confidence = 5/5 = 100 83? 915 ? 549,      confidence = 5/50 = 10 549? 83 ? 915,      confidence = 5/78 = 6 915? 83 ? 549,      confidence = 5/73= 7 If the minimum confidence threshold is 80%, only if 83 ? 915 ? 549and 915 ? 549 ? 83 can generate strong associate rules, after these rules, we can not only arrange the shelf of related goods in pairs or groups, but also combine those goods that have most consumers, promote the consumer the pretermission goods. For example, the stronger associate rule 83 ? 915? 549, from the database field product, we know product_id corresponding product_name, this stronger rule is pillow ? pillow clothe ? bedsheet , if there is someone bought pillow and pillow clothe, then promote good bedsheet is an efficient way of sale VI. CONCLUSION We put Hash candidates k-itemset into affair of Dk, that is itemset include in affair, so CPU running time decreased improved PHP algorithm is more efficiency, we can use this improved PHP algorithm in commercial database digging and other wide usages REFERENCES 1] YANG Xuejun. The application of CRM[J].computer application,2002\(5 2] CHEN Shuangqiu, LIU Dinghong, LI Hongxing, the costumer analysis and design based on data warehouse[J].computer engineering and application,2001\(4 3] Jawei Han and Micheline Kamber,Data Mining:Comcepts and 


Techniques \(2001 4] J.D.Holt,and S.M.Chung, Efficient Mining of Association Rules in Text Databases CIKM99,  Kansas City, USA, pp.234-242, \(Nov.1999 5] J.S.Park, M.S.Chen and P.S.Yu, Using a Hash-Based Method with Transaction Trimming for Mining Association Rules, IEEE Transactions on Knowledge and Data Engineering, Vol.9, No.5 Sept/Oct, 1997 6] S. A. zel and H. A. Gvenir, An Algorithm for Mining Association Rules Using Perfect Hashing and Database Pruning, in: Proceedings of  pnewitemarray=new short[SIZEOFPITEMARRAY pnewitemarray=ptempitem>productid newdbitemset->pitem=pnewitemarray newdbitemset->next=NULL thisnewdbitemset->next=newdbitemset thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next   The test result is as Tab TABLE 1 ITEMSETS IN DIFFERENT MINIMUM SUPPORT COUNT The minimu m support count F1 | F| 2 | F| 3 F  4  F 5 Frequent set  7 1559 406 0 0 0 1965 6 1559 1960 0 0 0 3519 


5 1559 8605 8 0 0 10172 4 1559 34172 129 1 4 2 35876 It can get the same frequent activities that PHP algorithm and improved PHP algorithm, in Tab.2 we can find that there is inverse ratio between frequent activity and minimum support count, when the minimum support counts decrease frequent activities increase, just as the real world. The cost of CPU time compared as Fig.2 of these two algorithms 213 124 89 73 57 53 52 65 0 40 80 120 160 200 240 4 5 6 7 Minimum Support Count T i m e  s e c o n d s  PHP Improved Fig.2 the compare CPU time cost of PHP and improved PHP From Fig.2, as support count decreasing, the CPU running 


time of these two algorithms gets increasing, but the PHP is much speeder than the other, especially when count in 6. so wen know the more frequent itemsets and their items, the more efficient of the algorithm V. THE APPLICATION After we got the frequent itemset by the improved PHP algorithm, we can generate the strong associate rule that satisfied minimum support and minimum confidence. The confidence can be computed by Confidence\(A ? B A | B A  B support_count\(A Support_count\(A  B support_count\(A And then we can use confidence threshold strengthen the association, we use improved PHP algorithm dig X company 2002 year sale data, minimum support is 5, 3-itemset, they are 83,549,915}, {485,558,1290}, {454,1097,1546 83,549,982}, {631,980,1490}, {454,1103,1546 810,1026,1469}, {360,830,1036}, and their none null subset as for frequent 3-item {83,549,915}, we can get 83 ? 549? 915,      confidence = 5/9 = 56 83 ? 915? 549,      confidence = 5/5 = 100 915 ? 549? 83,      confidence = 5/5 = 100 83? 915 ? 549,      confidence = 5/50 = 10 549? 83 ? 915,      confidence = 5/78 = 6 915? 83 ? 549,      confidence = 5/73= 7 If the minimum confidence threshold is 80%, only if 83 ? 915 ? 549and 915 ? 549 ? 83 can generate strong associate rules, after these rules, we can not only arrange the shelf of related goods in pairs or groups, but also combine those goods that have most consumers, promote the consumer the pretermission goods. For example, the stronger associate rule 83 ? 915? 549, from the database field product, we know product_id corresponding product_name, this stronger rule is pillow ? pillow clothe ? bedsheet , if there is someone bought pillow and pillow clothe, then promote good bedsheet is an efficient way of sale VI. CONCLUSION We put Hash candidates k-itemset into affair of Dk, that is itemset include in affair, so CPU running time decreased improved PHP algorithm is more efficiency, we can use this improved PHP algorithm in commercial database digging and 


other wide usages REFERENCES 1] YANG Xuejun. The application of CRM[J].computer application,2002\(5 2] CHEN Shuangqiu, LIU Dinghong, LI Hongxing, the costumer analysis and design based on data warehouse[J].computer engineering and application,2001\(4 3] Jawei Han and Micheline Kamber,Data Mining:Comcepts and Techniques \(2001 4] J.D.Holt,and S.M.Chung, Efficient Mining of Association Rules in Text Databases CIKM99,  Kansas City, USA, pp.234-242, \(Nov.1999 5] J.S.Park, M.S.Chen and P.S.Yu, Using a Hash-Based Method with Transaction Trimming for Mining Association Rules, IEEE Transactions on Knowledge and Data Engineering, Vol.9, No.5 Sept/Oct, 1997 6] S. A. zel and H. A. Gvenir, An Algorithm for Mining Association Rules Using Perfect Hashing and Database Pruning, in: Proceedings of  the Tenth Turkish Symposium on Artificial Intelligence and Neural Networks \(TAINN'2001 Eds Gazimagusa, T.R.N.C. \(June 2001 7] SUN Zhengxi, The theory and technology of Intelligent control M].Beijing Qinghua press, 1997 8] Mitani, Koji, CRM pursues economies of depth [J]. Japanese Journal of Diamond Harvard Business, 1999, \(617  the Tenth Turkish Symposium on Artificial Intelligence and Neural Networks \(TAINN'2001 Eds Gazimagusa, T.R.N.C. \(June 2001 7] SUN Zhengxi, The theory and technology of Intelligent control M].Beijing Qinghua press, 1997 8] Mitani, Koji, CRM pursues economies of depth [J]. Japanese Journal of Diamond Harvard Business, 1999, \(617  


algorithm could not extract the correct hierarchy with 30 assigning five labels incorrectly to the root label. None of the HE algorithms could extract the correct hierarchy in the absence of 40% multi-labels. With 40% and Voting, the number of labels falsely assigned to the root was 13, while with GT it was only three. For BoosTexter, Voting assigned two labels wrongly to the root label in the experiment with TABLE I 20 NEWSGROUPS ALL, -20%, -30% AND -40% RESULTS Measure all 20% 30% 40%ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT A 0.635 0.638 0.429 0.549 0.613 0.633 0.383 0.456 0.596 0.619 0.322 0.412 0.563 0.591 0.255 0.387 F1 0.694 0.696 0.565 0.677 0.675 0.688 0.528 0.604 0.662 0.677 0.469 0.566 0.640 0.657 0.392 0.542 F 0.691 0.692 0.480 0.605 0.671 0.688 0.429 0.507 0.658 0.676 0.364 0.465 0.630 0.652 0.296 0.441 OE 0.221 0.220 0.336 0.222 0.259 0.236 0.387 0.275 0.273 0.259 0.415 0.301 0.301 0.291 0.434 0.316 RL 0.100 0.098 0.124 0.073 0.108 0.110 0.128 0.077 0.098 0.103 0.132 0.079 0.101 0.106 0.135 0.082 C 4.188 4.168 6.080 4.164 4.397 4.446 6.184 4.286 4.246 4.340 6.326 4.328 4.334 4.463 6.397 4.379 AP 0.789 0.790 0.677 0.778 0.774 0.782 0.657 0.758 0.769 0.774 0.645 0.747 0.759 0.764 0.638 0.740 AUPRC 0.775 0.772 0.618 0.749 0.743 0.727 0.581 0.691 0.733 0.722 0.555 0.671 0.715 0.708 0.535 0.660 H-loss 0.103 0.123 0.121 0.094 0.102 0.098 0.124 0.108 0.106 0.103 0.132 0.117 0.115 0.111 0.145 0.122 Wins 1 5 0 3 1 6 0 2 2 6 0 1 2 6 0 1 LCAPD 0 0 0 0 0 0 0 0 0 0 0.12 0 0.05 0 0 0 0.51 0.26 0.17 0 CTED 0 0 0 0 0 0 0 0 0 0 0.14 0 0.07 0 0 0 0.50 0.21 0.21 0 TO* 0 0 0 0 0 0 0 0 0 0 0.11 0 0.05 0 0 0 0.39 0.18 0.15 0 30% removed labels and and six labels in the experiment with 40% removed labels. GT resulted in zero distances in the both cases. Assigning more labels to the root creates more shallow and wider hierarchies \(trivial case as stated before The good hierarchy extraction with ART networks demonstrates the system robustness  even with strongly damaged data the system can rebuild the original hierarchy C. RCV1-v2 Dataset The next experiment was based on the tokenized version of 


the RCV1-v2 dataset introduced in [21]. Only the topics label set consisting of 103 labels arranged in a hierarchy of depth four is examined here. Documents of the original training set of 23,149 were converted to TF-IDF weights and normalized Afterwards the set was splitted in 15,000 randomly selected documents as training and the remaining as test samples In this case, the Voting variant of HE applied to the TTML resulted in the LCAPD, CTED and TO* values 0.12, 0.15 and 0.13, respectively. The corresponding values of the GT variant are 0.05, 0.07 and 0.05. The poor performance of the Voting method is due to the fact that for the TTML only very high threshold values succeed in removing enough noise The Voting results are thus dominated by bad hierarchies extracted for all but the highest thresholds The classification and HE results for this dataset are shown in Table II. ML-ARAM has better performance results on this data set in all points than ML-FAM except for RL being the best of all classifiers in terms of the multi-label performance measures. BoosTexter is the best in terms of all ranking measures For both HE algorithms the distances of BoosTexter are the best, those of ML-FAM second, followed ML-ARAM and ML-kNN. All three distance measures correlate. Interesting is also that for ML-kNN the distance values obtained by both HE methods are almost the same The hierarchy extracted by GT from the TTML has much lower distances values as compared with the hierarchies extracted by both methods from predicted multi-labels. This reflects a specific problem of HE, since only a small fraction of the incorrectly classified multi-labels can prevent building of a proper hierarchy. For example, 16.5% of misassigned labels in the extracted hierarchy are responsible for about 80% of LCAPD calculated from the predictions of MLARAM. This large part of the HE error is caused by only 4% of the test data. Under these circumstances the other distances behave analogically. Most labels were not assigned making them trivial edges, but six labels were assigned to a false branch. This can happen when labels have a strong correlation and in the step Hierarchy Construction of the basic algorithm the parent is not unique in the confidence matrix. BoosTexters results suffer less from this problem because it generally sets more labels for each test sample 


Both HE algorithms behaved similarly on the predictions of the ART networks. They constructed a deeper hierarchy than the original one and wrongly assigned the same 11 labels to the root node. The higher distances come from Voting assigning more labels to the wrong branch. For MLkNN both algorithms again create very similar hierarchy trees, both misassigned 28 labels to the root label. For BoosTexter it was seven with Voting and eight with GT Voting produced a deeper hierarchy here D. WIPO-alpha Dataset The WIPO-alpha dataset1 comprises patent documents collected by the World Intellectual Property Organization WIPO ments. Preprocessing was performed as follows: From each document, the title, abstract and claims texts were extracted stop words were removed using the list from [20] and words were stemmed using the Snowball stemmer [22]. All but the 1%-most-frequent stems were removed, the remaining stems were converted to TF-IDF weights and these were normalized to the range of [0, 1]. Again, TF-IDF conversion and normalization were done independently for the training and the test set. The original hierarchy consists, from top to bottom, of 8 sections, 120 classes, 630 subclasses and about 69,000 groups. In our experiment, only records from the sections A \(5802 training and 5169 test samples H \(5703 training and 5926 test samples 1http://www.wipo.int/classifications/ipc/en/ITsupport/Categorization dataset/wipo-alpha-readme.html August 2009 TABLE II RCV1-V2 RESULTS Measure ARAM FAM kNN BoosT A 0.748 0.731 0.651 0.695 F1 0.795 0.777 0.735 0.769 F 0.805 0.787 0.719 0.771 OE 0.077 0.089 0.104 0.063 RL 0.087 0.086 0.026 0.015 C 11.598 11.692 8.563 5.977 AP 0.868 0.860 0.839 0.873 AUPRC 0.830 0.794 0.807 0.838 H-loss 0.068 0.077 0.097 0.081 Wins 4 0 0 5 LCAPD 0.29 0.22 0.25 0.20 0.34 0.34 0.21 0.18 


CTED 0.32 0.23 0.28 0.22 0.38 0.37 0.24 0.20 TO* 0.27 0.18 0.22 0.17 0.31 0.30 0.21 0.17 document in the collection has one so-called main code and any number of secondary codes, where each code describes a group the document belongs to. Both main and secondary codes were used in the experiment, although codes pointing to groups outside of sections A and H were ignored. We also removed groups that did not contain at least 30 training and 30 test records \(and any documents that only belonged to such small groups 7,364 test records with 924 attributes each and a label set of size 131 In this case, the Voting variant of the HE algorithm applied to the TTML resulted in the LCAPD, CTED and TO* values of 0.13, 0.12 and 0, respectively. GT showed the same values. Remarkable are the TO* distances, which are equal to 0. This is due to the fact that the WIPO-alpha hierarchy contains 16 single-child labels that are not partitioned by the true multi-labels: whenever a single-child label j is contained in a multi-label, so is its child, and vice versa. It is therefore theoretically impossible to deduce from the multilabels which of them is the parent of the other. As a result the HE algorithms often choose the wrong parent, resulting in higher LCAPD and CTED values. TO*, as described above is invariant under such choices The results obtained on the WIPO-alpha dataset are shown in Table III. The classification performance of the ART-based networks on this dataset is slightly worse than that of BoosTexter. Mostly in the terms of OE, RL, C, AP, AUPRC, and H-loss measures BoosTexter is better because its rankings are better and it assigned more labels to each sample. But the ART networks have better HE results because their predicted labels are more consistent with the original hierarchy. MLkNN has the worst classification results and distance values again. The reason for the high relative difference between LCAPD as well as CTED and TO* obtained for the ART networks or BoosTexter as compared to the results of the other datasets is because most of the labels were assigned in the right branch but not exactly where they belong Both HE algorithms extracted the same hierarchy from the predictions of ML-ARAM and a very similar hierarchy for ML-FAM. About 5% labels were assigned wrongly to the 


root label in the hierarchies of the ART networks. For MLTABLE III WIPO-ALPHA\(AH Measure ARAM FAM kNN BoosT A 0.588 0.590 0.478 0.564 F1 0.694 0.691 0.614 0.693 F 0.682 0.682 0.593 0.679 OE 0.052 0.057 0.110 0.042 RL 0.135 0.136 0.056 0.025 C 25.135 25.269 22.380 11.742 AP 0.790 0.785 0.724 0.802 AUPRC 0.720 0.684 0.688 0.762 H-loss 0.090 0.093 0.149 0.079 Wins 1 2 0 6 LCAPD 0.16 0.16 0.17 0.17 0.32 0.38 0.21 0.21 CTED 0.18 0.18 0.19 0.19 0.38 0.53 0.27 0.27 TO* 0.05 0.05 0.07 0.07 0.24 0.32 0.08 0.08 kNN both HE methods wrongly assigned about the half of the labels and about 20% of total labels were assgined to the root label. Here, GT extracted a much worse hierarchy as shown by CTED being 0.15 higher for GT than for Voting For BoosTexter both HE methods built the same hierarchy and no label was wrongly assigned to the root. All extracted hierarchies were one level deeper than the original one Although Voting produced worse hierarchies than GT on two previous datasets, this time its distance values were comparable or even better. In comparison to Voting, GT has higher values for all distances on the multi-labels of MLkNN. Voting has the advantage of being a much simpler method and of being more dataset independent. Still the tree distances have the same ranking order for all classifiers for both HE methods VI. CONCLUSION In this paper we studied Hierarchical Multi-label Classification \(HMC tive was to derive hierarchical relationships between output classes from predicted multi-labels automatically. We have developed a data-mining-system based on two recently proposed multi-label extensions of the FAM and ARAM neural networks: ML-FAM and ML-ARAM as well as on a Hierarchy Extraction \(HE algorithm builds association rules from label co-occurrences 


and has two modifications. The presented approach is general enough to be used with any other multi-label classifier or HE algorithm. We have also developed a new tree distance measure for quantitative comparison of hierarchies In extensive experiments made on three text-mining realworld datasets, ML-FAM and ML-ARAM were compared against two state-of-the-art multi-label classifiers: ML-kNN and BoosTexter. The experimental results confirm that the proposed approach is suitable for extracting middle and large-scale class hierarchies from predicted multi-labels. In future work we intend to examine approaches for measuring the quality of hierarchical multi-label classifications REFERENCES 1] M. Ruiz and P. Srinivasan, Hierarchical text categorization using neural networks, Information Retrieval, vol. 5, no. 1, pp. 87118 2002 2] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni, Incremental algorithms for hierarchical classification, The Journal of Machine Learning Research, vol. 7, pp. 3154, 2006 3] , Hierarchical classification: combining Bayes with SVM, in Proceedings of the 23rd international conference on Machine learning ACM New York, NY, USA, 2006, pp. 177184 4] F. Wu, J. Zhang, and V. Honavar, Learning classifiers using hierarchically structured class taxonomies, in Proceedings of the 6th International Symposium on Abstraction, Reformulation And Approximation Springer, 2005, p. 313 5] L. Cai and T. Hofmann, Hierarchical document categorization with support vector machines, in Proceedings of the thirteenth ACM international conference on Information and knowledge management ACM New York, NY, USA, 2004, pp. 7887 6] C. Vens, J. Struyf, L. Schietgat, S. Dz?eroski, and H. Blockeel Decision trees for hierarchical multi-label classification, Machine Learning, vol. 73, no. 2, pp. 185214, 2008 7] E. P. Sapozhnikova, Art-based neural networks for multi-label classification, in IDA, ser. Lecture Notes in Computer Science, N. M Adams, C. Robardet, A. Siebes, and J.-F. Boulicaut, Eds., vol. 5772 Springer, 2009, pp. 167177 8] M. Zhang and Z. Zhou, ML-kNN: A lazy learning approach to multilabel learning, Pattern Recognition, vol. 40, no. 7, pp. 20382048 2007 9] R. Schapire and Y. Singer, BoosTexter: A boosting-based system for text categorization, Machine learning, vol. 39, no. 2, pp. 135168 


2000 10] K. Zhang, A constrained edit distance between unordered labeled trees, Algorithmica, vol. 15, no. 3, pp. 205222, 1996 11] A. Maedche and S. Staab, Measuring similarity between ontologies Lecture notes in computer science, pp. 251263, 2002 12] G. Carpenter, S. Martens, and O. Ogas, Self-organizing information fusion and hierarchical knowledge discovery: a new framework using ARTMAP neural networks, Neural Networks, vol. 18, no. 3, pp. 287 295, 2005 13] A.-H. Tan and H. Pan, Predictive neural networks for gene expression data analysis, Neural Networks, vol. 18, pp. 297306, April 2005 14] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps, IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 698713, 1992 15] Y. Freund and R. Schapire, A decision-theoretic generalization of online learning and an application to boosting, Journal of computer and system sciences, vol. 55, no. 1, pp. 119139, 1997 16] K. Zhang and T. Jiang, Some MAX SNP-hard results concerning unordered labeled trees, Information Processing Letters, vol. 49 no. 5, pp. 249254, 1994 17] G. Tsoumakas and I. Vlahavas, Random k-labelsets: An ensemble method for multilabel classification, Lecture Notes in Computer Science, vol. 4701, p. 406, 2007 18] K. Punera, S. Rajan, and J. Ghosh, Automatic construction of nary tree based taxonomies, in Proceedings of IEEE International Conference on Data Mining-Workshops. IEEE Computer Society 2006, pp. 7579 19] T. Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization, in Proceedings of the Fourteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1997, pp. 143151 20] A. McCallum, Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering, 1996 http://www.cs.cmu.edu/ mccallum/bow 21] D. Lewis, Y. Yang, T. Rose, and F. Li, RCV1: A new benchmark collection for text categorization research, The Journal of Machine Learning Research, vol. 5, pp. 361397, 2004 22] M. Porter, Snowball: A language for stemming algorithms, 2001 http://snowball.tartarus.org/texts/introduction.html 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


