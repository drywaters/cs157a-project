Online Algorithms for Uploading Deferrable Big Data to The Cloud Linquan Zhang   Zongpeng Li   Chuan Wu   Minghua Chen   University of Calgary  linqzhan,zongpeng  ucalgary.ca  The University of Hong Kong cwu@cs.hku.hk  The Chinese University of Hong Kong minghua@ie.cuhk.edu.hk Abstract This work studies how to minimize the bandwidth cost for uploading deferral big data to a cloud computing platform for processing by a MapReduce framework assuming the Internet service provider ISP adopts the MAX contract pricing scheme We rst analyze the single ISP case and then generalize to the MapReduce framework over a cloud platform In the former we design a Heuristic Smoothing algorithm whose worst-case competitive ratio is proved to fall between 2  1   D 1 and 2\(1  1 e   where D is the maximum tolerable delay In the latter we employ the Heuristic Smoothing algorithm as a building block and design an ef“cient distributed randomized online algorithm achieving a constant expected competitive ratio The Heuristic Smoothing algorithm is shown to outperform the best known algorithm in the literature through both theoretical analysis and empirical studies The ef“cacy of the randomized online algorithm is also veri“ed through simulation studies I I NTRODUCTION Cloud computing is emerging as a new computing paradigm that enables prompt and on-demand access to computing resources As exempli“ed in Amazon EC2 and Linode  cloud pro viders in v est substantially into their data centre infrastructure providing a virtually unlimited sea of CPU RAM and bandwidth resources to cloud users often assisted by virtualization technologies The elastic and on-demand nature of cloud computing assists cloud users to meet their dynamic and uctuating demands with minimal management overhead while the cloud ecosystem as a whole achieves economies of scale through cost amortization Typical computing jobs hosted in the cloud include large scale web applications and big data analytics 4 In such data-intensive applications a large volume of information up to terabytes or even petabytes is periodically transmitted between the user location and the cloud through the public Internet Parallel to utility bill reduction in data centres computation cost control bandwidth charge minimization communication cost control now represents a major challenge in the cloud computing paradigm 6 7 where a small fraction of improvement in ef“ciency translates into millions of dollars in annual savings across the world Commercial Internet access particularly the transfer of big data is nowadays routinely priced by the Internet service This work is supported in part by the Natural Sciences and Engineering Research Council of Canada NSERC and grants from Hong Kong RGC under the contracts HKU 717812 and HKU 718513 978-1-4799-3360-0/14/$31.00 2014 IEEE providers ISPs through a percentile charge model  a dramatic departure from the more intuitive total-volume based charge model as in residential utility billing or the at-rate charge model as in personal Internet and telephone billing 9  10 Speci“cally  i n a  th percentile charge scheme the ISP divides the charge period e.g  30 days into small intervals of equal xed length e.g  5 minutes Statistical logs summarize traf“c volumes witnessed in different time intervals sorted in ascending order The traf“c volume of the  th percentile interval is chosen as the charge volume For example under the 95th-percentile charge scheme the cost is proportional to the traf“c volume sent in the 8208th  95  30  24  60  5  8208  interval in the sorted list  7 10 The MAX contract model is simply the 100 th percentile charge scheme Such percentile charge models are perhaps less surprising when one considers the fact that infrastructure provisioning cost is more closely related to peak instead of average demand Due to both its new algorithmic implications and its economic signi“cance in practice this interesting percentile charge model has soon spawned a series of studies Most of these endeavours examine cost saving strategies and opportunities through careful traf“c scheduling multihoming subscribing to multiple ISPs and inter-ISP traf“c shifting However they model the cost minimization problem with a critical although sometimes implicit assumption that all data generated at the user location have to be uploaded to the cloud immediately  without any delay 10 Consequently  the solution space is restricted to traf“c smoothing in the spatial domain only Real-world big data applications reveal a different picture in which a reasonable amount of uploading delay often speci“ed in service level agreement or SLA is tolerable by the cloud user providing a golden time window for traf“c smoothing in the temporal domain which can substantially slash peak traf“c volumes and hence communication cost An example lies in astronomical data from observatories which are periodically generated at huge volumes but require no urgent attention Another well-known example is human genome analyses where data are also big but not time-sensitive The main challenge of effective temporal domain smoothing lies in the uncertainly in future data arrivals Therefore a practical cost minimization solution is inherently an online algorithm making periodical optimization decisions based on IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 978-14799-3360-0/14/$31.00 ©2014 IEEE 2022 


hitherto input It is again surprising to discover that the online cost minimization for deferrable upload under percentile charging even when de“ned over a single link from one source to one receiver only is still highly non-trivial exhibiting a rich combinatorial structure yet never studied before in the literature of either computer networking or theoretical computer science with an only exception below The only study of the online cost minimization problem under percentile charges that we are aware of is a recent work of Golubchik et al  which focuses e xclusi v ely on the single point-to-point link case The online algorithm they present referred to as Simple Smoothing here is extremely simple and involves evenly smoothing every input across its window of tolerable delay for upload Nonetheless this seemingly straightforward algorithm is proven to approach the of”ine optimum within a small constant under the MAX model In this work we rst design our own online algorithm for a single link also adopting the MAX model in preparation for the MapReduce data processing case Based on the insight that Simple Smoothing ignores valuable information including the maximum volume recorded so far and the current amount of backlogged data and their deadlines we tailor a more sophisticated solution which incorporates a few heuristic smoothing ideas and is hence referred to as Heuristic Smoothing We prove that Heuristic Smoothing always guarantees a competitive ratio no worse than that of Simple Smoothing under any possible data arrival pattern Theoretical analysis shows that Heuristic Smoothing can achieve a worst-case competitive ratio between 2  1  D 1 and 2\(1  1 e   where D is the tolerable delay We further extend the single link case to a cloud scenario where multiple ISPs are employed to transfer big data dynamically for processing using a MapReduce-like framework Data are routed from the cloud user to mappers and then reducers both residing in potentially different data centres of the cloud  W e apply Heuristic Smoothing as a plug-in module for designing a distributed and randomized online algorithm with very low computational complexity The competitive ratio guaranteed by the randomized online algorithm increases from that of Heuristic Smoothing by a small constant factor Extensive evaluations are conducted to investigate the performance of the proposed online algorithms The results show that Heuristic Smoothing performs much better than Immediate Transfer ITA a straightforward algorithm that ignores temporal smoothing Meanwhile Heuristic Smoothing also achieves smaller competitive ratios than Simple Smoothing does In most cases tested the observed competitive ratio of Heuristic Smoothing is smaller than 1  5  better than the theoretical upper bound and relatively close to the of”ine optimum Such superior performance is attributed to less abrupt responses to highly volatile traf“c demand Empirical studies for the cloud scenario further verify the ef“cacy of the randomized cost reduction algorithm in terms of both scalability and competitive ratio In the rest of this paper we discuss related work in Sec II and introduce the system model in Sec III Heuristic Smoothing and the randomized algorithm for the cloud scenario are designed and analyzed in Sec IV and Sec V respectively Evaluation results are in Sec VI Sec VII concludes the paper II R ELATED W ORK Similar to deferring data upload to minimize the peak bandwidth demand there have been studies on scheduling CPU tasks to minimize the maximum CPU speed that is closely related to the power consumption Yao et al  initially provide an optimal of”ine algorithm the YDS algorithm to optimally minimize power consumption by scaling CPU speed under the assumption that the former is a convex function of the latter Bansal et al  further propose the BKP algorithm with a competitive ratio of e  for minimizing the maximum speed when facing arbitrary inputs with different delay requirements and arbitrary workload patterns Towards new challenges brought by the proliferation of multi-core processors Albers et al  design an online algorithm for multi-processor job scheduling without interprocess job migration Bingham et al  and Angel et al  further propose polynomial-time of ine optimal algorithms with migration of jobs considered Greiner et al  generalize a c competitive online algorithm for a single processor into a randomized cB  competitive online algorithm for multiple processors where B  is the  th Bell number Different from the MAX traf“c charge model in this work they focus on the total volume based energy charges computed by integrating instantaneous power consumption over time In recent years data centre workload scheduling with deadline constraints has been extensively studied in the cloud computing literature Gupta et al  analyze the ener gy minimization problem in a data center when available deadline information of the workload may be used to defer job execution for reduced energy consumption Yao et al 18 tackle the power reduction problem with deferrable workloads in date centers using the Lyapunov optimization approach for approximate time averaged optimization A few studies exist on the transfer of big data to the cloud Cho et al  design a static cost-a w are planning system for transferring large amounts of data to the cloud provider via both the Internet and courier services Considering a dynamic transfer scheme where data is produced dynamically Zhang et al  propose tw o online algorithms to minimize the total transfer cost Different from this work they assume mandatory immediate data upload and adopt a total volume based charge model instead of the percentile charge model Goldenberg et al  study the multihoming problem under 95-percentile traf“c charges Grothey et al  in v estigate a similar problem through a stochastic dynamic programming approach They both leverage ISP subscription switching for traf“c engineering so that the charge volume is minimized However data traf“c in their studies cannot be deferred Adler et al  focus on careful routing of data traf c between tw o types of ISPs Average contract Maximum contract to pursue the optimal online solution leading to an online optimization problem similar to the classic ski-rental problem Golubchik IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2023 


et al  study the minimization of transmission cost by exploiting a small tolerable delay when ISPs adopt a 95percentile or MAX charge model focusing on a single link only and proposing the Simple Smoothing algorithm III S YSTEM MODEL We consider a cloud user who generates large amounts of data dynamically over time required for transfer into a cloud or a federation of clouds for processing using a MapReducelike framework The mappers and reducers may reside in geographically dispersed data centres The big data in question can tolerate bounded upload delays speci“ed in their SLA A The MapReduce Framework MapReduce initially unveiled by Dean and Ghemawat is a programming model targeting at ef“ciently processing large datasets in parallel A typical MapReduce application includes two functions map and reduce  both written by the users Map processes input key/value pairs and produces a set of intermediate key/value pairs The MapReduce library combines all intermediate values associated with the same intermediate key I and then passes them to the reduce function Reduce then merges these values associated with the intermediate key I to produce smaller sets of values There are four stages in the MapReduce framework pushing  mapping  shuf”ing  and reducing  The user transfers workloads to the mappers during the pushing stage The mappers process them during the mapping stage and deliver the processed data to the reducers during the shuf”ing stage Finally the reducers produce the results in the reducing stage In a distributed system mapping and reducing stages can happen at different locations The system will deliver all intermediate data from mappers to reducers during the shuf”ing stage and the cloud providers may charge for inter-datacentre traf“c during the shuf”ing stage Recent studies 23 suggest that the relation between intermediate data size and original data size depends closely on the speci“c application For applications such as n gram models intermediate data size is much bigger and the bandwidth cost charged by the cloud provider cannot be neglected We use  to denote the ratio of original data size to intermediate data size B Cost Minimization for MapReduce Applications We model a cloud user producing a large volume of data every hour as exempli“ed by astronomical observatories As shown in Fig 1 the data location is multi-homed with multiple ISPs for communicating with data centers Through the infrastructure provided by ISP i  data can be uploaded to a corresponding data centre DC i  Each ISP has its own traf“c charge model and pricing function After arrival at the data centers the uploaded data will be processed using a MapReduce-like framework Intermediate data need to be transferred among data centers in the shuf”ing stage Towards a general model we again assume that multiple ISPs are employed by the cloud to communicate among its distributed data centers e.g  ISP A for communicating between DC1 and DC2 and ISP B for communicating between DC1 and DC3 If two inter-DC connections are covered by the same ISP it can be equivalently viewed as two ISPs with identical traf“c charge models    User Location DC 1 DC 2 DC 3   DC 1 DC 2 DC 3  Data Sources Mappers Reducers Fig 1 An illustration of the network for deferrable data upload under the MapReduce framework The system runs in a time-slotted fashion Each time slot is 5 minutes The charge period is a month 30 days M and R denote the set of mappers and the set of reducers respectively Since each mapper is associated with a unique ISP in the rst stage we employ m  M to represent the ISP used to connect the user to mapper m  All mappers use the same hash function to map the intermediate keys to reducers  The upload delay is de“ned as the duration between when data are generated to when they are transmitted to the mappers We focus on uniform delays i.e  all jobs have the same maximum tolerable delay D  which is reasonable assuming data generated at the same user location are of similar nature and importance We use W t to represent each workload released at the user location in time slot t  Let x m d,t be a decision variable indicating the portion of W t assigned to mapper m at time slot t  d  The cost of ISP m is indicated by f m  V m   where V m is the maximum traf“c that goes through ISP m at time slot t  To ensure all workload is uploaded into the cloud we have 0  x m d,t  1   m  M 1  m D  d 0 x m d,t 1   t 2 Given the maximum tolerable uploading delay D  the traf“c V t m between the user and mapper m is V t m  D  d 0 W t  d x m d,t  d   m  M 3 Let V m be the maximum traf“c volume of ISP m  which will be used in the calculation of bandwidth cost V m satis“es V m  V t m  0   t 4 We assume that ISPs in the rst stage connecting user to mappers employ the same charging function f m  and ISPs in the second stage from mappers to reducers use the same charging function f m,r  Both charging functions f m and f m,r are non-decreasing and convex We further assume that the rst stage is non-splittable i.e  each workload is uploaded IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2024 


through one ISP only The user decides to deliver the workload to mapper m in time slot t  Assume it takes a unit time to transmit data via ISPs Let M t 1 m denote the total data size at mapper m in time slot t 1  M t 1 m can be calculated as the summation of all transmitted workloads at time slot t  M t 1 m  D  d 0 W t  d x m d,t  d   m  M Assume the mappers take 1 time slot to process a received workload Therefore the mappers will transfer data to the reducer in time slot t 2  Let T t 2 m,r be the traf“c from mapper m to reducer r is in time slot t 2  V t 2 m,r  M t 1 m y t 2 m,r   m  M r  R 5 The maximum traf“c volume of the ISP  m r   V m,r  satis“es V m,r  V t 2 m,r  0   t 6 Notice that the MapReduce framework partitions the output pairs key/value of mappers to reducers using hash functions All values for the same key are always reduced at the same reducer no matter which mapper it comes from Furthermore we assume that data generated in the data locations are uniformly mixed therefore we have y t 2 m,r  z r   m  M r  R 7 This equation also implies that the superscript of y t 2 m,r can be ignored Now we can formulate the overall traf“c cost minimization problem for the cloud user under the MAX contract charge model minimize  m f m  V m   m,r f m,r  V m,r  8 subject to V m  V t m  0   t m 8 a  V m,r  V t m,r  0   t m r 8 b  D  d 0 x d,t m  n m   t m 8 c   m n m 1  8 d  0  x d,t m  1 n m  0  1    m 8 e   where V t m and V t m,r are de“ned in Eqn 3 and Eqn 5 respectively n m is a binary variable indicating whether ISP m is employed or not For ease of reference notations are summarized l in Tab I IV T HE S INGLE ISP C ASE We rst investigate the basic case that includes one mapper and one reducer only co-located in the same data center with no bandwidth cost between the pairs Given a MAX charge model at the ISP the algorithm tries to exploit the allowable delay by scheduling the traf“c to the best time slot within the allowed time window for reducing the charge volume This can be illustrated through a toy example in t 1 a TABLE I N OTATION  Symbol De“nition D the maximum delay from the time data is generated to the time the data location begins to transmit it to the mappers M the set of mappers R the set of reducers Some mapper and reducer may be in the same location i.e  M  R     W t the workload released in user location at time slot t  x m d,t the portion of the workload W t that is assigned to mapper m at time slot t  d   the ratio of the size of output of a mapper to the size of its input y t m,r the portion of the output of mapper m that is transmitted to reducer r at time slot t  z r the portion of the key space mapped into reducer r  V t m the total traf“c that goes through ISP m at time slot t  f m  y  the cost of ISP m for the input y  job  100 MB  max delay  9 time slots is released in the following time slots no jobs are released If the algorithm smooths the traf“c across the 10 time slots the charge volume can be reduced to 10 MB 5 min  from 100 MB 5 min if immediate transmission is adopted A The Primal  Dual Cost Minimization LPs We can drop the location index  m r  in this basic scenario of one mapper and one reducer locating in the same data centre Note that the charging function f is a non-decreasing function of the maximum traf“c volume Minimizing the maximum traf“c volume therefore implies minimizing the bandwidth cost Consequently the cost minimization problem in our basic single ISP scenario can be formulated into the following primal linear program LP minimize V 9 subject to min  D,t  1   d 0 W t  d x d,t  d  V  t T 9 a  D  d 0 x d,t 1   t T D 9 b  x d,t  0 V  0   d D t T D  9 c  where T 1 T   T D 1 T  D   D 0 D  and x d,t  0   t>T  D  d D Introducing dual variable y and z to constraints 9 a  and 9 b  respectively we formulate the corresponding dual LP maximize T  D  t 1 z t 10 subject to T  t 1 y t  1\(10 a  z t  W t y t  d  0   t T D d D 10 b  y t  0   t T 10 c  z t unconstrainted   t T D 10 d  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2025 


The input begins with W 1 and ends with W T  D  and W T  D 1 0   W T 0 is padded to the tail of the input We use P and D to denote feasible solutions to the primal and dual LPs respectively The optimization in 9 is a standard linear program For an of”ine optimal solution one can simply solve 9 using a standard LP solution algorithm such as the simplex method or the interior-point method B Online algorithms The simplest online solution in the basic one ISP scenario is the immediate transfer algorithm ITA Once a new job arrives ITA transfers it to mappers immediately without any delay Next we analyze the competitive ratio of ITA as compared to the of”ine optimum Theorem 1 ITA is  D 1 competitive Proof Consider the following input  W 0  0  0  0    ITA will process it immediately with bandwidth cost W  However the of”ine optimal algorithm will divide the workload into small pieces W  D 1 W  D 1  W  D 1  0  0  0     feasible within the deadline D  with maximum traf“c volume W  D 1  Competitive ratio   W  W  D 1  D 1 We hence obtain a lower bound on the competitive ratio of ITA D 1  Next we prove D 1 is also an upper-bound Without exploiting any delays ITA provides a feasible solution to the primal problem which is denoted as P ITA  P ITA max t W t Now we design a feasible solution to the dual problem as follows assume  arg max t W t  y t   1   D 1 if t      D 0 otherwise z t   1   D 1 W t if t   0 otherwise D  1 D 1 W  So the competitive ratio is Competitive ratio   P ITA OP T  P ITA D  D 1 Remarks if D 0  i.e jobs are not deferrable the of”ine optimal algorithm degrades into ITA agreeing with the theorem which claims ITA is 1-competitive  D 1=1  ITA is apparently not ideal and may lead to high peak traf“c and high bandwidth cost as compared with the of”ine optimum Golubchik et al  design a cost-a w are algorithm that strikes to spread out bandwidth demand by utilizing all possible delays referred to as the Simple Smoothing Algorithm  Upon receiving a new workload Simple Smoothing evenly divides it into D 1 parts and processes them one by one in the current time slot and the following D time slots as shown in Algorithm 1 Algorithm 1 The Simple Smoothing Algorithm 1 for  1 to T  D do 2 for d 0 to D do 3 x d 1   D 1 4 end for 5 end for Theorem 2  The competitive r atio of Simple Smoothing is 2  1 D 1  Theorem 2 can be proven through weak LP duality i.e  using a feasible dual as the lower bound of the of”ine optimal Simple Smoothing is very simple but guarantees a worst case competitive ratio smaller than 2  Nonetheless there is still room for further improvements since Simple Smoothing ignores available information such as the hitherto maximum traf“c volume transmitted and the current pressure from backlogged traf“c and their deadlines Such an observation motivated our design of the more sophisticated Heuristic Smoothing algorithm for the case D  1  as shown in Algorithm 2 Here T is the charge period  is the current time slot and H d is the total volume of data that have been buffered for d time slots Algorithm 2 The Heuristic Smoothing Algorithm 1 V max 0 2 W  0     T  D 1   T  3 H d 0   d 1   D  4 for  1 to T do 5 V  min  W    D d 1 H d  max  V max  W  D 1   D d 1 H d D   6 if V max V  then 7 V max  V   8 end if 9 Transfer the traf“c following Earliest Deadline First EDF strategy 10 Update H d   d 1   D  11 end for Theorem 3 The competitive ratio of Heuristic Smoothing is lower bounded by 2\(1  1 e   Proof Consider the following input  W W W 0   0 whose rst D 1 time slots are W  The traf“c demand V increases until time slot D 1  V D 1  W D 1  W D 1   D  1 W  D 1 D     D  1 D  1 W  D 1 D D  1  W D 1 1  D 1  1  1 D  D  We can nd a feasible primal solution which yields the charge volume D 1 2 D 1 W  This primal solution is an upper bound of the of”ine optimum Therefore the lower bound of the competitive ratio   2 D 1 V D 1  D 1  2 D 1  D 1 2 1  D 1  1  1 D  D   2\(1  1 e  as D     Notice that IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2026 


2 D 1   D 1 2 1  D 1  1  1 D  D  is a decreasing function for D  1      we further have   2\(1  1 e   Theorem 4 The competitive ratio of Heuristic Smoothing is upper-bounded by 2  1 D 1  Proof We take the Simple Smoothing algorithm Algorithm 2 as a benchmark and we prove that P smooth  P heuristic  where P heuristic is the charged volume produced by Algorithm 3 Algorithm 3 will only increase the traf“c demand when W  D 1   D d 1 H d D exceeds V max  Therefore we rearrange H d to compute the maximum traf“c demand Let V t  D   W t  D D 1  W t  D  1 D 1   D  1 W t  D  2  D 1 D     D  1 D  1 W t  D 1 D D  1  Then P heuristic max t V t  D  Let   arg max t V t  D  and we have P smooth max t t  D  i  t W t D 1    D  i   W  D 1  W   D D 1  W   D  1 D 1   D  1 W   D  2  D 1 D     D  1 D  1 W   D 1 D D  1  P heuristic Since the simple smoothing algorithm is 2  1 D 1  competitive the competitive ratio of Algorithm 3 cannot be worse than 2  1 D 1  From the proof above we have following corollary Corollary 1 For any given input the charge volume resulting from Heuristic Smoothing is always equal to or smaller than that of Simple Smoothing Algorithm Complexity All three online algorithms discussed have moderate time complexity making them light-weight for practical applications More speci“cally ITA Simple Smoothing and Heuristic Smoothing have a time complexity of O  T  D   O  T  D  D   and O  TD   respectively V C LOUD S CENARIO In this section we apply the algorithms designed for the single ISP case to the cloud scenario which utilizes a MapReduce-like framework for processing big data De“ne Cost 1   m f m  V m   Cost 2   m,r f m,r  V m,r   and adopt power charge functions by letting f m  x  f m,r  x  x    1  A Algorithm Design The two-phase MapReduce cost optimization problem is de“ned in 8  and is a discrete optimization with integer variables Consequently an of”ine solution that solves such an integer program has a high computational complexity further motivating the design of an ef“cient online solution A native online algorithm selects a xed mapper and schedules the traf“c on the corresponding ISP using the Simple Smoothing Algorithm Theorem 5 The competitive ratio of the native online algorithm is lower bounded by  M    1  where  M  is the number of mappers Proof  Consider the input  W  W 0    0 whose rst D 1 items are W  We can verify that the charge volume is DW 2 D 1  The corresponding cost is  DW 2 D 1     r  z r DW 2 D 1    Next we consider a more intelligent algorithm that assigns the j th workload to the mapper  j mod  M    This algorithm acts as the upper bound of the of”ine optimum Its charge volume is DW 2 D 1  M   The corresponding cost is  M   DW 2 D 1  M      M   r  z r DW 2 D 1  M     Therefore Competitive ratio   DW 2 D 1     r  z r DW 2 D 1    M   DW 2 D 1  M      M   r  z r DW 2 D 1  M      M    1 We next present a distributed randomized online algorithm for 8  For each workload the user chooses ISPs uniformly at random to transfer the data to a randomly selected mapper Formally let WA be the randomized workload assignment allocating each workload to mappers For each selected ISP the user runs Heuristic Smoothing to guide one-stage traf“c deferral and transmission as shown in Algorithm 3 Algorithm 3 Randomized Uploading Scheme 1 Generate a randomized workload assignment WA which allocates each workload to a randomly selected mapper 2 For each ISP m  apply the single ISP algorithm e.g  Algorithm 2 to schedule the traf“c We analyze Algorithm 3 by building a connection between the uploading scheme  and the randomized workload assignment WA  We combine  and WA to a new uploading scheme  WA  Let t 0 1 t 1  t e  T  During each interval  t i t i 1   each ISP is employed to transfer at most one workload in the uploading scheme   If a workload is processed in  t i t i 1   then it cannot be nished before t i 1  Due to the MAX charge model the transfer speed for workload w in  t i t i 1  is a single speed say v i,w  If workload w is not processed in  t i t i 1   we set v i,w 0  Therefore for any given i  there are at most  M  values of v i,w  0  Assume there are n workloads forming a set W  Let  m   w  all workloads assigned to ISP m  W  In scheme  WA  the user transfers data at speed of  w   m v i,w in time interval  t i t i 1   Let  n  m  be the probability that exactly the workloads  m are allocated to ISP m   n  m   1  M     m  1  1  M   n   m  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2027 


We next de“ne function  n  x  where x  R n  0    n  x   M    m  W  n  m    w   m x w    n  w 1 x  w Lemma 1 Given any uploading scheme  and a randomized workload assignment WA  we have a randomized uploading scheme  WA  which satis“es E  Cost 1   WA  Cost 2   WA   max x   M   x   Cost 2    Cost 1    Proof  Since the traf“c pattern in ISP  m r    r is exactly the same as ISP m  we only consider one stage Let us consider scheme  rst In the rst stage the cost is Cost 1     m  M max i,w  v m i,w    max i    M   v  i,w  where v m i,w indicates the transfer speed in ISP m during  t i t i 1  for workload w     M   v  i,w  is the sum of the largest  M  values of v  i,w when given i  The inequality holds because there are at most  M  non-zero speeds for any given duration  t i t i 1   We next have the cost of the second stage Cost 2     m  r max i,w  z r v m i,w       r z  r  m max i,w  v m i,w       r z  r max i    M   v  i,w  The cost of the rst stage in  WA is E  Cost 1   WA    m  M   WA m  W  n  WA m ax i   w   WA m v i,w     M  max i   WA m  W  n  WA m    w   WA m v i,w   The second equality above holds because the assignment is uniformly random Similarly The cost of the second stage in  WA is E  Cost 2   WA    M    WA m P  n  WA m   r max i  z r  w   WA m v i,w     M     r z  r max i   WA m  W  n  WA m    w   WA m v i,w   Again because for any  t i t i 1   there are at most  M  values of v i,w  0 Wehave  M    WA m  W  n  WA m    w   WA m v i,w       M   v  i,w    M    WA m  W  n  WA m    w   WA m v i,w    n w 1  v  i,w   n  v   M   v   where v  is an  M  dimensional subvector of v  R n  0   which contains all non-zero transfer speeds in  t i t i 1   Therefore the ratio for the rst stage is E  Cost 1   WA  Cost 1      M    WA m  W   WA m ax i   w   WA m v i,w   max i    M   v  i,w    M    WA m  W   WA m    w   WA m v i  w      M   v  i  w   max x   M   x  where i  argmax i   w   WA m v i,w    Similarly the ratio for the second stage is also bounded by max x   M   x   i.e  E  Cost 2   WA  Cost 2    012 max x   M   x   This proves Lemma 1 Let S   j  be the j th Stirling number for  elements de“ned as the number of partitions of a set of size  into j subsets Let B  be the  th Bell number de“ned as the number of partitions of a set of size   The Bell number is relatively small when  is small B 1 1 B 2 2 B 3  5 B 4 15  The de“nitions also imply   j S   j  B  The following lemma is proven by Greiner et al  Lemma 2 16    N and  012 M   max x   M   x    j 1 S   j   M    M  j   M  j   Theorem 6 Given a  competitive algorithm with respect to cost for the single ISP case then the randomized online algorithm is B    competitive in expectation Proof  Let   be the optimal uploading scheme the corresponding randomized uploading scheme is   WA  The algorithm we use is  WA  Since the workloads in   WA and  WA are the same we have E  Cost 1   WA    E  Cost 1    WA  11 since the algorithm is  competitive Similarly E  Cost 2   WA    E  Cost 2    WA  12 since the traf“c pattern in ISP  m r    r is exactly the same as in ISP m  Lemma 1 implies E  Cost 1    WA  Cost 2    WA   max x   M   x   Cost 1     Cost 2     13 Since   M   x  is a monotonically increasing function of  weuse    as an upper bound of  1  obtaining a corresponding upper bound of   M   x   Combining Eqn 11 12 and 13 as well as Lemma 2 we have the following expected cost of the randomized online algorithm IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2028 


E  Cost 1   WA  Cost 2   WA    E  Cost 1    WA  Cost 2    WA    max x   M   x   Cost 1     Cost 2           j 1 S     j   M     M  j   M  j   Cost 1     Cost 2           j 1 S     j   Cost 1     Cost 2      B    OP T Remark  For a single link we can employ Heuristic Smoothing whose competitive ratio is smaller than 2 with respect to maximum traf“c volume Then the competitive ratio of Algorithm 2 is 2  in cost Thus Algorithm 3 is 2  B  competitive in expectation When  2  the competitive ratio is 8 a constant regardless of the number of mappers VI P ERFORMANCE E VALUATION We have implemented Simple Smoothing Heuristic Smoothing as well as the randomized online algorithm for performance evaluation through simulation studies The default input W t is generated uniformly at random as shown in Fig 2 where all data are normalized i.e  scaled down by max t W t  We assume there are 5 mappers at different locations and 5 reducers at different locations We choose  2  thus the charge function f m  x  f m,r  x  x 2  A The Single ISP Case First we compare Heuristic Smoothing with Simple Smoothing The two algorithms are executed under a delay requirement D 5  Fig 3 illustrates the traf“c volume scheduled at each time slot Compared with Simple Smoothing Heuristic Smoothing results in a maximum traf“c volume this is about 28 smaller Heuristic Smoothing tries to exploit the available delay to average the traf“c and is less sensitive to the uctuation of traf“c demand as compared with Simple Smoothing For example at around t 10  the traf“c of Simple Smoothing increases abruptly due to high traf“c demand in the input around t 40  it goes down due to low traf“c demand In comparison Heuristic Smoothing results in more even traf“c distributions around t 10 and t 40  Next we examine how the tolerable delay affects the performance of the proposed online algorithms We execute Simple Smoothing Heuristic Smoothing and ITA against a variety of delays ranging from D 0 to D 24  We also compute the of”ine optimum as a benchmark The observed competitive ratios are shown in Fig 4 The results suggest that both Simple Smoothing and Heuristic Smoothing perform much better than ITA Heuristic Smoothing also beats Simple Smoothing by a smaller margin Heuristic Smoothing approaches the of”ine optimum rather closely the observed competitive ratios are always below 1  5 and usually around 1  2  much better than the theoretically proven upper bound in Theorem 4 Heuristic Smoothing is further evaluated under other random inputs including Poisson distribution in Fig 5 Gaussian distribution in Fig 6 and a speci“cally designed random input in Fig 7 All results verify that Heuristic Smoothing works best among the three online cost minimization algorithms B The Cloud Scenario We implemented the randomized algorithm in Algorithm 3 and the native algorithm in Sec V-A They are evaluated under three types of inputs uniform distribution Poisson distribution and Gaussian distribution We compare the costs of the two algorithms using these inputs as shown in Fig 8 Fig 9 and Fig 10 respectively We observe that the randomized algorithm achieves much lower cost than the native algorithm in particular with longer tolerable delays For example Fig 8 shows that the randomized algorithm saves approximately 45 cost as compared with the native algorithm when D 5  and it saves more than 68 when D 10  This suggests that longer tolerable delays provide the randomized algorithm more space of maneuver leading to more evident cost reduce We further investigate the in”uence of   the ratio of original data size to the intermediate data size Results are shown in Fig 11 When D is small a large  causes a rather high cost However when a large D is used e.g  D 40  even a large  only produces a relatively small cost   0 5 10 15 20 0 10 20 30 40 0 0.2 0.4 0.6 0.8 Delay window size  Normalize Cost Fig 11 Relationship between traf“c cost and parameters D    VII C ONCLUSION ISPs now charge big data applications with a new interesting percentile based model leading to new online algorithm design problems for minimizing the traf“c cost paid for uploading big data to the cloud We studied two scenarios for such online algorithm design in this work A Heuristic Smoothing algorithm is proposed in the single link case with proven better performance than the best alternative in the literature and a smaller competitive ratio below 2  A randomized online algorithm is designed for the MapReduce framework achieving a constant competitive ratio by employing Heuristic Smoothing as a building module We have focused on MAX charge rules and leave similar online algorithm design for 95-percentile charge rules as future work R EFERENCES 1 Amazon Elastic Compute Cloud  http://aws.amazon.com/ec2 2 Linode  https://www.linode.com/speedtest 3 Amazon EC2 Case-studies  http://aws.amazon.com/solutions/casestudies IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2029 


   0 20 40 60 80 100 0 0.2 0.4 0.6 0.8 1 Time Normalized Data Traffic   Uniform Input Fig 2 Uniformly Random Input   0 20 40 60 80 100 120 0 0.2 0.4 0.6 0.8 Time Normalized Scheduled Traffic   Simple Smoothing Heuristic Smoothing Fig 3 Simple Smoothing vs Heuristic Smoothing D 10   0 5 10 15 20 25 1 1.5 2 2.5 3   Delay window size Competitive Ratio   ITA Simple Smoothing  Heuristic Smoothing  Fig 4 Competitive ratio over various delay window sizes under input of uniform distribution   0 5 10 15 20 25 1 1.5 2 2.5   Delay window size Competitive Ratio   ITA Simple Smoothing  Heuristic Smoothing  Fig 5 Competitive ratio over various delay window sizes under input of Poisson distribution   0 5 10 15 20 25 1 2 3 4 5   Delay window size Competitive Ratio   ITA Simple Smoothing  Heuristic Smoothing  Fig 6 Competitive ratio over various delay window sizes under input of Gaussian distribution   0 5 10 15 20 25 1 2 3 4 5 6 7 8   Delay window size Competitive Ratio   ITA Simple Smoothing  Heuristic Smoothing  Fig 7 Competitive ratio over various delay window sizes under a speci“cally designed input   5 10 15 20 25 0 0.2 0.4 0.6 0.8 1 Delay window size Normalize Cost   Randomized Algorithm Native Algorithm Fig 8 Comparison between the proposed randomized algorithm and the native algorithm under input of uniform distribution and  2    5 10 15 20 25 0 0.2 0.4 0.6 0.8 1 Delay window size Normalize Cost   Randomized Algorithm Native Algorithm Fig 9 Comparison between the proposed randomized algorithm and the native algorithm under input of Poisson distribution and  2    5 10 15 20 25 0 0.2 0.4 0.6 0.8 1 Delay window size Normalize Cost   Randomized Algorithm Native Algorithm Fig 10 Comparison between the proposed randomized algorithm and the native algorithm under input of Gaussian distribution and  2   E E Schadt M D Linderman J Sorenson L Lee and G P  Nolan Computational Solutions to Large-scale Data Management and Analysis Nat Rev Genet  vol 11 no 9 pp 647…657 Sep 2010  L Golubchik S Khuller  K  Mukherjee and Y  Y ao T o Send or not to Send Reducing the Cost of Data Transmission in Proc of IEEE INFOCOM  2013  L Zhang C W u  Z  Li C Guo M Chen and F  Lau Mo ving Big Data to The Cloud An Online Cost-Minimizing Approach IEEE Journal on Selected Areas in Communications  vol 31 no 12 pp 2710…2721 2013  H W ang H Xie L Qiu A Silberschatz and Y  Y ang Optimal ISP Subscription for Internet Multihoming Algorithm Design and Implication Analysis in Proc of IEEE INFOCOM  2005  S Peak Be yond Bandwidth The Business Case F o r Data Acceleration White Paper  2013  D K Goldenber g L Qiuy  H  Xie Y  R Y ang and Y  Zhang Optimizing Cost and Performance for Multihoming in Proc of ACM SIGCOMM  2004  A Grothe y and X Y ang T op-percentile T raf c Routing Problem by Dynamic Programming Optimization and Engineering  vol 12 pp 631…655 2011  F  Y ao A Demers and S Shenk er   A Scheduling Model for Reduced CPU Energy in Proc of IEEE FOCS  1995  N Bansal T  Kimbrel and K Pruhs Speed Scaling to Manage Ener gy and Temperature J ACM  vol 54 no 1 pp 3:1…3:39 Mar 2007  S Albers F  M  uller and S Schmelzer Speed Scaling on Parallel Processors in Proc of ACM SPAA  2007  B Bingham and M Greenstreet Ener gy Optimal Scheduling on Multiprocessors with Migration in Proc of IEEE ISPA  2008  E Angel E Bampis F  Kacem and D Letsios Speed Scaling on Parallel Processors with Migration in Euro-Par 2012 Parallel Processing  ser Lecture Notes in Computer Science C Kaklamanis T Papatheodorou and P Spirakis Eds Springer Berlin Heidelberg 2012 vol 7484 pp 128…140  G Greiner  T  Nonner  and A Souza The Bell is Ringing in Speedscaled Multiprocessor Scheduling in Proc of ACM SPAA  2009  M A Adnan Y  Ma R Sugihara and R Gupta Dynamic Deferral of Workload for Capacity Provisioning in Data Centers http://arxiv.org/abs/1109.3839  Y  Y ao L Huang A Sharma L Golubchik and M Neely  Data Centers Power Reduction A Two Time Scale Approach for Delay Tolerant Workloads in Proc of IEEE INFOCOM  2012  B Cho and I Gupta Ne w Algorithms for Planning Bulk T ransfer via Internet and Shipping Networks in Proc of IEEE ICDCS  2010  M Adler  R  K  Sitaraman and H V enkataramani  Algorithms for Optimizing the Bandwidth Cost of Content Delivery Comput Netw  vol 55 no 18 pp 4007…4020 Dec 2011  J Dean and S Ghema w at MapReduce Simpli“ed Data Processing on Large Clusters Commun ACM  vol 51 no 1 pp 107…113 Jan 2008  S Rao R Ramakrishnan A Silberstein M Ovsiannik o v  and D Reeves Sail“sh A Framework for Large Scale Data Processing Yahoo!Labs Tech Rep 2012  B Heintz A Chandra and R K Sitaraman Optimizing MapReduce for Highly Distributed Environments Department of Computer Science and Engineering University of Minnesota Tech Rep 2012  H Beck er and J Riordan The Arithmetic of Bell and Stirling numbers American journal of Mathematics  vol 70 no 2 pp 385…394 1948 IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2030 


212\212 t 007 n\t 212 212 005 327 212\212  arg 1  n x ett x x ExpxF   dse t EOL F us s use total 2 2 2 0 63 2 1 exp1  max min 14 b 007 327\212\327 327 212 5.0 1 63 s V Exp A A Ct dd ref p use 007 13  spacing fairly well. However, the sigma in the space distribution function could vary with different products, which is the most  uncertain source in this failure rate calculation method. Therefore, a careful evaluation of the peak failure rate spacing with different sigma values is extremely critical for reliability engineers. Carefully determining a MinIns to assure that product reliability can be kept away from a massive fallout condition is very useful  11   12   Figure 28: Probability associated TDDB concept  Figure 29: Combination of Vbd and TDDB big data to determine field acceleration factor with many t63 data points   Figure 30: Calculated failure rate based on Equations 9 and 10. All cases show a critical spacing with the highest failure rate  Figure 31: Examples showing the peak failure rate spacing is insensitive to sigma and mean change while peak failure rate is a strong function of both sigma and mean change On the other hand, the spacing may not be the only parameter to determine the overall TDDB. Based on Equation 13, it is well known that a Weibull CDF is a function of t63 or V63 and As a t63 or V63 distribution and an intrinsic  versus t63 or V63 relation can be exclusively determined by our proposed big data method discussed above, naturally a superposition based approach can be directly applied to calculate the realistic chip level failure rate without a bridge to the spacing. As t63 and V63 are determined by all relevant breakdown parameters, not just spacing, therefore this method should be a universal method for chip level failure rate calculation. The concept also can be applied to all reliability failure mechanisms including BEOL EM, BEOL SIV, and FEOL gate dielectric with either Lognormal or Weibull statistics. The total failure rate summing from all chips with different t63 or V63 values and correspondingly different probabilities over the probability density function could be described by Equations 14 and 15  3A.1.10    dxxFxPxF x x total 


003 003\003 t n\t 1 2 2 2 2 2 1 15  where n is the area scaling ratio, x could be either t63 or V63 k is the shape parameter and is the scale parameter of the Weibull distribution. For VRDB and TDDB, using P\(x\ in a Weibull format is recommended. As in Weibull, the distribution shape could be preserved after area scaling, which was supported by some experimental data. Also using P\(x\ in Weibull format generates a more conservative failure rate number as compared to P\(x\ in Normal format. However regardless of which P\(x\ format is used, generally, the failure rates obtained by Equations 13-15 are much smaller as compared to the numbers calculated by the traditional method Table 1 and Figure 30 illustrate one lowk TDDB with k 2.55 and 64nm pitch interconnect as an example. With our new proposed method, the summation of failure rates from chips with t63=0 hour to chips with t63=100000 hours is significantly smaller than the single failure rate produced by the traditional method. Therefore, the E max for a realistic reliability can be significantly lifted up  Table 1 Method  vs. t63 used for FR calculation t63 Failure Rate at End of Life A.U Traditional 0.62 No fixed 3050 New 1.69 Yes Varied 6.01     Figure 32: An example of calculated failure rate versus t63 based on Equations 12-14 for P\(x\ith a Weibull format   IV  D ISCUSSION AND C ONCLUSIONS  In conclusion, a new big data \(aggregation of stress data diagnostics data, simulation data, and yield data\eneration and analytics method is proposed to address MOL PC-CA and BEOL lowk TDDB challenges. With this new method without the introduction of any new TDDB acceleration model, reliability can be met with good confidence for various processes based on the square-root of E model. Since BEOL lowk TDDB and MOL PC-CA TDDB are so sensitive to processing and to structural layout, different processes and different structure layouts could potentially require different TDDB models. The breakdown mechanisms at different stress voltage regions could also be different. Therefore, unless we perform long-term TDDB stresses to validate a TDDB model at all situations such as material change at different technology nodes, different lowk used at different levels, and critical process changes, the question about the correct TDDB model would still exist from different stress voltage regions, from different test structure layouts, and from one technology node to another.  Alternatively, restoring a true Weibull slope by our data deconvolution is an easier way to improve overall TDDB projection. Without any long-term TDDB stresses, a real and improved failure rate projection can be quickly established. Furthermore, wafer processing can also be carefully diagnosed and meaningfully compared with our proposed big data generation and analytics method. A new diagnostics reliability concept is naturally embedded in our method. Therefore in addition to a simple \223pass\224 or \223fail\224 reliability judgment for qualification and process development, a precise reliability failure root cause analysis with a potential process fix guideline can also be provided by our method. As a consequence, a huge cost and time saving for new technology development and qualification can be achieved. Lastly, as mentioned above, our new method could be a prerequisite to developing a reliable TDDB acceleration model if significant die-to-die variation is present in our stress data A CKNOWLEDGMENT  The authors wish to acknowledge all the people working within IBM alliance programs for 32nm SOI and Bulk CMOS technology development. This work has been supported by the independent Bulk CMOS and SOI technology development projects at the IBM Microelectronics Division, Semiconductor Research & Development Center, Hopewell Junction, NY 12533 R EFERENCES  1 S  Y o ko g a w a S  U n o  I   K a to H   T s uch i y a T  S h im iz u an d M S a kam o to  49 th Annu. IEEE Rel. Phys Symp, 2011, pp. 149-154 2 F  C h e n  M  S h i n o s k y an d J. A i t k e n I EEE T r an s. o n El e c tr o n D e v i ce s v58, no 9, pp. 3089-3098, 2011   F e ll er  A n n   M a th Sta t   v 1 4  pp 389 40 0  1 943  4 A  T y ag i an d  M  A  B a y o u m i  I EEE T r an s o n Se mico n d u c to r  Manufacturing, v5, no 3, pp. 196-206, 1992 5 F  Ch e n e t  al I EEE I R PS 2 0 1 3  P I 1  1 1  5  6 K  Y  Y i a n g  e t al I EEE I R PS 2 0 1 0  p p  5 6 2 5 6 5      7 F  Ch e n e t  al I EEE I R PS 2 0 1 2  p p  6 A 4  1  6 A 4  9     212\212  212 k k mean x Exp xk xP or xx Exp xP    3A.1.11 212 


Figure 7  Portfolios of hosted payload and procured assets been presented in order to gain some level of con\002dence on the results that the tool outputs This discussion has focused on the validation of the scheduling algorithm by benchmarking it with operational schedules from the TDRSS Finally this paper has presented two case studies for future implementations of the TDRSS system Based on a demand forecast for the network a 002rst case study has considered the problem of selecting the frequency band to be supported and how to allocate them into the relay satellites It has been shown that maximum performance architectures require a mix of optical and RF payloads that support high throughput communications as well as reliable low data rate communications If the traditional procurement strategy is assumed 050NASA buys and operates the relay satellites\051 then monolithic architectures are preferable unless more than two high gain antennas render the resulting satellite con\002guration too complex In turn the second case study has extended this analysis by introducing architectures with hosted payloads It has been shown that according to the current available pricing model hosted payload architectures are clearly preferable than the traditional procurement strategy with cost savings between 15 to 30 for the same level of system performance It has then been discussed the advantages of having mixed procured and hosted payload architectures as a compromise to obtain networks with reduced lifecycle costs that can still address the requirements of highly sensitive and reliable applications Results have demonstrated that high data rate payloads 050speci\002cally optical payloads\051 are the best candidates to be hosted 050with savings up to 28%\051 thanks to their reduced mass and power requirement On the other hand low data rate communication payload should be allocated in privately owned satellites as they require bigger antennas and power ampli\002ers that increase the burden on the host platform Future Work The main streams of future work are as twofold On one hand additional features should be added to the model in order to better capture the complexity of the network con\002gurations 050e.g coupling between the costs of the communication payloads depending on the level of on-orbit processing the they perform\051 Additionally the size of the tradespace is currently limited to less than two thousand architectures due to computational limitations a stringent limitation given the possible combinations from the identi\002ed architectural decisions The solution currently under development is to include a genetic algorithm that alleviates this problem by iteratively generates new populations of architectures by combining the best previously evaluated networks On the other hand the other main stream of future work is related to exercising the tool in a variety of architectural decisions and mission scenarios In the presented case studies only geosynchronous constellations were considered although the tool allows comparing them with systems that place relays in MEO and LEO orbits Additionally the tool can also provide insight in valuing inter-satellite links and how the cost of putting in orbit their extra communication payloads can be leveraged by reducing the number of operating ground stations Moreover couplings between the payload allocation and fractionation strategy should also be further explored so as to understand if monolithic architectures still become preferable if relay satellites can be decomposed in clusters of independent antennas and payloads Finally supplementary what-if analysis can be conducted based on 0501\051 the demand forecast for the 2020-2030 time frame 0502\051 granted spectrum allocations to NASA and 0503\051 technology improvements that increase the spectral ef\002ciency of the communication payloads 12 


A PPENDIX Table 7  Acronyms Arch Architecture CER Cost Estimating Relationship DESDYNI Deformation Ecosystem Structure and Dynamics of Ice DSN Deep Space Network EIRP Equivalent Isotropically Radiated Power GEO Geosynchronous Orbit GRTG Guam Remote Ground Terminal GSFC Goddard Space Flight Center HD High De\002nition HP Hosted Payloads ISL Intersatellite Link ISS International Space Station LCC Life Cycle Cost LEO Low Earth Orbit MEO Medium Earth Orbit MIT Massachusetts Institute of Technology MOC Mission Operating Center MPCV Multi-Purpose Crew Vehicle NASA National Aeronautics and Space Administration NCCDS Network Control Center Data System NDA Non-Disclosure Agreement NEN Near Earth Network NISN NASA Integrated Services Network NOAA National Oceanic and Atmospheric Administration RF Radio Frequency SA Single Access SBRS Space based Relay Study SCaN Space Communication and Navigation SN Space Network STGT Second TDRSS Ground Terminal STK Systems ToolKit TDRSS Tracking and Data Relay Satellite System TT&C Telemetry Tracking and Command USGS United States Geological Survey WSGT White Sands Grount Terminal A CKNOWLEDGMENTS This project is funded by NASA under grant NNX11AR70G Special thanks for Gregory Heckler David Milliner and Catherine Barclay at NASA GSFC for their help getting the dataset and their feedback on our study R EFERENCES   National Aeronautics and Space Administration 223Space Communications and Navigation 050SCaN\051 Network Architecture De\002nition Document 050ADD\051 Volume 1  Executive Summary,\224 Tech Rep 2011   227\227 A v ailable http   www.nasa.gov/directorates/heo/scan   e a Sanchez Net Marc 223Exploring the architectural trade space of nasas space communication and navigation program,\224 in Aerospace Conference 2013 IEEE  2013   P Brown O  Eremenko 223Fractionated space architectures a vision for responsive space,\224 Tech Rep   S M V  D C E Teles J 223Overview of TDRSS,\224 Advances in Space Research  vol 16 pp 67\22676 1995   Analytical Graphics Inc A v ailable http   http://www.agi.com   D Selva Valero 223Rule-Based System Architecting of Earth Observation Satellite Systems by,\224 Ph.D dissertation Massachusetts Institute of Technology 2012   K D W  S P Davidson A 223Pricing a hosted payload,\224 in Aerospace Conference 2012 IEEE  2012   W J Larson and J R Wertz Space mission analysis and design  Microcosm Inc 1992   M Adinol\002 and A Cesta 223Contributed Paper Heuristic Scheduling of the DRS Communication System,\224 vol 8 1995   National Aeronautics and Space Administration Space Network Users Guide 050 SNUG 051  2007 no August 2007   e a Tran J J 223Evaluating cloud computing in the nasa desdyni ground data system,\224 in Proceedings of the 2nd International Workshop on Software Engineering for Cloud Computing  2011 B IOGRAPHY  Marc Sanchez Net is currently a second year M.S student in the department of Aeronautics and Astronautics at MIT His research interests include machine learning algorithms and rule-based expert systems and their suitability to the 002elds of system engineering and space communication networks Prior to his work at MIT Marc interned at Sener Ingenieria y Sistemas as a part of the team that develops and maintains FORAN a CAD/CAM/CAE commercial software for shipbuilding Marc received his degrees in both Industrial engineering and Telecommunications engineering in 2012 from Universitat Politecnica de Catalunya Barcelona Dr Daniel Selva received a PhD in Space Systems from MIT in 2012 and he is currently a post-doctoral associate in the department of Aeronautics and Astronautics at MIT and an adjunct Assistant Professor in the Sibley School of Mechanical and Aerospace Engineering at Cornell University His research interests focus on the application of multidisciplinary optimization and arti\002cial intelligence techniques to space systems engineering and architecture Prior to MIT Daniel worked for four years in Kourou 050French Guiana\051 as a member of the Ariane 5 Launch team Daniel has a dual background in electrical engineering 13 


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


