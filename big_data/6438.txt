  An Efficient Algorithm for Privacy Preserving Maximal Frequent Itemsets Mining MIAO Yuqing 1 ZHANG Xiaohua 1   WU Kongling 1 SU Jie 1 000\003 Computer Science and Engineering College, Guilin University of Electronic Technology, Guilin 541004 1  Abstract  This paper addressed the insecurity and the inefficiency of privacy preserving association rule mining in vertically partitioned data. We presented a privacy preserving maximal frequent itemsets mining algorithm in vertically partitioned data The algorithm adopted a more secure vector dot protocol which used an inverse matrix to hide the original input vector and without any site revealing privacy vector. The mining strategy was based on depth-first search for the maximal frequent itemsets. Performance analysis and experimental analysis showed that the algorithm possessed higher security and efficiency   Keywords  Maximal Frequent Itemsets Mining Privacy Preserving Data Mining, Privacy Preserving association rule mining, Vertically Partitioned Data   1. Introduction  Privacy Preserving Data Mining \(PPDM\ is designed from the perspective of the privacy protection in data mining. In premise of protecting the owner privacy, make the data mining still go smoothly, and get the right knowledge 1~2  According to the char acteristics of privacy protection technology and different mining environment, choose proper privacy protection technology and data mining algorithm to form the privacy preserving algorithm This paper presented a privacy protection maximal frequent itemsets mining algorithm in vertically partitioned data \(PPMF\The vertically partitioned data means each site contains some attributes of transactions. The problem is to mine maximal frequent itemsets across the whole database, where some columns in the table are at different sites. The difficulty of mining maximal frequent itemsets in vertically partitioned data is how to count the support of itemsets without leaking the privacy information. PPMF adopted a new point product protocol. The protocol introduced inverse matrix and random number to hide the original input vector. In addition, PPMF mined maximal frequent itemsets instead of mining frequent itemsets so as to reduce the computation cost. And the algorithm combined with the effective technology to improve the mining efficiency  2. Related Work  Clifton and Vaidya proposed a privacy protection association rule mining algorithm for vertical partitioned data  The algorithm computed the scalar product to protect the initial data. Result of the scalar product is the support of the itemsets However, when party B sent data to party A, A can be obtained the value of the privacy vector by solve the equation y x s 000G 000G 002 003  Because A knew the value of s and x 003 000G At the same time, the algorithm is for mining frequent itemsets. But the number of frequent itemsets grows exponentially with the size of the itemsets. When the size of itemsets is long the computation cost of traversing all frequent itemsets is very high Clifton proposed several methods of secure multi-party computation which are often used in privacy protection data mining  Contrasting with points product protocol in [3 h e poi n t produ ct  showed higher efficiency, less communication and lower time complexity. But the protocol in some ways leaked the part of the input information. For some special input, when the dimension of privacy vector is small, the protocol is not safe Tianhong proposed a privacy protection association rule mining algorithm based on the reversible square  The algorithm reduced the communication cost and computation cost by introducing a safe and reliable third party. But it is hard to find a completely trusted third party. And the algorithm is also for mining frequent itemsets the algorithm calculation price is very high Both d [5] u s ed t h e Apriori algorithm to mining frequent itemsets. The Apriori algorithm is based on "produce-the test" iterative idea. The algortithm is simple. But its computation cost depends on the number of scanning database. And the number is related with the length of the frequent itemsets  In general, the size of frequent itemsets in vertical distributed database is large Therefore, the Apriori algorithm performance is lower in vertical distributed environment 3  Point Product Protocol  Point product protocol is a basis kind of secure 
2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming 978-0-7695-4575-2/11 $26.00 © 2011 IEEE DOI 10.1109/PAAP.2011.62 105 
2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming 978-0-7695-4575-2/11 $26.00 © 2011 IEEE DOI 10.1109/PAAP.2011.62 115 
2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming 978-0-7695-4575-2/11 $26.00 © 2011 IEEE DOI 10.1109/PAAP.2011.62 115 
2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming 978-0-7695-4575-2/11 $26.00 © 2011 IEEE DOI 10.1109/PAAP.2011.62 115 


  multi-party computation. According to different level security and the computational complexity there are a lot of different point product agreements    Secure multi-party computation: secret input of k, hope to use their party secret input to collaboratively compute a function. This computing requires all parties can receive the correct output and every part can only know their own input This paper presented a new point product protocol, as follows Input: Alice has a privacy vector     n 2 1 x x x x  000G  Bob has a privacy vector     n 2 1 y y y y  000G  Output: Alice and Bob collaboratively compute T n 1 i i i XY y x S  002  000  and didn't know any information of other’s privacy vector 1\roduces private matrix A, then change vector Y to as the following form   n 2 1 y 0 0 0 y 0 0 0 y We use column vector     N 2 1 Y Y Y to represent the result of two matrixes YA multiplication. So we can get the equation 000¦\000   n 1 i n 1 k ik i T n XY XY   and ik n is the element of matrix 1 A   inverse matrix of A Bob will transfer     N 2 1 Y Y Y to Alice 2\lice calculates n 2 1 XY XY XY   In order to hide the vector X, Alice produce random 004 004 004   2 1  and  004 000  n 1 i i 1. Then Alice calculates n N 2 2 1 1 XY XY XY 004 004 004    and sent it to Bob   3\ob calculates 000 000 000 000     004   004  004  004 n 1 k nk n n n 1 k k 3 3 3 n 1 k k 2 2 2 n 1 k k 1 1 1 n XY n XY n XY n XY  The result is S  4. The Privacy Preserving Maximal Frequent Itemsets Mining Algorithm  4.1 Problem Definition  The association rule mining problem can be formally stated as follows e t I i 1 i 2 i 3 i m  be a set of  items. Let D be a set of transactions where each transaction T is a set of items such that T 005 I Each transaction has a unique identifier called its TID We say that a transaction T contains X a set of some items in I if X 005 T An association rule is an implication of the form X 000 Y where X 006 I  Y 006 I and X 007 Y   The rule X 000 Y holds in the transaction set D with confidence c if c% of transactions in D that contain X also contain Y The rule X 000 Y has support s in the transaction set D if s% of transactions in D contains X 010 Y Frequent itemsets \(FI\ers to the itemset whose support is greater than user specified, the minimum support minsup\ in D  If X is a frequent itemsets, and all super sets of X are not frequent, then X is called maximal frequent itemsets \(MFI\ or maximal frequent pattern. We can get all the frequent itemsets by finding out all the subitemsets of the maximal frequent itemsets. But the number of the maximal frequent itemsets was less than frequent itemsets In order to reduce the computation cost, PPMF mined maximal frequent itemsets instead of mining frequent itemsets  The vertical division has distributed transaction database A and B. Let the total number of attributes be l  m where A has l attributes A 1 through A l  and B has the remaining m attributes B 1 through B m  Transactions  records are a sequence of l  m 1s or 0s. Let k be the support threshold required, and n be the total number of transaction  records. Let X 000G  and Y 000G  represent columns in the database, i.e., x i  1 if row i has value 1 for attribute X. The scalar or dot\ product of two cardinality n vectors X 000G  and Y 000G is defined as 000  002  002 n 1 i i i y x Y X 000G 000G Determining if the two-itemset XY is frequent thus reduces to testing if k Y X 011 002 000G 000G In Section 2 we presented an efficient way to computer scalar product Y X 000G 000G 002  without either side disclosing its vector. Suppose we want to compute if a particular 5-itemset is frequent, with A having 2 of the attributes, and B having the remaining 3 attributes. I.e., A and B want to know if the itemset       b b a b a B B B A A l  is frequent. A creates a new vector X 000G of cardinality n where b a A A X 000G 000G 000G 002  component multiplication\ and B creates a new vector Y 000G  of cardinality n where c b a B B B Y 000G 000G 000G 000G 002 002  Now the scalar product of X 000G and Y 000H  provides the \(in\ frequency of the itemset  4.2 Algorithm Idea   We considered the reality of the model of algorithm as describe in  And the problem could be described as privacy protection association rules mining in vertical partitioned data. The system structure of the algorithm is shown in figure 1. It consists of two parts: \(1\articipation party A and party B. \(2\er is the algorithm executives Miner can be one of the participants or the other third party. All the parties all belong to semi-honest party. Semi-honest party model is refers to all of the participants strictly carries out all the settlement so there may only have passive attacker        
106 
116 
116 
116 


         Figure 1 system structure  Search strategies are divided into depth first search and width first search. Depth first search traverses along a branch of tree and then returns until the end The itemsets number k of this search way increase or decrease in turn. Frequent itemsets are usually very long in vertical distributed database. For long frequent itemsets depth first search strategy is more efficient  so we use depth first traverse strategy and make the traverse space into the logic enumeration tree  During the process of mining maximal frequent itemsets, when mining support is little, superset checking is a kind of time-consuming and frequent operation in the mining algorithm. So PPMF adopted the local maximal frequent itemsets to check the supersets  The local maximal frequent itemsets can be formally stated as follows: Let D  denote a database. Let N be the node. Let DMfi be the maximal frequent itemsets which were found from D So the local maximal frequent itemsets LMfi  M  M  012 DMfi and    M n h 005 the LMfi was the local maximal frequent itemsets of node N  Combine with superset pruning technology, the foresight pruning technology, the father equivalence pruning technology, in order to mining maximum frequent itemsets instead of mining frequent itemsets. By safety computing vector scalar product obtains itemset support   4.3 Algorithm Pseudo-code  The pseudo code of algorithm PPMF is as follows 1 L1 large 1-itemsets 2. h\(node 013 t\(node L 1 Mfi 013  3. for 002 int i=0 002 i< freItemID.size; i 002  if\(SupersetCheck\(t\(node\Mfi then  return   4. for each   node t x 012 do begin  5.     if\(sup\(h\(node 010 x}\insup\en delete x from t\(node else if\(sup\(h\(node 010 x}\= sup\(h\(node do begin   delete x from t\(node add x to h\(node endfor 6. if\(t\(node 013  then  if\(Mfi 013  then add h\(node\to Mfi 7.for each   node t x 012 do begin       add x to h\(node\hen form h\(newnode    delete x form t\(node\en form t\(newnode LMfi = LocalMfi\(x,Mfi Mfi = Mfi 010 PPDM\(newNode,LMfi  Recursive execution endfor 8.return  Mfi In step 3, the function \(SupersetCheck \(t \(node Mfi\he head-tail forward pruning,as follows  SupersetCheck\(tail,Mfi for each M 012 Mfi do begin for each x 012 tail do begin if x 014 M then break        else if x is the last item in tail  then return 1   return 0  In step 5 and 6 computer scalar product Y X 000G 000G 002 as follows SupportCal\(attributes for all candidates  do begin if all the attributes in s are  entirely at A or B that Party independently calculates the support else call the secure two-party protocol to computer support   In Step 7,the function  LocalMfi \(x, Mfi found the local maximum frequent itemsets of all nodes in the tail set. Initially, we put the root node local maximum frequent itemsets empty \(this set is updated when finish checking a node LocalMfi\(x,Mfi LMfi 013  for each M 012 Mfi do begin if x 012 M then LMfi = LMfi 010 M  return  LMfi   5 Experimental Evaluation and  Performance Analysis  We analyzed the algorithm from the safety and efficiency. Finally, experiments are presents to prove the feasibility and efficiency of this algorithm Safety analysis: PPMF make sure the algorithm security and privacy through scalar product. From Section 3, we know the method to judge whether the scalar product reveal private data is whether or not reveal the private information when party A transmits data to party B and B transmits data to A The analysis is divided into two parts, one is the transmitting procedure when B to A , other is in the 000\003 Part y A Miner 000\003 000\003 003\003’\003£\003 003 003Q\003s 003Q 
107 
117 
117 
117 


  procedure A to B From party B to party A: party B need to transfer data to party A once. The data is     N 2 1 Y Y Y  Among these data, the matrix Y is the private matrix, and the matrix A is random matrix form B So party A can't get the private vector Y From party A to party B: party A also needs to transfer data to party B once. The data is n N 2 2 1 1 XY XY XY 004 004 004   Party B doesn't know the random data, so B can't get the private vector X Experimental comparisons showed that the performance of PPMF is superior to the algorithm in [3 a v a lan g u a g e to realize t w o al g o rith m s   The experimental environment is: two computers whose configuration settings are 2.19 GHz dual-core CPU, 2 GB memory and 1.61 GHz CPU for AMD, memory for 512 MB. One of them is mining server and another only is parties. Use plantCellSignaling data  as the experimental data sets. We took the 2000 row. Average arranges them in the two computers according to items. Since point protocols of the two algorithms have random numbers, it will influence the algorithm time. We carry out several experiments for each fixed support for every algorithm, and then take the average value. Figure 3 shows the result with 2000 row. X axis is the support and the y axis is operation time which unit is millisecond \(ms     Figure 3 Algorithms Running Time   From the figures, we can see that the operating time of PPMF is superior to the algorithm of literatu h e s m aller th e s u pport deg r ee PPMF algorithm produces higher efficiency. This is about the local calculation time, network transmission time and the complexity of scalar product, so we observe this question from these three points. Local computing time is mainly referring to generate local frequent itemsets and part of infrequent itemsets time. In PPMF, this time is about find all the maximal frequent itemsets time used. For [3 e ti m e  o f  A p rio r i alg o rith m   For the complexity of scalar produ n eeds  four steps to get the results and need to three transmission between two parties. The point product protocol of this paper needs three steps and needs to two transmission data among the two parties 6. Conclusion Data mining has made great progress in recent years. But the privacy preserving data mining is still in the early stages. This paper presented a privacy preserving maximal frequent itemsets mining algorithm \(PPMF\ vertically partitioned data. PPMF adopted a more secure vector dot protocol which used an inverse matrix to hide the original input vector, and without any site revealing privacy vector. PPMF used depth first traverse strategy and combined with superset pruning technology, the foresight pruning technology and the father equivalence pruning technology. And PPMF used the local maximal frequent itemsets to check the supersets in order to mining maximum frequent itemsets instead of mining frequent itemsets. These strategies obviously accelerated time of mining the maximal frequent itemsets greatly reduced the computation cost, and improved the mining efficiency.  Algorithm analysis showed that the security of algorithm was improved. Experimental analysis showed the high performance of PPMF at privacy preserving association rules mining   7. References  1 Dan i el E.O 031 Leary, Knowledge Discovery as a Threat to Database Security, In Proceed of the 1st International Conference on Knowledge Discovery and Databases\(1991\,107.516 2 C h r i s C lif ton a n d D ona l d Ma r k s  Se c u r i t y a nd privacy implications of data mining, In Proceedings of the ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery\(1996\,15.19 3 J V A IDY A C. CL IFT O N P r iv ac y P r eserv i n g  Association Rules Mining in Vertically Partitioned Data. In the 8th ACM SIGMOD International Conference on Knowledge Discovery and Data Mining. Edmonton, Canada, July 2002 4 C lif ton C  K a nta r c i og lo u M L i n X, Z hu M Y  T ool s  for privacy preserving distributed data mining. ACM SIGKDD Explorations,2002,4\(2\:28-34 5 T i a n Ho ng W a ng Y a W e i. Inv e rtible Ma trix b a s e d  Privacy-preserving Association Rules Mining[J  C o m pute r Eng i ne e r ing  200 9,v o l  3 5 N o 7  6 Mia o Y u Q i ng A s s o c i a tion R u le s Min i ng a n d I t s  Application To Gene Expression Data [D  Univ e r sity  of Science and Technology of China, 2007 7 Du W a n d A t a lla h M J. Se c u re m u ltipa r ty  Computation Problems and their Applications: A Review and Open Problems. In New Security Paradigm workshop, 2001. Cloudcroft, USA, 2001 11-20  W a n g J Han J  P e i J CL OS ET sear ch in g f o r th e  best strategies for mining frequent closed itemsets Proc, Ninth ACM SIGKDD Int'l Conf. On Knowledge Discovery and Data Mining, ACM Press, Washington D, C.2003, PP. 1-10 9 Y a n Y u e J in, C h e n H u oW a n g  A De pthFir s t Se a r c h  Algorithm for Mining Maximal Frequent Itemsets J  J o ur na l of C o m pute r R e s e a r ch a n d D e v e lopm e n t  2005, 42\(3\:462~467 10 htt p a rc hiv e ic s uc i e d u m l/da t a s e t s.htm l   
108 
118 
118 
118 


naturally occur in real programs due to mistakes made by developers [8  I n or de r t o  a v o i d f a ult  inte r a c tion, m u l t i p le  versions of each application \(mutants\ were created; each version was then seeded with ex actly one fault. Hence, a test case is regarded to be capable of detecting fault i if its output indicates a mismatch between version i \(i.e., the version that was created by seeding fault i\ and that of the original version. Some faulty versions were not used in our study because none of the test cases we obtained can reveal them Table II depicts summary of detailed information: for example, with regard to TerpPaint the total number of faulty versions is 263, there are 424 failing test cases out of the total 17977 test cases, and only 148 faulty versions can be revealed, so we only use the 148 faulty versions in our study TABLE II. T EST S UITE AND F AULT I NFORMATION OF THE S UBJECT P ROGRAMS   Subjects  total fault versions  total test cases  total failing test cases  detected fault versions  Te rpPai nt  263  17977  424  148  Te r pP re s en t  265  22770  32  139  TerpSpreadSheet  234  5272  1172  139  Te rp Wo rd  295  24288  772  224   B  Data collection According to the test cases downloaded from the online repository hosted by the University of Maryland [1 we extract the event sequence for every test case. Then we obtain the failing test cases and which fault can be revealed by which test case from the downloaded fault matrix Regarding to the faulty versions which can be revealed by the downloaded test cases, we download these faulty versions and artificially compare them with the original to confirm which event handlers contain these faults, eventually we manage to establish the one-to-one relationship between event handler and fault We should notice that currently our method makes use of the information of the whole test case suite. However, it is impractical to assume that large comprehensive test sets are always available. In the future, we should apply our method on random test case suite to further evaluate its feasibility C  Evaluation For all the experiments, we tentatively set Z\006\007  1,2   and compare the result of different MinSup Due to the limited length of test cases obtained and our analysis of the choice for C DEF  above, we set C DEF  The number of faulty versions revealed against the percentage of event handlers to be examined is charted for both the best case and worst case. Recall that the best case and worst case individually correspond to the best situation and worst situation  1  TerpPaint Fig. 3 and Fig. 4 respectively show the result of our method on TerpPaint in the best case and worst case under different parameter MinSup Different curves represent the result of different parameter MinSup for example, the curve MS-1 stands for the result of our method in condition of MinSup equals to 1.The horizontal axis represents the cumulative percentage of event handler to be examined and the vertical axis represents the total number of faulty versions for which bug can be detected by examining this percentage of event handler. If we set Z\006\007  1 we can detect 90% of total faults by examining only 21 event handlers out of the total 310 event handlers in the best case but in the worst case, we have to examine 199 event handlers to detect the same percentage of faults. Table III shows the number of event handler we have to examine to detect 90 of total faults in the best case and worst case under different parameter MinSup From this table we can draw a conclusion that to TerpPaint our method performs very well in both the best case and worst case if we set Z\006\007     Figure 3. Data from TerpPaint best case  Figure  4. Data from TerpPaint worst case TABLE III. N UMBER OF EVENT H ANDLER EXAMINED TO REV EAL 90 OF THE 148 FAULTY VERSIONS OF T ERP P AINT  Case  Z\006\007  1 2 3  4 5 6 7 8 Best 21  106  78  77  72  61  52  40  Wo r st  199  112  83  250  304  308  308  309  2  TerpSpreadSheet Fig. 5 and Fig. 6 respectively show the result of our method on TerpSpreadSheet in the best case 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS 6 MS-7 MS-8 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS 1 MS 2 MS-3 MS-4 MS-5 MS-6 MS-7 MS-8 
329 


and worst case under different parameter MinSup The axes and curves have the same meaning as described above. If we set Z\006\007  1 we can detect 90% of total faults by examining only 16 event handlers out of the total 176 event handlers in the best case, but in the worst case, we have to examine 89 event handlers to detect the same percentage of faults. Table IV shows the same meaning statistics as Table III but to TerpSpreadSheet From this table we can come to a conclusion that to TerpSpreadSheet our method performs very well in both the best case and worst case except that Z\006\007$%& equals to 1 Attention should be paid that although we may have to check same number of event handler to detect 90% of total faults both in the best case and worst case under some parameter MinSup but the two figures clearly indicate that we have to check fewer event handlers to detect a lower percentage of total faults in the best case. We should also not neglect that besides MinSup  equals to 1, the other 7 curves overlap each other highly both in the best case and worst case  Figure 5. Data from TerpSpreadSheet best case  Figure 6. Data from TerpSpreadSheet worst case  3  TerpWord Fig. 7 and Fig. 8 respectively show the result of our method on TerpWord in the best case and worst case under different parameter MinSup If we set MinSup 1 we can detect 90% of total faults by examining only 7 event handlers out of the total 617 event handlers in the best case but in the worst case, we have to examine 150 event handlers to detect the same percentage of faults. Table V shows the same meaning statistics as Table III but to TerpWord  Judging from this table, we can conclude that to TerpWord  our method performs very well in both the best case and worst case except that MinSup equals to 1, 7 or 8.Simliarly although we may have to check roughly same number of event handler to detect 90% of total faults both in the best case and worst case under some parameter MinSup on the whole, our method performs better in the best case. Also besides MinSup equals to 1 or 8, the other 6 curves coincide with each other highly both in the best case and worst case TABLE IV. N UMBER OF EVENT H ANDLER EXAMINED TO REVEAL 90 OF THE 139 FAULTY VERSIONS OF T ERP S PREAD S HEET  Case  Z\006\007  1  2  3  4  5  6  7  8  Best 16  30  28  28  27  26  26  26  Wor st  89  32  29  29  28  27  27  27   Figure 7. Data from TerpWord best case  Figure 8. Data from TerpWord worst case 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS 3 MS-4 MS-5 MS-6 MS-7 MS-8 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS-6 MS-7 MS-8 0 50 100 150 200 250 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS-6 MS 7 MS-8 0 50 100 150 200 250 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS 3 MS-4 MS-5 MS-6 MS-7 MS-8 
330 


TABLE V. N UMBER OF EV ENT H ANDLER EX AMINED TO REVEAL 90 OF THE 224 FAULT VERSIONS OF T ERP W ORD  Case  Z\006\007  1 2  3  4  5  6  7 8 Best  7 40  40  40  42  41  57  57 Wo r st  150  40  40  40  43  42  617  617   4  TerpPresent Fig. 9 and Fig. 10 respectively show the result of our method on TerpPresent in the best case and worst case under different parameter MinSup If we set MinSup 1, we can detect 90% of total faults by examining only 2 event handlers \(these two event handlers contain most of the faults for TerpPresent in the best case, but in the worst case, we have to examine 34 event handlers to detect the same percentage of faults. Judging from Table VI, we can reach a decision that to TerpPresent our method performs very well in both the best case and worst case except that MinSup equals to 1 or 8. Equally, we should take care that the other 6 curves overlap each other highly besides MinSup equals to 1 or 2 in the best case, meanwhile another 5 curves coincide with each other highly besides MinSup  equals to 1, 2 or 8 in the worst case From the above results we can say that our proposed method performs well for GUI software. After applying N gram analysis to the event sequences of all test cases, we can rank the event whose corresponding event handler contains faults high. Making use of the result, programmer can check a few event handlers to detect a majority of the total faults At present we have no common rule for the value selection of MinSup but we can observe from this result that if we give MinSup a too low value, then we may take into consideration some irrelevant N grams, in this condition we will get a pretty good result in the best case yet a relatively poor result in the worst case. On the contrary, if the value of MinSup is too high, some very important N grams will be abandoned by us, this will cause our fault localization technique to lose efficacy. Considering all these factors, we can give MinSup a median value when putting our method into practice, in such a case, we can expect to get a good result both in the best case and worst case. Our preliminary result indicates 3 is a candidate value for MinSup  TABLE VI. N UMBER OF EVENT HANDLER EXAMIN ED TO REVEAL 90 OF THE 139 FAULT VERSIONS OF T ERP P RESENT  Case  Z\006\007  1  2  3  4  5  6  7  8 Best  2  8  3  3  3  3  3  3 Wo r st  34  9  4  4  4  4  4  322  D  Threats to Validity There may be several threats to the validity of this paper that include, but are not limited to, the following. The results of the study, presented above, should be interpreted keeping in mind the following threats to validity  Figure 9. Data from TerpPresent best case   Figure 10. Data from TerpPresent worst case  First of all, although we can sort the events in order of probability of their corresponding event handler containing the fault successfully using N gram analysis, but to certain event of GUI software, the events event handler contains much source code and consequently there exists many faults in the event handler. Until now we can only identify this kind of event handler has high level of suspicion but can not identify which specific part of it is doubtful. Maybe we can combine with various traditional software fault localization techniques to further analyze this kind of event handler Another important threat to validity concerns the complexity of a few event handlers. Some event handlers may contain nested function call. Although when establishing the one-to-one relationship between event handler and fault, we take one thing with another to the best of our ability, but owing to the complication of GUI software we cant guarantee the fullness between event handler and fault, i.e., there may exist few event handlers that also contain certain fault but the relationship is neglected by us this also can to some degree affect the accuracy of our result But the relationship between event handler and fault we use in our study exists in no doubt, if we can find the complete 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS-6 MS-7 MS-8 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS-6 MS-7 MS-8 
331 


relationship between event handler and fault, we can anticipate a better numeral result As described above, our downloaded test cases are generated according to GUIs graph model EIG and are used for smoke testing, so the length of each test case is limited For one thing, our method makes use of the information of the whole test case suite, the experimental results may not be valid under random smaller test case suite, i.e., they may not be statistically repeatable. Moreover, restricted by the limited length of each test case, we cant systemically observe the effect of C DEF by giving it a variation range although we have full reason a choice of 3 for it should be reasonable  V  R ELATED W ORKS  In this section, we briefly review previous work related to GUI software fault localization To the best of our knowledge, this is the first work that focuses on GUI software fault localization. However, many fault localization techniques based on the contrast between failing and passing cases run-time information has been applied to conventional software. In [1   2   the autho r s  make use of various heuristics and apply them to program traces to rank statements in order of their relative suspiciousness. In [3   s t a tis ti c a ll y b a s e d tec h niq u e s r e l y o n  the instrumentations and evaluations of predicates in programs to produce a ranking of suspicious predicates. In 12  t h e au t h o r s m o d e l s o f t w a r e  e x e c u t io n t r ac es as  networks and two centrality measures are adopted to calculate the suspiciousness of each statement. Especially in 4  the m e tho d  N gram analysis is used to analyze the exact execution sequence of the program to rank the executable statements of software by level of suspicion Our proposed GUI software fault localization technique based on some important achievements in GUI testing several researchers and practitioners have discussed the following concepts and definitions that are relevant to its specific parts. In [5  Mem o n  b u ild s up  a g ood b a s i s  f o r th e  future research of GUI software by giving a reasonable and effective GUI Representation, including the representation of GUIs state, GUI events, GUI test cases, etc., also, the definition of EFG is brought up  us ing  s p e c if ic gr a p h traversal algorithms, an improvement for EFG, namely EIG is employed to generate smoke test cases. In [6  the autho r s  give a relatively formal definition of event handler   VI  C ONCLUSIONS AND F UTURE W ORKS  This paper presents a GUI software fault localization technique by analyzing the event sequences of faulty versions to sort the events in order of probability of their corresponding event handler containing the fault. Our proposed technique has been evaluated with respect to four subject programs under different parameters and results clearly demonstrate its effectiveness if we take appropriate parameter GUI software fault localization is a totally new research issue, how to carry out effective fault localization in connection with GUI softwares characteristic is a challenging problem. This research has presented several exciting opportunities for future work. To begin with, we can combine GUI software fault localization with traditional software fault localization, we can first successfully sort the event using our proposed method, then regarding the event whose corresponding event handler has much source code we can use traditional software fault localization technique to further analyze the event handler to get a more detailed result Moreover, a process of using certain random strategy to select some test cases from the whole test case suite and then applying our method on the relatively smaller test case suite can be implemented, the mean result of this repetitive process can be used to more comprehensively evaluate the feasibility of our proposed method. We are also working on applying our proposed technique on GUI applications with different kinds of characteristics to further demonstrate its effectiveness and commonality, such as programs with complex underlying business logic and a fairly simple GUI even other event-driven software \(EDS\, including Web applications, device drivers and embedded software  R EFERENCES  1  J. A. Jones and M. J. Harrold, Empirical evaluation of the Tarantula automatic fault-localization technique,Ž in Proc. 20th IEEE/ACM Int Conf. Automated Softw. Eng., Long Beach, CA, pp. 273…282, Nov 2005 2  W. E. Wong, V. Debroy, and B. Choi, A family of code coveragebased heuristics for effective fault localization,Ž J. Syst. Softw., vol 83, no. 2, pp. 188…208, Feb. 2010 3  C. Liu, L. Fei, X. Yan, J. Han, and S. P. Midkiff, Statistical debugging:a hypothesis testing-based approach,Ž IEEE Trans. Softw Eng., vol. 32 000\003 no. 10, pp. 831…848, Oct. 2006 4  S. Nessa, M. Abedin, W. E. Wong, L. Khan, Y. Qi, Software fault location using N-gram analysis,Ž in Lecture Notes in Computer Science including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics, \(2008\, vol. 5258 LNCS, pp 548-559 5  Atif M. Memon, A Comprehensive Framework for Testing Graphical User Interface, Ph.D. Dissertation, University of Pittsburgh, USA 2001 6  Zhao L and Cai K. Y., E vent Handler-Based Coverage for GUI  Testing,Ž in Proceedings of the tenth International Conference on Quality Software \(QSIC 2010\, pp.326-331, 14-15 July 2010 7  Atif M. Memon,   http://www.cs.umd.edu/~atif/TerpOfficeWeb TerpOfficeV3.0/index.html 8  A. M. Memon and Q. Xie, Studying the fault-detection effectiveness of GUI test cases for rapidly evolving software,Ž IEEE Trans. Softw Eng., vol. 31, no. 10, pp. 884…896, 2005 9  Han, J., Kamber, M.: Data Mining: Concepts and Techniques Morgan Kaufmann Publishers, San Francisco \(2001 10  Atif M Memon http://www.cs.umd.edu/~atif/Benchmarks UMD2005b.html 11  Cai K. Y., Zhao L., Hu H., and Jiang C.H.,On the Test Case Definition for GUI Testing,Ž in Proceedings of the Fifth International Conference on Quality Software \(QSIC 2005\, pp.19-28 Melbourne,Australia, September 2005 12  Zhu L.Z., Yin B.B., Cai K. Y., Software Fault Localization Based On Centrality Measures,Ž compsacw, pp.37-42, 2011 IEEE 35th Annual Computer Software and Applications Conference Workshops, 2011    
332 


3 Check the user is registered or not, if YES then 4       whether the user is first time visitor, if YES then 5  return W\(Pi 6    else 7      calculate FW\(Pi 4 8 SET   W\(Pi Pi 9 return W\(Pi 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 197 spent on a page is inversely proportional to the degree of information useful to user. The formula to calculate the weight based on time spent on each page is given in \(6  TW?u? = T??? ????? ?? ? ???????/P??? S??? ????????U?T??? ????? ?? ? ???????/P S??? ????                  \(6  The Frequency and Time spent based page Weight FTPW spent on a particular page, as biasing factors. Frequency and Time spent based page Weight can be calculated by the formula given in \(7  W\(u u u 7  Where FW\(u which can be computed from \(4 u page based on time spent on a page which can be computed from equation \(6 u based on frequency and time spent both The Pseudo code of FPW algorithm given as follows            


   V. PERFORMANCE EVALUATIONS The performance of the proposed approach in terms of various parameters is discussed in this section A.  Data Set We have evaluated the performance of proposed algorithms by using the synthetic data set for different users to calculate the weight of each web page. Consider the structure of a web site consisting of four web pages namely A, B, C and D and their respective interconnection shown by direct edges as shown below in Figure 1       Figure 1: Web Site Structure The following factors are considered for performance evaluation Outbound links which shows the out degree of nodes for the graph of Figure1 Page Rank \(PR which can be calculated on the basis of the equation \(3 Number of visit shows the visiting frequency of each web page by user, it can be decided on the basis data received from web log Page Size defines the size of page on the basis of information content of the web page, which can be measured in bytes Time spent describes the resumption of a particular web page by user in seconds  The above parameters used in proposed experimental set up for the Web Path Traversal, by considering the graph mentioned in Figure1 B. Evaluation Method The attributes considering for the training data set for each user of a web site is represented in terms of time spent on each page and frequency. From these data sets we are 


calculating the weight for each web page for respective web site. The proposed approach uses Visiting Frequency and Time Spent on a Web page as two parameters to measure the weight of each web page To estimate the performance of the proposed two algorithms i.e. FPW and FTPW, discussed in section IV based on the above parameters involved in estimation C. Experimental Results The performance of the proposed approach can be evaluated by comparing the performance of FPW and FTPW algorithms which differ in number of parameters considered for experimentation. The comparison is made by taking the attribute like Visiting Frequency in FPW, and further Visiting Frequency and Time spent on a web page are clubbed together in FTPW as another attribute. The experimental setup uses five users and weights are plotted against various parameters Figure 2 shows the plot of Visiting Frequency v/s Weight of a web page              Figure 2: Plot between frequency and weight Algorithm: FTPW Input: Web traversal path database Output: Weight for each page 1 Calculate PageRank for each page \(PRi 3 2 Initially Set W\(Pi 3 Check the user is registered or not, if YES then 4       whether the user is first time visitor, if YES then 5  return W\(Pi 6    else 7      calculate FW\(Pi 4 


8            calculate TW\(Pi 6 9   SET   W\(Pi Pi Pi 10   return W\(Pi A C B    D 0 0.05 0.1 0.15 0.2 0 5 10 15 20 w ei gh t Visiting Frequency User1 User2 User3 user4 User5 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 198  Using the weights calculated in Figure the higher weight for more frequently visited information in FPW algorithm the different the scenario described in Figure 1 for all th plotted in Figure 3            


   Figure 3: Recommendation for Web Path Trave Algorithm  The weight assigned to various pages Visiting Frequency and Time spent on web Figure 4                  Figure 4: Plot between \(frequency + time sp  The Figure 5 gives the details of recomm Traversal for different users by FTPW a weights calculated by considering two para and time spent in Figure 4 which indicates for more frequently visited pages and in term on web pages  0 0.02 0.04 0.06 0.08 0.1 0.12 


0.14 0.16 0.18 W ei gh t Web Pages Us Us Us Us Us 0 0.2 0.4 0.6 0.8 1 1.2 1.4 W ei gh t FTPW \(Frequecy  and 2 which indicate pages. Using this traversal paths in e users have been rsal based on FPW by combining the page is plotted in ent ended Web Path lgorithm and use meters frequency the higher weight s more time spent    


         Figure 5: Recommendation W Algo Figure 6 shows the relative on the synthetic data sets, in w more efficient than FPW algor performance of proposed a increase the complexity of alg and provide better Web Path Tr    Figure 6: Relative Access  The proposed FTPW algorithm parameters which otherwise ar FPW algorithm. A matrix dep comparison between above two parameters are performance ce show that the performance o increase the number of param FPW algorithm to FTPW algori The experimental results d better and provides a methodo optimized Web path traversal past navigation behavior by c page 0 1 2 3 4 5 1 2A 


cc es si bi lit y Ti m e fo r M or e re qu ir ed In fo rm at io n er1\(A->C->D->B er2\(D->B->A->C er3\(D->B->C->A er4\(C->B->A->D er5\(B->A->D->C Time User1 User2 User3 User4 User5 0 0.2 0.4 0.6 0.8 1 1.2 


W ei gh t Web Pages eb Path Traversal based on FTPW rithm  execution for FPW and FTPW hich we can see that FTPW is ithm. Hence, it is clear that the lgorithm increases when we orithm in terms of parameters aversal in less time  ibility time for FPW and FTPW consists of clubbing of various e not available in first proposed icted in the Figure 7 describes proposed algorithms. Here the ntric and a comparison results f the system improves as we eters i.e. when we move from thm rawn for FTPW algorithms are logy for effective, efficient and for various users based on their omputing weight for each web 3 4 5 User FTPW FPW User1\(C->B->A->D User2\(A->B->C->D User3\(B->C->A->D User4\(D->A->C->B User5\(B->D->A->C 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 199 Figure 6: Comparison Matrix VI.  CONCLUSION & FUTURE WORK 


 Web Usage Mining have been used in improving Web site design and marketing decision support, user profiling, and Web server system performance. Web page prediction technique is a very important role in web technologies. This paper proposes efficient algorithms for web path recommendation based on Weighted Association Rule. Two factors frequency and time spent were used to decide the web path traversal. The experimental results show that in the proposed approach when we increase the number of parameters for finding the web path the accuracy of the system is enhanced drastically and FTPW produces more accurate results than those achieved by FPW In the future, we shall improve the Web Path Traversal by considering the parameter Data Transfer Rate to provide the accurate Web Path traversal REFERENCES 1] M. S. Chen, X. M. Huang and I. Y. Lin, Capturing User Access Patterns in the Web for Data Mining, Proceedings of the IEEE International Conference on Tools with Artificial Intelligence, pp. 345348, 1999 2]  R. Cooley, B. Mobasher, and J. Srivastava, Web Mining: Information and Pattern Discovery on the World Wide Web, Proceedings of the 9th IEEE International Conference on Tools with Artificial Intelligence, pp 558-567, 1997 3]  B. Mobasher,N. Jain,E. Han et al, Web mining: pattern discovery from World Wide Web transactions, Tech Rep: TR96-050, 1996 4]  C. Shahabi, A. Zarkesh, J. Abidi, V. Shah, Knowledge discovery from user's Web-page navigation,  in Proceedings of the 7th IEEE International Workshop on Research Issues in Data Engineering, 1997 5]  Yue-Shi Lee, Show-Jane Yen, Ghi-Hua Tu and Min-Chi Hsieh, Web Usage Mining: Integrating Path Traversal Patterns and Association Rules, Proceedings of International Conference on Informatics Cybernetics, and Systems \(ICICS'2003 6]  Yue-Shi Lee, Show-Jane Yen, Ghi-Hua Tu and Min-Chi Hsieh, Mining Traveling and Purchasing Behaviors of Customers in Electronic Commerce Environment, Proceedings of IEEE International Conference on e-Technology, e-Commerce and e-Service \(EEE'2004 pp. 227-230, 2004 7]  J. Srivastava, et al. Web Usage Mining: Discovery and Applications of Usage Patterns from Web Data. SIGKDD Explorations, pp. 12-23 2000 


8]  Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine. Proceedings of the seventh international conference on World Wide Web 7: pp. 107-117, 1998 9]  J. Pei, J. Han, B. Mortazavi-Asl and H.Zhu, Mining Access Patterns Efficiently from Web Logs, Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 396-407 2000 10]  C. H. Cai, A. W. C. Fu, C.H. Cheng and W. W. Kwong, Mining Association Rules with Weighted Items, In Database Engineering and Applications Symposium, Proceedings IDEAS'98, pp. 68  77, 1998 11]  F. Tao, F. Murtagh and M. Farid, Weighted Association Rule Mining using Weighted Support and Significance Framework, In Proceedings of the 9th SIGKDD conference, 2003 12]  Show-Jane Yen, An Efficient Approach for Analyzing User Behaviors in a Web-Based Training Environment, International Journal of Distance Education Technologies, Vol. 1, No. 4, pp.55-71, 2003 13]  Show-Jane Yen, Yue-Shi Lee and Chung-Wen Cho, Efficient Approach for the Maintenance of Path Traversal Patterns, In Proceedings of IEEE International Conference on e-Technology, eCommerce and e-Service \(EEE 14]  M. Spiliopoulou and L. C. Faulstich, Wum: A web utilization miner EDBT Workshop WebDB98, Springer Verlag, 1996 15]  M. S. Chen, J. S. Park and P. S. Yu, Efficient data mining for path traversal patterns,  IEEE Transactions on Knowledge and Data Engineering, pp. 209-221, 1998 16]  H. Yao,H. J. Hamilton, and C. J. Butz, A Foundational Approach to Mining Itemset Utilities from Databases, Proceedings of the 4th SIAM International Conference on Data Mining, Florida, USA, 2004 17]  Z. Chen, R. H. Fowler and A. Wai-Chee Fu, Linear Time Algorithm for Finding Maximal Forward References, Proceedings of International Conference on Information Technology. Computers and Communications  \(ITCC'2003 18]  T. Jing, Wan-Li Zou and Bang-Zuo Zhang, An Efficient Web Traversal Pattern Mining algorithm Based On Suffix Array, Proceedings of the 3rd International Conference on Machine Learning and Cybernetics , pp 1535-1539, 2004 19]  Show-Jane Yen, Yue-Shi Lee and Min-Chi Hsieh, An efficient incremental algorithm for mining Web traversal patterns, Proceedings of the 2005 IEEE International Conference on e-Business Engineering ICEBE05 20]  L. Zhou, Y. Liu, J. Wang and Y. Shi, Utility-based Web Path  Traversal Pattern Mining, Seventh  IEEE International Conference on Data 


Mining Workshops, pp. 373-378, 2007 21]  C. F. Ahmed, S. K. Tanbeer, Byeong-Soo Jeong and Young-Koo Lee Efficient mining of utility-based web path traversal patterns, 11th International Conference on Advanced Communication Technology ICACT09 22]   http://en.wikipedia.org/wiki/PageRank 23] en.wikipedia.org/wiki/Association_rule_mining  Attributes? FPW Algorithm FTPW Algorithm Recognition of User behavior Visiting Frequency Page Rank Time Spent on Web page Page Size Accessibility of required information in less time Improving Web navigation and system design of Web applications  Enhancing server performance 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 200 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


