Incorporating Historic Knowledge into a Communication Library for Self-Optimizing High Performance Computing Applications Saber Feki and Edgar Gabriel Parallel Software Technologies Laboratory Department of Computer Science University of Houston Houston TX 77204-3010 USA Email f sfeki gabriel g cs.uh.edu Abstract  Emerging computing systems have a wide variety of hardware and software components in\003uencing the performance of parallel applications presenting end-users with a nearly unique execution environment on each parallel machine One of the big challenges of High Performance Computing is therefore to develop portable and ef\002cient codes for any execution environment The Abstract Data and Communication Library ADCL is a self-optimizing runtime communication library aiming at providing the highest possible performance for application level communication operations The library provides for a given communication pattern a large number of implementations and incorporates a runtime selection logic This selection aims at adaptively choosing the best performing implementation on the current platform and for the given problem In this paper we present a recent enhancement to the library which introduces the capability of utilizing information from previous executions in order to minimize the overhead of the runtime selection logic which mainly stems from testing underperforming implementations We introduce the notion of similar problems by using a proximity measure for a given operation The approach is evaluated for the n-dimensional neighborhood communication for two different network interconnects and for a large range of different problems Keywords self-optimizing communication libraries historic learning proximity measures I I NTRODUCTION Due to the availability of multi-core processors and the omni-presence of gigabit quality networks parallel programing is increasingly applied in the development of software products in areas such as gaming or data and image processing Companies turn towards parallel processing in order to exploit the capabilities of modern micro-processors and have the ability to solve  in the most general sense  larger problems in a shorter time frame However achieving good performance for a parallel application is highly challenging due to the wide range of hardware and software components available for off-the-shelf computer systems As an example the design of the new multi-core processors of Intel AMD SUN or IBM differ signi\002cantly in the organization and connectivity of the cores the cache hierarchies and the I/O capabilities of the processors It is similarly challenging to deal with the capabilities of the different network interconnects which don't just include standard Fast and Gigabit Ethernet but also more advanced technologies such as In\002niBand iWarp or 10Gigabit Ethernet Furthermore the software stack consisting of the operating system device drivers and communication libraries has a signi\002cant in\003uence on the performance achieved by the application Thus an application developer has to be aware of the fact that his code will basically face a unique execution environment for every single machine that it is running on Scienti\002c computing is already facing today most of the challenges outlined above In order to exploit the capabilities of large scale High Performance Computing HPC systems end-users and application developers apply resource and time consuming tuning of individual software components on each platform Certain software components can be tuned for a given platform before the execution of the application This approach has been taken by several projects such as ATLAS or ATCC The main dra wback of these approaches is that the tuning procedure itself often takes the same or a similar amount of time as running the application itself Additionally several factors in\003uencing the performance of the application can only be determined while executing the application These factors include process placement by the batch scheduler resource utilization due to the fact that some resources such as the network switch are shared by multiple applications operating system jitter and application characteristics e.g communication volume and frequencies There are only very few projects as of today in HPC trying to incorporate runtime adaptation or exposing self-tuning behavior For example the Abstract Data and Communication Library ADCL 6 allo ws applications to incorporate self-tuning operations which are optimized by ADCL while executing the application itself Thus application developers avoid expensive pre-tuning steps The library has been successfully used to tune various communication operations such as n-dimensional neighborhood communication parallel matrixmatrix operations and low-level parameters of a message passing library Although ADCL has been sho wn to deli v er close-to-optimal performance for a large number of platforms and network interconnects 8 there are still situations where the current approach used by ADCL results in a signi\002cant overhead due to the tuning procedure Most notably applica 
Second IEEE International Conference on Self-Adaptive and Self-Organizing Systems 978-0-7695-3404-6/08 $25.00 © 2008 IEEE DOI 10.1109/SASO.2008.47 265 
Second IEEE International Conference on Self-Adaptive and Self-Organizing Systems 978-0-7695-3404-6/08 $25.00 © 2008 IEEE DOI 10.1109/SASO.2008.47 265 


tions exposing frequently and dynamically varying problem sizes e.g due to adaptive mesh-re\002nements would require a very lightweight quick tuning procedure Similarly on certain platforms exploring the performance of a suboptimal communication pattern can add a tremendous penalty to the overall execution time resulting in a large discrepancy between a hand-tuned and an automatically optimized version of the code In this paper we present an extension to the tuning algorithm and the decision logic of ADCL which takes performance information from previous executions into account There are two challenges which complicate this task 002rst it is very seldom in real life that exactly the same problem/problem size is being executed on the same platform twice Thus the library has to introduce the notion of and a quanti\002cation for similar problems Second even on a single parallel machine conditions can vary between different executions signi\002cantly The library therefore has to introduce a safety net which results in dismissing historic performance data under certain conditions The paper describes the algorithms used in ADCL to incorporate historic knowledge and evaluates the properties of the system for a widely used communication pattern in high performance computing namely the n-dimensional neighborhood communication The remainder of the paper is organized as follows Sec II presents the main concepts and ideas behind ADCL Sec III presents the approach taken by ADCL to consider historic knowledge In sec IV we evaluate the approach for two different network interconnects a large number of different problem sizes and three different versions of the neighborhood communication In sec V we discuss some related work and 002nally Sec VI summarizes our 002ndings and presents the currently ongoing work in that area II T HE A BSTRACT D ATA AND C OMMUNICATION L IBRARY The Abstract Data and Communication Library ADCL enables the creation of self-optimizing applications by allowing an application to register alternative versions of a particular function From the conceptual perspective ADCL takes advantage of two characteristics of most scienti\002c applications 1 Iterative execution  most parallel scienti\002c applications are centered around a large loop and execute therefore the same code sequence over and over again Consider for example an application which solves a timedependent partial differential equation PDE These problems are often solved by discretizing the PDE in space and time and by solving the resulting system of linear equations for each time step Depending on the application iteration counts can reach six digit numbers 2 Collective execution  most large scale parallel applications are based on data decomposition i.e all processes execute the same code sequence on different data items Processes are typically also synchronized i.e all processes are in the same loop iteration This synchronization is often required for numerical reasons and is enforced by communication operations ADCL uses the initial iterations of the application to determine the fastest available code version Once performance data on a suf\002cient number of versions is available the library makes a decision on which alternative to use throughout the rest of the execution Two key components of ADCL are the algorithm used to determine which versions of a particular operation shall be tested and how to decide ef\002ciently across multiple process on the best performing version In the following we give some details on both components A Version Management and Selection A fundamental assumption within ADCL is that the library has multiple alternative versions for a particular functionality available to choose from These alternatives will be stored as different functions in the same function-set  The number of alternatives can reach from a few e.g the user providing two different version of a parallel matrix-multiply operation to many millions in case the user is exploring different values for internal or external parameters such as various buffer sizes loop unroll depth etc In case of such large numbers of alternative versions it is unrealistic to assume that the library can test all available versions before deciding which one performs best As of today ADCL incorporates two different strategies for version selection at runtime The 002rst version incorporates a simple brute-force search which evaluates all available alternatives Clearly this approach has the limitations outlined above and we are working on extending this approach by using some early stopping criteria as outlined in An alternative version selection algorithm is used if the user annotates the implementations by a set of attributes/attribute values These attributes are used to reduce the time taken by the runtime selection procedure by tuning each attribute separately Given a certain number of measurements conforming that a particular attribute value leads to better performance than versions using other values for the very same attribute all implementations/versions of the corresponding functionset not having the optimal value for that attribute are being discarded by the library As has been shown in this can signi\002cantly reduce the number of versions to be tested by ADCL and improve the overall execution time The main restriction of this approach lies in the assumption that attributes are not correlated However there are a number of algorithms known in the literature to overcome this restriction e.g from the experimental design theory the 2 k factorial design  algorithms B Collective Decision Logic Independently of the version selection approach used by the library the collective decision logic of ADCL will have to compare performance data of multiple functions gathered on different processes The challenge lies in the fact that in the most general case processes only have access to their own performance data and performance data for the same code version might in fact differ signi\002cantly across multiple processes Distributing the performance data of all processes 
266 
266 


for all versions to all other processes is however not feasible since the costs for communicating these large volumes of data would often offset the performance bene\002ts achieved by runtime tuning The approach taken by the library relies therefore on data reduction i.e each process provides only a single value for each alternative version of the code section being optimized In order to detail the algorithm lets assume ADCL gathers n measurements/data points for each version i on each process j  Let us denote the execution time of the k th measurement by t  i j k   In an initial step the library removes outliers i.e measurements not ful\002lling a condition C  t  i j k  j t  i j k   b 001 min k t  i j k   with b being a well de\002ned constant from the data set This leads to a 002ltered subset M f  i j   f t  i j k  j t  i j k  ful\002lls Cg 1 of measurements with cardinality n f  i j   Then the performance measurements for each version are analyzed locally on each process and characterized by the local average execution time m  i j   1 n X k t  i j k  2 and its 002ltered counterpart m f  i j   1 n f X k 2 M f  i;j  t  i j k  3 as estimates of the mean value In a global reduction operation the library determines for each version the maximum average execution time across all processes m  i   max j m  i j   4 m f  i   max j m f  i j  5 considering all respectively only 002ltered data 3 and the maximum number of outliers n o  i  over all processes n o  i   max j n o  i j   This reduction is motivated by a fundamental law in parallel computing which states that the performance of a synchronous application is determined by the slowest process/processor Finally the library selects the maximum execution time including or excluding outliers by r  i   032 m f  i  if n o  i    nmax o m  i  otherwise depending on whether the maximum number of outliers is exceeded or not The algorithm i 0 ful\002lling r  i 0   min i r  i  is chosen as the best one Assuming that the runtime environment produces reproducible performance data over the lifetime of an application this algorithm is guaranteed to 002nd the fastest of available implementations for the current tuple of f problem size runtime environment versions tested g  C An example function-set In the following we would like to give a concrete example for a function-set in ADCL by describing the alternative versions for the most relevant communication pattern in parallel computing namely the n-dimensional neighborhood communication This communication patterns often occurs for applications relying on data decomposition where each process owns a rectangular portion of the overall computational domain of equal size Typically processes are mapped onto a regular n-dimensional cartesian process topology Due to the numerical operations performed on the data a process often needs read access to data items owned by its neighboring processes A typical solution for this problem is to keep a copy of these data items in addition to the items owned by a process in so-called ghost cells  These ghost-cells have to be updated frequently e.g in every iteration of an iterative solver A process is not allowed to modify a ghost-cell Fig 1 shows an example for nine processes which are arranged in a 2-D logical process topology and the resulting messages between the processes when updating the ghost-cells Ghost cells are depicted in green while the main computational domain of each process is represented as white boxes Due to the local structure of the discretization scheme a processor has to communicate with at most two processes for a 1-D decomposition four processes for a 2-D decomposition and six processes for a 3-D decomposition Fig 1 Data exchange occurring in the 2-D neighborhood communication As of today ADCL uses three attributes in order to characterize a version/implementation of the n-dimensional neighborhood communication 1 Number of simultaneous communication partners this attribute characterizes how many communication operations are initiated at once For neighborhood communication the currently supported values by ADCL are all  ADCL attribute value aao  and one  pair  This parameter is typically bound by the network/switch 2 Handling of non-contiguous messages supported values are derived data types  ddt  and pack/unpack  pack  
267 
267 


The optimal value for this parameter will depend on the communication library and some hardware characteristics 3 Data transfer primitive a total of eight different data transfer primitives are available in ADCL as of today all of them based on the MPI speci\002cation These data transfer primitives can be categorized as either blocking communication e.g MPI Send MPI Recv  non-blocking/asynchronous communication e.g MPI Isend MPI Irecv  or one-sided operations e.g MPI Put MPI Get  Which data transfer primitive will deliver the best performance depends on the implementation of the corresponding function in the MPI library and potentially some hardware support e.g for one-sided communication Note that not all combinations of attributes really lead to feasible implementations As an example implementations using a blocking data transfer primitives such as MPI Send/Recv can not be applied for implementations having more than one simultaneous communication partner Therefore a total of 20 implementations are currently available within ADCL for the n dimensional neighborhood communication III H ISTORIC L EARNING In the following we would like to present the algorithms used to incorporate historic knowledge to improve the version management and selection algorithm of the library We start by showing the necessity to improve the algorithm outlined in section II-A using two concrete examples The algorithms developed to solve those problems can be split into two separate sections a statistical analysis of the existing data in the history 002le and how to derive decision based on the statistical data A Motivating Examples The 002rst problematic scenario has been observed on a cluster  cacau  which consists of 200 nodes each node equipped with a dual processor Intel EM64T processor at the High Performance Computing Center in Stuttgart Germany For our analysis we used the secondary network of the cluster namely a hierarchical Gigabit Ethernet network This network consists of six 48-port switches which are used to connect the nodes each 48-port switch has four links to the upper level 24 port Gigabit Ethernet switch Thus this network has a 12:1 blocking factor We have executed tests using 64 processors on 64 nodes in order to ensure that communication between the processes has to use two or more of the 48-port switches Although this network con\002guration is rarely found in largescale HPC centers it is not uncommon to have a hierarchical Gigabit Ethernet network in low-cost clusters found at many institutions ADCL managed to 002nd in all test-cases the best performing implementation for the 3-dimensional neighborhood communication for a particular application However comparing the performance of the ADCL version of the code to a handtuned version shows that the automatically tuned version has an overhead of 72 in the 002nal execution time A detailed analysis revealed that the overhead stems from the fact that the runtime selection logic has to test versions of that function set which show extremly poor performance on this network The difference in the execution time between the best and the worst performing version for 700 iterations of that code could be as high as 110 seconds vs 210 seconds Second a large class of HPC applications show a dynamic behavior with respect to the problem sizes used As an example simulations of the air-\003ow around the wings of an airplane require vastly different computational meshes depending on the wing-type distance to the wing air temperature etc Modern codes do not create therefore a uniform mesh to compute the 003ow but have local error criteria which are used to re\002ne the computational mesh upon demand This leads to a computational behavior where the problem size is 002xed for a small number of iterations before it might be re\002ned/modi\002ed again Using the version management and selection algorithm outlined in section II-A ADCL would very often not 002nish the evaluation of the available alternative versions for a given problem size before the computational mesh changes again B Pre-requisites for Historic Learning The main goal of this paper is to develop algorithms which enhance the ADCL library with the capability of making a faster but still accurate decision by reusing the knowledge learned from previous executions A fundamental requirement to incorporate historic knowledge into the ADCL decision logic is to store information gained in one run into a history 002le such that the corresponding information can be accessed and re-used by subsequent executions ADCL has therefore been extended by a history 002le which is stored in an ADCL speci\002c directory  adcl  Once a particular function-set has been evaluated for a given problem size ADCL stores problem characteristics as well as the performance data for that problem The history 002le itself is written in an XML format and can be displayed in a user friendly way in a web-browser by linking the XML 002le to an XSL 002le Characterizing the problem consists of the function-set which has been optimized/explored the process topology used by the application and the problem sizes The functionset identi\002es the operation which has been optimized such as n-dimensional neighborhood communication Furthermore it also identi\002es the collection of different versions which can be explored and optionally the attributes and attributevalues for each version The process topology contains the information about the number of processes used and the logical relation between the processes As an example processes might be logically organized in a cartesian topology where each process has a left/right/upper/lower neighbor As of today we require that the topology information between two problems is identical in order to explore similar entries in the history 002le i.e the application has to utilize the same function-set on the same number of processes We plan to relax 
268 
268 


however this requirement in the future since the number of processes is very often less relevant than the topology itself e.g a 2-D cartesian process topology exposes nearly identical characteristics from the communication perspective for 100 or for 1000 processes The problem size is a function-set speci\002c characterization of the dimensions utilized by the application For the neighborhood communication the problem size is described by the length of the messages used to communicate with each of the neighboring processes The focus of the algorithms presented in the subsequent subsections will be on identifying similar problems based on varying problem sizes which will require a meaningful distance measure between two problem sizes for a given function set Another item in the history 002le will contain a graph describing the network topology between the processes The connection between the processes can be characterized by the network interconnect utilized In\002niBand Gigabit Ethernet etc and some simple characteristics such as network latency and bandwidth This approach to characterize a network in a compact fashion has been explored e.g in the carto framework of the Open MPI library This graph is not required for the second scenario described in section III-A where we assume to have frequently changing problem size within the same execution but would be required for introducing portability of the history 002les across different platforms This feature is however not available as of today The performance data for a given function-set and problem size contains the execution time of the fastest implementation found in the function-set and the optimal attributes values characterizing this particular implementation Furthermore the history 002le also includes the performance data for all other versions tested Although this might sound like large quantities of data which have to be stored each code version tested contains in reality only a single 003oating-point data item in the history 002le C Analysis of Historic Data Algorithm 1 performs an analysis of the historic knowledge base and computes a statistical distance threshold to decide on similarities between different problem sizes The starting point of the algorithm is a number of entries  NbOfPBSizes  in the history 002le for a given function-set For the sake of clarity we consider for the rest of the paper only the entries for the function-set of interest and ignore that the history 002le might contain other entries as well The library de\002nes the relative maximum tolerable performance penalty compared to the best performing implementation as p max  which leads to acceptable performance window  The algorithm determines then for each entry in the history 002le the top cluster  i.e the group of functions in the function-set whose execution time is within p max  of the execution time of the version which achieved the best performance for that problem size The main loop of the algorithm determines for each entry PSR the distance to all other entries PST PST=1..NbOfPbSizes PST 6  PSR in the history 002le and whether the winner version of an entry PSR is in the top cluster of the entry PST  If this is the case a 003ag in a boolean array is set to true for the entry PST  Once all entries for a given problem size have been evaluated sort the boolean array according to the distance of their entries to PSR and de\002ne the maximum distance for the problem PSR as the distance to the last entry in the boolean array until which all entries are marked as true  To clarify the de\002nition of the maximum distance in this context consider a boolean array with the entries  true true false true false  In this case the maximum distance is the distance between the entry PSR and the problem size which lead to the second entry in the boolean array since this is the element until which the winner of PSR delivered an acceptable decision for all the problem sizes within p max  Algorithm 1 AnalyseHistoricData p max  Require Execution time of each implementation for different problem sizes and the corresponding winners f Determine the versions within p max  of the best one for each problem size PS g for PS=1 to NbOfPbSizes do ClusterTopVersions PS p max  end for f Check whether the winner of the problem size reference PSR is in the top versions of test problem size PST g for PSR=1 to NbOfPbSizes do for PST=1 to NbOfPbSizes do if PSR 6  PST then Distance\(PSR PST  CompDist\(PSR PST if Winner PSR  2 TopVersions PST  then IsSimilar PSR PST   1 else IsSimilar PSR PST   0 end if end if end for D max PSR p max   CompMaxDist\(Distance,IsSimilar end for Note that the algorithm automatically chooses the maximum distance to be zero in case a problem does not have any related problems in the history 002le Determining the distance between two problem sizes is function-set speci\002c For the ndimensional neighborhood communication we use as of now the standard euclidean distance using the message length of the data items to be transfered to each neighboring process as the components The result of this analysis is the maximum distance for each problem size in the history 002le This quanti\002es for a given problem size that using a winner version of any problem size within the maximum distance leads to an acceptable performance for the problem size of interest acceptable being de\002ned as not more than p max  above the optimal performance As of today the algorithm outlined above is run in a post-mortem analysis step The maximum distance for each entry in the history 002le is stored along with all the other 
269 
269 


information outlined in the previous subsection We envision these calculations however to be performed in a long-term every time a new entry is being added to the history 002le D Version Management and Selection Algorithms using Historic Data The algorithm as outlined above has one major restriction assuming that ADCL has to execute a problem size for which no entry is available in the history 002le algorithm 1 does not provide yet any hints on how to choose the code version to be used for the given function set However it is straight forward to extend this algorithm to provide a good starting point for a new problem size We suggest two alternative approaches given a new problem size PSNew  algorithm 2 PredictFromClosest predicts the code version by taking the winner version of the closest problem size in the history 002le if PSNew is within the maximum distance determined for the closest problem size Alternatively algorithm 3 PredictFromSimilar suggests to use the winner of a weighted majority vote from the similar problem sizes available in the history 002le considering again only the problem sizes for which PSNew is within their de\002ned maximum distance as a result of algorithm 1 Algorithm 2 PredictFromClosest\(PSNew D max  p max  f Find the closest problem size and its winner g D min  Distance\(PSNew PS1 PredictedWinner  Winner\(PS1 for PS=1 to NbOfPbSizes do D  Distance\(PSNew PS if D 024 D max PS p max   D 024 D min then D min  Distance\(PSNew PS PredictedWinner  Winner\(PS end if end for f Estimate the execution time of the best version for the new problem size by interpolation of the execution times of the closest problem size g EstExecTime  EstExecTime\(PredictedWinner f The estimated maximum execution time is the predicted execution time plus p max  g MaxExecTime  EstExecTime 1  p max 100 return f PredictedWinner MaxExecTime g In both algorithms we can furthermore estimate the execution time of the predicted winner version by interpolating the data available of the related problem sizes involved in the prediction process This predicted execution time can be used as a safety net In case the measured execution time of the predicted winner function deviates more than p max  from the predicted execution time the library dismisses the historic data base and starts an optimization run for PSNew  Algorithm 3 PredictFromSimilar\(PSNew D max  p max  Initialization Con\002dence[NbOfV  0 for PS=1 to NbOfPbSizes do f Find the closest problem sizes g if Distance\(PSNew PS 024 D max PS p max  then Con\002dence\(Winner\(PS  Con\002dence\(Winner\(PS  1/Distance\(PSNew,PS end if end for PredictedWinner  Versions Max\(Con\002dence  f Estimate the execution time of the best version for the new problem size by interpolation of the execution times of similar problem sizes g EstExecTime  EstExecTime\(PredictedWinner f The estimated maximum execution time is the predicted execution time plus p max  g MaxExecTime  EstExecTime 1  p max 100 return f PredictedWinner MaxExecTime g IV E VALUATION A Prediction Accuracy In the following we analyze the prediction accuracy for both prediction algorithms presented above the algorithm using the closest problem size as a reference for the decision and the algorithm using a weighted majority vote of similar problem sizes We explored our prediction techniques for three different function-sets namely the 1D 2D and 3D neighborhood communication For each communication pattern we execute a simple benchmark for a large set of problem sizes  017 50 problem sizes for 1D neighborhood communication from 24 to 128 data items per process 017 55 problem sizes for 2D neighborhood communication from 32x32 to 72x72 mesh points per process 017 60 problem sizes for 3D neighborhood communication from 32x32x32 to 64x64x64 mesh points per process For each problem size we use the execution time of 5000 iterations of the corresponding communication operation for each available implementation/version in ADCL We identify each time the winner function chosen by the ADCL brute force selection logic as well Furthermore we run the prediction algorithms to make a recommendation on the function to be used for each problem size based on the data of all other problem sizes in the history 002le Note that for the purpose of this evaluation the algorithms have been applied on the data gathered by the benchmarks in an off-line analysis not at runtime using ADCL The test cases analyzed use 16 32 and 48 processes on an AMD Opteron cluster using either In\002niBand or Gigabit Ethernet as a network interconnect For each scenario communication pattern and network interconnect we take every entry individually and use the remaining entries for that scenario to predict the winner function for this entry Since the real winner is known we can compute the number of problem sizes 
270 
270 


which are well predicted using a given prediction algorithm for a given acceptable performance window In the 002gures 2 to 7 we compare the prediction accuracy of the algorithms presented in the previous section for different values of p max  A detailed analysis of the results shows that in most cases both prediction algorithms have a success rate of more than 90 for an acceptable performance window of p max  10  This validates the correctness of our approach For the few problem sizes where the historic knowledge base provides a bad recommendation the library will be able to detect the wrong prediction using the predicted execution time Thus ADCL will consequently ignore the historic data base for that instance and start a new optimization using one of the original selection algorithms of ADCL As expected the rate of correct predictions is increasing with increasing the size of the acceptable performance window The success rate of the prediction algorithms is better for smaller number of processes but still reasonably good for the larger test cases analyzed As an example for the 2D pattern using the In\002niBand interconnect 002gure 4 and for an acceptable performance window of p max  10  the correct prediction rate is around 95 when using 16 or 32 processes and around 85 when using 48 processes In most of the scenarios analyzed the weighted majority vote algorithm is outperforming the closest problem size algorithm in terms of prediction accuracy This can be observed easily for the 1D pattern when using 32 and 48 processes 002gures 2 and 3 Fig 2 Prediction Accuracy for the 1D Neighborhood Communication over In\002niBand B Performance Bene\002t of Historic Learning As of now we have demonstrated that the algorithms presented in the previous sections can automatically predict with a reasonable accuracy the version to be used for a new scenario in case there are suf\002cient entries in the history 002le for the corresponding function-set In this second part of this section we demonstrate the performance bene\002t of using the historic Fig 3 Prediction Accuracy 1D for the Neighborhood Communication over Gigabit Ethernet Fig 4 Prediction Accuracy 2D for the Neighborhood Communication over In\002niBand learning capability introduced within ADCL for a simple test case For this we use a benchmark consisting of a parallel iterative solver as often applied in scienti\002c applications The software used in this subsection solves a set of linear equations that stem from discretization of a partial differential equation PDE using center differences The parallel implementation subdivides the computational domain into sub-domains of equal size The processes are mapped onto a regular threedimensional cartesian topology performing 3D neighborhood communications We evaluate the execution time of this application using the two existing selection algorithms already available within ADCL the brute force search and the attribute based search algorithm Furthermore we evaluate the performance of the code when using the historic knowledge base assuming that an entry for the corresponding problem size is already available 
271 
271 


Fig 5 Prediction Accuracy 2D for the Neighborhood Communication over Gigabit Ethernet Fig 6 Prediction Accuracy 3D for the Neighborhood Communication over In\002niBand in the history 002le The goal of this section is not to fully evaluate the new algorithms and their runtime behavior but to determine the potential bene\002t of historic learning in case of a correct prediction Fig 8 shows the results obtained for 32 processes test case on the same cluster as described in the previous subsection using the Gigabit Ethernet interconnect The results indicate that the attribute based selection logic outperforms the brute force search approach due to its ability to exclude some of the badly performing versions early in the selection procedure However the performance of the application in this scenario could be further improved by using the historic learning capability within ADCL which skips testing under-performing implementations mocking the PredictFromClosest algorithm Fig 7 Prediction Accuracy 3D for the Neighborhood Communication over Gigabit Ethernet Fig 8 Performance Comparison of an Application utilizing the Brute Force Search the Attribute Based Search and Historic Learning V R ELATED W ORK Among the numerical libraries incorporating adaptive techniques are ATLAS 15 and FFTW 16 A TLAS abstracts the BLAS interfaces and provides several implementations for each function During an extensive con\002gure step ATLAS determines the best performing implementation on a speci\002c platform with a speci\002c compiler Furthermore based on additional information such as cache sizes ATLAS determines optimal internal parameters such as the blocking factor for blocked algorithms However ATLAS does not perform any runtime optimizations Star-MPI incorporates runtim e optimization of collective operations similarly to ADCL Unlike ADCL Star-MPI has only a single runtime decision logic namely a brute force search whereas one of the main research focuses of ADCL is 
272 
272 


to develop alternative runtime decision algorithms in order to speed up the runtime decision logic The FFTW library optimizes Fast Fourier Transform FFT operations To compute an FFT the user has to invoke 002rst a planner specifying a problem which has to be solved The planner measures the actual runtime of many different implementations and selects the fastest one In case many transforms of the same size are executed in an application this plan delivers the optimal performance for all subsequent FFTs Since the planner can be time consuming FFTW also provides a mode of operation where the planner comes up quickly with a good estimate which might however not necessarily be the optimal one The decision process is initiated just once by the user Thus FFTW makes the runtime optimization upfront in the planner step which does not perform any useful work On the other hand ADCL integrated the runtime selection logic into the regular execution of the applications This is especially important since the ADCL approach enables the library to restart the runtime selection logic in case signi\002cant deviations from the original performance e.g due to changing network conditions have been observed FFTW has  to a limited extent  also the notion of historic learning namely with a feature called Wisdom  The user can export experiences gathered in previous runs into a 002le and reload it at subsequent executions However the wisdom concept in FFTW lacks any notion of related problems i.e wisdom information can only be reused for exactly the same problem size that was used to generate it Furthermore the wisdom functionality also does not include any mechanism which helps to recognize outdated or invalid wisdom e.g if the platform used for collecting the wisdom is signi\002cantly different than the platform used while reloading the wisdom VI C ONCLUSION This paper presents a new set of algorithms which have been designed in order to reduce the time spent in the runtime decision procedure of the Abstract Data and Communication Library ADCL by incorporating historic knowledge from previous executions The algorithms use the notion of an acceptable performance window  which is de\002ned as the relative performance penalty an application is willing to accept compared to the fastest possible version for the sake of a faster decision procedure Furthermore the algorithms establish the notion of a maximum distance for each entry in the history 002le The maximum distance provides a measure which indicates when the winner function of a particular entry will lead to an acceptable decision for other entries Using these mechanisms we de\002ned two alternative algorithms for recommending a version to a new unknown problem size one algorithm uses the recommendation of the closest problem found in case the distance between the new problem and the closest problem is below the maximum distance de\002ned for the closest problem the second algorithm introduces a weighted vote among all entries in the history 002le whose distance to the new problem size is below their maximum distance The algorithms presented also contain an automatic measure for indicating when to dismiss recommendations based on historic knowledge This safety net is based on comparing the predicted performance vs the real performance of the recommended code version We evaluated both algorithms for three different function sets using two different network interconnects The results indicate a reasonable rate of good recommendations by both algorithms the weighted vote algorithm outperforming usually the one based on the closest problem size Our future work includes an extended analysis of the new algorithms for other communication patterns such as circular shift operations or broadcast operations Of special importance is the evaluation of the new algorithms within the context of adaptive HPC applications In a long term we also plan to include further runtime selection algorithms such as including early stopping criteria or 2 k factorial design algorithms for v ery lar ge p arameter spaces R EFERENCES  R C Whale y and A Petite Minimizing de v elopment and maintenance costs in supporting persistently optimized BLAS Software Practice and Experience  vol 35 no 2 pp 101–121 2005  J Pjesi v ac-Grbo vic G Bosilca G E F agg T  Angskun and J J Dongarra MPI Collective Algorithm Selection and Quadtree Encoding Parallel Computing  vol 33 pp 613–623 2007  J J Ev ans C S Hood and W  D Gropp Exploring the Relationship Between Parallel Application Run-Time Variability and Network Performance in Proceedings of the Workshop on High-Speed Local Networks HSLN IEEE Conference on Local Computer Networks LCN  October 2003 pp 538–547  F  Petrini D J K erbyson and S P akin The Case of the Missing Supercomputer Performance Achieving Optimal Performance on the 8,192 Processors of ASCI Q in SC 03 Proceedings of the 2003 ACM/IEEE conference on Supercomputing  Washington DC USA IEEE Computer Society 2003 p 55  E Gabriel and S Huang Runtime optimization of application le v el communication patterns in Proceedings of the 2007 International Parallel and Distributed Processing Symposium 12th International Workshop on High-Level Parallel Programming Models and Supportive Environments  2007 p 185  E Gabriel S Feki K Benk ert and M Chaara wi The Abstract Data and Communication Library accepted for publication in Journal of Algorithms and Computational Technology  p t.b.d 2008  M Chaara wi J M Squyres E Gabriel and S Feki An Open Tool for Parameter Optimization in accepted for publication at the EuroPVM/MPI conference  Dublin Ireland September 2008  E Gabriel S Feki K Benk ert and M M Resch T o w ards Performance and Portability through Runtime Adaption for High Performance Computing Applications in accepted for publication at the International Supercomputing Conference  Dresden Germany June 2008  R V uduc J W  Demmel and J A Bilmes Statistical Models for Empirical Search-Based Performance Tuning International Journal for High Performance Computing Applications  vol 18 no 1 pp 65–94 2004  G E P  Box W  G Hunter  and J S Hunter  Statistics for Experimenters An Introduction to Design Data Analysis and Model Building  ser Wiley Series in Probability and Mathematical Statistics John Wiley  Sons Inc 1978  R K Jain The Art of Computer Systems Performance Analysis Techniques for Exp erimental Design Measurement Simulation and Modeling  Wiley 1991  K Benk ert E Gabriel and M M Resch Outlier Detection in P arallel Performance Data for an Adaptive Communication Library in 9th IEEE International Workshop on Parallel and Distributed Scienti\002c and Engineering Computing  Miami FL USA April 2008  Message P assing Interf ace F orum MPI A Message Passing Interface Standard  June 1995 http://www.mpi-forum.org 
273 
273 


 Open MPI on host topology detection  https://svn.openmpi.or g/trac ompi/wiki/OnHostTopologyDescription 2007  R C Whale y and A Petite Minimizing de v elopment and maintenance costs in supporting persistently optimized BLAS Software Practice and Experience  vol 35 no 2 pp 101–121 2005  M Frigo and S G Johnson The Design and Implementation of FFTW3 Proceedings of IEEE  vol 93 no 2 pp 216–231 2005  A F araj X Y uan and D Lo wenthal ST AR-MPI self tuned adapti v e routines for MPI collective operations in ICS 06 Proceedings of the 20th Annual International Conference on Supercomputing  New York NY USA ACM Press 2006 pp 199–208 
274 
274 


 11 R EFERENCES     Reddy, M.K. and S.M. Reddy 223Detecting FET Stuck-Open Faults in CMOS Latc hes and Flip-Flops,\224 IEEE Design and Test of Computers Vol. 3 , No. 5 , pp. 17-26, October 1986   2 R Mad g e , M. Vilg is, a nd V. Bhide, "Achieving Ultra High Quality and Reliability in Deep Sub-Micron Technologies using Metal Layer Configurable Platform ASICs", MAPLD 2005    Kewal Sal u ja, \223Di g i t a l Sy st em Fundam e nt al s, Lect ure 11\224, Department of Electrical Engineering, University of Wisconsin Madison    Yu W e i  Papos ng  M oo Ki t Lee Peng W e ng Ng C h i n  Hu Ong, \223IDDQ Test Challenges in Nanotechnologies: A Manufacturing Test Strategy\224, Asian Test Symposium 2007. ATS apos;07. 16 th Volume , Issue , 8-11 Oct. 2007 Page\(s\211 \226 211  NASA GSFC Advi sory NA-GSFC 2004-06   Dan El ft m a nn, Sol o m on W o l d ay and M i nal  Sawant  New Burn In \(BI\ethodology for Testing of Blank and Programmed Actel 0.15 \265m RTAX-S FPGAs MAPLD 2005   M i nal Sawant Dan El ft m a nn,  W e rner van den Abeel an John McCollum, Solomon Wolday and Jonathan Alexander 223Post Programming Burn-in of Actel O.25um FPGA\222s\224 MAPLD 2002  B IOGRAPHY   John worked 2 years at Faichild R&D on bipolar switching performance specifically platinum dopedlife time control and the development of Ion Implantation.  He worked 15 years at Intel developing Intel's first bipolar PROM, Ion Implantation, the world's first 16K DRAM, as well as 64K and 256K DRAMs.  Mr. McCollum developed Intel's first dual layer metal CMOS technology for the 386 microprocessor.  He co-founded Actel and worked the last 20 years on process, antifuse and flash cell development and FPGA Architecture at Actel.  He holds over 50 patents   covering Process Technology, Antifuse and NVM technology, FPGA Architecture, Analog Processing and Radiation Hardening.  He has presented numerous papers at IEDM, MAPLD, CSME, SPWG, and the FPGA Symposium. He is currently a Fellow in the Technology Development Department 


Time Time 50 350   10 0                   10 1                   10 2 12.5 50 350   10 0                   10 1  12 expected from Figure 7, the width of the uncertainty region is compressed by the curvature of the monopulse response resulting in a detection-primitive with greater uncertainty than the variance admits.  A filter lag or so-called cluster tracking can easily result in a 5% or greater offset and degraded consistency.  After 300 seconds the curves peak up because the target is appr oaching a low-elevation beampoint limit.  This occurs anytime a target is tracked into the edge of the radar\222s field of re gard and can lead to radar-toradar handover difficulty         300 300 80  s D 2 k,1 k y    s D 2 k,1 k y   100 150 200 250 10 1                    10 5 1 0  Figure 8 - Consistency versus distance from beam center Monopulse Mismatch The next set of curves plotted in Figure 9 show the sensitivity of detection-primitive consistency to a mismatch in the monopulse slope.  All of these curves were generated using a linear monopulse response derived from the slope of the true monopulse response at beam center.  The slope of the 80% curve is 0.8 times th e beam-center slope; the 90 curve is 0.9 times the beam-center slope; and so on for 100%, 110% and 120%.  Again, the order of curves in the graph is the same as the legend order A steep slope tends to expand y I 222s uncertainty while a gentle slope tends to compress it.  An expanded uncertainty leads to a smaller consistency while a compressed uncertainty leads to a larger consistency.  This behavior can be observed in the family of curves in Figure 9.  Curves for the steeper slopes are on the botto m while curves for more gentle slopes are on top.  The notable feature of this set of curves is that the sensitivity to a mismatch in the monopulse slope is not very significant       100 150 200 250 10 1                    90 100 110 120  Figure 9 - Consistency versus monopulse mismatch Range-Bias Error The complex nature of the monopulse radar models presents ample opportunity to introduce errors in the software implementation.  One such e rror introduced in a \275 rangecell-width bias in the detection-primitive range which in turn resulted in a significant degradation to 2 9 k D The fact that 2 9 k D is measured in different coordinates compared to the bias made it difficult to determine which value or algorithm was to blame.  Examining the intermediate consistency values led directly to the error source A comparison between biased 2 1 k D  2 2 k D and 2 3 k D values and unbiased 2 2 k D values is shown in Figure 10.  The unbiased 2 2 k D is the bottom-most curve and the biased 2 3 k D is the top-most curve with a value around 80.  This large value for 2 3 k D indicates that there is a lot more uncertainty in the range measur ements compared to what is predicted by the range varian ce.  Since the range-variance calculation is easy to confirm, the problem must be in the algorithms that model or manipulate range A notable feature of Figure 10 is the sensitivity of the centroiding algorithm to range bias in the detection primitives.  The range bias is ba rely noticeable in the biased 2 1 k D and 2 2 k D curves.  Of course, if the unbiased 2 2 k D  curve existed as a baseline it would be relatively easy to spot the error 


Time Time Time 50 350   10 0                   10 1                   10 2 50 350   10 1                   10 0                   10 1 50 350   10 0                   10 1  13         Isolated No SNR Adjust  Figure 11 - Centroiding for isolated range cells Filter Tuning Now that the centroided m easurements are reasonably consistent, the parameters that govern track filtering can be examined.  As previously promised, the effects and corrections for atmospheric refr action and sensor bias have been disabled so that 2 8 k D can be analyzed using a sliding window.  Of course the full analysis would include these effects and 2 8 k D at each time step would be collected and averaged over many trials Plots of the effect of changing process noise in a nearlyconstant-velocity filter are shown in Figure 12 and Figure 13 for Cartesian position and velocity respectively.  In both figures, the plotted values have been divided by 3 so that the desired value is always 1.  Increasing the process noise up to a point should increase the updated uncertainty and reduce 2 8 k D values.  Except near th e end of the trajectory when the measurements are off of beam center, the curves in Figure 12 and Figure 13 appear inconclusive for this expected trend If 2 8 k D values are way out of range there are additional intermediate filter values that can be examined.  For example, the state extrapolati on algorithms can be examined by comparing the consistency of 1 210 Isolated With SNR Adjust 300 300 300 0.005 212 212 212 212 212 kkkk T kkkk D xhzSxhz 35        s D 2 k Range   D 2 k,2 biased D 2 k,1 biased D 2 k,2  Figure 10 - Range bias error in detection primitive Centroiding Algorithm From Section 3, assuming that the centroided-range uncertainty for an isolated range cell is the same as its detection-primitive uncertainty may be incorrect Collecting and plotting 2 3 k D values only from isolated range-cell measurements can be used to analyze such assumptions.  The plots in Figure 11 compare differences between the isolated-cell algorithm defined in Section 3, an algorithm that modifies the uncertainty based on the SNR in the isolated cell, and the 2 3 k D values from all measurements 34\was used to modify the range uncertainty for the upper line labeled Isolated with SNR Adjust    4 22  2 2  resRi o R R Rn bdp bm  s D 2 k,3 Range    s D 2 k,8 Position     212 1 can also be examined using \(35 The residual is also commonly used to determine the assignment cost  212 kk z  P  k  k1 with z k The consistency of the innovation covariance k T kkkkk RHPHS 100 150 200 250 10 1                    D 2 k,3 biased 100 150 200 250 10 2                    100 150 200 250 10 1                    0.5 50  Figure 12 \226 Position consistency, filter tuning example  r  t t 34 If the All Centroided curve \(middle\as the baseline doing nothing \(lower\imates the uncertainty and 33\imates the uncertainty.  Dividing by the square root of the observed SNR leads to a more consistent covariance; however, there is currently no statistical evaluation to justify it             210 210 1 1 1 2 All Centroided 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


