New Algorithms for E\016cien t Mining of Asso ciation Rules 003 Li Shen Hong Shen and Ling Cheng Sc ho ol of Computing and Information T ec hnology Gri\016th Univ ersit y  Nathan QLD4111 Australia Abstract Disco v ery of asso ciation rules is an imp ortan t data mining task Sev eral algorithms ha v e b een prop osed to solv e this problem Most of them require rep eated passes o v er the database whic h incurs h uge I/O o v erhead and high sync hronization exp ense in parallel cases There are a few algorithms trying to reduce these costs But they con tains w eaknesses suc h as often requiring high pre-pro cessing cost to get a v ertical database la y out con taining m uc h redundan t computation in parallel cases and so on W e prop ose new asso ciation mining algorithms to o v ercome the ab o v e dra wbac ks through minimizing the I/O cost and effectiv ely con trolling the computation cost Exp erimen ts on w ell-kno wn syn thetic data sho w that our algorithms consisten tly outp erform Apriori  one of the b est algorithms for asso ciation mining b y factors ranging from 2 to 4 in most cases Also our algorithms are v ery easy to b e parallelized and w e presen t a parallelization for them based on a shared-nothing arc hitecture W e observ e that the parallelism in our parallel approac h is dev elop ed more su\016cien tly than in t w o of the b est existing parallel algorithms Keyw ords  data mining asso ciation rule frequen t itemset parallel pro cessing 1 In tro duction Disco v ery of asso ciation rules is an imp ortan t problem in the area of data mining The problem is intro duced in 2  and can b e formalized as follo ws Let I  f i 1 i 2  001\001\001 i m g be a set of literals called items  Let D b e a collection of transactions where eac h transaction d has a unique iden ti\014er and con tains a set of items suc h that d 022 I  A set of items is called an itemset  and an itemset with k items is called a k itemset  The supp ort of an itemset x in D  denoted as 033  x D  is the ratio of the n um b er of transactions in D  con taining x to the total n um b er of transactions in D  An asso ciation rule is an expression x  y  where x y 022I and x  y    The c on\014denc e of x  y is the ratio of 033  x  y D o 033  x D  W e use minsup and minconf to denote the user-sp eci\014ed minim um supp ort and con\014dence resp ectiv ely  An itemset x is fr e quent if 033  x D  025 minsup  An asso ciation rule x  y is str ong if x  y is frequen t and 033  x  y D  033  x D  025 minconf  The problem of mining asso ciation rules is to 014nd all strong asso ciation rules 003 This w ork w as partially supp orted b yAR C Large Researc h Gran t 1996-98 A849602031 Since it is easy to generate all strong asso ciation rules from all frequen t itemsets almost all curren t studies for asso ciation disco v ery concen trate on how to 014nd al l fr e quent itemsets e\016ciently  Our pap er will also fo cus on this k ey problem Sev eral algorithms 1  2  3 4 5  6  8 9 10 11  12  ha v e b een prop osed in the literature to solv e this problem Ho w ev er most of them are based on the Apriori approac h 1 and require rep eated passes o v er the database to determine the set of frequen t itemsets th us incurring high I/O o v erhead In the parallel case most algorithms p erform a sum-reduction at the end of eac h pass to construct the global coun ts also incurring high sync hronization cost Some algorithms 11  12  ha v e b een in tro duced to o v ercome the ab o v e dra wbac ks Ho w ev er they contains other w eaknesses First they use a v ertical database la y out and assume that it can be a v ailable immediately  But in practice a transaction database usually ha v e a horizon tal la y out Th us to get a v ertical la y out they need to build a new database with the same size Clearly  this pre-pro cessing step costs to o m uc hforav ery large database and ma y increase their costs greatly  Second they attac h a tid-list on eac h candidate p oten tially frequen t itemset in order to compute supp orts for its sup ersets But for a large database and a large amoun t of candidates the space cost for storing tid-lists of all candidates is v ery h uge and migh t not 014t in main memory  Third in parallel cases they generates clusters of candidates and then partition these clusters in to di\013eren t pro cessors Ho w ev er lots of these clusters are o v erlapping and man y candidates ma y o ccur in m ultiple and differen t clusters Th us on one hand these candidates will be coun ted for getting their supp orts at m ultiple pro cessors so that m uc h redundan t computation is pro duced on the other hand to a v oid sync hronization cost a large part of database has to be copied to m ultiple pro cessors whic h is not e\013ectiv ein b oth time and space costs In short parallelism can not b e fully dev elop ed in these algorithms In this pap er w e prop ose new e\013ectiv e asso ciation mining algorithms to o v ercome all the ab o v e dra wbac ks First they are more e\016cien t than Apriori b y reducing I/O cost greatly and con trolling computation cost e\013ectiv ely  Second they are based on the standard horizon tal la y out database and do not ha v e an y pre-pro cessing step Third they are v ery suitable for parallelization and parallelism is easy to be fully dev elop ed for them W e presen t a framew ork of our algorithms in Section 2 describ e our algorithms in Section 3 study their p erformances exp erimen tally in Section 4 prop ose a parallelization for them in Section 5 and conclude the pap er in Section 6 


2 A F ramew ork W e use L to denote the set of all frequen t itemsets and L k to denote the set of all frequen t k itemsets As men tioned b efore the k ey problem of asso ciation disco v ery is to generate L  Almost all curren t algorithms require to compute supp orts for a set of candidates C  and L is formed b y selecting all itemsets with supp ort exceeding minsup from C  Among them Apriori  is one of the b est b ecause it minimizes the size of C  only those itemsets whose subsets are all frequen t are included in C  Ho w ev er to realize that it has to emplo y a lev elwise approac h at eac h lev el k  it needs a scan o v er data to coun t supp ort for eac h c 2 C k  where C k is a sup erset of L k  next it generates L k from C k and then use L k generate C k 1  after that it can start to w ork for the next lev el k 1 Clearly  this approac h con tains high I/O costs and is not suitable for parallelization T o obtain a lo w I/O exp ense and a high parallelism a straigh tforw ard idea is to design a metho d as follo ws generate a candidate set C  023 L  at one or t w o times instead of uncertainly m ultiple times lik e Apriori  If w e can 014nd suc h a metho d w e only need to use one or t w o scans o v er data to obtain all necessary supp orts and then generate L immediately  Clearly  this metho d has a high parallelism b ecause w e can partition not only the candidate set C but also the database in to m ultiple pro cessors to do supp ort-computation in parallel It is v ery easy to see that suc h a metho d do es exist The simplest one is to tak e all itemsets i.e all subsets of I  as candidates Clearly  this naiv e metho d is extremely ine\016cien t due to the exp onen tial size 2 jI j of the candidate set C  T o design e\016cien t metho ds w e need to con trol the size of C  Let c b e an itemset W e use P k  c  to denote the set of all k subsets of c  No ww e prop ose a faster metho d as follo ws 014rst 014nd L 1 and L 2  next use L 2 to generate C all  f c 022 I j P 2  c  022 L 2  j c j 025 3 g  i.e all k itemsets  k 025 3 whose 2-subsets are all frequen t are regarded as candidates then coun t supp orts for all c 2 C all  014nally generate all frequen t k itemsets  k 025 3 b yc hec king supp orts for all c 2 C all  Actually  this approac h is v ery similar to ClusterApr algorithm prop osed in 12  The only di\013erence is that ClusterApr uses a v ertical database la y out W e assume that c is the maximal candidate in size in C all  Th us the size of C all is at least in the order of O 2 j c j  as all k subsets  k 025 3 of c m ust also b e in C all  Hence on the condition that j c j is small the ab o v e metho d runs e\016cien tly b ecause of the limited computation cost and the minimized I/O cost Ho wev er when j c j b ecomes large the computation cost increases greatly due to the h uge size of C all so that the algorithm migh t break do wn T oa v oid this exp onen tial b ottle nec k no ww e prop ose a framew ork whic h is based on the ab o v e metho d but coun ts supp ort for only a small part of candidates in C all when the size of C all is h uge In practice it is not wise to generate all candidates in C all  as it requires not only lots of time but also h uge space An alternativ e metho d is to generate and store those maximal candidates called item cliques  An itemset c 2 C all is an item clique if an y sup erset of c is not in C all  W e alw a ys use Q to denote the set of all item cliques Q i to denote the set of all item cliques with a 014xed size i  maxcliqsize to denote the size of the maximal item clique Clearly w eha v e Q  S maxcliqsize i 3 Q i  and C all  f c j c 022 cl iq  cl iq 2 Q j c j\025 3 g  An Complete Partition of Items CPI in short is de\014ned as a list of non-o v erlapping itemsets suc h that the union of them con tains all items Let I 0 030I n 000 1 b e a CPI By de\014nition w eha v e 1 I i I j   for i 6  j and i j 2f 0 030  n 000 1 g  and 2 S n 000 1 i 0 I i  I  F urthermore w esa y that this CPI has size n and item clusters  clusters in short I 0 030I n 000 1  An itemset c is called an intr a-CPI itemset if c is a subset of an yof I 0 030I n 000 1  otherwise c is an inter-CPI itemset The brief idea of our metho d is as follo ws First w e generate L 1  L 2 and Q  and then use Q to pro duce an e\013ectiv e CPI Second w e coun t supp orts only for all in tra-CPI candidates in C all  and then all frequen t in tra-CPI itemsets can b e obtained W e use C intr a to denote the set of all those in tra-CPI candidates i.e C intr a  f c j c 022 cl iq  cl iq 2 Q j c j 025 3 c is in traCPI g  Third w e build a set of in ter-CPI candidates C inter  f c j c 022 cl iq  cl iq 2 Q j c j\025 3 c is in ter-CPI all in tra-CPI subsets of c are frequen t g  It is easy to see that C inter 022 C all n C intr a con tains all frequen t in ter-CPI k itemsets for k 025 3 b ecause an y subset of an frequen t itemset m ust also b e frequen t Therefore 014nally w e can coun t supp orts for all c 2 C inter  and all frequen tin ter-CPI itemsets can b e obtained Clearly  this metho d is not only correct but also more e\016cien t b ecause of C intr a  C inter 022 C all  Since j C all j is exp onen tial in the size of the maximal item clique w e should try to generate an e\013ectiv e CPI to k eep b oth sizes of C intr a and C inter under con trol W e in tro duce a threshold maxcansize for con trolling the size of C intr a  W e sa y that a candidate c 2 C all is an oversize candidate if the size of c is greater than maxcansize  By setting maxcansize  w e try to generate a CPI suc h that C intr a con tain as few o v ersize candidates as p ossible Since a large maxcansize ma y imply a v ery h uge C intr a and a small one ma y produce a v ery h uge C inter  in practice w e often c ho ose a medium-sized maxcansize e.g 8 9 or 10 to k eep b oth j C intr a j and j C inter j reasonable Based on the premise that j C intr a j has b een con trolled not to b e extremely large b y using maxcansize w e should let C intr a con tains as man y candidates as p ossible to reduce the size of C inter greatly  T o realize that w e can not only k eep the CPI size as small as p ossible but also let eac h item clique be related to as few clusters as p ossible T o sum up w e conclude the framew ork of our metho d b y four phases as follo ws 017 Initialization  Generate all frequen t 1-itemsets and 2-itemsets Use all frequen t 2-itemsets to generate all item cliques p oten tial maximal frequen t itemsets 017 CPI Generation  Use all item cliques and a presp eci\014ed maxcansize to generate a CPI based on the follo wing principles 1 let C intr a con tain as few o v ersize candidates as p ossible 2 k eep the CPI size as small as p ossible 3 let eac h item clique b e related to as few clusters as p ossible 017 In tra-CPI Mining  Use all item cliques to generate C intr a  Coun t supp orts for all c 2 C intr a and generate all frequen tin tra-CPI itemsets 


017 In ter-CPI Mining  Use all item cliques and frequen t in tra-CPI itemsets to get C inter  Coun t supp orts for all c 2 C inter and get all frequen t in ter-CPI itemsets 3 Algorithms In this section w e 014rst design algorithms for completing the ab o v e four phases resp ectiv ely and then describ e our algorithms for mining frequen t itemsets F or simplicit y  w e alw a ys assume that I  f 0  1  001\001\001 N 000 1 g  and use the form of h x 0 x 1  001\001\001 x n 000 1 i to denote an itemset c suc h that c  f x 0 x 1  001\001\001 x n 000 1 g and x 0 x 1  001\001\001 x n 000 1  3.1 Initialization In this phase w e need to 014rst generate L 1 and L 2  and then use L 2 to generate all item cliques Similar to w e use a one-dimensional arra yandat w odimensional arra y to sp eed up the pro cess of obtaining L 1 and L 2  This simple pro cess runs v ery fast since no searc hing is needed After obtaining L 2  w e can use a metho d presen ted in 12  to generate all item cliques T o giv e a brief description w e 014rst intro duce some useful concepts Let x be a frequen t item The e quivalenc e class of x  denoted b y  x  is de\014ned as  x   f y j h x y i 2 L 2 g  The c overing set of  x  denoted b y  x  cv set  is de\014ned as  x  cv set  f y 2  x  j  x    y  n  z  6   for an y z 2  x  g  The algorithm is as follo ws please refer to 12 7 o r more details Algorithm 3.1 The item clique gener ation algorithm GEN CLQ Input  1 I  f 0 030  N 000 1 g  the set of al l items 2 L 2  the set of al l fr e quent 2 itemsets Output  Q 3 030 Q maxcliqsize  wher e maxcliqsize is the size of the maximal item clique and Q i is the set of al l item cliques with size i  Algorithm GEN CLQ for eac h x 2I do  x  f y jh x y i2 L 2 g  for eac h x 2I do  x  cv set  f y 2  x  j  x    y  n  z  6   for an y z 2  x  g  for  x  N 000 1 x 025 0 x 000\000  do  Gener ate al l item cliques  x  cl iq l ist  fh x ig  for eac h y 2  x  cv set do for eac h cl iq 2  y  cl iq l ist do if cl iq  x  then mark cl iq with r emove  if cl iq   x  6   then insert  f x g  cl iq   x  in to  x  cl q l ist suc h that 6 9 A B 2  x  cl q l ist A 022 B  for eac h x 2I do  Put al l item cliques in or der for eac h cl iq 2  x  cl iq l ist without mark r emove do insert cl iq in to Q i  where i is the size of cl iq  maxcliqsize  the size of the maximal item clique 3.2 CPI Generation In this phase w e need to use all item cliques and a pre-sp eci\014ed maxcansize to generate a CPI I 0 030I n 000 1  based on the follo wing principles 1 let C intr a contain as few o v ersize candidates as p ossible 2 k eep the CPI size n as small as p ossible 3 let eac h item clique b e related to as few clusters as p ossible Clearly  if maxcansize 025 maxcliqsize w e can easily put all items in to one cluster suc h that the generated CPI con tains only one cluster I  In this case all ab o v e principles ha v e b een satis\014ed No w w e consider the opp osite case maxcansize  maxcliqsize  W e use cpisize to denote the size of the desired CPI Let cl iq b e the maximal item clique Based on the 014rst principle w e hop e that all in tra-CPI subsets of cl iq ha v e sizes no greater than maxcansize  T o meet this requiremen t w e should ha v eat least maxcliqsize 000 1 maxcansize 1 clusters in the CPI b ecause eac h cluster can con tain at most maxcansize items in cl iq  T o satisfy the second principle w e simply set cpisize  maxcliqsize 000 1 maxcansize  1 in our metho d Let x b e an item W e use x:cl s 2f 0 030  cpisize 000 1 g to indicate that x is an item in cluster I x:cls  W e use I Q to denote the set of all items o ccurring in Q  Th us our main task is to generate x:cl s for eac h x 2 I  After initially setting all x:cl s  unk now n  w e can use a scan of Q to assign a v alue to x:cl s for eac h x 2 I Q  Certainly  giv en an item clique cl iq 2 Q  w e only need to generate x:cl s for eac h x 2 cl iq with x:cl s  unk now n  W ein tro duce suc h a pro cedure as follo ws Pro cedure assigncluster cl s  an cluster cl iq  an item clique A unk now n  f x 2 cl iq j x:cl s  unk now n g  if A 0   then return   Each item x 2 cl iq has b e en put in some cluster for  i 0 i cpisize  i  do A i  f x 2 cl iq j x:cl s  i g  Cho ose k from 0 030 cpisize 000 1 suc h that j A k j  max fj A 0 j\030j A cpisize 000 1 jg  if j A unk now n j  j A k j then cl s  k  for eac h x 2 A unk now n do x:cl s  cl s  In this pro cedure w e 014rst partition the giv en cl iq in to A unk now n A 0  001\001\001 A cpisize 000 1 based on A i  f x 2 cl iq j x:cl s  i g  Let A k b e the maximal partition in size among A 0 030 A cpisize 000 1  If j A unk now n j 025 j A k j  w e simply set x:cl s  cl s for eac h x 2 A unk now n  where cl s is an input parameter v alue Ho w ev er when j A unk now n j  j A k j  it is easy to see that there are quite a few items in cl iq b elonging to cluster I k and the size of A unk now n is rather small In this case w e put all elemen ts of A unk now n in to cluster I k to maximize the n um ber of in ter-CPI subsets of cl iq and let cl iq be related to as few clusters as p ossible the third principle No w b y a scan of Q and calling assigncluster cl s  cl iq  for eac h cl iq 2 Q  w e can generate x:cl s for all x 2I Q  W e let cl s ha v e an initial v alue 0 During the scan o v er Q ifw e call assigncluster cl iq 1  cl s  to process cl iq 1 w e alw a ys call assigncluster cl iq 2  cl s 1 mo d cpisize  to pro cess the next clique cl iq 2  where mo d refers to the mo dule op erator It is easy to see that our scan of Q should not start from an y item 


clique with size greater than maxcansize  b ecause otherwise o v ersize in tra-CPI candidates will b e generated immediately  Our strategy is to scan Q in the follo wing order Q maxcansize  Q maxcansize 000 1  001\001\001  Q 3  Q maxcansize  1  001\001\001  Q maxcliqsize  where item cliques within eac h Q i for all i 2 f 3 030 maxcliqsize g are k ept in lexicographic order Clearly  after 014nishing scans for Q 3 030 Q maxcansize  most items in I Q usually ha v e b een assigned in to some clusters Th us the follo wing scans for Q maxcansize  1 030 Q maxcliqsize do not tend to generate o v ersize in tra-CPI candidates in the CPI In addition b y starting from Q maxcansize  w e try to let eac h item clique b e related to as few cluster as p ossible to meet the third principle men tioned b efore After obtaining x:cl s for all x 2 I Q  w e prop ose t w o metho ds adjustcluster\(I and adjustcluster\(I I to adjust these x:cl s s and try to mak e C inter con tain as few o v ersize candidates as p ossible W e will describ e these metho ds later When w e 014nish all the ab o v e w ork it is easy to see that w eha v e InI Q  f x 2I j x:cl s  unk now n g  Note that all these items in InI Q are uninter esting for us b ecause they w on't o ccur in an yin tra-CPI or in ter-CPI candidate that concerns us With this observ ation w e simply set x:cl s  0 for all x 2InI Q  and th us all I 0 030I cpisize 000 1 can b e obtained b y scanning x:cl s for eac h x 2I  No ww e can presen t the description of our algorithms for CPI generation as follo ws where GEN CPI\(X denote the algorithm that calls adjustcluster\(X to do cluster adjustmen t and X ma y b e either I or I I Algorithm 3.2 The CPI gener ation algorithms GEN CPI\(I and GEN CPI\(II Input  1 Q  S maxcliqsize i 3 Q i  the set of al l item cliques 2 maxcansize  a pr e-sp e ci\014e d thr eshold for c ontr ol ling the size of C intr a  Output  I 0 030I cpisize 000 1  a CPI Algorithm GEN CPI\(X  X 2f I II g for eac h x 2I do x:cl s  unk now n  cl s 0 cpisize  maxcliqsize 000 1 maxcansize 1 for  i  maxcansize  i 025 3 i 000\000  do for eac h cl iq 2 Q i do assigncluster cl s  cl iq  and cl s  cl s 1 mo d cpisize  for  i  maxcansize 1 i 024 maxcliqsize  i  do for eac h cl iq 2 Q i do assigncluster cl s  cl iq  and cl s  cl s 1 mo d cpisize  if X=I then adjustcluster\(I else adjustcluster\(I I for  i 0 i cpisize  i  do I i    for eac h x 2I do if x:cl s  unk now n then I 0  I 0 f x g else I x:cls  I x:cls f x g  Pro cedure adjustcluster\(X  X 2f I II g for eac h x 2I do x:ad j 0 for eac h cl iq 2 Q do for eac h A satisfying A is a maximal o v ersize in tra-CPI subsets of cl iq do A 0  f x 1 030 x n g  f x 2 A j x:ad j 0 g and A 1  f x 2 A j x:ad j 1 g  for  i 1 i 024 n  i  do if  j A 1 j  i 024 maxcansize  then x i ad j 1 else x i ad j 2 if X=I I then cpisize  for eac h x 2I with x:ad j 2 do if X=I I then x:cl s  cpisize 000 1 else x:cl s  x:cl s 1 mo d cpisize  It is easy to see that the main pro cedure of our algorithm GEN CPI\(X emplo ys the idea men tioned b efore No ww e explain the adjustcluster\(X pro cedure where X ma ybeIorII.W e initially set x:ad j  0 for all x 2I  whic h means x has not b een found in an y o v ersize in tra-CPI candidate Then w e use a scan o v er Q to pro cess eac h maximal o v ersize in tra-CPI candidate A  F or eac h A  w e will set either x:ad j  1 or x:ad j  2 for all x 2 A with x:ad j  0 with the goal of making the size of A 1 as close to maxcansize as p ossible where A 1  f x 2 A j x:ad j 1 g  After completing the pro cessing for all those A s w emo v e all x 2I with x:ad j  2 to a new cluster If X=I x will be mo v ed to the next cluster b y setting x:cl s  x:cl s 1 mo d cpisize  If X=I I cpisize will b e increased b y 1 and then x will b e mo v ed to the new cluster I cpisize 000 1  The di\013erence of these t w o metho ds is as follo ws the 014rst one tries to satisfy the third principle while the second one tries to satisfy the 014rst principle It is easy to see that an y old o v ersize in tra-CPI candidate can b e brok en in to t w o smaller in tra-CPI candidates after this adjustmen t 3.3 In tra-CPI Mining In this phase w e will 014rst use the obtained CPI and all item cliques to generate C intr a  then coun t supp orts for all candidates in C intr a  and 014nally generate all frequen t in tra-CPI subsets First w e study the data structure for storing candidates The hash-tr e e structure in tro duced in 1  can only b e used to coun t supp orts for a set of candidates with the same size and so is not suitable for our problem Here w e use a pr e\014x-tr e e T to store a set of candidates with di\013eren t sizes Let r be the ro ot of T  Eac h no de t 2 T con tains four 014elds t:item  t:cn  t:son 1 030 t:cn  and t:sup  W e use t:item to store an item except that r item stores nothing W e use t:cn to store the n umberofc hildren of t  and use t:son 1 030 t:cn  t o store the addresses of all its c hildren W e alw a ys main tain the follo wing prop ert y t:item  t:son 1 item  001\001\001  t:son  t:cn  item  F urthermore eac h t n 2 T corresp onds to a unique candidate denoted b y t n can  suc h that t n can  h t 1 item 001\001\001 t n item i and  r t 1  001\001\001 t n  forms exactly a path or pre\014x from ro ot r to the curren t no de t n  W e use t:sup store the supp ort of t:can  Constructing and up dating suc h a pre\014x-tree is easy  and so w e simply use instr e e T  c  to denote the pro cess of inserting an candidate c in to a pre\014x-tree T  Note that when w e insert a candidate h x 1  001\001\001 x n i in to T an y its pre\014x h x 1  001\001\001 x m i for m 024 n will also b e regarded as a candidate in T  F or example Figure 1 sho ws a pre\014x-tree generated b y the follo wing candidate set C exam  f h 1  2  5 i  h 1  2  6  7 i  h 1  2  6  7  9 i  h 1  2  6  8 i  h 1  4  9 i  h 1  8  9 i  h 2  5  8 i  h 3  5  9 i  h 3  6  7 i  h 3  6  7  8  9 i  h 3  6  7  9 i  h 3  7  9 ig  Note that h 3  6  7  8 i is regarded as a candidate in the tree though it is not in C exam  No w w e can presen t the follo wing pro cedure to generate supp orts for all candidates stored in 


 1 8 56 7 9 24 8 9 2 5 3 5 9 6 9 7 9 7 root 9 8 8 9 Figure 1 An example of a pre\014x-tree pre\014x-tree T  Pro cedure Coun tSup T  a pre\014x-tree D  a transaction database Set t:sup  0 for eac h t 2 T and let r be the ro ot of T  for eac h transaction d 2D do Let h x 1 030 x n i b e the itemset con taining exactly all frequen t items in d  IncSup r  h x 1 030 x n i  Pro cedure IncSup t  a pre\014x-tree no de h x 1 030 x n i  an itemset t:sup  if n 0 then return  if 9 i 2f 1 030 t:cn g  t:son  i  item  x 1 then IncSup t:son  i  h x 2 030 x n i  for  i 2 i 024 n  i  do IncSup t  h x i 030 x n i  Based on the de\014nition of the pre\014x-tree it is easy to see the correctness of the ab o v e metho d No ww e can presen t our algorithm for In tra-CPI mining as follo ws Algorithm 3.3 The Intr a-CPI mining algorithm INTRA MINER Input  1 Q  the set of al l item cliques 2 I 0 030 I cpisize 000 1  a CPI 3 L 1 and L 2  the sets of al l fr equent 1 itemsets and 2 itemsets 4 minsup  a supp ort thr eshold 5 D  a tr ansaction datab ase Output  L intr a  the set of al l fr e quent intr a-CPI itemsets Algorithm INTRA MINER C intr a  f c j c 022 cl iq  cl iq 2 Q  j c j\025 3 c is in tra-CPI g  Let T b e an empt y pre\014x-tree and for eac h c 2 C intr a do instree T  c  Coun tSup T D  Collect all frequen tin tra-CPI k itemsets for k 025 3in to L intr a b y a scan of T  L intr a  L intr a  L 1 f x 2 L 2 j x is in tra-CPI g  This algorithm is easy to understand First w e use Q to generate C intr a  Next all candidates in C intr a are inserted in to a pre\014x-tree T  Then w e call Coun tSup T D  to coun t supp orts for all candidates in T  Finally  w e can generate L intr a b y a scan of T  L 1 and L 2  The correctness of the algorithm is straigh tforw ard 3.4 In ter-CPI Mining In this phase w e 014rst use all item cliques and all frequen t in tra-CPI itemsets to generate C inter  next coun t supp orts for all c 2 C intr a suball and then generate all frequen tin ter-CPI k subsets for k 025 3 The algorithm is as follo ws Algorithm 3.4 The Inter-CPI mining algorithm INTER MINER Input  1 Q  the set of al l item cliques 2 I 0 030 I cpisize 000 1  a CPI 3 L intr a  the set of al l fr e quent intr a-CPI itemsets 4 L 2  the set of al l fr e quent 2 itemsets 5 minsup  a supp ort thr eshold 6 D  a tr ansaction datab ase Output  L inter  the set of al l fr e quent inter-CPI itemsets Algorithm INTER MINER C inter  f c j c 022 cl iq  cl iq 2 Q j c j\025 3 c is in ter-CPI all in tra-CPI subsets of c are in L intr a g  Let T b e an empt y pre\014x-tree and for eac h cl iq 2 Q do instree T  c  Coun tSup T D  Collect all frequen tin ter-CPI k itemsets for k 025 3 in to L inter b y a scan of T  L inter  L inter f x 2 L 2 j x is in ter-CPI g  This algorithm is easy to understand First w e use Q and L intr a to generate C intr a  Next all candidates in C inter are inserted in to a pre\014x-tree T  Then w e call Coun tSup T D  to coun t supp orts for all candidates in T  Finally w e can generate L inter b y a scan of T and L 2  The correctness of the algorithm is straigh tforw ard 3.5 Algorithm Description No ww e can presen t our algorithms for 014nding all frequen t itemsets as follo ws Algorithm 3.5 The fr e quent itemset mining algorithms MINER\(I and MINER\(II Input  1 I  the set of al l items 2 D atr ansaction datab ase 3 minsup  a supp ort thr eshold 4 maxcansize apr e-sp e ci\014e d thr eshold for c ontr ol ling the size of C intr a  Output  L  the set of al l fr e quent itemsets Algorithm MINER\(X  X 2f I II g Use arra y tec hnique to generate L 1 and L 2  GEN CLQ  Get Q  S maxcliqsize i 3 Q i if maxcliqsize 024 maxcansize then C all  f c j c 022 cl iq  cl iq 2 Q j c j\025 3 g  Let T b e an empt y pre\014x-tree and for eac h c 2 C all do instree T  c  Coun tSup T D  Collect all frequen t k itemsets for k 025 3 in to L b y a scan of T  L  L  L 1  L 2  else GEN CPI\(X  CPI gener ation INTRA MINER  L intr a gener ation INTER MINER  L inter gener ation L  L intr a  L inter  This algorithm is also easy to understand First w e use arra y tec hnique to generate L 1 and L 2  Next w e call GEN CLQ to generate all item cliques If maxcliqsize 024 maxcansize w e kno w that the size of C all is 


limited Hence w e use a pre\014x-tree T to store all candidates in C all and then coun t supp orts for them After that L can b e obtained immediately  If maxcliqsize  maxcansize w e call GEN CPI\(X to do CPI generation call INTRA MINER to obtain L intr a  call INTER MINER to generate L inter  and 014nally w eha v e L  L intr a  L inter  The correctness of the algorithm is straigh tforw ard 4 P erformance Study W eha v e implemen ted our algorithms MINER\(I and MINER\(II for ev aluating their p erfermances In b oth implemen tations w e set maxcansize  10 F or comparison w e ha v e also implemen ted t w o Apriori v ersions One v ersion denoted b y ApriHash uses hashtr e e structure suggested b y to store candidate sets The other v ersion denoted b y ApriPr ef uses our pr e\014xtr e e structure to store candidate sets T o assess the p erformances of MINER\(I  MINER\(II  ApriHash and ApriPr ef w e p erformed sev eral exp erimen ts on a SUN UL TRA-1 w orkstation with 64 MB main memory running Sun OS 5.5 T ok eep the comparison fair w e implemen ted all the algorithms using the same basic data structures except that ApriHash used a differen t hash-tr e e structure to store candidate sets In addition for a fair comparison all our test results do not con tain the execution time for generating L 1 and L 2  b ecause Apriori uses a m uc h slo w er metho d to get L 1 and L 2 than our arra y tec hnique The syn thetic datasets used in our exp erimen ts w ere generated b y a to ol describ ed in W e use the same notation T x:I y D z to denote a dataset in whic h x is the a v erage transaction size y is the a v erage size of a maximal p oten tially frequen t itemset and z is the n um b er of transactions In addition w e generated all datasets b y setting N  1000 and j L j  2000 where N is the n um ber of all items j L j is the n um ber of maximal p oten tially frequen t itemsets Please refer to  for more details on the dataset generation Figure 2 sho ws the exp erimen tal results for four syn thetic datasets to compare the p erformances of these algorithms W e observ e that b oth our algorithms MINER\(I and MINER\(II outp erform b oth ApriHash and ApriPr ef  b y factors ranging from 2 to 4 in most cases Ho w ev er the di\013erence bet w een MINER\(I and MINER\(II is minor In theory  there are three reasons for that our methods outp erform the Apriori approac h First our metho ds only need at most 2 scans of databases for obtaining all frequen t itemsets with size greater than 2 while Apriori requires uncertainly m ultiple scans Hence the I/O cost in our metho ds is usually m uc h smaller than that in Apriori  Second w e design effectiv e metho ds to mak e the size of our candidate set reasonable so that the computation cost is also w ellcon trolled Third using pre\014x-tree structure to store candidates also has t w o adv an tages 1 its space requiremen tisv ery lo w 2 the supp ort-incremen t operations can b e made at eac h visited no de to reduce the computation cost while these op erations can only b e made at leaf no des when hash-tr e e is used T o sum up with these adv an tages our algorithms are more e\016cien t than Apriori  5 P arallelization Since mining frequen t itemsets requires lots of computation p o w er memory and disk I/O it is not only in teresting but also necessary to dev elop parallel algorithms for this data mining task Here w e study ho w to extend our algorithm MINER\(X for parallelization where X 2 f I II g  W e assume a sharednothing arc hitecture where eac hof n pro cessors has a priv ate memory and a priv ate disk The pro cessors are connected b y a comm unication net w ork and can comm unicate only b y passing messages The main idea of our parallelization is as follo ws First the transaction database D is ev enly distributed on the disks attac hed to the pro cessors W e use D i to denote the set of transactions at pro cessor P i  for all i 2 f 1 030 n g  Let c be an itemset The lo c al supp ort c ount of c at pro cessor P i refers to the n um ber of transactions con taining c in D i  W e generate all required candidates and coun t their lo cal supp orts at eac h pro cessor All the obtained lo cal supp ort coun ts will b e sen t to all other pro cessors Th us when a processor completes its lo cal supp ort-coun ting and also receiv es all other lo cal supp ort coun ts from other processors the global supp ort coun ts for all candidates can b e accum ulated and all frequen t itemsets can b e obtained at this pro cessor No w w e in tro duce some concepts to help our discussion Let T 1 and T 2 be t w o pre\014x-trees W e sa y that 1 T 1 is a subtr e e in shap e of T 2  denoted b y T 1 v T 2  if for eac h t 1 2 T 1 there exists t 2 2 T 2 suc h that t 1 item  t 2 item and the p osition or o ccurrence of t 1 in T 1 is the same as that of t 2 in T 2  2 T 1 and T 2 are in the same shap e if T 1 v T 2 and T 2 v T 1  W e in tro duce the follo wing pro cedure to do supp ort accum ulation for T 1 and T 2 in the same shap e and sa v e the result in T 1  Algorithm AccumSup T 1  T 2  for eac h t 1 2 T 1 do Let t 2 2 T 2 suc h that the p ositions of t 1 in T 1 and t 2 in T 2 are the same t 1 sup  t 1 sup  t 2 sup  Before giving our parallel solution w e 014rst parallelize INTRA MINER and INTER MINER as follo ws where w e assume that all required input data can b e a v ailable Algorithm P AR INTRA MINER Ap ar al lelization of INTRA MINER for i 1 to n at pro cessor P i do in parallel C intr a  f c j c 022 cl iq  cl iq 2 Q  c is in tra-CPI and j c j\025 3 g  Let T i b e an empt y pre\014x-tree and for eac h c 2 C intr a do instree T i  c  Coun tSup T i  D i  and send T i to all other pro cessors Receiv e T j s from all other pro cessors P j s  Synchr onization p oint for j 2 to n do AccumSup T 1  T j   T 1 030 T n ar e in the same shap e Collect all frequen tin tra-CPI k itemsets for k 025 3to L intr a b y a scan of T 1  L intr a  L intr a  L 1 f x 2 L 2 j x is in tra-CPI g  


          0 2 4 6 8 10 12 14 16 18 Minimum Support \(in T5.I2.D100K.data ApriHash ApriPref MINER\(I MINER\(II Time\(sec 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05          0 20 40 60 80 100 120 140 Minimum Support \(in T10.I4.D100K.data 0.25 0.15 0.45 0.40 0.35 0.20 0.30 0.60 0.50 0.55 Time\(sec ApriHash ApriPref MINER\(I MINER\(II          0 50 100 150 200 250 Minimum Support \(in T10.I6.D100K.data ApriHash ApriPref MINER\(I MINER\(II Time\(sec 0.60 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.55          0 50 100 150 200 250 300 350 400 450 500 Minimum Support \(in T20.I6.D100K.data ApriHash ApriPref MINER\(I MINER\(II 0.95 1.00 0.90 0.85 0.75 0.65 0.55 0.60 0.70 0.80 Time\(sec Figure 2 P erformance comparison Algorithm P AR INTER MINER  A p ar al lelization of INTRA MINER for i 1 to n at pro cessor P i do in parallel C inter  f c j c 022 cl iq  cl iq 2 Q  j c j\025 3 c is in ter-CPI all in tra-CPI subsets of c are in L intr a g  Let T i b e an empt y pre\014x-tree and for eac h cl iq 2 Q do instree T  c  Coun tSup T i  D i  and send T i to all other pro cessors Receiv e T j s from all other pro cessors P j s  Synchr onization p oint for j 2 to n do AccumSup T 1  T j   T 1 030 T n ar e in the same shap e Collect all frequen tin ter-CPI k itemsets for k 025 3to L inter b y a scan of T 1  L inter  L inter f x 2 L 2 j x is in ter-CPI g  Both algorithms ab o v e emplo y the same idea First eac h pro cessor P i for all i 2 f 1 030 n g indep enden tly inserts all in tra-CPI or in ter-CPI candidates in to a local pre\014x-tree T i  Next P i uses lo cal transaction data D i to obtain the lo cal supp orts for all candidates and sends the result tree T i to all other pro cessors Then it w aits for receiving all T j s from all other pro cessors P j s After all T 1 030 T n are a v ailable P i do es supp ort accum ulation for all candidates and store the global supp orts in to T 1  Finally  L intr a or L inter can b e generated easily at eac h P i  It is not hard to see that the ab o v e pro cedure is complete Th us w e can presen t the parallelization of MINER\(X as follo ws Algorithm P AR MINER\(X  X 2f I II g for i 1 to n at pro cessor P i do in parallel Coun t lo cal supp orts for all 1itemsets and 2-itemsets send the results to all other pro cessors receiv e lo cal supp orts from all other pro cessors accum ulate all lo cal supp orts to get global supp orts and then generate L 1 and L 2  GEN CLQ  Get Q  S maxcliqsize i 3 Q i if maxcliqsize 024 maxcansize then for i 1 to n at pro cessor P i do in parallel C all  f c j c 022 cl iq 2 Q j c j\025 3 g  Let T i b e an empt y pre\014x-tree and for eac h c 2 C all do instree T i  c  Coun tSup T i  D i  and send T i to all other pro cessors Receiv e T j s from all other pro cessors P j s  Synchr onization p oint for j 2 to n do AccumSup T 1  T j   T 1 030 T n ar e in the same shap e Collect all frequen t k itemsets for k 025 3in to L b y a scan of T i  L  L  L 1  L 2  else for i 1 to n at pro cessor P i do in parallel GEN CPI\(X  Get a CPI P AR INTRA MINER  L intr a gener ation at P 1 030 P n P AR INTER MINER  L inter gener ation at P i 030 P n for i 1 to n at pro cessor P i do in parallel L  L intr a  L inter  


It is easy to see that P AR MINER\(X is a natural parallelization of MINER\(X on a shared-nothing arc hitecture b y using our main idea men tioned b efore where X ma y be I or II This parallelization uses a simple principle of allo wing redundan t computations in parallel on otherwise idle pro cessors to a v oid comm unication whic h is also emplo y ed b y Count Distribution algorithm 3 Note that Count Distribution is the fastest parallelization of Apriori  Ho w ev er Count Distribution con tains uncertainly m ultiple sync hronization p oin ts due to the sum-reduction at the end of eac h pass to construct the global coun ts This fact greatly limits the dev elopmen t of parallelism Our P AR MINER\(X has o v ercome this dra wbac k b ecause it con tains at most 3 sync hronization p oin ts F urthermore di\013eren t from the metho ds in 11  all of our database partitions are non-o v erlapping whic h guaran tees there is no redundan t supp ort-coun ting op eration in our metho d Hence our metho d dev elops the parallelism more su\016cien tly than b oth the ab o v e existing metho ds 6 Conclusions W eha v e prop osed new algorithms for e\016cien t mining of asso ciation rules Di\013eren t from all existing algorithms w ein tro duce a concept of CPI Complete P artition of Items and divide all itemsets in to t w ot yp es in tra-CPI and in ter-CPI After obtaining all frequen t 1-itemsets and 2-itemsets w e generate and main tain a set Q of item cliques maximal p oten tially frequen t itemsets F urthermore w eha v e designed t w o metho ds for generating an e\013ectiv e CPI b y using Q  Then w e can use Q and our CPI to get a set C intr a of in traCPI candidates and coun t supp orts for them so that all frequen t in tra-CPI itemsets can be obtained Finally  w e use Q  our CPI and all frequen t in tra-CPI itemsets to generate a set C inter of in ter-CPI candidates and also coun t supp orts for them so that all frequen tin ter-CPI itemsets can b e obtained Our algorithms ha v e sev eral adv an tages First their I/O costs are quite limited b ecause they only require at most 3 scans o v er database Second they can mak e b oth sizes of C intr a and C inter reasonable so that their computation costs are also e\013ectiv ely con trolled Third they use a pre\014x-tree structure to store candidates whic h can also reduce computation cost As a result they app ear to be more e\016cien t than Apriori  one of the b est algorithms for asso ciation disco v ery  T o con\014rm that W e ha v e done the exp erimen ts to compare the p erformances of our algorithms together with Apriori  The test results sho w that our algorithms outp erform Apriori consisten tly  b y factors ranging from 2 to 4 in most cases Another adv an tage of our algorithms is that they are easy to be parallelized W e ha v e also presen ted a p ossible parallelization for our algorithms based on a shared-nothing arc hitecture W e observ e that the parallelism can b e dev elop ed more su\016cien tly in our parallelization than t w o of the b est existing parallel algorithms References 1 R Agra w al H Mannila R Srik an t H T oiv onen and A I V erk amo F ast Disco v ery of Association Rules A dvanc es in Know le dge Disc overy and Data Mining  Chapter 12 AAAI/MIT Press 1996 2 R Agra w al T Imielinski A Sw ami Mining Asso ciations bet w een Sets of Items in Massiv e Databases Pr o c of the A CM SIGMOD Int'l Confer enc e on Management of Data W ashington D.C Ma y 1993 207-216 3 R Agra w al J.C Shafer P arallel Mining of Association Rules IEEE T r ansactions on Know le dge and Data Engine ering  V ol 8 No 6 Decem ber 1996 4 D.W Cheung and Y Xiao E\013ect of Data Sk ewness in P arallel Mining of Asso ciation Rules Pr o c The Se c ond Paci\014c-Asia Conferenc e on Know le dge Disc overy and Data Mining P AKDD-98  Melb ourne Australia April 1998 48-60 5 Dao-I Lin and Zvi M Kedem Pincer-Searc h A New Algorithm for Disco v ering the Maxim um F requen t Set EDBT'98  Marc h 1998 6 Heikki Mannila Hann uT oiv onen and A Ink eri V erk amo E\016cien t algorithms for disco v ering asso ciation rules AAAI Workshop on Know le dge Disc overy in Datab ases KDD-94  pages 181 192 July 1994 7 G D Mulligan and D G Corneil Corrections to Bierstone's Algorithm for Generating Cliques J Asso ciation of Computing Machinery  19\(2 247 Apr 1972 8 Jong So o P ark Ming-Sy an Chen and Philip S Y u An E\013ectiv e Hash-Based Algorithm for Mining Asso ciation Rules Pr o c 1995 A CMSIGMOD Int Conf Management of Data  San Jose CA Ma y 1995 9 Jong So o P ark Ming-Sy an Chen and Philip S Y u E\016cien tP arallel Data Mining for Asso ciation Rules A CM CIKM 95  Baltimore MD USA 10 Ashok Sa v asere Edw ard Omiecinski Shamk an t Na v athe An E\016cien t Algorithm for Mining Asso ciation Rules in Large Databases Pr o c e e dings of the 21st VLDB Confer enc e  Zuric h Switzerland 1995 11 Mohammed Ja v eed Zaki Sriniv asan P arthasarath y  Mitsunori Ogihara W ei Li New P arallel Algorithms for F ast Disco v ery of Asso ciation Rules Data Mining and Know le dge Disc overy Sp e cial Issue on Sc alable High-Performanc e Computing for KDD  pp 343373 V ol 1 No 4 Decem b er 1997 12 Mohammed Ja v eed Zaki Sriniv asan P arthasarath y  Mitsunori Ogihara W ei Li New Algorithms for F ast Disco v ery of Asso ciation Rules T e chnic al R ep ort UR CS TR 651  Univ ersit yof Roc hester 1997 


                 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


