  1 Real-Time Assessment of Robot Performance during Remote Exploration Operations Debra Schreckenghost 1 Terrence Fong 2 Tod Milam 1 Estrellina Pacis 3 and Hans Utz 4 1 TRACLabs, 1012 Hercules, Houston, TX 77058 ghost@ieee.org, tmilam@traclabs.com 2 NASA Ames Research Center, Moffett Field, CA 94035 terry.fong@nasa.gov 3 Space and Naval Warfare System s Command, San Diego, CA 92152 estrellina.b.pacis@nasa.gov 4 Research Institute for Advanced Co mputer Science, Moffett Field, CA 94035 hans.utz@nasa.gov 
  Abstract 227To ensure that robots are used effectively for exploration missions, it is important to assess their performance during operations. We are investigating the definition and computation of performance metrics for assessing remote robotic operations in real-time. Our approach is to monitor data streams from robots, compute performance metrics, and provide Web-based displays of these metrics for assessing robot performance during operations. We evaluated our approach for measuring robot performance with the K10 rovers from NASA Ames Research Center during a field test at Moses Lake Sand Dunes \(WA\n June 2008. In this paper we present the results of evaluating our software for robot performance and discuss our conclusions from this evaluation for future robot 
operations 12  T ABLE OF C ONTENTS  1  I NTRODUCTION 1  2  R ELATED W ORK 2  3  P ERFORMANCE M ONITORING S OFTWARE 3  4  E VALUATION OF R OBOT 
P ERFORMANCE 6  5  D ISCUSSION 8  6  F UTURE W ORK 9  7  C ONCLUSIONS 10  8  A CKNOWLEDGEMENTS 10  R EFERENCES 11  B 
IOGRAPHY 11  1  I NTRODUCTION  Future exploration of the M oon will utilize robots for site survey and reconnaissance as well as a variety of lunar surface utility work [6]. Effec tive use of robots for these applications requires new type s of remote operations. For Lunar operations, Earth-based operators will remotely supervise multiple robots perfo rming tasks, independently and jointly. Ground control teams \(including scientists and engineers\will monitor the results of these tasks to adjust robot plans. To ensure that robots are used effectively for 1 
 1 978-1-4244-2622-5/09 25.00 \2512009 IEEE 2 IEEEAC paper #1684 Version 1, Updated 2008:11:01 exploration missions, it is important to assess the performance of these operational models We are investigating the definition and computation of performance metrics for assessing remote robotic operations in real-time Our approach is to monitor data streams from robots, compute performance metrics, and provide Webbased displays of these metrics for assessing robot performance during operations. We have identified task performance, reliability, and efficiency metrics for remote robotic operations. We have developed software that performs inline computation of these metrics by monitoring 
robotic data streams. Metrics ar e distributed in real-time via a Web server. The current valu e of metrics can be viewed on dashboard displays. Plots of historical values of metrics overlaid with significant operational events can be viewed on timelines We are assessing the usefulness of the performance monitoring software for mission operations.  We are working with the Exploration Technology Development Program \(ETDP\an-Robot System \(HRS\ions assessment team to evaluate the use of performance metrics during remote science operations.  We believe our metrics for robot task performance can be useful to operations personnel in the following ways 1  Mission Manager \(MM 226 the person responsible for 
the success of the mission.  We expect metrics summarizing the completion of science objectives to be useful in assessing progress on the mission.  We also expect metrics summarizing rover health and productivity to be useful in assessing the effectiveness of rover operations 2  Rover Operator \(RO 226 the person who remotely supervises autonomous r over operations and who teleoperates the rover when needed.  The Rover Operator will have good in sight into ongoing rover operations by virtue of direct participation in those operations.  We expect, however, the accumulated metrics \(such as Drive Time and Run Time\o provide useful summaries of progress on tasks and rover health 


  2 over longer time periods \(such as a duty cycle or a mission\We also expect that over time the Rover Operator will gain insight into \223typical\224 rover performance and be able to use metrics as an indicator of degrading or abnormal performance 3  Science Analyst \(SA 226 the personnel who define the science plan and analyze the data collected as a result of executing that plan.  We expect metrics summarizing progress on data collection by the rover to be useful in helping the science team maintain situation awareness remotely.  High-level indicators of rover health also can be informative when science progress is not as expected We expect that metrics summarizing the quality of remote communication will be useful to all users, since they indicate the reliability of performance metrics We evaluated our approach for measuring robot performance during a field test at Moses Lake Sand Dunes WA\n W e  m oni t o red sci e nce operat i ons performed at Moses Lake Sand Dunes by two NASA Ames K10 robots from a simulated Mission Control located at JSC and computed task performance metrics for both K10 223Black\224 and K10 \223Red\224 when performing site survey and reconnaissance tasks. These metrics were available on operational displays for use during the simulated mission After the test, we used the complete set of data recorded from the robots to compute metrics for robot efficiency and reliability throughout the simulated mission In this paper we present the results of evaluating our software for robot performance during simulated science operations at Moses Lake Sand Dunes, and discuss our conclusions from this evaluation for future robot operations 2  R ELATED W ORK  The real-time assessment of pe rformance metrics is central to our approach. This requires incremental computation of metrics while operations are ongoing. Other work on the incremental computation of metrics includes the estimation of incremental task progress as a function of time to compute the expected time interval that a robot can function without human attention, called neglect time    Real-time assessment of performance also requires strategies to monitor operational context to determine when to compute performance \(e.g., infer activity intent Computation and interpretation of such metrics requires considering environmental operational, and technology limitations to establish a realistic baseline of comparison An important conclusion of our research is the identification and categorization of constraints affecting performance baselines \(described in Secti on 6\We have seen examples of such constraints in other research, such as constraints on measuring traverse performance to a single regional terrain type \(an example of an environmental constraint by Tunstel W e know of no work, however, t h at  characterizes these different types of constraints The metrics computed during the test at Moses Lake Sand Dunes were informed by prior work on human-robot interaction metrics [1, 5, 11, 15 W e u se trav ersal m etrics such as drive time and distan ce traveled since both site survey and reconnaissance tasks require the robot to traverse a series of waypoints. Our metrics on sensed data differ from the perception metrics discussed by Fong  however. The instrument metrics \(Lidar and GPR computed for K10 characterize the collection of data for science return without interpretation of this data. The perception metrics discussed by  are i n t e nded t o  characterize the robot\222s understanding of the environment and thus describe a more active interpretation of sensed data by the robot than the instrument metrics. The real-time computation of performance metrics can be viewed as a form of robot self-awareness, specifically addressing the robot\222s \223capacity for self-mon itoring \(health, state, task progress\e have plans to evaluate the computation of real-time metrics onboard the robot in future tests. We have implemented but not yet evaluated measures of human intervention, specifically \223M ean Time To Intervene\224 and the \223Mean Time Between Interv enti W e al so have a metric for robot productivity derived from metrics proposed for astronaut productivity, the \223Work Efficiency Index\224 \(WEI W E I i s t h e rat i o  of robot  product i v e t i m e i.e., time spent on successful task\e Much of the research on me trics for robotic performance has been for the purpose of assessing and comparing robot technologies [1, 4 Tunst e l 14 defi nes t echnol ogy  metrics for the Mars Exploration Rovers. His objective is to define metrics characterizing the effects of technology on science return as a baseline for evaluating future rover technologies and establishing baselines for predicting technology impacts on science return. We define multiple different perspectives on metrics that address not only science return \(in terms of successful performance of science plans\ also rover productivity and health Dillman [4 d escrib es b en ch m ark s fo r th e co m p ariso n o f  robotic technologies. Such benchmarks provide a common basis for understanding technology differences and improvements, and require the execution of a set of common tasks with well-understood environmental state transitions. Similarly benchmarking for diagnostic and prognostic technologies utilizes standardized specifications for fault cataloging and test scenario  As a resul t  the baseline for comparison in benchmarking is well understood a priori \(i.e., ground truth is known\We are investigating the use of performance metrics for real-time assessment of human-robot operations. These differ from benchmark evaluations \(1\ being performed on a broader range of tasks than benchmark tasks, \(2\ having less 


  3 certain knowledge of the state of the environment and consequent ground truth, and \(3\ computing metrics in real-time. Work on benchmarking can however inform what metrics we compute and how we interpret these computations. Technology benchmarks correspond to engineering constraints on performance baselines. Research benchmarks can be relevant to such engineering assessment but are less relevant to operational and safety assessments Metrics for diagnostic and prognostic technologies characterize the ability of the technology to detect or predict problem They i n cl ude m easures such as detecting a non-existent problem \(false positive\to detect a problem \(false negative\etrics that consider signal  easures of algorithm  accuracy 12 These metrics could be useful in assessing the robot\222s selfawareness, such as its ability to detect and react to component problems. They also address the assessment of instrumentation accuracy and relia bility that could be useful in determining whether to use data measurements when computing our performance metrics. Finally these metrics could be used to assess the ability of our performance monitoring software to correctly detect events triggering the computation of metrics Performance monitoring to improve system performance has been utilized in a number of applications other than robotics, including nucl process cont rol 17  and traffic fl Nucl ear power pl ant s m oni t o r dat a  t o  detect faulty or degraded instruments \(i.e., instrument performance\and to improve plant performance \(e.g increase thermal efficiency Such degraded instrument performance can impact plan t performance by triggering incorrect operational Process cont rol  engineers need metrics that re late controller performance to business objectives [17].  De sborough and Miller categorize performance metrics into two types - business metrics and operational metrics.  They identify a requirement for metrics that aid operations in determining when system-wide performance has changed \(termed orientation obotic operations have similar needs for metrics that relate robot task performance \(operational metrics\ mission objectives business metrics\e work described in this paper is a first step toward defining and deploying such metrics for robot operations Performance monitoring for space robotic systems does pose challenges not encountered in nuclear power or chemical processing plants.  Mobility results in a high degree of interactivity with novel environments.  And there is limited ability to alter or constrain the environment to mitigate adverse effects.  There are also special challenges in space such as reduced gravity and communication bandwidth and latency.  These challenges reinforce the need for techniques such as performance monitoring to aid operations  3  P ERFORMANCE M ONITORING S OFTWARE  The purpose of our performance monitoring software is to provide a configurable, reusable system for monitoring robot telemetry, computing metrics about the robot\222s performance, and presenting these metrics to a user. We support multiple ways of pres enting information including 1\displays of current values of performance metrics, \(2 reports that summarize performance over a period of time and \(3\ification about significant performance changes or events. For the recent robotic field test at Moses Lake Sand Dunes, WA, our objective was to evaluate the feasibility of computing and displaying robot performance in real-time. To perform th is evaluation, we computed performance metrics for rovers performing remote reconnaissance and site survey activities The performance monitoring soft ware evaluated at Moses Lake consists of a computation engine that monitors rover data and computes performance values, data servers for performance values, and Web-based displays. The computation engine passes incoming rover data to the performance algorithms associated with that data. It also can pass the output of one algorithm as input to another algorithm. The computed performance metrics are provided to users via two Apache Tomcat data servers: \(1 real-time server that serves the current values of performance computations to dashboard displays developed in the Google Web Toolkit and \(2 report server that serves archived values of computa tions to a timeline display developed using MIT\222s Simile TimePlot  The dashboard displays provide th e user with the most recent values of performance metrics \(Figure 1\These displays update automatically when a new value is computed. Values are organized into tables of related metrics, such as \223Robot Task Performance\224 or \223Lidar Instrument Performance\224  Figure 1 Dashboard Display from Moses Lake Field Test The timeline display provides the user with a time-based plot of the history of performance values \(Figure 2 


10 sec 2  where t i time of current pose message t i-1 time of previous pose message   3  1 x if d   4 displays update to reflect new values at the user\222s request The user can request a current report at any time and can save the report for reviewing late r. The user also can change which data should be plotted and which events should be shown in a report. The available choices can be changed using a configuration file  Figure 2. Timeline Display from Moses Lake Field Test We have developed a library of Java objects that encode robot performance algorithms. XML configuration files specify which algorithms begin executing at system startup These files identify which algorithms should be actively monitoring data for a particular application and what robot data and computa tion output are associated with each algorithm. These files also define the value of constants used in the algorithm \(such as thresholds\putation is performed whenever an upda ted input value is received either robot data or updated output from another algorithm and trigger conditions are met. For example, to compute the total distance traveled by a robot, robot pose messages are passed to an algorithm for computing the distance between two poses \(TravelBetweenPoints puted distance exceeds a threshold intended to exclude noisy data \(.025 meters\he distance value is passed to a second algorithm that keeps a running sum of these distances DistanceSummation\ting sum can be viewed on a dashboard display. A portion of the configuration file for this example is shown below 212 212 1   212  t i RobotDistanceBetweenPoints TravelBetweenPoints Pose_Estimate 0.025 RobotDistanceTraveled DistanceSummation RobotDistanceBetweenPoints t i  x 212  212 t i i 1 t i  212  212  y i 2 2 2 2  Entry Name 1 0.025        y y  212 0.025 x   1 1 212 212   y i t y    x t 1 y CompletedDist 1 t i i x i 1 1    212  1  where t i time of current pose message t i-1 time of previous pose message if separation > 0.025 x, y\ = pose vector Name ObjectClass ObjectClass Config SubscribeMessage  SubscribeMessage ChangeThreshold ChangeThreshold Config Entry Entry Name Name ObjectClass ObjectClass Config SubscribeMessage   DriveTime m d i i if d m  where x i y i current robot pose estimate RunTime if t i    212  x i y i 212  SubscribeMessage Config Entry  Figure 3 illustrates the architecture of the performance monitoring software   Figure 3. Architecture for Performance Monitoring We computed performance metrics for the robot instruments mounted on the robot, and communication quality during the Moses Lake Sand Dunes field test. These metrics are described below Robot Task Performance measures rover performance on tasks by computing the time spent driving and the distance traveled during that time Observed performance is compared to estimated performance derived from the rover\222s plan to indicate whether the robot is performing as expected. See Table 1 for a description of the operational use of these metrics x i x i y i  1 212  


1   1  14  where s = Lidar subsystem status k = name of task in Sur vey Manager Status message n = flag indicating a new plan has been uplinked  Table 2. Operational Use of Lidar Performance MM \226 Mission Manager; RO \226 Rover Operator; SA \226 Science Analyst  Metric Interpretation Operational Use Panorama in Progress Should be true while taking a panorama RO indicates whether Lidar is functioning normally Lidar Run Time Should take ~24 minutes for 12 scan panorama RO indicates if Lidar performance is typical Observed Scan Count Should increase each time a scan completes RO indicates whether Lidar is functioning normally Estimated Scan Count Basis of comparison for Observed Scan Count RO helps interpret Observed Scan Count  Percentage Scan Complete Should be near 100 for nominal execution of plan RO indicates how well rover is executing plan SA indicates progress on data collection Observed Panorama Count Should increase when panorama completes RO indicates whether Lidar is functioning normally  Estimated Panorama Count Basis of comparison for Observed Panorama Count RO helps interpret Observed Panorama Count  Percentage Scan Complete Should be near 100 for nominal execution of plan RO indicates how well rover is executing plan SA indicates progress on data collection 12 13  where k = name of task in Sur vey Manager Status message n = flag indicating a new plan has been uplinked   CompletedDis P ercentPanoramaComplet e p k C p E P C k           212  1  1  8  where  t start time PIP\(p\ changes to true t end time PIP\(p\ changes to false         s s s true  where k = name of task in Sur vey Manager Status message n = flag indicating a new plan has been uplinked  Lidar n CompletedDist 212  212 212  212     E stimatedScanCount k E SC k E P C k   11  where p = LidarPanorama subsystem status k = name of task in Sur vey Manager Status message n = flag indicating a new plan has been uplinked  ObservedPanoramaCount p OP C p OPC active p i idle  where p = LidarPanorama subsystem status  ObservedScanCount OS C OPC active s i active  where s i Lidar subsystem status at time i  imary         7  where p = LidarPanorama subsystem status   s k Pr P ercentScanComplete p C E S C OPC OP A vgDistOverTime t E lapsedTime    5 x i-1 y i-1 previous pose estimate   1  1      10   EPC y     003     002 003  n n k n 4  where wx i wy i waypoint pose at position i      end star t   12            y i PercentageDistComplete PlannedDist Pr wx k k s   9  x i  212 ogress    E stimatedPanoramaCoun t E P C k k OS PIP active false if p active x i y i y i   x wy  n n   212  212 212  002     212 212   1 1 1   p  true if p  5   wy i PlannedDist   2 2 2 2 p 1 1      p   wx i L idarRunTime t t t  6   Table 1. Operational Use of Robot Task Performance MM \226 Mission Manager; RO \226 Rover Operator; SA \226 Science Analyst  Metric Interpretation Operational Use Drive Time  Should be a significant percentage of Run Time for science ops. For site survey, Drive Time may approach Run Time RO indicates time rover is on-task Run Time Should be significant percentage of the time in duty period RO indicates if rover is experiencing significant down time Completed Distance Should generally increase for science ops MM indicates task progress RO indicates task progress Planned Distance Basis of comparison for Completed Distance RO helps interpret Completed Distance  Percentage Distance Complete Should be near 100 for nominal execution of plan MM indicates progress on science objectives  RO indicates how well rover is executing plan SA indicates progress on data collection Avg Dist Time Should be comparable to ops limits on velocity RO indicates rover mobility performance  Instrument Performance measures performance of the Lidar instrument during reconna issance activities. Lidar is used for 3D terrain mapping. During reconnaissance, the rover acquires multiple scans to construct a panorama at specified locations. See Table 2 for a description of the operational use of these metrics  if k EPC p if p i if s i 003  wx i d i i PanoramaIn OPC n  n n n n n 


D ataGapCoun t D G C     6  Communication Quality measures the quality of remote data communication by detecting when data communication drops out \(called data gaps  10 sec  false   16    tp if DG   fiftp i 10 sec DataGap tp tp 1  tp tp tp i  15 where tp i time of pose message i  DGC DG true where tp i time of pose message i  Table 3. Operational Use of Communication Quality MM \226 Mission Manager; RO \226 Rover Operator; SA \226 Science Analyst  Metric Interpretation Operational Use Data Gap  Should be true when receiving no data from robot All indicates if metrics are being updated with robot data  Data Gap Count Should be relatively low number indicating minimal loss of data All indicates if quality of metrics might be impacted by loss of data 4  E VALUATION OF R OBOT P ERFORMANCE  The performance monitoring software computed performance metrics for the K10 planetary rovers during the Human-Robotic Systems \(HRS field test at Moses Lake Sand Dunes, Washington  The K10 robot was devel oped by the Intelligent Robotics Group at NASA Ames Research Center. K10 is four-wheel drive and all-wheel steering rover with a passive rocker suspensi on. The standard sensor suite for a K10 rover includes a Novatel differential GPS system a Honeywell digital compass, Firewire stereo cameras, a suntracker, and wheel encoders. K10 data interfaces are implemented in CORBA. For the field test at Moses Lake additional instruments were mounted on both K10 Red and K10 Black to support science operations. K10 Red was equipped for robotic reconnaissance \(i.e., using a planetary rover to scout traverses, or sites, prior to EVA activity\with three additional instruments: \(1\dar \(Optech ILRIS3D\mapping, \(2 a consumer-grade digital camera mounted on a pan/tilt unit for high-resolution, color panoramas, and \(3\icroscopic imager for very highresolution images of surface materials. K10 Black was equipped with a GSSI SIR-3000 Ground Penetrating Radar GPR\apping and a microscopic imager for high-resolution surface images to support systematic site surveys Planetary science operations were simulated between June 9 226 12, 2008, from an analog ground control at the Johnson Space Center. During that time period, the performance monitoring software was connected to a real-time data stream from the K10 robots while operating at Moses Lake Rover performance was computed throughout the day resulting in a daily summary of performance. K10 Red performance was computed for total of 28 hrs and K10 Black performance was computed for a total of 9.5 hrs K10 Red Performance K10 Red performed reconnaissance activities in support of simulated science operations daily from June 9 \226 11. A typical reconnaissance performed by K10 Red consisted of navigating to a sequence of waypoints, taking panoramic and microscopic images at each waypoint, and additionally taking Lidar scans at a subset of these waypoints. The drive time for reconnaissance is expected to be less than the run time, since the rover stops while taking instrument readings For Lidar, taking a full panorama takes around 24 minutes which can be a significant percentage of the operating time During the 3 days of reconnaissance operations at Moses Lake Sand Dunes, the K10 Red rover was powered up for Drive Time 44.8% of the time it was powered up \(Figure 4\rover attempted to execute eight scie nce plans over the three days and completed four of these plans without human intervention. The total expected distance for these plans was 2235 meters \(Planned Distance\ actually traversed a total of 2375 meters \(Completed Distance\to perform these reconnaissance activities \(Figure 5 Performance metrics also were computed for the Lidar instrument mounted on K10 Red. A typical Lidar activity was taking a panorama. A panorama consists of 12 Lidar scans covering 360 degrees, taken by rotating the robot in 30-degree increments and scanning at each increment. K10 Red took four complete Lidar panoramas during reconnaissance operations, plus two additional scans for a total of 50 Lidar scans. The total time spent taking Lidar during reconnaissance activities was 1.64 hours  Figure 4. K10 Red Operating Time at Moses Lake DGC 1 1   003         212  212    1 DG tp i tp i 212 212 tiftp i tp tp i 212   


  7  Figure 4 compares drive time blue line\ run time \(red line\for K10 Red.  On June 9, drive time and run time are nearly equal, indicating that the robot was driving during most of the duty period as expected.  On June 10 and 11 however, the drive time is significantly less than the run time, indicating that the robot was stationary roughly half the time it was running.  This mismatch can indicate performance problems, either due to increased robot down time or robot wait time between plans.  An inspection of events during these days indicates the robot did experience problems on both days.  On June 10, communication difficulties and low lighting resulted in reduced rover performance.  The wait time between plans was between 30 and 50 minutes, which is much longer than on June 9.  On June 11 the rover base controller had problems and the site terrain impacted traversability, both of which impacted the rover's ability to complete plans in a timely manner Figure 5 compares distance trav eled by the K10 Red rover to the planned distance.  An inspection of this figure indicates a limitation in using accumulated total distance as a metric.  By the end of the test, K10 Red had traveled 2375 meters, which agrees well with the planned distance of 2235 meters.  A comparison of planned distance to distance traveled for each duty cycle, how ever, reveals that the rover traveled further than planned on June 9, which masked the fact that the rover traveled less than planned on June 10  Figure 5. K10 Red Distance at Moses Lake There were significant problems with the quality of data communication to JSC for remote science operations on June 10 and 11, indicated by the high data gap count on these days. On June 10, there were 54 gaps ranging from 10 seconds to 53.5 minutes and on June 11 there were 65 gaps ranging form 10 seconds to 54 minutes. The reduced data quality affected the accuracy of the performance metrics computed remotely for K10 Red on these days K10 Black Performance K10 Black performed systematic site surveys in support of simulated science operations on June 12. A typical site survey performed by K10 Black consisted of navigating through a sequence of waypoints while taking GPR readings continuously. Often microscopic images were taken at waypoints as well. During the day of survey operations at Moses Lake Sand Dunes, the K10 Black robot was powered up for 5.67 hrs \(Run Time\and drove for .64 hrs \(Drive Time\or 11.3% of the time it was powered up Figure 6\Drive Time was lower than expected due a combination of GPR hardware problems and navigation problems due to inaccurate odometry in the sand at the survey site. K10 Black attempted to execute four science plans for a total planned distance of 776 meters \(Planned Distance\ actually traveled a total of 838 meters Completed Distance\ perform these survey activities Figure 7  Figure 6. K10 Black Operating Time at Moses Lake  Figure 7. K10 Black Distance at Moses Lake 


  8 The quality of data communication to JSC for remote science operations on June 12 was much better than that on the two days prior. There were a total of 5 gaps, ranging from 10 seconds to 30.75 minutes. These data gaps did not have a significant effect on th e accuracy of the performance metrics computed remotely for K10 Black 5  D ISCUSSION  The Role of Context Understanding the intent of rover activities provides an important basis for grounding the computation and interpretation of performance metrics. In particular, we assert that metrics, whether us ed for real-time monitoring or post-processed analysis, can only be interpreted in context  i.e., with respect to expect ations of performance for a particular task, activity plan, robot mode of operation, etc For example, the computation of Percentage Distance Complete \(ratio of Completed Distance to Planned Distance\requires constraining the computation of Completed Distance to be performed only while K10\222s plan is executing and to be reset to zero when a new plan is uplinked. Similarly the computation of metrics during autonomous operations should be distinguishable from metrics during teleoperation because expected performance can be quite different in these different control modes Finally, the computation of metrics during anomalous operations should be separa ble from nominal operations The rover may be successful at recovering from a problem while doing less well at achieving nominal mission objectives. In some cases, a me tric needs to be computed concurrently in more than one context For example, it is useful to accumulate Comple ted Distance for each plan distance traveled within a pl an\entire mission \(total distance traveled Another example is that the accuracy of real-time performance computations de pends upon the quality of the communication with the robot. We used the Data Gap Count as an indicator of communication quality during the field test at Moses Lake. While useful, this count does not represent information about the duration of communication problems. Based on our experience at Moses Lake Sand Dunes, we have identified a new metric for communication quality. We propose to compute the percentage of the duty period without data from the robot \(called Percentage Time in Data Gap\ as a measure of the quality of metrics, since a high percentage would indicate lossy communication and reduced confidence in the com puted values of performance metrics Real-time Displays During the test we identified and implemented a number of new features for the dashboard displays in response to operational needs. These feature include \(1\a user-initiated snapshot of all computed values we later expect to add the ability to restore the state of the computation engine to this checkpoint state, \(2\ogging of time-stamped user comments \(called user events pect to display user events in the timeline disp lay, and \(3\o the value of constants in the dashboard displays without restarting the computations. We also added new dashboard displays of rover data during the test. These displays provide information about the rover that aids in understanding performance \(i.e., execution status of planned tasks, robot subsystem status\add these new displays indicates the flexibility and extensibility of our architecture for performance monitoring Duty Periods During the HRS field test at Moses Lake Sand Dunes, we computed metrics whenever data were being transmitted from the rover. We did this to ensure that metrics reflected all rover activities. We expected that the value of metrics at the end of each day would then provide a summary of the day\222s activities. This resulted, however, in the computation of metrics during time periods when the rover was not intended to be operating \(e.g lunch periods, waiting for a demo, etc\e-based metrics such as Run Time and Drive Time were computed at times when the rover was not intended to be powered up or moving. This resulted in inflated Run Times \(see Figures 4 and 7\and artificially reduced the ratio of Drive Time to Run Time We believe a more realistic assessment of performance should constrain the computation of metrics to time periods when the rover is intended to be performing tasks. We define a concept called a duty period that corresponds to a contiguous interval of operations \(such as from rover startup in the morning until a planned ground control break period\We propose to compute accumulated metrics only during duty periods in future tests Technology Deployment at NASA The evaluation of our performance monitoring software during robotic field tests has provided useful feedback on the utility of the proposed metric s.  In particular we have gained valuable experience in using such metrics when data are subject to periodic dropouts We recently evaluated the use of our performance metrics during an analog test of rover reconnaissance as a precursor to planning astronaut EVAs where we got feedback a bout the utility of metrics for flight operations.  There is remaining work, however, before this technology can be operationally deployed A variety of data types are used by the real-time performance monitoring of robots.  These data types have correlates in current NASA operations, including spacecraft telemetry, Comps \(combinations of raw telemetry\anning information, and flight rules.  Deployment of performance 


  9 monitoring technology in flight operations will require that such information be captured electronically and made available for computer-based reasoning.  Recent efforts to move flight products like Sp ace Station activity plans and attitude timelines into XML supports this objective and future programs are investigating standards for representing such information, but current NASA programs have not yet achieved the level of information integration that will be needed to automate performan ce monitoring as described in this paper The availability of real-time performance data raises the possibility of using this data to adjust operations for improved performance.  Such real-time performance management has become a consideration for commercial plant operations as plant automation is introduced.  Often the barriers to real-time performance management are as much cultural as technical In a recent article \(Spiegel 2007\t was observed the adjustment of plant operations by the enterprise has met strong resistance due to mutual mistrust between plant operators and corporate Information Technology departments.  Such barriers are also a possibility when introduci ng real-time performance management into NASA operations 6  F UTURE W ORK  Assessing Performance Based on Expectations An important result from this test is an approach for improving the interpretation of performance metrics. Prior to the test, we established performance expectations from robot task plans \(called plan performance\Satisfactory performance was defined as the achievement of planned targets \(e.g., successful completion of planned tasks or collection of planned samples\ion of robot performance during the test made it clear there are other considerations in defining satisfactory performance The limits imposed by robotic hardware and software design define expectations about the robot\222s engineered performance. Engineered performance defines robot capabilities under ideal circum stances. Robot operations however, often occur under less than ideal circumstances which can constrain maximum speed and sensor accuracy The limits imposed by the environment in which the robot operates define expectati ons about robot operational performance. Engineered perform ance also can be degraded over the course of multiple missions due to normal component wear as well as systemic problems that reduce performance The limits imposed by component use and subsystem problems define the expectations about robot degraded mode performance. Robot operations can be further constrained by flight rules. Maximum robot speeds may be reduced when operating near other robots. Or, certain robot behaviors may be precluded when operating near humans The limits imposed by the flight rules established for a mission define expectations about safe performance Moreover, when comparing robot metrics over multiple duties periods or missions \(called historical performance\or comparing the performance of different robots, it is important to establish a basis of comparison that identifies which of these performance dimensions predominate and how these dimensions combine to establish performance expectations, such as: Did robotic hardware or software change between missions? And were the robots being compared operating under the similar operational constraints Summarizing Performance We are currently preparing to use our real-time performance monitoring software to support a test of science operations at NASA Ames Research Center in November 2008. Based on our experience at Moses Lake Sand Dunes, we have identified a few key metrics that should provide a high-level summary of rover performance. These metrics will be displayed on a summary Web page for operational use. We will summarize rover performance from the following three perspectives 200  Mission: metrics that describe the rover\222s contribution to the mission. The Work Efficiency shows the ratio of rover productive time to rover overhead time. When WEI exceeds 1.0, the rover is spending more time accomplishing mission objectives than performing other activities. The Percentage of Time on Task shows the ratio of rover productive time to total operating time 200  Science: metrics that describe the rover\222s performance with respect to the scie nce plan. The Percentage Distance Complete summarizes the percentage of the planned distance that has been traveled by the rover The Percentage of Tasks Complete describes the percentage of planned tasks successfully completed 200  Rover: metrics that describe the rover\222s health and status. We detect when personnel intervene in rover operations to handle anomalies, and we compute the Mean Time to Intervene a nd the Mean Time between Interventions We also plan to compute the Percentage Time in Data Gap as a measure of the quality of computed values, since a high percentage indicates lossy communication and a lower confidence in metrics To illustrate how we expect this information to summarize performance for mission managers, consider the following example based on data collected by K10 Red at Moses Lake on the afternoon of June 11, 2008. In this example, we 


  10 played back recorded data into our performance monitoring software and took a screen shot of the Ops Synopsis dashboard display partway through the run \(Figure 8 this situation, the rover is executing a science plan to perform reconnaissance of the Moses Lake Sand Dunes. At the time of the screen shot, the rover has spent 59% of its time on planned tasks for a WEI of 1.446. This indicates the mission is going reasonably well, since WEI is greater than 1.0 and the rover is spending the majority of its time on planned tasks. The science metrics indicate good progress is being made on data collection, requiring the rover to drive 326 meters to complete 70% of the plan. The rover metrics indicate minimal unintended human intervention, with MTBI \(00:39:10.7\gnificantly greater than MTTI 00:02:01.2\The Percentage Time in Data Gap is zero indicating that metrics are based on all available robot data and are thus considered reliable  Figure 8. Example of Summary Metrics for Mission Manager  7  C ONCLUSIONS  Our performance computation so ftware operated reliably for over 46 hours during a four-day period. Task performance metrics computed for the K10 robots during the field test include robot drive time, robot run time, and performance during plans \(e.g., percentage of planned distance completed, percentage of planned samples complete\We measured the number of times we lost signal from the robot at the simulated Mission Control as a measure of the quality of our real-time metrics. Based on these results, we conclude that our proposed approach for computing and displaying rover performance metrics in real-time is viable 8  A CKNOWLEDGEMENTS  This work was funded under NASA\222s Small Business Innovative Research \(SBIR\ program, Topic X7.02 HumanSystem Interaction and supported by the NASA Exploration Systems Technology Development Program \(ETDP Human-Robotic Systems \(HRS\. We wish to thank Dr. Matthew Deans and Dr. David Lees of the Intelligent Robotics Group at NASA Ames Research Center for consulting on the definition of the metrics used for the test at Moses Lake Sand Dunes 


  11 R EFERENCES    and Fran cesco Amigoni, \223On Evaluating Performance of Exploration Strategies for Autonomous Mobile Robots,\224 Workshop on Performance Evaluation and Benchmarking for Inte lligent Robots and Systems Proceedings, IEEE Internati onal Conference on Intelligent Robots and Systems, September 26, 2008  Jacob C r andal l M i chael Goodri c h, Dan Ol sen, Jr., and Curtis W. Nielsen, \223Validating Human\226Robot Interaction Schemes in Multitasking Environments\224, IEEE Transactions on Systems, Man, and Cybernetics\227Part A Systems and Humans, Vol. 35, No. 4, July 2005  H. Depol d, J. Si egel  and J Hull, \223Metrics for Evaluating the Accuracy of Diagnostic Fault Detection Systems\224 IGTI Turbo Expo, June 2004  ann, "KA 1 10 Benchmarks for Robotics Research," European Robotics Network \(EURON\IST2000-26048, April 24 2004  A. St ei nfel d, T. Fong, D. Kaber, M Lewi s, J. Schol t z  A Schulz, and M. Goodrich, \223Common Metrics for HumanRobot Interaction,\224 2006 ACM Conference on HumanRobot Interaction. March 2004   T Fong, M B u al at M Deans, M Al l a n  X Bouyssounouse, M. Broxton, L. Edwards, R. Elphic, L Fluckiger, J. Frank, L. Keely, L. Kobayashi, P. Lee, S. Y Lee, D. Lees, E. Pacis, E. Park, L. Pedersen, D Schreckenghost, T. Smith, V. To, and H. Utz, \223Field Testing of Utility Robots for Lunar Surface Operations, \223 AIAA Space 2008, AIAA-2008-7886. September 2008  M i ke Gernhardt, W o rk Effi ciency Indices. Presentation at Johnson Space Center. November 15, 2005  e b Toolkit http://code.google.com  webtoolkit   Tol g a Kurt ogl u Ol e J. M e ngshoel and Scot t Pol l A Framework for Systematic Benchmarking of Monitoring and Diagnostic Systems,\224 2008 International Conference on Prognostics and Health Management Proceedings October 6-9, 2008  ile http://code google.com/p/simile-widgets  D. Ol sen and M  Goodri c h 223M et ri cs for Eval uat i ng Human-Robot Interactions,\224 Proceedings of NIST Performance Metrics for Intelligent Systems, Sep 2003  SAE \(Soci e t y of Aut o m o t i v e Engi neers t h  and Usage Monitoring Metrics, Monitoring the Monitor\224 SAE ARP 5783, February 2008  Abhi nav Saxena, Jose C e l a y a  Edward B a l a ban Kai  Goebel, Bhaskar Saha, Sankalita Saha, and Mark Schwabacher, \223Metrics for Evaluating Performance of Prognostic Techniques,\224 2008 International Conference on Prognostics and Health Management Proceedings October 6-9, 2008  Edward Tunstel, \223Perform ance M e trics for Operational Mars Exploration Rover,\224 Journal of Field Robotics Volume 24 Issue 8-9, 651 \226 670, September 2007   H. A. Yanco, J. L., Drury and J. Schol t z B ey ond Usability Evaluation: Anal ysis of Human-Robot Interaction at a Major Robotics Competition," Journal of Human-Computer Interaction, Volume 19, 117\227149 2004   J. Arnol d, \223Towards a Fram ework for Archi t ect i ng Heterogeneous Teams of Humans and Robots for Space Exploration\224, M.S. Thesis, Dept. of Aeronautics and Astronautics, Massachusetts Institute of Technology 2006 17] Lane Desborough and Ra ndy Miller, \223Increasing Customer Value of Industrial Control Performance Monitoring -- Honeywell's Experience,\224 Sixth International Conference on Ch emical Process Control Jan 2001  J.W Hi nes and E. Davi s, "Lessons Learned from  t h e U.S. Nuclear Power Plant On line Monitoring Programs Progress in Nuclear Energy, Volume 46, Issues 3-4, 2005 pp 176-189   Henry  X Li u, W e nt eng M a Xi nkai W u and Heng Hu 223Development of a Real-Time Arterial Performance Monitoring System Using Traffic Data Available from Existing Signal Systems,\224 Minnesota Department of Transportation Technical Report MN/RC 2009-01 December 2008   R ob Spi e gel  223C onnect i ng Sensor Dat a t o t h e Boardroom,\224 Automation World, March 2007 B IOGRAPHY   Debra Schreckenghost is a Senior Scientist with TRACLabs in Houston, TX. She is Principal Investigator of the NASA Phase II SBIR developing a soft ware framework for inline monitoring of robot health and performance. Previously she has led projects developing adjustable autonomy software for monitoring and control of crew life support systems and personal ag ents for distributed space 


  12 operations. She has published over 70 technical publications in the areas of adjustable autonomy, architectures for intelligent software agents, and human-computer interaction. She received a BS in electrical engineering from the University of Houston and an MS in electrical engineering from Rice University Terrence Fong is the director of the Intelligent Robotics Group at the NASA Ames Research Center. Prior to this, he was the deputy leader of the Virtual Reality and Active Interfaces Group at the Swiss Federal Institute of Technology Lausanne. Fong has published mo re than fifty papers in field robotics, human-robot inter action, virtual reality user interfaces, and parallel processi ng. He received his BS and MS in Aeronautics and Astronautics from the Massachusetts Institute of Technology and his Ph.D. in Robotics from Carnegie Mellon University  Tod Milam is a Research Scientist with TRACLabs, Houston, TX.  Previously he was a Senior Software Engineer at Lockheed Martin Space Mission Systems Services, working on traffic management systems, as we ll as software for Mission Control Center at Johnson Sp ace Center. He began working with TRACLabs in 2000 and was a key developer of TRACLabs' Distributed Collaboration and Interaction System, providing intelligent agents for human-automation interaction in mission operations.  He is currently the lead developer for a NASA Phase II SBIR on developing software for in-line computation of robot measures of performance. Mr. Milam received his Bachelors degree in Computer Science from Drake University in 1988 Estrellina Pacis is a Project Manager at SPAWAR Systems Center, Pacific where she has been developing unmanned systems for the past 6 years. Mrs. Pacis has led numerous projects focused on the development integration, and testing of autonomous behaviors for military applications. Her current focus is on the study of robotic mapping and exploration techniques suitable for supporting dismount troops during urban operations Hans Utz is a Mobile Robot Software Architect for the Research Institute for Advanced Computer Science \(RIACS working with the Inte lligent Robotics Group at NASA Ames Research Center \(ARC work is concerned with the development of advanced software architectures for autonomous mobile robotics Before joining NASA ARC, he was a PhD student at the Dept of Neuroinformatics, University of Ulm, Germany. He designed the robotics middlew are Miro and coached the Ulm Sparrows, a project on "autonomous mobile robotics in highly dynamic environments", which is probably better known as robot soccer.  He has a PhD in Computer Science from the University of Ulm, Germany 


  13  


  14 B IOGRAPHY  Capt. Chris Badgett is a System Engineer for the STP-SIV at the Space Development and Test Wing Kirtland AFB. He previously served as developmental engineer at Air Force Research Laboratory Munitions Directorate working on a weapon data link and F-16 small munitions dispenser. He has a B.S.E.E. from University of Tennessee and an M.S. in Space Systems from the Air Force Institute of Technology  Nicholas Merski is currently the Program Manager for the STP-SIV program at the Space Development and Test Wing, Kirtland AFB. His previous experience focused on small satellite command and control systems in project management, systems engineering and operations roles. He has a B.S in Industrial Engineering from University of Pittsburgh and is completing an M.S. in Space Systems from the Air Force Institute of Technology Mike Hurley is the Spacecraft Development Section Head at NRL in Washington DC. Mr. Hurley has helped shape and implement ORS and TacSat experimentation since 2002. Beginning April 2003, he was Program Manager for the Secretary of Defense\222s \(OSD\ ORS initiative, as well as the Principle Investigator for Office of Naval Research\222s \(ONR\ TacSat Innovative Naval Prototype \(INP program. In 2005 he received the DOD Civilian Meritorious Medal for his work in ORS and TacSat experimentation. He is an engineering graduate of Virginia Tech and holds an M.B.A. from George Mason University  Paul Jaffe is an electrical engineer and the Integration and Testing Section Head at the Naval Center for Space Technology at NRL. He has worked on NASA and DOD space missions, including Solar TErrestrial RElations Observatory STEREO\ and TacSat-1. Recently he developed standards and led spacecraft computer hardware development for TacSat-4 as part of the DOD ORS effort He holds a B.S. in Electrical Engineering from the University of Maryland, College Park and an M.S. in Electrical Engineering from The Johns Hopkins University Hallie Walden is an Advanced Systems Manager at Ball Aerospace Technologies Corp. She worked as the STP-SIV Capture Manager and as the Deputy Program Manager from program inception through the Preliminary Design Review. Prior efforts include thermal, fluids, and thermodynamic analyses and design for Ball Aerospace, Lockheed Martin, and AlliedSignal \(now Honeywell\. She holds B.S.M.E and M.S.M.E. degrees from Stanford University  Alan Lopez is currently a Principal Systems Engineer at Ball Aerospace Technologies Corp. in Boulder CO since 2005. He received his undergraduate degree from Bradley University in Peoria, Illinois and began working at TRW in Redondo Beach, CA in 1985. After completing his M.S.E.E. degree at the University of Southern California in 1990, he began working at Hughes Space and Communications in El Segundo, CA  Mike Pierce is a Principal Systems Engineer at Ball Aerospace Technologies Corp. He is the Chief System Engineer for the STP-SIV program and wrote the STP-SIV PLUG. Prior efforts include system modeling and simulation, mission operations, and mission design working for Spectrum Astro \(now General Dynamics\ and the USAF At Spectrum Astro, he was a key member of Discover II Space Based Infrared System\( SBIRS\ Low and Near Field Infrared Experiment \(NFIRE\ programs. He holds a B.S. in Aerospace Engineering from the United States Naval Academy and an M.S. in Aerospace Engineering degree from the University of Colorado at Boulder  David Kaufmann is Program Manager for STP-SIV at Ball Aerospace & Technologies Corp He also served as Program Manager for DARPA Orbital Express NextSat Commodity Spacecraft program. He previously served as a Thermal Engineer at Hughes Space and Communications Co. in El Segundo, CA. He holds a B.S. from Stanford University and an M.S. and Ph.D. from California Institute of Technology in Mechanical Engineering 


 15  The architecture permits random access to any EDAC word within the recorder. Simultaneous writing and reading is supported.  Memory is divisible into blocks with an unlimited number of blocks forming a Sector.   Each sector contains information that permits reconstruction of the File Structure independent of the CDH Sector Definition As unique Data Units are identified by the CDH host, the SSR commences placement of information in the first available memory location The information placed therein contains information identifying the Sector and relevant information with respect to data unit identification  Figure 22 - Sector & Data Unit Structure Each sector of information has contained within identification as to the next sector tag as well as the tag of the previous sector.  In this way, should the file structure need to be rebuilt, a scan of memory anywhere will reveal not only the type and source of information encountered but also the logical placement of a ll relevant information before and after this data block Consumables For Flash memory technology, the data is stored, and erased, by the imposition of high voltages onto the substrate and controlling gates.  Therefore, this technology is stressful to the device and is rated to 100,000 write or erase cycles NROM technology uses a different charge storage mechanism and is therefore not subject to write/erase degradation Stability Testing has shown commercial NROM devices to retain data up to 300 krads, a simple refreshment following exposure permits a likewise gain in radiation tolerance That is 300krads, refresh, 600 krads, etc [1   Flash devices are prone to damaging charge-trapping and therefore do not benefit by the refreshment scheme outlined above.  The users of Flash must take the surrounding radiation environment into consideration when declaring the number of write cycles a certain design can accommodate Chassis Dimensions and mass for the HC-SSR assume the chassis is manufactured in 100 mils aluminum,  One possible design is shown in Figure 23  Figure 23 \226 Packaging Concept \226 HC-SSR Power Worst case power estimation for the Flash unit is 6W and 30W for an NROM-based design 9 The power estimation includes both SpaceWire interfaces enabled and simultaneous read and write of information occurring Unused non-volatile memory devices are unpowered and the non-addressed memory contro ller ASICs are in a Deep Sleep mode 8  C ONCLUSION  A review of data storage technologies was presented and evidence presented to affirm the assertion known as 223Moore\222s Law\224 that storage capacity of a single element will increase every \(a pprox.\0 months A prognostication of data recorder capacity requirements for future JPL mission and programs has been made.  Based upon a past history spanning over thirty years, it is clearly evident that required capacity will progress at an exponential rate, effectively mimicking the rate provided in Moore\222s Law.  A new Axiom is coined:  A reasonable accurate prediction of data recorder volume and power can be made when based upon today\222s technology and today\222s data storage requirements   9 Power margin \(uncer tainty\cluded in these figures Header/Sector ID Next Previous Data Unit Header Data Data Sector   Data Data Unit End Flag Next Previous                        


 16  A High Capacity Solid Sate Recorder \(HC-SSR\is suggested using devices predicated to be available for each decade.   The design is robu st and easily implemented using conservative design and manufacturing techniques D EFINITIONS  rad radiation absorbed dose\:   the dose causing 0.01 joule of energy to be absorbed per kilogram of matter.   As the absorption is greatly affected by the molecular structure of the material, citations should al so indicate the material as a subscript to the term 223rad\224, as in rad Si indicating Silicon equivalency.  For the purposes of this paper, radiation equivalency always assumes Silicon For completeness, it should be noted that System International replaced the \223rad\224 with the unit Gr ay \(Gy\nd having an equivalency of 100 rads = 1 Gy [27 How e v e r   the use of rads, kilorads, megarads remains in the industry vernacular and is used in this document Moore\222s Law Named after Fairchild Semiconductor technologist Gordon Moore, Moore\222s law was derived from empirical data which shows that the dimensions of basic memory cells will shrink by approximately 50% of the previous value every 30 to 36 months.  It is Moore\222s Law more or less, that forms the backbone of the ITRS examinations for memory devices A DDITIONAL M ATERIAL  Standard Dose Rates for Various Orbits and Missions per year Earth    LEO  100 rad \(protons  MEO  100 krad \(protons electrons  GEO  1 krad \(electrons  Transfer Orbit  10 krad \(protons electrons Mars     Surface  2 krad \(electrons  Orbit  5 krad \(protons  Transit  5 krad \(protons Jovian     Transfer  100 Mrad \(protons electrons A CKNOWLEDGEMENT  The research described in this paper was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration.   The Author thanks the many who guided the concept and offered support all along the way.   With special thanks to fellow-JPL\222ers Gary Noreen who provided funding\nd Taher Daud who provided editing ad-hoc extraordinaire  R EFERENCES    G M o o r e C r a m m i ng m o re C o m pone nt s o n t o  Integrated Circuits Electronics vol. 38, no. 8, April 1965 2 G M o ore, "No Expo n e n tial is Forev er: B u t 223Forever\224 Can Be Delayed Digest of Technical Papers, International Solid State Circuits Conference pp. 1.1-1 thru 1.1-19, 2003  S eagate Tec hnology Com p any. Seagate Tec hnical Corporation. [Onlin http://www.seagate.com/docs/pdf/marrketing/Article _Perpendicular_Recording.pdf   B l u R ay Di sc Ass o ci at i on  2 00 5 M a rc h B l u-R a y  Disc Technical Papers  J Vel e v K  D B e l a shche n ko  an d et _al   2 0 0 7  October\ MSREC - University of Nebraska Onlin http://www.mrsec.unl.edu/research/nuggets/nugget_2 6.shtml  6 R Katti, "Honeywell Rad i atio n Hard en ed  No nVolatile Memory \(MRAM\ Product Development in Proceedings, IEEE No n-Volatile Memory Technology Symposium Orlando, 2004, pp. L2:1-15 7 S aif u n  Sem ico n d u c tor  2 008 NV M Techno log y  Over http://www.saifun.com/content.asp?id=113  


 17    J. Ta usc h  S. T y son a n d T T a i r ba nks  Multigenerational Radiation Response Trends in SONOSb ased NROM Flash Memories with Neutron Latch-up Mitigation," in NSREC Radiation Effects Workshop Honolulu, 2007, pp. 189-193 9  Semico n du c to r In du str y A s sociatio n SIA  2 008   August\ Home. [Online  www.itrs.net  1  S. Ty s o n P ri v a t e C o m m uni que Tra n sEl  Semiconductor, Albuquerque, NM, 2008 1  T. M i k o l a ji c k  and C U Pi n n o w  2 00 8 N o vem b er Indo-German Winter Academy, 2008, Course 3 Onlin http://www.leb.eei.unierlangen.de/winterakadem ie/2008/courses/course3_ material/futureMemory/Mikolajick_TheFutureofNV M.pdf   BAE System s North Am erica, [Data Sheet Microcircuit, CMOS, 3.3V, NVRAM 8406746, April 28, 2008, Rev A 1  N Ha dda d a n d T Scot t  A da pt i n g C o m m erci al  Electronics to the Natura lly Occurring Radiation Environment," in IEEE Nuclear and Space Radiation Effects Conference Short Course Tucson, 1994, pp iv-14 1  D. R  R o t h a n d et _al S EU a n d TI D Test i n g of t h e Samsung 128 Mbit and the Toshiba 256 Mbit flash memory," in Radiation Effects Data Workshop  Reno, 2000 1  F. I r o m and D N guy e n  S i n gl e E v ent  Ef fe ct  Characterization of High Density Commercial NAND and NOR Nonvolatile Flash Memories Honolulu, 2007 1  C Ha fer M  L a hey a n d et _al R adi a t i o n H a rd ness  Characterization of a 130nm Technology," in Proceedings IEEE Nuclea r and Space Radiation Effects Conference Honolulu, 2007 17  T. R O l dh am J. Fr iend lich  an d et_ a l, "TID  an d SEE Response of an Advanced Samsung 4Gb NAND Flash Memory," , Honolulu, 2007  R. C. Lac o e C MOS Scaling, Desi gn Princi ples a n d Hardening-by-Design Methodologies," in Nuclear and Space Radiation Effects Conference Short Course Notebook Monterey, 1993, pp. II-1 thru II142 1 J. Pat t e rs o n a n d S  Gue rt i n   E m e rgi ng S E F I M o des and SEE Testing for Highly-Scaled NAND FLASH Devices," in Proceedings 2005 Non-Volatile Memory Technology Symposium vol. CD-ROM, Dallas, TX 2005, pp. G-3, Session G ; Paper 3 2 J. Ta usc h  S. T y son a n d T F a i rba nks  Mulitgenerational Radiation Response Trends in SONOSb ased NROM Flash Memories with Neutron Latch-up Mitigation," in Honolulu Radaition Effects Data Workshop, NSREC, 2007, pp. 189-193 2 M Janai  B Ei t a n A Sha p pi r I B l o o m and G  Cohen, "Data Retention Reliability Model of NROM Nonvolatile Memory Products IEEE Transactions on Device and Materials Reliability vol. 4, no. 3, pp 404-415, September 2004 2 D N g uy en a n d F I r o m Tot al Io ni zi n g  Do se \(T ID  Tests on Non-Volatile Memories: Flash and MRAM," in 2007 IEEE Radiation Effects Workshop  vol. 0, Honolulu, 2007, pp. 194-198  G. Noree n  a n d et_al L ow Cost Deep Space Hybrid Optical/RF Communications Architecture," , Big Sky, Montana, 2009, Pre-print 2 T. Sasa da a n d S. I c hi kawa  A p p l i cat i o n o f  Sol i d  State Recorders to Spacecraft," in Proceedings, 54th International Astronautical Cogress Bremen, 2003 2 H Ka nek o  E rr or C o nt r o l C odi ng f o r  Semiconductor Memory Systems in the Space Radiation Environment," in Proceedings, 20th IEEE International Symposium in Defect and Fault Tolerance in VLSI Systems, DFT2005 Monterey 2005 2 T. Sasa da a n d H Ka nek o  D evel o p m e nt an d Evaluation of Test Circuit for Spotty Byte Error Control Codes," in Proceedings, 57th International 


 18  Astronautical Congress Valencia, 2006 27  Bu reau  In tern atio n a l d e s Po ids et Mesures. \(2 008  August\SI Base Units. [On http://www.bipm.org/en/si/base_units   B IOGRAPHY  Author, Karl Strauss, has been employed by the Jet Propulsion Laboratory for over 22 years.  He has been in the Avionics Section from day One.  He is considered JPL\222s memory technology expert with projects ranging from hand-woven core memory \(for another employer\o high capacity solid state designs.  He managed the development of NASA\222s first Solid State Recorder, a DRAM-based 2 Gb design currently in use by the Cassini mission to Satu rn and the Chandra X-Ray observatory in Earth Orbit.  Karl was the founder, and seven-time chair of the IEEE NonVolatile Memory Technology Symposium, NVMTS, deciding that the various symposia conducted until then were too focused on one technology.  Karl is a Senior IEEE member and is active in the Nuclear and Plasma Scie nce Society, the Electron Device Society and the Aerospace Electronic Systems Society Karl is also an active member of SAE Karl thanks his wonderful wife of 28 years, Janet, for raising a spectacular family: three sons, Justin, Jeremy Jonathan.  Karl\222s passion is trains and is developing a model railroad based upon a four-day rail journey across Australia\222s Northern Outback   


 19 Bollobs, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathmatiques Appliques de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


