IEEE Conference ID 
39669 5 th 
Advancement in Research Trends 
International Conference on System Modeling 
Proceedings of the SMART 
College of Computing Sciences 
November  
1 Department of Information Technology Government Polytechnic Jalgaon India 2 Department of Computer Science and IT Dr  8abasaheb Ambedkar Marathwada University Aurangabaad India 3 Department of Computer Engineering J 
  
Shubhangi D Patill  Dr Ratnadeep R Deshmukh 2 and D K Kirange 3 
2016 
Adaptive Apriori Algorithm for Frequent Itemset Mining 
T 
25 th _27'h 
Mahcyan College 
2016 
Information Technology Teerthanker Mahaveer University Moradabad India 
of Engineering Faizpur India E-mail lsdkirange@gmail.com.2rrdeshmukh.csit@bamu.ac.in.3dkirange@gmail.com 
Abstrac:l--Obtaining 
is well known that the size of the database for defining candidates has great 
It 
frequent itemsets from the dataset is one of the most promising area of data mining  The Apriori algorithm is one of the most important algorithm for obtaining frequent itemsets from the dataset But the algorithm fails in terms oftime required as well as number of database scans Hence a new improved version of Apriori is proposed in this paper which is efficient in terms of time required as well as number of database scans than the Apriori algorithm 
effect on running time and memory need We presented experimental results showing that the proposed algorithm always outperform Apriori  To evaluate the performance of the proposed algorithm we have tested it on Turkey student's database as well as a real time dataset 1 
Keywords Database Scans Apriori Data Mining 
I 
et al 
INTRODUCTION In Data Mining  for location and fascination of relations in variables in large databases Association Rule Mining is a standard and well researched technique Before applying various data mining techniques such as classification clustering and prediction  for data analysis  association rule mining is used The association rule mining was first proposed by Agrawal 
are bought by the customer at the same time  Such rules are represented like 
where 
It is one of the most recommended research area which is applicable in most of the fields like analysis of market trends forecasting and detection of faults While analysis of the market trends  association rule mining is used to obtain all association rules like  Items X and 
define the support of association rule 
and Yare sets of items that from a transactional database The percentages of transactions in the database containing X U 
X Y Y 
A database can be analysed by finding interesting 
Y 
X 
X 
 
Y 
relationships and patterns among items in the database by using association rule mining The process of association rule mining is divided in two steps first fmd all frequent items from the dataset and then discovering the relationships among the items in the database Itemset denotes a set of items Item sets with support count more than the minimum support threshold 
978-1-5090-3543-4 are referred as frequent item sets Mostly the performance of the association rule mining is affected by the first step  as next step of association rule mining is simple 2 Hence mostly association rule mining is mostly called as frequent itemset mining also The two 
251 
ISBN 
Copyright 
2016 
SMART 
most frequently used algorithms of association rule mining are Apriori and FP-Growth 3 4 Both of these algorithms are having different approaches for finding frequent item sets The Apriori Algorithm generates the frequent item sets level wise using the apriori property But the major drawback of the apriori algorithm is that more execution time is needed for generating the candidate item sets Also the number of database sand required is more The number od database scans required for FP growth is less as it creates the tree structure which is used for signatures of the transactions signatures of transactions on a tree structure  Recently  a Matrix Apriori algorithm proposed by 
5 takes the advantages of both Apriori and FPGrowth The number of database scans is reduced in this proposed work because it creates signatures of itemset in the form of matrix The overall performance of the algorithm is good as compared to FPGrowth 6 Although in all of these improved versions of Apriori the number of database scans required is less but the time required is more 7 While performing the association rule mining using Apriori algorithm  the first level candidate itemset are generated and then these are used to generate the second level candidate itemset and so on The number of database scans as well as time 
required for frequent itemset mining is more in this case An adaptive itemset mining is used to overcome this problem Here the frequent itemset for the last level are generated  and then all transactions contained in the last level are found  These transactions are copied for all lower levels also and new transaction database is created by deleting the already copied transactions for each subsequent level In this paper  Adaptive Apriori Algorithm is proposed as a new and efficient way for frequent itemset mining and compared with Apriori algorithm In this paper the Apriori algorithm is improved in terms of time required as well as number of database 7 


College of Computing Sciences the similar way  Hadoop based on MapReduce  cluster based parallel data mining are some of works for big data mining Various classical data mining algorithms are based on Hadoop Various parallel algorithms using Apriori such as SPC  FPC and DPC 11 are proposed by Lin the pruning step  the infrequent candidate item sets are filtered out This step ensures that every subset of a frequent itemset is also frequent Hence  if the candidate item set contains more infrequent item sets  will be removed from the process of frequent itemset and association mining 4 This process is called pruning 25 th _27'h 12 scans the transactions database for counting the frequent item sets in the Map stage  and the statistical operations are performed for obtaining frequent item sets in the Reduce phase  but But this algorithm also required to start MapReduce tasks repeatedly S Hammoud 13 proposed a parallel in each round of the iterative procedure  where cut datasets are alloted to every Map hub and measurable competitor incessant item sets  then converging to get continuous itemsets in the Reduce stage  As a result of the MapReduce system s high dormancy and absence of emphasis  Apriori calculation doesn't fit the MapReduce structure well Spark is a memory-based parallel registering system  and it can incredibly enhance the ongoing information handling and guarantee the group's high adaptation to internal failure and high versatility 14 in enormous information situations A parallel Apriori algorithm was introduced by Qiu H Apriori algorithm was the first algorithm for finding the frequent item sets and association rule mining  The Apriori algorithm is divided in two major steps join and prune The new candidate seta is generated in the join step Depending on the support count  the candidate set can be defined as frequent or infrequent For generating higher level candidate itemsets Ci previous level frequent itemsets Li-l are joined  scans The intermediate dynamic dataset is created separately using MATLAB by using the database transactions at each level separately Thus instead of scanning the entire database  we need to scan only the extracted rows and columns at each level The proposed improved Apriori algorithm outperforms the basic Apriori because at each level  the transactions with minimum support are eliminated  Hence not considered for higher levels This helps to reduce the size of the database at each level which saves a lot of time  and a noticeable improvement in the speed by reducing the frequent database scans 11 November In In In In 5 th One of the mostly used algorithms in association rule mining is the Apriori algorithm  Input D  a database of transactions Min _ sup  the minimum threshold support Output L.K Maximal frequent item sets in D Ck Set of Candidate k-itemsets RELATED WORK THE APRlORI ALGORITHM International Conference on System Modeling this algorithm the transaction database is divided in to data blocks of same size  These information squares are gotten to for producing the I-incessant itemsets then the hopeful 2-successive item sets independent from anyone else join  finally it consolidates the I-continuous itemsets and competitor 2-reguJar item sets that every piece created  The procedure is rehashed until there is no new itemsets or achieve the breaking point These algorithms are able to perform well when the size of the database is small or the dimensionality of the data is not so high But while considering the big data  these algorithms can not perform well MapReduce[lO framework produced by Googlein 2004 is able to handle the big data which is related to Spark YAFIM 15 The results obtained using SPARt are more promising than that using Hadoop Ill 2016 Information Technology  Teerthanker Mahaveer University  Moradabad  India this algorithm the high dimensional frequent item sets are obtained using the low dimensional frequent item sets  The process is iterative  But the basic Apriori algorithm faces some major drawbacks such as the number of database scans required is more This results in the I/O overburdened  the process of obtaining frequent item sets at the higher level from the lower level needs more time as the number of itemsets are more at the lower level  the basic Apriori algorithm should not consider the transactions that need not scan Most of the researchers have proposed the various improved versions of the Apriori algorithm  In the DHP algorithm proposed by Park Apriori Algorithm 1 Apriori Algorithm ISBN 978-1-5090-3543-4 2016 A Advancement in Research Trends SMART   251 8 the algorithm is based on dynamic hash hashing algorithms and pruning algorithm  Here the transactions that are not involved in generating frequent item sets are not considered while traversing the database Thus the efficiency for frequent itemset mining is improved  The dynamic itemset counting algorithm proposed by Brin 9 requires less number of database scans et al et al et al et al These are based on MapReduce 8 In the SPC algorithm the dataset is mapped to all Map nodes and mining is performed parallelly Afterwards the combining operation is executed in the reduce phase  SPC algorithm required to start the map and reduce phase only once The DPC and FPC algorithm needs to repeatedly start the Map and Reduce phase  The process is defined by the number of dimensions of the frequent item sets mining  The parallel frequent item set mining algorithm proposed by Li Copyright 


B Adaptive Apriori Algorithm The Adaptive Apriori algorithm proposed here is able to overcome the basic Apriori algorithm in terms of number of database scans as well as time required The size of the database is reduced at each level This algorithm uses a dynamic technique to reduce the time required for candidate itemset generation 3 Algorithm to K 225 Delete these transactions from the database D and update the database 225 Now consider this updated reduced in size database for finding all transactions for level to K-I 225 Repeat the steps subsequently and update the database 225 Consider this updated database for candidate itemset generation at each step 225 L1  Frequent items oflength that are contained in t 225 Lk  Method 225 L1  Frequent items oflength 1 1 2 Drawbacks of Apriori Algorithm An Example of Adaptive Apriori Algorithm SMART 2016 ISBN 978-1-5090-3543-4 Adaptive Apriori Algorithm for Frequent Itemset Mining 1 1 1 225 For k  1 Lk   k   do 225 Ck  225 The Apriori algorithmic program takes longer time for candidate generation technique 225 The Apriori algorithmic program needs many scans of the database 225 Many trivial rules are derived and it will be hard to extract the most interesting rules  225 Rules can be inexplicable and fme grained 225 Redundant rules are generated  251 225 For k  I;Lk   k   do 225 Consider D as intermediate updated database for level k 225 Ck 1 candidates generated from Lk 225 For each transaction t in database D do 225 Increment the count of all candidates in Ck 1 that are contained in t 225 Lk 1  candidates in Ck  1 with minimum support 225 end do 225 Return the set Lk as the set of all possible frequent item sets 225 In this algorithm  the intermediate database is generated to reduce the time required for candidate itemset generation is claimed that the size of the database is reduced at each level starting from last to first and hence the time required for candidate itemset generation is reduced as compared with basic Apriori algorithm Here we generate the dynamic intermediate database for each level separately For example when scanning each transaction in the database find all the transactions which contain all items These transactions are considered for generation for level K itemset The same transactions are also considered for generating level 1 to level k-I itemsets So these are copied for all these levels Now the database is updated by deleting all these transactions This updated database is again considered for generating level L k-I itemsets Hence the size of the database is reduced at level of candidate itemset generation as well as the time required is also minimized  Firstly  scan all transactions which are required to get frequent K itemset Copy all these transactions from database to the intermediate database for level 1 to K Remove all these transactions from the original database Now again scan the new updated database to find the transactions that are needed to build frequent K-I itemset Again modify the intermediate database from level I:K-Iand remove the related transactions from the database  At each step of candidate 9 1 Input D a database of transactions Min _ sup the minimum threshold support Output Lk Maximal frequent item sets in D Ck Set of Candidate k-itemsets Method 225 Generate the Intermediate Database 225 Find all transactions to be considered for level K containing all item sets 225 Copy these transactions in the database to be considered for level  candidates generated from Lk 225 For each transaction t in database D do 225 Increment the count of all candidates in Ck   candidates in Ck  C with minimum support 225 End do 225 Return the set Lk as the set of all possible frequent item sets The main notation for association rule mining that is used in Apriori algorithm is the following 225 A k itemset is a set ofk items 225 The set Ck is a set of candidte k-itemsets that are potentially frequent 225 The set Lk is a subset of Ck and is the set of k-itemsets that are frequent Copyright l It l l Assume that a large bookstore sales data by stock keeping unit SKU for each item  such as  Scale   eraser   Book   Pencil   Pen   Paper  is identified by a numerical SKU The bookstore database contains the transactions which are a set of items purchased together At the particular time  consider the transaction database as shown in Table 


College of Computing Sciences November 25 th _27'h ID ID ID ID ID T7 T7 T7 T7 T7 5 th Itemsets No Itemsets P aper  Eraser  Sca le B oo k P en  P enc il Itemsets Itemsets Sca le Sca le B oo k Paper  Eraser  Sca le Book 7 Paper  Eraser  Sca le Book  P en  P enc il T ABLE I THE TRA NSAC TIO NS I Tl 2 T2 4 T4 5 T5 6 T6 T AB LE 2 I N TERMEDIATE DATABASE FOR LEVEL K  6 T ABLE 3 I N TERMEDIATE DATABASE FOR LEVEL K  5 T ABLE 4 I N TERMEDIATE DATABASE FOR LEVEL K  4 2 T2 4 T4 5 T5 6 T6 I Tl 4 T4 5 T5 6 T6 I 5 2 4 5 I 2 2 6 6 T 6 4 T4 6 T6 I Tl 2 T2 2 T2 4 4 6 2 2 4 2 5 2 I 2 P aper  Eraser  Sca le B oo k 7 Pap er Pen 3 3 Pe ncil 3 B oo k 3 Table 9 shows frequent 2 items et generated using the intermediate table for level K Pap er Pen P aper  Eraser 3 P aper  Sca le 3 Pap er  B oo k Pen Eraser Pen Sca le 7 Pen B oo k 8 Eraser  Sca le 3 9 Eraser  Book 3 The first experiment compares the number of database scans of original Apriori and our improved algorithm by applying the Turkiye s tudent s evaluation Now to generate the frequent 3 item set  we need to consider only the intermediate database created for level 3 As can be noted here the size ofthe database is reduced to 4 only  Thus the propo se d adaptive Apriori algorithm helps to improve the running time ofthe program IV ANALYSISANDEVALUATlONOF THE ADAPTIVE ApRlORl Apriori Algorithm used to check the database thrice yet this paper exhibits a change on it by utilizing versatile calculation and the idea of moderate database scans The examination demonstrates that the time expended and number of database outputs required in enhanced Apriori in each group of transactions is le ss than the first Apriori  The memory space is lessened by utilizing the dynamic database approach which segments the first database at first and select one specific database out of this TABLE 5 I N TERMEDIATE DATABASE FOR LEVEL K  TABLE 6 I N TERMEDIATE DATABASE FOR LEVEL K  2 TABLE 7 I N TERMEDIATE DATABASE FOR LEVEL K  I TABLE 8 CAND ID A TE IITEM SET TABLE 9 CAND ID A TE IITEM SET It is a change as prior the calculation took exponential time however now it is decreased incredibly  Eraser  Scale  Pen Eraser  Sca le Pen Eraser  Sca le P en Eraser  Sca le P en Eraser International Conference on System Modeling Information Technology  Teerthanker Mahaveer University  Moradabad  India itemset generation we get the reduced database which helps to improve the running time ofthe algorithm 3 Sr No  Transaction 2 A Analysis and Evaluation of the Adaptive Apriori for Number of Database Scans ISBN 978-1-5090-3543-4 2016 6 Such transaction is T7 in the above example which contains all the items So this transaction is removed from the dataset and copied for intermediate database level 1 K 1 1 5 Similarly intermediate database is constructed for all levels as shown in Table 2 to 8 Sr Transaction Itemset s Pa er  Eraser  Sca le B oo k P en  P enc il Sr No  Tran sa ction Itemset s Sr No Transaction Itemset s Sr No Transaction Sr No  Items Support Sr No  Item s Support 2016 Advancement in Research Trends SMART     P aper  P en Pap er  P enc il  Scale 3 T3 Book  Eraser P enc il  P aper P aper  Eraser  Scale  B oo k 7 P aper  Eraser  Sca le B oo k P en Pe ncil Now the transaction database is scanned to find the transactions that can be used for generating frequent item sets at level K P aper  Eraser  Sca le B oo k P en  P enc il Paper Pe ncil  Sca le P aper  Eraser  Sca le Book 7 P aper  Eraser  Sca le Book  P en  P enc il Paper Pen Paper Pe ncil  Sca le 3 T3 B ook  Eraser P enci l P aper Paper Pen P aper  Pe ncil  Sca le 3 T3 B oo k Eraser Pe ncil  P aper P aper  Eraser  Sca le Book 7 P aper  Eraser  Sca le B oo k P en  P enc il Now by considering this newly constructed intermediate database at each level  firstly check all transactions to get successive 1-itemset Ll containing the 10 items and their support count and the exchanges ids that contain these things and afterward take out the competitors that are rare or their backing are not exceeding the min _ s up as appeared in Table 2 The frequent 1-itemset is shown in Table 3 Table 8 shows the candidate itemset generated at level 1 using intermediate database for level 1 251 10 Copyright 


Fig  I Number of Databa se Scans for Different Values of Support for Turkiye Databa se Faculty I Figure 2 shows the improvement of the proposed apnon algorithm for Turkiye student's evaluation database for faculty 2 By considering 100 transactions from the database Table 11 shows that the improved Apriori reduce the number of database scans from the original Apriori TABLE 11 SUPPORT Fig  2 Number of Databa se Scans for Different Value s of Support for Turkiye Databa se Faculty 2 Table 12 shows that the improved adaptive apriori reduces the number of database scans as compared to original apriori for faculty 3 TABLE 12 SUPPORT          I  S up port   IIII   100  _       600 _ NUMBER OF DATABASE SCANS FOR TURKlYE FACULTY I Sr  No Support Number of Database Scans Apriori Adaptive Apriori NUMBER OF DATABASE SCANS FOR T U RKlYE FAC U LTY 2 SrNo Support Number of Database Scans Apriori AdaDtive ADriori NUMBER OF DATABASE SCANS FOR T U RKlYE FA CU LTY 3 Sr No Support Number of Database Scans AJlI'iori Adaptive Apriori   u-uw w u-o Cl 1 u  I           I I     o  w The second experiment compares the execution time original Apriori and our improved algorithm by applying the Turkiye student's evaluation database for faculty 1 in the implementation  500  11-0 0-0 0 0 0 0-0   100    4 00        1     256     2 00 database 16 for facuIty 1 in the implementation The 100 transactions considered here for analysis The result is shown in Fig Performance Evaluation Support q FP Growth Support q FP Growth IJ Bas i c.ARr i o,r i    Adapt i ve BaS IC Aprlorl A   0.1 580 297 2 0.2 580 297 3 0.3 580 297 4 0.4 580 297 5 0.5 580 297 6 0.6 580 297 7 0.7 580 297 8 0.8 580 297 9 0.9 580 297 10 1.0 7 7 5 00 0.1 579 302 2 0.2 579 302 3 0.3 579 302 4 0.4 579 302 5 0.5 579 302 6 0.6 579 302 7 0.7 579 302 8 0.8 579 302 9 0.9 296 154 10 1.0 7 7 0.1 518 280 2 0.2 518 280 3 0.3 518 280 4 0.4 518 280 5 0.5 518 280 6 0.6 518 280 7 0.7 518 280 8 0.8 455 280 9 0.9 144 92 10 1.0 6 7 Z   I I J Bas i c,ARr i o,r i _   Adapt i ve BaS IC Apnon Apr i or i  600 11-0 0-0 0 0 0 400 en  0 vs vs vs  SMART 2016 ISBN 978-1-5090-3543-4 Adaptive Apriori Algorithm for Frequent Itemset Mining Performance Evaluation q 251   500   7   400   priOri Fig  3 Execution Time for Different Value s of Support for Turkiye Student's Evaluation Database for Faculty I 1 FP Growth tJ Bas ic AP.f i or i   Adapt i ve Bas ic Apr i or i Apr i or i   11 Table 10 shows that the improved Apriori reduce the number of database scans from the original Apriori TABLE 10 SUPPORT Copyright B Analysis and Evaluation of the Adaptive Apriori for Execution time 


College of Computing Sciences Suppo rt I I I I I  I  Adaptive Apriori 100 0 ___ 225 225 _ _ 225\225 ___ _ ___ 25 th _27'h l300 q I 160C V this algorithm  the intermediate database is created at each level Hence scanning the entire database at each subsequent level is avoided Which reduces the time required for candidate item set generation as well as the number of database scans The performance of the proposed algorithm is evaluated using the Turkiye standard student faculty evaluation dataset as well as the real time dataset 5 th Execution Time 200 500 Analysis and Evaluation of the Adaptive Apriori for Real Time Data The 100 transactions considered here for analysis The result is shown in Fig 3 Fig 3 shows that the improved Apriori reduce the execution time from the original Apriori Figure 4 shows that the improved Apriori reduce the execution time from the original Apriori for Turkey faculty 2 dataset  t tr _-<:t O--O C O-O q l      225 ___ 225 ___ 225 ___  I      I  I I  I I I I I I I I 20u CONCLUSION Association rule mining plays the major role in the field of data mining  The association rule mining is divided in two steps Firstly it finds all frequent item sets and then it generated the association rules Apriori algorithm is one of the most important algorithms proposed for frequent itemset mining  But the Apriori algorithm required more time for generation of frequent itemsets as well as the number of database scans is more  In this paper the improved Apriori algorithm is proposed which is more efficient in terms of time as well as number of database scans Copyright 600 400   AI-o o-c-o o-o o  Support  h _ International Conference on System Modeling 5 400 November Information Technology Teerthanker Mahaveer University Moradabad India 1400 1000 0.1 2492 1017 2 0.2 2492 1017 3 0.3 2492 1017 4 0.4 2492 1017 5 0.5 2492 1017 6 0.6 2492 1017 7 0.7 2492 1017 8 0.8 2490 1017 9 0.9 1069 435 10 1.0 28 22 Table 14 shows that the improved adaptive apriori reduces the execution time as compared to original apriori for real time dataset TABLE 14 SU PPORT 0.1 2764.4949 2166.3458 2 0.2 2776  5642 2164  0938 3 0.3 2789.4587 2162  6268 4 0.4 2773  9934 2166  7127 5 0.5 2768  8197 2167  2221 6 0.6 2771.1176 2187  1428 7 0.7 2819  0367 2182  2447 8 0.8 2044.3822 1750.3789 9 0.9 56.3806 58  8522 10 1.0 0 24622 0 99224  _ vs   II I II II E FP Growt h tJ Bas ic AWiori   Adaptive Bas ic Apriori Apriori  t FP Growt h tJ Bas ic AWiori   Adapt iv e Bas ic Apriori Apriori C vs  o 1 5 10 978-1-5090-3543-4 2016 1800 p 0 0-c0 00  1200 TIME FOR REAL TIME DATA S ET Sr No 2016 ISBN fJ    __    Fig  4 Execution Time for Different Value s of Support for Turkiye Stud e nt s Evaluation Databa se for Faculty 2 Fig  5 Execution Time for Different Value s of Support for Turkiye Student s Evaluation Databa se for Faculty 3 Figure 5 shows that the improved Apriori reduce the number of database scans from the original Apriori for turkey faculty 3 dataset Advancement in Research Trends SMART We have gathered the real time faculty evaluation data from 393 students of faculties from J.T Mahajan College of Engineering  Faizpur We have compared the 12 execution time as well as number of database scans of original Apriori  and our improved algorithm by applying the real time database implementation Table 13 shows that the improved adaptive apriori reduces the number of database scans as compared to original apriori for real time dataset TABLE 13 SU PPORT    q q NU MBER OF DATABASE SCANS FOR REAL TIME DATA S ET Sr No  Support Number of Database Scans AJ!!'iori Adaptive Apriori 251 800  0  225 _ _ __ ___ _ __ _  Performance Eval u ation Performance Evaluation In 


VLDB 94  San Francisco  CA  USA pp  487-499  Morgan Kaufinann Publishers Inc  1994 4 Han  J  J Pei  and Y Yin   Mining frequent patterns without candidate generation  In ACM International Conference on Software Engineering  Artificial Intelligence  Networking and Parallel 7 Agrawal R Srikant R   Fast algorithm for mining association rules   Proceedings of 20th International Conference on Very Large Data Bases VLDB  Morgan Kaufman Press  I 994  487-499 8 J S Park  M S Chen  P S Yu   Efficient parallel data mining of association rules  4th International Conference on Information and Knowledge Management  1995  I I 233-235P 9 S Brin et ai   Dynamic itemset counting and implication rules for market basket data  Proceedings of the ACM SIGMOD International Conference on Management of Data  1997  123I 40 10 Jeffrey Dean  Sanjay   Map  Reduce Simplified  Data Processing on Large Clusters  OSDI  04 Sixth Symposium on Operating System Design and Implementation 2004  I th DBA  06  Anaheim  CA USA,2006  pp  75 82  ACT A Press  6 Yildiz  B and SIGMOD 93  New York  NY  USA  1993  pp  207 216  2 Han  J  and M Kamber  Data 13   Distributed Computing SNPD  12  Kyoto  20 I 2  IEEE 236 24 I  13 S Hammoud   MapReduce Network Enabled Algorithms for Classification Based on Association Rules  Thesis  20 I I  14 Yanjie Gao   Data Processing with Spark Technology  Application and Performance Optimization M  China machine Press  201411  1-2  15 Qiu H Gu R Yuan C Distributed Processing  Symposium Workshops IPDPSW  2014 IEEE International IEEE  2014 1664-1671 16 Gunduz  G Y AFIM A Parallel Frequent Itemset Mining Algorithm with Spark[C  Parallel Agrawal  R  T Imielinski  and Lin M  Lee P I Ergenc   Comparison of two association rule mining algorithms without candidate generation  In 2nd ed  ed  Morgan Kaufinann  2006 3 Agrawal  R and R Srikant   Fast algorithms for mining association rules in large databases  In  Fokoue  E   UCI Machine Learning Repository  Irvine  CA University of California  School of Information and Computer Science  20 I 2  I Proceedings of the Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data  Proceedings of the 24th lASTED International Conference on Database and Applications  Proceedings of the 70th lASTED International Conference on Artificial Intelligence and Applications  Shi Z   Parallel Implementation of Apriori Algorithm Based on MapReduce  In Proceedings of the 13 Swami   Mining association rules between sets of items in large databases  In Mining Concepts and Techniques N ACKNOWLEDGMENT Any comments and suggestions are welcome from the reviewers The authors will be thankful to them The authors would also be thankful to all those people who involved in carrying out this research work The authors are also thankful to the Department of Computer Science of Dr Babasaheb Ambedkar Marathwada University Aurangabaad for providing the infrastructure to carry out the research REFERENCES Proceedings of the 20th International Conference on Very Large Data Bases  SMART 2016 ISBN 978-1-5090-3543-4 Adaptive Apriori Algorithm for Frequent Itemset Mining  251 ACM SIGMOD International Conference on Management of Data  et al  A 7993 B  SIGMOD 00  New York  NY  USA 2000  pp  112  5 Pavon  J  S Vi ana  and S Gomez   Matrix Apriori Speeding up the search for frequent patterns  In SIGMOD  93  2010  pp  450-457  ACM  Copyright Hsueh S   Apriori-based Frequent Itemset Mining Algorithms on MapReduce  Proc  of the 16th International Conference on Ubiquitous Information Management and Communication ICUlMC 12  New York  NY  USA  ACM  Article No  76  20 I 2  12 Li N  Zeng L  He Q 


    Fig. 8. Comparative Analysis Classification Algorithms V  CONCLUSION Associative Classification techniques are used to make better decision in critical situations. The proposed associative classification called as Classification of microarray gene expression data using associative classification and gene expression intervals used to clas sify the gene expression with gene intervals in affected gene expression. The experimental results are carried out by using the gene expression of breast cancer. The associative classification on gene expression data obtained the best prediction and accuracy of the classification result. The proposed algorithm was tested with two class and multi class data sets. The classification algorithm was compared with the classical classification algorithms such as Linear Discriminant Analysis, SVM, and Decision Tree. After the comparison of traditional classification algorithms, as per the view of possible error rates the Associative Classification algorithm is best for biological data. The results of this work are used to drug designer for cancer diseases. The proposed algorithm works on gene expression data. In future, it will be implemented on hadoop and big data mining for biological data VI  R EFERENCES  1   Morgan Kaufmann Publishers Elsevier 2002 2   Second Edition PicasetOy Helsinki, 2005 3  Nagata, K., Washio, T., Kawahara, Y. and Unami, A  prediction from toxicogenomic data based on class association rule  ELSEVIER journal Toxicology Reports, vol.41, no.10 pp. 1133-1142, 2014 4  Garcia, S., Luengo, J., Sáez, J. A., López, V. and Herrera, F survey of discretization techniques: Taxonomy and empirical  Knowledge and Data Engineering, IEEE Transactions vol. 25, no.4, pp.734-750, 2013 5  Alves,R., Rodriguez.B.D.S and Aguilar. R.J.S  analysis: a survey of frequent pattern mining from gene expression  Briefings in Bioinformatics 2009, vol.2, no.2, pp.210-224 6   Miner: Maximal Confident Association Rules Miner Algorithm for Up/Down Applied Mathematics and Information Sciences vol.8 no.2, pp.799-809, 2014 7    BMC Bioinformatics vol.19, no.1, pp.7986, 2003 8  Snousy, A. M. B., El-Deeb, H. M., Badran, K. and Al Khlil, I. A  based classification algorithms on cancer  Egyptian Informatics Journal vol.12 no.2 pp.73-82. 2011 9  Refaeilzadeh, P., Tang, L. and Liu, H   Encyclopedia of database systems, Springer US pp. 532-538 2009   R. Agrawal and R. Srikant, Fast Algorithms for Mining Association Rules Proceedings of the 20th Int. Conf. on Very Large Data Bases VLDB94\,475486, Santiago de Chile, Chile 1994   Alagukumar, S., and Lawrance R., "A Selective Analysis of Microarray Data Using Association Rule Mining Procedia Computer Science Vol.47, pp.3-12, 2015 doi:10.1016/j.procs.2015.03.177   Alagukumar  Cancer Data Analysis Using Frequent Pattern Mining and Gene  International Journal of Computer Applications ISSN 0975 8887, no.1, pp.9-14, June 2015   Pasquier, N., Bastide, Y., Taouil, R., & Lakhal, L Pruning closed itemset lattices for association rules  In BDA'1998 international conference on Advanced Databases pp. 177-196. 1998   Giugno R, Pulvirenti A, Cascione L, Pigola G, Ferro A MIDClass: Microarray Data Classification by Association Rules and Gene Expression Intervals. Tang H, ed. PLoS ONE 2013;8\(8\:e69873. doi:10.1371/journal.pone.0069873   http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE1379   Wang, Zuncai, et al. "The prognostic biomarkers HOXB13 IL17BR, and CHDH are regulated by estrogen in breast cancer Clinical Cancer Research 13.21 pp. 6327-6334, 2005    cancer progression and host polymorphisms in the chemokine system: role of the macrophage chemoattractant protein-1 \(mcp 2518 g allele Clinical Chemistry 51: 452 5.2005   Dash, Rajashree, Rajib Lochan Paramguru, and Rasmita Dash Comparative analysis of supervised and unsupervised discretization techniques." International Journal of Advances in Science and Technology 2.3 \(2011\: 29-37   0 10 20 30 40 50 60 70 80 90 100 LDA SVM Decision Tree CACGE Accuracy and Error rate Classification Algorithms Accuracy Error Rate 


on items contained in each item group When the number of pivots increases the entire database is split into a ner granularity and the number of partitions increase correspondingly Such a ne granularity leads to a reduction in distance computation among transactions On the other hand when the pivot number k continues growing the number of transactions mapped into one hash bucket signiÞcantly increases thereby leading to a large candidate-object set and high shufßing cost see Figs 3b and 3c Consequently the overall execution time is optimized when k is 60 for both algorithms see Fig 3a 6.2 Minimum Support Recall that minimum support plays an important role in mining frequent itemsets We increase minimum support thresholds from 0.0005 to 0.0025 percent with an increment of 0.0005 percent to evaluate the impact of minimum support on FiDoop-DP The other parameters are the same as those for the previous experiments Fig 4a shows that the execution times of FiDoop-DP and Pfp decrease when the minimum support is increasing Intuitively a small minimum support leads to an increasing number of frequent 1-itemsets and transactions which have to be scanned and transmitted Table 2 illustrates the size of frequent 1-itemsets stored in FList and the number of nal output records of the two parallel solutions under various minimum-support values Fig 4a reveals that regardless of the minimum-support value FiDoop-DP is superior to Pfp in terms of running time Two reasons make this performance trend expected First FiDoop-DP optimizes the partitioning process by placing transactions with a high similarity into one group rather than randomly and evenly grouping the transaction Fig 4b conÞrms that FiDoop-DPÕs shufßing cost is signiÞcantly lower than that of Pfp thanks to optimal data partitions offered by FiDoop-DP Second this grouping strategy in FiDoop-DP minimizes the number of transactions for each GList under the premise of data completeness which leads to reducing mining load for each Reducer The grouping strategy of FiDoop-DP introduces computing overhead including signature-matrix calculation and hashing each band into a bucket Nevertheless such small overhead is offset by the performance gains in the shufßing and reduce phases Fig 4a also shows that the performance improvement of FiDoop-DP over Pfp is widened when the minimum support increases This performance gap between FiDoop-DP and Pfp is reasonable because pushing minimum support up in FiDoop-DP lters out an increased number of frequent 1-itemsets which in turn shortens the transaction partitioning cost Small transactions simplify the correlation analysis among the transactions thus small transactions are less likely to have a large number of duplications in their partitions As a result the number of duplicated transactions to be transmitted among the partitions is signiÞcantly reduced which allows FiDoop-DP to deliver better performance than Pfp 6.3 Data Characteristic In this group of experiments we respectively evaluate the impact of dimensionality and data correlation on the performance of FiDoop-DP and Pfp by changing the parameters in the process of generating the datasets using the IBM Quest Market-Basket Synthetic Data Generator 6.3.1 Dimensionality The average transaction length directly determines the dimensions of a test data We conÞgure the average transaction length to 10 40 60 and 85 to generate T10I4D 130 blocks T40I10D 128 blocks T60I10D 135 blocks T85I10D 133 blocks datasets respectively In this experiment we measure the impacts of dimensions on the performance of FiDoop-DP and Pfp on the 8-node Hadoop cluster The experimental results plotted in Fig 5a clearly indicate that an increasing number of dimensions signiÞcantly raises the running times of FiDoop-DP and Pfp This is because increasing the number of dimensions increases the number of groups thus the amount of data transmission sharply goes up as seen in Fig 5b The performance improvements of FiDoop-DP over Pfp is diminishing when the dimensionality increases from 10 to 85 For example FiDoop-DP offers an improvement of 29.4 percent when the dimensionality is set to 10 the improvement drops to 5.2 percent when the number of dimensions becomes 85 In what follows we argue that FiDoop-DP is inherently losing the power of reducing the number of redundant transactions in high-dimensional data When a dataset has a low dimensionality FiDoop-DP tends to build partitions Fig 4 Impact of minimum support on FiDoop-DP and Pfp TABLE 2 The Size of FList and the Number of Final Output Records Under Various Minimum-Support Values minsupport 0.0005 0.001 0.0015 0.002 0.0025 FList 14.69k 11.6k 9.71k 6.89k 5.51k OutRecords 745 588 465 348 278 XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 109 


each of which has distinct characteristics compared with the other partitions Such distinct features among the partitions allow FiDoop-DP to efÞciently reduce the number of redundant transactions In contrast a dataset with high dimensionality has a long average transaction length therefore data partitions produced by FiDoop-DP have no distinct discrepancy Redundant transactions are likely to be formed for partitions that lack distinct characteristics Consequently the beneÞt offered by FiDoop-DP for highdimensional datasets becomes insigniÞcant 6.3.2 Data Correlation We set the correlation among transactions i.e corr to 0.15 0.25 0.35 0.45 0.55 0.65 and 0.75 to measure the impacts of data correlation on the performance of the two algorithms on the 8-node Hadoop cluster The Number of Pivots is set to 60 see also Section 6.1 The experimental results plotted in Fig 5c clearly indicate that FiDoop-DP is more sensitive to data correlation than Pfp This performance trend motivates us to investigate the correlation-related data partition strategy Pfp conducts default data partition based on equal-size item group without taking into account the characteristics of the datasets However FiDoop-DP judiciously groups items with high correlation into one group and clustering similar transactions together In this way the number of redundant transactions kept on multiple nodes is substantially reduced Consequently FiDoop-DP is conducive to cutting back both data transmission trafÞc and computing load As can be seen from Fig 5c there is an optimum balance point for data correlation degree to tune FiDoop-DP performance e.g 0.35 in Fig 5c If data correlation is too small Fidoop-DP will degenerate into random partition schema On the contrary it is difÞcult to divide items into relatively independent groups when data correlation is high meaning that an excessive number of duplicated transactions have to be transferred to multiple nodes Thus a high data correlation leads to redundant transactions formed for partitions thereby increasing network and computing loads 6.4 Speedup Now we are positioned to evaluate the speedup performance of FiDoop-DP and Pfp by increasing the number of data nodes in our Hadoop cluster from 4 to 24 The T40I10D 128 blocks dataset is applied to drive the speedup analysis of the these algorithms Fig 6 reveals the speedups of FiDoop-DP a nd Pfp as a function of the number of data nodes The experimental results illustrated in Fig 6a show that the speedups of FiDoop-DP and Pfp linearly scale up with the increasing number of data nodes Such a speedup trend can be attributed to the fact that increasing the number of data nodes under a xed input data size inevitably 1 reduces the amount of itemsets being handled by each node and 2 increases communication overhead among mappers and reducers Fig 6a shows that FiDoop-DP is better than Pfp in terms of the speedup efÞciency For instance the FiDoop-DP improves the speedup efÞciency of Pfp by up to 11.2 percent with an average of 6.1 percent This trend suggests FiDoopDP improves the speedup efÞciency of Pfp in large-scale The speedup efÞciencies drop when the Hadoop cluster scales up For example the speedup efÞciencies of FiDoopDP and Pfp on the 4-node cluster are 0.970 and 0.995 respectively These two speedup efÞciencies become 0.746 and 0.800 on the 24-node cluster Such a speedup-efÞciency trend is driven by the cost of shufßing intermediate results which sharply goes up when the number of data nodes scales up Although the overall computing capacity is improved by increasing the number of nodes the cost of synchronization and communication among data nodes tends to offset the gain in computing capacity For example the results plotted in Fig 6b conÞrm that the shufßing cost Fig 5 Impacts of data characteristics on FiDoop-DP and Pfp Fig 6 The speedup performance and shufßing cost of FiDoop-DP and Pfp 110 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


is linearly increasing when computing nodes are scaled from 4 to 24 Furthermore the shufßing cost of Pfp is larger than that of FiDoop-DP 6.5 Scalability In this group of experiments we evaluate the scalability of FiDoop-DP and Pfp when the size of input dataset dramatically grows Fig 7 shows the running times of the algorithms when we scale up the size of the T40I10D data series Figs 7a and 7b demonstrate the performance of FiDoop-DP processing various datasets on 8-node and 24-node clusters respectively Fig 7 clearly reveals that the overall execution times of FiDoop-DP and Pfp go up when the input data size is sharply enlarged The parallel mining process is slowed down by the excessive data amount that has to be scanned twice The increased dataset size leads to long scanning time Interestingly FiDoop-DP exhibits a better scalability than Pfp Recall that see also from Algorithm 1 the second MapReduce job compresses an initial transaction database into a signature matrix which is dealt by the subsequent process The compress ratio is high when the input data size is large thereby shortening the subsequent processing time Furthermore Fidoop-DP lowers the network trafÞc induced by the random grouping strategy in Pfp In summary the scalability of FiDoop-DP is higher than that of Pfp when it comes to parallel mining of an enormous amount of data 7R ELATED W ORK 7.1 Data Partitioning in MapReduce Partitioning in databases has been widely studied for both single system servers e.g and distributed storage systems e.g BigTable PNUTS[31 The existing approaches typically produce possible ranges or hash partitions which are then evaluated using heuristics and cost models These schemes offer limited support for OLTP workloads or query analysis in the context of the popular MapReduce programming model In this study we focus on the data partitioning issue in MapReduce High scalability is one of the most important design goals for MapReduce applications Unfortunately the partitioning techniques in existing MapReduce platforms e.g Hadoop are in their infancy leading to serious performance problems Recently a handful of data partitioning schemes have been proposed in the MapReduce platforms Xie et al  developed a data placement management mechanism for heterogeneous Hadoop clusters Their mechanism partitions data fragments to nodes in accordance to the nodes processing speed measured by computing ratios In addition Xie et al  designed a data redistribution algorithm in HDFS to address the data-skew issue imposed by dynamic data insertions and deletions CoHadoop is a H a d oop s lightweight extension which is designed to identify relateddataÞlesfollowedbyamodiÞeddataplacement policy to co-locate copies of those related les in the same server CoHadoop considers the relevance among les that is CoHadoop is an optimization of HaDoop for multiple les A key assumption of the MapReduce programming model is that mappers are completely independent of one another Vernica et al  broke such an assumption by introducing an asynchronous communication channel among mappers T his c hannel e nables the m appers to see global states managed in metadata Such situationaware mappers SAMs can enable MapReduce to exibly partition the inputs Apart from this adaptive sampling and partitioning were proposed to produce balanced partitions for the reducers by sampling mapper outputs and making use of obtained statistics Graph and hypergraph partitioning have been used to guide data partitioning in parallel computing Graph-based partitioning schemes capture data relationships For example Ke et al applied a graphic-execution-plan graph EPG to perform cost estimation and optimization by analyzing various properties of both data and computation Their estimation module coupled with the cost model estimate the runtime cost of each vertex in an EPG which represents the overall runtime cost a data partitioning plan is determined by a cost optimization module Liroz-Gistau et al proposed the MR-Part technique which partitions all input tuples producing the same intermediate key co-located in the same chunk Such a partitioning approach minimizes data transmission among mappers and reducers in the shufße phase The approach captures the relationships between input tuples and intermediate keys by monitoring the execution of representative workload Then based on these relationships their approach applies a min-cut k-way graph partitioning algorithm thereby partitioning and assigning the tuples to appropriate fragments by modeling the workload with a hyper graph In doing so subsequent MapReduce jobs take full advantage of data locality in the reduce phase Their partitioning strategy suffers from adverse initialization overhead Fig 7 The scalability of FiDoop-DP and Pfp when the size of input dataset increases XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 111 


7.2 Application-Aware Data Partitioning Various efÞcient data partitioning strategies have been proposed to improve the performance of parallel computing systems For example Kirsten et al  developed two general partitioning strategies for generating entity match tasks to avoid memory bottlenecks and load imbalances Taking into account the characteristics of input data Aridhi et al proposed a novel density-based data partitioning technique for approximate large-scale frequent subgraph mining to balance computational load among a collection of machines Kotoulas et al built a data distribution mechanism based on clustering in elastic regions Traditional term-based partitioning has limited scalability due to the existence of very skewed frequency distributions among terms Load-balanced distributed clustering across networks and local clustering are introduced to improve the chance that triples with a same key are collocated These selforganizing approaches need no data analysis or upfront parameter adjustments in a priori Lu et al studied k nearest neighbor join using MapReduce in which a data partitioning approach was designed to reduce both shufßing and computational costs In LuÕs study objects are divided into partitions using a Voronoi diagram with carefully selected pivots Then data partitions i.e Voronoi cells are clustered into groups only if distances between them are restricted by a speciÞc bound In this way their approach can answer the k-nearest-neighbour join queries by simply checking object pairs within each group FIM for data-intensive applications over computing clusters has received a growing attention efÞcient data partitioning strategies have been proposed to improve the performance of parallel FIM algorithms A MapReducebased Apriori algorithm is designed to incorporate a new dynamic partitioning and distributing data method to improve mining performance This method divides input data into relatively small splits to provide exibility for improved load-balance performance Moreover the master node doesnÕt distribute all the data once rather the rest data are distributed based on dynamically changing workload and computing capability weight of each node Similarly Jumbo adopted a dynamic partition assignment technology enabling each task to process more than one partition Thus these partitions can be dynamically reassigned to different tasks to improve the load balancing performance of Pfp Uthayopas et al  investigated I/O and execution scheduling strategies to balance data processing load thereby enhancing the utilization of a multi-core cluster system supporting association-rule mining In order to pick a winning strategy in terms of data-blocks assignment Uthayopas et al incorporated three basic placement policies namely the round robin range and random placement Their approach ignores data characteristics during the course of mining association rules 8F URTHER D ISCUSSIONS In this study we investigated the data partitioning issues in parallel FIM We focused on MapReduce-based parallel FPtree algorithms in particular we studied how to partition and distribute a large dataset across data nodes of a Hadoop cluster to reduce network and computing loads We argue that the general idea of FiDoop-DP proposed in this study can be extended to other FIM algorithms like Apriori running on Hadoop clusters Apriori-based parallel FIM algorithms can be classiÞed into two camps namely count distribution and data distribution  For the count distribution camp each node in a cluster calculates local support counts of all candidate itemsets Then the global support counts of the candidates are computed by exchanging the local support counts For the data distribution camp each node only keeps the support counts of a subset of all candidates Each node is responsible for delivering its local database partition to all the other processors to compute support counts In general the data distribution schemes have higher communication overhead than the count distribution ones whereas the data distribution schemes have lower synchronization overhead than its competitor Regardless of the count distribution or data distribution approaches the communication and synchronization cost induce adverse impacts on the performance of parallel mining algorithms The basic idea of Fidoop-DPÑgrouping highly relevant transactions into a partition allows the parallel algorithms to exploit correlations among transactions in database to cut communication and synchronization overhead among Hadoop nodes 9C ONCLUSIONS A ND F UTURE W ORK To mitigate high communication and reduce computing cost in MapReduce-based FIM algorithms we developed FiDoop-DP which exploits correlation among transactions to partition a large dataset across data nodes in a Hadoop cluster FiDoop-DP is able to 1 partition transactions with high similarity together and 2 group highly correlated frequent items into a list One of the salient features of FiDoopDP lies in its capability of lowering network trafÞc and computing load through reducing the number of redundant transactions which are transmitted among Hadoop nodes FiDoop-DP applies the Voronoi diagram-based data partitioning technique to accomplish data partition in which LSH is incorporated to offer an analysis of correlation among transactions At the heart of FiDoop-DP is the second MapReduce job which 1 partitions a large database to form a complete dataset for item groups and 2 conducts FP-Growth processing in parallel on local partitions to generate all frequent patterns Our experimental results reveal that FiDoop-DP signiÞcantly improves the FIM performance of the existing Pfp solution by up to 31 percent with an average of 18 percent We introduced in this study a similarity metric to facilitate data-aware partitioning As a future research direction we will apply this metric to investigate advanced loadbalancing strategies on a heterogeneous Hadoop cluster In one of our earlier studies see for details we addressed the data-placement issue in heterogeneous Hadoop clusters where data are placed across nodes in a way that each node has a balanced data processing load Our data placement scheme can balance the amount of data stored in heterogeneous nodes to achieve improved data-processing performance Such a scheme implemented at the level of Hadoop distributed le system HDFS is unaware of correlations among application data To further improve load balancing 112 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


mechanisms implemented in HDFS we plan to integrate FiDoop-DP with a data-placement mechanism in HDFS on heterogeneous clusters In addition to performance issues energy efÞciency of parallel FIM systems will be an intriguing research direction A CKNOWLEDGMENTS The work in this paper was in part supported by the National Natural Science Foundation of P.R China No.61272263 No.61572343 Xiao QinÕs work was supported by the U.S National Science Foundation under Grants CCF-0845257 CAREER The authors would also like to thank Mojen Lau for proof-reading R EFERENCES  M J Zaki Parallel and distribu ted associat ion mining A survey IEEE Concurrency  vol 7 no 4 pp 14Ð25 Oct 1999  I Pramudiono and M Kitsuregawa  Fp-tax Tree structure based generalized association rule mining in Proc 9th ACM SIGMOD Workshop Res Issues Data Mining Knowl Discovery  2004 pp 60Ð63  J De an a n d S Gh e ma wa t M ap re du ce  S i mp l i e d da ta pr o ce s si n g on large clusters ACM Commun  vol 51 no 1 pp 107Ð113 2008  S Sakr A Liu and A G Fayoumi The family of mapred uce and large-scale data processing systems ACM Comput Surveys  vol 46 no 1 p 11 2013  M.-Y Lin P.-Y Lee and S.-C Hsueh Apriori-based frequent itemset mining algorithms on mapreduce in Proc 6th Int Conf Ubiquitous Inform Manag Commun  2012 pp 76:1Ð76:8  X Li n  Mr a pr io ri  As so ci a ti o n ru le s a lg o ri th m ba se d on mapreduce in Proc IEEE 5th Int Conf Softw Eng Serv Sci  2014 pp 141Ð144  L Zhou Z Zhong J Chang J Li J Huang and S Feng Balanced parallel FP-growth with mapreduce in Proc IEEE Youth Conf Inform Comput Telecommun  2010 pp 243Ð246  S Hong Z Huaxuan C Shiping and H Chunyan The study of improved FP-growth algorithm in mapreduce in Proc 1st Int Workshop Cloud Comput Inform Security  2013 pp 250Ð253  M Riondato  J A DeBrabant R Fonseca and E Upfal Parma A parallel randomized algorithm for approximate association rules mining in mapreduce in Proc 21st ACM Int Conf Informa Knowl Manag  2012 pp 85Ð94  C Lam Hadoop in Action  Greenwich USA Manning Publications Co 2010  H Li Y Wang D Zhang M Zhang and E Y Chang PFP Parallel FP-growth for query recommendation in Proc ACM Conf Recommender Syst  2008 pp 107Ð114  C Curino E Jones Y Zhang and S Madden Schism A workload-driven approach to database replication and partitioning Proc VLDB Endowment  vol 3 no 1-2 pp 48Ð57 2010  P Uthayop as and N Benjamas Impact of i/o and execution scheduling strategies on large scale parallel data mining J Next Generation Inform Technol  vol 5 no 1 p 78 2014  I  P r a m u d i o n o a n d M  K i t s u r e g a w a  P a r a l l e l F P g r o w t h o n P C cluster in Proc.Adv.Knowl.DiscoveryDataMining  2003 pp 467Ð473  Y Xun J Zhang and X Qin Fidoop Parallel mining of frequent itemsets using mapreduce IEEE Trans Syst Man Cybern Syst  vol 46 no 3 pp 313Ð325 Mar 2016 doi 10.1109 TSMC.2015.2437327  S Owen R Anil T Dunning and E Friedman Mahout Action  Greenwich USA Manning 2011  D Borthakur  Hdfs architecture guide HADOOP APACHE PROJECT Available  http://hadoop.apache.org/common/docs current/hdfs design.pdf 2008  M Zaharia M Chowdhury M J Franklin  S Shenker and I Stoica Spark Cluster computing with working sets in Proc 2nd USENIX Conf Hot Topics Cloud Comput  2010 p 10  W Lu Y Shen S Chen and B C Ooi EfÞcient proces sing of k nearest neighbor joins using mapreduce Proc VLDB Endowment  vol 5 no 10 pp 1016Ð1027 2012  T Kanung o D M Mount N S Netanya hu C D Piatko R Silverman and A Y Wu An efÞcient k-means clustering algorithm Analysis and implementation IEEE Trans Pattern Anal Mach Intell  vol 24 no 7 pp 881Ð892 Jul 2002  A K Jain Data clustering 50 years beyond k-means Pattern Recog Lett  vol 31 no 8 pp 651Ð666 2010  D Arthur and S Vassilvitskii  k-means  The advantages of careful seeding in Proc 18th Annu ACM-SIAM Symp Discr Algorithms  2007 pp 1027Ð1035  J Leskovec A Rajaraman and J D Ullman Mining Massive Datasets  Cambridge U.K Cambridge Univ Press 2014  A Stupar  S Mich el and R Schen kel Rankred uceÐpr ocessin g k-nearest neighbor queries on top of mapreduce in Proc 8th Workshop Large-Scale Distrib Syst Informa Retrieval  2010 pp 13Ð18  B Bahmani A Goel and R Shinde EfÞcient distributed locality sensitive hashing in Proc 21st ACM Int Conf Inform Knowl Manag  2012 pp 2174Ð2178  R Panigrahy Entropy based nearest neighbor search in high dimensions in Proc 17th Annu ACM-SIAM Symp Discr Algorithm  2006 pp 1186Ð1195  A Z Broder M Charikar  A M Frieze and M Mitzenma cher Min-wise independent permutations J Comput Syst Sci  vol 60 no 3 pp 630Ð659 2000  L Cristofor ARtool Association rule mining algorit hms and tools 2006  S Agrawal V Narasayya  and B Yang Integrating vertical and horizontal partitioning into automated physical database design in Proc ACM SIGMOD Int Conf Manag Data  2004 pp 359Ð370  F Chang J Dean S Ghema wat W Hsieh D Wallach  M  Burrows T Chandra A Fikes and R Gruber Bigtable A distributed structured data storage system in Proc 7th Symp Operating Syst Des Implementation  2006 pp 305Ð314  B F Cooper R Ramakrishn an U Srivastava A Silberstein P Bohannon H.-A Jacobsen N Puz D Weaver and R Yerneni Pnuts Yahoo!Õs hosted data serving platform Proc VLDB Endowment  vol 1 no 2 pp 1277Ð1288 2008  J Xie and X Qin The 19th heterogenei ty in computing workshop HCW 2010 in Proc IEEE Int Symp Parallel Distrib Process Workshops Phd Forum  Apr 2010 pp 1Ð5  M Y Eltabakh Y Tian F  Ozcan R Gemulla A Krettek and J McPherson Cohadoop Flexible data placement and its exploitation in hadoop Proc VLDB Endowment  vol 4 no 9 pp 575 585 2011  R Vernica A Balmin K S Beyer and V Ercegovac Adaptive mapreduce using situation-aware mappers in Proc 15th Int Conf Extending Database Technol  2012 pp 420Ð431  Q Ke V Prabhakar an Y Xie Y Yu J Wu and J Yang Optimizing data partitioning for data-parallel computing uS Patent App 13/325,049 Dec 13 2011  M Liroz-Gis tau R Akbarinia D Agrawal E Pacitti  and P Valduriez Data partitioning for minimizing transferred data in mapreduce in Proc 6th Int Conf Data Manag Cloud Grid P2P Syst  2013 pp 1Ð12  T Kirsten L Kolb M Hartung A Gro H K  opcke and E Rahm Data partitioning for parallel entity matching Proc VLDB Endowment  vol 3 no 2 pp 1Ð8 2010  S Kotoulas E Oren and F Van Harmelen Mind the data skew Distributed inferencing by speeddating in elastic regions in Proc 19th Int Conf World Wide Web  2010 pp 531Ð540  L Li and M Zhang The strategy of mining associat ion rule based on cloud computing in Proc Int Conf Bus Comput Global Inform  2011 pp 475Ð478  S Groot K Goda and M Kitsuregawa  Towards improv ed load balancing for data intensive distributed computing in Proc ACM Symp Appl Comput  2011 pp 139Ð146  M Z Ashra D Taniar and K Smith ODAM An optimiz ed distributed association rule mining algorithm IEEE Distrib Syst Online  vol 5 no 3 p 1 Mar 2004 Yaling Xun is currently a doctoral student at Taiyuan University of Science and Technology She is currently a lecturer in the School of Computer Science and Technology Taiyuan University of Science and Technology Her research interests include data mining and parallel computing XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 113 


Jifu Zhang received the BS and MS degrees in computer science and technology from the Hefei University of Tchnology China and the PhD degree in pattern recognition and intelligence systems from the Beijing Institute of Technology in 1983 1989 and 2005 respectively He is currently a professor in the School of Computer Science and Technology TYUST His research interests include data mining parallel and distributed computing and artiÞcial intelligence Xiao Qin received the PhD degree in computer science from the University of Nebraska-Lincoln in 2004 He is currently a professor in the Department of Computer Science and Software Engineering Auburn University His research interests include parallel and distributed systems storage systems fault tolerance real-time systems and performance evaluation He received the U.S NSF Computing Processes and Artifacts Award and the NSF Computer System Research Award in 2007 and the NSF CAREER Award in 2009 He is a senior member of the IEEE Xujun Zhao received the MS degree in computer science and technology in 2005 from the Taiyuan University of Technology China He is currently working toward the PhD degree at Taiyuan University of Science and Technology His research interests include data mining and parallel computing  For more information on this or any other computing topic please visit our Digital Library at www.computer.org/publications/dlib 114 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


