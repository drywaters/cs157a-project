Theorem 1 Keywords- frequent itemset mining; Apirori algorithm; scalar product; set intersection The set is the be a transaction dataset on a set of items I that occur often enough as values of given by    3 rd Mediterrane an Conference on  Embedded Com puting  MECO 20 14  Budva M ontenegro A transaction data set on and transaction dataset is a finite set; we refer to the elements of items for  are the transaction identifiers \(TIDs Given a transaction dataset The numbers 1, \203  Note that   The following rather straightforward statement is fundamental for the study of frequent itemsets. It is known as Apriori principle If  260  and Definition 1 Definition 2 Definition 3 Definition 4 Let  Frequent itemset mining problem consists of finding the set is are two itemsets, then  implies   is the number   the c ollection of all and by  th transaction is the number  the collection of all frequent itemsets relative to the transaction dataset frequent relative to the transaction dataset    Upper Bounds on the Number of Candidate Itemsets in Apriori Like Algorithms on the set in is a function  for given minimal support The support count of a subset We denote by  I NTRODUCTION  We suggest a new model for frequent itemset mining which is based on linear algebra theory. In that way we give new mathematical foundation for frequent itemset mining task The idea comes from the definition of frequent itemset frequent itemset is set of items that appear in sufficiently high number of transactions in given database. In linear algebra terminology it means that sufficiently high number of transactions have to intersect on particular itemset in order to make it frequent In this paper we, in more detailed manner, developed the idea that had been initially presented in [1  In  1   w e  considered frequent itemsets, while in this paper we make upper bounds on the number of candidate itemsets in level wise mining approach. As in [1   w e ado pted  r e s u lt s fr om set intersection and scalar product theory to estimate maximal number of candidate itemsets that can be generated in levelwise mining approach. To illustrate the idea we use the modification of well known Apriori algorithm [2 fo r f r eque n t  itemset mining, called Apriori Multiple algorithm. Details of the Apriori Multiple algorithm are presented in [3 an d 4   Results from this paper can be effectively applied to all algorithms that use level-wise mining approach [2   II PRELIMINARIES This se ction contains usual mathematical foundation of frequent itemset mining. We primarily use notions from [5    Suppose that An itemset  The support of the itemset   Faculty of Mathematics and Natural Sciences University of Montenegro Podgorica, Montenegro savotom@rc.pmf.ac.me Predrag Stani\235i  Faculty of Mathematics and Natural Sciences University of Montenegro Podgorica, Montenegro pedjas@ac.me we would like to determine those subsets of frequent itemsets that contain be a transaction dataset on a set of items Frequent itemset mining has been a focused theme in data mining research for years. It was first proposed for market basket analysis in the form of association rule mining. Since the first proposal of this new data mining task and its associated efficient mining algorithms, there have been hundreds of followup research publications. In this paper we further develop the ideas presented in [1  In  1   w e co n sid er t w o  p r o b lem s f r o m  linear algebra, namely set intersection problem and scalar product problem and make comparisons to the frequent itemset mining task. In this paper we formulate and prove new theorems that estimate the number of candidate itemsets that can be generated in the level-wise mining approach Savo Tomovi 204  I I I k T n T I I T I K I T K K T T r T I K Abstract of the set as items Let  if  


is not known in advance, but we can use the following very simple approach to find it. In the first scan, Apriori Multiple generates frequent 1-itemsets. During this scan Apriori Multiple can determine the length of the longest transaction in the database  so the algorithm can set generates sets     su ch that at least  multiple_num 3 rd Mediterrane an Conference on  Embedded  Computing  MECO 20 14  Budva M ontenegro in Ariori like algorithms In [1 w e h a v e  pr o v e d th e f o llow i ng the o rem   W ith   multiple_num is collection of  The Apriori Multiple can use any value for  where maximum is taken on all In other words, the set  frequent itemsets. The value  Also, under the conditions    037 multiple_num define the set    multiple_num Apriori Multiple Algorithm Input: T - transactional database  minsup Output Method 1 frequent 1\205 itemsets in T 2 multiple_num =  AVG\(transaction length 3 FOR \(r=2  r+=multiple_num DO candidate_generation  FOR i = 1 TO multiple_num - 1 candidate_generation  END FOR FOR i = 0 TO multiple_num - 1 support_count  END FOR FOR i = 0 TO multiple_num - 1     END FOR END FOR 4    while set  If for some  we denote the maximal number of candidate parameter The support counting phase consists of calculating support for all previously generated candidates \(which are not pruned according to the Apriori principle in the preceding candidate generation phase\ In the support counting phase, it is essential to efficient determine if the candidates are contained in particular transaction  IV UPPER BOUNDS ON THE NUMBER OF CANDIDATE ITEMSETS IN APRIORI LIKE ALGORITHMS In this section we further develop the ideas from [1  Because of completeness, let us briefly introduce set intersection problem and make comparison between it and frequent itemset mining Let  be set of subsets of is in advance given integer. We choose  With  III L EVEL WISE FREQUENT ITEMSET MINING In this section we will briefly explain our Apriori Multiple algorithm for frequent itemsets mining from [3 I t i s  modification of well known Apriori algorithm from [2    Apriori Multiple implements level-wise approach in frequent itemset mining. The main characteristic of this approach is in the following: it generates frequent itemsets starting with frequent 1-itemsets itemsets consisted of just one item\; next, it iteratively generates frequent itemsets of size 2, 3, etc Iteration consists of two phases: candidate generation and support counting In the candidate generation phase potentially frequent itemsets or candidate itemsets are generated. The Apriori principle is used in this phase \(theorem 1\garding the original Apriori algorithm, in this phase, we have added new parameter named  parameter. If  parameter and change this value in the following iterations The         in order to increment their support Because of that, the candidates are organized in special structures like TS-Tree from [4, 6, and 7  The ca ndidates  which have enough support, are termed as frequent itemsets Pseudo-code for Apriori Multiple algorithm comes next we denote  parameter such that   items is taken from  holds 261 is generated, while our Apriori Multiple algorithm in the iteration parameter to average size of transactions. This does not guarantee that the algorithm finishes in two database scans, but it will generate less number of candidates than the original Apriori algorithm [3   Al s o  Apriori Multiple can start with some value for itemsets in that determines the \215length\216 of iteration. Actually, in the original Apriori algorithm, in the iteration multiple_num be finite set that contains and  parameter to r multiple_num = 0 multiple_num items set  holds, where  parameter can also be defined by user, just like  and define condition  where such that any two itemsets intersect on that satisfies previously defined conditions In frequent itemset terminology set  Let  threshold. It means that user, according to domain knowledge or some other estimate can specify the value for is the maximal size of that satisfies  is set of items our Apriori Multiple 215becomes\216 the original Apriori algorithm. If we want to ensure that Apriori Multiple finishes in just two database scans, we need to choose value for It is clear that   Another approach is to set      multiple_num itemsets from set of items  r n k M I k r r Theorem 2 can be considered as collection of candidate sets in the iteration different elements. Let   multiple_num 


It is also illustrated on figure 1 is actually set of all Consider  3 rd Mediterranea n Conference on  Embedd ed Computing  MECO 201 4  Budva M ontenegro        multiple_num and in that case we have that 1 we have    In the following paragraphs, we analyze conditions from the theorem 2 and formulate new theorems in order to make upper bounds on the number of candidate itemsets in Apriori Multiple algorithm or any algorithm that relays on level-wise mining approach Conditions from the previous theorem just ensure that sets   Proof. Notice that the set  If we allow different, i.e   2 Condition \(2\is not possible. We set   3 Recall that  4 From \(4\ we get      5 The previous is equivalent to the     6 Finally, we have     7 Condition \(7\ is satisfied for sufficiently large  262 such that any two of them intersect on  Theorem 3 Theorem 4 sets of candidate itemsets     itemsets Details can be found in [6 an d  7    and itemsets of  contains the set Consider of candidate  is true, definitely exists We can now formulate theorem that estimates number of candidate itemsets in Apriori Multiple algorithm from the previous section. Recall that in Apriori Multiple we have longer iterations, which means that in  are defined correctly. First, consider  of candidate 1itemsets, the second le vel contains the set with property that any two itemsets from from [8 h a v e   The previous condition says that we have to satisfy condition  as in our case, because are much less. It can be shown that integer th candidate iteration step in Apriori Multiple algorithm. Let  can be estimated with  for which   items. In other words itemsets in In order to prove our theorem we will apply results from theorem 2 In order to find that implies   we have     surely intersect on This condition ensures that   th step, where  is  elements which implies   The opposite is  In Apriori Multiple algorithm holds   The set   The theorem 3 can be reformulated in order to estimate upper bound on the number of candidate itemsets in any Apriori based algorithm that applies level-wise mining approach th candidate generation phase algorithm generates the following of candidate 2itemsets, the level Second, consider  and check  \(1         th candidate generation step in Apriori algorithm from [2  Nu m b e r  o f candida te i t e m sets in   Notice that all these candidate itemsets intersect on items Figure 1. TS-Tree Figure 1 illustrates the idea. For fast implementation of Apriori Multiple algorithm special tree structure is used. The tree is called TS-tree and it is based on Ryman set enumeration tree. Each level in the tree contains candidate itemsets; the 0th level contains just the root that represents empty set, the first level contains the set that satisfies \(1\ is unique [8  so we use   so we have         Notice that    itemsets from that is equivalent to  r r F  t t r w n n t w r is typically several thousand while itemsets corresponds to the set of Finally, consider  that can be easily transformed to  that is equivalent to  intersect on If we use estimation    


itemsets in the way that F REQUENT ITEMSET MINING TASK IN VECTOR THEORY  In [1  a s fut u re w o r k  we plan to d e f i ne frequ e n t i t emset  mining task with support of vector theory, i.e. using vector scalar product Scalar product is an algebraic operation that takes two equal-length vectors and returns a single number. This operation can be defined as the sum of the products of the corresponding entries of the two vectors of  numbers. Scalar product of two vectors   database of transactions on 3 rd Mediterrane an Conference on  Embedded  Computing  MECO 20 14  Budva M ontenegro represents candidate where    itemset  corr esponds to size of intersection of any two candidate Parallel to the problem from the previous section is obvious We can now define the candidate itemsets maximal number estimation problem using scalar product theory. Let  215Mining the Most k-Frequent Itemsets with TS-Tree\216, Proceedings of the IADIS International Conference WWW/Internet 2009, 2009 8 P. Stani\235i P. Stani\235i P. Stani\235i Let 200 200 Maximal number of candidate and there is no pair of orthonormal vectors \(vectors with scalar product 0 itemset because  in that contains vectors with property        corresponds to the canidate      and want to estimate maximal size of any set  th iteration. That common item can be any node from the first level in TS-tree as it is illustrated in figure 1 From [8 w e  w ill  j u s t ta ke t h e f o llow i ng  e s ti m a t i o n    is correctly defined and  do have at least one item in common is   and Theorem 5 set of items and  G. R. Agrawal, T. Imielinski, A. N. Swami, \215Mining association rules between sets of items in large databases\216, Proceedings of the ACM International Conference on Management of Data, pp. 207\205216, 1993 3 214A New Rymon Tree Based Procedure for Mining Statistically Significant Frequent Itemsets\215, International Journal of Computers, Communication & Control, Vol. 5\(4\, pp. 567-577, 2010 7 We fi x  be prime number  be natural numbers that satisfy and measures  215Apriori Multiple Algorithm for Mining Association Rules\216, Information Technology and Control, Vol. 37. No 4, ISSN 1392-124X, 2008 4 At the same time vector product of vectors from P. Stani\235i is defined as   8 In the following paragraphs we will make modification of the original scalar product problem from [8 i n  or de r t o  ma k e  it more familiar with frequent itemset mining task Let   Consider the following set of vectors      is  In the next paragraph we formulate the theorem that is modification of the corresponding theorem from [8   The  proof  is similar to the proof of the Theorem 2 in [1    Additionally, let   The previous theorem provides estimation of the maximal number of candidate itemsets that can be generated from one common item in  Notice that in theorem 5 we define the set  R EFERENCES  1 in order to achieve better estimation and define eventually dependency between S. Tomovi S. Tomovi S. Tomovi S. Tomovi S. Tomovi 2007 263   P. Stani\235i and y   be set of items. Any vector from or            215Frequent Itemset Mining as Set Intersection Problem\216, MECO 2013, Budva, Montenegro, 2013 2 215A New Data Structure for Frequent Itemset Mining: TS-Tree\216, Proceedings of 4 th South East European Doctoral Student Conference, 2009 5 itemset from make much better estimation in theorem 3 V VI CONCLUSION In this paper we present new mathematical model for frequent itemset mining problem. We use linear algebra method to estimate the size of candidate itemsets in Apriori based algorithms that implement level-wise approach in mining frequent itemsets Results from this paper can be effectively used in implementation of any level-wise algorithm because it gives upper bounds on the number of candidate itemsets, so we can know maximal memory requirements in advance As future work we plan to t r r p r in which there is no pair of vectors with scalar product D. A. Simovici, C. Djeraba, \215Mathematical Tools for Data Mining \205 Set Theory, Partial Orders, Combinatorics\216, Springer, 2008 6 Obviously, the set vector  th iteration of level-wise mining approach with property that any two candidate itemsets incorporate minimal support parameter if if and number of 


  Granular Computing \(GrC 2010 Silicon Valley, USA August 14-16 IEEE Computer Society Press pp 402408, 2010 9      W u   X    K u m a n   V    Q u in la n   J  R    G h o s h   J    Y a n g   Q   Motoda, H., Mclachlan, G.J., Ng A Liu, B., Yu, P.S Zhou Z Steinbach M.,Hand D.J  Steinberg D 2008 Top 10 algorithm in data mining Knowledge and Information Systems J Ramaan@1981  T able VI. Values of various parameters used in experiments Experiment No  Transactions  Items Correlation Level   Maximal potentially large itemsets Average size of Transactions Average size of maximal potentially large itemsets Value of minimum support  minsup  1 100 K 100 0.5 200 5 2 4% to 10% with a step increment of 1 2 100 K 100 0.5 200 10 2 10% to 20% with a step increment of 1 3 100 K 100 0.5 200 10 4 10% to 20% with a step increment of 1 4 100 K 100 0.5 200 20 4 15% to 30% with a step increment of 3 5 100 K 100 0.5 200 20 6 6%, 9%, 12 15%, 20%, and 25    27 


   Figure 1.1: Experiment 1-Pruning Vs Filtration approach  Figure 1.2: Experiment 2Pruning Vs Filtration approach Figure 1.3 Experiment 3Pruning Vs Filtration approach   001 002 001\001 003\001\001 001\001 005\001\001 001\001 001\001 b\001\001 005  b  Time in Seconds Support 001 002 001\001 003\001\001 001\001 005\001\001 001\001 002\001 002\002 002\003 002  002\005 002  002  002\b 001 005 001 001 002\003\001 002 001 003\001\001 003\005\001 003 001 003\001  001 002\001 002\002 002\003 002  002\005 002  002  002\b  Filtration approach   Pruning Vs Filtration approach   Pruning Vs Filtration approach   Figure 1.4: Experiment 4Pruning Vs Filtration approach  Figure 1.5 Experiment 5Pruning Vs Filtration approach 013 002  001 Pruning Filtration 002\b 002  002\013 003\001 Pruning Filtration 002\b 002  002\013 003\001 Pruning Filtration 001 003 001\001 005\001\001 001\001 001\001 002\001\001\001 002\003\001\001 002  002  003\002 Time in Seconds Support 001 001 001 002\001\001\001 002 001\001 003\001\001\001 003 001\001 001\001\001  001\001 005\001\001\001 005 001\001  013 002\003  Pruning Vs Filtration approach   Pruning Vs Filtration approach  003\005 003\b 001 Support Pruning Filtration 002  003\001 003  f 016\017\020\017\021 022\020\023\024 025\024\020\026\017 28 


VI Tanbeer, S. K., Ahmed, C. F., Jeong, B.-S., & Lee, Y.-K, \215Sliding window-based frequent pattern mining over data streams,\216 Information Sciences, 179\(22\, pp. 3843\2053865, 2009 9 Chang, J., & Lee, W. S, \215Finding recently frequent itemsets adaptively over online transactional data streams,\216 Information Systems, 31\(8\, pp. 849\205869, 2006 4 Agrawal, R., & Srikant, R, \215Fast algorithms for mining association rules,\216 In Proc. VLDB int. conf. very large databases \(pp. 487\205 499\, 1994 3 Tsai, P. S. M, \215Mining frequent itemsets in data streams using the weighted sliding window model,\216 Expert Systems with Applications, 36\(9\, pp. 11617\20511625, 2009  minimum change threshold Y. Chi, H. Wang, P. S. Yu and R. R. Muntz. Catch the moment maintaining closed frequent itemsets over a data stream sliding window. In KAIS, 10\(3\: pp. 265-294, 2006 6 V. kumar, S. satapathy, \215A review on algorithms for mining frequent itemsets over data stream,\216 in ijarcsse V3 I4, 2013 8 CONCLUSION AND FUTURE WORK  Considering the continuousness of a data stream, the traditional methods or techniques for finding frequent itemsets in conventional data mining methodology may not be valid in a data stream. This is because we cannot consider whole data and must identify when a data becomes obsolete or invalid As the old information of a data stream may be no longer useful or possibly invalid at present.  In order to support various requirements of mining data stream, the mining window or the interesting recent range of a data stream needs not to be defined static but must be flexible. Based on this range, a data mining method can be able to identify when a transactions becomes stale or needs to be disregarded  In this paper, we have investigated the problem of mining frequent itemset over data stream using flexible size sliding window model and proposed a new algorithm for this problem. The size of sliding window is adaptively adjusted based on the amount of observed concept change in the underlying properties of incoming data stream. The size of window enlarges or increase when there is no significant amount of change observed. While the window size reduced or decrease when there is considerable amount of concept change or significant change in set of frequent itemsets occurs Based on the value of given by user, the size of window is being controlled. After every pane insertion the set of frequent itemsets are updated and value of concept change is calculated. If the value exceeds the given minimum change threshold the window gets smaller by deleting all the obsolete information before a point defined called checkmark  Experimental results shows that our algorithm tracks the concept change efficiently while mining data stream and is more adaptive to recent frequent itemsets than fixed size sliding window models or time fading window models. For the future work, we are trying to enhance the performance by using fuzzy sets for minimum change threshold value so that the values like low, medium, high and very high instead of certain value between ranges of 0 to 1 R EFERENCES  1                    H. Li, S. Lee, and M. Shan, \215An Efficient Algorithm for Mining Frequent Itemsets over the Entire History of Data Streams\216, In Proc. of First International Workshop on Knowledge Discovery in Data Streams, 2004  F. Nori, M. Deypir, M. Sadreddini, \215A sliding window based algorithm for frequent closed itemset mining over data streams\216 journal of system and software, 2012  Zaki, M. \(2000\. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12\(3\, 372\205 390  Woo H. J., & Lee, W. S. \(2009\. estMax: Tracing maximal frequent item sets instantly over online transactional data streams IEEE Transactions on Knowledge and Data Engineering, 21\(10 1418\2051431  Mozafari B, Thakkar H, Zaniolo C, \215Verifying and mining patterns from large windows over data streams,\216 In Proc. Int. conf. ICDE pp. 179-188, 2008  Koh, J.- L., & Lin, C.- Y, \215Concept shift detection for frequent itemsets from sliding window over data streams\216, lecture notes in computer science: Database systems for advanced applications \(pp 334\205348\ DASFAA Int. Workshops, Springer-Verlag.2009  Han, J., Cheng, H., Xin, D., & Yan, X. Frequent pattern mining Current status and future directions. Data Mining and Knowledge Discovery, 15\(1\, pp.  55\20586, 2007 5 C. Giannella, J. Han, J. Pei, X. Yan, and P. S. Yu. Mining frequent patterns in data streams at multiple time granularities. In Kargupta et al.: Data Mining: Next Generation Challenges and Future Directions, MIT/AAAI Press, 2004 7 2014 IEEE International Advance Computing Conference IACC 510 J. H. Chang and W. S. Lee. estWin: Adaptively Monitoring the Recent Change of Frequent Itemsets over Online Data Streams. In Proc. of CIKM, 2003  J. Yu, Z. Chong, H. Lu, and A. Zhou. False Positive or False Negative: Mining Frequent Itemsets from High Speed Transactional Data Streams. In Proc. of VLDB, 2004      Aggarwal, C, \215A framework for diagnosing changes in evolving data streams,\216 In Proc. ACM SIGMOD int. conf. on management of data \(pp. 575\205586\ 2003 2 Manku, G. S., & Motwani, R. Approximate frequency counts over data streams. In Proc. VLDB int. conf. very large databases \(pp 346\205357\ 2002  


002 
                          
R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large databases. In Proc. VLDB, pages 487Ö499, 1994 2 R. J. Bayardo, Jr. Efficiently mining long patterns from databases SIGMOD Rec., pages 85Ö93, 1998 3 M. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. Parallel algorithms for discovery of association rules. Data Min. and Knowl. Disc., pages 343Ö373, 1997 4 J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. In Proc. OSDI. USENIX Association, 2004 5 Apache hadoop. http://hadoop.apache.org/, 2013 6 Jiawei Han and Micheline Kamber. Data Mining, Concepts and Techniques. Morgan Kaufmann, 2001 7 M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley M. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82, EECS Department University of California, Berkeley, Jul 2011 8 M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica Spark: Cluster Computing with Working Sets. In HotCloud, 2010 9 J. Han, J. Pei, and Y. Yin: Mining Frequent Patterns without Candidate Generation. In: Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, 29\(2\:1-12, 2000 10 M. J. Zaki. Parallel and distributed association mining: A survey IEEE Concurrency, pages 14Ö25, 1999 11 J. Li, Y. Liu, W.-k. Liao, and A. Choudhary. Parallel data mining algorithms for association rules and clustering. In Intl. Conf. on Management of Data, 2008 12 E. Ozkural, B. Ucar, and C. Aykanat. Parallel frequent item set mining with selective item replication. IEEE Trans. Parallel Distrib Syst., pages 1632Ö1640, 2011 13 B.-H. Park and H. Kargupta. Distributed data mining: Algorithms systems, and applications. 2002 14 L. Zeng, L. Li, L. Duan, K. Lu, Z. Shi, M. Wang, W. Wu, and P. Luo Distributed data mining: a survey. Information Technology and Management, pages 403Ö409, 2012 15 Li L. & Zhang M. \(2011\. The Strategy of Mining Association Rule Based on Cloud Computing. Proceeding of the 2011 International Conference on Business Computing and Global Informatization BCGIN è11\. Washington, DC, USA, IEEE: 475- 478 16 Li N., Zeng L., He Q. & Shi Z. \(2012\. Parallel Implementation of Apriori Algorithm Based on MapReduce. Proc. of the 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel & Distributed Computing SNPD è12\. Kyoto, IEEE: 236 Ö 241 17 Lin M., Lee P. & Hsueh S. \(2012\. Apriori-based Frequent Itemset Mining Algorithms on MapReduce. Proc. of the 16th International Conference on Ubiquitous Information Management and Communication \(ICUIMC è12\. New York, NY, USA, ACM: Article No. 76 18 Yang X.Y., Liu Z. & Fu Y. \(2010\. MapReduce as a Programming Model for Association Rules Algorithm on Hadoop. Proc. of the 3rd International Conference on Information Sciences and Interaction Sciences \(ICIS è10\. Chengdu, China, IEEE: 99 Ö 102 19 S. Hammoud. MapReduce Network Enabled Algorithms for Classification Based on Association Rules. Thesis, 2011 20 Synthetic Data Generation Code for Associations and Sequential Patterns. Intelligent Information Systems, IBM Almaden Research Center http://www.almaden.ibm.com/software/quest/Resources/index.shtml 21 C.L. Blake and C.J. Merz. UCI Repository of Machine Learning Databases. Dept. of Information and Computer Science, University of California at Irvine, CA, USA. 1998 http://www.ics.uci.edu/mlearn/MLRepository.html 22 HadoopApriori. https://github.com/solitaryreaper/HadoopApriori 2 3 H.V. Nguyen, E. Muller, K. Bohm. 4S: Scalable Subspace Search Schema Overcoming Traditional Apriori Processing. 2013 IEEE International Conference on Big Data. 2013 24 S. Moens, E. Aksehirli and Goethals. Frequent Itemset Mining for Big Data. University Antwerpen, Belgium. 2013 IEEE International Conference on Big Data. 2013 25 Y. Bu et al . HaLoop: E cient iterative data processing on large clusters. Proceedings of the VLDB Endowment, 3\(1-2\:285Ö296 2010 26 Frequent itemset mining dataset repository. http://fimi.us.ac.be/data 2004   
002 
Our experiments show that YAFIM is about 18 faster than Apriori algorithms implemented in MapReduce framework Furthermore, we can achieve a better performance in both sizeup and speedup for different datasets. In addition, we also evaluated YAFIM for medical application and revealed that YAFIM outperforms MRApriori about 25 speedup  A CKNOWLEDGMENT  This work is funded in part by China NSF Grants \(No 61223003\, and the USA Intel Labs University Research Program R EFERENCES  1 
002 
1671 


