An Efìcient Vertical-Apriori Mapreduce Algorithm for Frequent Item-set Mining Dawei Sun,Vincent CS Lee,Frada Burstein,Pari Delir Haghighi Faculty of Information Technology Monash University Melbourne Australia Email  dawei.sun,vincent.cs.lee,frada.burstein,pari.delir,haghighi  monash.edu Abstract Algorithms such as OPUS and Apriori-based Mapreduce for enhancing the efìciency of mining frequent itemset for pattern recognition application from transactional dataset have been proposed in the literature Most of these algorithms are however evaluated ofîine on relatively small data size When 
confronting with larger data size which is inevitable for todays organisation most if not all algorithms performed not as efìcient as required to meet the real time big data driven decision making needs We therefore attempt to solve these efìciency problems by proposing a VAMR Vertical-Apriori Map-reduce algorithm VAMR is based on data attribute identiìer which is exploited as capability metric for mining frequency item-set from large dataset in a single node for example in a single site enterprise that has no distributed and parallel computing system environment Our evaluations using synthetic datasets and data from public repository suggest that VAMR algorithm can offer superior efìciency in mining frequent item-sets from large transaction 
dataset Keywords  Frequent item-set mining Mapreduce Apriori big data attribute identiìer I I NTRODUCTION Recent advance in ICT and smart sensor technologies has enabled massive data being transmitted and received across commercial industrial and health care transaction processes Such massive amount of transactional data has embedded strategic and operational information that can be exploited by organisations management to make right time competitive advantage and sustainable growth decisions One of the key requirements for making right time decisions is the efìcient mining of the big data analytics Hadoop is the computing 
platform that enjoys a good reputation as big data solution in recent years since its core technique MapReduce which inspired by Google MapReduce Mapreduce framework provided a good way to accelerate the processing of large dataset via its simple map and reduce processes Current Mapreduce is used by Hadoop with its distributed database It divided a large le into several chunks which stored by different nodes When Hadoop is called to execute a job each node will run map and reduce process on each node individually then a centralized node will aggregate the results collected from nodes The distributed mechanism makes the big data to be handled within acceptable time Not only the distributed system assists Hadoop to handle the big 
data but also the map reduce mechanism contributes great proportion of efforts for handling big data with simple processes Some parallel frequent mining algorithms are proposed by researchers to handle the large datasets  Ho we v e r  by introducing parallel mining it would raise other problems as well such as job assignment and aggregation data distribution and storage node communication and parameter exchanging and etc Large amount of time is required to manage the job assignment resource usage and synchronization between nodes which could lead to extra time to mining task Apriori is the rst proposed frequent pattern mining algorithm for transactional datasets it starts from frequent 1itemset to n-itemset by joining itemsets gradually until no 
further is discovered from the frequent n-frequent itemset Many researchers proposed new algorithms to overcome the drawbacks of Apriori such as FP-Growth tree CHARM  and etc These algorithms were usually tested by selected datasets from some standard dataset repositories which contain limited number of records and limited number of attributes The performance of these algorithms to handle large-volume dataset is yet to be evaluated Most frequent itemset algorithms traverse the entire dataset in multiple passes especially the Apriori-like algorithms when new candidate n-itemset is generated it traverses the entire dataset to compute its frequency for generating frequent n-itemset and candidate n+1 If the dataset contains large number of records it takes long time to process the 
mining task Vertical frequent mining is different from traditional horizontal frequent pattern mining method It computes the intersections of Transaction IDs TID which is identiìer for each transaction to achieve same result with Apriori-like algorithms and this method only traverses the entire dataset once i.e one-pass Hence lesser mining time is required Vertical frequent mining is different from traditional horizontal frequent pattern mining method It computes the intersections of Transaction IDs TID which is identiìer for each transaction to achieve same result with Apriori-like algorithms and this method only traverses the entire dataset once i.e onepass Hence lesser mining time is required By combining vertical frequent mining method Mapreduce 
mechanism and Apriori we propose a new algorithm known as VAMR Vertical Apriori Mapreduce that is capable of mining large transactional dataset in an efìcient manner within a single node VAMR algorithm will be tested using self-generated synthetic datasets as well as public dataset We run experiments to support the efìcient mining performance of VAMR 108 978-1-4799-8389-6/15/$31.00 c  2015 IEEE 


algorithm with comparison to OPUS Miner and Apriori Mapreduce separately  Another spin-of f contrib ution of our research is that our data generator also provides a new way to test the frequent pattern mining algorithm since it allows us to generate different sizes of transactional datasets with the same standard format of Frequent Itemset Mining Dataset Repository II PRELIMINARY A Apriori The term Apriori means that all nonempty subsets of a frequent itemset must also be frequent More speciìcally if an itemset I fails to satisfy the minimum support threshold then I is considered infrequent If an item E is added to the itemset I  then the new itemset I  cannot occur more frequently than I  Hence new itemset I is not frequent either I  minimum support  By applying this theory the Apriori algorithm scans the entire dataset to nd the frequent 1-itemsets where the 1-itemsets are generated by counting the item occurrences Then the frequent 1-itemsets are used to form the candidate itemset in order to generate the frequent 2-itemsets where the frequent 2-itemset is generated by joining the frequent n-1 with frequent n-1 in this case n-1 is 1-itemset After the join step the prune step is started and those itemset containing any subset that does not appear in frequent k-1 will be removed  The entire procedure will continue to search recursively until no more frequent nitemsets are produced  TID Itemset 1 Milk,bread 2 Egg,milk,sugar 3 Bread,egg 4 Milk,egg,îour 5 Milk,îour,bread,egg 6 Egg,îour 7 Milk 8 Milk,bread TABLE I S AMPLE T RANSACTION Let us use the above dataset in I as an example After the rst scan is nished we have the following\(in II 1-itemset and 2-itemset with support count Itemset Count Itemset Count Milk 6 Milk egg 3 Egg 5 Milk bread 3 Bread 4 Egg,bread 2 Flour 3 Sugar 1 TABLE II F REQUENT 1ITEMSET AND 2ITEMSET Once the 1-itemsets are found we test whether all the itemsets satisfy the minimum support Assume the minimum support of 50 consequently Sugar Flour will be removed because they are lower than minimum threshold Then the remaining itemsets are considered candidate itemsets for generating 2-itemsets The next step is to generate the 2-itemset by joining the candidate itemsets by itself We will get TableII itemsets and their support will be calculated as well which demonstrated in the TableII third and fourth column Comparing the support of each 2-itemset with the minimum threshold all 2-itemset fail to satisfy the condition so there is no 3-itemset candidate the program is then terminated Consequently the nal result only contains frequent 1-itemset B Vertical Frequent Mining Vertical frequent itemset discovery is different from traditional Apriori frequent pattern mining it was rst used in  17 It computes the T ransaction Identiìers to disco v e r frequent patterns We use transactions in TableI as example Item TID Milk 1,2,4,5,7,8 Egg 2,3,4,5,6 Bread 1,3,5,8 Flour 4,5,6 Sugar 2 TABLE III S AMPLE TID SET As shown in III if we want to nd the frequency of itemset milk bread which can be computed by intersecting TID set of milk and TID set of bread which is 1,5,8 C Mapreduce Mapreduce is the core technique of Hadoop it derived from Google Mapreduce This framework is designated for distributed system In Hadoop HDFS\(Hadoop distributed le system is designed to work with mapreduce Each le in HDFS will be separated into couple of chunks storing in different nodes There is a node named namenode\(in Hadoop which is responsible to manage how many copies of each les and maintain the information of which node holds a part of a le In a distributed system there is master node designated for job scheduling and result collection and aggregation Other nodes are named slave node which are designated for job execution Mapreduce works under this environment When a job is initialized each node doing map process which outputs the key-pairs K value-list then the master will assign the key-pairs with same key value to speciìc nodes for reduce process Reduce operation is trying to aggregate the result collected from different nodes into one in format of keypairs°Key,Value ultimately all results are saved into a single le III THE PROPOSED VAMR ALGORITHM Vertical-Apriori Mapreudce takes advantages from Apriori Vertical frequent mining and Mapreduce to minimize their drawbacks in order to efìciently discover frequent item-sets from large datasets In Apriori for each frequent k-itemset it will scan the entire dataset for computing k-itemset frequency and its support It would consume large amount of time for scanning Although distributed system provide an efìcient way to process large dataset in parallel the master node consumes time on management and coordination jobs involving job scheduling parameter exchanging result aggregation and etc Vertical-Apriori mapreduce employs the vertical frequent pattern mining to avoid the multiple scanning of entire dataset 2015 IEEE 10th Conference on Industrial Electronics and Applications ICIEA 109 


Fig 1 Process of VARM It also applies Mapreduce program paradigm on a single node in order to process large dataset in an efìcient way Apriori cooperates with vertical frequent pattern to maintain the frequent 1-itemset TID set for minimizing memory usage The above Figure 1 illustrated the process of Vertical-Apriori Mapreduce Data  DataSet D Map\(Key,Value-list foreach Record r i in D do  foreach Item I in r i do output\(I,1 end end Algorithm 1 Initial Map Data  Key-pairs KP\(Key,Value-list Reduce\(Key,Value foreach Key k in KP do foreach value in ks value list do k.value  kês size of value list if k.value  minimum support then output\(k,k.value end end end Algorithm 2 Initial Reduce As shown in Figure 1 it scan the entire dataset at the initial stage for discovering frequent 1-itemsets and their TID sets In the following phases it keep discovering frequent k+1 itemset till there is no candidate itesmet  Bread Egg    Bread Milk   and  Egg Milk   Once the k+1 candidate is ready Map process will compute its TID set and then the Reduce process reports the frequent itemset and its frequency By doing this process recursively to discover frequent itemsets until there is no candidate generated the algorithms is terminating Algorithm 1 and 2 showed the fundamental of the Mapreduce algorithm for initial scan Data  Candidate Itemsets C Map\(Key,Value-list foreach Candidate c in C do foreach Item i in c do c.value-list  intersection\(i.TIDSet end end Algorithm 3 Main Map Process Data  Key-pairs KP\(Key,Value-list Reduce\(Key,Value foreach Key k in KP do foreach value in ks value list do k.value  kês size of value list if k.value-list.size  minimum support then output\(k,k.value end end end Algorithm 4 Main Reduce Process Data  Key-pairs KP\(Key,Value-list NextCandidate\(new keys foreach Key k in KP do foreach k  i 1 in KP do new keys join k i  k  i 1  end end Algorithm 5 NextCandidate Process Data  Key-pairs KP\(Key,Value-list Main Inital Scan Candidate  NextCandidate\(Reduce\(Map\(Dataset D  while candidate.isEmpty do Candidate  NextCandidate\(Reduce\(Map\(Dataset D  end Output le Algorithm 3 4 5 and 6 demonstrated the entire VerticalApriori Mapreduce algorithm Use above TableI as an example our minimum support is 50 so after initial scan the frequent 1-itemset and its TID would be  Milk:1,2,4,5,7,8    Egg:2,3,4,5,6   and  Bread:1,3,5,8   Then the nextCandidate will take these keys as input to generate next candidate IV EXPERIMENTS AND EVALUATIONS In our experiments this Vertical-Apriori Mapreduce VAMR algorithm is implemented in Java and all experiments are running on the Windows platform with core i5-2520M and 4GB RAM 110 2015 IEEE 10th Conference on Industrial Electronics and Applications ICIEA 


Datasets used for experiments including datasets from public repository frequent itemset dataset repository and self-generated datasets with different number of records and different number of attributes A Data Generator This data generator has three phases generation adjustment and writing into le In the generation phase it requires the users to input the number of attributes and the number of records they want The number of variables is the maximum number of attributes each record can have For instance we specify the number of variables as 25 so for each single record it might have 1,2,3,4 till 25 as a record Each attribute is determined by a random Boolean value which means there is 50 probability that an attribute can be generated Otherwise it skips this number n to n+1 which is the next attribute For instance assume that now n is 9 If the Boolean value is true then the system will output the number 9 Otherwise this number is skipped to 10 Once the generation is nished then next stage is adjustment phase In the adjustment stage the dataset will be adjusted based on the predeìned rules We have following four cases Case 1 both antecedence\(s and consequence\(s are existed There is 75 probability that keeps the consequence\(s and 25 probability that removes the consequence\(s E.g record  1,3,4,5   pre-deìned rule 3  4 Consequence 3 and antecedence 4 are both existed 25 probability consequence 3 will be removed Case 2 Antecedence\(s not exist\(s and consequence exist\(s There is 75 probability that removes the consequence\(s and 25 probability that remains the consequence\(s E.g record  1 4   predeìned rule 4  3  75 probability that consequence 4 will be removed Case 3 Antecedence\(s exist\(s and consequence not exit\(s There is 75 probability that adds the consequence\(s into record 25 probability that remains the same E.g record  1,4   predeìned rule 3  4  there is 75 probability that 3 will be added into record  1,4  Case 4 Antecedence\(s and consequence\(s neither does not exist there is 75 probability that remains the same 25 probability that adds the consequence into record E.g record  1   predeìned rule 3  4  there is 25 3 will be added into record  1   B Experiments In experiment 1  we generated 28 datasets in terms of different number of attributes including 10-attribtue dataset 15-attribute dataset 20-attribute dataset 25-attribute dataset and 30-attribute dataset and different number of records including 12500-record dataset 50000-record 100000-record dataset 200000-record 250000-record dataset and 300000record dataset All datasets will run through Vertical-Apriori Mapreduce and OPUS Miner for comparison in terms of the running time in seconds OPUS is a branch and space search algorithm especially for unsorted spaces Selfsufìcient itemset uses OPUS Algorithm to search itemset in a given dataset In this experiment we turn off the lter which will screen the self-sufìcient itemset as we only wish to compare the discovery speed The result is demonstrated in the following Table 4 and Figures 2 and 3 Fig 2 Result of OPUS Fig 3 Result of VAMR It is obvious to see that OPUS miner takes longer time to process those datasets With the increase of number of records and number of attributes the running time rised up as well Especially with increase of the number of attributes the computation time of both algorithms is shooting up exponentially In terms of number of records with two times number of records the running time is doubled approximately Figures 4 and Figure 5 illustrated the running time of both algorithm separately In Experiment 2  we compared the Vertical-Apriori Mapreduce with Apriori Mapreduce in a distrib uted system using dataset from Frequent Itemset dataset repository T10I1D100K The results are as sho wn in the Figures 4 and 5 Comparing with results our results are much more efìcient With the increased minimum support the performance on this dataset is rather stable ranging from 0.53 seconds to 0.56 seconds Apriori Mapreduce takes longer time to process this dataset its run time decreased as when minimum support increased C Limitation and Future Work As we can seen from Table 5 both OPUS Miner and Vertical-Apriori Mapreduce consumes signiìcant memory to process datasets From 25-attribute with 50,000 records OPUS miner fails to complete the task while Vertical-Apriori Mapreduce can handle 30-Attributes dataset but failed to process more than 50,000 records This Out of memory situation was caused by following factors i Although the platform equipped with 4GB RAM real RAM usage is lower than 4GB due to 2015 IEEE 10th Conference on Industrial Electronics and Applications ICIEA 111 


 10-Attribute 10-Attribute 10-Attribute 10-Attribute 10-Attribute VARM OPUS VARM OPUS VARM OPUS VARM OPUS VARM OPUS 12500 0.01 0.03 0.22 1.4 0.74 10.4 2.79 78.5 9.79 OM 50000 0.19 1 0.33 2.1 1.2 23.8 22.01 OM 76.23 OM 100000 0.27 1.1 1.05 5.9 4.86 94.2 44.7 OM OM OM 200000 0.48 1 2.05 11.1 31.29 179.1 OM OM OM OM 250000 0.64 1.6 2.43 13.3 36.89 221.3 OM OM OM OM 300000 0.74 2 2.55 15.5 51.6 263.9 OM OM OM OM TABLE IV R ESULT OF OPUS AND VAMR\(OM:O UT OF M EMORY  Fig 4 Result of VAMR T10I4100K Fig 5 Result of Apriori Mapreduce of T10I4100K the operating system allocation ii Language used including the application of some policies to suppress the usage of memory for instance Java has mechanism to control the usage of memory Memory consumption is the biggest challenge for discovering frequent itemset on a single node since it requires to maintain a tree or a table of frequent patterns for further computation Even with simple and well-designed data structure increase in volume of dataset leads to signiìcant increase on memory usage Our future work underway includes the design of technique to reduce memory usage for frequent pattern mining V C ONCLUSION We have proposed Vertical-Apriori Mapreduce algorithm and evaluated its performance using public repository dataset and self-generated datasets Our evaluation results demonstrated VAMR algoirhtm is capable to process large dataset in an efìcient manner compared with OPUS Miner and Apriori Mapreduce  our V ertical-Apriori Mapreduce is more efìcient This proposed algorithm adopted the advantages from Apriori Vertical frequent pattern mining and Mapreduce to minimize the demerits enables a single node to process large dataset in an efìcient manner However it still consumes signiìcant memory to process large dataset This proposed algorithm adopted the advantages from Apriori Vertical frequent pattern mining and Mapreduce to minimize the demerits enables a single node to process large dataset in an efìcient manner However it still consumes signiìcant memory to process large dataset R EFERENCES  H Inc Hadoop  Inc Hadoop  J Dean and S Ghema w at Mapreduce a  e xible data processing tool  Communications of the ACM  vol 53 no 1 pp 72Ö77 2010  S Cong J Han J Hoeîinger  and D P adua  A sampling-based framework for parallel data mining in Proceedings of the tenth ACM SIGPLAN symposium on Principles and practice of parallel programming  ACM 2005 pp 255Ö265  A Mueller   F ast sequential and parallel algorithms for association rule mining A comparison 1998  T  Shintani and M Kitsure ga w a  Hash based parallel algorithms for mining association rules in Parallel and Distributed Information Systems 1996 Fourth International Conference on  IEEE 1996 pp 19Ö30  K.-M Y u  J  Zhou T P  Hong and J.-L Zhou  A load-balanced distributed parallel mining algorithm Expert Systems with Applications  vol 37 no 3 pp 2459Ö2464 2010  M J Zaki P arallel and distrib uted association mining A surv e y   IEEE concurrency  vol 7 no 4 pp 14Ö25 1999  D Jiang B C Ooi L Shi and S W u  The performance of mapreduce an in-depth study Proceedings of the VLDB Endowment  vol 3 no 1-2 pp 472Ö483 2010  R McCreadie C Macdonald and I Ounis Mapreduce inde xing strategies Studying scalability and efìciency Information Processing  Management  vol 48 no 5 pp 873Ö888 2012  M Stonebrak er  D  Abadi D J DeW itt S Madden E P aulson A Pavlo and A Rasin Mapreduce and parallel dbmss friends or foes Communications of the ACM  vol 53 no 1 pp 64Ö71 2010  R Agra w al H Mannila R Srikant H T o i v onen A I V erkamo et al  Fast discovery of association rules Advances in knowledge discovery and data mining  vol 12 no 1 pp 307Ö328 1996  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in ACM SIGMOD Record  vol 29 no 2 ACM 2000 pp 1Ö12  M J Zaki and C.-J Hsiao Charm An ef cient algorithm for closed itemset mining in SDM  vol 2 SIAM 2002 pp 457Ö473  G I W ebb Self-suf cient itemsets An approach to screening potentially interesting associations between items ACM Transactions on Knowledge Discovery from Data TKDD  vol 4 no 1 p 3 2010  M.-Y  Lin P Y  Lee and S.-C Hsueh  Apriori-based frequent itemset mining algorithms on mapreduce in Proceedings of the 6th international conference on ubiquitous information management and communication  ACM 2012 p 76 16 Frequent Itemset Dataset Repository  2013  N P asquier  Y  Bastide R T aouil and L Lakhal Ef cient mining of association rules using closed itemset lattices Information systems  vol 24 no 1 pp 25Ö46 1999  C Lam Hadoop in action  Manning Publications Co 2010  G W ebb OPUS Miner  Melbourne 2013  G I W ebb Opus An ef cient admissible algorithm for unordered search Journal of Artiìcial Intelligence Research  pp 431Ö465 1995 112 2015 IEEE 10th Conference on Industrial Electronics and Applications ICIEA 


              012      015       015                                                                                      0   1    2         3                                4                  5                                                                         5                   6 5               3             7     012      015       015                          7  7  8 9        9    A   B C   C        9  D  E  F  C 9 F          G                                    H                                       3  0                       4                  5                     I                J   H  I    J                   K                       I                              5                   6 5               3       7  L  8 9        9    A   B C   C       C  D D  E  F  C 9 F           L    012      015         M       015     N O P Q R S                                         T                               U                                V             W X Y Z      Z  0                                        J   H  I    J                         I     5                                   3                012      015         M       015      _ _                                 5                           T                                          a                          b                                 c       5             b                                                   3  6                d           5                                       I                 I U  J        e f g   T                   U                                             3  h  U    4        5                  T               I       e   g 3   i j i k i l i m i n i i n j i o pj pp q j ni r q s t u v w x y z    y u     u v z    y  y w   w   z  x      n i  k  l i i                                                     i j p p i o p n i i n j p n p i n o p j i i j j p j p i n ipqjni r p i r q s t u v w x y z    y u     u v z    y  y w   w   z  x                                                                   i i r p n n r p j o pj pp q j ni r q s t u v w x y z    y u     u v z    y  y w   w   z  x                                                              i j i k i l i m i n i i n ik q j n s t u v w x y z    y u    u v z    y  y w   w   z  x                y x u                                                                                                                                                                          


          012    012   015                           015       015                        015            015  015                    015       015                   015      015            015       015                       015         015    015         015          015          015                                    015       015                    015              015  015             015                        015                                                                 015         015                         015                  015            015                 015                    012  0 1 2 3 4  5 6 7 8 9 8     4       A B C   8 D E  5 4  F 9 8    G H D  D 6            015            015            015                 I  J K L M N O LP Q  R S T U V W X  015    Y      Y Y   1 Z Z     0 Y 2      _   I                               015   015  015   I a b U c d S W e S f b W Q f g f h b e i b S S V i b e X N h h h  j V f b k f T g i U b k  X       l Y   Z m   Y m m m    0  2    n  o   015  n  p    I  015 q               015                015  015             015  r       015  p                I    s    015    015     015   t  015     015    015                       015              015       1 l Z  1 u s   1 Z Z Z   0 v 2   015         015     w  015   I         x  y   015         015    015  015         015                  I a b U c d S W e S z  f k S W  M  k g S  k  X       Y m   015    v        Y Z       Y m m l   0  2  4   8 D    4        4    D   3 4  H8     H D  D 6   7     D    8    7 D B            015          015       015 y          015         015       8   7  8       8  8  H D  D 6  8 D E   D  9   E 6     B     7       4  u   Y m m v          u l   0 s 2 t            I t        015            o         I  0  015   015  2            y      y               015                  0 l 2 x  015    015   n  015    015    015      015     _     I           015        015         015  015   015                  015    015     I          015                  015    Y m 1         s s         1      Z v  1 1 l  0 u 2       r   q   r     _              015        t        015          015      015         015              Y m m Z   0 Z 2 I          015  015   o        I  0  015   015  2            y      y            015               015          0 1 m 2  t        015           I x      015            015  015   I  N R h k  Q f g f Li b i b e a b U c d Q i k T U  Y m 1 Y  Y y v  l  v  s    y  1 m  1 m m Y      1 m l v                                                                                                                    


0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.48 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0   Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time 
0.1 1 10 100 3000 2000 1000 0 1000 2000 3000 4000 5000 
0.6 0.5 0.4 0.3 0.2 0.1 0.0 10 100 
1 10 100 6000 4000 2000 0 2000 4000 6000 8000 10000 
0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07 0.06 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  0 10000 5000 15000 c Webdocs Figure 5 The performance results of Apriori-AP on three real-world benchmarks DP time SR time and CPU time represent the data process time on AP symbol replacement time on AP and CPU time respectively Webdocs switches to 16-bit encoding when relative minimum support is less then 0.1 8-bit encoding is applied in other cases 
E vs Eclat Eclat et al Eclat Eclat 
0.80 0.75 0.70 0.65 0.60 0.55 0 100 200 300 400 Computation Time \(s Relative minimum support 1.0X Symbol replacement time 0.5X Symbol replacement time 0.1X Symbol replacement time Figure 7 The impact of symbol replacement time on Apriori-AP performance for Pumsb how symbol replacement time affects the total AprioriAP computation time A reduction of 90 in the symbol replacement time leads to 2.3X-3.4X speedups of the total computation time The reduction of symbol replacement latency will not affect the performance behavior of AprioriAP for large datasets since data processing dominates the total computation time 
Apriori Eclat Equivalent Class Clustering   is another algorithm based on Downward-closure uses a vertical representation of transactions and depth-ìrst-search strategy to minimize memory usage Zhang  proposed a h ybrid depth-ìrst/breadth-ìrst search scheme to expose more parallelism for both multi-thread and GPU versions of  However the trade-off between parallelism and memory usage still exists For large datasets the nite memory main or GPU global memory will become a limiting factor for performance and for very large datasets the algorithm fails While there is a parameter which can tune the trade-off between parallelism and memory occupancy we simply use the default setting of this parameter for better performance Figure 8 shows the speedups that the sequential 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 AP counting speedup Apriori-AP overall speedup    Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  b Accidents 
T40D500K Counting T40D500K Overall T100D20M Counting T100D20M Overall Webdocs5X Counting Webdocs5X Overall Speedup Relative Minimum Support Figure 6 The speedup of Apriori-AP over Apriori-CPU on three synthetic benchmarks 
1 10 100 
a Pumsb 
696 


0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 10 Pumsb Accidents T40D500K Webdocs Webdocs5X T100D20M ENWiki Speedup Relative Minimum Support 
Figure 8 The performance comparison of CPU sequential and algorithm achieved with respect to sequential Apriori-CPU Though has 8X performance advantage in average cases the vertical bitset representation become less efìcient for sparse and large dataset high trans and freq item ratio This situation becomes worse as the support number decreases The Apriori-CPU implementation usually achieves worse performance than  though the performance boost of counting operation makes Apriori-AP a competitive solution to parallelized  Three factors make a poor t for the AP though it has better performance on CPU 1 requires bit-level operations but the AP works on byte-level symbols 2 generates new vertical representations of transactions for each new itemset candidate while dynamically changing the values in the input stream is not efìcient using the AP 3 Even the hybrid search strategy cannot expose enough parallelism to make full use of the AP chips Figure 9 and 10 show the performance comparison between Apriori-AP 45nm for current generation of AP and sequential multi-core and GPU versions of  Generally Apriori-AP shows better performance than sequential and multi-core versions of  The GPU version of shows better performance in Pumsb Accidents and Webdocs when the minimum support number is small However because of the constraint of GPU global memory Eclat-1G fails at small support numbers for three large datasets ENWiki T100D20M and Webdocs5X ENWiki as a typical sparse dataset causes inefìcient storage of bitset representation in Eclat and leads to early failure of EclatGPU and up to 49X speedup of Apriori-AP over Eclat-6C In other benchmarks Apriori-AP shows up to 7.5X speedup over Eclat-6C and 3.6X speedup over Eclat-1G This gure also indicates that the performance advantage of AprioriAP over GPU/multi-core increases as the size of the dataset grows The AP D480 chip is based on 45nm technology while the Intel CPU Xeon E5-1650 and Nvidia Kepler K20C on which we test are based on 32nm and 28nm technologies respectively To compare the different architectures in the same semiconductor technology mode we show the performance of technology projections on 32nm and 28nm technologies in Figure 9 and 10 assuming linear scaling for clock frequency and square scaling for capacity The technology normalized performance of Apriori-AP shows better performance than multi-core and GPU versions of Eclat in almost all of the ranges of support that we investigated for all datasets with the exception of small support for Pumsb and T100D20M Apriori-AP achieves up to 112X speedup over Eclat-6C and 6.3X speedup over Eclat-1G The above results indicate that the size of the dataset could be a limiting factor for the parallel algorithms By varying the number of transactions but keeping other parameters xed we studied the behavior of Apriori-AP and Eclat as the size of the dataset increases Figure 11 For T100 the datasets with different sizes are obtained by the IBM synthetic data generator For Webdocs the different data sizes are obtained by randomly sampling the transactions or by concatenating duplicates of the whole dataset In the tested cases the GPU version of fails in the range from 2GB to 4GB because of the nite GPU global memory Comparing the results using different support numbers on the same dataset it is apparent that the smaller support number causes Eclat-1G to fail at a smaller dataset This failure is caused by the fact that the ARM with a smaller support will keep more items and transactions in the data preprocessing stage While not shown in this gure it is reasonable to predict that the multicore implementation would fail when the available physical memory is exhausted However Apriori-AP will still work well on much larger datasets assuming the data is streamed in from the hard drive assuming the hard drive bandwidth is not a bottleneck VII C ONCLUSIONS AND THE F UTURE W ORK We present a hardware-accelerated ARM solution using Micronês new AP architecture Our proposed solution includes a novel automaton design for matching and counting frequent itemsets for ARM The multiple-entry NFA based design was proposed to handle variable-size itemsets MENFA-VSI and avoid routing reconìguration The whole design makes full usage of the massive parallelism of the AP and can match and count up to itemsets in parallel on an AP D480 48-core board When compared with the based single-core CPU implementation the proposed solution shows up to 129X speedup in our experimental 
Apriori Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat F Normalizing for technology Eclat G Data size Eclat Eclat Eclat Apriori 
18 432 
 
697 


0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 1 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm a Webdocs\(1.4GB 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails d Webdocs5X 7.1GB Figure 10 Performance comparison among Apriori-AP Eclat-1C Eclat-6C and Eclat-1G with technology normalization on four large datasets 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm b Accidents 34MB 0.40 0.36 0.32 0.28 0.24 0.20 0.16 0.12 0.08 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm c T40D500K\(49MB Figure 9 Performance comparison among Apriori-AP Eclat1C Eclat-6C and Eclat-1G with technology normalization on three small datasets results on seven real-world and synthetic datasets This APaccelerated solution also outperforms the multicore-based and GPU-based implementations of 0.60 0.55 0.50 0.45 0.40 0.35 0.30 0.25 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails c T100D20M 6.3GB 
ARM a more efìcient algorithm with up to 49X speedups especially on large datasets When performing technology projections on future generations of the AP our results suggest even better speedups relative to the equivalent-generation of CPUs and GPUs Furthermore by varying the size of the datasets from small to very large our results demonstrate the memory constraint of parallel ARM particularly for GPU 
Eclat Eclat 
0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm 
0.20 0.15 0.10 0.05 0.00 1 10 100 1000 10000 100000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails b ENWiki\(3.0GB 
a Pumsb 16MB 
698 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


