Association Rule Mining with Modified Apriori Algorithm using Top down Approach  Ashish Shah Dept. of Computer Science and Engineering Manipal Institute of technology, Manipal University, Manipal, India ashish.shah1512@gmail.com   Abstract Data Mining is a field of computer science that is concerned with extracting useful information from varied sources. In an era where information has become the inherent necessity of human beings, its increased relevance and usefulness has taken focus as need of the hour. The most important part of this association rule mining is the mining of item sets that are frequent. Market basket analysis is done by companies in order to retrieve itemsets that are frequent and often used together by customers. Apriori algorithm is a widely used technique in order to find those combinations of itemsets. However, when any of these frequent itemsets increases in length, the algorithm needs to pass through many iterations and, as a result, the performance drastically decreases. In this paper, we propose a modification to the apriori algorithm by using a hash function which divides the frequent item sets into buckets. Further, we propose a novel technique to be used in conjunction with the apriori algorithm by eliminating infrequent itemsets from the candidate set. In this top down approach, it finds the frequent itemsets without going through several iterations, thus saving time and space. By discovering a large maximal frequent itemset very early in the algorithm, all its subsets are also frequent hence we no longer need to scan them. Clearly, the proposed technique has an advantage over the existing apriori algorithm when the most frequent itemsetês length is long  KeywordsÑData mining, Aprori algorithm, Hash function Association rule mining, Maximal frequent itemset I  I NTRODUCTION  The most widely used application of association rule mining is market basket analysis. The buying habits of all customers are analysed by finding correlations and associations between all the various items that have been bought by the customer. The knowledge and discovery of these correlations will help retailers and shopkeepers come up with marketing strategies. They will gain an insight into the items that are frequently bought together by a customer. For eg, if a customer buys milk, it will tell the retailer what is the tendency of the customer to buy bread as well. And what kind of bread also. In one trip to the market, what is the probability that the customer will buy both? Or other combinations or groups of items will the customer buy on a trip to the store. This information is very useful and can help the shop achieve greater sales by helping retailers market certain products and also organize their shelf space. They may keep certain products together. This market analysis is also performed on the sales data of the customers buying things at the shop. The results can help change advertisement strategies or in designing a new catalogue for the store. It can help you try out different layouts of a store. In one style, all items that are bought together frequently can be placed next to each other to drive sales up of these combination of products. All customers who are purchasing computers also have a tendency to buy antivirus software along with it. So by that logic, keeping the hardware units in close proximity to the software display may help augment the sales of both items. In an opposing strategy, by keeping hardware and software displays at opposite ends of the shop may entice customers who are buying these items to pick up and buy other items put on display along the way and increasing sales. For eg, a customer buy an expensive computer, he wants to buy an antivirus for it but while walking, the home theatre system was on display. So he picks up this as well. Market basket analysis helps retailers decide the items to be put on sale at a discounted price. If it is noticed that customers buy computers and printers together then discounting the price of printers may increase the sale of both, printers and computers. All associations are grouped into 3 categories today 1 Frequent pattern: A pattern consisting of a set of items, that occurs frequently together in a data set. For instance, milk and bread which have a correlation and are frequently bought together is a frequent item set 2 Subsequence, is a itemset where the order matters. If a customer buys a PC first and then followed by a printer is called a frequent sequential pattern if they have a strong correlation and appear frequently 3 Substructure refers to various structural forms like subgraphs, subtrees, or sub-lattices in a form of itemsets or subsequences. If it occurs frequently, it is a frequent structured pattern II  RELATED  WORK There has been a lot of development in the area of association rule mining. Abhijit Sarkar and Apurba Paul used a tree based approach in order to find useful patterns. Their technique is similar to FP growth algorithm which involves building a tree and performing association discovery on the nodes. Apriori algorithm being one of the most widely used algorithms faces numerous drawbacks. Hence many alterations were proposed to improve the efficiency and reduce the time 747 978 1-5 090 23 99 8 1 6 31 00 c  2 0 1 6 IEEE 


2 A g r a w a l c a m e up w ith A I S alg o r ithm  w h ich cr ea te d candidate item sets on-the-fly on each pass of the database scan. Any itemets which are large from previous pass are checked if they are there in the current transaction.  Ke Sun and Feng Shan Bai also propose a unique technique to mine association rules without assigning any weights to the itemsets 3 n c ep t m a p s is  a n o t her tec hniq u e f o r finding fre q u e n t  itemsets. The current method to construct concept maps considers only the rules of the questions which are not answered. It misses some information about about questions that are answered correctly. Shyi-Ming Chen 4  pr o pos e s  a  technique to find association rules which doesnít build extraneous relationships or break relationships between concepts in concept maps III  ASSOCIATION  RULES Let I = \(I1, I2,..., Im\e an itemset. Let D be a dataset of all the transactions involved in the calculation. Each transaction T is marked with a unique identifier TID. Let A be a set of items An association rule is an implication of the form A => B. The rule A => B in the transaction set  has a support s, where s is the percentage of transactions in D that contain A U B. It is the probability, P\(A U B\. The rule A -> B has confidence c in the transaction set D, where c is the percentage of transactions in D containing A that also contain B. This is taken to be the conditional probability, P\(B  A Rules that satisfy both, a minimum confidence \(min_conf\ and support threshold \(minsup\ have a correlation and are called strong. All support and confidence values occur betwwen 0  and 1 00  A  Defining Support and Confidence A set of items is called an itemset. An itemset that contains k items is a k-itemset. The set {printer,computer } is a 2itemset. The support count of an itemset is the number of transactions that contain the itemset  If the support of an itemset satisfies a pre-specified minimum support  threshold,  then it is a  frequent itemset  IV  ASSOCIATION  RULE  MINING Step 1: Here we try to find all itemsets that are frequent Each of these itemsets will be in more number of transactions as the min support count Step 2: Here we generate strong association rules from the itemsets which are frequent in the above step. All these rules must be above the minimum support and should be above the minimum confidence A  Problems in mining frequent itemsets A major hindrance and challenge in retrieving and mining these itemsets from a huge data set is that the outcome of mining often generates a large number of itemsets, all which satisfy the minimum support \(min sup\ threshold, especially when minimum support is set low. This is due to the apriori property which states that if an itemset is frequent and above threshold then each of its constituents will be frequent as well Solution: Mine closed patterns and max-patterns 1  Closed Patterns An itemsets is closed if none of its proper supersets has an equal support as it has. An itemset X is called a closed frequent itemset if X is both frequent and separately closed in the dataset For eg, If Freq. Pat.: = {A:3, B:3, D 4 E:3, AD:3 From the above set {A:3,}  cannot be  closed frequent since  its superset { AD:3} has same support Closed set C= {B:3, D 4 E:3, AD:3 2  Max Patterns An itemset X is called a max-pattern if X is frequent and there is no more super-pattern that is frequent. Suppose that a database has only two transactions a11, a12, : : : , a11 0 a1, a2, : : : , a5 0  Let the minimum support count be 1 We find two closed frequent itemsets and their support counts C={a1, a2, : : : , a1 00 1, {a1, a2, : : : , a5 0 2 There is only one maximal frequent itemset: M={ {a11, a12 a11 0 1 We cannot include {a1, a2, : : :  ,  a5 0 as  a  maximal frequent itemset because it  has  a  superset which is frequent a1,  a2,  :  :  :  , a1 00  V  APRIORI  ALGORITHM A frequent itemset and all its subset will be frequent. This is the apriori property. If an item-set is below the minimum support threshold, minsup, then it is not frequent. Now if an item A is added to the itemset, then also the final itemset will not be frequent as it will not occur frequently than the min sup count. Hence, I U A will not be frequent either, that is, P\(I U A\ < minsup. This is called as downward closure property. Eg If { PC, printer, cartridge} is frequent, so is {printer cartridge A  Scan Let T be the set of transactions in the database. In this step we first generate the candidate itemset. This is done by pairing and forming 2-itemsets. Then we count the number of times these itemsets occur in each transaction B  Prune Since the number of candidate itemsets can be large in number, this leads to heavy computation. To reduce the size of 748 2nd I nternational Conference on Applied and Th eoretical Computing and Communication T ec h nolog y iCA T cc T  


candidate itemset, we prune those itemsets which  occur below the minimum support. When the frequent itemsets from the database is achieved, it is simple to generate strong association rules. All these rules agree both minimum support and minimum confidence C  I llustration Total number of transactions:15 Min Support = 2 0  Min Support threshold = 2 0 15  1 00 3 TID List of Items 1 1,5 6  8  2 2 4  8  3 4 5 7  4 2,3 5 5 6  7  6 2,3 4  7 2 6  7  9  8 5 9  8  1 0 3,5 7  11 3,5 7  12 5 6  8  13 2 4  6  7  1 4 1,3,5 7  15 2,3 9   Scan 1 Item set Support Count 1 2 2 6  3 6  4  4  5 8  6 5 7  7  8  4  9 2  Prune 1 Itemset Support Count 2 6  3 6  4  4  5 8  6 5 7  7  8  4   Scan 2 Itemset Support Count Itemset Support Count 2,3} 3 4 5} 1 2 4 3 4  6 1 2,5 0  4  7 2 2 6 2 4  8 1 2 7 2 {5 6 3 2 8 1 {5 7 5 3 4 1 {5 8 2 3,5} 3 6  7 3 3 6  0  6  8 2 3 7 3 7  8  0  3 8  0    Prune 2 Itemset Support Count 2,3} 3 2 4 3 3,5} 3 3 7 3 5 6 3 5 7 5  6  7 3 2 n d I nt e rnational Con fe r e n ce on App li ed an d Th e or e ti c al Co mp utin g an d Co mm uni c ation T ec h nolo g y iC A T cc T  749 


 Scan 3 I t emse t Support Coun t  2,3 4   1 3,5 7   3 5 6  7   1  Prune 3 Itemset Support Count 3,5 7 3  The data contain frequent itemset X ={3, 5 7 Association rules are retrieved from X. The nonempty subsets of X are {3 5 7 I3, I5}, {5 7 3 7 3}, {5} and 7  The final association rules along with their confidene are listed below Association Rules Confidence 3,5 7 3  3 = 1 00  3 7 5 3  3 = 1 00  5 7 3 3  5 60  3 -> {5 7 3 6 5 0  5 -> {3 7 3 8 3 7 5 7 3,5 3 7  4 2 8   VI  LIMITATIONS  OF  APRIORI  We see that the apriori algorithm functions in a bottom-up breadth-first search method. The computation starts from the smallest set of frequent itemsets and moves upward till it reaches the largest frequent itemset. The number of times the algorithm passes a database is equal to the largest size of the frequent itemset. When an itemset becomes longer and it is frequent then algorithm becomes slower and takes a performance hit. There are many methods to enhance the efficiency of the algorithm. Either we can remove a transaction that does not contain any frequent k-itemset. It is being useless in subsequent scans. Or we could  partition the itemset that is potentially frequent. It must be frequent in at least one of the partitions of DB. We could also mine on a sample of the dataset, compromising on accuracy but improving efficiency VII  MODIFIED  APRIORI  ALGORITHM A popular enhancement to the apriori algorithm is the FP tree algorithm. By reducing the costly database scans, it decomposes the dataset into smaller ones. We focus here on a hash based technique which divides the dataset into parts using a hash function Hashing itemsets into its buckets: Here,a hash-based technique is used to decrease the size of the candidate k- itemsets, Ck, for k > 1. For instance, when scanning the dataset to generate itemsets which are frequent of size 1, we can generate the itemsets which are frequent of size 2 and hash \(i.e., map\them into various buckets of a table structure, and count the corresponding bucket counts. An itemset of length 2 with a corresponding bucket count in the hash table which is below the support threshold cannot be frequent and is should be removed from the candidate set. This technique may substantially decrease the number of candidate k-itemsets examined \(especially when k = 2 TID List of Items 1 1,2,5 2 2 4  3 2,3 4 1,2 4  5 1,3 6 2,3 7 1,3 8 1,2,3,5 9 1,2,3  H\(x,y\=\(\(order of X\*1 0 order of Y\\mod 7   012\012 015                                            For eg, the minimum support count is 3, then the itemsets in buckets 0 1, 3, and 4 are not frequent and are removed. They should not be included in C2 7 5 0 2 n d I nt e rnational Con fe r e n ce on App li ed an d Th e or e ti c al Co mp utin g an d Co mm uni c ation T ec h nolo g y iC A T cc T  


VIII  PROPOSED  TECHNIQUE We propose a method which tries to find the frequent itemsets in a bottom-up manner as well as a top down manner It maintains a list of al itemsets. While passing through the database, it counts the support of these large candidate itemsets to see check if they are actually frequent. In such an event, we know that all the subsets of these frequent sets are going to be frequent and so we can remove them from the list to be scanned. This will increase our performance. If we are lucky we may discover a very large maximal frequent itemset very early in the algorithm Consider a pass k,which signifies the length of itemsets to be searched. If some itemset that is a maximal candidate itemset say X, if frequent then all its subsets and consituents must be frequent as well. Hence, all of its subsets should be pruned from the set to be scanned. In this procedure,all candidate sets considered in the bottom-up direction in the pass are removed If this set subsumes all the candidate sets of level k, then we need not proceed further and thus we save many database passes. Clearly, the proposed technique has an advantage over apriori algorithm when the largest frequent itemset is long In every step, we prune all items whose support is less than  the min sup count from the maximal candidate itemset. Let us call the maximal candidate itemset M. After generating candidate itemsets, we divide them into 2 categories. One whose support is above the minimum support threshold go into category L and ones whose support is below the threshold into category S Using the same example we used in Apriori algorithm, we count the number of times each item appears in the set of transactions. Min Support = 2 0 2 0 15  1 00 3 Itemse t Support Coun t  1 2 2 6  3 6  4  4  5 8  6 5 7  7  8  4  9 2  Then we prune all itemsets in S from the maximal frequent itemset M. For each item in the itemset, we remove it from M in order to generate more maximal frequent itemsets Eg: S={a,b} M={a,b,c After pruning we get, M={ {b,c},{a,c} } Dividing the itemset into L and S, we get M={1,2,3 4 5 6  7  8  9  L={2,3 4 5 6  7  8 S={1 9  Prune S from M M={1,2,3 4 5 6  7  8  9 2,3 4 5 6  7  8  Next, we generate 2-itemsets and divide it into itemsets above and below the support threshold, into L and S respectively Itemset Support Count Itemset Support Count 2,3} 3 4 5} 1 2 4 3 4  6 1 2,5 0  4  7 2 2 6 2 4  8 1 2 7 2 {5 6 3 2 8 1 {5 7 5 3 4 1 {5 8 2 3,5} 3 6  7 3 3 6  0  6  8 2 3 7 3 7  8  0  3 8  0    L={ {2,3},{2 4 3,5},{3 7 5 6 5 7  6  7  S={{2,5},{2 6 2 7 2 8 3 4 3 6 3 8  4 5 4  6  4  7  4  8 5 8  6  8  7  8  Now our M = {2,3 4 5 6  7  8  We proceed by dividing M based on S. We first see {2,5 First we remove 2 from M to get {3 4 5 6  7  8 Next we remove 5 from M to get {2,3 4  6  7  8  2,5}  =>  {2,3 4 5 6  7  8 Remove  2  =>     {3 4 5 6  7  8  2,5} => {2,3 4 5 6  7  8 Remove 5 => {2,3 4  6  7  8  Now M={ {2,3 4  6  7  8 3 4 5 6  7  8  Next we see {2 6  2 6 2,3 4  6  7  8 Remove 2 => {3 4  6  7  8  2 6 2,3 4  6  7  8 Remove 6 2,3 4  7  8  We  wonít  divide  {3 4 5 6  7  8 as it  doesnít  contain 2  from 2 6  Now M={{3 4  6  7  8 2,3 4  7  8 3 4 5 6  7  8  But since {3 4  6  7  8 is a subset of {3 4 5 6  7  8 we use the superset in this top down approach Using this approach we keep on pruning S from M in order to arrive at the maximal itemset M 2 n d I nt e rnational Con fe r e n ce on App li ed an d Th e or e ti c al Co mp utin g an d Co mm uni c ation T ec h nolo g y iC A T cc T  7 5 1 


S M  2,3 4 5 6  7  8  2,5} {3 4 5 6  7  8 2,3 4  6  7  8  2 6 3 4 5 6  7  8 2,3 4  7  8  2 7 3 4 5 6  7  8 2,3 4  8  2 8 3 4 5 6  7  8 2,3 4  3 4  4 5 6  7  8 3,5 6  7  8 2 4  2,3 3 6 3,5 7  8  4 5 6  7  8 2,3 2 4  3 8 3,5 7  4 5 6  7  8 2,3} {2 4   4  6 3,5 7 5 6  7  8  4 5 7  8  2,3} {2 4   4  7 3,5 7 5 6  7  8  4 5 8 2,3 2 4   4  8 3,5 7 5 6  7  8  4 5} {2,3 2 4  5 8 3,5 7  6  7  8 5 6  7  4 5 2,3} {2 4   6  8 3,5 7  7  8 5 6  7  4 5 2,3} {2 4   7  8 3,5 7  8 5 6  7  4 5} {2,3 2 4   Hence we get M= { {3,5 7  8 5 6  7  4 5} {2,3} {2 4  From this we see we have 2 3-itemsets, {3,5 7 and {5 6  7  We calculate its support and find that {3,5 7 has a support of 3 whereas {5 6  7 has a support of 1 which is below the threshold. Hence the frequent itemset is {3,5 7 and all its subsets are also frequent Using the proposed technique, we know {3,5 7 is a frequent itemset and all its subsets will also be frequent. Hence we can prune all itemsets containing 3,5 and 7 This saves a lot of space as well as time while determining frequent itemsets. One of the major shortcomings of the apriori algorithm is that we have to scan the database very frequently. It increases the time and decreases the performance of the database. We designed this algorithm to take this fact into account and minimize the time spent on passing through a database giving us a better outcome and result IX  CONCLUSION We see that the apriori algorithm operates in a bottom-up breadth-first search method. The computation starts from the smallest set of frequent itemsets and moves upward till it reaches the largest frequent itemset. The number of times the algorithm passes a database is equal to the largest size of the frequent itemset. When an itemset becomes longer and it is frequent then algorithm becomes slower and takes a performance hit. We proposed a modification to the existing apriori algorithm which reduced the number of iterations required hence increasing performance. Further, we proposed a new technique to increase efficiency and reduce the time taken for generating frequent itemsets. This top down approach maintains a list of candidate itemset to reduce the number of database scans. Through this technique we were able to significantly reduce the time required to aquire frequent itemsets. Hence increasing efficiency of the entire algorithm X  REFERENCES 1 a n J  K a m b e r M Da ta M i ning Co n c ep ts a n d Techniques.Higher Education Press,2 00 1 2 A b h i j i t S a rk a r  A p u r ba  P a u l  S a in ik Ku m a r Mah ata   Deepak Kumar,Modified Apriori Algorithm to find out Association Rules using Tree based Approach 3 R e va thi   M  Ge etha Re M od ified Apr i o ri Algor i t hm in E-Commerce Recommendation System  4  i-M ing Che n Shih-M ing Ba i   U s ing d a t a m i ning techniques to automatically construct concept maps for adaptive learning systems 5 e Sun a n d F e ngsha n Ba i  M i ning W e ighte d Asso cia t ion Rules without Pre-assigned Weights  6  H a n  Pe i  Y Y i n a nd R M a o M i ning F r eq ue nt P a tterns without Candidate Generation:A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery,2 004   7  Jo ng S  P  M i ng S C,P hi li p  S Y  An e ffe ctive ha sh ba sed algorithm for mining association rules. In Proceedings of the 2 00 5 ACM SIGMOD International Conference On Management of Data.2 00 5  8  ng Fa ng , Q i a n X u e z h o ng R e sea rc h o f im p r o v ed apriori algorithm in mining association rules, Computer Engineering and Design, 2 008   9  i ngz h o ng , Wa ng Ha i y a ng Y a n Zho ngm in E ffic ient mining of association rules by reducing the number of passes over the database, Computer Science and Technology,2 008  1 0   G e e t ha Sk. Mo hidd in An E ffic i e nt Da t a  M i nin g Technique for Generating Frequent Item sets, International Journal of Advanced Research in Computer Science and Software Engineering Volume 3, Issue 4 April 2 0 13 ISSN 22 77 12 8 X  R A g r a w a l  T I m ielin ski and A  Sw am i M ining Association Rules between Sets of Items in Large Datasets Proc. ACM SIGMOD 9 3, pp. 2 07 21 6 1 99 3 7 5 2 2 n d I nt e rnational Con fe r e n ce on App li ed an d Th e or e ti c al Co mp utin g an d Co mm uni c ation T ec h nolo g y iC A T cc T  


 International Conference on Computing, Communication and Automation \(ICCCA2016   93     Fig. 7  Execution time of trie based Apriori on three different block distributions x-none In Fig   7, BD1 exhibits minimum execution time compared to BD2 and BD3 In BD1 blocks are located only on three DNs i.e two physical a nd one VMs DN So in this case Mappers are not running on both slower DNs tha t make the execution faster. Execution time for BD2 is poor than that o f BD1 due to using both VMs DNs  BD3 exhibits the worst performance since all the blocks are available on each DN MapReduce processes a data block locally on the DN where the block is present. F or block distribution BD3, all Mappers are running on the same DN since all blocks are locally available to each node In different attempt of running a job DN may be d ifferent each time but all the Mappers are being run on a same DN. All Mapper s running on the same DN does not make use of available resources which leads to increased execution time Here it can be seen that due to higher replication factor data locality may be a hurdle that slow down the execution  x-none E  Controlling Parallelism with Split Size x-none Hadoop is designed to process big datasets that does not mean one cannot be benefited for small datasets. Apriori is a CPU-intensive algorithm and consumes a significant time for smaller datasets. To reduce the execution time we need more than on e task running in parallel Split is used to control the number of map tasks for a MapReduce job A split may consist of multiple blocks and there may be multiple splits for a single block So without changing the block size user can control the number of Mappers to be run for a particular job We have used the method setNumLinesPerSplit\(Job job int numLines  of class NLineInputFormat  from MapReduce library to set the number of lines per split. In our earlier cases we were running multiple Mappers against different parts of the same block Here we set the split size 5K lines on block distribution BD3 which contains 5 data blocks This creates 12 splits i.e 12 Mappers  size for BD3 the n  blocks are considered as input s plits. Fig   8 shows the difference in execution time for these two cases  Here it can be seen that how the split size controls the parallelism Smaller split size launches more number of Mappers which consequently increase the parallelism. It does not mean that more number of Mappers always results into better performance. Increasing the number of Mappers beyond a particular point starts to degrade the performance due to unnecessary overheads and shortage of resources   To achieve the right level of parallelism it must be taken care that the map task is CPU-intensive or CPU-light as well as the size of dataset to be processed Fig. 8  Execution time of trie based Apriori with Input Split and without Input Split on BD3 x-none F  Issues Regarding MapReduce Implementation The efficiency of an algorithm running as a MapReduce job is extensively influenced by data structure used and algorithm itself A third factor that cannot be ignored is the implementation technique.  Implementation technique may be regarding to implementation of various modules of Apriori e.g candidate generation support counting of candidates against each transaction pruning of infrequent itemsets or regarding to MapReduce implementation of Apriori MapReduce implementation of Apriori is central to discussion here. A major issue in MapReduce based Apriori is to invoke candidate generation i.e apriori-gen  Algorithm 7 at appropriate place inside Mapper class. In our implementations we have invoked apriori-gen  inside customized method map of Mapper class. In Mapper class, two methods setup  and map  are customized and one method apriori-gen  is defined Method setup  is called once at the beginning of a task It is customized to read frequent itemsets of previous iteration from distributed cache and to initialize prefix tree Method apriori-gen  generates candidates using prefix tree containing frequent itemsets The map  is invoked for each line of input split of dataset If there are 100 lines of input assigned to a Mapper then map method will be invoked for 100 times Subsequently it invokes apriori-gen  repeatedly each time Since apriori-gen  method produces candidates which is independent of input instance, so need not to invoke repeatedly inside map  method The apriori-gen  metho d is  computation intensive and increases the execution time when invoked repeatedly. This repeated computation can be fixed if we invoke apriori-gen  outside of map  Theoretically it sounds good but did not work when invoked inside setup  method We have also tried another way in which apriorigen  is invoked inside overrided method run  of Mapper class but again could not achieve expected reduction in execution time VI  C ONCLUSIONS  In this paper we have investigated a number of factors affecting the performance of MapReduce based Apriori algorithm on homogeneous and heterogeneous Hadoop 


 International Conference on Computing, Communication and Automation \(ICCCA2016   94   cluster and presented strategies to improve the performance It has been shown that how hash table trie data structure and transaction filtering technique can significantly enhance the performance Factors like speculative execution physical  VMs DataNodes, data locality block distribution and split size are such that their proper tuning can directly enhance the performance of a MapReduce job even without making algorithmic optimization Approaches of MapReduce implementation of Apriori is another important factor that also influence the performance R EFERENCES  1  J. Han and M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers 2006  2  J S Ward a  b y d ata a survey of big d ata d  http://arxiv.org/abs/1309.5821v1 3  Apache Hadoop, http://hadoop.apache.org 4  Big data is useless without algorithms Gartner says http://www.zdnet.com/article/big-datais useless-withoutal gorithmsgartner-says/, Retrieved Nov. 2015 5   algorithms for mining association rules  Proceedings Twentieth International Conference on Very Large Databases, Santiago 1994 pp. 487 499 6   and distributed association mining a survey Concurrency, IEEE, vol 7, no. 4,pp. 14 25, 1999 7  K. Bhaduri, K. Das, K. Liu, H. Kargupta and J. Ryan, Distributed Data Mining Bibliography 2008  8  HDFS  Architecture Guide https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html Retrieved Sept 2015  9  MapReduce Tutorial http://hadoop.apache.org/docs/current/hadoopmapreduce-client/hadoop-mapreduce-clientcore/MapReduceTutorial.html, Retrieved Sept. 2015 10  Yahoo Hadoop Tutorial http://developer.yahoo.com/hadoop/tutorial/index.html 11   ACM SIGOPS Operating Systems Review vol 37 no 5 pp 29 43 2003  12    Commun., vol. 51, pp 107 113, 2008 13   mining using clouds an experimental implementation of apriori over mapreduce    14   Apriori: association rules algorithm based on mapreduce  IEEE 2014  15   as a programming model for association rules algorithm on hadoop  nternational Conference on Information Sciences and Interaction Sciences ICIS 2010 vol. 99, no. 102, pp. 23 25  16   implementation of apriori algorithm based on mapreduce  h ACIS International Conference on  Software Engineering Artificial Intelligence Networking and Parallel  Distributed Computing IEEE 2012 pp 236 241  17   Hadoop as a platform for distributed association rule mining   COMPUTING 2013 the Fifth International Conference on Future Computational Technologies and Applications, pp. 62 67  18  M-Y Lin P-Y Lee and S based frequent itemset mining algorithms on mapreduce in  Proceedings 6th International Conference on  Ubiquitous Information Management and  2012, Article 76 19   itemset mining on Hadoop  Proceedings IEEE 9th International Conference on Computational Cybernetics \(ICCC\Hungry, 2013, pp. 241 245  20   strategy of mining association rule based on cloud computing   Proceedings IEEE International Conference on Business Computing and Global Informatization BCGIN 2011 pp 29 31 21  Honglie Yu, Jun Wen, Hongmei Wang and Li Jun, "An improved apriori algorithm based on the boolean matrix and Hadoop Procedia Engineering 15 \(2011\1827-1831, Elsevier 22  Matteo Riondato, Justin A. DeBrabant, Rodrigo Fonseca and Eli Upfal PARMA: a parallel randomized algorithm for approximate association rules mining in mapreduce in Proceedings 21st ACM international conference on information and knowledge management 2012 pp 8594  23  Jiong Xie et al Improving mapreduce performance through data placement in heterogeneous hadoop clusters in IEEE International Symposium on Parallel & Distributed Processing,  Workshops and Phd Forum \(IPDPSW\ 2010, pp. 19  24  Matei Zaharia Andy Konwinski Anthony D Joseph Randy Katz and Ion Stoica Improving mapreduce performance in heterogeneous environments in 8th USENIX Symposium on Operating Systems Design and Implementation \(OSDI\ 2008, vol. 8, no. 4, pp. 2942  25  Hsin-Han You, Chun-Chung Yang and Jiun-Long Huang, "A load-aware scheduler for MapReduce framework in heterogeneous cloud environments in Proceedings of the ACM Symposium on Applied Computing, 2011, pp. 127-132 26  Faraz Ahmad Srimat Chakradhar Anand Raghunathan and T N Vijaykumar, "Tarazu optimizing MapReduce on heterogeneous clusters," ACM SIGARCH Computer Architecture News, vol. 40, no. 1 pp. 61-74, 2012 27  HADOOP PERFORMANCE TUNING white paper Impetus Technologies Inc October 2009 https://hadooptoolkit.googlecode.com/files/White%20paperHadoopPerformanceTuning.pdf 28  Apache Hadoop NextGen MapReduce YARN http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarnsite/YARN.html, Retrieved Sept. 2015 29  SPMF Datasets http://www.philippe-fournierviger.com/spmf/index.php?link=datasets.php 30   ata structure for data  in Mathematical and Computer Modelling, vol 38  no. 7, pp. 739-751, 2003 31  Ferenc Bodon A fast apriori implementation in Proceedings IEEE ICDM workshop on frequent itemset mining implementations  90, 2010 32  Sudhakar Singh, Rakhi Garg and P. K. Mishra  analysis of apriori algorithm with different data structures on hadoop cluster  International Journal of Computer Applications, vol. 128, no. 9, pp. 4551  2015  33  Christian Borgelt Efficient implementations of apriori and 351clat in Proceedings IEEE ICDM workshop on frequent itemset mining   34  Ferenc Bodon, "Surprising results of Trie-based fim algorithms," FIMI 2004  35  Ferenc Bodon A trie-based APRIORI implementation for mining frequent item sequences," in Proceedings 1st international workshop on open source data mining: frequent pattern mining  implementations ACM, 2005 36  Hadoop Wiki Virtual Hadoop https://wiki.apache.org/hadoop/Virtual%20Hadoop  


002 004\002 005\002 006\002 007\002 003\002 033\002 034\002 035\002 002\004\002\005\002 006\002 
002 005 007 033 035 004\002 004\005 004\007 004\005\006\007\003\033 
C Increased Rule Length 
002 005 007 033 035 004\002 004\005 004\007 004\033 004\035 005\002 020\020\030\027\011\025\021\013 026\032\025\025\011\020\021\030\032 025 011\021\012\030\017 023\004\002\002 023\004\002\004 
 
002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 
Figure 6 Throughput for increasing length rule candidates The Y-axis is measured in billion evaluations per second The X-axis shows the corresponding rule length As before the gures include the theoretical trend-line computed from amplifying naive-sc on the GPU using Eq 4 reason the observed improvement is stable depending mostly on the item number Assigning multiple candidate rule collections to a single block resulted in Figure 7 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using synthetic data   Figure 8 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using real data For both kernels we observe a similar behaviour when we increase rule candidate length to a number larger than 32 After that point we require evaluating the preìx in two phases following a technique similar to parallel reduction although using warp vote functions This extra phase requires an additional synchronization step which increases the total execution time per iteration Additionally when we have prominent rules with item indices in sequence i.e accidents dataset as indicated by its sparsity pattern caching transactions does not provide any improvement However when there are many rules with out of sequence preìxes the cost of uncoalesced memory accesses matches the synchronization cost as indicated by experiments on dataset 1 VIII C ONCLUSION In this paper we studied the support count operation commonly used in association rule mining problems We proposed a work-efìcient parallel algorithm that is suitable for massively parallel architectures Furthermore we presented a data layout scheme used to enable low overhead coordination of the processing elements reduce the memory requirements and achieve high off-chip memory bandwidth utilization Furthermore we discussed in detail low level optimization strategies related to effective use of shared memory while presenting a simple strategy for resolving shared memory bank conîicts incurring minimal additional work However there is still some additional issues that we need to address Firstly we already considering resolving the issue of 
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
improvement over the default-tpsc execution time A combination of loop unrolling and increase shared memory utilization from storing more candidate rules in the same block was the reason for the observed improvement In contrast enabling caching of transactions in shared memory with kernel mrs-tpsc presented less improvement in the relative execution time compared to mr-tpsc The culprit is this case is the additional synchronization step which is required after loading the data in shared memory Finally experiments performed on dataset 2 and 4 indicate similar behavior to our previous experiments where multiprocessor underutilization was limiting the maximum possible performance increase Even in the case where we increase the workload of participating blocks interleaved execution of warps is limited since as the block size is small In this section we discuss the effects of discovering rules with length larger than 32 Due to lack of space we present the results from the execution on synthetic data 1 and accidents which are good representatives of the observed behaviour We focus on the mrs-tpsc and mr-tpsc variations which we established to be highly optimized throughout our experiments   
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
037\005\005\010 \004\012\007!\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002 006\002 011\012\012\004\005\007\010\011\012\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002\006\002 004\007\024\010$\013  014%\031&&\013 
18 
027\011$\012\014\017\021\036\021\031\013\020  025\012%&\011\036\013\020\022\036\022'\031\014  025\012%&\011\036\013\020\022\036\022\020\031\014 021\(\011\032\015\011\021\030\020    002 004 005 006 007 002\004\002\005\002 006\002 
1431 
1431 


 volume 22 pages 207Ö216 ACM 1993  R Agra w al R Srikant et al F ast algorithms for mining association rules In  volume 1215 pages 487Ö499 1994  E Ansari G Dastghaibif ard M K eshtkaran and H Kaabi Distrib uted frequent itemset mining using trie data structure  35\(3 2008  M Atzmueller and F  Puppe Sd-mapÖa f ast algorithm for e xhausti v e subgroup discovery In  pages 6Ö17 Springer 2006  C Creighton and S Hanash Mining gene e xpression databases for association rules  19\(1 2003  W  F ang M Lu X Xiao B He and Q Luo Frequent itemset mining on graphics processors In  pages 34Ö42 ACM 2009  K Geurts G W ets T  Brijs and K V anhoof Proìling of high-frequenc y accident locations by use of association rules  1840 2003  A Ghoting G Buehrer  S P arthasarathy  D Kim A Nguyen Y K Chen and P Dubey Cache-conscious frequent pattern mining on modern and emerging processors  16\(1 2007  G Grahne and J Zhu Ef ciently using preìx-trees in mining frequent itemsets In  volume 90 2003  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation In  volume 29 pages 1Ö12 ACM 2000  J Hipp U G  untzer and G Nakhaeizadeh Algorithms for association rule miningÑa general survey and comparison  2\(1 2000  R Jin and G Agra w al An algorithm for in-core frequent itemset mining on streaming data In  pages 8Öpp IEEE 2005  R Jin and G Agra w al Systematic approach for optimizing comple x mining tasks on multiple databases In  pages 17Ö17 IEEE 2006  E Lindholm J Nick olls S Oberman and J Montrym Nvidia tesla A uniìed graphics and computing architecture  2 2008  J Liu Y  P an K W ang and J Han Mining frequent item sets by opportunistic projection In  pages 229Ö238 ACM 2002  L Liu E Li Y  Zhang and Z T ang Optimization of frequent itemset mining on multiple-core processor In  pages 1275Ö1285 VLDB Endowment 2007  B Mobasher  R Coole y  and J Sri v asta v a Automatic personalization based on web usage mining  43\(8 151 2000  E  Ozkural B Ucar and C Aykanat Parallel frequent item set mining with selective item replication  22\(10 2011  J Pei J Han H Lu S Nishio S T ang and D Y ang H-mine Hyper structure mining of frequent patterns in large databases In  pages 441Ö448 IEEE 2001  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster  In  pages 467Ö473 Springer 2003  C Silv estri and S Orlando gpudci Exploiting gpus in frequent itemset mining In  pages 416Ö425 IEEE 2012  A T ajbakhsh M Rahmati and A Mirzaei Intrusion detection using fuzzy association rules  9\(2 2009  T  T assa Secure mining of association rules in horizontally distrib uted databases  26\(4 2014  K W ang M Stan and K Skadron Association rule mining with the micron automata processor In  2015  M J Zaki Scalable algorithms for association mining  12\(3 2000  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors In  pages 43Ö43 IEEE 1996  F  Zhang Y  Zhang and J D Bak os Accelerating frequent itemset mining on graphics processing units  66\(1 2013  Y  Zhang F  Zhang Z Jin and J D Bak os An fpga-based accelerator for frequent itemset mining  6\(1 2013 
002 004 005 006 007 002 004\005\035 005\003\033 006\035\007 003\004\005 002 033 004\005 004\035 005\007 002 004\005\035 005\003\033 006\035\007 003\004\005 
ACM SIGMOD Record Proc 20th int conf very large data bases VLDB IAENG International Journal of Computer Science Knowledge Discovery in Databases PKDD 2006 Bioinformatics Proceedings of the fth international workshop on data management on new hardware Transportation Research Record Journal of the Transportation Research Board The VLDB Journal FIMI ACM SIGMOD Record ACM sigkdd explorations newsletter Data Mining Fifth IEEE International Conference on Data Engineering 2006 ICDEê06 Proceedings of the 22nd International Conference on IEEE micro Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining Proceedings of the 33rd international conference on Very large data bases Communications of the ACM Parallel and Distributed Systems IEEE Transactions on Data Mining 2001 ICDM 2001 Proceedings IEEE International Conference on Advances in Knowledge Discovery and Data Mining Parallel Distributed and Network-Based Processing PDP 2012 20th Euromicro International Conference on Applied Soft Computing Knowledge and Data Engineering IEEE Transactions on Proceedings of the 2015 IEEE 29th International Parallel and Distributed Processing Symposium Knowledge and Data Engineering IEEE Transactions on Supercomputing 1996 Proceedings of the 1996 ACM/IEEE Conference on The Journal of Supercomputing ACM Transactions on Reconìgurable Technology and Systems TRETS 
Figure 9 Execution time measured for increasing rule size  the Xaxis indicates the rule length multiprocessor under-utilization For dataset with low number of items we can assign individual groups of threads in the same block to different rule collections effectively increasing the block size as well as utilization Secondly we would like to adapt our solution to an architecture consisting of multiple GPUs and address challenges related to partial result sharing A CKNOWLEDGMENT This work was supported by the U.S National Science Foundation under grant ACI-1339756 R EFERENCES  Frequent itemset mining dataset repository  2015 URL http://ìmi.ua.ac.be/data  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases In 
014\010\015\004\013\016!\004\005!\021\013 026\027\012\007\030\004\007\010\005\013\031\013 014\010\015\004\016!\004\005!\021\013 037\005\005\010 \004\012\007!\013 
015\036\021\031\013\020 015\013\036\021\031\013\020 
1432 
1432 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


