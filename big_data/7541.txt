Efficiently Mining Maximal Frequent Sets For Discovering Association Rules Krishnamoorthy Srikumar, Bharat Bhasker Indian Institute of Management, Prabandh Nagar, Lucknow  226 013. INDIA Phone: 091-0522-361 889 to 091-0522-361 897 Fax: 091-0522-361 843 E-mail srikumar@iiml.ac.in  bhasker@iiml.ac.in Abstract We present Metamorphosis, an algorithm for mining 
Maximal Frequent Sets \(MFS\ng novel data transformations. Metamorphosis efficiently transforms the dataset to Maximum Collapsible and Compressible \(MC 2 rmat and employs a top down strategy with phased bottom up search for mining MFS. Using the chess and connect dataset [benchmark datasets created by Univ. of Califor  w e  demonstrate that our algori thm offers better performance in mining MFS compared to dGenMax an algorithm that offers better performance compared to other known algorithms 
thermore, we evaluate our algorithm for mining Top-K maximal frequent sets in chess and connect datasets. Our algorithm is especially efficient when the maximal frequent sets are longer 1. Introduction Mining frequent itemsets is a fundamental and essential operation in many data mining applications including discovery of association rules, strong rules, correlations, sequential rules, episodes, multi-dimensional In general, frequent itemset mining problem is formulated as follows: Given a large database of transactions, our task is 
to identify frequent itemsets, where frequent itemsets are those itemsets that occur in at least a user-specified percentage of the database Most of the current itemset mining algorithms are variants of Apriori and it has been demonstrated that such algorithms are inadequate on data sets with long patterns  ori based algorit hms em ploy a pure bottom up, breadth first search, mining a frequent pattern of length m require mining all its 2 m 2 subsets, which would be computationally expensive if m is very large 
30\ Hence, there has been recent interest in mining Maximal Frequent Sets \(MFS\nt itemsets whose supersets are infrequent and all its subsets are frequent Recent approaches to MFS mining include Pincer-Search  h at uses a mixed searc h strat egy to e n umer rfor ms a brea dth first trave rsa l of the search space with look-ahead. Ge use s  vertical data representation and tidset intersections to mine quickly the MFS. dGe d ve rsion of GenMax which uses a novel vertical data representation 
called diffsets that only keep s trac k of di ffere nces i n the tids of a candidate pattern first search of a lexicographic tree. It employs a transaction projection mechanism for counting the support of itemsets quickly. Mafia uses t h ree pr uning strate gies to rem ove  non-maximal sets. They are look-ahead pruning, superset frequency pruning, and parent equivalence pruning. A topdown algorithm proposed in uses a novel conce pt of dominancy factor for efficiently mining MFS Our primary motivation in this paper is to demonstrate 
that it is possible to mine quickly the entire set of MFS by employing a top-down strategy with phased bottom-up search in dense domains. Additionally, we employ novel data transformation techniques to mine quickly the entire set of MFS The organization of the rest of paper is as follows:  In the next section we introduce the model, notations, and pruning properties used in our algorithm. Section 3 gives the implementation details of our algorithm in various phases In Section 4, we present our experimental results. Finally in section 5, we conclude with a discussion of future work 
2. Model And Notations The mining algorithm introduced in this paper presumes a dataset consisting of transactions that contain multiple items in each transaction. A set of items present in a transaction is referred to as an itemset. We assume that the Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00  2003 IEEE 


items in the transaction are ordered sequence of numbers as per IBM artificial data set generator forma Let i 1 i 2 i m be a set of distinct items. A set X is called an itemset. An itemset X with k items \(X k  succinctly referred as k-itemset A transaction, T i x  1, 2N i x where N i is the number of items in transaction T i A transaction T i is said to support an itemset X if and only if X T i The support of an itemset X, denoted by Sup \(X\he total number of transactions in the dataset that supports X. The user defined minimum support is denoted as minsupport An itemset X Frequent itemsets\f Sup X minsupport In order to assist in identifying the unique transactions we define a hash value of transaction T i as follows Let LogP be an array containing a set of logarithm of prime numbers. The hash value of transactions T i denoted as Hval \(T i is defined as Hval \(T i  1 Ni LogP [x   x T i  We know that the product of prime numbers is unique and log \(m*n\= log \(m\og \(n\nce, it can be easily verified that the computed hash values would be unique only for unique transactions. In earlier passes of the algorithm we collapse the duplicate set of transactions using computed hash values. For more details regarding hash value computation and its usage in collapsing duplicate transactions, the reader is referre The dataset or transaction list, D, is an ordered set of triplet D = {\(T i N i Sup i 1, 2L; where T i is the i th transaction containing itemset x  N i is the number of frequent 1-items in T i  Sup i Sup \(T i  Where L is the total number of unique transactions in the dataset Table 1 gives a birds eye view of the transaction list D Table 1.  Transaction List, D TID T i x  N i Sup i T 1  1 2 3 4 5 5 1 T 2  1 2 4 5 4 2 T 3  1 2 3 5 4 1 T 4  2 3 5 3 1 T 5  2 3 4 3 1 Note: Sup \(1\= 4; Sup \(2\6; Sup \(3\4 Sup \(4\= 4; Sup \(5\= 5 We define TSUP as the set of supports of transactions in D. That is TSUP={Sup i i=1, 2, 3L For example, the transaction list D in Table 1 has TSUP={1, 2, 1, 1, 1 We denote NumF1 as the number of frequent 1-itemsets in the dataset at the user defined minimum support. The set of all frequent 1-itemsets \(in ascending order of 1-item support\denoted as 1 x  1, 2 NumF1; Sup \(x nsupport Sup\(x i  Sup\(x i+1 i=1,2\(NumF1-1 For the transaction list D in Table 1, assuming a user defined minimum support of 3 NumF1 = 5 and 1 1,3,4,5,2 Similarly, the set of frequent 2-itemsets is denoted as 2  As frequent 2-itemsets has just two items, each item in 1 will have set of other items \(in 1 whom it is frequent. We refer to this as set of extensions of an item, denoted as Extension \(x xample in Table 1, assuming a user defined minimum support of 3 Extension \(1\= {2, 5} as \(1,2\\(1,5\re frequent 2,5\quent Extension \(3\= {5} as \(3,4\is infrequent and Extension 4\ as \(4,5\not frequent The maximum number of elements in Extension \(x  denoted as Maxlen. For the above example, Maxlen = 3 as Extension \(2\has the maximum number of elements of 3 To mine the MFS efficiently, we transform the transaction list D to a collapsed and compressed format. In this work, we refer to this format as Maximum Collapsible and Compressible \(MC 2 format. We now describe the notations specific to this transformation Let us assume W as the Word Size of a computer. That is, for a 32-bit computer W=32 and for a 128-bit computer W=128 We compress the itemsets in every W transactions of D to a single transaction using vertical bitmaps. Table 2 gives the vertical bitmap compression for the transaction list D in Table 1 with W=3. That is, Table 2 shows the bitmap equivalent for the items in every 3 transactions \(i.e. NewT 1 for T 1 T 2 T 3 and NewT 2 for T 4 T 5  Table 2.  Vertical Bitmap Representation \(W=3 NewTID Item1 Item 2 Item 3 Item 4 Item 5 NewT 1 111 111 101 110 111 NewT 2 000 110 110 010 100 Table 3 gives the decimal notations for the bitmaps in Table 2. We transform the dataset in Table 1 to Table 3 and refer it as MC 2 Maximum Collapsible and Compressible format. Note that although we store the bitmaps in decimal notations as in Table 3, the numbers are internally stored in computer in binary form as in Table 2. Further, W is taken as 32 in the actual implementation though the illustrations in Table 2 and Table 3 has W=3 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00  2003 IEEE 


Table 3.  Dataset in Maximum Collapsible and Compressible \(MC 2 at NewTID Item 1 Item 2 Item 3 Item 4 Item 5 NewT 1 7 7 5 6 7 NewT 2 0 6 6 2 4 Our algorithm starts mining the MFS using the dataset in MC 2 format. Now let us describe the notations specific to MC 2 format. The transactions number i in MC 2 format is referred as NewT i  Bit \(x i\notes the bitmap for item x in transaction NewT i Here, by bitmap we refer to the equivalent integers stored for item x in MC 2 format. For instance, in Ta5, 2\4 The bitwise AND of all the items in an itemset X k in transaction NewT i is denoted as AND \(X k i when k=2, AND \(X 2 i\\(x 1 i x 2 i the operator & is a bitwise operator During the mining process, we check for infrequency of subsets of a k-itemset. For accomplishing this we hold a set of infrequent itemsets in a tree named as INF_Tree We store the MFS generated during the mining process in a Maximal List ML = {X | X and \(X x   1 The itemsets in maximal list are stored in compressed form, similar to the one shown in Table 3 with W=32 We define the MFS of longest length as LM \(Longest Maximal For efficient mining our algorithm uses two pruning properties namely, Apriori ve rse Apriori [7 for limiting the search space We define Top-K MFS mining problem as a set of K MFS of longest length. So, essentially the top-K MFS mined will have very low supports compared to all other MFS 3. Implementation Of Metamorphosis Our algorithm follows a top-do wn approach with phased bottom up search for mining MFS. In this section we describe the algorithm in various phases 3.1 Transaction List Generation In the first pass, the algorithm scans the dataset and computes support for all 1-itemsets. The infrequent 1itemsets are removed from further evaluation. In the second scan of the dataset, we compute the hash value for all frequent 1-itemsets in each transaction. Before we add a transaction to the Transaction List D, we check for uniqueness of transactions using ha que has h  value already exists we increment the support of corresponding transaction. Otherwise, we add it as a new transaction in D. A sample transaction list, D is given in Table 1 assuming a user specified minimum support of 2\We store the support value of transactions in TSUP While generating D, we store the support values of transaction in TSUP. After the transaction list, D is generated; we make a pass over D to compute 2 and Maxlen 3.2 MC 2 format Dataset Generation The algorithm converts the transaction list, D to MC 2 format by compressing the dataset using vertical bitmaps M etamorphosis \(Data-set DB Inputs 1. Data Set, DB; 2. minsupport; 3. K \(number of MFS desired\ to be given only for Top-K MFS mining 1 1 Frequent 1-itemsets 2 For each T i DB do begin 3.      Hval i  1 Ni LogP [x  4 Scan D and check for unique Hval i if exists increment its support Otherwise, sort items in ascending order of support and add T i to D 5 End 6.  Scan D and compute 2 supports \(frequent 2-item supports 7.  Convert D to MC 2 format and write the result to NewDB 8.  ML 9.  Initialize k to Maxlen 10 For k = Maxlen to 2 do begin 11 Generate k-itemsets, check for maximality and infrequency of subsets using INF_Tree 2 su pp orts and phased subset frequency checks 12 For each NewT i NewDB do begin  Compute support of k-itemset by bit-wise AND operation and if the support crosses minsupport requirement, and if no superset of it are frequent, add it to Maximal List, ML 13 End 14 If Top-K MFS mining\do 15 If ML has at least K maximal frequent set\o step 18 16.      decrement k by 2 17 End 18 Return Out p ut Fi g ure 1. Pseudo-code for Metamor p hosis Al g orithm   Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00  2003 IEEE 


with W=32\Once the MC 2 format dataset is generated we discard the transaction list D. Table 3 gives a birds eye view of the dataset in MC 2 format. For a machine with W=32, we can logically store vertical bitmaps of 32 transactions. This 32-bit number can be represented as 10 digit unsigned integer. Table 3 shows these vertical bitmaps in decimal form with W=3 3.3 Subset Generations, and Itemset Pruning As our algorithm follows a top down approach, we start with generating all itemsets of size, k equal to Maxlen Our algorithm generates k-itemsets recursively using the items in 1 For the k-itemset generated, we check whether it is subsumed by a previously generated MFS stored in ML. If so, we generate the next k-itemset from 1 and repeat the above procedure. Then we check for infrequency of subsets for a k-itemset in three steps. In the first step, we check whether all 2-itemsets present in kitemset are frequent. If not, generate a new k-itemset and repeat all of the above procedures. In the second step, we check whether the k-itemsets subset is infrequent by using the infrequent tree, INF_Tree. If infrequent subsets of a k-itemset are present in INF_Tree, generate a new kitemset and repeat all of the above procedures. In the third step, we evaluate whether the k-itemsets subsets are frequent in a phased manner. The details of phased subset frequency check are as follows 1. In phase 1, we check for the frequency of first 4 and first 6 items present in the k-itemset. If it is found to be infrequent, we add the infrequent subsets to INF_Tree and repeat all of the above procedures 2. In phase 2, we check for the frequency of first 8 and first 10 items present in the k-itemset. If it is found to be infrequent, we add the infrequent subsets to INF_Tree and repeat all of the above procedures 3. In the next phase, we check for the frequency of first 12 and first 14 items present in the k-itemset. Subsequently for first 16 and first 18 items present and so on till the number of items to be evaluated for subset frequency are less than k. If any of the intermediate phases of subset frequency evaluation fails \(that is the subset is infrequent\dd the infrequent subset into the infrequent tree, INF_Tree and repeat all of the above procedures 4. Compute the support for the k-itemset as well as k+1\pass. Add the frequent itemsets to ML if it is not subsum ed by an existing MFS The phased subset frequency evaluation is a heuristics employed to identify the smaller infrequent nodes in the dense domain quickly All of the above procedures are repeated by decrementing k by 2 \(as the support for a k and k+1 itemsets are computed in a single pass\he pseudo-code for Metamorphosis is given in Figure 1. Lines 14 and 15 of the pseudo-code are used when mining Top-K MFS. In Top-K MFS mining, as soon as the total number of MFS crosses the user specified number of MFS \(referred as K\our algorithm stops and outputs the result 3.4 Support Counting The support for an itemset, X k is computed by performing a bit-wise AND operation on all the items in the itemset X k The resulting number is converted to equivalent binary number, say result, and the support is computed by using the position of 1s in result and the TSUP values. For example, support for an itemset {1,3,5} is computed from Table 3 as follows From NewT 1  AND \({1,3,5}, 1\ = 7 & 5 & 7 = 5. Note that logically this operation would be treated in computer as equivalent to 111 & 101 & 111. The equivalent binary notation for 5 would be 101. So, result=101. From position of 1s in result, we can infer that the itemset {1,3,5} is supported by transactions T 1 and T 3 in the original transaction list D. As the support values of transaction list, D is contained in TSUP, the support of itemset {1,3,5} in NewT 1 is the sum of TSUP 1 and TSUP 3 which is 2 From NewT 2  AND \({1,3,5}, 2\ = 0 & 6 & 4 = 0. The equivalent binary notation is 000. So, result=0. As there are no 1s in result, the support of itemset {1,3,5} in NewT 2 is zero Hence, the support of the itemset {1,3,5} from Table 3 is the sum of the above two values, which is equal to 2. The results can be easily verified from Table 1 3.5 Maximality Checking As soon as a k-itemset is found frequent, after checking if it is subsumed by any other MFS, it is written to ML The Maximal List, ML stores the itemset in compressed bit-vector form similar to the example in Table 3 with W=32 4 Experimental Results All our experiments were performed on a 450MHz Pentium-III PC with 64MB of memory, running RedHat Linux 7.2. We use two benchmark datasets for experimentation viz. chess and connect. The characteristics of chess dataset include: 3196 transactions, 75 items and average transaction size of 37. The characteristics of connect dataset include: 67557 transactions, 129 items and average transaction size of 43 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00  2003 IEEE 


4.1 Evaluation of Metamorphosis for Mining All the Maximal Frequent Sets For performance comparisons, we used the original source codes for GenMax provi ded to us by t h eir authors. The timings reported in this paper include all preprocessing costs \(such as time for horizontal-to-vertical conversion in GenMax and transaction collapsing, compressing in Metamorphosis\and doesnt include the output time for displaying MFS for GenMax, dGenMax and Metamorphosis Figure 2 shows the performance comparison of Metamorphosis against GenMax and dGenMax on chess and connect datasets. Note that the graphs are plotted for Metamorphosis Vs GenMax and Metamorphosis Vs dGenMax separately to improve the visibility. It is clearly evident from the graph that Metamorphosis outperforms GenMax at supports above 50% \(cut-off not shown in the graph\nd dGenMax at supports above 71% for the chess dataset. For the connect dataset, Metamorphosis outperforms GenMax at support above 53% \(cut-off not shown in the graph\and dGenMax at supports above 68%. At supports below this point Metamorphosis performs poorly due to the fact that the Longest Maximal pattern size is far below Maxlen and the primary search strategy employed by Metamorphosis is top-down Figure 3 depicts the graphs of NumF1, Maxlen and LM pattern size against different minimum support levels. As can be evident from the graph, the cleft between the NumF1 and Maxlen as well as Maxlen and LM Pattern Size grows faster as the minimum support decreases. The faster growth of the cleft explains why Metamorphosis performs poorly in mining all the MFS at lower support levels So, the real application domain of Metamorphosis is on those datasets for which the MFS are much closer to Maxlen as well as NumF1 4.2 Evaluation of Metamorphosis for Mining TopK MFS on Benchmark Datasets Figure 4 gives the mining time required for mining the Top-100 and Top-200 MFS on chess and connect dataset As can be evident from Figure 4, the time required for mining the Top-100 and Top-200 MFS are significantly lower compared to mining the entire set of MFS Fi g ure 2. Performance Com p arison of Metamor p hosis Vs GenMax and dGenMax on chess and connect  chess 0 5 10 15 20 25 0.95 0 85 0 75 0.65 0.55 Minimum Support Time \(sec Metamo rphosis GenMax chess 0.0 0.1 0.2 0.3 0.4 0.5 0.95 0.85 0.75 0.65 Minimum Support Time \(sec Metamorp hosis dGenMax connect 0 50 100 150 200 0.95 0.85 0.75 0.65 0 55 Minimum Support Time \(sec Me ta m o r phosis GenMax connect 0.0 5.0 10.0 15.0 20.0 0 95 0 85 0 75 0 65 Minimum Support Time \(sec Metamorp hosis dGenMax Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00  2003 IEEE 


Top-K MFS mining holds promise on applications in which the total number of MFS generated is exponentially large and mining the entire set of MFS becomes difficult In general, the characteristics of Top-K MFS include \(a they have much lesser support compared to that of all other MFS, \(b\y can be used for generating large number of association rules with very high confidence especially for large number of items in antecedent of a rule 5 Summary And Future Research Directions In this paper we presented and evaluated Metamorphosis, an algorithm for mining MFS using novel data transformations A hybrid system that invokes the Metamorphosis algorithm at higher support levels and dGenMax at lower support levels can be developed for mining in a real or production environment References 1 Agrawal, R., Imielinski, T., Swami, A., Mining association rules between sets of items in large databases, In proceedings of ACM SIGMOD Conference on Management of Data 1993, pp 207-216  chnic al  report, IBM Almaden Research Center, 1996, Retrieved from http://www.almaden.ibm.com/cs/quest/ in October 2002 3] Agrawal R., Aggarwal C., Prasad VVV., Depth first generation of Long patterns 7th International conference on Knowledge discovery and Data mining 2000  mining long patterns from databases ACM SIGMOD conference on management of data 1998  Calimlim, M., Gehrke, J., MAFIA: A Maximal Frequent Itemset Algorithm for Transactional Databases, In Intl Conf. On Data Engineering 2001  Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers. San Francisco, CA, 2001 Figure 4.  Top-K Maximal Frequent Set Mining Time for chess and connect datasets Figure 3. LM Pattern Mining Range for chess and connect datasets  Top-K Mining for chess 0.0 2.0 4.0 6.0 8.0 10.0 12.0 0.85 0.70 0 55 0 40 0 25 Minimum Support Time \(sec Top-100 Top-200 Top-K Mining for connect 0 5 10 15 20 25 0 9 5 0 80 0  65 0  50 0 3 5 Minimum Support Time \(sec Top-100 Top-200 LM Pattern Mining Range for chess 0 5 10 15 20 25 30 35 40 0 9 5 0.85 0.75 0.65 0.55 Minimum Support Number of Item s NumF1 Max len LM Pattern Size LM Pattern Mining Range for connect 0 5 10 15 20 25 30 35 40 0  95 0 8 5 0  75 0.65 0  55 Minimum Support Number of Item s NumF1 Max len LM Pattern Size Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00  2003 IEEE 


 dem, Z M., Pincer Search A new algorithm for  discovering the maximum frequent set Intl Conf. on Extending database technology 1998  cessing for Association Rule Mining, Working Paper Series 2002-19, Indian Institute of Management, Lucknow, 2002  r Mining Maximal Frequent Sets based on Dominancy of Transactions, To To Appear in Proc. of Intl. Conf. on Enterprise Information Systems Angers, France, 2003  i, M.J., Gouda, K., Fast vertical mining using diffsets  RPI Technical Report, 01-1, 2001 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00  2003 IEEE 


121 T. Y. Lin  The Power and Limit of Neural Networks   Proceedings of the 1996 Engineering Systems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 13] T. Y. Lin and N. Cercone, Rough Sets and Data Mining Analysis of Imprecise Data, T. Y. Lm and N. Cercone eds 2nd print Foundation of Database Mining  in: the Proceedings of 9   International Conference, RSFDGrC 2003, Chongqing China, May 2003, Lecture Notes on Artificial Intelligence LNAI 2639, Springer-Verlag, 403-405 15]. Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 I61 A. Barr and E.A. Feigenbaum, The handbook of Artificial Intelligence, Willam Kauhann 1981 7] T. Y. Lin  Deductive Data Mining: Mathematical 8] Tsau Young Lin  Data Mining: Granular Computing IO] T. Y. Lin  Data Mining: Granular Computing 14] T. Y. Lin  Deductive Data Mining: Mathematical 62 pre></body></html 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


