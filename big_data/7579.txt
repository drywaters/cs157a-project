Softwar e V&V Support by Parametric Analysis of Large Software Simulation Systems  Johann Schumann 1  Karen Gundy-Burlet 2  Corina P 013 as 013 areanu 3  Tim Menzies 4  and Anthony Barrett 5 1 RIACS NASA Ames Johann.M.Schumann@nasa.gov 2 NASA Ames Karen.Gundy-Burlet@nasa.gov 3 CMU NASA Ames corina.s.pasareanu@nasa.gov 4 Lane CS  EE West Virginia University tim@menzies.us 5 
Jet Propulsion Laboratory California Institute of Technology barrett@jpl.nasa.gov Abstract\227 Modern aerospace software systems simulations usually contain many dependent and independent parameters Due to the large parameter space and the complex highly coupled nonlinear nature of the different system components analysis is complicated and time consuming Thus such systems are generally validated only in regions local to anticipated operating points rather than through characterization of the entire feasible operational envelope of the system We have addressed the factors deterring such a comprehensive analysis with a tool to support parametric analysis and envelope assessment a combination of advanced Monte Carlo generation with n-factor combinatorial parameter varia 
tions and model-based testcase generation is used to limit the number of cases without sacri\002cing important interactions in the parameter space For the automatic analysis of the generated data we use unsupervised Bayesian clustering techniques AutoBayes and supervised learning of critical parameter ranges using the treatment learner TAR3 This unique combination of advanced machine learning technology enables a fast and powerful multivariate analysis that supports 002nding of root causes 1 I NTRODUCTION The design and development of complex systems like Aerospace applications heavily depend on simulation Only by exploring detailed simulation models the exact behavior of the system can be analyzed in nominal and off-nominal 
scenarios These systems simulations often contain a large number of dependent and independent parameters The analysis of such simulation systems is very complicated and timeconsuming due to a large parameter space and the complex highly coupled nonlinear nature of the different system components Still these parameter settings need to be scrutinized during veri\002cation and validation V&V as those parameters contribute to the performance of the aerospace system and can affect its safety Typically the performance of an aerospace vehicle simulation depends on a large number of vehicle hardware parameters including center of gravity mass moment of inertia propulsion system environment as well as control system parameters such as gains and deadbands A major task is to establish safe ranges for control 
system parameters that ensure a safe on-target landing given the expected variation in vehicle performance Exhaustive ex1978-1-4244-2622-5/09/$25.00 c r 2009 IEEE IEEEAC Paper 1316 Version 4 Updated 01/17/2009 ploration of all parameter combinations is infeasible for such a complex system so traditionally parameters are randomly sampled from a de\002ned distribution for a statistically signi\002cant number of runs Monte Carlo testing Vast amounts of data can be generated that way and manual inspection of this data is usually con\002ned to gross features of the solution such as absolute compliance with requirements Valuable trend and parameter sensitivities are often ignored and anomalous 
or unexpected data can easily be overlooked Thus such systems are generally validated only in regions local to anticipated operating points rather than through characterization of the entire feasible operational envelope of the system Researchers have recognized that 223designers must be able to examine various design alternatives quickly and easily among myriad and diverse con\002guration possibilities 224 The number of con\002guration possibilities can be dauntingly large A model with only 20 binary choices already has 2 20  1  000  000 possible con\002gurations far beyond the capability of human comprehension Accordingly since 2000 we ha v e e xplored sampling 
those con\002gurations at random running the resulting model scoring the output with some oracle and then using data mining techniques to 002nd the con\002guration options that most improve model output In this paper we describe a new combination of methods and tools to study the con\002guration parameters and safety envelopes for large software simulations 017 An n-factor combinatorial test vector generation algorithm to target tests toward regions of the parameter space where interactions among parameters are key to performance 017 Model-based generation of testcases allows the analyst to speci\002cally drive simulations toward the execution of interesting states in the mode logic 017 The TAR3 minimal contrast set learner is a supervise d 
learning method that returns the minimal deltas between desired outcomes hitting the target and all the other outcomes 017 EM clustering algorithms that are autogenerated by the A UTO B AYES program synthesis tool Clustering is an unsupervised learning method that obtains the most probable estimates for class frequency and governing parameters It was found that the combination of methods yielded more information than any method used in isolation The operation of each algorithm can be intelligently informed and usefully constrained by using the output of the other Combi 


2 natorial test exposes interactions between parameters TAR3 focuses the analysis on a small number of variables while A UTO B AYES reveals structures missed by TAR3 The rest of this paper is structured as follows In Section 2 we will introduce the MCB project and the HTV Hover Test Vehicle which is used in this paper to illustrate our approach Section 3 gives a high-level description of our tool architecture which will be discussed in Section 4 generation of the simulation scenarios and Section 5 data analysis Section 6 presents results of the HTV analysis to determine best parameter ranges for such tasks as safely hovering and landing the hover test vehicle Section 7 concludes and discusses future work 2 M ODULAR C OMMON B US The Modular Common Bus MCB project is being developed as a family of small low-cost spacecraft composed of modular components The intent is that science instruments are solicited that meet the spacecraft's mass and power budget rather than the current practice of designing a custom spacecraft for each new science instrument The modular design includes components that enable a range of missions including orbital and lander type missions Simulink/Matlab R r is used to rapidly prototype the design of the 003ight software and autogenerate the on-board software for the vehicle This process also enhances the validation and veri\002cation V&V of the vehicle requirements and 003ight software An initial prototype of the MCB lander con\002guration named the Hover Test Vehicle HTV has been developed and 003own in order to assess this spacecraft development methodology The HTV is a prototype of a lunar landing con\002guration The vehicle is comprised of a main payload module and propulsion module For earth-based testing the propulsion module contains a pair of scuba tanks that power one main and 6 attitude control system thrusters The payload module contains an inertial measurement unit IMU avionics and communications equipment An image of the HTV is shown in Figure 1 see for a complete description The data mining techniques discussed in this paper have been applied to the HTV simulation It contains models of the vehicle including 6DOF dynamics propulsion system effectors sensors and avionics Parametric analysis of the system was performed in order to assess 003ight readiness and system margins in preparation to the 003ight tests Numerous parameter variations are considered in order to determine the resilience of the algorithms to dispersions in mass properties orientation and tank pressures Other analyses were performed in order to assess optimal controller settings given the expected dispersions in vehicle con\002guration Requirements on the behavior of the model are encoded in order to accumulate failure data and all solutions are rated relative to their adherence to requirements such as soft landing and minimum excursion from the target point The data mining Figur e 1  Hover Test Vehicle techniques discussed here are intended to characterize the operational envelope of the simulation and to 002nd the ranges of parameters that lead to the lengthiest 003ights that lands close to the target The margin between the best ranges to the failure points of each parameter can then be determined 3 T OOL A RCHITECTURE Our tool Figure 2 is centered around the simulation environment currently the tool interfaces with the NASA Trick simulation environment and with Mathwork's Simulink/State\003ow The user sets up the speci\002cations of the desired parameter variations and their statistical properties and provides high level models e.g UML statecharts Once the test-suite is generated simulation runs are being executed and the resulting data saved Data analysis and visualization is controlled via a Matlab graphical user interface In the following sections we will describe the main components of this tool mechanisms to generate the simulation cases and tool support for the analysis of the simulation data We will then present some results on experiments with the HTV simulation 


3 Figur e 2  Tool architecture 4 G ENERATION OF S IMULATION C ASES Due to the inherent complexity a combinatorially exhaustive parameter exploration is not feasible for a realistically large system We therefore use a combination of extended Monte Carlo random generation with n-factor combinatorial exploration and model-based testcase generation Covering the Option Space We have explored two approaches toward covering the option space a standard Monte Carlo and a 3-factor combinatorial technique The standard Monte Carlo approach is simplest It generates parameters for a simulation run by randomly selecting from user de\002ned probability distributions such as Gaussian or Uniform The main drawback of this approach is a lack of any coverage guarantee resulting in the need to run a large number of simulations to attain a given level of user con\002dence Unlike the standard Monte Carlo approach the combinatorial approach makes a coverage guarantee while attempting to perform a minimal number of simulations In the case of an n-factor combinatorial the guarantee is that any setting of any n discrete parameters appears in at least one of the simulations For instance a 2-factor combinatorial test suite for 20 binary parameters has only 11 tests which is much less than the million tests needed to exhaustively cover every combination A 3-factor case over the same parameters only increases the number of tests to 26 While the number of tests performed using the combinatorial approach is minuscule compared to exhaustive testing anecdotal evidence suggests that this small number of tests is enough to catch most coding errors 5 6 The underlying premise behind the combinatorial approach can be captured in the following statements 017 The simplest failures in a program are triggered by a single input parameter 017 The next simplest failures are triggered by an interaction of two input parameters 017 Progressively more obscure failures involve interactions between more parameters 017 Exhaustive testing involves trying all combinations of all inputs So errors can be grouped into families depending on how many parameters need speci\002c settings to exercise the error The n-factor combinatorial approach guarantees that all errors involving the speci\002c setting of n or fewer parameters will be exercised by at least one test When real-valued parameters have to be varied each parameter is represented as a partition of discrete ranges When turning a computed test into a simulation run each range is replaced by a real value chosen from a uniform distribution across that range The result is a multidimensional space of simulation runs that projects down to a uniform distribution on any plane of input parameters There is a number of algorithms in the literature e.g to generate 2-factor combinatorial test suites Our algorithm is a generalization of IPO and ha s additional features that a real world test suite generator would need These features include the ability to explicitly include particular combinations explicitly exclude particular combinations require n-factor combinatorial coverage of speci\002c subsets of parameters and tie the existence of particular parameters to the setting of another parameter The resulting algorithm is 1041 lines of documented Java code Even with these extra capabilities the algorithm generates test suites that are comparable to those generated by the more restricted systems in the literature and are generated very rapidly as shown in Table 1 Times are measured on a 400MHz Windows laptop machine Model-based Scenario Generation The generation of simulation scenarios discussed so far focuses on the exploration of the parameter space In many cases however a systematic exploration with respect to code coverage or modes is important Complex aerospace software usually contains state machines which govern the behavior of the system In order to be able to explore speci\002c modes systematically we combine our generation approach with the automatic model-based generation of simulation cases A model of the relevant system formalized as a Simulink/State\003ow diagram or a UML statechart is used as the speci\002cation After a translation of the diagram into an in 


4 Problem Sizes T ests T ime 3 4 031 10 2 9 034 1 sec 3 13 031 10 6 19 034 1 sec 4 1 002 3 39 002 2 35 031 10 29 29  1 sec 10 20 216 1 sec 3 1000 031 10 477 48 22 sec Table 1  2-factor generation X Y means that there are Y X valued parameters ternal procedural form SPF Symbolic Path\002nder is used to automatically generate testcases that when executed exercise a speci\002c part or state of the state diagram The synergistic combination of model checking Java Path\002nder and symbolic execution of the SPF tool produces ranges of inputs and parameters required to reach the speci\002ed state This information can be used to constrain the Monte Carlo and nfactor combinatorial generation of simulation cases to focus on the exploration of important modes e.g hover landing 5 D ATA A NALYSIS T OOLS  M ETHODS In this section we will discuss two tools to analyze the multivariate data set produced by the simulation runs TAR3 a minimal contrast set learner to study the in\003uence of input variables and model parameters on the outcome of the simulation and the A UTO B AYES tool to automatically generate ef\002cient algorithms for unsupervised clustering of multivariate data TAR3 Treatment Learning Multi-Dimensional Optimization\227 BORE short for b est o r r e st  takes instances scored on multiple utilities as input and classi\002es each of them 223best\224 or 223rest\224 BORE maps the instance outputs into a hypercube which has one dimension for each utility BORE normalizes instances scored on N dimensions into 223zero\224 for 223worst\224 and 223one\224 for 223best\224 The corner of the hypercube at 1  1     is the apex of the cube and represents the desired goal for the system All examples are scored by their normalized Euclidean distance N i to the apex Usually a domain-speci\002c distance function e.g deviation from desired landing spot is used For each run i of the simulator the n outputs X i are normalized to the range 0    1 as N i   X i 000 min X   max X  000 min X   The Euclidean distance of f N 1  N 2   g to the ideal position of f N 1  1  N 2  2   g is then computed and normalized to the range 0  1 as W i  1 000 q  N 2 1  N 2 2      n where higher W i  0 024 W i 024 1  correspond to better runs This means that the W i can only be improved by increasing all of the utilities To determine the 223best\224 and 223rest\224 values all the W i scores are sorted according to a given threshold BEST The top BEST are then classi\002ed as 223best\224 and the remainder as 223rest\224 Treatment Learning with TAR3\227 Once BORE has classi\002ed the data into 223best\224 and 223rest\224 a data miner is used to 002nd input settings that select for the better outputs This study uses the TAR3 data miner at it returns the smallest theories that most effect the output This means that TAR3 tries to determine a minimal set of model parameters which have the most in\003uence on the behavior of the simulation system The inputs to TAR3 is a set of training examples E  Each example maps a set of attribute ranges to some class symbol i.e f R i  R j      C g  The class symbols C 1  C 2     are stamped with some utility score that ranks the classes f U 1  U 2      U C g  Within E  these classes occur at frequencies F 1  F 2      F C where P F i  1  A 223treatment\224 T of size M is a conjunction of attribute ranges f R 1  R 2     R M g  For a subset e 022 E that is consistent with the treatment the classes occur at frequencies f 1  f 2      f C  TAR3 seeks the smallest treatment T which induces the biggest changes in the weighted sum of the utilities times frequencies of the classes Formally this is called the lift of a treatment lift  P C U C f C P C U C F C Classes in treatment learning get a score U C and the learner uses this to assess the class frequencies resulting from applying a treatment i.e applying constraints to the inputs In normal operation a treatment learner does controller learning that 002nds a treatment which selects for better classes and reject worse classes By reversing the scoring function treatment learning can also select for the worse classes and reject the better classes This mode is called monitor learning since it 002nds the thing we should most watch for Formally treatment learning is a weighted-class minimal contrast-set association rule learner The treatments are associations that occur with preferred classes These treatments serve to contrast undesirable situations with desirable situation where more of the outcomes are favorable Treatment learning is different to other contrast set learners like STUCCO since those other learners don t focus on minimal theories Conceptually a treatment learner explores all possible subsets of the attribute ranges looking for good treatments Such a search is infeasible in practice so the art of treatment learning is quickly pruning unpromising attribute ranges This study uses a version of the TAR3 treatment learner with stochastic search AutoBayes A well-known method to 002nd structure in large sets of data is to perform clustering Clustering is an unsupervised learning method that tries to estimate class membership matrix and 


5 model mog as Multivar Mix of Gaussians const int D as number of variables const int N as number of data points const int C as number of classes with 1  C with C  N double phi\(1..C as class probabilities with 1  sum\(_i  1..C phi\(_i double mu\(1..D 1..C sigma\(1..D 1..C output int c\(1..N as latent variable c\(_ 230 discrete\(phi data double x\(1..D 1..N x\(_i,_j 230 _i,c\(_j max pr\(x f phi mu sigma g  wrt f phi mu sigma g  Figure 3  AutoBayes speci\002cation for mixture model Keywords are underlined class parameters only given the data A variety of algorithms can be used for example the EM-algorithm A number of EM-implementations is available e.g Autoclass EMMIX or MCLUST 19 and could be used for this problem However in order to re\002ne the statistical model e.g by incorporating other probability distributions for certain variables or to introduce domain knowledge the EMalgorithm needs to be modi\002ed substantially for each problem variant making experimentation a time-consuming and error-prone undertaking Thus we are using A UTO B AYES tool to produce customized variants of the EM-algorithm A UTO B AYES  14 is a fully automatic program synthesis system that generates ef\002cient and documented C/C code from abstract statistical model speci\002cations From the outside A UTO B AYES looks similar to a compiler for a very high-level programming language it takes an abstract problem speci\002cation in the form of a Bayesian statistical model and translates it into executable C/C code that can be called from Matlab On the inside however A UTO B AYES works quite different A UTO B AYES 002rst derives a customized algorithm skeleton implementing the model and then transforms it into optimized C/C code Hereby the input speci\002cation is translated into a Bayesian Network The program synthesis system uses a schema based approach to break down large problems into statistically independent subproblems and tries to solve them symbolically If no solution can be found a customized numerical algorithm is instantiated This task is heavily supported by a domain-speci\002c schema library an elaborate symbolic subsystem and an ef\002cient rewriting engine After optimization C or C code is generated for various platforms e.g embedded systems Matlab Simulink or Octave For our experiment we used AutoBayes to generate code that can be called from Matlab as a MEX function The basic statistical model used for this study describes the properties of the data in a fully declarative fashion for each data double x\(1..D 1..N data double flg\(1..D 1..N data double arr\(1..D 1..N x\(_i,_j 230 _i,c\(_j flg\(_i,_j 230 discrete\(rho_flg\(_i,c\(_j arr\(_i,_j 230 poisson\(rate\(_i,c\(_j max pr f x flg arr g  f phi mu sigma p_flg rate g  wrt f phi mu sigma p_flg rate g  Figure 4  Speci\002cation for mixture model with different probability density functions problem variable of interest i.e observation or parameter properties and dependencies are speci\002ed via probability distributions and constraints Figure 3 shows how Gaussian mixture model with diagonal covariance matrices can be represented The model assumes that the data consists of N points in D dimensions such that each point belongs to one of C classes the 002rst few lines of the speci\002cation just declare these symbolic constants and specify the constraints on them Each point x\(1..C j where  cor responds to Matlab's subrange operator  and i  j are inde x variables is drawn independently from a univariate Gaussian with mean mu i,c j and standard deviation sigma i,c j  The unknown distribution parameters can be different for each class and each dimension hence we declare them as matrices The unknown assignment of the points to the distributions i.e classes is represented by the latent variable c  since we are interested in the classi\002cation results as well and not only the distribution parameters c is declared as output  c is distributed as a discrete distribution with the relative class frequencies given by the also unknown vector phi  Since each point must belong to a class the sum of the probabilities must be equal to one Finally we specify the goal inference task maximizing the conditional probability pr\(x f phi mu sigma g  with respect to the parameters of interest phi  mu  and sigma  This means that we are interested in a maximum likelihood estimate MLE of the model parameters However not all simulation variables are Gaussian distributed e.g boolean values or angles  0    2 031  Although in most engineering applications these values are converted to Gaussians e.g by adding noise the A UTO B AYES system automatically handles mixtures with different probability density functions All that needs to be done to the speci\002cation in Figure 3 is to add declarations for the distribution parameters the additional data variables and the distribution and goal statement\227Figure 4 shows a speci\002cation snippet For details see 6 R ESULTS For the experiment shown below we used standard Monte Carlo and three-factor combinatorial techniques For the Monte Carlo cases one thousand cases were run with random values chosen for each of the input parameters from their respective probability distributions Here the parame 


6 ter ranges are determined such that they are likely to include failure cases Figure 5  HTV trajectories for 1000 simulation runs with different simulation parameters The output data encompass a wide range of variables that are saved at each iteration point Thus they are actually time series data over a large number of variables Figure 5 shows a 3D representation of 1000 simulated trajectories generated through a traditional Monte Carlo style test generation technique 1 Data representing key parameters such as maximum altitude and excursion time of 003ight and landing velocities were extracted from the simulation for the analysis that follows After preprocessing we obtained a data set with 10 dimensions These normalized data then were clustered using the Matlab/C code as was generated by A UTO B AYES 790 lines of C code The generated data analysis algorithm determined that with 10 classes a good separation can be achieved The results of clustering relative to time of 003ight and initial wet mass is shown in Figure 6 Different colors indicate into which class a speci\002c simulation run falls The classes are ranked using a penalty function based on landing velocity and position error Blue indicates the lowest penalty function while red indicates the highest penalty function The trend seen in the 002gure is that lower times of 003ight generally have lower penalty functions Wet mass is key to time of 003ight but other factors contribute to the failure clusters exhibited in the plot Here the yellow through red classes show repeated violations of the landing requirements Overall statistics for the compliance with performance requirements was over the 1000 runs 017 Maximum Position Excursion no greater than 3m 258 cases failed 017 Vertical Velocity on Landing no greater than 4.0m/s 167 failed 1 In this paper colors encode the class membership as determined by the A UTO B AYES clustering algorithm 017 Horizontal velocity on Landing no greater than 1.0m/s 49 failed Some of the cases failed more than one of the requirements so the total number of cases without failure was 656 This simulation exhibited a suf\002cient range of acceptable/failure cases such that the key parameter values de\002ning the failure front could be determined Figur e 6  Relationship between Time of Flight and Initial Wet Mass Colors indicate class membership To determine the root cause of the poor landing clusters TAR3 was utilized to determine the key parameters driving the simulation behavior It identi\002ed several key parameters that in conjunction with each other drove the failure behavior In particular TAR3 identi\002ed the following combined set of parameter ranges that induced failure 017 MAS cgy location wet=0.005951   0.007885 017 MAS Iyz wet=-0.961177   0.835669 017 MAS wet=68.826103   69.489502 017 INI rotx=0.928976   2.996930 This indicates that the initial mass y center of gravity in combination with one the Iyz moment of inertia and the initial x rotation of the vehicle would cause unacceptable behavior This 002nding introduced a set of day-of-\003ight 003ight rule restrictions on initial orientation and a more careful evaluation of the center of gravity of the vehicle It was determined that reduced mass in the tanks would lead to a lower y-cg offset so a reduced tank pressure was also utilized for the 002rst freehover test 003ight Figure 7 shows contours of 223likelihood\224 for two parameters identi\002ed as critical by TAR3 To generate this plot the input domain was discretized and the likelihood of success was computed for each cell Here likelihood combines the overall probability of success in the whole population ratio of success in the local cell and local cell population Likelihood will approach one in well-populated cells with a high ratio 


7 of success and will approach zero if either there is poor statistical support or a low ratio of success If a variable is not correlated then the local likelihood will approach the overall ratio of success in the complete population Using this metric it is seen that choosing a lower mass and carefully orienting the vehicle improves the resilience of the simulation to dispersions in other key parameters Figur e 7  Likelihood of successful 003ight relative to initial x rotation and mass 7 C ONCLUSIONS In this paper we have explored a combination of two learners A UTO B AYES and TAR3 to explore the internal state space of some 003ight guidance software with combinatorial test techniques The combination of these technologies revealed features that would have been invisible for state of the practice Further the experiment suggests some novel ways that these technologies could usefully augment each other In the case of the 002rst test 003ight of the hover test vehicle the combined technologies provided guidance as to strategies that would increase the chances of a successful test 003ight In fact the hover test vehicle performed in the manner suggested by the simulation and a successful set of free-\003ying test 003ights were conducted More generally given the growing importance of modelbased reasoning in software engineering the ability to use data miners to 002nd and constrain the most important parts of our software models should prove to be a technique of growing importance in the years to come A CKNOWLEDGMENTS This research was sponsored in part by NASA under the ISRDS Contract NNA07BB97C RIACS/USRA Part of this work was performed at the Jet Propulsion Laboratory California Institute of Technology under a contract with NASA R EFERENCES  Menzies T and Sinsel E Practical large scale whatif queries Case studies with software risk assessment In Proceedings ASE 2000  2000 Available from http menzies.us/pdf/00ase.pdf  Gray  J Lin Y  and Zhang J Automating change evolution in model-driven engineering IEEE Computer  39\(2 February 2006  Bell J E A Ho v er testing of a prototype small planetary spacecraft In Submitted to IAC'08 the 59th International Astronautical Congress Glasgow  2008  Menzies T  and Hu Y  Data mining for v ery b usy people In IEEE Computer  November 2003 Available from http menzies.us/pdf/03tar2.pdf  Dunietz I S Ehrlich W  K Szablak B D Mallo ws C L and Iannino A Applying design of experiments to software testing experience report In Proc ICSE 97  pages 205\226215 1997  W allace D R and K uhn D R F ailure modes in medical device software an analysis of 15 years of recall data International Journal of Reliability Quality and Safety Engineering  8\(4 2001  Mats Grindal Jef f Of futt S F  A Combination testing strategies a survey Software Testing Veri\002cation and Reliability  15\(3 2005  T ai K and Lie Y  A test generation strate gy for pair wise testing IEEE Transactions on Software Engineering  28\(1 2002  Czerw onka J P airwise testing in real w orld practical extensions to test case generators In Proceedings of 24th Paci\002c Northwest Software Quality Conference  2006  D Cohen S Dalal J P arelius and G P atton 223The combinatorial design approach to automatic test generation,\224 Software IEEE  13\(5 Sep 1996  C S P 013 as 013 areanu J Schumann P Mehlitz M Lowry G Karsai H Nine S Neema Model Based Analysis and Test Generation for Flight Software In preparation 2009  Bay  S and P azzani M Detecting change in cate gorical data Mining contrast sets In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining  1999  C S P 013 as 013 areanu P C Mehlitz D H Bushnell K Gundy-Burlet M Lowry S Person and M Pape 223Combining unit-level symbolic execution and systemlevel concrete execution for testing NASA software,\224 in Proc of ISSTA  2008  J Schumann H Jaf ari T  Pressb ur ger  E Denne y  W Buntine and B Fischer AutoBayes Program Synthesis System Users Manual NASA/TM-2008-215366 2008  B Fischer and J Sc h um ann 223 AutoBayes A system for 


8 generating data analysis programs from statistical models,\224 J Functional Programming  13\(3 2003  Dempster  A P  Laird N M and Rubin D B  Maximum likelihood from incomplete data via the EM algorithm with discussion J of the Royal Statistical Society series B  39:1\22638 1977  Cheeseman P  and Stutz J Bayesian classi\002cation AutoClass Theory and results In Fayyad U M Piatetsky-Shapiro G Smyth P and Uthurusamy R editors Proc 2nd Intl Conf Knowledge Discovery and Data Mining  pages 153\226180 AAAI Press 1996  McLachlan G Peel D Basford K E and Adams P The EMMIX software for the 002tting of mixtures of normal and t-components J Statistical Software  4\(2 1999  Frale y  C and Raftery  A E MCLUST Softw are for model-based clustering density estimation and discriminant analysis Technical Report 415 Department of Statistics University of Washington October 2002  Y  Hu 223T reatment learning 224 masters thesis Uni v ersity of British Columbia 2002  K Gundy-Burlet J Schumann T  Menzies and T  Bar rett 223Parametric analysis of Antares re-entry guidance algorithms using advanced test generation and data analysis,\224 in Proc iSAIRAS  2008  Buntine W  L Operations for learning with graphical models J AI Research  2:159\226225 1994 B IOGRAPHY Dr Johann Schumann PhD 1991 Dr habil 2000 Munich Germany is a Senior Scientist with RIACS and working in the Robust Software Engineering Group at NASA Ames He is engaged in research on veri\002cation and validation of autonomy software and adaptive controllers and on automatic generation of reliable code for data analysis and state estimation Dr Schumann is author of a book on theorem proving in software engineering and has published more than 80 articles on automated deduction and its applications automatic program generation and neural network oriented topics Karen Gundy-Burlet Karen GundyBurlet is a research scientist in the Robust Software Engineering group at NASA-Ames Research Center Her current interests are in the application of advanced information analysis techniques to validation and veri\002cation of simulation models and Guidance Navigation and Control GNC software She was the group lead for the NASA-Ames Intelligent Flight Control program that developed and implemented neural-adaptive algorithms for control of damaged aircraft and demonstrated and evaluated them in full-motion piloted simulations Other research efforts include optimal control of low-thrust spacecraft trajectories and computational modeling of unsteady 003ows in turbomachinery with an emphasis on rotor/stator interaction in compressors and hot streak migration in turbines She has interests in parallel systems and high-performance computing Dr Gundy-Burlet graduated with a B.S in Mechanical Engineering from U.C Berkeley and obtained M.S and Ph.D degrees in Aerospace Engineering from Stanford Corina P 013 as 013 areanu Dr Corina P 013 as 013 areanu is a Research Scientist at the NASA Ames Research Center Robust Software Engineering Group She is working on using abstraction and symbolic execution in the context of software model checking and testing and on automating assume-guarantee compositional veri\002cation She has served on program committees for many meetings in the formal analysis area such as CAV ISSTA FSE and ICSE She is also associate editor for the ACM TOSEM journal More information can be found at http://ti.arc.nasa.gov/people/pcorina  Tim Menzies Dr Tim Menzies has been working on advanced modeling and AI since 1986 He received his PhD from the University of New South Wales Sydney Australia and is the author of over 164 refereed papers A former research chair for NASA Dr Menzies is now a associate professor at the West Virginia University's Lane Department of Computer Science and Electrical Engineering For more information see http://menzies.us  Anthony Barrett Dr Anthony Barrett is a member of the Arti\002cial Intelligence Group at the Jet Propulsion Laboratory California Institute of Technology where his R&D activities involve planning  scheduling plan execution diagnosis and test suite generation applied to spacecraft He holds a B.S in Physics Computer Science and Applied Mathematics from James Madison University and both an M.S and PhD in Computer Science from the University of Washington His research interests are in the areas of planning scheduling and execution/diagnostics in the context of single and multi-agent systems 


9 CMM - Ca pa bility Ma turi ty Mode l. G u ide line s f o r  Improving the Software Process. SEI Series, SEI \(Software Engineering Institute\, 2004  10 IT G o v e rna nc e Institute   CobiT 4.0 EUA, Ilinois: IT Governance Institute, 2005, 207pp  11 m p ie ri, Robe rt o H C o ll a do, Ca rl os F  L u c i o   Pila r B. Metodología de la investigación. México: McGraw-Hill 1991  12 Y i n, R. K  Es tu do de Ca s o  P l a n e j a m e n to e M todos 2  ed. São Paulo: Bookman, 2001  13 H o ppe n  N o rbe r t o e t a l A v a lia  o de a r tig os de pesquisa em sistemas de informação: proposta de um guia In.: ENANPAD, 21, 1997, Rio das Pedras. Anais... Rio das Pedras: ANPAD, 1997  14 Fink  A r le ne T h e s u rv ey ha nd bo ok T h o u s a nd O a k s   Sage Publications, 1995  15  Sa m b am ur thy  V.; Zm ud R  W  A r r a ng em e n ts f o r  information technology governance: A theory of multiple contingencies. MIS Quarterly, Vol.23, No.2 pp.261-290 1999  16 Mo tjol opa ne I; B r ow n, I. Stra te g i c bus ine s s IT alignment, and factors of influence: a case study in a public tertiary education institution. ACM International Conference Proceeding Series, vol. 75, pp. 147-156, 2004  17 L u f t m a n, J A s s e ssing busine ss-IT  a lig n m e n t m a turit y   Communications of the Association for Information Systems 4, 14, 1-50, 2000  1 Kaplan  R S  N o rton  D   P  T h e Balan ced S c o r ecard  Measures That Drive Performance. Harvard Business Review, 70\(1\:72 79, 1992  19 L e e  J.; Hu y nh, M. Q.; K w ok R. C.; P i S. IT outsourcing evolution---: past, present, and future. Commun ACM 46, 5, 84-89, 2003  20  V o as, J. L i m ited So f t ware  W a rranties. Eng i neering of Computer Based Systems, \(ECBS2000\ Proceeding, pp. 5661, 2000  21 Ma  Z.; C o ll of e llo, J.S Sm i t h-Da n ie ls, D.E Im prov ing  software on-time delivery: an investigation of project delays IEEE Aerospace Conference Proceedings, vol.4, pp.421-434 2000  22  Va n Sol i ng e n R  B e rg hou t, E   Va n L a tum  F Interrupts: just a minute never is. IEEE Software, vol. 15 issue 5, pp.97-103, 1998  23  V o a s J  Ce rtif ic a tion: Re d u c ing the hi dde n c o s t  of poor  quality, IEEE Software, pp.22-25, 1999  24 G r iff iths, J.; Els on B.; A m os, D. A c u stom e r-supplie r interaction model to improve customer focus in turbulent markets. Managing Service Quality, 2001, vol. 11\(1\, pp.5766  25 Om a r R.; Za ila ni, S.; Sula im a n M.; Ra m a y a h, T   Supplier involvement, customer focus, supply chain technology and manufacturing performance: Findings from a pilot study. IEEE International Conference on Management of Innovation and Technology, vol.2, pp.876-880, 2006  2  S u rm acz J T u rn o v er is Exp e n s ive CIO M a gazin e  1 5  June 2004  27 B u rn es P.T  V o lu n t ary E m p l o y ee T u ro v e r: W h y I T  Professionals Leave. IT Professional, vol.8 , issue: 3, pp.4648, 2006  28 B h a r a ti P., Be rg D Se rv ic e qua lity f r o m the ot he r side   Information systems management at Duquesne Light International Journal of Information Management, Volume 25, Issue 4, August 2005, Pages 367-380, 2005  29 D e a n J  W  Bow e n, D   E. Ma na g e m e nt the o ry a nd total quality: Improving research and practice through theory development. Academy of Management Review, 3\( 19\, 392418, 1994  30 K u lpa   M. K  J o h n s o n  K  A  Inte rpre ti ng the CMMI A Process Improvement Approach. 2. ed. USA: Auerbach Publications, 2008  31 C u rtis, B.; He f l e y W  E.; Milla r, S. A  T h e P e o p le  Capability Maturity Model: Guidelines for Improving the Workforce. SEI Series in Software Engineering, USA Addison-Wesley Professional, 2001  32 Ca te r-Ste e l A  T a n, W  G   T o le m a n, M. Cha lle ng e of  Adopting Multiple Process Improvement Frameworks Proceedings of the 14th European Conference on Information Systems, 2006  33 Mi ng a y S.; Bitting e r, S. Com bine CobiT a nd IT I L  f o r  Powerful Governance. Gartner Inc, 2006  34 Sallé, M. IT Serv ice Ma n a g e m e n t an d IT G o v e rn an ce  review, comparative analysis and their impact on utility computing: Hewlett-Packard Company, 2004  3 E d gem a n  R L   Bi gi o  D   F e rl eman  T  S i x S i g m a an d  Business Excellence: Strategic and Tactical Examination of IT Service Level Management at the Office of the Chief Technology Officer of Washington, DC. Quality and Reliability Engineering International, vol. 21, 2005   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


0 200 400 600 800 1000 1200 Days 24h since Rover 1 440 nm Rover 1 880 nm Figure 12 Daily values of the atmospheric constant T 8 CONCLUSIONS This paper has described the Pancams EDL Rover 2 440 nm 2.5 T 1.5 0 200 400 600 800 1000 1200 Days 24h since EDL Rover 2 880 nm 2.5 EDL 2.5T 2 on the MER on the horizon was carried out at the Jet Propulsion Laboratory California Institute of Technology under on all was conducted Based means that the cover glass On never get high are significant ghost images of the or imply its endorsement by the United States Government or the Jet Propulsion Laboratory California Institute of Technology REFERENCES 1 URL Jet Propulsion Laboratory Pasadena US vers.,html cited 12/29/2004 2 J F Bell III et al Pancam Multispectral Imaging Results from the Opportunity Rover at Meridiani Planum Science Vol 306 Issue 5702 1703-1709  3 December 2004 3 G.H.Smith E.C.Hagerott L.M.Scherr K.E.Herkenhoff J.F.Bell III Optical designs for the Mars 03 sun close to the horizon is presented It shows that locations rover 2 880 nm the ghost contains 10 of the primary image intensity A future mission to Mars Mars Science Laboratory will potentially land at high latitudes rover cameras Proceedings of SPIE Vol 4441-14 4 A Eisenman et al Mars Exploration Rover Engineering Cameras SPIE Proceedings of International Symposium sun Also on the Martian surface During Martian winter time this a contract with the National Aeronautics and Space Administration References herein to any specific commercial product process sun in the pictures caused by reflections in the sun will sun images an image of the can be similar brightness sun images A number of representative images have been presented It is observed that there sun near the horizon in order to obtain attitude information An example of EDL 2.5 0 200 400 600 800 1000 1200 Days 24h since rovers that acquire the sun and the position measurement processing of over the horizon and the pancams will have to image the as the an investigation to determine the atmospheric constant of the Martian atmosphere in the 440 nm and the 880 nm band was established to be 0.9 A seasonal change and dust storms in u is observed ACKNOWLEDGEMENT The research described here or service by trademark manufacturer or otherwise does not constitute on 10 0 200 40D 600 24h sin 1200 Days 24h since 


Remote Sensing 17 21 September 2001 Toulouse France 5 A Eisenman C.C.Liebe R.Perez Sun Sensing on the Mars Exploration Rovers Proceedings of the 2002 IEEE Aerospace Conference Volume 5 9-16 March 2002 Pages 5-2249 5-2262 vol.5 6 C.C.Liebe accuracy Performance of Star Trackers A Tutorial IEEE Transactions on Aerospace and Electronic Systems AES volume 38 No 2 April 2002 pp 587 599 7 Howard A Perko John D Nelson and Jacklyn R Green Review Of Martian Dust Composition Transport Deposition Adhesion And Removal Proceedings of Space 2002 a biannual conference of the American Society of Civil Engineers _ Dr Lawrence Scherr is an  l 2-li designer He received an MS.S in _  Il _  Physics from university of  received his Ph.D in biophysics from U C Berkeley in 1980 He   I ranalyzed stray light on an enjoyable variety of optiCS Tese include prototype medical instruments intraocular lenses scaterometers large surveillance telescopes automated optical test systems and Mars camera lenses 8 Geoffrey A Landis Materials Adherence Experiment Results From Mars Pathfinder Proceedings of the 26th IEEE Photovoltaic Specialists Conference 1997 IEEE NJ 1997 pp 865-869 ISBN 0-7803-3767-0 9 C.C.Liebe L.Scherr R Wilson Sun Induced Veiling Glare in Dusty Camera Optics SPIE Optical Engineering Febuary 2004 Vol 43 No 2p 493-499 Biographies Dr Carl Christian Liebe received the M.S.E.E degree in 1991 and the Ph.D degree in 1994 from the Department of Electrophysics Technical University of Denmark Since 1997 he has been an employee at the Jet Propulsion Laboratory California Institute of Technology Currently he is a senior member of the technical staff in the Active Optical Sensing Group He has authored/co-authored more than 50 papers Jim Alexander received an A.B from U.C Berkeley and an M.A and C Phil from UCLA Since 1983 he has been heavily involved at JPL in star tracker and scanner testing analysis requirements scene simulation calibration algorithm design and implementation for missions such as the Astro-1 shuttle experiment Mars Pathfinder and the Cassini spacecraft Currently he is the MSL lead for surface GNC and the supervisor for the Precision Motion Control and Celestial Sensors group at JPL 11 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es — the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


