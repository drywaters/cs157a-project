LEVERAGING SOCIAL MEDIA FOR TRAINING OBJECT DETECTORS E Chatzilari S Nikolopoulos I Kompatsiaris Informatics and Telematics Institute ITI CERTH GR-57001 Greece email f ehatzi nikolopo ikom g iti.gr E Giannakidou A Vakali Department of Informatics Aristotle University 54124 Thessaloniki Greece email f eirgiann avakali g csd.auth.gr ABSTRACT The fact that most users tend to tag images emotionally rathe r than realistically makes social datasets inherently awed from a computer vision perspective On the other hand they can be particularly useful due to their social context and their po tential to grow arbitrary big Our work shows how a combination of techniques operating on both tag and visual information spaces manages to leverage the associated weak annotation s and produce region-detail training samples In this direct ion we make some theoretical observations relating the robustness of the resulting models the accuracy of the analysis al gorithms and the amount of processed data Experimental evaluation performed against manually trained object dete ctors reveals the strengths and weaknesses of our approach Index Terms  Social media object detection weak annotations Flickr 1 INTRODUCTION Semantic object detection is one of the most useful operatio ns performed by human visual system and constitute an exciting problem for computer vision scientists Robust models capa ble of capturing the diversity of an object's form and appear ance need to be learned from a large number of highly descriptive training examples However current literature had showed us that such examples are not existent and therefore very expensive to obtain In this perspective semantic object detection can be viewed as a problem of either supervised 1   2    3    4  o r unsupervised learning 5   6    7    8    9    1 0    1 1    12  13   1 4   I n t h e  r s t c a s e a c l a s s i  e r i s t r a i n e d t o r e c o g n i ze an object category e.g a face 1   4   a b u i l d i n g  2  o r a c a r 3  u s i n g a s e t o f h a n d l a b e l e d t r a i n i n g i m a g e s  T h e d r a w back of these schemes is that they require a large amount of strongly annotated images the generation of which is a labo rious and time consuming procedure To tackle this issue th e methods resorting to unsupervised learning attempt to solv e the problem by using weakly annotated training examples In this case the idea is to estimate a joint probability distri bution on a space of semantic labels and visual characteristics A high number of diverse ideas has been proposed in the literature for this purpose In 15   1 3  t h e p r o b l e m i s viewed as a top-down image segmentation procedure where the recognition of visual objects is incorporated as an inte rmediate step of segmentation Aspect models like probabili stic Latent Semantic Analysis pLSA 7   1 6  a n d L a t e n t Dirichlet Allocation LDA 17   1 8  h a v e b e e n u s e d w i t h weakly annotated datasets to estimate the joint probabilit ies between semantic labels and visual features In some cases these models are coupled with conditional random elds 12  19 t o i n c o r p o r a t e s p a t i a l a n d h i e r a r c h i c a l i n f o r m a t i o n o riginating from context or use Probabilistic Graphical Model s PGM 11 t o c o n s i d e r t h e r o l e o f s t r u c t u r e w i t h i n t h e d e t e c tion process Other techniques that also rely on observati ons statistics to estimate these joint probabilities include 10   8  where Expectation Maximization is employed and 9 w h e r e stochastic processes are used Some pioneer work in this direction has been presented in 5 w h e r e m u c h i n f o r m a t i o n is learned from a handful of images by taking advantage of knowledge coming from previously learned categories and 6 w h e r e t h e a d v a n t a g e s o f s u p e r v i s e d a n d u n s u p e r v i s e d a p proaches are combined by solving a multiclass classicatio n problem This work concentrates on social media and their potential to serve as the training examples of an object detection scheme Social sites like ickr accommodate image corpora that are being populated with hundreds of user tagged images on a daily basis We are interested on whether such corpora can be leveraged to facilitate the robust estimation of models By looking at the literature above we realize that most of the proposed schemes have been tested on purpose specic datasets For instance 5   1 8    7    1 0   a r e e v a l u a ted using the Caltech dataset which is a set of images manually organized in categories while 16   2 0  o p e r a t e s o n i m a g e s collected from the web using key-word based search Similarly 6   9    8    2 1    1 3  u s e t h e C o r e l d a t a s e t  w h i c h i s a set of images annotated with realistic tags while 11   1 2   18 o p e r a t e o n M i c r o s o f t R e s e a r c h d a t a b a s e w h i c h i s a s e t of strongly annotated images Few are the attempts where object detection schemes exploit social data as in 22   2 3  
n	\r\n\n\n\n             n\n 


14 w h e r e p h o t o c o l l e c t i o n s o b t a i n e d f r o m  i c k r a r e u s e d f o r this purpose The advantage of using social sites like ickr is that we can obtain a high number of images without spending much effort or time Consequently as opposed to supervised approaches there is no limitation on the types of objects th at can be trained since social sites accommodate images depic ting a huge variety of objects Our work bears many similarities with 8  w h e r e s e g mentation visual feature extraction and region clusterin g are applied on a set of tagged images to facilitate object detectors training However we examine from both theoretical and experimental perspective the way the robustness of the generated detectors is affected by the relation associatin g the accuracy of the image analysis algorithms with the size of th e processed dataset 2 FRAMEWORK DESCRIPTION The goal of our framework is to start from a set of user tagged images obtained from social sites and automatically extr act training examples suitable for learning an object detecti on model Social media processing segmentation visual features extraction clustering and machine learning constit ute the analysis components incorporated by our framework as shown in Fig 1 We mainly focus on the components of social media processing and clustering with the intention to tack le the reduced amount of supervision foreseen by our framework and the low quality of tags contributed by the social users I n Social Media \r Processing\r Segmentation\r Vis. Features \r Extraction\r Clustering\r Machine\r Learning\r Tag\r-\rbased clustering\r rSocial Knowledge\r rSemantic Knowledge\r Un\r-\rsupervised image \r segmentation\r MPEG\r-\r7 Descriptor extraction \r from image regions\r Region clustering based on \r visual features\r Learn models for recognizing \r specific objects\r Focus of our work\r Fig 1  Analysis components incorporate by our framework our framework we identify six analysis steps that are appli ed consecutively on a set of user tagged images a Cluster images using their tags and acquire image groups each one emphasizing on a particular topic The linguistic descriptio n of this topic is usually reected in the most frequent tag b Pi ck an image group so as its most frequent tag to conceptually relate with the object of interest c Segment all images in the selected image group into regions that are likely to represe nt objects d Extract the visual features of these regions wit h the expectation that all regions representing the same obje ct will share a relative high amount of common characteristics  e Perform feature-based clustering so as to create groups o f similar regions We anticipate that the majority of regions representing the object of interest will be gathered in one o f the clusters pushing all irrelevant regions to the others f Use the visual features extracted from the regions belonging to the cluster representing the object of interest to train a mach ine learning-based object detector Although there are issues to be addressed such as a how to derive image groups with an increased level of semantic coherence b how to determine the number of clusters for the feature-based region clustering procedure and c how t o select the cluster containing the regions depicting the obj ect of interest our great advantage relies on the social aspect of the analyzed dataset and its potential to grow particular ly large It has been shown 24 t h a t t h e m a j o r i t y o f u s e r s t e n d to contribute similar tags when faced with similar type of vi sual content This is attributed to the common background that most users share and is expected to lead the prevailing concepts in tag and visual information space to convergence  Based on this assumption we adopt the following solutions in order to fully automate the aforementioned process Semantically coherent groups of images are generated using a tag-based clustering approach that incorporates both soci al and semantic knowledge detailed in Section 4 The number of clusters for the feature-based region clustering ste p is determined in an un-supervised manner by employing the Maximin algorithm tuned using cross validation as describ ed in Section 4 Finally the most populated of the generated region-clusters is chosen to provide the machine learning a lgorithm with the necessary training examples as explained in Section 3.3 It is evident that selecting the most populated of the generated clusters would certainly constitute the appropr iate choice if all analysis components of computer vision i.e  segmentation discrimination by visual features worked p erfectly However since current literature has shown us that this is not true we examine how the size of the analyzed dataset affects the legitimate error space of the analysis m odules for letting the aforementioned cluster selection to b e the appropriate choice The following section investigates th e issue from a theoretical perspective 3 THEORETICAL ANALYSIS 3.1 Preliminary Denitions  Conventions Table 1 summarizes the notations used throughout the presented analysis Given the diversity characterizing an object's form and appearance both segmentation and visual fe ature extraction are likely to introduce errors in the analys is pipeline of Fig 1 However if we consider that our nal goal is to create clusters of image regions depicting the object o f interest we can accept that all these errors are eventually reected on the efciency of the clustering procedure Thus w e will make the convention that the clustering error incorpor ates all these sources of error 


Table 1  Legend of Introduced Notations Symbol Denition S The complete social dataset N The number of images in S L A particular topic S L An image group subset of S that emphasizes on topic L n The number of images in S L I q An image from S R I q  Segments identied f r I q i  i  1      m g in image I q f d  r I q i   Visual descriptor f f i  i  1      z g extracted from a region r I q i T I q Set of tags associated with image I q C  Set of objects that appear f c i  i  1      t g in an image group S L W  Set of clusters created by the f w i  i  1      o g feature-based clustering algorithm p c i probability that social media processing draws from S an image depicting c i Moreover we will assume that there is a one-to-one relation between an image and an object i.e we do not consider cases where the same object is depicted in two different loca tions of the image 3.2 Social Media processing The goal of social media processing is to cluster images into semantically coherent groups S L  S  We are interested in the frequency distribution of objects c i 2 C appearing in S L based on their frequency rank If we focus on a single image group S L  we can view this process as the act of populating S L with images selected from a large dataset S using certain criteria see Section 4 In this case the number of images in S L that depict the object c i  can be considered to be equal with the number of successes in a sequence of n independent success/failure trials each one yielding success with pro bability p c i  Considering that an image depicts more than one concepts we can say that the probabilities p c i  8 c i 2 C are independent from each other and they depend on the nature of the dataset Given that S is sufciently large drawing an image from this dataset can be considered as an independent trial Thus the number of times an object c i 2 C appears in S L can be expressed by a random variable K following the binomial distribution with probability p c i  In this way we can use the corresponding probability mass function  P r  K  k   depicted in eq 1 to estimate the probability that S L contains k images depicting c i  P r  K  k    n k  p k 1  p  n  k 1 Moreover since the social media processing aims at creating groups of images emphasizing on a particular topic we can assume that there will be an object c 1 that is drawn with c1 c2 c3 c4 c5 0 10 20 30 40 50 60 70 80 90 100 Objects appearances a c1    c2 c1    c2 c1    c2 0 20 40 60 80 100 120 140 160 180 200 Objects appearances n=50 n=100 n=200  df=10 df=20 df=40 b Fig 2  a Distribution of appearances of the objects C in S L  for n=100 and p c 1 0.9 p c 2  0  7  p c 3  0  5  p c 4  0  3  p c 5  0  1  b Difference of populations between c 1  c 2  using different values of n probability p c 1 higher than p c 2  which is the probability that c 2 is drawn and so forth for the remaining c i 2 C  This assumption is experimentally veried in Section 5.1 where the tag-frequency histograms of different image groups are measured Given the above we can use the expected value E\(K of a random variable following the binomial distribu tion eq.\(2 to estimate the number of times an object c i 2 C will appear in S L  if its drawn from the initial dataset S with probability p c i  This is actually the value of k maximizing the corresponding probability mass function E  K   np 2 In this way we are able to estimate how the number of appearances appearances of objects c i 2 C are distributed in S L  based on their frequency rank Fig 2\(a show how such a distribution would look like given that  p c 1  p c 2       Based on this distribution and given the fact that as N increases n will also increase we examine how the population of the generated region clusters relates with the clusterin g error space and n  3.3 Clustering The goal of feature-based region clustering is to group together regions representing the same object Ideally the distribution of clusters population based on their popula tion rank coincides with the distribution of objects appearances based on their frequency rank In this case the most populated cluster w 1 contains all regions depicting the most frequently appearing object c 1  However there is very little chance that we will get perfectly solid clusters each on e containing regions representing a single object Nevertheless given the fact that object models can be robustly learned even from rather noisy training sets we seek to detect the point where w 1  which is the cluster containing the majority of regions depicting c 1  will stop be the most populated cluster and therefore not selected by our framework to train c 1  Clearly this depends on the clustering error and 


the difference in population separating the rst two most fr equently appearing objects c 1  c 2 2 C  This difference depends on p c 1  p c 2 and increases proportionally to n as derived from eq 2 and shown in Fig 2\(b Here we work under the assumption that it is more likely for the second most highly ranked cluster w 2 to become more populated than w 1 as the clustering error increases Thus we only consider c 1 and c 2 and examine how their difference in population relates with n and clustering performance In order to do this we make an initial assignment of objects to clusters based on their ranks c i  w i  and express clustering error using the notations of Table 2 Table 2  Notations for Clustering Symbol Denition T C i Number of regions depicting object c i tc i Number of regions depicting c i  correctly assigned to cluster w i P op i Population of cluster w i F P i False positives of w i with respect to c i F N i False negatives of w i with respect to c i DR i  Displacement rate of w i  F P i  F N i with respect to c i Given the above F P i  P op i  tc i and F N i  T C i  tc i  By substituting tc i we have P op i  T C i  F P i  F N i  However T C i is actually the number of times the object c i appears in S L appearances and according to eq 2 we have T C i  np i  Now w 1 will be selected by our framework for learning c 1 as long as P op 1  P op 2  0  T C 1  T C 2   F P 1  F N 1    F P 2  F N 2   0  n  DR 2  DR 1 p c 1  p c 2 3 The displacement rate DR i shows how the P op i of cluster w i modies according to the clustering error and with respect to the ideal case where this error is zero Positive val ues of DR i indicates leakages in w i population while negative values indicate inows Using eq 3 we 3D plot in Fig 3 the space where P op 1  P op 2  0  Every horizontal slice of this volume corresponds to the legitimate values of DR 1 and DR 2 for a certain value of n  As n increases the surface of the corresponding slices increases also and thus the legiti mate error space for clustering increases too 4 IMPLEMENTING THE FRAMEWORK Social media processing For acquiring image groups with an increased amount of semantic coherence we adopted the SEMSOC approach introduced by Giannakidou et al in 25  In this work an unsupervised model for efcient and scalabl e mining of multimedia social-related data is presented The Fig 3  Space in which w 1 remains the most populated of the generated clusters derived from eq 3 reason for adopting this approach is to overcome the limitations that characterize collaborative tagging systems suc h as tag spamming tag ambiguity tag synonymy and granularity variation and increase the semantic coherence of the gener ated groups Each group emphasizes on a particular topic and the set of its containing tags reects the way users perceive it SEMSOC manages to create meaningful groups by jointly considering social and semantic features Its outcome is a s et of image groups S L i  S i  1      m where L i is an indicator of the emphasized topic and m is the number of created clusters In this case the number of clusters is determi ned empirically as described in 25  Every image I q has an associated set of tags T I q  We choose the image group S L i where its most frequent tag conceptually relates with the object that we want to detect In t his way we obtain a semantically coherent group of images the majority of which is expected to depict the object of interes t Segmentation Segmentation is applied to all images in S L with the aim to extract the spatial masks of visually meaningful regions In our work we have used a K-means with connectivity constraint algorithm as described in 26  T h e output of this algorithm is a set of segments R I q  f r I q i  i  1      m g  which in the ideal case correspond to meaningful objects c i 2 C  Visual descriptors Seven descriptors proposed by MPEG-7 27 c a p t u r i n g d i f f e r e n t a s p e c t s o f c o l o r  t e x t u r e a nd shape were used These descriptors namely mpeg 7  f Dominant Color DC Color Layout CL Color Structure CS Scalable Color SC Edge Histogram EH Homogeneous Texture HT Region Shape RS g were extracted 8 r I q i 2 R I q and 8 I q 2 S L  Different descriptors combinations were composed by concatenating their normalized values on a single vector f d  r I q i   f f i  i  1      z g  In this case d 2 mpeg 7 determines the descriptors combination and z the dimensionality of the feature space see Section 5.3 Th e concatenation approach was used only for training the objec t 


models using SVMs Clustering For performing feature-based region clustering we applied k-means on all extracted feature vectors f d  r I q i   8 r I q i 2 R Iq and 8 I q 2 S L  For calculating the distance between two regions we have used the functions presented in 27 b y i n d e p e n d e n t l y m e a s u r i n g t h e d i s t a n c e in each feature space and summing their normalized values However the problem that arises from the use of a parametric clustering algorithm like k-means is that a the number of th e clusters must be known in advance and b its performance is sensitive to the initial positions of the cluster centers I n order to overcome these problems we employed the Maximin algorithm as described in 26  b o t h f o r s e l e c t i n g t h e n u m b e r of clusters and estimating the initial positions of their ce nters Learning model parameters Support Vector Machines SVMs 28 w e r e c h o s e n f o r g e n e r a t i n g t h e o b j e c t d e t e c t i o n models due to their ability in coping efciently with highdimensionality pattern recognition problems All feature vectors assigned to the most populated of the created clusters are used as positive examples for training a binary classie r Negative examples are chosen arbitrary from the remaining dataset Tuning arguments include the selection of Gaussia n radial basis kernel and the adoption of a brute force strateg y for selecting the kernel parameters 5 EXPERIMENTAL STUDY The goal of our experimental study is twofold On the one hand we wanted to get an experimental insight on the error introduced by the analysis algorithms and check whether our theoretical claims stand On the other hand we aimed at comparing the quality of object models trained using the propos ed framework against the ones trained using high quality man ually provided region-detail annotations Experiments n ecessary for tuning some of the employed algorithms are also presented To carry out our experiments we utilized three datasets a strongly annotated dataset constructed manually by askin g people to produce region-detail image annotations and two weakly annotated social datasets obtained from Flickr For the rst dataset S M  a lexicon of 7 objects C M  f Vegetation Rock Sky Person Boat Sand Sea g  was used to strongly annotate 536 images at region-detail The output of this pro cess was to record relations associating an image segment r I q i  identied automatically by the segmentation algorithm wi th an object from C M  On the other hand two datasets from Flickr were crawled using the wget 1 utility and Flickr API facilities The rst dataset S 3 K consists of 3000 images depicting among others C 3 K  f cityscape seaside mountain roadside landscape sport-side g  while the second one S 10 K consists of 10000 images mostly related to C 10 K  f jaguar 1 wget http://www.gnu.org/software/wget 0 50 100 150 200 Sky Vegetation Building People outdoors Stone, rock\(s Number of images 0 100 200 300 400 Vegetation Sky People outdoors Stone, rock\(s Roadside Number of images 0 50 100 150 200 250 300 Sea People outdoors Sand Vegetation Waves \(sea Number of images 0 20 40 60 80 People outdoors Roadside Building Vegetation Sky Number of images 0 50 100 150 200 Sky Vegetation Building Stone, rock\(s Sea Number of images a Sky 0 100 200 300 400 500 Vegetation Roadside Sky Stone, rock\(s Animal Number of images b Vegetation 0 50 100 150 200 250 300 Sea People outdoors vegetation Building Sand Number of images c Sea 0 20 40 60 80 People outdoors People indoor Roadside Turkey country Sky Number of images d Person Fig 4  Distribution of objects appearance in an image group S L  obtained from S 3 K upper line and S 10 K lower line turkey apple bush sea city vegetation roadside rock  tennis g  For the purposes of our experimental study and after applying SEMSOC 25 o n b o t h S 3 K and S 10 K  we ended up with four object categories C bench  f sky sea person vegetation g  that exhibited signicant presence in all three datasets These object categories served as benchmarks for comparing the quality of different models 5.1 Social media processing As claimed in Section 3.2 we expect the gap between the number of appearances of the rst  c 1  and second  c 2  most highly ranked objects of C  to broaden as the volume of the analyzed dataset increases In order to verify this experim entally we plot the distribution of objects appearances in an image group S L  Each of the bar diagrams depicted in Fig 4 describes the distribution of objects appearances insid e an image group S L  as evaluated by human subjects The image groups are created by applying SEMSOC on both S 3 k and S 10 K  and selecting the groups emphasizing in one of the benchmark object categories  It is clear that as we move from S 3 k to S 10 K the gap between the number of images depicting c 1 and c 2  increases in all four cases 5.2 Tuning Maximin As mentioned before Maximin is used to decide the number of clusters and generate an initial estimation of the clu ster centers to be used by K-means However Maximin largely depends on a parameter called r  that species the threshold according to which new clusters are created or not The purpose of this experiment was on the one hand to optimally tune r  in order to use it for all subsequent experiments and on the other hand to check whether this value deviates substantial ly as the training examples and the object category vary This i s to ensure that the tuned value can be safely used under various contexts For this purpose we use S M and apply 10-fold cross validation for all available objects of C M and all pos 


  Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Fold 6 Fold 7 Fold 8 Fold 9 Fold 10 boat vegetation sky sea rock sand person 0.2 0.4 0.6 0.8 1 Concepts Cross validation results for Cl, EH and RS Testing Folds g Fig 5  Cross-validation results for descriptor combination CL EH and RS  r avg  0  633  r min  0  4  r max  0  8  sible descriptor combinations d 2 mpeg 7  Given that S M is strongly annotated clustering efciency can be measured e xplicitly using typical classication metrics i.e F-Meas ure For every object c i 2 C M  the subset of images S c i depicting this object is selected using the manually provided annotations Images are segmented and visual features are extracted Subsequently the regions are divided in 10 folds using each time one fold for testing and 9 for training For every run of the experiment we vary the value of parameter r within 0.2 0.96 u s i n g s t e p s o f 0  0 5  F o r e a c h v a l u e o f r  the number of clusters determined by applying Maximin on the training folds is used to perform clustering using k-mea ns in the regions belonging to the testing fold The F-measur e of the most populated cluster w 1 is calculated with respect to the most frequently appearing object c 1 in S c i  Given that for each value of r we can measure the clustering efciency F i;j;\r  on the basis of a S c i and fold j  we are able to determine the optimal value of r as r opt  arg max r  F i;j;\r   Finally the average of the optimal values among folds and objects  r  0  633  was used for the remaining of our experiments Fig 5 is a 3D plot summarizing the aforementioned results for the feature space derived by combining CL EH and RS It is clear that the optimal values of r does not deviate substantially as the object category and the folds var y Similar observations were made for all other combinations o f MPEG-7 descriptors the results of which are not included in this manuscript due to lack of space 5.3 Optimal Feature Space Visual descriptors determine the attributes by which a mode l tries to capture an object's form an appearance After tuning the Maximin algorithm for all different combinations of MPEG-7 descriptors we utilized the strongly annotated dataset S M to determine the optimal feature space in terms of clustering efciency As in the previous case 8 c i 2 C M  a subset S c i  S M of images depicting c i was selected to serve as the image group For each of those image groups cluster  None DC CL SC CS DC_CL DC_SC DC_CS CL_SC CL_CS SC_CS DC_CL_SC DC_CL_CS DC_SC_CS CL_SC_CS DC_CL_SC_CS None HT EH RS HT_EH HT_RS EH_RS HT_EH_RS 0 0.5 1 1.5 2 2.5 3 3.5  9¶≈¶ÀaeƒyÑ&	µÅ‘¥`’±Å∞÷§àìà&>Tn„d/ÑçÁæ.KúDÙ·9¬Ç?˘¥X€∫‰8d8∞É{0…]S˛êv`*~∞œF l…6NR(Á‚›ˇ∂Ï≈¿√‡∑ò˛ÒŸMø}0Ì[F1#œ”VÜÆ6.Ÿ@'Uïnvú’d™ÍRÊ_eOn%UıüÚSïëD` EUùT\R&}VG“Ug BùXªôŒújì¿÷l sÊ”0,ÃW2ù◊ì¿.∆l sÊ—2,ÃÆôŒΩÀ$∞áödN≥ıFû’G0ùkøÈü}<+Aÿ≤’§1≤V¡tŒªLˇL K 9ˆXú˚ør†$ÄN">	sõƒ≠`âˇò Ö≤s±J¿E^"·%êE  Texture and shape descriptors Clustering efficiency for all descriptor combinations   ACıÜ≈°Ú5‘¿ômƒn §\[∞Ds_Pn¡ôm2c Color descriptors Clustering efficiency Fig 6  Clustering efciency for all combinations of MPEG-7 descriptors ing efciency was measured by calculating the F-Measure of the most populated cluster w 1  with respect to the most highly ranked object c 1 in S c i  Finally these values were summed over all different objects c i 2 C M  to form a cumulative f-measure metric assessing the clustering efciency for a c ertain combination of visual descriptors i.e feature spac e Fig 6 summarizes the results by plotting in the z axis the value of cumulative f-measure obtained for the feature spac e determined by combining the descriptors indicated by the x  and y  axis We can see that clustering efciency maximizes when CL EH and RS are combined This experimental observation is also compliant with human intuition since colo r texture and shape are considered important attributes of vi sual perception for discriminating between different obje cts The feature space determined by d  f CL EH RS g was utilized for the remaining of our experiments 5.4 Cluster Selection Having tuned the Maximin algorithm and selected the optimal feature space the purpose of this experiment was to validate using real data our theoretical claim that the most pop ulated cluster contains the majority of regions depicting t he object of interest In order to do so 8 c i 2 C M we obtain S c i  S M and apply k-means clustering using r  0  633 and d  f CL EH RS g  In Fig 7 we visualize the way regions are distributed among the clusters by projecting their feat ure vectors in three dimensions using PCA Principal Component Analysis The regions depicting the object of interest c i are marked in squares while the other regions are marked in dots  Color code indicating a cluster's rank according to their po pulation i.e red 1st black 2nd blue 3rd magenta 4rt h green 5th cyan 6th is used Thus in the ideal case all squares should be painted red and all dots should be colored differently Squares being painted in colors other than red  indicate false negatives and dots painted in red indicate fa lse positives We can see that our claim is validated in 5 i.e sky sea person vegetation and rock  out of 7 examined cases 


  4 2 0 2 4 4 2 0 2 4 3 2 1 0 1 2 3 red:911 black:712 blue:428 magenta:375 a Sky   4 2 0 2 4 4 2 0 2 4 3 2 1 0 1 2 3 red:875 black:741 blue:424 magenta:369 b Sea   4 2 0 2 4 2 0 2 4 3 2 1 0 1 2 3 red:602 black:487 blue:351 magenta:339 green:238 c sand   4 2 0 2 4 4 2 0 2 4 3 2 1 0 1 2 3 red:306 black:302 blue:283 magenta:217 green:189 cyan:138 d Person   4 2 0 2 4 4 2 0 2 3 2 1 0 1 2 red:105 black:100 blue:79 magenta:68 green:59 e Boat   4 2 0 2 4 4 2 0 2 4 3 2 1 0 1 2 3 red:311 black:243 f Vegetation   4 2 0 2 4 2 0 2 4 2 1 0 1 2 3 red:261 black:166 g Rock Fig 7  Regions distribution amongst clusters This Figure is bes t viewed in color with magnication The visual diversity of objects boat and sand  causes segmentation and visual feature extraction to introduce signica nt error that prevents clustering from gathering the regions of interest into the most populated cluster 5.5 Object models comparison Assessing the quality of object detection models generate d using both the proposed framework and the manually provided region-detail annotations is the purpose of this exp eriment Additionally we want to validate our claim that as the scale of the utilized social dataset increases the erro r allowed to be introduced by the analysis components increases also and the models produced by the proposed framework are more robust With this intention we generated object model s using S M  S 3 K and S 10 K for the object categories of C bench  For each object c i 2 C bench one model was trained in a fully supervised manner using the strong annotations of S M  and two models were trained without supervision using the weak annotations of S 3 K and S 10 K and the proposed framework In order to evaluate the performance of these models we utilized a portion i.e 268 images of the strongly annotated dataset S M test  S M as ground truth not used during training By looking at the bar diagram of Fig 8 we note that models trained in a fully supervised manner perform optimally in all cases However the performance achieved by the models trained without supervision although inferior  is still satisfactory especially if we take into account the t ime and effort gained using the proposed framework Another interesting observation concerns the improvement in perfo rmance achieved in all cases between the models trained using S 10 K and S 3 K  respectively This tendency veries our claim that there is a relation between the size of the utilize d social dataset and the robustness of the generated models 6 CONCLUSIONS  FUTURE WORK Although the quality of the object models trained using the proposed unsupervised technique is still inferior from the one achieved using supervised approaches we have shown that under certain circumstances social data can be effectively used to learn the parameters modeling an object's form and appearance Moreover as it is reasonable to expect that the proposed framework would not graciously scale to every posSky Vegetation Sea Person 0 10 20 30 40 50 60 70 80 90 100 Manual annotation vs automatic annotation F measure Manual Flickr 10000 Flickr 3000 Fig 8  Comparing the quality of different object models sible object category the social aspect of user contribute d content and its potential to scale in terms of content divers ity and size advocates it's use for the type of objects that appear frequently in social context Our plans for future work include exploiting more of the user contributed informatio n e.g Flickr groups for obtaining suitable from a comput er vision perspective datasets and the employment of outlie r detection techniques for training the models using less noi sy region-clusters 7 ACKNOWLEDGMENT This work was funded by the X-Media project www.xmedia-project.org sponsored by the European Commission as part of the Information Society Technologies IST programme under EC grant number IST-FP6-026978 and the European Community's Seventh Framework Programme FP7/2007-2013 under grant agreement n215453 WeKnowIt 8 REFERENCES 1 P a u l A  V i o l a a n d M i c h a e l J  J o n e s   R a p i d o b j e c t d e tection using a boosted cascade of simple features in CVPR 1  2001 pp 511ñ518 2 Y i L i a n d L i n d a G  S h a p i r o   C o n s i s t e n t l i n e c l u s t e r s for building recognition in cbir in ICPR 3  2002 pp 952ñ956 


3 B a s t i a n L e i b e  A l e s L e o n a r d i s  a n d B e r n t S c h i e l e   A n implicit shape model for combined object categorization and segmentation in Toward Category-Level Object Recognition  2006 pp 508ñ524 4 K a h K a y S u n g a n d T o m a s o P o g g i o   E x a m p l e b a s e d learning for view-based human face detection IEEE Trans Pattern Anal Mach Intell  vol 20 no 1 pp 39ñ51 1998 5 F e i F e i L i  R o b e r t F e r g u s  a n d P i e t r o P e r o n a   O n e shot learning of object categories IEEE Trans Pattern Anal Mach Intell  vol 28 no 4 pp 594ñ611 2006 6 G u s t a v o C a r n e i r o  A n t o n i B  C h a n  P e d r o J  M o r e n o  and Nuno Vasconcelos Supervised learning of semantic classes for image annotation and retrieval IEEE Trans Pattern Anal Mach Intell  vol 29 no 3 pp 394ñ410 2007 7 J o s e f S i v i c  B r y a n C  R u s s e l l  A l e x e i A  E f r o s  A n d r e w Zisserman and William T Freeman Discovering objects and their localization in images in ICCV  2005 pp 370ñ377 8 P i n a r D u y g u l u  K o b u s B a r n a r d  J o  a o F  G  d e F r e i t a s  and David A Forsyth Object recognition as machine translation Learning a lexicon for a xed image vocabulary in ECCV 4  2002 pp 97ñ112 9 J i a L i a n d J a m e s Z e W a n g   A u t o m a t i c l i n g u i s t i c i n dexing of pictures by a statistical modeling approach IEEE Trans Pattern Anal Mach Intell  vol 25 no 9 pp 1075ñ1088 2003 10 R o b e r t F e r g u s  P i e t r o P e r o n a  a n d A n d r e w Z i s s e r man Object class recognition by unsupervised scaleinvariant learning in CVPR 2  2003 pp 264ñ271 11 G i u s e p p e P a s s i n o  I o a n n i s P a t r a s  a n d E b r o u l I z q u i e r d o On the role of structure in part-based object detection in ICIP  2008 pp 65ñ68 12 J a k o b J  V e r b e e k a n d B i l l T r i g g s   R e g i o n c l a s s i  c a t i o n with markov eld aspect models in CVPR  2007 13 M a n u e l a V a s c o n c e l o s  N u n o V a s c o n c e l o s  a n d G u s t a v o Carneiro Weakly supervised top-down image segmentation in CVPR 1  2006 pp 1001ñ1006 14 T i l l Q u a c k  B a s t i a n L e i b e  a n d L u c J  V a n G o o l   W o r l d scale mining of objects and events from community photo collections in CIVR  2008 pp 47ñ56 15 T h a n o s A t h a n a s i a d i s  P h i v o s M y l o n a s  Y a n n i s S  Avrithis and Stefanos D Kollias Semantic image segmentation and object labeling IEEE Trans Circuits Syst Video Techn  vol 17 no 3 pp 298ñ312 2007 16 R o b e r t F e r g u s  F e i F e i L i  P i e t r o P e r o n a  a n d A n d r e w Zisserman Learning object categories from google's image search in ICCV  2005 pp 1816ñ1823 17 F e i F e i L i  P i e t r o P e r o n a  a n d C a l i f o r n i a I n s t i t u t e of Technology A bayesian hierarchical model for learning natural scene categories in CVPR 2  2005 pp 524ñ531 18 B r y a n C  R u s s e l l  W i l l i a m T  F r e e m a n  A l e x e i A  E f r o s  Josef Sivic and Andrew Zisserman Using multiple segmentations to discover objects and their extent in image collections in CVPR 2  2006 pp 1605ñ1614 19 A n t o n i o B  T o r r a l b a  K e v i n P  M u r p h y  a n d W i l l i a m T  Freeman Contextual models for object detection using boosted random elds in NIPS  2004 20 K e i j i Y a n a i   G e n e r i c i m a g e c l a s s i  c a t i o n u s i n g v i s u a l knowledge on the web in ACM Multimedia  2003 pp 167ñ176 21 K o b u s B a r n a r d  P i n a r D u y g u l u  D a v i d A  F o r s y t h  Nando de Freitas David M Blei and Michael I Jordan Matching words and pictures Journal of Machine Learning Research  vol 3 pp 1107ñ1135 2003 22 A l e x a n d e r J a f f e  M o r N a a m a n  T a m i r T a s s a  a n d M a r c Davis Generating summaries and visualization for large collections of geo-referenced photographs in Multimedia Information Retrieval  2006 pp 89ñ98 23 J a m e s P h i l b i n  O n d r e j C h u m  M i c h a e l I s a r d  J o s e f S i v i c  and Andrew Zisserman Object retrieval with large vocabularies and fast spatial matching in CVPR  2007 24 C a m e r o n M a r l o w  M o r N a a m a n  D a n a h B o y d  a n d M a r c Davis Ht06 tagging paper taxonomy ickr academic article to read in Hypertext  2006 pp 31ñ40 25 E i r i n i G i a n n a k i d o u  I o a n n i s K o m p a t s i a r i s  a n d A t h e n a Vakali Semsoc Semantic social and content-based clustering in multimedia collaborative tagging systems in ICSC  2008 pp 128ñ135 26 V a s i l e i o s M e z a r i s  I o a n n i s K o m p a t s i a r i s  a n d Michael G Strintzis Still image segmentation tools for object-based multimedia applications IJPRAI  vol 18 no 4 pp 701ñ725 2004 27 B  S  M a n j u n a t h  J  R  O h m  V  V  V i n o d  a n d A  Y a mada Colour and texture descriptors IEEE Trans Circuits and Systems for Video Technology Special Issue on MPEG-7  vol 11 no 6 pp 703ñ715 Jun 2001 28 B  S c h o l k o p f  A  S m o l a  R  W i l l i a m s o n  a n d P  B a r t l e t t  New support vector algorithms Neural Networks  vol 22 pp 1083ñ1121 2000 


 Therefore, we could choose the doc distribution as our reference point and use it on all the sets. Going back to Figure 1, we can see that it almost serves as a composite of the rest with several local peaks. Actual data will effectively mask out parts of the ranking e.g. compressed data will have little use for entropy ranks to the left of its bell, whereas text data will not need anything to the right of its bell The answer to the second question is a confident yes even random data exhibits a bell-shaped distribution of entropy scores so even within a relatively small of window of 64-128 bytes, a few of the features are likely to stand out as statistically improbable, relative to the rest  6. Measuring File Similarity  We are now ready to go back to our original problem of finding similar objects. To establish a baseline measurement of the effectiveness of the described methods, we ran a controlled experiment using random data and known amounts of overlapping content. Specifically, we used two randomly generated files and produced mixed versions of them in the following fashion: take x  percent of file #1 and mix with 100x percent from file #2. The mixing was done in blocks of 512 bytes and we used 21 values for x 0, 5, 10 100. Note that we selected the blocks at random so even the 100% case is not an identical file but a file containing the same data block in a random permutation Further, we fixed W 64, and varied t from 16 to 64 with a step of 8. Figure 6 summarizes the results As we can see from the chart, the number of common features increases linearly with the increase of the amount of data in common and the slope of the increase is determined by the threshold parameter t  Generally, a lower the value for t means that more features are retained, whereas a higher value selects fewer features and improves compression. In this case t 16 retains an average \(over the different runs\ 847 features, whereas t 64 only 110. Using an MD5 hash to compress the feature representation would mean that our storage requirements would be 13,552 and 7,040 bytes, respectively. This yields corresponding compression ratios of 3.8:1 and 7.2:1 that can be further improved 10 times by using a Bloom filter with 10 bits per element and 0.8% FP rate. Further reductions are possible by selecting a bigger value for W such as 128, 256, 512 The above results can readily be replicated for compressed data types such as jpg, pdf and gz For text, the results are quite similar for unrelated text however, things like style and common topic do tend to yield higher results as the syntactic similarity increases. Generally, the results for non-random data need to be evaluated with respect a baseline FP rate which is relatively straightforward to obtain from a representative set of files. Alternatively, one could use the set to generate a set of duplicate features and ignore them in the actual data A large-scale experimental validation of an early prototype implementation on 4.6GB of real data Table 1\ is the subject of a separate paper. In summary, we were able to detect files within simulated network traffic based on a single packet with 0.9987 to 0.9844 accuracy \(depending on file type\or feature size of 64 and threshold of 16. Our current implementation is capable of 100MB/s sustained throughput on a quad-core processor this includes feature selection, feature hashing, and comparison with a reference set. We expect that an improved implementation would need no more than two cores to sustain that rate  7. Conclusions  In this paper, we presented a new approach to selecting syntactic features for similarity measurements. Our work makes the following contributions   Through empirical data, we have shown that a basic entropy measure can provide a valuable guideline with respect to the uniqueness of a particular feature   We have shown that different data types exhibit different behavior and that similarity measures can be tuned to keep a lid on overall false positive rate   We proposed a new method for selecting characteristic features that does not rely on a Rabin scheme to be content-sensitive. Instead we use an entropy measure and empirical distribution data to sel ect statistically improbable features   The new method is very stable and predictable with the respect to the coverage it produces, and unlike previous work, can easily be tuned to the underlying data  In the immediate future we plan to full develop a practical, high-performance tool that can be used to correlate evidence on a large scale      Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


 8. References  1  B. Bloom Space/Time Tradeoffs in Hash Coding with Allowable Errors  Communications of the ACM  vol 13 no 7, pp. 422-426, 1970 2  S. Brin, J. Davis, H. Garcia-Molina Copy detection mechanisms for digital documents In Proceedings of the ACM SIGMOD Annual Conference San Francisco, CA, May 1995 3  A. Broder, M. Mitzenmacher Network Applications of Bloom Filters: A Survey  Internet Mathematics  vol 1 no 4, pp. 485-509, 2005 4  A. Broder, S. Glassman, M. Manasse, and G. Zweig Syntactic Clustering of the Web In Proceedings of the 6 th International World Wide Web Conference pp 393-404, 1997 5  M. S. Charikar Similarity Estimation Techniques from Rounding Algorithms  In Proceedings of the 34 th Annual ACM Symposium on Theory of Computing 2002 6  C. Y. Cho, S. Y. Lee, C. P. Tan, and Y. T. Tan Network forensics on packet fingerprints. 21st IFIP Information Security Conference \(SEC 2006 Karlstad, Sweden, 2006 7  Henziger, M Finding Near-Duplicate Web Pages: A Large-Scale Evaluation of Algorithms In Proceedings of the 29 th Annual International ACM SIGIR Conference on Research & Development on Information Retrieval Seattle 2006 8  R. Karp, M. Rabin. "Efficient randomized patternmatching algorithms". IBM Journal of Research and Development 31 \(2\249-260, 1987 9  A. Kirsch, M. Mitzenmacher Distance-Sensitive Bloom Filters  Proceedings of the Algorithms Engineering, and Experiments Conference ALENEX 2006 10  J. Kornblum Identifying almost identical files using context triggered piecewise hashing  Proceedings of the 6 th Annual DFRWS Aug 2006, Lafayette, IN 11  U. Manber. Finding similar files in a large file system In Proceedings of the USENIX Winter 1994 Technical Conference, pages 1-10, San Fransisco, CA, USA 1994 12  M. Mitzenmacher Compressed Bloom Filters  IEEE/ACM Transactions on Networks 10:5, pp. 613620, October 2002 13  K. Monostori, R. Finkel, A. Zaslavsky, G. Hodasz, M Pataki Comparison of Overlap Detection Techniques In Proceedings of the 2002 International Conference on Computational Science Amsterdam The Netherlands, \(I\pp 51-60, 2002 14  M. Ponec, P, Giura, H. Brnnimann, J. Wein Highly Efficient Techniques for Network Forensics, In Proceedings of the 14th ACM Conference on Computer and Communications Security, 2007 Alexandria, Virginia 15  H. Pucha, D. Andersen, M. Kaminsky Exploiting Similarity for Multi-Source Downloads using File Handprints In Proceedings of the Forth USENIX NSDI, Cambridge, MA. Apr, 2007 16  M. O. Rabin Fingerprinting by random polynomials Technical report 15-81, Harvard University, 1981 17  S. Rhea, K. Liang, and E. Brewer. Value-based web caching. In Proceedings of the Twelfth International World Wide Web Conference, May 2003 18  V. Roussev, Y. Chen, T. Bourg, G. G. Richard III md5bloom Forensic filesystem hashing revisited  Proceedings of the 6 th Annual DFRWS Aug 2006 Lafayette, IN 19  V. Roussev, G. Richard III and L. Marziale, "Multiresolution similarity hashing", Proceedings of the Seventh Digital Forensic Research Workshop, 2007 20  K. Shanmugasundaram, H. Bronnimann, N. Memon Payload Attribution via Hierarchical Bloom Filters  Proceedings of the ACM Symposium on Communication and Computer Security CCS'04 2004 21  N. Shivakumar and H. Garcia-Molina SCAM: a copy detection mechanism for digital documents In Proceedings of the International Conference on Theory and Practice of Digital Libraries June 1995 22  N. Shivakumar, H. Garcia-Molina Building a scalable and accurate copy detection mechanism In Proceedings of the ACM Conference on Digital Libraries March 1996\, 160-168 23  N. Shivakumar, H. Garcia-Molina Finding nearreplicas of documents on the web In Proceedings of the Workshop on Web Databases March 1998\ 204212 24  S. Schleimer, D. S. Wilkerson, and A. Aiken Winnowing: local algorithms for document fingerprinting. In SIGMOD '03: Proceedings of the 2003 ACM SIGMOD international conference on management of data, pages 76-85, New York, NY USA, 2003. ACM Press Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


This figure presents the data flow through main blocks that can be used a few times and special blocks that must execute only particular functions Here also Configurator of chain and Sequences library for WiMAX/UMTS modules are equipped First one is responsible for right construction of next processing path and second one stores necessary number of elements After signal verification system is reconfigured according to input data and with using the chosen protocol type The OUTPUT connects to radio link and then signal must be transmitted over one of six channel types And at the reception side we configure the receiver in relation to transmitted mode 5 HARDWARE PLATFORM SELECTION Current technologies in a hardware environment allow to test our system in real-time implementation There are a couple of DSP based platforms that can be selected for validation from Lyrtech Inc the Small Form Factor SFF Software Communication Architecture SCA Development the Small Form Factor SFF Softwaredefined Radio SDR Development Platform the Small Form Factor SFF Software-defined Radio SDR Evaluation Module 20 All these platforms are based on TMS320DM6446 DMP SoC from Texas Instruments 21 For the proposed SDR based system we chose the SFFSDR Evaluation board see Figure 13 as far as this platform supports WiMAX technology model based design tools accelerating prototyping implementation of all protocols layers for complete radio stacking extra boards operates with 297MHz ARM926EJ-s RISC CPU and 594MHz C64x DSP in sense of power management this module has MSP430 MCU Due to an availability of Virtex-4 SX35 FPGA from Xilinx 22 this module can perform implementation of full modem processing functions that is very important feature in meaning of multi-protocol architecture of our system We are able to vary our requirements to each protocol inside the same hardware structure Figure 13 SFF SDR development platform by Lyrtech 6 PROTOTYPING THE WiMAX/UMTS SYSTEM The WiMAX/UMTS system is implemented in high-level language as C with the class library The main accent was done on the correct form of the signal processing sequence The path for WiMAX or UMTS signal is determined in the beginning and system should verify its entity leads the system in a relevant direction The main goal of this software implementation is to check how our system can handle the input signal sequence The simulation was carried out for following parameters for each subsystem For UMTS we consider transport block with 1280 bits size frame size is 2400 bits channelization code with 16 chips sequence For WiMAX we generate the bit block is equal to 1280 bits however during channel coding operation this block is divided on turbo coding block that include 384 bits The size of turbo coding block forms from the block determination corresponding modulation type and number of subchannels During software verification we obtained that our proposed model can separate different paths subject to a type of an inter sequence in the software environment The framework of our WiMAX/UMTS system went through one scenario step and now we are directed to an extension of this model For the more detailed system visualization we integrate our C code modules into MATLAB library by means of proper dynamic linking libraries dll compiled by using the MATLAB C compiler Each block can be formed with extended parameters Modules with C code configurate the system work in a host This host implementation will present a prototype This prototype will help to analyze future real-time hardware implementation MATLAB prototype can also provide debugging of real system the result of real system must be equal to our MATLAB prototype Next step of the work development and verification is to implement it into the hardware platform based on DSP DSP based platform allows to organize signal processing in a digital presentation that serves SDR based part of our general system 7 CONCLUSIONS AND FUTURE WORK In this paper we considered the framework of WiMAX/UMTS baseband level system for mobile device in UL transmission direction We presented the different signal processing structures based on OFDM and WCDMA physical layer procedures Our research work is mainly devoted to developing the approach of seamless switching between different subsystems that can be realized by SDR technology implementation To this end we proposed a possible solution to allow coexistence of different data transmission technologies 11 


The position of SDR blocks in common UMTS/WiMAX architecture for mobile terminal was shown in this paper We presented the different blocks of each subsystem and have identified three blocks which can be implemented as common SDR blocks These blocks include channel coding module interleaver module and data mapping module We also demonstrated the work of our system in the software environment Next steps of the UMTS/WiMAX system development are preparation of the specification and implementation of all possible scenarios Each scenario will include particular blocks parameters and common description of main blocks But we have to be carefully in case of main blocks description because there are a plenty of features 8 ACKNOWLEDGMENTS We gratefully acknowledge the company Arslogica that kindly provided us the hardware support for our experimental studies 9 REFERENCES 1 A Samukic UMTS Universal Mobile Telecommunications System development of standards for the third generation Proc of 1998 IEEE GLOBECOM Conf Sydney AUS Nov 8-12 1998 vol.4 pp.19761983 2 N Fourty T Val P Fraisse and J.-J Mercier Comparative analysis of new high data rate wireless communication technologies From Wi-Fi to WiMAX Proc of the IEEE Autonomic and Autonomous Systems and International Conf on Networking and Services ICAS-ICNS 05 Oct 23-28 2005 pp.66-66 3 M Komara SDR Architecture Ideally Suited for Evolving 802.16 WiMAX AirNet Communications SDR Forum Exhibition 2004 4 I Held 0 Klein A Chen C.-Y Huang and V Ma Receiver Architecture and Performance of WLAN Cellular Multi-Mode and Multi-Standard Mobile Terminals Proc of 2004 IEEE VTC Fall Conf Los Angeles CA Sept 26-29 2004 vol 3 pp 2248 2253 5 IEEE Standard for Local and Metropolitan Area Networks Part 16 Air Interface for Fixed Broadband Wireless Access Systems 2004 6 R Weigel and L Maurer  D Pimingsdorfer A Springer RF Transceiver Architectures for W-CDMA Systems Like UMTS State of the Art and Future Trends Proc of the Intern Symp on Acoustic Wave Devices for Future Mobile Communication Systems Chiba JP March 5-7 2001 pp 25-34 7 P-W Fu and K.C Chen A Programmable Transceiver Structure of Multi-rate OFDM-CDMA for Wireless Multimedia Communications Proc of 2001 IEEE Vehicular Technology Conf VTC-Fall 2001 Atlantic City NJ Oct.7-11 2001 vol 3 pp 1942-1946 8 L Zhigang L Wei Z Yan G Wei A Multi-standard SDR Base Band Platform Proc of 2003 International Conference on Computer Networks and Mobile Computing Shanghai PRC Oct 20-23 2003 pp 461 464 9 C Moy A A Kountouris L Rambaud and P Le Corre  Full Digital IF UMTS Transceiver for Future Software Radio Systems  Proc of ERSA 01 Conf Las Vegas NV June 25-28 2001 10 3GPP TS 25.201 Physical layer general description 11 K.R Santhi and G.S Kumaran Migration to 4 G Mobile IP based Solutions Proc of International Conference on Internet and Web Applications and Services/Advanced International Conference Feb 2006 pp 76 76 12 S Zhu M Song Y Li J Song and F Ren Simulation platform of WCDMA based on software defined radio Proc of 2nd ACM International Conference on Mobile Technology Applications and Systems Nov 2005 pp 1-5 13 L Ma and D Jia The Competition and Cooperation of WiMAX WLAN and 3G Proc of 2nd International Conference on Mobile Technology Applications and Systems Nov 2005 pp 15 14 J Mitola III Software Radio Architecture ObjectOriented Approaches to Wireless Systems new ed Wiley New York 2004 15 R Seungwan 0 Donsung S Gyungchul and K Han Perspective of the next generation mobile communications and services Proc of IEEE 2004 Int Symp on Personal Indoor and Mobile Radio Communications PIMRC 2004 Barcelona SP 5-8 Sept 2004 vol.1 pp 643-647 16 E Biglieri Coding for Wireless Channels  Springer New York 2005 12 


17 3GPP TS 25.212 Multiplexing and channel coding FDD 18 IEEE Standard for Local and metropolitan area networks Part 16 Air Interface for Fixed and Mobile Broadband Wireless Access Systems Amendment 2 Physical and Medium Access Control Layers for Combined Fixed and Mobile Operation in Licensed Bands and Corrigendum 1 2006 pp 0_1 822 19 3GPP TS 25.211 Physical channels and mapping of transport channels onto physical channels FDD 20 Data sheet from Lyrtech Inc http available at htp c hwwkneopusff _.l/p.s/lrtehs _sr d21]D ateforomTdf 21 Data sheet from Texas Instruments http available at 22 Data sheet from Xilinx http available at httll/www.xilinx.com 23 L Hanzo W Webb and T Keller Singleand Multicarrier Quadrature Amplitude Modulation  Wiley New York 2000 titled Wireless and Satellite Communications  The research interests of Dr Sacchi are mainly focused on wideband mobile and satellite transmission systems based on space time andfrequency diversity multi-user receivers based on non conventional techniques neural networks genetic algorithms higher-order statistics-based receivers etc cross-layer PHY-MAC design and high-frequency broadband satellite communications He is currently local coordinator for University of Trento of research projects dealing with reconfigurable communication platforms based on MIMO techniques and space-time signal processing ICONA project funded by MIUR and with exploitation of W-band for broadband satellite communications WA VE programs funded by ASI Claudio Sacchi is author and co-author of more than 50 papers published in international journals and conferences and reviewer for international journals and magazines IEEE Transactions on Communications IEEE Transactions on Wireless Communications IEEE Communications Letters IEEE Transactions on Aerospace and Electronic Systems Electronics Letters Wireless Networks IEEE Communications Magazine etc Dr Sacchi is member of the Organizing Committees and Technical Program Committees of international conferences like ICIP ICC GLOBECOM ACM-MOBIMEDIA etc Claudio Sacchi is member of IEEE M'01 SM'07 BIOGRAPHIES Olga Zlydareva is a PhD student of the University of Trento Italy She obtained her Master degree in Design Electronics Systems with specialization in High Radio Frequency Devices from MATI Moscow State Aviation Technological University named after KE Tsiolkovsky Moscow Russia Her research interests have oriented on the Software Defined Radio Technology Wireless Technologies Cellular Technologies Tunable devices Multi-standard systems Multi-protocol systems Physical layer of mobile devices Reconfigurability and Reprogramming of mobile devices The recent research focuses on the development of the baseband level of multistandard mobile devices based on SDR technology Claudio Sacchi was born in Genoa Italy in 1965 He obtained the Laurea degree in Electronic Engineering and the Ph.D in Space Science and Engineering at the University of Genoa Italy Since August 2002 Dr Sacchi has been holding aposition as assistant professor at the Faculty of Engineering of the University of Trento Italy In 2004 he was appointed by the Department of Information and Communication Technology of the University of Trento as leader of the Research Program 13 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


