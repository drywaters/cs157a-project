Computer Aided Detection of SARS Based on Radiographs Data Mining Xie Xuanyang, Gong Yuchang, Wan Shouhong, Li Xi Department of Computer Science & Technology University of Science and Technology of China Hefei, Anhui 230022, CHINA Abstract This paper introduces our work on how to use image mining techniques to de tect SARS, the Severe Acute Respiratory Syndrome, automatically as the prototype of computer aided detection/diagnosis \(CAD\stem. Data used in this paper are digitalized PA\(Posterior Anterior\X-ray 
images stored in the real-life Picture Archiving and Communication System \(PACS\Affiliation Hospital of Guangzhou Medical College. Association rule mining was applied first but results showed there was no significant difference between the locations of the lesions or infiltrate Classification based on image textures was performed. A sample set contains both the pneumonia and SARS X-ray images was built in the first pla ce. After modeling each sample by a feature vector, the sample set was partitioned to match the detection purpose: classification. Three methods were used 
C4.5, neural network\(NN\T. Final result shows that 70.94% SARS cases can be detected by CART. Data preparation, segmentation, feature extraction and data mining steps, with corresponding techniques are included in this paper ROC charts and confusion matrix by all three methods are given and analyzed I.  I NTRODUCTION Computer aided detection/diagnosis \(CAD\ which is used by physicians to detect and diagnose lesions, has been successfully applied in solving many problem S  also named as Atypical Pneumonia' in China, was first 
found in Guangdong, China, 2002. By July 31, 2003, 5327 patients had been infected with it in China, accounts to 65.6% of all the cases reported in the worl h ere is a pressing necessity of developing a computer aided detection of SARS system, especially for the countries or regions that have no experience in dealing with this disease This paper introduces our work on how to use image data mining techniques to detect SARS cases from typical pneumonia cases. As noted in 5 h e X-ray rad i og raph plays important role in diagnosing whether a case belongs 
to SARS. Typical image mining process is used, which include data preparation, seg mentation, feature extraction and mining. All these steps are discussed in following sections. Experiment results of all steps are included too II.  M AT E R I A L S  U SED AND D ATA P REPARATION Posterior anterior \(PA\ographs were used. Data set includes 818 imag 
es of about 2048x2048 in size, 14 bits in grey level, which include 234 images for SARS patients and 584 for typical pneumonia cases Default window level and window width are used to scale 14-bit grey level depth to 8-bit grey level  All data are stored in PACS of the 2nd Affiliation Hospital of Guangzhou Medical College, which was developed by a cooperate team of our laboratory. Various modalities of data were stored in PACS, such as digital X-ray \(DX\, computed tomography \(CT\agnetic 
resonance \(MR\Only DX images of chest radiographs are needed. The selection of DX image can be easily done by query each image's modality because all images are compatible with DICOM, an in dustrial standard for medical imaging. A good survey in [3 analyzed l o ts of pa pers t o  show that almost all computer aided diagnosis treated only PA images. Then the task of data preparation remains to filter out lateral images. Simple strategy can be applied   Our experiment results show that the accuracy can reach 99.12 III.  S 
EGME NT A T IO N THE  R EGION OF I NTEREST Segmentation is the process of selecting region of interesting \(ROI\rts of an image automatically. The purpose of this step is to outline the two lung fields. Simple grey-level based method cannot fulfill this task. Active Proceedings of the 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference Shanghai, China, September 1-4, 2005 0-7803-8740-6/05/$20.00 2005 IEEE 7459 


shape m and i t s m u lti-resolution ve rsion  beca use they are robust e n ough t o  accommodate the complex image resulted from overlapping organs MRASM uses a set of points, called a shape to represent the ROI. A shape can be represented by a vector x in 2 n 000  as in \(1 n  is the number of landmark points  000^\000 112 2   T nn x yx y x y 000 x 000 1 MRASM uses a training set x i to capture the mean shape x and variances of each landmark points. After training, the mean shape x  is used as a template to match new images by adjusting parameters heuristically. Gaussian image pyramid was built to spee dup the search. The search procedure starts at the root of the tree, which is small and coarse, and gets the current shape match the ROI. After that the search goes down one level to refine the result During our experiment, we found that a good initial position and scale parameter for x  can make the iteration converge more rapidly. This is easily to be understood: less time is needed if the target is nearby. We introduce the concept window of lung fields to initialize these parameters   w h ic h is a f o ur elem e nt vector de fine d as the  bounding box of lung fields 000^\000 1212  T row row col col 000 w 2 where the four elements represent upper/lower and left/right bounds of w The initial position dX,dY  T and scale parameter s can be derived easily and other applications of window of lung fields can be found in  Fig.1 shows that only two or three steps are needed for the convergence if the window of lung fields is applied. 561 X-ray chest radiographs were used in our experiment Results showed only a few iter ations are needed in more than 90% cases a\irst 5 Iteration of MRASM b\ 5 Iteration of ASM with window of lung fields used Fig.1. Comparison of iteration between whether the window is used or not IV.  F EATURE E XTRACTION AND F EATURE V ECTORS According to the common stand used by radiologists three zones are divided vertically \(inner, middle, outer\nd another three are divided horizontally \(upper, middle lower\as demonstrated in Fig. 2. These blocks were also used to build association rules, but results \(in the next section\howed that there are no obvious differences between locations of lesions Features were extracted from each one of the 18 blocks These features include: mean, standard deviation, skewness and kurtosis grey level to describe the average data distribution of each block; energy, entropy, correlation inertia and local contrast ge nerated from the co-occurrence matrix of five directions, say 0  4  2 3  4 000S\000S 000S to describe the texture of each block. Thus for each block, 24 features are extracted. So for each image, there are 24x18=432 features generated. Finally, another three features are added to represent the overall condition of lung fields, see Fig. 3 1 Cardiothoracic ratio \(CTR\ A cardiothoracic ratio of more than 50% is considered abnormal. For clarity, we draw the two diameters in differ ent horizontal levels in Fig 4, then CTR can be simply calculated as 1 2 CTR L L 000  2 Costophrenic angles\(CPA\obliterated angle reminds abnormality. Both left and right CPA 1 000D and 2 000D are calculated In summary, each image is represented by a feature vector in 435 000  000^\000 12 18 12    CTR 000D\000D 000 Fff f 000 where i f denotes the feature vector of the i th block Fig.2. Dividing lung Fig.3 .CTR and CPA V  D ATA M INING AND E XPERIMENTS R ESULTS Because our 'simplicity first principle', association rule mining was chosen at first. We want to find whether there exist differences of lesions' locations between SARS and 7460 


pneumonia cases. One radiologist helped us to delineate which of the 18 blocks are considered abnormal. An image set contains 75 SARS and 125 pneumonia cases, was presented to the expert. Let R1~R9 denote the 9 blocks of right lung fields, L1~L9 denote the left ones. Then the expert just tick off each block to indicate it is abnormal. For direct comparison of the two cl asses, count of each position is normalized in percentage. Fig.4. shows that there seems no significant difference of the locations of abnormal between SARS and pneumonia t test 1 value p 000o s this observation. Association rule mining was performed on the tick matrix resulted from the expert. TABLE I gives the results. The rules are sorted ascending by confidence CON\port \(SUPT\lift. Then the top 10 rules are given in TABLE I. Many rules are identical. So this again confirms that association rule cannot be used to classify SARS and pneumonia. The classifier based on the feature vector was built and results are given below R1 R2 R3 R4 R5 R6 R7 R8 R9 L1 L2 L3 L4 L5 L6 L7 L8 L9 0 0.05 0.1 0.15 0.2   blocks percentage   SARS   PNEUMONIA   Fig.4. Occurrence frequency of each block  TABLE I A SSOCIATION R ULES M INING R ESULTS SARS PNEUMONIA CON SUPT LIFT RULE CON SUPT LIFT RULE 100 64.38 1.28 L8 > L7 100 40.00 1.74 L8 > L7 100 53.42 1.28 L8 & R9 > L7 100 36.00 1.74 L8 & R9 > L7 100 49.32 1.28 L8 & R8 > L7 100 33.60 1.74 L8 & R8 > L7 100 45.21 1.28 L8 & R9 & R8 > L7 100 32.00 1.74 L8 & R9 & R8 > L7 100 27.40 1.38 R7 > R8 100 22.40 2.66 R4 > R5 100 24.66 1.38 R9 & R7 > R8 100 20.80 1.74 L8 & R5 > L7 100 24.66 1.28 L8 & R5 > L7 100 20.00 2.50 L9 & L7 > L8 100 23.29 1.28 L8 & R8 & R5 > L7 100 20.00 1.74 L9 & L8 > L7 100 21.92 1.28 L8 & R9 & R5 > L7 100 20.00 1.74 L8 & R9 & R5 > L7 100 20.55 2.03 L7 & R7 > L8 & R8 100 20.00 1.74 L8 & R8 & R5 > L7 To make the classification results more credible, the sample set was partitioned into training, validation and testing sets in the ratio of 6:1:3 by simple stratified partition Three mining techniques were used to build the classifier 000z Decision tree: C4.5 with Gini reduction as the splitting criteria is used. Fig.5 gives the success rate versus the number of leaves. It can be noticed that a 6-leaf tree can reach about 75% success rate. As the tree grows larger, say with 18 leaves, the success rate of validation will decrease Fig.5. Decision tree building 000z Standard back propag ation multilayer perceptron neural network\(Fig.6\42 iterations. The success rate is about 73%, a good lift compared to decision tree Fig.6. Neural network 000z CART: classification and regression tree, a robust and advanced data analysis algori thm. Ten fold cross validation is used to ensure more reliable results. Fig.7 shows how the cost related to number of leaves during training. Though compared to Fig.6, CART uses more leaves \(21 leaves is chosen\o reach about 70% overall success rate. However CART performs the best as analyzed hereunder Fig.8. CART training Total success rate for these three methods are 75.9  73.0 and 69.32 respectively. It seems that C4.5 decision tree performs best. Bu t further analy sis showed that contrary to this, CART is the best one. Fig. 8 gives the receiver operating characteristic \(ROC\urves of C4.5 and NN. Roughly speaking, the NN performs better than C4.5 7461 


But the CART performs the best as can be observed from Fig. 9, which is drawn separately for clarity. Confusion matrix confirmed these observ ations numerically \(Table II All rows in the table are presented in percentage to make them comparable. As pointed previously, although the C4.5 makes the highest total success rate, it detects 41.67% of SARS cases. This is not applicable to use. NN performs better but not well enough to match the 70.94% success rate given by CART. The bold faced data are results provided by CART Fig.8. ROC curves of C4.5 and NN Fig.9. ROC curve of CART TABLE II: C ONFUSION M ATRIX for C4.5 / NN /CART Predicated class SARS Pneumonia C4.5 NN CART C4.5 NN CART SARS 41.67 45.83 70.94 58.83 54.17 29.06 Actual class P neumonia 10.17 15.25 31.34 89.83 84.75 68.66 VI. C ONCLUSION Image mining for SARS detection, including detailed techniques for each key procedure is presented in this paper Because simple location association rules cannot distinguish between SARS and normal pneumonia, we built classifier based on image text ure. Experiment results from three classification methods of data mining are presented and analyzed. Final conclusion is that CART performs well 70.94% of SARS cases can be detected SARS detection plays central role in computer aided diagnosis \(CAD\system. It is believed that the letter D in CAD involves two implications: detect first than diagnosis Though 70% is inapplicable for real life usage, we believe this prototype is the stepping stone for further research Some works are ongoing by our group R EFERENCES  Os m a r R.Zaiane Maria Luiza Antonie, and Alexandru Co m a n Mammography Classification by an Association Rule-based Classifier Proc. of the MDM/KDD 2002 2002.7, pp.62-69  M. K a ki m o to, C Morita and H  Tsu k im oto Data Mining fro m  Functional Brain Image Proc. of MDM/KDD 2000 2000.8, pp 91-97  Br am van Ginneken Bar t  M  te r  Haar Ro m e ny and M a x A  Viergever, Computer Aided Diagnosis in Chest Radiograph: A Survey IEEE Trans. on Medical Imaging Vol.20,No.12,2001.12 pp.1228-1241  Su m m ary of SAR S case b y country W o rld Health Org a nization http://www.who.int/csr/sars, 2003.9  Anil T Ahuja et.al  Radiological Appearance of Recent Cases of Atypical Pneumonia in Hong Kong The Dept. of Diagnostic Radiology and Organ Imaging,The Chinese Univ. of Hong Kong http://www.droid.cuhk.edu.hk/web/atypical_pneumonia,2003.10  T.F.Cootes, C  J.Ta y l or D H.Cooper,etc.al., Active Shape Models-Their Training and Application Computer Vision and Image Understanding Vol.61,No.1,1995,pp.38-59  T.F.Cootes, C  J.Ta ylor A Lanitis A c tive Shape Model s Evaluation of a Multi-Resolution Method for Improving Image Search Proc. of the British Machine Vision Conference 1994, pp.327-336  Xie Xuany a ng,Li Xi,Zhang Jin,Gong Yuchang,The W i ndow of Lun g  Fields: Automatic Determination and  its Applications in Chest Radiograph Processing The 3rd IASTED International Conference on Biomedical Engineering 2005-2,p458-086,ISBN 0-88986-476-4   Xie Xuanyang, Li Xi, Xu Y u feng W a n Shouho ng and Gong Y u chang  Mining X-ray images of SARS patients Proc. of the 3rd Australasian Data Mining Conference 2004.12, Cairns, Australia, Simeon J. Simoff and Graham J. Williams, eds. ISBN 0-646-44379-8 7462 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi 2-5 November 2003 implementation of these algorithms which are unoptimizable programs We use a real-life database with about 50,000 records It has seven attributes four are quantitative and three are categorical There are 25 linguistic terms in total after transformation We have compared their execution time for five different support thresholds. The result is shown in Figure 3  Apriori zn I Apriori-TF Apriori-TFP-PN J D Support Value  Figure 3 Comparisons of three algorithms Both Apriori-TFP and Apriori-TFP-PN outperform our implementation of Apriori This also verifies the experimental results in We notice our algorithm need less time than Apriori-TFP in some cases When the support threshold is relatively large or small, the difference between them is not obvious After analysis we think the phenomenon is based on two reasons On one hand the algorithm may prune a small quantity of nodes because most singleton itemsets are frequent with a small threshold On the other hand it doesn't need multiple scans because the size of the largest frequent itemset is small with a big one But it is known that we will discover too much meaningless association rules with a small threshold and lose useful information with a high threshold So even in some cases our algorithm doesn't show clear advantage, it is still a positive improvement on Apriori-TFP algorithm As we use memory-resident data in the experiment we believe our algorithm will work better if the data cannot be retained in main memory 7 Conclusion We present an algorithm for computing fuzzy support value using a variant of a set-enumeration tree that is stored with partial support values The tree is built by one database scan The fuzzy support value of any candidate itemset can be worked out just by traversing the tree Most existing methods of mining fuzzy association rules can take advantage of this data structure to improve their performance As the P-tree \(or Fuzzy Pdree\has a simple structure and is an compressed representation of source data to some extent it normally requires far less storage space than source data HoweverFit still may create an exponential storage requirement when source data is densely populated and thus the tree-is not possible to completely be retained in main memory Coenen and his co-workers 13 recommend partitioning the tree to several subtrees and every subtree may b_e retained in main memory. As candidate subsets may belofig to different subtrees we still cannot avoid multiple U0 operations in this way We put forward a new algorithm that may prune a large quantity of nodes and the P-tree can be memory-resident after the first traverse of the tree References R Agrawal R Srikant Fast Algorithm for Mining Association Rules Proc of 20 VLDB Conference Santiago, pp.487-499 1994 G Goulbourne F Coenen P Leng Algorithm for Computing Association Rules using a Partial-Support Tree. Knowledge-Based Systems 00 13 pp.141-149 2000 F Coenen G Goulboume P Leng Computing Association Rules Using Partial Totals Proc of PKDD 2001 eds L.De Raedt and A Siebes LNAI W Au K Chan An effective algorithm for discovering fuzzy rules in relational databases. Proc of IEEE International Conference on Fuzzy Systems FUZZ IEEE 98, Alaska, pp.1314-1319,1998 W Au K Chan Classification with Degree of Membership A Fuzzy Approach Proc of the 1st IEEE Int'l Conf on Data Mining San Jose CA pp J Han M Kamber Data Mining Concepts and Techniques. Morgan Kaufmann Publishers 2001 R Srikant R Agrawal Mining Quantitative Association Rules in Large Relational Tables, Proc Of the ACM SIGMOD Int'l Conf., Monreal, Canada P Hong C S Kuo S C Chi Mining association rules from quantitative data", Intelligent Data Analysis Vol 2168 pp.54-66,2001 35-42.2001 pp 1-12 1996 3 NO 5 pp. 363-376,1999 117 


we can make sure whether the similarity between a pattern and the standard pattern is in reasonable range, thus we can detect intrusions As for standard pattems set P, we can use the encoding algorithm to compute each pattem code at one time, let the codes set to be Ep. As for any pattern PI, we can develop the following algorithm to check whether it is intrusion pattem Begin 1 2 3 standard pattern set is compared with %,,code of pattern pl,and select the minimum begin diffp = I 5, -e I if diff, &lt; min I * the absolute difference between %I and e then min = diff end 4 End 6. Conclusions then alarm\(rep0rt pattem PI is intrusion pattern association rules and frequent episode, and converts pattems of data mining to digits using the encoding method It proves the 5 theorems of the relationship between pattern encoding and pattem similarity, and according to the patterns similarity principle implicated in the theorems and through computing and comparing similarities of two pattems inhusion patterns can be detected. The method that converts intrusion detection pattems in data miniig to digits provides a simple and feasible way for intrusion detection system to automaticalIy , intuitively , efficiently detect network of great trafh Acknowledgements This subject is supported by National Natural Science Foundation of China \(No. 60273075 References l] Paul Dokas, Levent Ertoz, Vipin Kumar,et al. Data Mining for Network IntrusionDetection[Z h t t p : / / w w w . c s . u m n . e d ~ r e s e a r c ~ ~ n d s / p a p g 121 Ningning Wu. Audit data analysis and mining Doctor thesis 1 USA, George Mason University.2001 3] Sudiipto Gnha, Rajeev Rastogi, Kyuseok Shim ROCK : A Robust Clustering Algorithm for Categorical Athibutes[J]. Information Systems, 2000 25\(5 4] W. Lee. A Data Mining Framework for Consmcting Features and Models for Intrusion Detection Systems[D]. USA:COLUMBIA UNIVERSITY.1999 dm-2002.pdf. 2002 This article describes encoding algorithms of 1297 pre></body></html 


A p  b 1  1  2 B p  b 1  1 2 Figure 7 Plot of DIR  J-measure and conditional entropy w.r.t p  a 1 b 0 8.\(A-D show DIR is the most ltering index for the four datasets whichever the threshold chosen between 0 and 1 DIR prunes more rules than the others This is especially useful within the context of association rules where the mining algorithms often generate huge amounts of rules Let us explain why DIR is very ltering In gure 8.\(E in parallel coordinates each line represents a rule The gure shows representative rules from T10.I4.D5k that are judged good by condence but not by the Loevinger index they have a good deviation from equilibrium but not from independence On the other hand gure 8.\(F shows representative rules from BREAKDOWNS that are judged good by the Loevinger index but not by condence they have a good deviation from independence but not from equilibrium DIR gives bad values to all these rules since it takes into account both independence and equilibrium 5 Conclusion In this article we have presented the Directed Information Ratio  DIR   a new rule interestingness measure which is based on information theory DIR is specially designed for association rules and in particular it respects their value-based semantics by differentiating the opposite rules a  b and a  b  Moreover to our knowledge DIR is the only rule interestingness measure which rejects both independence and equilibrium i.e it discards both the rules whose antecedent and consequent are negatively correlated and the rules which have more counter-examples than examples Experimental studies have also shown that DIR is a very ltering measure which is useful for association rule post-processing To continue this research work we will integrate DIR into a data mining platform in order to experiment with this new measure in real applications Like all the information-theoretic measures DIR is a frequential index This means that it takes into account the size of the data only in an relative way and not in an absolute way see More generally  i n o rder to ha v e a complete assessment of the rules one has to measure not only the deviations from equilibrium and independence but also the statistical signicance of these two deviations For example  2  or implication intensity 6 allo w t o m easure the statistical signicance of the deviation from independence while IPEE allo ws to measure the statistical signicance of the deviation from equilibrium These approaches are complementary to DIR  References  R Agra w al H Mannila R Srikant H T o i v onen and A I Verkamo Fast discovery of association rules pages 307 328 AAAI 1996  R J Bayardo and R Agra w al Mining the most interesting rules In Proceedings of ACM KDD1999  pages 145154 ACM Press 1999  N M Blachman The amount of information that y gives about x  IEEE Transcations on Information Theory IT14\(1 1968  J Blanchard F  Guillet H Briand and R Gras Assessing rule interestingness with a probabilistic measure of deviation from equilibrium In Proceedings of the 11th international symposium on Applied Stochastic Models and Data Analysis ASMDA-2005  pages 191200 2005  J Blanchard F  Guillet R Gras and H Briand Mesurer la qualit  edesr  egles et de leurs contrapos  ees avec le taux informationnel TIC Revue des Nouvelles Technologies de lInformation  E-2:287298 2004 Actes EGC2004  J Blanchard P  K untz F  Guillet and R Gras Implication intensity from the basic statistical denition to the entropic version In Statistical Data Mining and Knowledge Discovery  pages 473485 Chapman  Hall 2003 Chapter 28 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


A T10.I4.D5k B T10.I4.D100k C BREAKDOWNS D PROFILES E rule sample from T10.I4.D5k F rule sample from BREAKDOWNS Figure 8 Measure distributions on the whole sets of rules A B C D and on two samples of rules E and F in parallel coordinates  S Brin R Motw ani and C Silv erstein Be yond mark et baskets generalizing association rules to correlations SIGMOD Record  26\(2 1997  S Brin R Motw ani J D Ullman and S Tsur  D ynamic itemset counting and implication rules for market basket data SIGMOD Record  26\(2 1997  P  Clark and T  Niblett The CN2 induction algorithm Machine Learning  3\(4 1989  J Holland K Holyoak R Nisbett and P  Thagard Induction  Processes of inference learning and discovery MIT Press 1986  X.-H Huynh F  Guillet and H Briand ARQA T  An e xploratory analysis tool for interestingness measures In Proceedings of the 11th international symposium on Applied Stochastic Models and Data Analysis ASMDA-2005  pages 334344 2005  S Jarosze wicz and D A Simo vici A general measure of rule interestingness In Proceedings of PKDD2001  pages 253265 Springer-Verlag 2001  I Lerman F oundations in the lik elihood linkage analysis classication method Applied Stochastic Models and Data Analysis  7:6976 1991  B Liu W  Hsu S Chen and Y  Ma Analyzing the subjective interestingness of association rules IEEE Intelligent Systems  15\(5 2000  J Loe vinger  A systematic approach to the construction and evaluation of tests of ability Psychological Monographs  61\(4 1947  B P a dmanabhan and A T uzhilin Une xpectedness as a measure of interestingness in knowledge discovery Decision Support Systems  27\(3 1999  G Piatetsk y-Shapiro Disco v e ry  a nalysis and presentation of strong rules In Knowledge Discovery in Databases  pages 229248 AAAI/MIT Press 1991  J Quinlan editor  C4.5 Programs for Machine Learning  Morgan Kaufmann 1993  M Sebag and M Schoenauer  Generation of rules with certainty and condence factors from incomplete and incoherent learning bases In Proceedings of EKAW88  pages 28.128.20 1988  C Shannon and W  W e a v er  The mathematical theory of communication  University of Illinois Press 1949  P  Smyth and R M Goodman An information theoretic approach to rule induction from databases IEEE Transactions on Knowledge and Data Engineering  4\(4 1992  P N T an V  K umar  and J Sri v asta v a  Selecting the right objective measure for association analysis Information Systems  29\(4 2004  H Theil On the estimation of relationships in v olving qualitative variables American Journal of Sociology  76:103 154 1970  B V a illant P  Lenca and S Lallich A c lustering of interestingness measures In Proceedings of the 7th International Conference on Discovery Science  pages 290297 2004  M J Zaki Mining non-redundant association rules Data Mining and Knowledge Discovery  9\(3 2004 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


21  and denote wjk as the set of weights for Yj where    k j k jw 1 1.  A classifier H is defined as YD ? such that it assigns a weight of the correct class label to an instance as  iWdH where deD, and k j i WW ? . For a set of single-class instances I = &lt; \(x1 y1 x2, y2  xn, yn Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Table 4. Classification accuracy of PART RIPPER, CBA and MMAC Dataset PART RIPPER CBA MMAC Tic-Tac 92.58  97.54  98.60  99.29 Contactlenses 83.33  75.00  66.67  79.69 Led7 73.56 69.34  72.39  73.20 Breastcancer 71.32  70.97  68.18  72.10 Weather 57.14  64.28  85.00  71.66 Heart-c 81.18  79.53  78.54  81.51 Heart-s 78.57  78.23  71.20  82.45 Lymph 76.35  77.70  74.43  82.20 Mushroom 99.81 99.90  98.92  99.78 primarytumor 39.52  36.28  36.49  43.92 Vote 87.81  87.35  87.39  89.21 CRX 84.92  84.92  86.75  86.47 Sick 93.90  93.84  93.88  93.78 Balancescale 77.28  71.68  74.58  86.10 Autos 61.64  56.09  35.79  67.47 Breast-w 93.84  95.42  94.68  97.26 Hypothyroid 92.28 92.28  92.29  92.23 zoo 91.08  85.14  83.18  96.15 kr-vs-kp 71.93 70.24  42.95  68.75 is  m k ii i ydHw m 1  1 ?  , where          yxif yxif yx 0 1  For example, if an item \(A ,a labels  c1    c2  and  c3  7, 5  and 3 times 


labels  c1    c2  and  c3  7, 5  and 3 times respectively, in the training data. Each class label will be assigned a weight, i.e. 7/15, 5/15, and 3/15, respectively for labels  c1    c2  and  c3  This technique assigns the predicted class label weight to the case if the predicted class label matches the case class label. For instance if label  c2  of item \(A, a test data that has  c2  as its class, then the case will be considered a hit, and 5/15 will be assigned to the case 5. Experimental Results We investigated our approach against 19 different datasets from [20] as well as a different datasets for forecasting the behaviour of an optimisation heuristic within a hyperheuristic framework [5, 16]. Stratified tenfold cross-validation was used to derive the classifiers and error rates in the experiments. Cross-validation is a standard evaluation measure for calculating error rate on data in machine learning. Three popular classification techniques a decision tree rule \(PART CBA have been compared to MMAC in terms of classification accuracy, in order to evaluate the predictive power of the proposed method The choice of such learning methods is based on the different strategies they use to generate the rules. Since the chosen techniques are only suitable for traditional classification problems where there is only one class assigned to each training instance, we therefore used classification accuracy derived by only the top-label evaluation measure for fair comparison All experiments were conducted on a Pentium IV 1.6 GH PC.  The experiments of PART and RIPPER were conducted using the Weka software system [20]. Weka stands for Waikato Environment for Knowledge Analysis. It is an open java source code for the machine teaching community that includes implementations of different methods for several different data mining tasks such as classification, clustering, association rule and regression. CBA experiments were conducted using a VC++ implementation version provided by [19]. Finally MMAC was implemented using Java We have evaluated 19 selected datasets from Weka data collection [20], in which, a few of them \(6 reduced by ignoring their integer and/or real attributes Several tests using ten-fold cross-validation have been performed to ensure that the removal of any real/integer attributes from some of the datasets does not significantly affect the classification accuracy. To do so we only considered datasets where the error rate was not more than 6% worse than the error rate obtained on the same dataset before the removal of any real/integer attributes.  Thus, the ignored attributes do not impact on the error rate too significantly Many studies have shown that the support threshold plays a major role in the overall classification accuracy of the set of rules produced by existing associative classification techniques [9, 12]. Moreover, the support value has a larger impact on the number of rules produced in the classifier and the processing time and storage needed during the algorithm rules discovery and generation. From our experiments, we noticed that the support rates that ranged between 2% to 5% usually achieve the best balance between accuracy rates and the size of the resulted classifiers. Moreover, the classifiers derived when the support was set to 2% and 3 achieved high accuracy, and most often better than that of decision trees rule \(PART the MinSupp was set to 3% in the experiments. The confidence threshold, on the other hand, is less complex and does not have a large effect on the behaviour of any associative classification method as support value, and thus it has been set to 30 


Table 4 represents the classification rate of the classifiers generated by PART, RIPPER, CBA and MMAC against 19 benchmark problems from Weka data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 3 5.00 2 5.00 15.00 5.00 5.00 15.00 2 5.00 3 5.00 4 5.00 55.00 6 5.00 75.00 8 5.00 9 5.00 1 2 3 4 5 6 7 8 9 Nine  Scheduling Runs D iff er en ce  in  A cc u ra cy  CB A To p-label A ll-label A ny-label Figure 3a. Difference of accuracy between MMAC evaluation measures and CBA algorithm 35.00 25.00 15.00 5.00 5.00 15.00 25.00 35.00 45.00 55.00 65.00 75.00 85.00 95.00 1 2 3 4 5 6 7 8 9 Nine  Scheduling Runs D iff er en ce in  A cc u ra cy  P A RT To p-label A ll-label A ny-label Figure 3b. Difference of accuracy between MMAC   evaluation measures and PART 


MMAC   evaluation measures and PART algorithm 0 2 4 6 8 10 12 14 16 18 20 22 24 26 Run1 Run2 Run3 Run4 Run5 Run6 Run7 Run8 Run9 Ten Runs  Scheduling Data N um be r o f R ul es To p Label P A RT CB A Figure 4. Classifier sizes of MMAC \(toplabel the scheduling   data collection. The accuracy of MMAC has been derived using the top-label evaluation measure. Our algorithm outperforms the rule learning methods in terms of accuracy rate, and the won-loss-tied records of MMAC against PART, RIPPER and CBA 13-6-0, 15-4-0 and 154-0, respectively The evaluation measures of MMAC have been compared on 9 solution runs produced by the Peckish hyperheuristic [5] with regard to accuracy, and number of rules produced. Figures 3a and 3b represent the relative prediction accuracy that indicates the difference of the classification accuracy of MMAC evaluation measures with respect to those derived by CBA and PART, respectively. In other words, how much better or worse MMAC measures perform with respect to CBA and PART learning methods. The relative prediction accuracy numbers shown in Figures 3a and 3b are conducted using the formula PART PARTMMAC Accuracy AccuracyAccuracy  and CBA CBAMMAC Accuracy AccuracyAccuracy  respectively. After analysing the charts, we found out that there is consistency between the top-label and label-weight measures, since both of them consider only one class in the prediction. The top-label takes into account the topranked class, and the label-weight considers only the weight for the predicted class that matches the test case Thus, both of these evaluation measures are applicable to traditional single-class classification problems. On the other hand, the any-label measure considers any class in the set of the predicted classes as a hit whenever it matches the predicted class regardless of its weight or rank. Is should be noted that, the relative accuracy of MMAC evaluation methods against dataset number 8 in Figure 3a and 3b, is negative since CBA and PART 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





