RDF Visualization  Thinking Big 003 Ji 020 r  021 Dokulil Faculty of Mathematics and Physics Charles University Prague Czech Republic dokulil@ksi.mff.cuni.cz Jana Katreniakov  a Faculty of Mathematics Physics and Informatics Comenius University Bratislava Slovakia katreniakova@dcs.fmph.uniba.sk Abstract Graph visualization is one of the popular ways to present RDF data But all practical RDF visualizers have to somehow deal with the problem of the size of the data Not only can the total number of triples in RDF data be very large but even degrees of the nodes of the graph can be very high This article discusses some problems and solution related to visualization and exploration of such data usually in relation to the RDF node merging technique 1 Introduction The Semantic Web idea is already well established as well as some of the standards that accompany it One of those standards is the RDF data format which is intended to be the low-level format for semantic data By their very nature the RDF data form a directed labeled graph This may be used to 002ght one problem  the RDF data often tend to be large complex and hard to read and explore when serialized to some text-based format especially in the case of RDF-XML If we present the data visually we may be able to give the user much better idea about the content and structure of the data It is important to note that this visualization is aimed at the same group that would otherwise look at the raw data in some serialized form  developers of RDF-based software not the users of the Semantic web But to handle data that may contain millions or more of nodes and edges the visualization itself is not suf\002cient and some kind of navigation is necessary A few years ago we developed a navigating RDF visualizer that uses several new techniques notably triangle layout and node merging 6 4 It is one of the visualizers that display the pure RDF data The node merging technique was used to reduce the space needed to display the data and to present possibilities for further navigation to the user The basic idea is to draw one node of the RDF graph as a rectangle that not 003 This research was supported in part by Grant Agency of Charles University GAUK and VEGA 1/0726/09 Figure 1 Merged node only displays information about the node but also about its neighbors and edges that connect the node to the neighbors For an example see the Figure 1 The triangle layout is an algorithm for layered drawing of trees It was designed to handle different sizes of merged nodes but the exact layout need not concern us in this paper since the techniques described in this paper do not depend on it in any way The section 2.1 deals with the problem of selecting the 002rst node to be presented to the user and the section 2.2 focuses on nodes with high degree The section 3 explains the idea of tabbed browsing in RDF visualizers Handling of large graph drawings is discussed in the section 4 The last section of the paper is concerned with the usability of the proposed solutions in other visualizers and it is followed by the concluding remarks 
20th International Workshop on Database and Expert Systems Application 1529-4188/09 $25.00 © 2009 IEEE DOI 10.1109/DEXA.2009.64 459 


2 Determining points of interest The great disproportion between the amount of information that can be stored in an RDF graph and the information that can 002t onto a screen means that the user can only see small portion of the graph This section deals with several types of situation where the visualizer is forced to choose one or several of many items 2.1 Starting node Our visualizer starts by displaying one node in the form of a merged node and lets the user navigate to other parts of the graph The problem is selecting the best node to start the visualization One important aspect to mind is the size of the displayed graph  it is often not possible or viable to load and analyze the whole data when the visualizer starts The size of the data may exceed the available memory or loading might take too long Even if the visualizer is using a database as its data source it may not be possible to compute the optimal starting node at the server side since most such algorithms use graph operations which are poorly supported by most RDF database On the other hand there is the fact that RDF databases can store arbitrary data and that namespaces can be used to store data from different domains without breaking other applications This way the visualizer can store some metainformation about the data directly in the database For instance one of the nodes can be marked as the best starting node and the visualizer can start by querying the database for such node This eliminates the need to compute the starting node every time the visualizer starts but still it has to be computed at some point The advantage is that this process can occur at the server thus eliminating the huge data transfers For example application server modules of the Trisolda architecture are ideal for this task b ut some solution could be used with any centralized RDF store This architecture allows us to use even complex analysis of the whole data to select the starting node But let us start with some simpler possibilities In some cases the starting node can be speci\002ed manually For example consider RDF data containing organizational structure  in this case the root of the structure is probably the best place to start a visualization and it can easily by de\002ned by the database administrator or by the user during his or her previous session Another simple case is selection of a random node This would work for data that correspond to a graph with relatively small diameter In that case any node can be used to quickly navigate to any part of the graph The question is determining the random node One possibility is to use the server-side processor that selects a node and marks it as a root Another much simpler is making a database query whose solution is the list of all triples or nodes  depending on the query language used The visualizer can either choose a random node from the complete result set or simply select the 002rst record returned by the server A whole class of algorithms can be de\002ned as graphbased selection  These focus on the structure of the graph de\002ned by the RDF statements and usually require some server-side processing since they process the whole data Obvious examples are 002nding the center of the graph or 002nding the node with the highest degree The idea is that such nodes provide good access to the rest of the graph Using the center of the graph produces undesirable result if the graph contains a long path  in that case the start node can easily be near the center of the path forcing the user to navigate to one end This situation can occur if there is a big collection represented as a 002rst-rest type of a linked list Using a node with the highest degree is even more likely to select a bad starting point The reason is that e.g node that represents the numeric literal 1 is very likely to have a high degree and it may be connected to many different types of nodes In that case we face a problem very similar to the starting node selection problem only the number of possibilities is limited An interesting option is some variant of the PageRank  algorithm But it is problematic since in the visualization incoming and outgoing edges are equally important and the PageRank would likely select a node with high number of incoming edges resulting in the same problem that was described for the highest degree selecting algorithm A completely different approach is to start the visualizer with a search dialog that allows the user to select a node The problem is that the user has to be at least partially familiar with the data An alternative is to list all predicates used in the data Their number should be much lower usually tens or hundreds of different URIs After that some of the previous techniques can be used to select the starting node with the limitation that it must be an endpoint of an edge labeled by the selected predicate For instance one such node can be selected randomly Yet another variant is to strengthen the cooperation between the server and the visualizer As the user navigates the data the visualizer informs the server about the nodes the user visited and the server keeps the statistics As was mentioned earlier this information can be stored by the RDF store alongside the actual data The starting node is determined by randomly selecting one of the nodes that was visited most times Note that displaying a node as one item in the merged node is not considered to by a visit to the node The user must actually navigate to the node  this eliminates the problems caused by common literals like the number 1 
460 


So far we have assumed that the visualized graph is connected i.e there is a non-directed path between any two nodes But the RDF data need not look that way although it is common since for example the whole data may share the same class hierarchy or two parts may be connected via a literal If the data is not connected and we cannot use serverside analysis there is no way to handle the problem since we need almost the whole data to check whether it is connected or not With server side processing we can identify the components of the graph and then select starting node for each of them separately Then we create a virtual node and connect it to all of the individual starting nodes The virtual node and the new edges are sent to the user as the starting merged node While we could present the user with some list of components this solution should be just as useful and does not introduce new concepts to the visualizer application Based on these options we have decided to use the following solution If the data on the server contain information about preferred starting node it is used If no server side processing can be done and the data is read from a remote data source a database we can only use a random starting node This means that if the data is not connected the user cannot access the whole data If the data can be made available locally e.g if the visualizer is used to process one local RDF 002le or we know that the size of the data is comparable to the transfer rate from the server and locally available memory we use the random node approach but extended with component handling described in the previous paragraph If server-side processing is available we perform component analysis on the server and then return random node for each component A search dialog is an interesting option for any situation but its design and capabilities have to be tailored to the actual data access layer used by the visualizer Still it should be present at least in some limited form in all situations To at least handle the situation where the user is interesting in one speci\002c node i.e the URI or literal value is known It is tempting to include the statistics of users behavior The problem with this approach is the use-case we are trying to handle i.e pure RDF visualization for software developers For this approach to work we need to collect reasonably large sample This means having large group of developers working on the same data set When new developer joins the team he or she may bene\002t from the usage data already collected but it is questionable whether the relatively large effort is worth it Despite all these doubts we plan to test such system in the future 2.2 Nodes with high degree In order to create only reasonable drawings of the displayed graph we cannot allow merged nodes to have arbitrary height The maximal reasonable number of edges displayed in a merged node seems to be somewhere between 20 and 40 But the actual degree of a node can go to thousands or in extreme cases even millions one of our test data sets contains a node with degree of about 1.3 million although it was rather pathological situation As we have already mentioned while the number of incident edges i.e statements that contain the value represented by the node in question can be very large the total number of different predicates is usually small If this is the case we can display only one row for each predicate either listing the predicate and total number of edges or predicate total number of edges and one example The choice depends on the data layer  whether it is capable of providing just the number in a much more ef\002cient way e.g a variant of SQL's select count    group by     than providing the number and the example e.g by listing all nodes To help the user backtrack his or her way through the graph the edge connecting a node to its parent the node it was reached from is always displayed as a separate line This is also done with the edges that connect the node to any other node that is currently displayed providing that the number of such edges is small There is one typical situation where the predicate based compression would not work The RDF recommendation provides vocabulary for containers and members of the container are speci\002ed by sequentially numbered predicates This usually means that there are as many different predicates as there are outgoing edges Fortunately this situation is easily distinguishable and can be solved by displaying only some elements stored in the container The actual of displayed elements depends on the count of other incident edges  such edges have precedence over the elements in the container and they are handled as if there were no elements only with slightly lower limit for the number of rows  we always want to display at least a few elements stored in the container If even the reduced number of rows that should be displayed in the merged node is too high we display only the most important with highest cardinality rows and use the last row to inform the user about the total number of edges and predicates that had to be omitted It is obvious that we need to provide the user with some way of accessing even the edges that have not been displayed in the merged node Such situation is always represented graphically  either the number of edges with the same predicate is displayed or the rows-omitted notice is present In such case the user can display a special dialog that can be used to search the complete list of incident 
461 


edges The user can specify a substring or regular expression this once again depends on the capabilities of the underlying data layer  some RDF stores can handle regular expressions some cannot for the value of subject predicate or object and all edges that satisfy the condition are displayed Note that it does not make sense to specify both subject and object since one of them must be equal to the node whose neighbors we are exploring The predicate can be selected from the list of all relevant predicates since  as we have already stated  their number is small in most cases The user can select any node displayed this way and expand the current view of the graph by addition of that node 3 Multiple views One of the signi\002cant advances in web browsers was the addition of tabs i.e the ability to display more pages withing one browser instance and quickly switch between them It has quickly become very popular Some people use even tens of tabs while they surf the Web requiring additional improvements to the tabs support in browsers or third party plug-ins to better organize the tabs Two common usage scenarios are browsing pages like Wikipedia or e-shopping In the 002rst case many users tend to read one page and when they encounter an interesting link they open it as a new tab but continue reading the original page The opened tabs contain topics the user intends to read in the future When shopping one common practice is to browse the categories in the shop and open pages for items that appear interesting at a 002rst glance in new tabs After browsing the categories the user explores the selected items in greater detail and closes the tabs with items he or she is no longer interested in These are similar to some of the ways an RDF visualizer can be used The user is either exploring the data and he or she may come to a point where two or more promising directions of further navigation are at hand Or the user may be shopping for interesting data i.e searching for data items that are worth further analysis or processing Addition of tabs to an RDF visualizer is very straightforward  all possible drawings the basic view node neighbor explorer special views like rei\002cation view or neighbor view     can be displayed as a tab At any point where such view can be opened it can be opened as a new tab Furthermore any node in any view can be used to open new tab with the visualization containing only the selected node because it is displayed as a merged node so the user can navigate explore the neighborhood of the node 4 Big fat graphs As we have already stated we cannot display the whole data as their amount cannot even be processed in reasonable time let alone display it within reasonable space Of course there are visualizations where this can be done for example visualizations of web page relations or large social networks their purpose is to only suggest the overall structure of the data not display individual items Although our visualization  the triangle layout algorithm  produces asymptotically optimal area the layout alone can not overcome the problem raised by the large size of the graph Some navigation in the graph is necessary In authors summarize three basic w ays for na vig ation in large graphs Zoom is traditional tool in visualization It is well suited for graphs since in most cases the zoom can be made easily by scaling the image However for the zoom we have to draw the whole image whole RDF graph 002rst This is often impossible since the size of the data may exceed the available memory or the load time may be way too long Focus+context Another well known problem with zooming is the loss of the context This can be partially overcome by displaying a map of the whole graph Other possibility is to draw focus and context in the same image For this purpose the 002sheye technique can be used The technique is independent of the layout algorithm and is practically a separate processing step on the graphical layout of the graph The view of the graph enlarges the area of interest the so called focus  and shows the other parts of graph with less details e.g more distant parts are smaller Incremental exploration As we have already mentioned the size of the graph may prevent us from processing it all at once Incremental navigation is a good choice in such cases Only a small part of the graph  a logical frame  is displayed possibly even just one node and then according to some strategy new logical frames are generated as modi\002cations of previous frames In our RDF visualizer we use the incremental exploration where the user navigates the data by adding connected nodes to the already visible part of the graph However even by the navigation the graph may grow more than we can display Therefore we need additional navigation in the displayed view For this purpose either a map see Figures 2\(a and 2\(b of the context or a cartesian 002sheye view see Figure 2\(c may be used Since the current frame displayed part of the whole data is smaller than the 
462 


a Map b Focus c Cartesian 002sheye view the same focus Figure 2 Additional navigation data by several orders of magnitude we can use the techniques without worrying about required memory and computational time 5 Related work Some of the approaches described in the section 2 are dependent on the node merging technique but most of them can either be used directly in other visualizers adapted for different drawings or the visualizers can be extended to support node merging this would be possible in many of them The starting node selection problem is common for all visualizers Since most of the visualizers always display all neighbors of a node the problem of picking the rows to display in a merged node do not apply to them directly But in order to display nodes with extremely high degree some of them would have to adopt a similar approach since most drawings of the RDF graph cannot reasonably handle nodes with a million or more neighbors The node neighbor explorer could be used in any visualizer as well as tabbed browsing Most of the visualizers already have their ways to handle large graphs 6 Conclusions The extreme size of many RDF data sets is a big concern for any RDF visualizer In this article we pointed out some of the problems with several variants for their solution One big problem is the selection of the 002rst node to be presented to the user We have presented several options but the different deployment scenarios and the size of the data together with the fact that there are no limitations to their structure connected/unconnected graph arbitrary degree of nodes etc limit the usability for most of them making even the most simple solution  selecting a random node  an interesting and reasonable option A similar but fortunately more limited situation is handling of nodes with high degree Typical properties of RDF data most notably the limited number of distinct predicates allow for better handling of such situation The user can be presented with a condensed overview while being able to access all neighbors via a search dialog An interesting way of extending a navigating visualizer not only for RDF is to use tabbed browsing known from most current web browsers While unusable to visualize the data directly some of the common graph drawing techniques can be used in conjunction with the navigation References  R Angles and C Gutierrez Querying RDF Data from a Graph Database Perspective In Proceedings of ESWC 2005  Springer-Verlag Berlin Heidelberg 2005  T  Berners Lee J Hendler  and O Lassila The Semantic Web Scienti\002c American  May 2001  J J Carroll and G Klyne Resource Description Frame w ork Concepts and Abstract Syntax W3C Recommendation 2004 http://www.w3.org/TR/2004/REC-rdf-concepts-20040210  J  Dokuli l and J Katreniak o v  a Visualization of large schemaless RDF data In International Conference on Mobile Ubiquitous Computing Systems Services and Technologies UBICOMM 2007 includes SEMAPRO 2007 WSNEXT 2007 MUTL 2007 VVSSearch 2007  pages 243–248 Los Alamitos IEEE Computer Society 2007  J  Dokulil and J Katreniak o v a Na vig ation in RDF Data In IV 08 Proceedings of the 2008 12th International Conference Information Visualisation  pages 26–31 Washington DC USA 2008 IEEE Computer Society  J Dokulil and J Katreniak o v  a Visual Exploration of RDF Data In SOFSEM 2008 Theory and Practice of Computer Science  pages 672–683 Springer-Verlag Berlin Heidelberg 2008  J Dokulil J Y aghob and F  Za v oral T risolda The en vironment for semantic data processing International Journal On Advances in Software  1\(1 2008  I Herman G Melanc  on and M S Marshall Graph V isualization and Navigation in Information Visualization A survey IEEE Trans Vis Comput Graph  6\(1 2000  L P age S Brin R Motw ani and T  W inograd The P ageRank Citation Ranking Bringing Order to the Web 1999 
463 


  6 that can be downloaded to each of the its components while they are running to provide hot swaps of software FRONTIER is composed of a hardware-specific interface to the external world and seven reusable components \(see Figure 3 Figure 3: FRONTIER Architecture Diagram  The eight components of the system, when unified, form a dynamically programmable gene ral purpose front end data processor that provides the following functionality   Source of all external data for all reasoning components via a consistent external interfaces   A uniform and ubiquitous exchange of data and information between reasoning components   A subscription-based relational database to store real-time data from extern al sources and distribute real-time and historic numeric and symbolic information between reasoning components   Filtering of real-time measurements to eliminate irrelevant data   Up- and down conversion of the sampling of measurements through programmable rate converters   Automatic correlation and alignment of timeskewed measurements so measurements properly arrive on actual or pseudo frame boundaries   Programmable numeric preprocessing of data \(e.g engineering units conversion, feature extraction   Programmable event detection based upon complex relational expressions through a rulebased language to provide inference annotation of measurements   All components of the FRONTIER are commanded and programmed through the subscription-based database   Optional socket-based interfaces between reasoning components so components can be remotely located CONCLUSIONS Verification and validation of software systems \(V&V\has been addressed by a broad range of methods, including static analysis, model checking, testing, runtime monitoring and formal proofs. In this paper, we describe the design of an introspection infrastructure for applications in an environment with a large number of parallel threads.  Our approach to introspection integrates the monitoring of executions, analysis of system state, and changes to the system state.  Compared to th e current state-of-the-art, the innovative features of our work include the following  We have designed a generic software framework for introspection providing a “backplane” capability with plug-in functionality, and realized by a set of asynchronous agents  The objects of the introspection capability are applications characterized by explicit and implicit concurrency, and executing on multi-processing hardware systems providing a large number of threads  Our design focuses on the requirements of future spaceborne systems that need to support an enhanced degree of autonomy and  provide application-specific adaptive fault tolerance  The combination of monitoring, analysis, and feedback-oriented modifica tion of system state has been described in the literature and partially implemented in a number of systems, but no design based on massively parallel multi-core architectures has yet been implemented Current approaches to V&V usually deal with individual methods in isolation.  While we do not at this time present a unified approach to V&V, we plan to combine elements of static analysis, formal proofs, and monitoring in our future work R EFERENCES AND C ITATIONS  Afjeh,A., Homer,P., Lewandowski,H., Reed,J.  and Schlichting,R.  Development of an intelligent monitoring and control system for a heterogeneous numerical propulsion system simulation In Proc 28th Annual Simulation Symposium Phoenix, AZ, April 1995 Brockmeyer,M.,  Jahanian F., Heitmeyer, C.,and Labaw B.An approach to monitoring and assertion-checking of real time specifications in Modechart In Workshop on Parallel and Distributed Real-Time Systems April 1996 Gu,W., Eisenhauer,G., Schwan,K and Vetter,J.  Falcon: Online monitoring for steering parallel programs 


  7 Concurrency: Practice and Experience 10\(9\699–736 Aug 1998 Fijany, F.  Vatan, A.  Barrett, M.  James, C.  Williams and R Mackey, “A Novel Model-Based Diagnosis Engine Theory and Applications”, 2003, IEEE Aerospace Conference, 2003 Fijany, F.  Vatan, A.  Barrett, M.  James, and R.  Mackey An advanced model-based diagnosis engine, The 7th International”, Symposium on Artificial Intelligence Robotics and Automation in Space, 2003 James, M., “NMC, A Data Exchange Server, Reference Manual”, Jet Propulsion Laboratory, California Institute of Technology, 2003 James, M., “Quick Cache.  A Multi-Tiered Real-Time, Cache System for Monitoring Hardware Systems, Reference Manual”, Jet Propulsion Laboratory, California Institute of Technology, 2002 James, M., “SHINE,a High-Speed Inference for the Diagnosis of Complex Software and Hard ware  Systems, Reference Manual”, Jet Propulsion Laboratory, California Institute of Technology, 2003 James, M., “FRONTIER, a General Purpose Front-End for Real-Time Reasoning Systems”, Jet Propulsion Laboratory, California Ins titute of  Technology, 2004 Mehrotra, P.  and Zima, H.P High Performance Fortran for Aerospace Applications Parallel Computing Special Issue on Parallel Computing in Aerospace, Vol.27 No.4,pp.477--501 \(2001 Miller,B., Callaghan, M.,Cargille,J., Hollingsworth,J., Irvin B.,Karavanic, K., Kunchithap adam,K.  and Newhall,T Paradyn parallel performance measurement tools IEEE Computer 28, November 1995 Schroeder,B., Aggarwal, S.,and Schwan,K.  Software analysis of safety constraints In Proceedings 16th Symposium on Reliable and Distributed Systems pages 80–87 IEEE Computer Society, October 1997 Snodgrass,R.  A relational approach to monitoring complex systems IEEE Transactions on Computers 6\(2\156–196 May 1988 Sterling, T.L.  and Zima, H.P.  Gilgamesh: A Multithreaded Processor-In-Memory Architecture for Petaflops Computing, Proceedings of SC2002 -- High Performance Networking and Computing, November 2002 Widom,J.  and Ceri,S.\(Editors Active Database Systems  Morgan Kaufmann, 1996 Zima, H.P., Chapman, B.M.  Supercompilers for Parallel and Vector Computers. ACM Press Frontier Series/AddisonWesley \(1990 Zima, H.P., Chapman, B.M.  Compiling for DistributedMemory Systems. Invited Paper, Proceedings of the IEEE.  Special Section on Languages and Compilers for Parallel Machines, pp.  264-287 \(February 1993 B IOGRAPHY  Mark L.  James is a senior R & D researcher of the Advanced Computing Algorithms and ISHM Technologies Group in the Flight Software and Data Systems Section within NASA’s Jet Propulsion Laboratory Pasadena, CA, California Institute of Technology.  At JPL Mark is Principal Investigator and Task Manager in realtime inference and knowledge-based systems.  His primary research focus is on high-speed inference systems and their application to planetary and d eep spacecraft systems, and medical applications.  His expertise includes core artificial intelligence technology, high-sp eed real-time inference systems, flight system software architectures and software design and implementation for those systems Mark has received a number of NASA awards which include: Monetary NASA Board Awards, NASA Software of the Year Award Nominee, JPL Team Excellence Award JPL JET Productivity Award, Major NASA Monetary Award, NASA Certificate of Recognition, NASA Achievement Award, NASA E xceptional Service Medal and NASA Exceptional Service Plaque for 1989.  He is author of two inventions, 36 scientific and technical papers, 10 technical manuals, 3 patents, and 73 NASA recognized new technologies Hans P. Zima is a Principal Scientist in the Information Systems Directorate of the Jet Propulsion Laboratory in Pasadena and a Professor Emeritus of the University of Vienna, Austria. He received his Ph.D. degree in Mathematics and Astronomy from the University of Vienna in 1964  His major research interests ha ve been in the fields of highlevel programming languages compilers, operating systems, and advanced software tools.  In the early 1970s while working in industry, he designed and implemented one of the first high-level real-time languages for the German Air Traffic Control Agency.  During his tenure as a Professor of Computer Science at the University of Bonn Germany, he contributed to the German supercomputer project “SUPRENUM”, leading the design of the first Fortran-based compilation system for distributed-memory architectures \(1989\.  After his move to the University of Vienna, he became the chief designer of the Vienna Fortran language \(1992\ that provided a major input for the High  


  8 Performance Fortran de-facto standard. His research over the past four years focused on the design of the “Chapel programming language in the framework of the DARPAsponsored High Productivity Computing Systems \(HPCS program. During the past year, Dr. Zima has become involved in a design effort targeting space-borne faulttolerant high capability computi ng systems. He is the author or co-author of about 200 publications, including 4 books 


that is based on stereo range information Taken together all of the stereo range maps derived from stereo image pairs form a cloud of 3D points all around the rover that conform to the terrain surface We can derive a height map or elevation map from the stereo point cloud by creating an artificial grid in X northing and Y easting and letting the Z axis be elevation We can arbitrarily select the spacing horizontal and vertical grid to achieve the desired mosaic image resolution Early work in this area suggests that 0.01 collisions between multiple points that occupy the same grid cell the maximum elevation should be selected In undulating terrain it is important to preserve relatively large grid regions where no points are available such regions are often the invisible rock and terrain features from the rover's point of view that are the failing of the current state of practice Very small regions of a few contiguous cells or even single cells in the grid will sometimes occur due to failures in stereo correlation due to lack of sufficient Figure 6-Overhead mosaics of Opportunity Navcam images Above a mosaic based on a ground plane projection Below a mosaic based on an elevation map meter per pixel spacing is effective for Navcam elevation maps The overall size of the mosaic can be selected arbitrarily as well Empirical use of Navcam stereo data for targeting and navigation indicates a distance of 15 meters from the rover in any direction is the maximum beyond which the data becomes too sparse to derive an effective elevation map To create the elevation map the point cloud can be inspected one point at a time and the elevation value assigned to the X-Y grid cell in which it falls To resolve texture or other factors To fill in these very small regions with no data a median filter can be applied over the elevation map to assign the median elevation value of those in the local neighborhood to the cell Once assembled the elevation map is now usable as a mosaic projection surface A mosaic image the same size as the elevation map may be constructed by projecting the 3D point formed by the X-Y cell coordinates and the Z 9 


elevation value through the camera model into the original Navcam or Pancam image The corresponding color value at the pixel coordinates ideally an interpolated value is assigned to the mosaic image Figure 6 shows an example of such a mosaic When overlaid onto a HiRISE image this type of mosaic adds useful context including surface features observed by the rover as well as enabling the correlation of targets where observations made by other instruments in the rover science payload are located Further development and integration of this mosaicking methodology is currently underway at the time of this writing and will be further matured in the coming years leading up to Mars Science Laboratory operations 5 CONCLUSIONS Adaptive level-of-detail tile based delivery of images is now supporting operations of the MER and Phoenix missions The performance of this strategy has proven to be superior by orders of magnitude to the previous state of practice The original Science Activity Planner system required entire image collections to be transferred to remote users in order to browse mosaics The transfer time for Pancam image collections was typically on the order of 15 minutes or longer and the delay was compounded by Internet connections over very long distances such as across the United States or to other countries The coast-to-coast transfer time is now only a few seconds or less for each screen of tiles for the initial transfer Tiles are only transferred once and then stored in a local browser cache to prevent redundant transfers JPEG2000 image compression is being used to compress the tiled image data that is served to the science investigators The quality of this image compression has proven to be very suitable for Mars rover images and the performance superior to other formats such as JPEG and PNG The JJ2000 reference implementation for Java applications 5 has proven in practice to be a capable implementation for encoding and decoding JPEG2000 image data 6 FUTURE WORK Microsoft's HD Photo compression algorithm offers reduction in image size similar to JPEG2000 with a lower computational cost This technique may be an improvement over JPEG2000 encoding of Mars rover images although at the time of this writing it still lacks the multiplatform support that is required to serve a diverse community of science investigators If this limitation is overcome it would be worthwhile to experiment with HD Photo and quantify its performance when compared with JPEG2000 in quality size reduction and computational cost On July 31 2007 the Joint Photographic Experts Group and Microsoft announced that this compression technique was under consideration for a new JPEG standard tentatively titled JPEG XR 6 This will hopefully lead to a multiplatform implementation of this compression algorithm that could be applied to many domains such as Mars rover imaging REFERENCES 1 Justin N Maki Todd Litwin Mark Schwochert Ken Herkenhoff Operation and Performance of the Mars Exploration Rover Imaging System on the Martian Surface 2005 IEEE International Conference on Systems Man and Cybernetics October 10-12 2005 2 Y Yakimovsky and R Cunningham A system for extracting three-dimensional measurements from a stereo pair of tv cameras Computer Graphics and Image Processing vol 7 pp 195-210 1978 3 Jeffrey S Norris Mark W Powell Marsette A Vona Paul G Backes Justin V Wick Mars Exploration Rover Operations with the Science Activity Planner IEEE International Conference on Robotics and Automation April 2005 4 J S Norris M W Powell J M Fox K J Rabe I Shu Science Operations Interfaces for Mars Surface Exploration 2005 IEEE Conference on Systems Man and Cybernetics October 15-17 Big Island HI October 15 2005 5 JJ2000 JPEG2000 reference implementation of JPEG2000 http://jj2000.epfl.ch 6 Microsoft HD Photo press release ACKNOWLEDGEMENTS The research described in this publication was carried out at the Jet Propulsion Laboratory California Institute of Technology under a contract with the National Aeronautics and Space Administration BIOGRAPHY Mark Powell is a Senior Member of Technical Staff at the Jet Propulsion Laboratory Pasadena CA since 2001 He received his Ph.D in Computer Science and Engineering in 2000 from the University of South Florida Tampa His dissertation work was 10 


in the area of advanced illumination modeling color and range image processing applied to robotics and medical imaging and received the award for Outstanding Dissertation from the University of South Florida At JPL his area offocus is science data visualization and science planning for telerobotics He supported the 2004 Mars Exploration Rover MER mission operations as a Science Downlink Coordinator facilitating the timely downlink and analysis of science data from the rovers He received the NASA Software of the Year Award for his work on the Science Activity Planner science visualization and activity planning software used for MER operations He also received the Imager of the Year award from Advanced Imaging Magazine for his work on Maestro the publicly available version of the Science Activity Planner for MER Mark has been programming in Java and loving every minute of it since it was first used in web browsers in 1995 He his wife Nina and daughters Gwendolyn and Jacquelyn live in Tujunga CA Thomas Crockett started working at JPL in 2005 after completing his Bachelor of Science degree in Computer Science at the University of Arizona where he discovered an interest in graphics He joined the Maestro team in 2006 and began developing advanced image browsing capabilities that would be used by the MER Phoenix and MSL missions to view and smoothly navigate around very large images Recently he has been extending this work to tackle the problem of mapping and spatial browsing of the science data collected by a mission Jason Fox received his Masters of Science degree in Computer Science from Purdue University in the spring of 2003 At that time he began working for the NASA Jet Propulsion Laboratory in Pasadena California in the Planning Software Systems group Jason first worked on the Mars Exploration Rover MER mission performing verification and validation of the Activity Planning and Sequencing Subsystem's modeling infrastructure After the rovers  successful landing his role shifted to that of Tactical Activity Planner on the Integrated Sequencing Team and was responsible for the daily construction of the integrated rover activity plan After leaving MER Jason joined the Maestro team and began development on the science operations tool Maestro the successor to the Science Activity Planner co-winner of the NASA Software of the Year in 2004 In addition to Maestro he is also the JPL leadfor the Phoenix Science Interface tool that will support scientific operations of the Phoenix lander 2007 At JPL his areas of focus include collaborative distributed operations for Mars rovers and landers and science planning for telerobotics In his spare time Jason is in training for the Coeur d'Alene Ironman competition to be held in June 2008 Joseph Joswig received his Masters of Science degree in Computer Science from the University t o California Los Angeles in the Spring of 2005 He is a Software Engineer in the Planning Software Systems various~Gou flth NAssiAn Jeta ER heix n S and tchnolgy prjec Propul s inc Laboratiory in or wa ouedo eeorked fo mutherpast twoun softwaresysteyears asL aTLT moembe NS-ofnthe Cener K10 ove lso h Ma estrog thea Masupproting wrhafoue ondeveloping o a multi-robot gomnadcn roundste to support White Sands Missile Range Jeffporruisy isath Exportin ovrsorwhc supy ervisorwne of the 2004NASASofwareof te Plannn Awr.H softwarenl leadng te deelopentf operatlions system s for ah20 Mar SieneabratryRoerndaMars et Sofut r Cassni Satunian Orbter Mar Reco naisanc Orbter and.the.pirit an pportunityM....M ars Explratin.Roers-.fo.whih.thy.wre.c-winer.o.th 2004 NASA Software of the...Year...Award.....He..is..currently leading the develoment.of.the.uplinksystem.for.the.200 MarScencLaoraoryRovroandalvaiety Laofrlunry and Martian operations technology projects Jeff is a strong 11 


advocate for the application of agile development to Cora and has a lovely daughter Sara In his spare time methodologies and open source software in mission critical he likes to run marathons and continually bugs his friends applications He received Bachelor's and Masters degrees to join him in Computer Science from MIT and lives with his wife and two children near Pasadena CA Khawaja Shams joined the Planning Software Systems group at the NASA Jet Propulsion Laboratory in 2005 and he has since been focused on development of OSGI-based web services to enable Maestro's rich client applications His prior work experience includes employment at Malin Space Science Systems and the Internet Protocol Team at Nokia Mobile Phones Khawaja earned a Master's degree in Computer Sciencetfom Cornell University and a Bachelors degree in Computer Science from University of California San Diego Khawajaes current research interests include browser-based telemetry monitoring systems for robotics peer-to-peer systems and PESTful web based services memobern of Tathe a Uplaning edrsosbefrpann Sotwae systems itgropatigwtthsceetam an theadn Jth Pnieropliongemtruhtedyt a Laboratiory.o HER recer ived rton uis,h oe fomt bigaprofthe Ca Monaetoeabinthlasnfr woredME ponetiherueo the Sciencelanin sQfta sotae develsopingeoe frte asScec Lactivit PMLane and Seqene InHLTEgRatono Engineer constructing validating..and.veri.ying..and.bundling sequences used to command the MER rovers....He.moved onto being a Tactical Uplink Lead responsible for planning the sequences integratingwith the science team an softare.o Hey iorsalso a developer fortheMar.Scenc Lander MSL and the ATHLETE Robot project    investigating robots to be sent to the Moon Jay is married 12 


11 Xiao Yang L Haizhon S Choi 2004 Protection and Guarantee for Video and Voice Traffic in IEEE 802.1 le Wireless LANs INFOCOM 2004 Twenty-third Annual Joint Conference of the IEEE Computer and Communication Societies Volume 3 Issue 7-11 21522162 12 W Spearman J Martin A Distributed Adaptive Algorithm for QoS in 802.1 le Wireless Networks Proceedings of the 2007 International Symposium on Performance Evaluation of Computer and Telecommunication Systems SPECTS'07 San Diego CA July 2007 pp 379-386 13 Lim L.W Malik R Tan P.Y Apichaichalermwongse C Ando K Harada Y Panasonic Singapore Labs A QoS Scheduler for IEEE 802.1l e WLANs Consumer Communications and Networking Conference 2004 pp 199-204 14 V Vleeschauwer J Janssen G Petit and F Poppe Quality bounds for packetized voice transport Alcatel Tech Rep 1st Quarter 2000 15 ITU Series H Audiovisual and Multimedia Systems Infrastructure of audiovisual services Coding of moving video H.264 03/2005 International Telecommunication Union 12 BIOGRAPHY cooperative signal received his B.S Engineering from respectively processing and sensor networks He M.S and Ph.D degree in Electrical UCLA in 1993 1995 and 2000 Will Spearman is a Master's Candidate at Clemson University's School of Computing His work focuses on QoS in 802.cle and wireless networks His background includes a B.S in Psychology with a minor focus in Computer Science He currently is employed at Network Appliance Inc Dr Jim Martin is an Assistant Professor in the School of Computing at Clemson University His research interests include broadband access autonomic computing Internet protocols and network performance analysis He has received funding from NASA the Department of Justice BMW IBM and Cisco Dr Martin received his Ph.D from North Carolina State University Prior to joining Clemson Dr Martin was a consultant for Gartner and prior to that a software engineer for IBM Jay Gao joined the Jet Propulsion Laboratory in 2001 and is currently a senior research staff in the Communications Networks Group in the Telecommunication Research and Architecture section His research is primarily focused on space-based wireless communications and networking with emphasis on applications for the Mars Network He is currently conducting research for developing quality-of-service QoS protocols for the envisioned Interplanetary Network IPN and study optimization and protocols for deep space Ka-band communications He also supports requirements definition and interface design activities for the Department of Defense's Transformational Communications MilSatcom project and system engineering effort for NASA's Exploration System and Mission Directorate ESMD supporting the Constellation Program for return of human to the Moon and Mars Other research interests include optical-based sensorweb discrete event simulation of distributed communication/sensor systems energy efficient routing and self-organization algorithm for 13 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


