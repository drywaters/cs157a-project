Comparative Analysis of Genetic Based Approach and Apriori Algorithm for Mining Maximal Frequent Item Sets Mir Md. Jahangir Kabir School of Engineering and ICT University of Tasmania Launceston, Australia mmjkabir@utas.edu.au  Shuxiang Xu School of Engineering and ICT University of Tasmania Launceston, Australia Shuxiang.Xu@utas.edu.au  Byeong Ho Kang School of Engineering and ICT University of Tasmania Hobart, Australia Byeong.Kang@utas.edu.au  Zongyuan Zhao School of Engineering and ICT University of Tasmania Launceston, Australia Zongyuan.Zhao@utas.edu.au   Abstract 227In the data mining research area, discovering frequent item sets is an important issue and key factor for mining association rules. For large datasets, a huge amount of frequent patterns are generated for a low support value, which is a major challenge in frequent pattern mining tasks. A Maximal frequent pattern mining task helps to resolve this problem since a maximal frequent pattern contains information about a large number of small frequent sub patterns. For this study we have developed a genetic based approach to find maximal frequent patterns using a user defined threshold value as a constraint To optimize the search problems, a genetic algorithm is one of the best choices which mimics the natural selection procedure and considers global search mechanism which is good for searching solution especially when the search space is large. The use of evolutionary algorithm is also effective for undetermined solutions. Therefore, this approach uses a genetic algorithm to find maximal frequent item sets from different sorts of data sets A low support value generates some large patterns which contain the information about huge amount of small frequent sub patterns that could be useful for mining association rules. We have applied this genetic based approach for different real data sets as well as synthetic data sets. The experimental results show that our proposed approach evaluates less nodes than the number of candidate item sets considered by Apriori algorithm especially when the support value is set low  Keywords\227 association rules; data mining; maximal frequent item sets; genetic algorithm; lexicographic tree I  I NTRODUCTION  Mining frequent item sets is one of the most popular and well known methods in the data mining research area Nowadays it is used in different tasks like finding interesting relationships between variables, although it was initially developed for business transaction problems like market basket analysis. The main aim of the frequent pattern mining task is to search interesting relationships among different item sets. It can be used to solve different problems such as revealing association rules, concluding sequential patterns, correlations and other significant data mining tasks. A transactional database can be defined as a data set of transactions, each transaction consists of a set of items, termed as an item set. An item set is frequent if it appears abundantly in a given database In other words, certain ratios of all the transactions exist with respect to a user define threshold value. Mining frequent item sets is a time consuming task especially for dense databases where long patterns of item sets are emanated, which have been prolonged to mining maximal frequent item sets. An item set is called maximal if it contains the longest frequent patterns under user define threshold value Let D={t 1 t 2 t 3 205, t k 205,t n be the database, where t 1 t 2 t 3  205, t k 205,t n are an n number of transactions in the database Each transaction t n is a set of items I i 1 i 2 205.,i k 205,i n where item number 1 is i 1 and i n is item number n and so on Transaction t n is represented as a binary vector. If t n k 1  t h en  it means that t n bought the item i k otherwise t n k 0 L e t X is a set of few items in I i.e X I The set t n  X  I is true for all items in item set X for transaction t n The support value of an item is how many times the item appears in the transaction database as a subset. The support value of an item set is denoted by \(1 X\{t 1 X\t 2 X\ \205 \205+t n-1  X\t n X\}|/|D|                  \(1 Here t n  X gives the binary value. If the examined item set X appears as a subset in a transaction t n then t n  X  otherwise t n  X An item set with 1 item is called a 1itemset, an item set with k-items is called a k-item set. An item set is called frequent if its support value is more than or equal to a user defined threshold value, which is denoted by min_supp \(minimum support\ i.e X min_supp. We denote frequent item sets by FI. If an item set X is frequent and no superset of X is frequent then we can claim that X is a maximal frequent item set and we denote the set of all maximal frequent item sets by MFI To generate maximal frequent item sets \(MFI\m a large database is a most time consuming task in the present day. In this paper, we present an evolutionary approach to finding maximal frequent item sets from large databases by using the principles of Genetic Algorithm \(GA Mining maximal frequent item sets using a   genetic algorithm is the main approach of this research. The length of a frequent item set depends on its relationship among the item 978-1-4799-7492-4//15/$31.00 C 2015 Crown 39 


sets. The major advantage of a GA based approach is that it performs a global search and its time complexity is less than that of other algorithms. Another advantage is that it generates frequent item sets independently of the size of the data-base This work differs from existing research [1 in th e  following aspects: 1\ Unlike Apriori, this approach uses a lexicographic tree [2 as  a s ea r ch s p ac e an d i t  d o es n o t n ee d t o  enumerate frequent item sets level by level; 2\ For comparative analysis, we applied Apriori and this approach in different real datasets as well as synthetic datasets. Finally the results are compared with the results of Apriori algorithm. 3\ Unlike a Boolean based approach  and F P gro w t h a l gor i t h m 4 t h i s  approach does not need memory for loading a lexicographic tree which avoids the large consumption of memory space This technique dramatically reduces the time for accessing a large database to calculate the support value of unnecessary individuals to find frequent item sets. Although it was invented a long time ago but still Apriori is one of the famous algorithms and it performs better than other existing algorithms like Eclat, Partition, DIC and so on when the support value is set hi   H i p p G unt ze r a n d N a khae i za de h sh o w e d  t h e performance analysis of Apriori and other existing famous algorithms of present day. For this reason, we choose Apriori for comparison with our new approach. CPU time \(Run time is needed by the existing mining approaches for calculating support value of examined nodes. The efficiency of an algorithm depends on how many number of frequent or infrequent item sets it considers to get the final solution i.e maximal frequent item sets [6  In th is p a p e r w e w ill ex am in e how many number of nodes i.e. item sets are considered by GA based approach and compared the result with Apriori algorithm for different support values and data sets This paper is composed of the following sections: section 2 summarizes essential back-ground information including basic concepts of genetic algorithms and data mining. The methodology of traversing the search space and the proposed algorithm is described in section 3. Section 4 represents the comparative analysis of genetic based mining technique and Apriori algorithm. Finally, a short summary of our results is included in section 5 II  BACKGROUND Three aspects of the literature are reviewed in this section Two of these are the fundamental component of the proposed approach, namely the genetic algorithm and frequent item sets mining, which are the techniques for pattern mining tasks Finally, a related study of frequent item sets mining is presented. Elaborated information is given below A  Genetic Algorithm The Concept of Genetic Algorithm  Genetic algorithm considers adaptive methods which are used to solve search as well as optimization problems. This algorithm is inspired by natural selection and the \223survival of the fittest\224 mechanisms which are clearly stated by Charles Darwin in the book name \223The Origin of Species\224. Based on the fitness value, in a competing environment only the stronger individuals will survive. The processes in natural population which are essential for evolution are simulated by GA. Holland 7 f i rs t  pr o pos e d t h e bas i c p ri n ci pl es  of g e n e t i c a l g o ri t h m  8    Thereafter, a large number of researchers worked on genetic algorithm [9  1 2   Na tu r a lly in d i v i d u als a r e c o m p etin g w ith  each other for their shelter, food, clothes, water and so on Even members of the same class often compete to attract their partner. Those individuals are referred to as strong if they are successful in surviving and attracting a partner. Strong individuals will produce a large number of offspring. On the other hand poorly performing individuals are referred to as weak and have less probability to produce newer offspring The combination of good attributes from different parents can produce \223superfit\224 offspring. That is the fitness of this offspring is higher than the fitness of the parents. In this fashion, species becoming more and more well suited in the present environment Procedure of Genetic Algorithm Genetic algorithm plays a vital role for this study which simulates the natural behavior of biological organisms. Genetic algorithm based techniques are robust and can be used to solve a wide range of problems including those which are hard to solve by other methods. Researchers conclude that, it is not guaranteed that GA always provides optimum solutions to a problem rather it provides \223acceptably good\224 solutions to a problem which is solved by other methods \223quickly\224. Existing methods which are working well as a solution for a particular problem, improvement of those methods can be done by hybridizing with GA A Traditional Genetic Algorithm generates an   initial population, and then computes the fitness value of that population. Two individuals are selected from the old generation and applying crossover, mutation operators to produce two offspring. It selects the survivors which have the best fitness value and inserts those in the new generation. If the population is converged to a solution then the algorithm is terminated. In this algorithm, fitness function provides the fitness value of an offspring which is a specification of the offspring B  Data Mining Data mining is the process of finding relationships in large data sets applicable to methods of artificial intelligence statistics and machine learning. Different activities are included by data mining. It considers data from different sources and then translates, formats and cleans these data sets for further use, like analysis, integration and validation. The goal of data mining is to extract patterns and knowledge instead of mining data itself from large amounts of data sets. By analysing large amounts of data, data mining is used for extracting unknown important patterns such as association rule mining, anomaly detection, clustering and so on. Predictive modelling clustering techniques, summarization methods, link analysis and classification techniques are the five analytical domains which demonstrate the importance of data mining in real world applications. In business transactions, frequent pattern mining gives an idea about the popularity of buying item sets to the users. By using this information industries stock those popular products and get benefitted by it 40 


In human genetics research, the aim of sequence mining is to finding the changes in DNA sequences of individuals which are responsible for increasing the risk of common diseases like cancer. This study helps to develop the methods of diagnosing and preventing these diseases. Frequent pattern mining plays a vital role in mining correlations, associations and other interesting relationships among data sets. Moreover, it helps in different data mining tasks such as indexing, clustering classification and so on. For this reason, mining frequent patterns is an important data mining task and a focused topic in data mining area C  Related Works It is well known that the Apriori algorithm generates a candidate set and tests it in a breadth first manner. It discovers all the frequent item sets at level k before moving to its next level \(k+1\. It counts the support value of each node in level k and prunes those nodes if the support values of those nodes do not satisfy a user define support value. It generates candidate item sets at each level and scans the database so frequently that it is costly, especially when there is a long pattern [13   The Princer-Search algorithm [1 tr av ers es  a  l att ic e  through a bidirectional method that follows both top-down and bottom-up approaches. To find maximal frequent item sets it applies pruning methods by the following two properties: all the subsets of frequent item sets are pruned and all the supersets of infrequent item sets are pruned Breadth first traversal \(a level by level search strategy on a search space\ is applied for a MaxMiner search algorithm. To prune the branches of a tree it performs a look-ahead method MaxMiner uses a breadth first approach for limiting the number of passes over the database but look-ahead, which involves superset pruning, works better for depth first search methods [15   DepthProject performs depth first traversal on a lexicographic tree along with variations of superset pruning. To order child nodes, it applies dynamic reordering methods. By trimming infrequent items out of each node\222s tail, it reduces the size of the search space. To eliminate non maximal frequent item sets DepthProject would require post pruning methods   MAFIA, proposed by Burdick, Calimlim, and Gehrke [17  extends the idea of DepthProject. Similar to DepthPorject MAFIA also uses vertical bitmap representation where the support value/count of an item set is based on AND operations among the item sets. The search strategy of MAFIA integrates a depth first method to traverse the tree to find maximal frequent item sets along with effective pruning methodology In  G o u d a a nd Za ki p r opo sed a  no v e l a ppr oa c h c a l l e d GENMAX to find maximal item sets. In this approach they used a novel technique called Progressive Focusing. This technique maintains local maximal frequent item sets \(LMFI which are used for making comparison with newly found frequent item sets \(FI\. GENMAX uses vertical representation of a database and stores a     transaction identifier set \(TIS\ for each item set instead of a bit vector. Researchers of GENMAX concluded that, through experimental results this algorithm it performs better than existing algorithms on different types of databases Bilal Alata and Erhan Akin d e si gned a n e ffi ci e n t  genetic algorithm as a search strategy to mine both positive and negative quantitative association rules. Association rules are deduced from frequent patterns. This method mined the association rules without generating frequent item sets. The proposed genetic algorithm does not depend on minimum support and a confidence value which is hard to define for a database. A new genetic operator named uniform operator is used in this approach which ensures genetic diversity In [2   a qu i c k r e s p on s e da t a  m i n i n g  m ode l  b a s e d on  a  genetic algorithm has been de-signed. This approach gives more flexibility to the user. It only scans the database for those frequent item sets users are more interested in To mine quantitative association rules researchers proposed a new algorithm which is based on genetic algorithm named QUANTMINER [2   22 B y op t i m i z i ng supp or t and confidence value, this system dynamically identify good intervals in association rules. Researchers applied this algorithm in different data sets and showed the usefulness of this algorithm as a data mining tool Hipp, Guntzer and Nakhaeizadeh [5 s h ow ed th e performance analysis of Apriori and other existing famous algorithms of present day such as Eclat, Partition, DIC and so on. Performance analysis of different algorithms demonstrate that, Apriori algorithm performs better than other existing algorithms for high support value III  METHODOLOGY The proposed method of frequent item set mining for different data sets is described in this section. The following subsections will describe Lexicographic tree, Problem definitions and the proposed method, a genetic algorithm based technique, respectively A  Lexicographic Tree In this paper, we will consider a lexicographic tree as a search space which consists of all feasible solution [17   2 3    A Lexicographic tree maintains lexicographic ordering of items I in a datasets D. If item i occurs before item j in a dataset D then it maintains lexicographic ordering, i.e., i L j. If two subsets S 1 and S 2 where S 1  S 2 and S 1 S 2 S  then it maintains the following lexicographic order S 1 L S 2 There is no lexicographic ordering relationship between two subsets  S 1  and S 2 if S 1 and S 2 are disjoint subsets Fig. 1. shows an example of a lexicographic tree which considers lexicographic ordering of four given item sets 1,2,3,4}. The root of the tree is an empty set and each k-level contains k-items. All the nodes in the tree contain a different size of item sets. In each level, k-item sets maintain lexicographic ordering with the tail nodes, containing items lexicographically larger than elements of the head node. The support value of the head node is more than that of the tail node. It can be seen that the nodes closer to the root are more frequent than those far from the root. There is a nonlinear line 41 


called a cut\ in the tree which separates frequent item sets from infrequent ones. This cut is designed in such a way that it depends on a user defined support value. The nodes which are above the cut are frequent item sets and the elements below this cut are infrequent ones. All the nodes in lexicographic tree have a support value Lemma 1: If dataset D contains n items, then it enumerates 2 n 1 frequent sub item sets. Fig. 1. verifies Lemma 1, where there are 4 items, and it enumerates 2 4 1 = 15 nodes including the root node. Generally root node is an empty node Fig. 1. Lexicographic tree of four items based on a user defined support value  Support Value     30  Note that memory space will not be used for storing the nodes and links between the nodes of a lexicographic tree B  Problem Definition A huge number of frequent patterns are generated from big data sets which satisfy user define threshold value especially when user assign a lower value for min_supp. Because of generating enormous number of frequent item sets from large data sets is a major challenge in frequent pattern mining task This happened because if an item set is frequent, all of its sub item sets are frequent. To solve this problem, researchers proposed closed and maximal frequent pattern mining task. For this study, we consider maximal frequent pattern mining task Definition \(Maximal Frequent Pattern Mining An item set is a maximal frequent item set in a data set D if is frequent and there exists no superset such that  which is frequent in data set D. That is a frequent item set is called maximal, if all of its sub sets are frequent whereas all of its supersets are infrequent In this paper, we consider the user define support value which acts as a constraint to mine useful and interesting item sets from large data sets D C  The Proposed Algorithm Since the original database contains a large number of items, a transaction t 1 of a data set D is of the form t 1 i 11 i 12 205,i 1k 205,i 1n The value of item i 1j j  is either 1 or 0, depends on either it is present or absent in transaction t 1  Individual Representation An individual is represented by the set of items of the form individual i v, where individual i is the i th individual. Here v is a value from its item sets domain. For example, a datasets D contains 4 items and 1000 transactions. Let\222s say, transaction k contains item 2 and item 4 This transaction represents the node \(2, 4\n lexicographic tree of Fig. 1., and the individual coding of this node is   Population Generation The population of genetic based system is generated as follows: for first individual, it considers the whole domain in lexicographic tree. And for the following individuals it considers those item sets which are not classified in frequent or infrequent ones. When the individual is generated it classifies into frequent or infrequent ones by using the user define support value. Through this way the search space become narrower for generation of next population Genetic Operators  To improve the quality of next individual, crossover and mutation operator is used to transform one individual into another one. At the initial stage of population generation, two parent individuals are selected randomly from the domain of item sets in lexicographic tree and after applying crossover operator two new offspring are generated. Mutation changes a bit randomly in each segment of the individuals for the improvement of new offspring Fitness Function The fitness function of this proposed method provides fitness value of an individual which is equal to the support value of an item set i.e f individual i support, where f individual i is the fitness value of individual i. When an individual is generated support value of that individual is counted from the datasets. If the fitness value of an item set satisfies the user defined support value i.e f individual i  min_supp, then this item set is classified as frequent item set and store it in an array called FI_Superset_Member. Otherwise it will save in an array, called NFI_Subset_Member. Member of FI_Superset_Member array are the frequent item sets of a data sets and always replaced by the supersets of member item sets. Similarly, member of NFI_Subset_Member array are the infrequent item sets of a data sets and always replaced by the subsets of member item sets A prototypical genetic algorithm based scheme is followed by the proposed method. Minimum support value \(min_supp number of generation \(NbGen\, mutation rate \(MR\, crossover rate \(CR\, a dataset \(TuplesNb\are the inputs of the algorithm Fitness \(item set Function  Temp_Fitness = SupportCount\(item set Count the support value of item set  from given dataset if Temp_Fitness min_supp then   return Temp_Fitness, item set else  return 226Temp_Fitness, item set 0 1 0 1 42 


MFItemsets Function  Input min_supp, NbGen, MR, CR, dataset composed of TuplesNb Output Maximal frequent item sets MFI NbTestingNodes Generate a random population While i NbGen do  Select two parents from generated individuals and applying crossover and Mutation operators to get new two offspring for each individual  check FI_Member_Add array, if this individual or any of its subset is in this array  check NFI_Member_Add array, if this individual or any of its superset is in this array If none of the above array contain this individual or any of its subset or superset then Fitness_Value = Fitness \(individual NbTestingNodes  if Fitness_value \037 0 then  Update FI_Member_Add array e lse  Update NFI_Member_Add array i MFI = FI_Member_Add FI_Mmeber_Add array contain the latest maximal frequent item sets return MFI, NbTestingNodes  IV  EXPERIMENTAL  RESULTS  AND  PERFORMANCE  ANALYSIS To verify the performance of the proposed method, we have used a most popular algorithm named Apriori algorithm for finding maximal frequent item sets for a comparison study Both of these algorithms have been applied on the same datasets. C programming language is used for coding both of the algorithms. The experiments were performed on an Intel\(R core i5-3210M CPU @2.50GHz, 4 GB RAM running on Windows 7 Enterprise. Microsoft Visual Studio 2012 was used to compile the code of the proposed method. The experiments were carried out on Real data sets as well as synthetic datasets 13  R eal  d a tas e ts w e re t a k e n f r o m th e  Un iv ers ity  o f  California at Irvine \(UCI\achine learning repository  http://archive.ics.uci.edu/ml/datasets.html In this experiment we consider the following datasets T8I5D100K, T6I4D100K Zoo, TicTacToe. For synthetic dataset, T10I5D100K where T represents the average size of the transaction is 10, I represents the average size of the maximal frequent item set is 5 and number of transactions is 100,000. Zoo and TicTacToe data sets contain 17 and 9 attributes and number of instances are 101 and 958, respectively. We use the following GA parameters to conduct the experiments We consider the population size, popsize = 100 and initial population is produced by random generation. Other GA parameters such as selection \(P s crossover \(P c and mutation \(P m probability are defined as follows: P s 0.95, P c 0.85 and P m 0.01  Different support values were applied on these datasets to check how many nodes have been tested, and the numbers of individuals have been generated to get the exact number of maximal frequent item sets, run times, and so on. Here run time is the total execution time. The purpose of this new approach is for converging to a solution as fast as possible. A full experiment on these databases was conducted demonstrating the proposed method\222s ability to yield solutions rapidly by accessing the databases for fewer numbers of nodes in a lexicographic tree. Unlike Apriori, the proposed method generates an individual X in any level which satisfies a minimum support value, then all the other subsets of X in any level will be automatically pruned which dramatically reduces the time for accessing a large database. This is also true the other way around: if the propose method generates an individual Y in any level which does not satisfy a minimum support value, then all the other supersets of Y in any level will be automatically pruned With Apriori algorithm, one would test all the nodes in a specific level and generate a candidate set. This candidate set generation needs a long time for finding maximal frequent item sets Fig. 2. Zoo data  From the experimental results it can conclude that the proposed mining algorithm, calculated the support value for much fewer numbers of nodes than the conventional Apriori algorithm, especially when the support value was low.  But for higher support value, Apriori gets the solution at level k which is near to the root node in the lexicographic tree. In that case it considers fewer number of candidate item sets than the proposed algorithm. The length of the maximal frequent item sets depends on the support value. A Lower support value provides longer patterns. From longer patterns, users can get a better idea about the relationships among frequent item sets. In 43 


that case our proposed mining algorithm outperforms the conventional Apriori algorithm Fig. 3. TicTacToe data   Fig. 4. T8I5D100K   Fig. 5. T6I4D100K  V  CONCLUSIONS  AND  SUMMARY In this paper, we have developed a genetic based approach and compared the results with the results given by the Apriori algorithm for mining maximal frequent item sets. We have obtained the results through experimental analysis on real data sets using both of these algorithms. Several advantages have been demonstrated by the experimental analysis of this algorithm in comparison with Apriori algorithm, which are as follows 200  It gives better results than the Apriori algorithm by accessing large data sets for less numbers of nodes especially when the support value is set low by the users 200  For large data sets and low support value, both of these algorithms give the same solution by giving the same number of maximal frequent item sets. To get this solution Apriori considers a large number of candidate item sets with respect to a genetic based approach 200  For large data sets and high support values, Apriori performs better than a genetic based approach, since the genetic algorithm uses global search mechanism Apriori uses a level by level search procedure and it gets the solution by accessing less numbers of nodes because solution is near the root node. The nodes close to the root of lexicographic tree have higher support values 200  Low support value generates a long size frequent pattern which provides information like frequency of an exponential number of smaller sub patterns. In that case a genetic based approach performs better than other existing algorithms 200  The experimental results of a genetic based approach demonstrate the effect of generations of individuals, and prune all the subsets and supersets in a lexicographic tree, which is cost effective in the case of counting the support value and reducing the search space dramatically The other areas which should be focused for further research include developing genetic operators in such a way that it will help to reduce generating the same individuals and it can increase valid individuals for further generation Considering repetitive individuals sometimes takes a longer time to get the solution A CKNOWLEDGMENT  This research work was funded by School of Engineering and ICT, University of Tasmania, Australia, and website http://www.utas.edu.au/cricos, under CRICOS Provider Code 00586B  R EFERENCES  1  M M. J  K a bir  S  X u  B. H  K a ng a nd Z  Z h ao 223 A N o ve l Approach to Mining Maximal Frequent Itemsets Based on Genetic Algorithm,\224 in International Conference on Information Technology and Applications \(ICITA 2014 2  R C. A g ar w al C. C A g g ar w al and V V  V P r asa d  223 A Tr ee  Projection Algorithm For Generation of Frequent Itemsets,\224 Parallel Distrib. Comput. Spec. Issue High Perform. Data Min vol 61, no. 3, pp. 350\226371, 2001 3  A  S a l leb  Z M a a z ou zi  and C V r a in  223Mi n i n g M a xi m a l F r eq u e nt Itemsets by a Boolean Based Approach,\224 in European Conference on Artificial intelligence 2002, pp. 285\226289 4  J  H a n, J  P e i a n d Y  Y i n 223 M i n i n g F r e que nt P a tte r n s w i tho u t  Candidate Generation,\224 ACM SIGMOD vol. 29, no. 2, pp. 1\22612 2000 5  J  H i p p  U  G 374ntz e r a nd G  N a kh ae iz ade h 223 A l g or ithm s f o r  association rule mining\227a general survey and comparison,\224 ACM sigkdd Explor. \205 vol. 2, no. 1, pp. 58\22664, 2000 6  R. J  K u o an d C  W  S h i h 223 A s s o ciati o n r u l e m i ni ng t h r o ug h the a n t  colony system for National Health Insurance Research Database in 44 


Taiwan,\224 Comput. Math. with Appl vol. 54, no. 11\22612, pp. 1303\226 1318, Dec. 2007 7 J  H Holla n d   Adaptation in Natural and Artificial Systems Ann Arbor: University of Michigan Press, 1975 8  K  F M a n  K S  T a n g   a n d S  Kwo n g  223 G e n e t i c Al g o r i t h m s 037   Concepts and Applications,\224 IEEE Trans. Ind. Electron vol. 43 no. 5, 1996 9  D  Be as l e y  D  R. B u l l  an d R R Ma r tin 223 A n O v e r v iew of  G e ne tic  Algorithms\037: Part 1 , Fundamentals,\224 Univ. Comput vol. 15, no. 2 pp. 58\22669, 1993   M  S r i n i v a s an d L  M  P a tn ai k 223G e n et i c A lgo r i t h m s A S u r v ey  224  Computer \(Long. Beach. Calif vol. 27, no. 6, pp. 17\22626, 1994  D E  G o ld b e rg  Genetic Algorithms in Search, Optimization and Machine Learning Addison-Wesley Longman Publishing Co., Inc Boston, MA, USA, 1989  Z  M i ch a l ew i c z  Genetic algorithms + data structures = evolution programs Berlin: Springer, 1992  R  A gra w a l a n d R  Sri kan t  223F a s t A l gori t h m s for M i n i n g  Association Rules,\224 in 20th International Conference on Very Large Data Bases 1994, pp. 487\226499 14  D  I  L i n and Z  M. K e de m  223 P ince r S e a r c h A  N e w  A l go r ithm f o r  Discovering the Maximal Frequent Set,\224 in 6th International Conference on Extending Database Technology   R  J   B a y a rd o 223E ffic i e nt ly M i ni n g L on g Pa tt ern s from D a t a b a s e s  224 ACM SIGMOD pp. 85\22693, 1998 16  R  C  A g a r w a l  C  C  A g g a r w a l  a n d V  V  V  P r a s a d  223 D e p t h f i r s t  generation of long patterns,\224 Proc. sixth ACM SIGKDD Int. Conf Knowl. Discov. data Min. - KDD \22200 vol. 2, pp. 108\226118, 2000  D Bu rdi c k   M  Ca li m l i m   an d J   Geh r k e  223M AFI A a m a xi m a l  frequent itemset algorithm for transactional databases,\224 Proc. 17th Int. Conf. Data Eng no. X, pp. 443\226452 18  K   G o uda an d M. J  Z a ki, \223 G e n Max 037   A n E f f icie n t A l go r ithm f o r  Mining,\224 Data Min. Knowl. Discov vol. 11, no. 3, pp. 223\226242 2005    A l a t a and E. Akin, \223An efficient genetic algorithm for automated mining of both positive and negative quantitative association rules,\224 Soft Comput vol. 10, no. 3, pp. 230\226237, Apr 2005 20  W D o u  J  Hu  K Hi r a s a wa   a n d  G Wu  223 Q u i c k r e s p on s e d a t a  mining model using genetic algorithm,\224 2008 SICE Annu. Conf pp 1214\2261219, Aug. 2008  A  Sa ll e b a ou i ssi C Vra i n   C  Norte t  X K o n g  a n d D  C a ss a r d  223QuantMiner for Mining Quantitative Association Rules,\224 Mach Learn. Res vol. 14, no. 1, pp. 3153\2263157, 2013 22  A  S a l le b a o u is s i C  V r ai n an d C. N o r te t, \223 Q ua n tM ine r 037  A  Genetic Algorithm for Mining Quantitative Association Rules,\224 in 20th International Joint Conference on Artificial Intelligence 2007 pp. 1035\2261040  J  Hua n g Y  Ch e-t s un g  a nd  C  F u 223 A Gen e t i c A l g ori t h m  B a s e d Searching of Maximal Frequent Itemsets,\224 in International conference on artificial intelligence 2004    45 


0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.48 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0   Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time 
0.1 1 10 100 3000 2000 1000 0 1000 2000 3000 4000 5000 
0.6 0.5 0.4 0.3 0.2 0.1 0.0 10 100 
1 10 100 6000 4000 2000 0 2000 4000 6000 8000 10000 
0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07 0.06 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  0 10000 5000 15000 c Webdocs Figure 5 The performance results of Apriori-AP on three real-world benchmarks DP time SR time and CPU time represent the data process time on AP symbol replacement time on AP and CPU time respectively Webdocs switches to 16-bit encoding when relative minimum support is less then 0.1 8-bit encoding is applied in other cases 
E vs Eclat Eclat et al Eclat Eclat 
0.80 0.75 0.70 0.65 0.60 0.55 0 100 200 300 400 Computation Time \(s Relative minimum support 1.0X Symbol replacement time 0.5X Symbol replacement time 0.1X Symbol replacement time Figure 7 The impact of symbol replacement time on Apriori-AP performance for Pumsb how symbol replacement time affects the total AprioriAP computation time A reduction of 90 in the symbol replacement time leads to 2.3X-3.4X speedups of the total computation time The reduction of symbol replacement latency will not affect the performance behavior of AprioriAP for large datasets since data processing dominates the total computation time 
Apriori Eclat Equivalent Class Clustering   is another algorithm based on Downward-closure uses a vertical representation of transactions and depth-ìrst-search strategy to minimize memory usage Zhang  proposed a h ybrid depth-ìrst/breadth-ìrst search scheme to expose more parallelism for both multi-thread and GPU versions of  However the trade-off between parallelism and memory usage still exists For large datasets the nite memory main or GPU global memory will become a limiting factor for performance and for very large datasets the algorithm fails While there is a parameter which can tune the trade-off between parallelism and memory occupancy we simply use the default setting of this parameter for better performance Figure 8 shows the speedups that the sequential 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 AP counting speedup Apriori-AP overall speedup    Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  b Accidents 
T40D500K Counting T40D500K Overall T100D20M Counting T100D20M Overall Webdocs5X Counting Webdocs5X Overall Speedup Relative Minimum Support Figure 6 The speedup of Apriori-AP over Apriori-CPU on three synthetic benchmarks 
1 10 100 
a Pumsb 
696 


0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 10 Pumsb Accidents T40D500K Webdocs Webdocs5X T100D20M ENWiki Speedup Relative Minimum Support 
Figure 8 The performance comparison of CPU sequential and algorithm achieved with respect to sequential Apriori-CPU Though has 8X performance advantage in average cases the vertical bitset representation become less efìcient for sparse and large dataset high trans and freq item ratio This situation becomes worse as the support number decreases The Apriori-CPU implementation usually achieves worse performance than  though the performance boost of counting operation makes Apriori-AP a competitive solution to parallelized  Three factors make a poor t for the AP though it has better performance on CPU 1 requires bit-level operations but the AP works on byte-level symbols 2 generates new vertical representations of transactions for each new itemset candidate while dynamically changing the values in the input stream is not efìcient using the AP 3 Even the hybrid search strategy cannot expose enough parallelism to make full use of the AP chips Figure 9 and 10 show the performance comparison between Apriori-AP 45nm for current generation of AP and sequential multi-core and GPU versions of  Generally Apriori-AP shows better performance than sequential and multi-core versions of  The GPU version of shows better performance in Pumsb Accidents and Webdocs when the minimum support number is small However because of the constraint of GPU global memory Eclat-1G fails at small support numbers for three large datasets ENWiki T100D20M and Webdocs5X ENWiki as a typical sparse dataset causes inefìcient storage of bitset representation in Eclat and leads to early failure of EclatGPU and up to 49X speedup of Apriori-AP over Eclat-6C In other benchmarks Apriori-AP shows up to 7.5X speedup over Eclat-6C and 3.6X speedup over Eclat-1G This gure also indicates that the performance advantage of AprioriAP over GPU/multi-core increases as the size of the dataset grows The AP D480 chip is based on 45nm technology while the Intel CPU Xeon E5-1650 and Nvidia Kepler K20C on which we test are based on 32nm and 28nm technologies respectively To compare the different architectures in the same semiconductor technology mode we show the performance of technology projections on 32nm and 28nm technologies in Figure 9 and 10 assuming linear scaling for clock frequency and square scaling for capacity The technology normalized performance of Apriori-AP shows better performance than multi-core and GPU versions of Eclat in almost all of the ranges of support that we investigated for all datasets with the exception of small support for Pumsb and T100D20M Apriori-AP achieves up to 112X speedup over Eclat-6C and 6.3X speedup over Eclat-1G The above results indicate that the size of the dataset could be a limiting factor for the parallel algorithms By varying the number of transactions but keeping other parameters xed we studied the behavior of Apriori-AP and Eclat as the size of the dataset increases Figure 11 For T100 the datasets with different sizes are obtained by the IBM synthetic data generator For Webdocs the different data sizes are obtained by randomly sampling the transactions or by concatenating duplicates of the whole dataset In the tested cases the GPU version of fails in the range from 2GB to 4GB because of the nite GPU global memory Comparing the results using different support numbers on the same dataset it is apparent that the smaller support number causes Eclat-1G to fail at a smaller dataset This failure is caused by the fact that the ARM with a smaller support will keep more items and transactions in the data preprocessing stage While not shown in this gure it is reasonable to predict that the multicore implementation would fail when the available physical memory is exhausted However Apriori-AP will still work well on much larger datasets assuming the data is streamed in from the hard drive assuming the hard drive bandwidth is not a bottleneck VII C ONCLUSIONS AND THE F UTURE W ORK We present a hardware-accelerated ARM solution using Micronês new AP architecture Our proposed solution includes a novel automaton design for matching and counting frequent itemsets for ARM The multiple-entry NFA based design was proposed to handle variable-size itemsets MENFA-VSI and avoid routing reconìguration The whole design makes full usage of the massive parallelism of the AP and can match and count up to itemsets in parallel on an AP D480 48-core board When compared with the based single-core CPU implementation the proposed solution shows up to 129X speedup in our experimental 
Apriori Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat F Normalizing for technology Eclat G Data size Eclat Eclat Eclat Apriori 
18 432 
 
697 


0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 1 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm a Webdocs\(1.4GB 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails d Webdocs5X 7.1GB Figure 10 Performance comparison among Apriori-AP Eclat-1C Eclat-6C and Eclat-1G with technology normalization on four large datasets 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm b Accidents 34MB 0.40 0.36 0.32 0.28 0.24 0.20 0.16 0.12 0.08 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm c T40D500K\(49MB Figure 9 Performance comparison among Apriori-AP Eclat1C Eclat-6C and Eclat-1G with technology normalization on three small datasets results on seven real-world and synthetic datasets This APaccelerated solution also outperforms the multicore-based and GPU-based implementations of 0.60 0.55 0.50 0.45 0.40 0.35 0.30 0.25 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails c T100D20M 6.3GB 
ARM a more efìcient algorithm with up to 49X speedups especially on large datasets When performing technology projections on future generations of the AP our results suggest even better speedups relative to the equivalent-generation of CPUs and GPUs Furthermore by varying the size of the datasets from small to very large our results demonstrate the memory constraint of parallel ARM particularly for GPU 
Eclat Eclat 
0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm 
0.20 0.15 0.10 0.05 0.00 1 10 100 1000 10000 100000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails b ENWiki\(3.0GB 
a Pumsb 16MB 
698 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


