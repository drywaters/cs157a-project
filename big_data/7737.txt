Digital Image Compression using Neural Networks Dipta Pratim Dutta dave_85@rediffmail.com Samrat Deb Choudhury popfmskailashahar@gmail.com Md. Anwar Hussain bubuli99@gmail.com Swanirbhar Majumder swanirbhar@gmail.com Department of ECE, NERIST \(Deemed University\, Arunachal Pradesh, India  Abstract Compression of data in any form is a large and active field as well as a big business. Image compression is a subset of this huge field of data compression, where we undertake the compression of image data specifically. Research in this field aims at reducing the number of bits needed to represent an image. Inter-pixel relationship is highly non-linear and unpredictive in the absence of a prior knowledge of the image itself. Thus Artificial Neural Networks has been used here for image compression by training the net using the image to be compressed. The ANN takes into account the psycho visual features, dependent mostly on the information contained in images. The algorithms, on application on the image data preserves most of the characteristics of the data while working in a lossy manner and maximize the compression performance. The results have been checked with and without the use of quantization, and without median filtering of the image. The ANN algorithm used here is mainly the back-propagation of multilayer perceptrons Keywords Compression, ANN, median filter, backpropagation, multilayer perceptrons  I   I NTRODUCTION  Data compression has become a necessity for saving bandwidth, power, storage space, etc. Thus it has turned out to be a present day craze as well as source of competition in the race for technology and research with so much manpower, time and money involved for its development. Out of the image compression techniques available, transform coding is the most preferred method  2 3  S i nc e e n er gy d i st r i b u t i o n a f t e r transform coding varies with each image, compression in the spatial domain is not an easy task [4  I m a ge s do h o w e ve r t e nd to compact their energy in the frequency domain making compression in the frequency domain much more effective Transform coding is simply the compression of the images in the frequency domain [5 S o t r a n sfo r m b a s e d t e c hni q u e s l i ke  DWT, DCT, SVD, DWT-DCT, DWT-SVD, etc. have been extensively used [6    It is known that compression algorithms can be classified into two types – ‘lossy’ and ‘lossless’. If the recovered image after decompression\ does not have the same quality as the original image then there has been a loss of some image data during compression. This is called a ‘lossy compression algorithm’. But some algorithms have the ability to retain the quality of the image, even after the compression, and the decompression processes. Such algorithms come under the category of ‘lossless compression algorithms    Among learning algorithms, back-propagation algorithm is a widely used learning algorithm in Artificial Neural Networks The Feed-Forward Neural Network architecture is capable of approximating most problems with high accuracy and generalization ability [8  9   T h is a l g o r ith m is b a s e d o n  th e  error correction learning rule. Error propagation consists of two passes through the different layers of the network, a forward pass and a backward pass. In the forward pass the input vector is applied to the sensory nodes of the network and its effect propagates through the network layer by layer. Finally a set of outputs is produced as the actual response of the network During the forward pass the synaptic weight of the networks are all fixed. During the back pass the synaptic weights are all adjusted in accordance with an error-correction rule. The actual response of the network is subtracted from the desired response to produce an error signal. This error signal is then propagated backward through the network against the direction of synaptic conditions. The synaptic weights are adjusted to make the actual response of the network move closer to the desired response II  T HE I MAGE C OMPRESSION P ROCEDURE  Digital images have become very popular in recent times Every digital image is specified by the number of pixels associated with the image. Each pixel in a gray-level image is described by an intensity of the image at that point. An image that is 256 x 256, means that there are 65536 pixels \(intensity points\in the image in a matrix form with 256 rows and 256 columns Digital images are basically classified into two types grayscale images and color images. Any color can be defined by the combination of the three primary colors – red, green and blue. A grayscale image has no color information. Therefore very pixel in a grayscale image has different shade of gray which is commonly represented by 8\(Unsigned integers of 8 bits\. So, there is 2 8 256 possible intensity values \(shades of gray\ for a grayscale image ranging from 0 to 255. The depth of the image is said to be 8, since 8 bits are used to represent each pixel Since 8 bits are used to represent each pixel, to represent an image which is of dimension [256 x 256, 256 x 25 524288 bits are needed to represent the image. With limited memory space, it becomes useful to compress the digital image so that it occupies less memory and also becomes easier to share over a medium such as the internet MATLAB  has been used to implement the program. The well-known ‘Lena’ \(tiff format used here\rayscale image 256 x 256\ has been used to demonstrate the technique. Each pixel in an image can be denoted as a coefficient, which represents the intensity of the image at that point. Then, the idea of compressing an image is to encode these coefficients with reduced bits and at the same time, retain the quality of the image to satisfactory limits. Once compressed, these coded 
2009 International Conference on Advances in Computing, Control, and Telecommunication Technologies 978-0-7695-3915-7/09 $26.00 © 2009 IEEE DOI 10.1109/ACT.2009.38 116 


  images which occupy less memory space can be transferred over the internet medium. At the receiving end, these compressed images need to be again decoded or decompressed so that one can recover the original image. The quality of the received image can be tested by some standard error calculations. The mean of all the squared errors for all the pixels, called the MSE \(Mean Square Error\ can be used for this purpose. The higher the value of this MSE, lower the quality of the decompressed image The multi-layer feed-forward neural net has been used to compress images. The Lena image that has been used for compression purposes is a 256 x 256 image. This image can be broken into blocks of size 8 x 8. There will then be 64 pixels per block. Totally, there will be [32 x 32 1 0 2 4 b l o c k s  T h e  64 pixels in each block then becomes the input vector to the neural net. The main idea in using a neural network to compress images is to code these 64 coefficients using a reduced number of coefficients \(reduce the dimension of the data\ The neural network architecture is very helpful in this context. If one can reduce the number of dimensions in the hidden layer \(number of hidden neurons\ to be much less than the number of dimensions in the input layer, then there will be a reduction in the number of coefficients during the transition from the input to the hidden layer. An input layer with 64 dimensions and a hidden layer with 8 \(any number less than 64\dimensions, for example, means that the 64 pixels of the image \(8 x 8\ock which is applied to the input layer has been transformed into 8 coefficients in the hidden layer. Then, one could again use an output layer which has 64 dimensions to recover the original 64 pixels. The basic idea here is to learn the identity mapping or rather associative mapping which means the output of the neural net is the same as its input Thus, with a 64 dimensional input layer, an 8 dimensional hidden layer, and a 64 dimensional output layer – the neural network has been used for image compression In this image compression technique, the compression is achieved by training a neural network with the image and then using the weights and the coefficients from the hidden layer as the data to recreate the image. This will be very clear with an example If each pixel is encoded using 8 bits, then the total number of bits to be transmitted without compression is [256 x 256 x 8  for a [256 x 2 p i xe l  i m a g e  A  2 56 x 256] pi xe l i m a g e i s  split into [4 x or [8 x 8 or  16 x  1 6  p i xel sub i m a g e s  T h e  normalized pixel value of the sub-image is the input to the nodes. 64 input layers \(in case of [8 x  s u bim ag e s i z e  ar e  taken with 1 pixel input to each layer. The three-layered back propagation-learning network has been trained with each subimage. The number of neurons in the hidden layer will be taken according to the desired compression. Here, we have taken 8 hidden layers. The number of neurons in the output layer will be the same as that in the input layer \(64 in our case\ The input layer and output layer are fully connected to the hidden layer The weights of synapses connecting input neurons and hidden neurons and weights of synapses connecting hidden neurons and output neurons are initialized to small random values from say –1 to +1 Only the weights between the hidden layer and the output layer are required for reconstruction. So, the numbers of weights are [64  a n d numb e r  o f bi t s used t o  r e pr e s e n t t h e m  are [64 x 8 x    T h e i n p u t l a y e r us e s l i n ear ac t i v at i o n func t i o n  The hidden layer units evaluate the output using the sigmoid function. The output layer neuron evaluates the output using linear activation function. As the image is split into [8 x 8  pixel blocks, the total number of blocks become [32 and for each sub-image, the number of coefficients out of the hidden layer is 8. So, the total numbers of coefficients are [32 x 32 x  T hus, t o t a l num b e r  o f  b i t s req u i r e d t o re pre s e n t  t h e  coefficients are [32 x 32 x 8 x 8  Total number of bits to be transmitted without compression T b 256 x 256 x 8 A f t e r t r a i ni ng t h e ne t w o r k, num b e r o f  bi t s  to be transmitted, C b 32 x 32 x 8 x 8\ + \(8 x 64 x 8 Therefore compression achieved is Compression= [\(1 – C b T b 100  000 87%                     \(1  At the receiving end, the image is reconstructed by multiplying the weights between the hidden layer and the output layer to the corresponding coefficients obtained from the hidden layer The network is trained using the Polak – Ribiere Conjugate Gradient Algorithm which is a variation of the backpropagation algorithm used for faster convergence. The algorithm is implemented in MATLAB as the traincgp in built function. In conjugate gradient algorithms, a search is performed in the conjugate gradient direction to determine the step size, which is varied at each iteration to reduce the performance function. The compression performance is assessed in terms of Compression ratio, PSNR and execution time. The execution time is measured from the time of start of training until the error saturates to a minimum and the traincgp  function stops execution Histogram equalization is also implemented in our project in order to reduce training time. It does not introduce new intensities in the image. Existing values will be mapped to new values resulting image with less number of the original number of intensities. Hence, frequency of occurrence of gray levels in the image will be more are less equal or rather uniform by the mapping. Due to this most of the image blocks will be similar and hence the learning time should get reduced. However using histogram equalization has been found to be not of good use as the error using this technique remains high In another attempt to reduce training time, highly correlated sub-images have been eliminated. Typical values of correlation coefficients above which the sub-image is removed, lies in between 0.7 to 0.8. As a result, the training set gets reduced and hence the training time. This process has been satisfactory in achieving lower training times. The quality of reconstructed images is assessed using measures like Peak Signal to Noise Ratio \(PSNR III  T HE T RAINING A LGORITHM  Firstly normalize the inputs and outputs with respect to their maximum values. It has been observed that the neural networks work better if input and outputs lie between 0-1 
117 


  Assume that the number of neurons in the hidden layer lie within a particular defined range. Initialize the weights connecting the input neurons and hidden neurons and the weights connecting the hidden neurons and output neurons between 0 and 1. For the training data, present one set of inputs and outputs. Present the pattern to the input layer as inputs. By using linear activation function, the output of the input layer may be evaluated. Compute the inputs to the hidden layer by multiplying corresponding weights of synapses. Let the hidden layer units evaluate the output using the sigmoid function Compute the inputs to the output layer by multiplying corresponding weights of synapses. Let the output layer units evaluate the output using linear activation function. Calculate the error and the difference between the network output and the desired output. Lastly adjustments in the weights are made until the error \(MSE\ reaches the desired level with allowable tolerance value IV  R ESULTS AND D ISCUSSION  The two main images ‘Lena’ and ‘Baboon’ have been analyzed without using median filtering \(which is very important for the salt and pepper type noise created due to the blocking artifacts\. These are represented in Table I and II respectively TABLE I   256 X 256  L ENA IMAGE   8 HIDDEN LAYER NEURONS  WITHOUT USING M EDIAN F ILTER  Type PSNR before Quantization in dB PSNR after Quantization in dB Time of Convergence in Sec Compression  With Histogram Equalization 24.70 22.43 169.23 86.72 Without Histogram Equalization  28.87  24.40  175.29  86.72 Removing Highly Corelated training vectors 28.34 25.22 85.23 86.72 TABLE II  256 X 256  B ABOON IMAGE   8 HIDDEN LAYER NEURONS  WITHOUT USING M EDIAN F ILTER  Type PSNR before Quantization in dB PSNR after Quantization in dB Time of Convergence in Sec Compression  With Histogram Equalization 17.92 17.97 88.98 86.72 Without Histogram Equalization 23.91 22.76 109.26 86.72 Removing Highly Corelated training vectors 23.84 23.09 98.25 86.72  The images are shown with the highly co-related training vectors removed from the training input set of the neural network with maximum allowable correlation coefficient < 0.8 in Figure 1 and 2  a\   \(b  c\   \(d Figure 1  Images with the highly co-related training vectors removed from the training input set of the neural network \(a\ Original Image \(b\ Image reconstructed without quantization \(c\ Image reconstructed after quantization d\ Image after using median filter after quantization  a\   \(b  c\   \(d Figure 2  Images with the highly co-related training vectors removed from the training input set of the neural network.\(a\ Original Image \(b\ Image reconstructed without quantization \(c\ Image reconstructed after quantization d\ Image after using median filter after quantization 
118 


  The highly correlated training vectors have been eliminated and the following results have been obtained for different values of correlation coefficients as in Table III. A 256 x 256 satellite image has been used. No. of hidden layer neurons is 8 PSNR values are obtained without using histogram equalization and median filter. The details for three different allowable correlation coefficients are provided as in figure 3, 4 and 5. Here the maximum allowable correlation coefficients are chosen to be less than 0.65, 0.75 and 0.85 respectively. These have been checked for PSNR values before and after quantization as well as after quantization and median filtering The time for convergence data provided in all three tables I, II and III are for Pentium  4 Core2Duo  1.8 GHz PC with RAM being 512MB in the Windows XP  operating system. This time of convergence reduces as we re-run the program again and again\(this is for the first successful run in each case TABLE III  256 X 256  S ATELLITE IMAGE   8 HIDDEN LAYER NEURONS  FOR DIFFERENT MINIMUM CORRELATION COEFFICIENTS  Minimum correlation coefficient upto which training vectors are eliminated PSNR before Quantization in dB PSNR after Quantization in dB PSNR after Quantization And Median Filtering in dB Time of Convergence in Sec Compression  0.65 29.24 28.55 29.05 58.87 86.72 0.75 29.16 27.20 28.93 65.16 86.72 0.85 28.07 25.08 26.20 125.52 86.72   a\   \(b  c\   \(d Figure 3  Images with the highly co-related training vectors removed from the training input set of the neural network \(maximum allowable correlation coefficient < 0.65\ \(a\ Original Image \(b\ Image reconstructed without quantization \(c\ Image reconstructed after quantization \(d\ Image after using median filter after quantization  a\   \(b  c\   \(d Figure 4  Images with the highly co-related training vectors removed from the training input set of the neural network \(maximum allowable correlation coefficient < 0.75\ \(a\ Original Image \(b\ Image reconstructed without quantization \(c\ Image reconstructed after quantization \(d\ Image after using median filter after quantization  a\   \(b  c\   \(d Figure 5  Images with the highly co-related training vectors removed from the training input set of the neural network \(maximum allowable correlation coefficient < 0.85\ \(a\ Original Image \(b\ Image reconstructed without quantization \(c\ Image reconstructed after quantization \(d\ Image after using median filter after quantization 
119 


  Increasing the number of hidden layer neurons reduces compression. This happens because the compressed data consists of the coefficients from the hidden layer neurons; and the weights between the hidden layer and the output layer Reducing the number of hidden layers reduces the number of these weights. Hence, less number of bits is required to represent the data. For example, for a 256 x 256 grayscale image, 4 hidden layer neurons give a compression of 93.36 whereas 16 hidden layer neurons give a compression of 73.44 A multilayer neural network may be used to approximate any arbitrary Non-linear function provided it has one or more hidden layers with sufficient number of neurons in them. The more the number of hidden neurons, the better is the approximation and lesser is the error. As the number of hidden neurons was increased, we obtained better values of PSNR and lesser error, although at the cost of reduced compression. For example, for a 256 x 256 grayscale image, 4 hidden layer neurons and compression without histogram equalization PSNR of 26.24 dB is obtained whereas 16 hidden layer neurons give a PSNR of 30.92 dB after median filtering  The histeq in – built function from MATLAB  has been used to implement histogram equalization. It has been observed that on performing histogram equalization and using that image to train the network, the network converges a bit earlier However, the error \(MSE\turates at a higher value and the quality of the reproduced picture is not good. This is clearly visible from the data provided for both 128 x 128 and 256 x 256 grayscale images. For example, for a 256 x 256 grayscale image and 4 hidden layer neurons, training time is 88.98 seconds whereas without histogram equalization, training time is 169.23 seconds Each training vector \(corresponding to each sub-image\ is compared with the other training vectors. For correlation coefficients greater than some specified value \(usually 0.7 to 0.8\, they are eliminated. It has been found that training time is greatly reduced and at the same time, higher values of PSNR are achievable. For example, for a 256 x 256 grayscale image and 4 hidden layer neurons, training time is 88.98 seconds whereas by reducing the correlation, the training time required is 43.95 seconds, a reduction of about 50%. Moreover, for a maximum allowable correlation coefficient upto and not including 0.8, training vectors were reduced from 1024 to 552 for a 256 x 256 Lena Image\. Consequently, training time also got reduced by a factor close to \(552/1024\m 175 sec to 85 sec The image pixels which are represented by 8-bits initially are converted to double data type \(in MATLAB normalized before training the neural network. All necessary calculations are done in double format. The compressed data is then multiplied by +127 and converted to signed 8-bit format for storage \(and transmission if necessary\. However, in this process, some significant bits after the decimal point are lost As a result, a mesh of black dots \(salt and pepper noise\ is introduced in the image after reconstruction and PSNR gets reduced. The noise is more visible for high intensity images as compared to darker images. For example, for a 256 x 256 grayscale image and 8 hidden layer neurons, PSNR before quantization \(without median filtering and without histogram equalization\ is 28.97 dB whereas after quantization, it is 26.10 dB To remove the salt and pepper noise, 2 dimensional median filtering is performed over the quantized and reconstructed image. 3 x 3 blocks are selected from the reconstructed image and median filtering is done on each of these blocks until the whole image is covered. It has been observed that after filtering, the noise reduces significantly in case of high intensity images. It has also been observed that if the network is trained using 8 x 8 sub-images, then the salt and pepper noise is most effectively reduced by performing median filtering over 3 x 3 blocks of the reconstructed image. For example, for a 256 x 256 grayscale image and 8 hidden layer neurons, PSNR before median filtering \(without quantization\ is 24.40 dB and after median filtering, it is 26.10 dB V  C ONCLUSION  Introducing correlation into the image results in faster training of the network. In this paper, faster training has been accomplished by eliminating the highly correlated training vectors, and at the same time, we obtained satisfactory values of PSNR as compared to the technique employing histogram equalization. It has also been observed that as the image size increases, higher compression is obtained and errors are also reduced. However, training time increases significantly R EFERENCES  1  Salomon, D., Data Compression, 4th Edition. Springer, 2006-07 2  Sayood K.  Introduction Data Compression, 2nd Edition 2000, Morgan Kauffmann 3  Henrique S. Malvar Fast Progressive Image Coding without Wavelets  IEEE Data Compression Conference – Snowbird, Utah, March 2000 4  Gerekand, O.N. et al, “Adaptive polyphase subband decomposition structures for image compression,” IEEE Trans.  Image Processing, vol 9, pp. 1649–1659, Oct. 2000 5  Wei Jun et al, Volumetric image compression by 3D Discrete Wavelet Transform \(DWT\ SPIE, The International Society for Optical Engineering, 1995 6  S. Majumder, Md. A. Hussain Wavelet and DCT-Based Image Coding Reconstruction For Low Resolution Implementation pg 814-818 proceedings of International Conference on Modeling and Simulation 2007, AMSE, Kolkata 3-5 December 2007 7  S. Majumder, Md. A. Hussain A comparative study of image compression techniques based on SVD, DWT-SVD and DWTDCT CI2.2 pg 500-504 at International Conference on Systemics Cybernetics, Informatics \(ICSCI-2008\ under Pentagram Research Hyderabad held 2-5 January 2008 8  Simon Haykin, 'Neural Networks, A Comprehensive Foundation', 2nd Edition 9  S. Anna Durai, E. Anna Saro, “Image Compression with BackPropagation Neural Network using Cumulative Distribution Function”,pp185-189 International Journal for  Applied Science and Engg. Technology  
120 


The authors would like to acknowledge the su pport by the National High-Tech Re search and Development Plan of China under grant No. 2004AA113040. Tha nks to Runzhi Yang and Aichen Shi for their previous work R EFERENCES 1 C Ro b e r t s 215 D ig it a l I d e n ti ty a n d  F e d e r a te d  S y st e m s  216  Proceedings of the 2007 ACM workshop on Digital identity management Pseudorandom identifiers list Pseudorandom identifiers list The format is decided by the former field Special resource The special resource in the local domain, such as 215common@gjxPC_Linux_resource_BUAA_edu\216 means the user has the right of accessing the resource gjxPC using the \215common\216 role maybe due to temporary need 215none@gjxPC_Linux_resource\216 means the user has no right of the resource maybe due to the privilege revoking Digital Identity Basic information User\220s basic information, such as sex, phone number email and so on the IEEE Computer Society pp 1540-7993, 2007 6 D S h i n   G   J  A hn, a n d P  S h e n oy   215 E ns uri ng I n f o rm a t i o n A s s u r a nc e i n  Federated Identity Management,\216 IEEE International Conference on Performance, Computing and Communications Table.2 LDAP User Model Information Structure Proceedings of the 2007 International Symposium on Applications and the Internet Workshops Username Username used by a person 2007 9 A   B a ld w i n   M  C  M o n t an d Y B e r e s   215 On I d e n ti ty A ssu r a n ce in th e  Presence of Federated Identity Management Systems,\216 pp. 821-826, 2004 7 P  M a d s en   Y Ko g a  a n d K Ta k a h a s h i  215Fed er a t e d  I d en t i t y  Management for Protecting Users from ID Theft,\216 pp. 94-103, 2005 3 A  B S p a n t z e l  A   C S q ui c c i a ri ni   a n d E. Be rt i n o   215 E s t a b l i s h i n g a n d protecting digital identity in federation systems,\216 common WindowsXP_resource_IEG_CS_BUAA_edu\216 means the user has another set of information in the node of \215michol people_ENGLISH_BU_edu\216 and the pseudorandom name is 215sd12se1\216, and the user also has other system access ability Finally, if the value of \215Is global\216 is 9, then the field of 215pseudorandom identifiers list\216 should point to the identity source So, the field of \215pseudorandom identifiers list\216 records the user\220s cross-domain access abilit y or points to the identity source. And for the \215common\216 user, the field of \215Is global\216 is 0 and the \215special resource\216 is a big list, while the other fields are blank To show the relationship of the resource, we employ several characters including \215_ 216, \215@\216,\216$\216 and so on. What about the situation of the character exist in the username or the resource name? In order to avoid mistake, we will employ escape character-\215\\\216, which is rarely used. So if the username is \215Jose@phy\216, then we should use \215Jose\\@phy\216 in the String To enable self-service of attribute modifying, only basic information and authentication information can be modified by the user him/herself. All other is managed by the administrator V.  C ONCLUSIONS In this paper, we firstly propose unified identity management model and the identity life cycle management Then based on this, we give a federated identity management model and the detailed cross-domain access procedure, at the same time talk about account linking, identity mapping and identity provision. At last, we design the model\220s application that can be regarded as the example of model\220s application The contribution of this paper is as follows 1 Unified identity management model in different systems 2 Identity life cycle management both in one UIM and in federation 3 Pseudorandom identifi er enabled acco unt linking and identity mapping 4 Identity information directory tree structure design and LDAP user model information structure design However, further study is still needed: expanding the scope of unified identity management model; improving the security mechanism of the model and analyze the implementing details of the directory service. More work need to be done on this study and the model also need actual application to showing its advantages and disadvantages A CKNOWLEDGMENT Australian Computer Society Proceedings of the 2005 ACM workshop on Digital identity management pp. 77-83, 2005 8 N   Ki n g e n s t e i n  215At t r i b u t e  Aggr ega t i o n a n d Fed er a t ed  I d en t i t y  216  Is global 0,1,2,9 0: the user needs no cross-domain operation 1: the user needs cross-domain operation and the local domain is the user\220s only identity storage domain 2: the user needs cross-domain operation and there is the user\220s identity storage in the other domain 9: the user is an account link Proceedings of the 2005 ACM Workshop on Digital identity Management Data Element Meaning  pp. 11-19 2005 4 A J o s a n g  J  Fa b r e a n d B  H a y   215Tru s t  R eq u i rem en t  i n  I d en t i t y  Management,\216 pp. 27-35 2007  pp. 12-20, 2005 2 H  G o mi  M  H a t a ke y a ma  a n d S  H o s o no  215 A  de l e g a t i o n fr a m e w o r k f o r federated identity management,\216 vol. 44, pp. 99-108, 2005 5 A  Bha rg a v spa nt z e l A  C  S qui c c i a ri ni  a n d  E Be rt i n o   215 T rus t  Negotiation in Identity Management,\216 list\216 whose value is determined by the field of \215Is global\216 If the value of \215Is global\216 is 0, this field is left null. If 1 this field should list the resources the user can access. If the value of \215Is global\216 is 2, which means some of the user\220s information is stored in the outer domains; the field of identifier list should be filled with the user linking of the identity storage domain and the pseudorandom identifiers list of the other domains. We should have a flag to make it clear that which item is the identity storage one, for example 215$_sd12se1/michol@people_ENGLISH_BU_edu Position A user\220s duty Proceedings of the 2005 ACM workshop on Digital identity management Authentication Information Information used to authenticate the user, such as passwords, cryptography key and so on 


of technical artifacts The cases shown document an increasing inclination of companies to invest in enterprise architecture management to meet the increasing complexity coupled with the introduction of service oriented architectures Most of these activities are focused on IS architectures by now but the requirement to address artifacts within the broad range of enterprise architecture is clearly visible A complementary study in sho ws the emer gence of suitable structures and processes to govern architecture development and enforcement The inclusion of business departments in enterprise architecture activities is a phenomenon showing a broad range of challenges and is very interesting from a research point of view Beyond technical questions especially service orientation requires the construction of appropriate methodologies to foster a sustainable alignment between business and IS departments As mentioned in the introduction of section III the company data especially the measures employed the repository reports used and project documentation standards illustrated that the measurement and evaluation of service oriented architectures is still in its infancy Therefore we started a research project dedicated to the establishment of a proper SOA and EA government framework which requires reformulating the measurement of structures and processes The measures will be derived from the corporate goal system with special emphasis on agility as laid out in section II The resulting metrics will allow statistical testing within the company Within the joint research project the other companies involved will partially apply the resulting measures and metrics allowing both intraorganizational analysis and the derivation of causal models based on a detailed statistical foundation R EFERENCES  S Aier and M Sch  onherr Eds Enterprise Application Integration  Serviceorientierung und nachhaltige Architekturen  ser Enterprise Architecture Berlin GITO-Verlag 2004 vol 2  D S Linthicum Enterprise Application Integration  Reading Massachusetts Addison Wesley 2000  G Stark e and S T ilk o v  Eds SOA-Expertenwissen  Methoden Konzepte und Praxis serviceorientierter Architekturen  Heidelberg dpunkt 2007  S Aier  Sustainability of enterprise architecture and EAI  i n Proceedings of The 2004 International Business Information Management Conference Information Technology and Organizations in the 21st Century Challenges  Solutions IBIMA2004 Jul 4…6 2004 Amman Jordan  K S Soliman Ed IBIMA 2004 pp 182…189  M Kaib Enterprise Application Integration  Grundlagen Integrationsprodukte Anwendungsbeispiele  Wiesbaden DUV 2002  W  K eller  Enterprise Application Integration  Heidelberg dpunkt 2002  S Aier and M Sch  onherr EAI als integrierendes Element einer nachhaltigen Unternehmensarchitektur in Unternehmensarchitekturen und Systemintegration  S Aier and M Sch  onherr Eds Berlin GITO-Verlag 2005 pp 4…56  R K Y in Case Study Research Design and Methods  3rd ed ser Applied Social Research Methods Series London Sage Publications 2002 vol 5  J N Luftman K e y issues for IT e x ecuti v e s 2004  MISQ Executive  vol 4 no 2 pp 269…285 2005  J N Luftman R K empaiah and E Nash K e y issues for IT e x ecuti v e s 2005 MISQ Executive  vol 5 no 2 pp 81…99 2006  J N Luftman and E R McLean K e y issues for IT e x ecuti v es  MIS Quarterly Executive  vol 3 no 2 pp 89…104 2004  P  W eill Don t just lead go v ern Ho w top-performing rms go v ern IT MIS Quarterly Executive  vol 3 no 1 pp 1…17 2004  P  W eill M Subramani and M Broadbent IT Infrastructure for Strategic Agility Massachusetts Institute of Technology Center for Information Systems Research CISR Working Paper CISR WP No 329 2002  F  Beck er  Or ganisational agility and the kno wledge infrastructure  Journal of Corporate Real Estate  vol 3 no 1 pp 28…37 2001  S L Goldman R N Nagel and K Preiss Agile competitors and virtual organizations strategies for enriching the customer  New York NY Van Nostrand Reinhold 1995  H Shari and Z Zhang  A methodology for achie ving agility in manufacturing organisations An introduction International Journal of Production Economics  vol 62 no 1…2 pp 7…22 1999 Available http://dx.doi.org/10.1016/S0925-5273\(98\5  R J V okurka and G Fliedner  The journe y t o w ard agility   Industrial Management  Data Systems  vol 98 no 4 pp 165…171 1998  Y  Y  Y usuf M Sarhadi and A Gunasekaran  Agile manuf acturing The drivers concepts and attributes International Journal of Production Economics  vol 62 no 1…2 pp 33…43 1999 Available http://dx.doi.org/10.1016/S0925-5273\(98\9  Z Zhang and H Shari  A methodology for achie ving agility in manufacturing organisations International Journal of Operations  Production Management  vol 20 no 4 pp 496…512 2000  C R Duguay  S  Landry  and F  P asin From mass production to e xible/agile production International Journal of Operations  Production Management  vol 17 no 12 pp 1183…1195 1997  W  C on En vironment and De v elopment Our common future  Oxford New York Oxford University Press 1987  J Conrad Nachhaltige Entwicklung  einige be grif iche Pr  azisierungen oder der heroische Versuch einen Pudding an die Wand zu nageln Free University of Berlin Freie Universit  at Berlin Forschungsstelle f  ur Umweltpolitik Berlin FFU-Report 00-07 2000 A v ailable http://www.fu-berlin.de/ffu/download/rep  00-07.PDF  V  T eichert Indikatoren zur Lokalen Agenda 21 ein Modellprojekt in sechzehn Kommunen  ser Indikatoren und Nachhaltigkeit Opladen Leske  Budrich 2002 vol 1  J Huber  Nachhaltige Entwicklung durch Suf zienz Ef zienz und Konsistenz in Nachhaltigkeit in naturwissenschaftlicher und sozialwissenschaftlicher Perspektive  P Fritz J Huber and H W Levi Eds Stuttgart Hirzel Wissenschaftliche Verlagsgesellschaft 1995 pp 31 46  N Gronau Wandlungsf  ahige Informationssystemarchitekturen Nachhaltigkeit bei organisatorischem Wandel  Berlin Gito 2003  H Leitschuh-Fecht and U Ste ger  Wie wird Nachhaltigk eit f  ur Unternehmen attraktiv Business Case f  ur nachhaltige Unternehmensentwicklung in Handbuch Nachhaltige Entwicklung  G Linne and M Schwarz Eds Opladen Leske  Budrich 2003 pp 257…266  D Hahn and H Hungenber g PuK  Wertorientierte Controllingkonzepte  6th ed Wiesbaden Gabler 2001  K Bleicher  Organisation Strategien Strukturen Kulturen  2nd ed Wiesbaden Gabler 1991  C Hagen Inte grationsarchitektur der Credit Suisse  i n Enterprise Application Integration  Flexibilisierung komplexer Unternehmensarchitekturen  S Aier and M Sch  onherr Eds Berlin GITO-Verlag 2003 pp 61…81  M Herr  U  Bath and A K oschel Implementation of a Service Oriented Architecture at Deutsche Post MAIL in Proceedings of the European Conference on Web Services ECOWS 2004 September 27-30 2004 Erfurt Germany  ser Lecture Notes in Computer Science LNCS L.-J Zhang and M Jeckle Eds vol 3250 Berlin  Heidelberg Springer 2004 pp 227…238 0302-97433-540-23202-8  A Schwinn and C Hagen Measured Inte gration  Metrik en f  ur die Integrationsarchitektur in Integrationsmanagement  J Schelp and R Winter Eds Berlin et al Springer 2006 pp 267…292  R W inter and R Fischer  Essential Layers Artif acts and Dependencies of Enterprise Architecture in Proceedings of the 10th IEEE International Enterprise Distributed Object Computing Conference Workshops EDOCW06 on Trends in Enterprise Architecture Research TEAR 2006 October 17 2006 Hong Kong  P C K Hung Ed Los Alamitos CA USA IEEE Computer Society Press 2006 pp 1…8 CD…ROM  J Schelp and R W inter   T o w ards a methodology for service construction in Proceedings of the 40th Annual Hawaii International Conference on Systems Sciences HICSS07 Jan 3-6 2006 Waikoloa Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


Hawaii USA  R H Sprague Ed Los Alamitos CA USA IEEE Computer Society Press 2007 pp 64a 1…7  R W inter and J Schelp Enterprise Architecture Go v ernance The Need for a Business-to-IT Approach in Proceedings of the 23rd Annual ACM Symposium on Applied Computing SAC2008 Mar 1620 2008 Fortaleza Cear  a Brazil  L M Liebrock Ed Association for Computing Machinery ACM New York NY USA ACM Press 2008 pp 548…552 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


BIOGRAPHY Sachin Kumar received the B.S degree in Metallurgical Engineering from the Bihar Institute of Technology and the M.Tech degree in Reliability Engineering from the Indian Institute of Technology Kharagpur He is currently pursuing the Ph.D degree in Mechanical Engineering at the University of Maryland College Park His research interests include reliability electronic system prognostics and health and usage monitoring of systems Vasilis Sotiris received the B.S degree in Aerospace Engineering from Rutgers University in New Brunswick New Jersey and the M S degree in Mechanical Engineering from Columbia University in New York He worked as a Systems Engineer for Lockheed Martin Corporation concentrating on software development projects for the Federal Aviation Administration He is currently pursuing the Ph.D degree in Applied Mathematics at the University of Maryland College Park His research interests are in the field of applied statistics and computational mathematics related to diagnostics and prognostics for electronic systems   _ n Acoustics an M.S in Electrical _  in Engineering Mechanics from the g  University of Wisconsin at Madison He is a Professional Engineer an IEEE Fellow and an ASME Fellow He has received the 3M Research Award for electronics packaging the IEEE Award for chairing key Reliability Standards and the IMAPS William D Ashman Memorial Achievement Award for his contributions in electronics reliability analysis He has written over twenty books on electronic products development use and supply chain management He served as chief editor of the IEEE Transactions on Reliability for eight years and on the advisory board of IEEE Spectrum He has been the chief editor for Microelectronics Reliability for over eleven years and an associate editor for the IEEE Transactions on Components and Packaging Technology He is a Chair Professor and the founder of the Center for Advanced Life Cycle Engineering CALCE and the Electronic Products and Systems Consortium at the University of Maryland He has also been leading a research team in the area of prognostics and formed the Prognostics and Health Management Consortium at the University of Maryland He has consulted for over 50 major international electronics companies providing expertise in strategic planning design test prognostics IP and risk assessment of electronic products and systems 9 


Engineering Education Annual Conference & Exposition 2005 3  Chong N., and M. Yamamoto Collaborative Learning Using Wiki and Flexnetdiscuss: a Pilot Study  Proceedings of the fifth IASTED International Conference on Web-based Education Puerto Vallarta, Mexico, Jan 23-25 2006 4  Engeström, Y. Learning by Expanding. OrientaKonsultit Oy, Helsinki, 1987 5  Forte A., and A. Bruckman From Wikipedia to the Classroom: Exploring Online Publication and Learning  Proceedings of the 7 th International Conference on Learning Sciences, Bloomington, Indiana, 2006, pp. 182-188 6  Grierson H., Nicol D., Littlejohn A., and A Wodehouse Structuring and Sharing Information Resources to Support Concept Development and Design Learning  Proceedin gs of the Networked Learning Conference, 2004 7  Gross Davis, B., Tools for Teaching, Jossey-Bass Publishers, 1993 8  Hon A. and W. Chun The Agile Teaching/Learning Methodology and its E-learning Platform Wenyin Liu, Yuanchun Shi, Qing Li \(Eds Advances in Web-Based Learning - ICWL 2004, Third International Conference, Beijing, China, August 8-11, 2004 9  Johnson, R. and D. Johnson An Overview of Cooperative Learning originally published in J. Thousand A. Villa and A. Nevin \(eds\ativity and Collaborative Learning, Brookes Press, Baltimore, 1994 10  Järvinen, E-M Education about and through Technology. In search of More Appropriate Pedagogical Approaches to Technology Education Acta Universitates Ouluensis, E 50. Oulu: Oulun yliopisto, 2001 11  Kim S., Han H., and S. Han The Study on Effective Programming Learning Using Wiki Community Systems Innovative Approaches for Learning and Knowledge Sharing, Springer Berlin, Vol. 4227/2006, pp 646-651 12  Koufman-Frederick, A., Lillie, M., PattisonGordon, L., Watt, D., and R. Carter Electronic Collaboration: A Practical Guide for Educators The LAB at Brown University, 1999 13  Leuf B., and W. Cunningham The Wiki Way Quick Collaboration on the Web Addison-Wesley, 2001 14  Mirijamdotter A., Somerville M. and Holst M An Interactive and Iterativ e Evaluation Approach for Creating Collaborative Learning Environments The Electronic Journal Information Systems Evaluation, Vol. 9 No. 2, 2006 pp. 83-92 15  Murugesan S Understanding Web 2.0 IT Pro July/August 2007 16  Parker K. and Chao J Wiki as a Teaching Tool  Interdisciplinary Journal of Knowledge and Learning Objects. Vol. 3, 2007 17  Patokorpi, E., Tétard F., Qiao F. and N. Sjövall Mobile Learning Objects to Support Constructivist Learning in Learning Objects: Applications, Implications and Future Directions, Koohang A. and K. Harman \(eds 2007 18  Poikela, E. and A.R. Nummenmaa Ongelmaperustainen oppiminen tiedon ja osaamisen tuottamisen strategiana Problem-based learning as a strategy for knowledge building\ Poikela, E. \(ed Ongelmaperustainen pedagogiikka teoriaa ja käytänt Problem-based pedagogy theory and practice\ Tampere Tampere University Press, 2002 19  Reinhold S., and D. Abawi Concepts for Extending Wiki Systems to Supplement Collaborative Learning in Technologies for E-Learning and Digital Entertainment: Proceedings of the First International Conference on Edutainment, Berlin : Springer, 2006, p. 755 767 20  Richardson, W Blogs, Wikis, Podcasts, and Other Powerful Web Tools for Classrooms Sage Publications: California, US; London UK; New Delhi, India 2006 21  Schaffert S., Bischof D., Bürger T., Gruber A Hilzensauer W., and S. Schaffert Learning with Semantic Wikis in First Workshop SemWiki2006 - From Wiki to Semantics, co-located with the 3rd Annual European Semantic Web Conference \(ESWC\ Budva, Montenegro 11th - 14th June, 2006 22  Schwartz L, Clark S., Cossarin M., and J. Rudolph Educational Wikis: Features and Selection Criteria  International Review of Research in Open and Distance Learning, Vol. 5, No. 1, 2004 23  Stvilia, B., Twidale, M. B., Smith, L. C., and L Gasser Assessing information quality of a communitybased encyclopedia in: F. Naumann, M. Gertz, S. Mednick Eds.\, Proceedings of the International Conference on Information Quality - ICIQ 2005, Cambridge, MA: MITIQ 2005, pp. 442-454 24  Tétard, F. and E. Patokorpi A Constructivist Approach to Information Systems Teaching Journal of Information Systems Education, 16\(2\05, pp. 167-176 25  Viégas F., Wattenberg M., Kriss J., and F. van Hamn Talk Before You Type: Coordination in Wikipedia  Proceedings of the 40th A nnual Hawaii In ternational Conference on System Sciences, 2007, p. 78 26  Wagner, C. and P. Prasarnphanich, P Innovating Collaborative Content Creation: The Role of Altruism and Wiki Technology Proceedings of the 40th Annual Hawaii International Conference on System Sciences, 2007 27  Wang C., and D. Turner D Extending the Wiki Paradigm for Use in the Classroom Proceedings of the International Conference on Information Technology Coding and Computing, 2004, p. 255 28  Wheeler S., Yeomans P. and D. Wheeler D. \(2008 The good, the bad and the wiki: evaluating studentgenerated content for collaborative learning in British Journal of Education Technology, 2008 29  Wiki EduTech Wiki retrieved April 9, 2008 from http://edutechwiki.unige.ch/mediawiki/index.php?title=Wiki oldid=17132   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


impodt Top Leve M d Ile Level Dom1ain Lave i 1po Scenwd1o Level Figure 8 Ilium Ontology Suite typical It is possible and in fact common to employ only a few of these ontologies in particular studies For example many studies are only concerned with physical platforms and therefore only need the MilAsset ontology which imports IlumAsset IliumFramework and DUL Figure 8 illustrates the existing Ilium Ontology Suite and a typical import pattern for a complex Modeling and Simulation application scenario Currently the illustrated suite less Millnfo MilComm and UAV defines 1240 OWL Classes 274 OWL Object Properties and 188 OWL Datatype Properties 6 AN EXAMPLE APPLICATION We are currently using the described approach to conduct systems requirements and other engineering analyses for military aerospace systems In these investigations it is useful to consider detailed aspects of the system design or software prototype behavior in the context of large scale network-centric military operations In the past year in particular we have assembled a collection of well known and widely accepted military simulations to prepare for an extensive investigation of requirements for autonomy in unmanned military platforms It is believed that useful insights in the study will be sensitive to important details of system behavior supporting sensor platform and communications technologies as well as the combined effects of these technologies on an advanced highly networked military force Thus the study environment must provide a means to reliably model and evaluate significant technical detail and to measure the effects as well as propagation of those effects throughout a broad operational context 11 


I r Z  Figure 9 A Military Modeling and Simulation Configuration of the Ilium Framework To do that we have assembled a collection of trusted military simulations that span the range of such applications from fine-grained simulations that focus on detailed interactions between two entities to coarse-grained simulations of military campaigns Figure 9 illustrates the Ilium Framework configured to accommodate those simulations as well as representative prototype systems In this case the legacy applications are simulations analysis systems and design tools Prototypes are notional UAV autopilots route planners decision support systems and similar applications The Framework augments these applications with control objects and agents that coordinate computations in the composed system and specialize agents that typically represent the characteristics of a new system or technology that cannot be easily or adequately simulated in any of the component legacy systems In our current study the following systems have Ilium Framework plugins and are used as components in the study environment 12 Al f<cOmmi.ain FIa 1A  


THUNDER is a campaign simulation that models national military forces and military operations that extend over months 30  Characteristics of individual systems are evaluated statistically and their effects on the overall campaign are not explicitly known Political social and cultural objects and concepts are not included in the model In this configuration THUNDER is used to generate force level tasking mission orders and to evaluate and adjust for the results of missions with respect to campaign plans SEAS is a force level simulation that simulates battles between major forces in combat operations that typically last for hours and as much as a day SEAS features a flexible rules based decision logic that can influence behavior at both the commander and individual combatant level SEAS executes the missions requested by THUNDER and provides a manageable dynamic context for examining the behavior of prototypes in a range of typical operational situations  31 A dynamic plug-in supports interaction with the Framework and other pugged-in components EADSIM is a trusted model of air defense systems that is in particular sensitive to many important design attributes of individual systems In certain modified forms it can reference advanced sensor and engagement decision models In our study configuration EADSIM simulates enemy air defenses in selected e.g significant to our investigation portions of the virtual battle A dynamic plug-in supports system control as well as interaction with the Framework and other pugged-in components A prototype aircraft mission planning system plug-in supports virtual real time mission plan creation and updates In particular the system provides automatic route planning for selected platforms that have been assigned missions by THUNDER and that are subsequently executed in simulation in SEAS and EADSIM The Ilium Framework itself provides software agents that are used to model notional or experimental UAV characteristics and behaviors Depending on the objectives of a particular study the Framework may also provide agents that address advanced Command and Control concepts to coordinate the interaction of the various component systems We maintain a semantic consistency among the plugged-in component applications by developing a single operational scenario as initial input for a study and deriving the necessary application configuration data from that source We create an RDF model of the scenario based on the Ilium suite of ontologies that includes political context issues objectives sensitivities etc military context centers of gravity campaign objectives etc geophysical environment military units order of battle unit equipment lists etc command and control assets platforms weapons ISR systems etc Figure 11 below is an excerpt from an operational scenario set in the Southwest U.S depicting a description of an Air Force Wing assigned to a notional Joint Task Force The Wing is based at Bishop Air Base and has six Fighter Squadrons assigned to it Additional detail about each of those squadrons as well as the base is found in the model in this case an rdf:resource  associated with it morg:AirForceWing rdf:ID="USAF_366 Air_Exp Wg dc:creator>Doug Holmes</dc:creator mgeo:basedAt rdf:resource="#BishopAB geo:positionedAt rdf:resource="#BishopAB_pos morg:assignedOrg rdf:resource="#USAF 390_Ftr_Sq morg:assignedOrg rdf:resource="#USAF_494_Ftr_Sq dc date I 0/20/07</dc date morg:assignedOrg rdf:resource="#USAF_496 Ftr_Sq rdfs:comment>366 AEW F-22 F-15 KC-135 130]</rdfs:comment morg:assignedOrg rdf:resource="#USAF 389_Ftr_Sq morg:assignedOrg rdf:resource="#USAF 391 Ftr_Sq morg:assignedOrg rdf:resource="#USAF 495_Ftr_Sq rdfs:label>366 Air Expeditionary Wing</rdfs:label morg:AirForceWing ab9 7P.b ii uI1 L L CFigure 11 An RDF model of a notional USAF Wing Figure 12 is another excerpt from the same scenario illustrating a model of a particular Fighter Squadron and one of the F-15E aircraft it operates Figure 10 Legacy Components in the Ilium Framework 13 suka 


morg:AirliorceSquadron rd:lD U SAF _8 htr Sq rdfs:label>8th Fighter Squadron</rdfs:label geo:positionedAt rdf:resource="#GeorgeAB_pos msim:hasSeasModel rdf:resource="#BAFGA mast:hasModel rdf:resource="#BAFGA rdfs:comment>An F15E Squadron</rdfs:comment dc:date>9/26/07</dc:date dc:creator>Doug Holmes</dc:creator mgeo:basedAt rdf:resource="#GeorgeAB morg:AirForceSquadron F1 5E rdf:ID="F1 5E 05 dul:isReferenceOfRealization rdf:resource="#AirObj ect_2007 mast:equipment-of rdf:resource="#USAF 7 Ftr_Sq dc:creator>Doug Hollmes</dc:creator dc:date>9/26/07</dc:date msim:hasSeasPlane rdf:resource="#F15E mgeo:basedAt rdf:resource="#GeorgeAB geo:positionedAt rdf:resource="#GeorgeAB-pos rdfs:label>F-15E 005</rdfs:label F-15E Figure 12 An RDF model of a Fighter Squadron and one of its aircraft Note that these models also have associated SEAS models that contain information peculiar to the SEAS simulation about these entities This information is used to configure SEAS to properly represent these particular entities We are also able to insert objects and information that may be of interest in our study that is not represented in any of the component simulations or other applications Where those objects are related to an object that is simulated that relationship permits inferences about the effects on them as a result of actions that are computed in a simulation For example it is possible to indicate that GeorgeAB defends the capital city and lend special significance to the actions of aircraft that are based there even though none of the simulations in the composite system have any notion of capital city Finally the operational scenario provides an explicit record of the assumptions that underly the study and can also include and explicit representation of system and study goals This practice improves analysis and may enable future knowledge based analytical tools Throughout the process of constructing the operational scenario and when it is complete we use one or more Description Logic Reasoners to ensure the logical consistency of the the model A number of these automatic theorem provers are freely available including Pellet 32 33 FaCT  34 and OWLIM 35 are used in the Framework to compute logical entailments and to complete RDF models as well as to ensure the consistency of models In the later capacity frequent checks will identify errors in the construction some e.g Pellet also indicate the source of the error and aid in repairing it As a result we are confident that the operational scenario is a sound model The primary use of the Reasoners allows us to significantly extend the explicit RDF model that is created For example a squadron is explicitly specified as assignedTo a wing and that relationship is the inverse of assignedOrg then the system can infer that the wing has the squadron as an assigned organization even though that fact has never been asserted Similarly if the same relationship is defined as a transitive relationship it is possible to infer that a flight that is assigned to the squadron is also assigned to the wing Once we have an operational scenario that has been classified by a Reasoner we then use the completed model to configure the composite system Ilium and the pluggedin systems to execute the simulation We use SPARQL a W3C standard query language designed to access RDFL to extract data from the scenario to create the input files needed to configure the various applications.[36 SELECT name pos long lat alt vis W1HLERE name rdf:type mast SeasLocation name geo:positionedAt pos pos pos:long long pos pos:lat lat pos pos:alt alt name msim:Visible vis  Figure 13 A typical SPARQL query In a similar fashion SPARQL is used to extract data from the scenario to create the necessary Ilium java surrogate control and agents SPARQL can also be used to review the scenario and answer questions that have arisen since the inception of the study At this point we are able execute the scenario with confidence that it will produce reliable results 7 CONCLUSIONS We have developed a methodology and supporting tools for creating an operational scenario that supports the semantic interoperability of an ad hoc collection of legacy applications and extends their capabilities The method depends on and ensures the logical integrity of the composite system Therefore when we assemble as a collection of legacy component systems that were not originally intended to interoperate with other systems we can be confidant that the composite system will produce consistent results Logical consistency implies that to the degree that we trust the interpretation underlying the model the results of operations on the model are trustworthy If for example the model is based on a Newtonian interpretation of physics the model ought to provide reliable answers to questions about automotive and even aeronautical engineering but probably not to all questions in astronomy or cosmology This methodology extends the utility of trusted simulations allowing integration of finegrained simulations that are sensitive to design requirements with high level coarse-grained simulations that are sensitive to acquisition issues and policy The potential benefits of 14 


this sort of interoperation may range from obvious production efficiencies to clearer insights into system requirements Finally an important side-effect of the approach is the OWL/RDF knowledge base formed by the combination of the operational scenario and the results of the operations of the legacy systems That knowledge base and the use of SPARQL queries and SWRL rules effectively expands the functionality of the system and greatly improves the analysis of the output of the system of cooperating simulations and tools We have prepared a foundation for the application of Semantic Web and other knowledge based tools in the analysis and design of unmanned systems We anticipate the development and application of these tools and the addition of autonomy directed Multiple Agent Systems MAS that use RDF/XML inter-agent communications in the coming year REFERENCES 1 NAFCAM 2001 Exploiting EManufacturing:Ineroperability of Software Systems Used by U.S Manufacturers available at 12 Protege Ontology Editor documentation available at http proteae.stanf6rd.edu 13 Top Braid Composer Datasheet available at 14 W and Nicola Guarino 2001 Support for Ontological Analysis of Taxonomic Relationships J Data and Knowled 39\(1 October 2001 15 Natalya F Noy and Deborah L McGuinness Ontology Development 10 1 A Guide to Creating Your First Ontology  Stanford University Stanford CA 94305 16 Alan Rector Modularisation of Domain Ontologies Implemented in Description Logics and related formalisms including OWL K-CAP'03 October 23-25 2003 Sanibel Island Florida USA pp 121-8 2003 17 Cyc Homepage available at htc.c 18 Open Cyc home page available at 19 SUMO Description and Home Page available at 19 SENSUS Description and Home Page available at 2 Bemers-Lee Tim Hendler Jim Lasilla Ora The Semantic Web Scientific American available at 20 DOLCE A Descriptive Ontology for Linguistic and Cognitive Engineering ontology and documents available at 3 Bemers-Lee Tim Blog on Design Issues available at 4 RDF Primer available at s chema/#ref-rdf-primer 5 Lacey Lee OWL Representing Information Using the Web Ontology Language Trafford Publishing 2005 6 OWL Web Ontology Language Guide available at Jten pHomewPage availal adt 7 Jena Home Page available at 8 Protege Home Page available at h1ttp  protegest nftanford.edu,X 1 9 Baader Calvanese McGuiness Nardi and PatelSchnieder Description Logic Handbook 10 Oberle Daniel Semantic Management of Middleware Springer 2006 OWL Web Ontology Language Guide available at 21 Nicola Gauarino Claudio Masolo Stefano Borgo Aldo Gangemi and Alessandro Oltramari Ontology Infrastructure for the Semantic Web Wonder World Deliverable DI 8 Laboratory for Applied Ontology Trento Italy 2001 available on line at ht X1t1 22 DUL.owl ontology available at www.1oa23t og DLl 23 Amy Knutilla Steven Polyak Craig Schlenoff Austin Tate Shu Chiun Cheah Steven Ray and Richard Anderson Process Specification Language An Analysis of Existing Representations NIST report available at http llwww.mel.nlist.gov/msid.librarZ/d.oc/psl-1 _.df 24 Process Specification Language Ontology available at http-//www,55,me l.nit gov/psl 25 Ontology for Geography Markup Language GML3.0 owl ontology available at ok i.cae.drexel.edu./-wbs/onltology/2004/09/ogc-gmI1 26 Ontology for Geography Markup Language GML3.0 of Open GIS Consortium OGC Home Page available at 15 


 Peter Maguire Using THUNDER for Campaign Studies DSTO-TN-0303 DSTO Melbourne August 2000 28 User Manual SEAS Version 3.7 U.S Air Force SMC/XR February 2007 29 Bijan Parsia and Evren Sirin Pellet and OWL DL Reasoner MINDSWAP Research Group University of Maryland College Park available at 30 Pellet Home Page available at 31 FaCT Home Page available at 32 OWLIM Home Page available at 3 A SPARQL Tutorial available at BIOGRAPHY Douglas Holmes is co-founder and Senior Partner of Java Professionals Inc In the past twenty-two years he has managed and participated in numerous artificial intelligence and knowledge-based programs for DARPA and other research agencies as well as commercial applications in the petroleum and other sectors He is currently developing ontologies and applying Semantic Web technology to support research and development of military unmanned systems He also has over twenty years experience as an Air Force Fighter Pilot and Fighter Weapons School Instructor Mr Holmes has a B.S in Mathematics and Basic Sciences from the U.S Air Force Academy and a M.S in Management Information Systems from Golden Gate University Richard Stocking is the lead Program Investigator/PM for Net Centric Operations Warfare Analysis efforts for Lockheed Martin Aeronautics Advanced Development Programs The Skunk WorksTM He is currently leading efforts researching autonomous UAV operations Current efforts include the integration of Multiple Agent Systems and other autonomy systems within the Ilium Framework He has over thirty years experience and over 11,000 flight hours with multiple C4ISR systems in the US Army and US Navy Mr Stocking has a M.S in Systems Technology from the Naval Postgraduate School 16 


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


