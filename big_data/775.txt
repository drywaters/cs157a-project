A Ph.D. Curriculum for Digital Forensics Dr. Frederick B. Cohen, Ph.D California Sciences Institute dr.cohen at mac.com Dr. Thomas A. Johnson, Ph.D California Sciences Institute tomjohnson at attglobal.net Abstract This paper presents a curriculum for a doctorate in digital forensics and discusses the implementation of that curriculum in a graduate program It includes overviews of all of the classes and in-depth coverage of specific areas that go beyond the Masters level. It also 
discusses how that program is being implemented at the California Sciences Institute a Non-profit California Public Interest Educational Institution oriented toward graduate education in the areas of Advanced Investigation and National Security 1 Background This paper is about a curriculum for a doctorate level degree in digital forensics, and it is based on the curriculum currently being implemented at California Sciences Institute 1.1.The state of the art in digital forensics experts 
In examining the state of the art in the field of digital forensics we considered the fundamental baseline to be the knowledge, skill, training, education and experience identified in the Daubert decision [1 a s the requirement for producing expertise relevant to the field and the extent to which current witnesses qualified as experts in the digital forensics field compare to experts in other scientific fields Witnesses admitted as experts to testify in almost every field of scientific study today hold doctorate 
degrees in a specific field of expertise associated with their testimony. It is very likely that few if any courts would accept medical testimony in a legal matter unless it came from someone holding a medical doctorate The testimony of a nurse for example would certainly be discounted if it related to the things that only an expert can testify about, and even an MD in a different specialty would likely be disallowed unless they could show specific knowledge in the key 
subfields of import to the matter at hand. The same is true in statistics or other mathematical fields in biology in chemistry in psychology and in fact in almost any scientific field, this is also true. However in the field of digital systems, digital forensic evidence is almost never presented in court or legally opined upon by someone holding a Ph.D. in a relevant field And, because of the relative youth of the field and lack of academic attention to it until now in most cases 
where a Ph.D. is involved they have relatively little knowledge, skill, training, education, or experience in the field of digital forensics. Typically, they are experts in some other field relevant to the matter at hand, such as computer science or electrical engineering While we are not aware of any formal studies in this area even in Federal criminal cases, almost every expert we have identified presenting opinions related digital forensic evidence is less educated than experts in other fields. Typically, they have B.S. degrees with training 
certificates from commercial companies To get a sense of how dire the situation is,  select reviews of documents presented to judges for searches showed that many of these documents contain stock language that is not strictly speaking factually accurate Review of expert reports commonly shows errors in the analysis such as incorrect interpretation of Internet RFCs. And many expert witnesses treat RFCs and similar documents as if they were definitive statements about the manner in which networks actually operate and in some cases, as if they defined a 
legal mandate for behavior in the Internet There seems to be little interest or attention paid to experimental technique and as a result many statements made in legal matters are not consistent with empirical data. Most of the expert reports fail to indicate any calibration or validation of the tools in use information on the reliability of these tools is Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 


largely lacking, and there is rarely any depiction of the accuracy of numerical results, descriptions of sources of error, and indication of how errors are resolved and results validated. It seems that most of this gets past the legal system because nobody involved in the matter knows enough bothers, or is adequately resourced to challenge the evidence and testimony If this continues to be the case, it is highly likely that the legal system will start to build substantial precedents that are based on expertise that is inadequate to the need, and that these precedents will be used for many years to come, producing outcomes and process that is not in the interest of justice Furthermore unless and until Universities start to create doctorate level programs and expertise in this area, the seed corn will not exist to generate the large number of individuals with expertise to educate others do research to advance the field and work on and testify in legal matters 1.2.Masters degree programs Many Masters programs in digital forensics exist today, and their coverage is reasonably well known.[3 Typically they involve classes in evidence identification  imaging and other collection transportation, storage, and analysis, and presentation and they commonly involve laboratory experiments or other practical experience in doing standard disk images Internet searches basis investigations computer-related crime and some coverage of legal issues Many MS degrees involving digital forensics are concentrations within MS programs in computer science or criminal justice departments. For example in one program in the context of a computer science department, [2 e cu rricu lum in clud e s Forensic Digital  Imaging Introduction to Cybercrime Advanced Digital Evidence Detection and Recovery Digital Evidence Search and Seizure In our own program which emerged from the program at the University of New Haven, the Masters degree is in the context of either a national security or advanced investigation program, and includes Computer crime and legal and investigative issues Digital Forensics 1 Digital Forensics 2 Challenges to digital forensic evidence The program also has mandatory courses in white collar crime, deception, co unter-deception, and critical thinking law and evidence a survey of forensic science, and a research project or internship. But even this is not the full set of issues that must be addressed in order to truly understand the issues in the field Simply put, there just isn't enough time in a Masters program to teach more than the basics of evidence identification and collection, limited analysis limited presentation, limited legal coverage, and very limited experience 2 The doctorate program In order to address what we believe to be the need for more in-depth knowledge better education and more experience and skill, we identified the key items that we believe to be missing in the present digital forensics programs at the MS level and created a program based on other doctorate level programs in similar fields 2.1.The objective of the additional curricula What we believed to be missing from current programs including our own was a clarity and completeness of coverage This goes from understanding how digital systems work from the physics through the user interfaces to understanding the legal issues from the basics of the laws through presentation and challenges in court. Of course there are limits to what can be done in a few years of full time study in a graduate program but the education and knowledge we provide should be focussed on what the student will need to know as well as what they will need to be able to understand throughout the remainder of their career and life, based on what we can teach them While we would very much like every digital forensics expert to understand everything from device physics to electromagnetic of communicants systems to theory of computation, to operating system design to ergonomics, and everything about the legal system associated with a lawyer; in practice, we simply cannot go that far But we can take them from the lowest levels of physics to realistic legal settings stopping every step along the way for enough understanding to seed their knowledge and interests and enough information so that when they give depositions, write Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 


reports, advise others, and testify, the things that they write and say are strictly accurate in every way from a scientific standpoint and reasonably well presented and relevant from a legal standpoint. At least that is our goal 2.2.Additional courses At the doctorate level, we augment the M.S. level expertise which must be achieved in order to gain standing as a Ph.D student with the following advanced courses  The physics and mechanisms of digital systems  Information physics Time Space and Computation  Operating systems, networks, and applications  Programming digital forensic analysis  Digital crime scene reconstruction  Challenges to digital forensic evidence \(at the doctorate level  Research methods  Probability and statistics  Research with guidance The outline of this curriculum projected into a quarter system is provided in Figure 1. The required courses must be taken in sequence in order to get the full effect of the mock trial, which occurs as part of the final quarter and is integrated into the Challenges to Digital Forensic Evidence II courses During the first 30 credit hours \(red\ students must have completed all 600 level courses or have accepted equivalent transfer courses and credits All students must participate in at least 2 internships during their first 30 credit hours and must maintain a suitable average and not be on probation at the end of the first 30 credit hours of courses Students wishing to terminate the program at this point, may opt to receive an M.S degree in digital forensics by taking an additional 6 credits During the last quarter of the second 30 credit hours yellow students normally study and sit for their qualifying exams Upon advancement to candidacy the student will seek a dissertation topic begin work on their dissertation proposal, and select a dissertation committee Before starting their dissertation green the student must form a dissertation committee, propose a thesis topic, and gain approval for that topic by their dissertation committee  Fall Winter Spring Summer Law and Evidence Digital Forensics 1 Digital Forensics 2 Challenges to digital forensic evidence Internet and criminal activities Deception counterdeception critical thinking White collar crime 600-level research Internship Internship Internship Internship Probability and Statistics Research Methods Operating Systems Networks, and Applications Programming Digital Forensic Analysis The Physics and Mechanisms of Digital Systems Information Physics: Time Space, and Computation Digital Crime Scene Reconstruction Challenges to digital forensic evidence 2 Research Research Research Qualifying exams Ph.D Seminar Ph.D Seminar Ph.D Seminar Ph.D Seminar Dissertation Dissertation Dissertation Dissertation Figure 1 The overview of the curriculum The physics and mechanisms of digital systems This course focuses on the details of digital systems and how they operate at the physical electronics optical, and other low-levels. This course is designed to provide knowledge of the underlying technical basis for how digital systems work so that the digital forensics expert can gain clarity around the underlying mechanisms involved in digital systems and understand explain and testify about these mechanisms as well as evaluate the use of these mechanisms when applied to legal matters Information physics Time Space and Computation This course focuses on the underlying mathematics of computation and communication covering the basics of information theory issues of time space complexity and computability number theory their general applications, and their use in analysis of digital forensic evidence questioned digital documents and related matters Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 


Operating systems, networks, and applications This course reviews principles and specifics of operating systems and applications ranging from cell phones to large-scale distributed computing networks It covers system bootstrap execution shutdown file systems network interfaces protocols system calls execution control, sharing, locking mechanisms, and a range of other related topics Programming digital forensic analysis This course focuses on writing special purpose programs to do analysis of digital evidence. It includes laboratory exercises ranging from the creation of simple shell and perl scripts to the customization of special purpose programs for use in forensic analysis It brings in open source software tools and customizes them, works through issues in the use of commercial tools, and works on the development of test tools to test other forensics software for properties, validation and calibration Digital crime scene reconstruction This course focuses on the use of technology to make high quality reconstructions of digital crime scenes and the limitations of those reconstructions in a legal setting It goes from simple testing of basic claims and theories to partial and nearly complete reconstructions of complex digital crime scenes involving multiple systems and networks Challenges to digital forensic evidence at the doctorate level This course focusses on how digital forensic evidence is challenged in a legal setting At the MS level, this course uses the characteristics and features described in Masters level courses, but at the doctoral level, students with the advanced courses from the rest of the Ph.D. curriculum challenge the challenges of the MS students and challenge the evidence by taking advantage of the additional knowledge gained through deeper understanding of the physics of the digital and informational mechanisms probability and statistics and  research methods advanced knowledge of operating systems networks and applications programmed analytical techniques and digital crime scene reconstruction techniques. This course uses real evidence from real cases and includes realistic creation of reports, taking of depositions challenges to other students and witness testimony and concludes in a mock trial Probability and Statistics This course covers basic issues in probability and statistics from the perspective of challenging claims about these results. It addresses underlying notions of causality the use of statistical methods to refute the null hypothesis and the limits of those methods for proving causality It also looks at the issues of correlation and the implications of these methods as they apply to digital evidence Research methods This course focuses on qualitative and quantitative research methods. It is a course that helps the student gain clarity around the issues of research including understanding the limitations of research and science the reliability of different scientific claims scientific validation processes, and how to evaluate and present limitations of scientific analysis This is critical to being able to properly present and evaluate digital forensic techniques and define error rates and reliability necessary for presentation in legal settings 2.3.Research requirement All students must complete at least two quarters of research before taking their qualifying exams This research is under the guidance of a professor in the program and is typically in some area of digital forensics. In these courses, the students are expected to become familiar with research as it is done by the faculty and to apply the things they are learning in their courses to that research In most cases during the course of this research, students will co-author one or more publication quality articles and gain experience in the whole process of research and research publication 2.4.Admissions and qualifications To be admitted to the program students must possess a B.S degree in a related field from an accredited university submit examples of previous writings or publications that demonstrate their ability to do research submit scores from the GRE and submit transcripts from their previous university courses and degrees Students holding M.S degrees in a relevant field from an accredited university may seek to have select courses from their previous programs counted as equivalent courses to those within this program to reduce the course requirements prior to entering into the doctorate level courses. These will be evaluated on a case by case basis or, for schools with pre-existing Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 


agreements accepted based on performance in the previous institution in authorized M.S.-level transfer courses However in no case shall the 30 resident credit requirement for admission to candidacy be waived or reduced. All courses from the first 30 credit hours must be satisfactorily completed before entering the second 30 hours, however; select courses from the 2 nd 30 hours may be entered upon instructor approval Specifically the probability and statistics research methods and operating systems networks and applications courses The remaining courses must be taken in sequence for full effect 2.5.Dissertation and research requirements As in most doctoral programs students must do meaningful research that advances the state of the art and write up their results along with the necessary background in a dissertation that is comparable in quality to what would be found in a refereed journal associated with a professional society Each student must complete and publish an original empirical or theoretical research dissertation Prior to starting the actual dissertation students must first submit a prospectus for their dissertation that includes a detailed background of the specific area of research, and they typically include this, in large part, in the body of the dissertation as background to their specific work 2.6.Ph.D. seminar, internships, and related work In addition, the program requires that students take part in a seminar series in which experts from various disciplines related to digital forensics present up-todate information on the field. This is likely to involve presentations of issues in legal cases, talks from local state and federal law enforcement results of investigations, talks by visiting faculty members from other universities and so forth Internships are typically required however for students who are currently employed in the field internships may be waived if their regular work meets the same requirements 3 Other challenges There are four major challenges that we face in trying to build this emerging program 3.1.Obtaining faculty At this point in time, there are very few individuals with Ph.D.s in related fields and who have research and practical experience in the field of digital forensics Most of them are very occupied. As a result, there is a substantial challenge in finding faculty to support a program such as this, and in creating a program that will produce Ph.D.s in this field. As in any emerging discipline this challenge is met by using a mix of experts from other fields and creating a faculty that fuses their expertise to build the new discipline. As the discipline emerges some of the doctorate level graduates then help to populate other universities which produce more resources that then allow the universities that seeded these programs to reseed themselves The approach we have taken is to build faculty from the practical community that exists. CSI is starting to operate in the Bay Area near San Francisco, and as a location where much of the software and hardware of the information age was developed this area is particularly rich in expertise and experienced in dealing with computer-related crime While we draw faculty in specialized areas from all over the country we don't have the resources or demand required to hire the full time faculty necessary to completely cover all of the expertise involved in our curriculum. As a result we use active and recently retired professionals from the various communities involved engage the local community leaders in relevant fields, and spend time and effort getting involved with groups like the local Electronic Crimes Task Force, local law enforcement agencies and their forensic laboratories high technology businesses and engage those who have worked in the national security arena. Each of these communities have both substantial expertise and substantial need for additional expertise, and by acting as a conduit for the exchange of knowledge and formalization of that knowledge we can help them while engaging those who have the proper background experience, knowledge, and desire to become members of the faculty At the same time, teaching Ph.D. students requires that all or most of the faculty have a Ph.D. in a relevant area We are very fortunate to have formed relationships with professors from around the country who have expertise and experience in this field, and our faculty also helps to engage with those other universities to collaborate in helping them to provide expertise for their programs.  For the next 5-10 years, a combination of Ph.D.s from other fields and professors from other universities, industry, and government, will be used to build this program and hopefully other universities will engage in this process as well to provide us with the seed corn that we need to sustain Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


our program over time and build the next generation of our faculty 3.2.Up-to-date materials and cases A major impediment to success in such programs that has been reported to us by others, and that we have experienced as well, is the difficulty in getting up-todate materials for many of these courses. For example in seeking challenges to digital forensic evidence, the use of current cases is potentially problematic because we may find challenges that destroy existing cases While this is potentially an substantial benefit for the overall system of justice the system of justice as it exists today is oppositional by nature. The strategy and tactics of cases limits the willingness of many attorneys to provide information on active cases, and many cases are under seal, or otherwise problematic in terms of getting access to evidence While the ideal drama may come from a current multiple homicide case in which one of the accused claims innocence and is proven so by a challenge to digital forensic evidence, the reality is that most cases are civil litigations that are settled with all evidence and other materials destroyed or returned without public disclosure. In many senses, the openness of the university is exactly the opposite of the closed and confidential nature of the relationship between a client and their attorney We have been very fortunate in that out relationships with local law enforcement those in the legal community, and those in the business community, have allowed us to have a unique perspective on and access to information and situations that allows us to get involved with public information relating to a few cases per year. For those without such relationships, a bit more effort can bring similar results from the public records or from information published by individuals in specific cases For example in one case we are using in classes, we were pointed to a public release of formerly sealed court papers including an indictment and the supporting information from a search warrant This provides the sort of material that makes for a realistic homework assignment for some classes Another approach we have taken is to create our own cases Of course this is problematic in many ways because in order to get a real case with real evidence you have to commit a real crime. The quality of your simulation drives the realism of the digital forensic evidence the investigation process and the results produced. Another problem in this space is that real cases may take years to evolve while in graduate classes, we have to get something completed within a semester, or in our case, a quarter. While patience is a virtue, our students need a high volume of experience in a short period of time. So unlike real matters where you might drill down for weeks or longer on a specific matter, our students often take small fragments of large complex cases and drill down into them and then move on to the next matter. This is a reasonably good simulation of what law enforcement does because of the high volume of cases they process per unit time 3.3.Arranging practical internships Practical internships are one of the keys to success in a program such as the one we are forming and the program that it emerged from at the University of New Haven While many internship programs end up placing students in work environments related to their fields many such students end up not doing work similar to what they expect from their field of interest In digital forensics an intern without a clearance cannot work on national security-related forensics, and unless they sign non-disclosure agreements and are adequately trusted by the hiring party they cannot reasonably work on almost any case that can be identified. As a student, they cannot get involved in the aspects of cases requiring testimony because among other things, they are expecting to graduate and move on, legal matters often have delays of months or years and things change on a moment's notice as a case is closed or settled A best-case scenario for many programs is for a student to get an internship in a local crime laboratory working on digital forensic evidence related to cases but on fairly standard cases where one after another piece of evidence is treated in a uniform manner While this is excellent training for a student expecting to get a Masters degree and start to work after graduation in a crime lab and helps to build up relationship for both the university and its students there are only so many crime lab jobs near any given location and this sort of internship at the doctorate level is not as useful and does not use the full set of skills that would be desired for dealing with legal matters won't likely produce opportunities to review live legal cases, and provides only limited insight One of the approaches we have been trying to get to work better is in the development of long-term relationships with local attorneys, prosecutors offices and national security facilities that deal with the more complex issues in cases, and for whom limited reviews Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


can be helpful. As an example, the language used in many legal processes ends up being copied from previous cases by individuals who either don't know enough about the details of digital systems to do it themselves or don't have the time to do a custom document for each case If a police officer needs a search warrant to get evidence from computers, they cannot really be expected to have a doctorate in digital forensic evidence, and the prosecutor's office may have similar limitations. Projects that serve the community as a whole may be effective here. For example, taking the sorts of paragraphs in use today, you might create an internship program in which the students develop standard paragraphs for describing standard situations from a legal perspective with regard to digital forensic evidence. These can then be fused into libraries that make the processes more accurate and more efficient for the legal community as a whole The university benefits from the good relationship the students benefit from the experience, the faculty gets engaged in verifying the results of the effort, and the legal and law enforcement community benefits from having a more sound basis for legal process and better precedents over time We were very fortunate in building these sorts of relationships at the University of New Haven because of the efforts of many of our faculty and the support of our administration of this effort, and this has translated well into the new programs we are now creating 3.4.Mock trials and similar legal processes Mock trials and other similar legal process simulations are important and very useful to integrating the knowledge acquired within the program into practical use. Of course for students who have never seen a courtroom or had any experience on a witness stand or in a legal proceeding, almost any simulation even with a faculty member acting as judge might seem like a novel experience.  Many of our students are mid-career professionals who have had some of this experience already. For them, the learning comes from seeing how different approaches are challenged and succeed or fail, and take things to a deeper level particularly for new situations that are emerging Substantially meaningful mock trials take considerable time and effort to coordinate and execute In order for this to be effective, students and faculty must be working on the cases for months in advance This starts with introducing the cases to the students making the evidence available for review having students do the analysis and write expert reports taking depositions from the students based on those reports having students review other student depositions and reports and prepare counter-reports and counter-arguments preparation of exhibits for presentation at trial and setting the order of presentation for the date of trial. This is done as part of the course work in the quarter in which the mock trials are to be held and these trials acts as the capstone experience for the year of study. For this reason, it is our goal to have one week of trials per year, in which judges and lawyers are brought in to try the cases, all of the pending cases are presented to juries and decisions are rendered. The court reporters' transcripts are then used to review the trial efforts with the students and become part of the permanent record for use in future studies and mock trials By combining mock trials between the students in the Masters program and the Ph.D. program, redundant effort can be saved, and Ph.D. students can take the stand after Masters students to demonstrate how taking the science to the next level can help to make or break a case. If each of 25 students is to gain experience in testifying it will take several days to get through a normal trial process Jury are empaneled from the student body and the faculty It is advantageous if there are several relatively small and simple cases rather than one large case, because this reduces complexity and allows those who are on one side or another in one case to be jurors in another case These trials are designed to have only expert witnesses, no jury selection process \(usually volunteers from the audience\ and lawyers with limited roles and almost no preparation time Juries don't get to deliberate for long but they do get to vote on the outcomes with majority rule instead of total agreement Specific issues are brought up across the period of the trials like commonly used trick questions and questioning techniques. There are improper questions with objections surprise pieces of evidence for the witnesses and other similar things to keep the trial lively, but these are also designed to have educational value For example if a student makes a mistake at trial, it may be identified for discussion after the trial when the transcripts are reviewed We envision participation by local judges and prosecutors and we hope that they can also gain experience with specific types of evidence they are likely to see in legal cases through these mock trials This prepares all of us to better understand the issues at Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


trial, to better understand the challenges and testimony and it helps build relationships. This also helps to open opportunities for students and faculty and has the potential to engage local law schools in joint collaborative efforts 4 Summary and conclusions The Ph.D program and curriculum is designed to move from the level of knowledge typically available in legal matters today, to a level comparable to what is expected in other scientific fields It does this by extending both toward the underlying physics and details of how digital systems operate and toward more in-depth examination of and experience with legal processes It is intended to provide the  seed corn required to meet future needs in both education and at trial, and is highly cooperative with the local legal and investigative communities while still meeting the academic standards common to doctorate programs in other fields This program is still under development and the authors welcome feedback collaboration and participation from and with the community. For those wishing to find out more, please contact the authors at California Sciences Institute 5 References 1 Dau b ert v Merrell Dow  P h a r m ace u t ical s   In c 509 US 579 125 L Ed 2d 469 113 S Ct 2786 1993 2 Marsh a ll Un iv er sity s dig ital foren s ics specialization http://forensics.marshall.edu/MSDegree/MSDegreeComputer.html 3 T a y l o r  En d i co tt-P o p o v sk y   P h illip s  Fo ren s ic s Education: Assessment and Measures of Excellence Proceedings of the Second International Workshop on Systematic Approaches to Digital Forensic Engineering \(SADFE'07\ 0-7695-2808-2/07 $20.00 2007, IEEE Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


 9 correlated to failure times.  For example, this would imply that operation environment parameters such as temperature have no effects on the failure rates of devices.  The covariates regression approach introduced in this section eliminate this restriction.  Th e basic problem can be formed as follows: given survival time T 0, a vector     2 1 s z z z z of covariates that may influence T is observed.  Furthermore, the vector z itself can also be time dependent and may include bo th quantitative and qualitative indicative variables. In this approach, Cox's \(1972 proportional hazards model was initially treated as largely an empirical regression model, but later it was found that the framework of the model posse sses exceeding flexibility to capture major hazards effects and failure mechanisms.  The Cox model has been extended numerously and it forms the foundation for the covariates regression approach \(Therneau and Grambsch 2000  There are two ways to construct covariates regression models. The first is to extend survivor distributions such as the exponential and Weibull dist ribution, and this approach forms so-called mixture models. The second approach has no assumptions on the survival distribution, also called distribution-free approach, which is the approach adopted by Cox \(1972, 1975\. We briefly introduce the first approach by using exponential and Weibull distributions as examples. The approach can be extended to other distributions, including discre te distributions \(Kalbfleisch and Prentice 1980, 2002  2.5.1. Distribution-Dependent Regression Models Parametric Regression Models  With exponential distribution, the hazard rate is constant The hazard rate at time t for an individual with covariates z  can be written as    t  z   z  31  i.e, the hazard for a given vector z is constant; however, it varies with covariates z The  z ay be modeled in many ways.  In the simplest case it assumes a linear relationship  z and thus   t  z  g  z  32  where is the vector of regression coefficients for vector z and z is the  inner product of the two vectors.  Here g is a function form, for example   g  x  exp x  33  With g ing on an exponential form — the exponential distribution models under covariates z the hazard function is  t  z  exp z  34  The conditional pdf of T   given z is   f  t  z  e z exp te z  35  The above hazard function implies that the log failure rate is a linear function of covariates z Let Y ln T     ln   and W be a random variable with extreme value distribution  the log of the exponential failure time T nder covariates z  can be expressed as   Y z W  36  This is a log-linear model with many nice properties  Similar to the previous extens ion, Weibull distribution can be extended with covariates regression.  The conditional hazard for Weibull distribution is    t  z   t  1 e z  37  where  represent the covariate regression vector to avoid the confusion with the parameter of Weibull distribution The conditional pdf for Weib ull distribution is then     exp      1 z z e t e t z t f  38  The effects of covariates also act multiplicatively on the hazard in the Weibull distribution.  In terms of the log  failure time, the above conditi onal hazard function specifies the log-linear model. With Y ln T    ln     1 and   we get the log-linear Weibull model  W z Y   39  where W follows the extreme value distribution.  The above extensions of exponential and Weibull distributions demonstrate two important properties \(Lawless 2003 Kalbfleisch and Prentice 2002\. First, the effects of covariates act multiplicatively on the hazard function This general relationship inspired the general proportional hazards model.  Second, the lo g-linear models suggest that the covariates act additively on the log failure time This inspired the development of the accelerated failure time model.  Both models are described briefly below  2.5.2. Distribution-Free Regression Models Semi-parametric Regression Models  The next two subsections introduce two of the most important survival analysis models.  They do not assume any specific statistical distribu tions, and are therefore called distribution-free or semi-parametric approaches.  Both models have been extended numerously.  The Cox proportional hazard model is probably the most influential one in the entire survival analysis subject  Cox's Proportional Hazards Model \(PHM  Let  t  z  be the hazard function at time t for an individual with 


 10 covariates z The proportional hazards model \(PHM\ first proposed by Cox \(1972, 1975\, is    t  z  0  t  e z   40  where 0  t  is an arbitrary unspecified base-line hazard function for continuous failure time T In the PHM, the covariates act multiplicativel y on the hazard function.  By substituting 0  t  with the corresponding hazard function for exponential or Weibull distributions, the previous distribution-dependent models for exponential or Weibull distribution can be derived as special cases  The conditional pdf of T given z corresponding to the general hazard function    z t is     exp      0 0 0 t z z du u e e t z t f  41  The conditional surv ival function for T under z is   S  t  z   S 0  t  exp z   42  where     exp   0 0 0 t du u t S 43  The allowance of an arbitrary 0  t  makes the model sufficiently flexible for many applications  Extensions of Cox's Proportional Hazards Model PHM There are numerous extensions to Cox's PHM Therneau and Grambsch 2000\. Among those extensions two of the extensions strata and time-dependent covariates  are particularly important, but do not substantially complicate the parameters estimation \(Kalbfleisch and Prentice 2002, Lawless 2003   Suppose there is a factor that occurs on q levels and for which the proportionality assumption of PHM may be violated.  The hazard function for an individual in the jth stratum or level of this factor is    exp      0 z t z t j j 44 for j 1,2 q  where z is the vector of covariates for PHM The baseline hazard functions 01  0 q  for the q  strata are permitted to be arbitrary and are completely unrelated.  The direct product of the group \(stratum likelihood can be utilized to estimate the common parameter vector  Once the parameter is estimated, the survivor function for each stratum can be estimated separately  The second generalization to the proportional hazards model is to let the variable z depend on time itself.  For unstratified PHM, the hazard function is     exp       0 t z t t z t  45  and for stratified PHM it is      exp       0 t z t t z t j j     2  1 r j 46  The procedure used to estimate the is to maximize the socalled partial likelihood functions as described by Cox 1975\ and Kalbfleisch & Prentice \(1980, 2002  Time-dependent covariates may be classified into two broad categories: external and intern al \(Kalbfleisch and Prentice 2002, Lawless 2003, Klein and Moeschberger. 2003\. The failure mechanism does not directly involve external covariates. External covariates may be distinguished into \(1 fixed measured in advance and fixed during the observation\, \(2 defined determined in advance but not fixed, for example, contro lled stress levels\nd \(3 ancillary output of a stochastic process external to an observed object\n internal covariate is attached to the individual, and its existence depends on the survival of the individual. It is the output of a stochastic process that is induced by the individual under study.  The process is only observable if the individual survives and is uncensored   The Accelerated Failure Time \(AFT  The PHM represents the mul tiplicative influences of covariates z n the hazard.  However, without specific   0 t this model does not tell how z affects failure time itself. The previous log-linear models for T answer this question   Suppose that Y ln T  is related to the covariance z in a linear model  Y z W  47  where is a constant or intercept parameter z is a vector of covariates is a vector of parameters to be estimated is a scale parameter which is the reciprocal of the shape parameter, and W is a random variable with a specified distribution.  Many distributions, including the Weibull exponential, log-norma l, and log-logistic  can be used to describe W   Exponentiation of Y gives    exp T z T  48 where 0  exp  W T has a hazard function 0  t  that is independent of It follows that the hazard function for T  can be written in terms of this baseline hazard  0 as    exp  exp     0 z z t z t 49  The survival function is  S  t  z  exp 0  u  du 0 t exp z   50 and the pdf  is the product of    z t and S  t  z  


 11 This model specifies that covariates act multiplicatively on time t r than on the hazar d function.  That is, we assume a baseline hazard function to exist and that the effects of the covariates are to alter the rate at which an individual proceeds along the time axis.  In other words, the covariates z accelerates or decelerates the time to failure Kalbfleisch and Prentice 2002, Lawless 2003  It should be pointed out that the distribution-based regression models for exponential and Weibull distributions in the previous section are th e special cases of both PHM and AFT.  This correspondence is not necessarily true for models based on other distribu tions. Indeed, two-parameter Weibull distribution has the uniq ue property that it is closed under both multiplication of fa ilure time and multiplication of the hazard function by an arbitrary nonzero constant Lawless 2003, Kalbfleisch & Prentice 2002, Klein Moeschberger 2003  2.6. Counting Process and Survival Analysis   In the previous sections, we introduced censoring and survival analysis models th at can handle the censored information; however, we did not discuss how the censored information is processed.  Accommodating and maximally utilizing the partial information from the censored observations is the most challenging and also the most rewarding task in survival anal ysis.  This also establishes survival analysis as a unique fiel d in mathematical statistics Early statistical inferences for censored data in survival analysis were dependent on asymptotic likelihood theory Severini 2000\ Cox \(1972, 1975\ proposed partial likelihood as an extension to classical maximum likelihood estimation in the context of hi s proportional hazards model as a major contribution. Asymptotic likelihood has been and still is the dominant theory for developing survival analysis inference and hypothesis testing methods \(Klein and Moeschberger 2003, Severini 2000\. There are many monographs and textbooks of survival analysis containing sufficient details for applying survival analysis \(Cox and Oakes 1984, Kalbfleisch and Prentice 1980, 2002, Lawless 1982, 2003, Klein and Moeschberger, 2003\. A problem with traditional asymptotic lik elihood theory is that the resulting procedures can become very complicated when handling more complex censoring mechanisms \(Klein Moeschberger 2003\. A more elegant but requiring rigorous measure-theoretic probability theo ry is the approach with counting stochastic processes and the Martingale central limit theorems.  Indeed, this approach was used by Aalen 1975\ to set the rigorous mathematical foundation for survival analysis, and later further developed and summarized by Fleming and Harrington \(1991\, Andersen et al. \(1993\several research papers.  In reliability theory Aven and Jensen \(1999\ dem onstrated such an approach by developing a general failure model, which we briefly introduced in Section 1.2. However, the counting process and Martingale approach require measure theoretic treatments of probability and st ochastic processes, which is often not used in engineering or applied statistics.  A detailed introduction of the topic is obviously beyond the scope of this paper, and we only present a brief sketch of the most important concepts involved.  Readers are referred to the excellent monographs by Andersen et al. \(1993 Fleming and Harrington \(1991\ Aven and Jensen \(1999\ for comprehensive details, and Kal bfleisch and Prentice \(2002 Klein and Moeschberger \(2003\, Lawless \(2003\ for more application–oriented treatments The following discussion on this topic is drawn from Klein and Moeschberger \(2003  A counting stochastic process N  t  t 0 possesses the properties that N  0 ro and N  t   with probability one. The sample paths of N  t ht continuous and piecewise constant with jumps of size 1  step function In a right-censored sample, \(we assume only right censoring in this section N i  t  I  T i t  i   which keep the value 0 until individual i fails and then jump to 1  are counting processes. The accumulation of N i  t ocess     1 t N t N n i i is again a counting process, which counts the number of failures in the sample at or before time t   The counting process keeps track of the information on the occurrences of events,   for instance, the history information such as which individual was censored prior to time t and which individual died at or prior to time t as well as the covariates information This accumulated history information of the counting process at time t is termed filtration at time t denoted by F t For a given problem F t  rests on the observer of the counting process.  Thus, two observers with different recordings at different times will get different filtrations.  This is what Aven and Jensen 1999\ referred to as different information levels or the amount of actual available information about the state of a system may vary  If the failure times X i and censoring times C i  are independent,  then the probability of an event occurs at time t given the history just prior to t  F t\n be expressed as  t T if dt t h t C t X dt t C dt t X t P F dt t T t P i i i i i i r t i i r          1     t T if F dt t T t P i t i i r 0   1    51  Let dN  t be the change in the process N  t over a short time interval    t t t Ignoring the neglig ible chance of ties 1   t dN if a failure occurred and 0   t dN otherwise  Let Y  t denote the number of individuals with an observation time T i t Then the conditional expectation of dN  t   dt t h t Y F dt t C dt t X t with ns observatio of number E F t dN E t i i i t              52 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es — the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


