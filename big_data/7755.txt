Ad-Hoc Association-Rule Mining within the Data Warehouse   Svetlozar Nestorov Department of Computer Science The University of Chicago Chicago, IL, USA evtimov@cs.uchicago.edu    Nenad Juki  School of Business Administration Loyola University Chicago Chicago, IL, USA njukic@luc.edu    Abstract  Many organizations often underutilize their already constructed data warehouses.  In this paper, we suggest a novel way of acquiring more information from corporate 
data warehouses without the complications and drawbacks of deploying additional software systems Association-rule mining, which captures co-occurrence patterns within data, has attracted considerable efforts from data warehousing researchers and practitioners alike.  Unfortunately, most data mining tools are loosely coupled, at best, with the data warehouse repository Furthermore, these tools can often find association rules only within the main fact table of the data warehouse thus ignoring the information-rich dimensions of the star schema\ and are not easily applied on non-transaction level data often found in data warehouses.  In this paper we present a new data-mining framework that is tightly 
integrated with the data warehousing technology.  Our framework has several advantages over the use of separate data mining tools.  First, the data stays at the data warehouse, and thus the management of security and privacy issues is greatly reduced.  Second, we utilize the query processing power of a data warehouse itself without using a separate data-mining tool.  In addition this framework allows ad-hoc data mining queries over the whole data warehouse, not just over a transformed portion of the data that is required when a standard datamining tool is used.  Finally, this framework also expands the domain of association-rule mining from transactionlevel data to aggregated data as well  
 1. Introduction  Data warehousing has become a standard practice for most large companies worldwide.  The data stored in the data warehouse captures many different aspects of the business process such as manufacturing, distribution sales, and marketing.  This data reflects explicitly and implicitly customer patterns and trends, business practices, strategies, know-how and other characteristics Therefore, this data is of vital importance to the success of the business whose state it captures, which is why companies choose to engage in the relatively expensive undertaking of creating and maintaining the data 
warehouse \(a recent study  rt s t h e m e di a n cos t of  5 million for creating a data warehouse with additional 0.5 million for annual operating cost While some information and facts can be gleaned from the data warehouse directly, much more remains hidden as implicit patterns and trends.  The discovery of such information often yields important insights into the business and its customers and may lead to unlocking hidden potentials by devising innovative strategies.  The discoveries go beyond the standard on-line analytical 
processing \(OLAP\hich mostly serves reporting purposes \(albeit in an increasingly complex and sophisticated manner One of the most important and successful methods for finding new patterns is association-rule mining Typically, if an organization wants to employ associationrule mining on their data warehouse data, it has to acquire a separate data mining tool.  Before the analysis is to be performed, the data must be retrieved from the database repository that stores the data warehouse, which is often a cumbersome and time-consuming process.  The vendors 
of data management software are becoming aware of the need for integration of data mining capabilities into database engines, and some companies are already allowing for tighter integration of their database and data mining software \(e.g. IBM\222s DB2 Database and Intelligent Miner or NCR\222s Teradata Database and Teradata Warehouse Miner ile t h is does  represent an improvement, it still does not eliminate the need for additional software.  In this paper we describe a direct approach to association-rule data mining within data warehouses that utilizes the query processing power of the data warehouse itself without using a separate data 
mining tool.  In addition, this approach is capable of answering a variety of questions based on the entire set of Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


 data stored in the data warehouse, which is in contrast to regular association-rule mining that is limited to portions of the data warehouse.  Another limitation of regular association-rule mining is that it requires transaction-level data, even though much of the data stored in businessrelated data warehouses is in aggregated form.  In this paper, in addition to presenting a direct approach to association-rule data mining of entire data warehouses we present a methodology for association-rule mining of aggregated data The remainder of this paper is organized as follows Section 2 gives a brief overview of association-rule mining and illustrates its applicability in a data warehouse environment; Section 3 introduces the new approach of Extended Association-Rule Mining; Section 4 discusses the issue of association-rules mining from aggregate \(nontransaction level\ata in data warehouses; Section 5 introduces the implementation architecture for Extended Association-Rule Mining; and Section 6 describes the results of an experimental study.  Finally, Section 7 gives the conclusions and indicates the future work  2. Association-Rule Data Mining in Data Warehouses  Dimensional modeling [8 w h ic h is t h e m o st prevalent technique for modeling data warehouses organizes tables into fact tables containing basic quantitative measurements of a business subject and dimension tables that provide descriptions of the facts being stored.  The data model that is produced by this method is known as a star-schema 5   o r a st a r sc he m a  extension such as snowflake or constellation igure  shows a simple star-schema model of a data warehouse for a retail company   LOCATION DIMENSION LocationID Store Region Territory SALES FACT TABLE ProductID LocationID CalendarID CustomerID Quantity Sale Amount Time of Sale CALENDAR DIMENSION CalendarID Day Week Month Quarter Year PRODUCT DIMENSION ProductID Product Group Subcategory Category CUSTOMER DIMENSION CustomerID Name Gender Zip City State   Figure 1.  Example Star-Schema  The fact table contains the sale figures for each sale transaction and the foreign keys that connect it to the four dimensions: Product, Customer, Location, and Calendar Standard association-rule mining   c ov ers  correlations among items within transactions \(the prototypical example of utilizing association-rule mining is determining what things are found together in a basket at a checkout line at the supermarket; hence the oftenused term: market basket analysis h e cor r elation s  are expressed in the following form Transactions that contain X are likely to contain Y as well Letters X and Y represent sets of items.  There are two important quantities measured for every association rule support and confidence The support is the fraction of transactions that contain both X and Y.  The confidence is the fraction of transactions containing X, which also contain Y.  Intuitively, the support measures the significance of the rule, so we are interested in rules with relatively high support.  The confidence measures the strength of the correlation, so rules with low confidence are not meaningful.  In practice, the discovery of association rules has two phases.  In the first phase, all sets of items with high support \(often called frequent itemsets covered.  In the second phase, the association rules among the frequent itemsets with high confidence are constructed.  Since the computational cost of the first phase dominates the total computational cost  s o ci at i o n r u l e  m i ni ng i s of t e n defi n e d as  t h e  following question over transactional data, where each transaction contains a set of items What items appear frequently together in transactions A typical data warehouse, however, has multiple dimensions, which are completely ignored by the above single-dimension question.  Consider the data warehouse depicted by Figure The standard association-rule mining question for this environment would be What products are frequently bought together This question examines the fact table as it relates to the product dimension only.  An analyst at the corporate headquarters may ask the following question What products are frequently bought together in a particular region and at a particular month This question examines multiple dimensions of the fact table.  Unfortunately, standard association-rule mining ARM\orithms cannot find the answer to this question directly.  In fact, ARM may not discover any association rules in situations when there are several meaningful associations that involve multiple dimensions.  The following example illustrates an extreme case scenario of this situation Example 1 A major retail chain operates stores in two regions South and North and divides the year into two Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


 seasons Winter and Summer The retail chain sells hundreds of different products including sleeping bags and tents.  Table shows the number of transactions for each region in each season as well as the percentage of transactions that involved a sleeping bag, a tent, or both  Table 1   North South  Total x 1000  Bags  Tents  Both Total x 1000  Bags  Tents  Both W 200 4 0 00 2 4  S 00 2 4 2 200 4 2   Suppose we are looking for association rules with  support and 50% confidence. Let's first consider the transactions in all regions and all seasons.  There are 600 thousand transactions and only 5 thousand of them involve both a sleeping bag and a tent, so the support of the pair of items is .83% \(less than Thus, no association rule involving sleeping bags and tents will be discovered. Let's consider the transactions in the North and the South regions separately.  There are 300 thousand transactions in each region.  In the North sleeping bags and tents appear together in 2 thousand transactions, so their support is .66%.  Thus, no association rule involving both of them will be discovered.  In the South, sleeping bags and tents appear together in 3 thousand transactions so their support is The confidence of sleeping bag 001 001\001 001 tent is 30% and the confidence of tent 001 001\001 001 sleeping bag is 37.5% \(both less than 50%\o no rules involving both sleeping bag and tent will be discovered in the South either.  Similarly, no associations will be discovered between sleeping bags and tents when we consider all the transactions in each season separately.  These conclusions no association rules\ are rather surprising because if each region and season were considered separately the following association rules would be discovered In the North during the summer sleeping bag 001 001\001 001 tent  sup=2%, conf 00 In the North during the summer tent  001 001\001 001 sleeping bag  sup conf=50 In the South during the winter sleeping bag 001 001\001 001 tent  sup conf=50 Standard ARM cannot discover these association rules directly.  Instead, the data need to be preprocessed or transformed in one of the following ways Solution 1 Expand the definition of an item.  Define an item to be not just a product, but a region or a season as well.  Each transaction \(basket\tains the products as well as two additional items: the season when the purchase was made and the region where the store is located.  Now we can apply any standard ARM algorithm and filter out all rules that do not contain a season, a region, and at least two products There are obvious problems with this approach.  The initial number of rules generated by the algorithm will be several orders of magnitude larger than the number of rules of the required form.  Consequently, the amount of time and computational resources spent to eliminate spurious rules will be rather excessive Solution 2 Partition the data into several subsets according to the season and the region of the transaction Apply ARM to each of the subsets The drawback of this approach is the need for partitioning of the data warehouse every time a question is asked.  Each subsequent new request will require a pass over all the data in order to re-partition the data warehouse.  Thus, this approach will also require an excessive amount of time and computational resources Solution 3 Redefine the definition of an item.  Define an item to be a triple consisting of a product, a region, and a season.  Apply ARM the new market basket data While this approach does not require filtering out spurious rules or partitioning the data warehouse, it still has a major disadvantage of requiring an additional step creating the redefined items \226 triples in this case\which again requires significant additional time and computational resources, due to the explosion of the number of different items As illustrated by the above example-solutions, regular ARM based approaches for finding answers to questions involving multiple dimensions can overwhelm the system with requests for resources.  In the following section, we describe a different and more feasible approach  3.  Extended Association Rules  This section introduces a new concept of extended association rules.  Standard association rules can express correlations between values of a single dimension of the star schema.  However, as illustrated in Example values of the other dimensions may also be correlated Furthermore, some associations become evident only when multiple dimensions are involved.  In Example  sleeping bags and tents appear uncorrelated in the sales data as a whole, or even when the data is considered separately by region or season.  Yet, several association rules are discovered if the focus is on the data for a particular region during a particular season.  One such rule is sleeping bag 001 001\001 001 tent for the transactions that occurred in the North region during the Summer season This association rule involves more than just a pair of Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 001 001\001 001 sleeping bag sup=2%, conf=50 In the South during the summer tent 


 products; it also involves the location as well as the time of the transaction.  In order to capture this additional information, we augment the association rule notation as follows sleeping bag 001 001\001 001 tent region=north, season=summer Formally, we define extended association rules as follows Definition Let T be a data warehouse where X and Y sets of values of the item-dimension.  Let Z be a set of equalities assigning values to attributes of other \(nonitem\ dimensions of T.  Then X 001 001\001 001 Y \(Z  is an extended association rule with the following interpretation Transactions that satisfy Z and contain X are likely to contain Y While extended association rules are natural extensions of standard association rules, they contain more detailed and diverse information that offer several distinct advantages First, the phenomenon that underlines an extended rule can be explained more easily.  For example if we change the percentages shown in the last column of Table form to 2%, standard ARM would find the rule tent 001 001\001 001  sleeping bag within the specified confidence and support threshold and 50%\or all sales.  However, only the extended rules would give the explanation that the correlation of sales between tents and sleeping bags holds exclusively in warmer weather conditions \(because all found extended rules would contain either south region or summer season Second, extended rules tend to be local, in a sense of location or time, and thus more actionable.  For example it is much simpler to devise a marketing strategy for sleeping bags and tents that applies to a particular region during the a particular season then a general strategy that applies to all regions during all seasons Third, for the same support threshold, the number of discovered extended association rules is likely to be much less that the number of standard rules.  The reason is that extended rules involve more attributes and thus transactions that support them fit a more detailed pattern than standard rules.  Even though the overall number of extended rules is likely to be much less than the number of regular rules, there may be some items that are involved only in extended rules \(as shown in Example  Intuitively, standard rules show associations of a coarser granularity while extended rules show finer associations Augmenting standard association rules has previously been proposed in  T h i s appr oach appl i e s at t ri b u t eoriented generalization algorithm to characteristic attributes of discovered association rules.  Our approach differs from [6 t h at w e i n v o lv e all attrib u t es n o t j u st items\ finding the frequent itemsets.  Thus, there are more opportunities for optimizations \(see Section 5.2\d the discovered rules have finer-granularity Before we describe the mining process for extended association rules, let us recall standard association-rule mining.  Given a set of transactions, and support and confidence thresholds, it discovers association rules among items that have support and confidence higher than the chosen thresholds.  With this setting, the only parameters that the user controls directly are the support and confidence thresholds.  Very often the result of the mining is one of two extremes.  The number of discovered rules is either overwhelmingly large \(tens of thousands\ery small \(dozens  both cas es th e user has no other recourse but to revise the support and confidence and run the mining process again \(as a postprocessing step, the user can choose to focus on certain items but that goes against the notion of mining; in fact given the unlimited amount of time, the user may very well compute the support and confidence of the items directly using OLAP Since the mining process often takes hours 0  most users are not likely to continually refine the thresholds Thus, the mining process is often seen as not particularly useful to non-technical users In contrast, the process of mining extended association rules requires more input from the user and thus affords more control and lends itself to customization.  Since extended association rules involve attributes of non-item dimensions, the user needs to specify these attributes.  For example, the user can ask for association rules that involve a region and a season in addition to products Note that the user does not need to specify the particular values of the attributes.  The actual values are discovered as part of the mining.  This mining process differs from the one-size-fits-all approach for standard association rules.  Different users can ask mining questions involving different attributes of the dimensional hierarchies.  For example, a marketing executive who is interested in setting up a targeted marketing campaign can ask the following Find products bought frequently together by customers of a particular gender in a particular region A sample association rule discovered by this question may be ice cream 001 001\001 001 cereal \(region=California gender=female  Discovering that ice cream and cereal sell well together in California with female customers can facilitate creating a more focused and effective marketing campaign that is at the same time less costly \(e.g. adds for ice cream and cereal can appear on consecutive pages of a local female-audience oriented magazine On the other hand, a vice president in charge of distribution may ask the following Find products bought frequently together in a particular city at a particular month Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


 A sample association rule discovered by this question may be sleeping bag 001 001\001 001 tent \(city=Oakland month=May Such a rule is of more value for managing and planning the supply-chain than the equivalent standard association rule \(e.g. it can indicate that the shipping of sleeping bags and tents to the certain stores should be synchronized, especially during certain time periods In Section 5 we will describe the implementation details of Extended Association Rule Mining.  However before that, in the following section we will examine yet another way of finding out more information from data warehouses  4. Mining from Aggregated Data  Standard association-rule mining works with transactionlevel data that reflects individual purchases.  However the industry practice for many large corporations is to keep such data in the data warehouse only within a certain limited time horizon \(e.g. 60 days quarter, etc Afterwards transaction-level data is stored off-line on a medium suited for bulk management of archival data such as magnetic tape or optical disk u ch dat a i s  still available electronically but it is not on-line because it does not reside in the data warehouse.  The data warehouse continues to store summarizations of the transaction-level data; for example data aggregated by day.  Figure 2 shows the schema of the data warehouse from Figure with the product sales data aggregated by day for each store   LOCATION DIMENSION LocationID Store Region Territory SALES FACT TABLE ProductID LocationID CalendarID Quantity \(Sum Sale Amount \(Sum Number of Sales CALENDAR DIMENSION CalendarID Day Week Month Quarter Year PRODUCT DIMENSION ProductID Product Group Subcategory Category   Figure 2.  Example Star-Schema for Aggregated Data  This situation is not pertinent for most association-rule mining tools \(if the transaction-level data is available on off-line media\e the tools do not use the query processing capabilities of the data warehouse.  Instead, as we mentioned in introductory part of the paper, most such tools require loading of the data \(which can be an especially slow process with off-line data\ur approach however, is to leverage the query-processing power of the database management system \(DBMS\.  Thus, we need to define the concept of association rules on aggregated data First, let\222s consider the granularity of the data in the data warehouse.  Aggregated data is created by rolling up the transaction-level data by one or more attributes.  One of the most common cases is aggregating data by some measure of time.  In the example case shown in Figure 2 the, data contains information about the daily sales of items instead of individual purchases.  Thus, the base unit in the data warehouse is no longer an individual transaction but rather the daily sales at a store Next, in order to define association rules for aggregated data we need to re-examine the meaning of together in questions of type Find products bought frequently together Note that we can find out the total number of times each item appears in transactions.  However, we no longer know the exact number of transactions that contained two or more particular items from the daily sales data.  We can approach this problem in several different ways Approach 1 The approach that most closely follows the original definition of association rules is to assume that a certain fraction \(percentage\ all possible transactions at each store during each time period \(e.g. day, if the data is aggregated by day\ contain the two items.  A reasonable approach is to take f * min\(Sx,Sy where Sx and Sy are the total sales for items X and Y on a given day in a give store.  The actual selected value of fraction f can even be somewhat arbitrary as long as it is constant for all stores and days.  In fact, we can incorporate the chosen value of f into the support measure since the support inequality will be   f * min\(Sx, Sy\\ >= support threshold  store, day  So if we adjust the support threshold accordingly \(divide by f we can get the results without changing the association rule algorithm The above-described approach \(Approach based on approximating togetherness.  A different strategy involves redefining the meaning of together where we instead of looking for pairs of items that are sold during the same transaction, look for pairs of items that are sold on the same occasion In a case when the data is aggregated by day, an occasion represents a day in a store The information that a store sells many items of X and Y  on the same day, even if the items are not sold together in Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


 every instance, is quite meaningful and useful \(e.g. for improving the distribution and shipping process Approach 2 One possible occasion-based approach involves breaking the support into two parts to better reflect the granularity of the mined data.  Intuitively, we are looking for pairs of items that sell well together on many occasions.  The meaning of "sell well together" is that both items must have been sold more than a certain threshold at the same store on the same day.  Formally, we define the condition as   1 if  min\(Sx, Sy\ >= sales threshold, else 0 occasion store, day       threshold  In principle, we first find, for every pair of items, all daystore pairs \(corresponding to occasions\for which the items were both sold more than the sales threshold.  Then we count the number of such pairs, and if the number exceeds the occasion threshold we add the pair to the discovered rules.  For example, a pair of items is bought frequently if there are at least 00 different occasions  occasion threshold = 100 on which both items are bought separately in more than 0 transactions sales threshold = 10  Approach 3 Another approach is to split the support condition into three parts.  Intuitively, we are looking for pairs of items that sell well together on many occasions in many stores Formally, we have the following three conditions Let O be the set of occasions such that min\(Sx, Sy  sales threshold  Let P be the set of stores such that the number of occasions in O for each store day threshold Consider X and Y frequent only if the number of stores in P is store threshold  For example, find all pairs of items such that they are both sold more than 0 times sales threshold = 10  0 stores store threshold = 10 n 0 different days day threshold = 10 that this approach is a specialization of the occasion-based strategy.  In other words, any pair of items that are bought frequently, based on this definition, will also be considered bought frequently according to the occasion definition.  In practice, the choice of approach will be based on its relevance for the organization and user and its actionability After defining the association rules for aggregate data we can define the extended rules in the same way as we did with transaction-level data.  For example, if we are looking for items sold together in a particular month we can restate the previous example as follows:  Find all pairs of items such that they are both sold more than 0 times in 0 stores on 0 different days of the same month In the last two sections we discussed new approaches to association-rule mining of data warehouses.  The following section will discuss the implementation of these approaches within an operational system  5. System Architecture and Implementation  In this section we describe our implementation of extended association rules within the data warehouse This implementation is granularity-independent as it can accommodate both transaction and non-transaction level data.  The basic architecture of our system is shown in Figure 3   Data Warehouse  External Optimizer very thin client  GUI  SQL queries Final Results Mining Refinement Hints O     D     B     C Parameters Attributes & Thresholds Final results Intermediate results size      Figure 3.  System Architecture for the Ad-Hoc Data Mining System   The most notable feature of the system architecture is the tightly coupled integration with the relational database that powers the data warehouse.  The benefits of this integration are threefold  No data leaves the data warehouse so we reduce the redundancy and avoid any privacy, security, and confidentiality issues related to data movement 2 The relational database does all query processing, so we leverage the computational and storage resources of the data warehouse 3 Extended association rules can be mined from any set of tables within the relational database that is storing the data warehouse, so we enable wide-range ad-hoc mining  Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


 5.1 The Mining Process  The mining process starts with the user defining the extended association rule by choosing the non-item attributes to be involved \(the system can accommodate regular ARM as well - if a user chooses zero non-item attributes, regular ARM will be performed The choice can be made through a simple interface with pull down menus for each dimension.  In cases when non-transaction level data is being mined, the user can also choose an appropriate function for implementing 223frequently together\224  The default is Approach  presented in Section 4\at mimics the original definition of standard association rules, but the user can also choose one of the other approaches presented in Section 4 Finally, the user also specifies the support threshold and possibly other threshold arising from the choice of 223frequently together\224 function.\wever, our choice of architecture allows certain flexibility about choosing threshold.  Later in this section, we show how the threshold can be changed midstream Once all parameters are chosen, the next step of the mining process is the creation of a sequence of SQL queries that implements the extended association rule specified by the user\222s choice of parameter values.  There are many different SQL sequences that can implement the same rule, so the choice of the sequence is made according to an optimization algorithm \(which will be outlined in sub-section 5.2 Once the optimal sequence is selected, the relational database starts to execute its SQL queries one by one The results of each query remain in the data warehouse in temporary tables.  Only the sizes of the intermediate results are sent to the external optimizer \(as shown in Figure 3\erein lies an opportunity to make the mining process interactive.  If the size of certain intermediate results is too large then we can expect that the complete mining process will take a long time and possibly generate too many rules.  In this case, the system can alert the user and suggest revising the support threshold upwards.  Similarly, if the size of certain intermediate results is too small and we expect that the number of mined rules is going to be very small, we can advise the user to decrease the support threshold The last query of the sequence computes the discovered rules.  The rules are sent to the external optimizer, which displays them for the user The example of final result \(set of extended association rules\will be shown in Section 6 First, however, we will describe the SQL sequence optimization algorithm  5.2 Optimization Algorithm  The algorithm involves several different parameters that are database specific.  The following example illustrates the general optimization principles without going into system specific issues Example 2 Consider mining extended association rules from the data warehouse shown in Figure based on the following question Find products bought frequently together by customers from a particular zip-code in a particular month This question involves four tables, namely Product Customer, Calendar, and Sales.   Typically, the size of a fact table \(such as Sales\will be several orders of magnitude bigger than the size of any of the dimension tables.  In order to make the example concrete, suppose that the sizes and attribute cardinalities for these tables are as follows  SELECT A.ProductID, B.ProductID, Zip Month, COUNT FROM Sales A, Sales B, Customers Calendar WHERE A.CustomerID = B.CustomerID AND A.CalendarID = B.CalendarID AND Calendar.CalendarID = A.CalendarID AND Customers.CustomerID = A.CustomerID GROUP BY A.ProductID, B.ProductID, Zip Month HAVING COUNT   Figure 4: Na\357ve SQL Query  The cost of this join is likely to dominate the cost of the mining process so the optimization goal is to reduce the size of the portion of Sales before we do the self-join This optimization is crucial for the success of our tightly coupled approach.  Mining without such optimization \(by running the na\357ve query\ will be extremely slow and may require massive additional storage space for the internal Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 002\003 Table Product has 0 thousand tuples \(records 002\003 Table Customer has 0 thousand tuples within 00 different zip-codes 002\003 Table Calendar has 300 tuples within 2 different months 002\003 Table Sales has million tuples Furthermore, suppose that the support threshold is 00 Ultimately, in order to find pairs of products bought frequently together we have to join some portion of the Sales table with itself.  A na\357ve approach, writing the query directly in SQL is shown in Figure 4  


 intermediate results \(as large as the entire data warehouse Standard association rule employs the so-called apriori technique  o accom p lis h a s i m ilar opti m izatio n goal.  The crux of a-priori is the observation that a set of items can be frequent only if all proper subsets are also frequent.  Based on this observation, a-priori applies stepwise pruning to the sets of items that need to be counted starting by eliminating single items that are infrequent The a-priori technique works for association rules but we can augment it, similar to query-flocks method   accommodate the extended association rules.  Consider the following three questions Question A Find all products bought in at least 100 transactions Question B Find all products bought in at least 100 transactions in the same month Question C Find all products bought in at least 100 transactions by customers with the same zip  We can use the result of any of the three questions to reduce the size of the portion of the Sales table by semijoining Sales with the result \(semi-join of table A with table B produces all tuples of A that would participate in join of A and B.\he problem is which one of the three to choose Consider question A.  The expected number of transactions per product is 00 \(size of Sales / size of Product\ it is unlikely that many products will be eliminated by question A Consider question B.  We can expect that there will be on average less than 9 transactions that contain a particular product in a particular month \(size of Sales size of Product * number of months\o question B is likely to reduce the number of relevant Sales records by a significant factor Consider question C.  The expected number of transactions per product, zip-code combination is size of Sales / \(size of Product * number of zip-codes\o the reduction will be even more significant Thus, if we only consider the extent of the reduction we will choose question C.  However, the problem is more complicated since both question B and question C involve joining the fact table \(Sales\with a dimension Calendar and Customer respectively\.  On the other hand question A can be computed directly from Sales.  The exact tradeoff between the cost of the join and the amount of reduction can be made explicit by taking into account various system parameters such as the type of database system, amount of main memory and buffer pool processor and disk speed, etc.  \(The number of reduction choices is at least exponential in the number of dimensions involved in the question, which can make the optimization problem intractable for large number of dimensions.  In practice, however, data warehouse models typically have less than 20 dimensions u rt h e r m ore data-mining questions involving more than a small number of dimensions are unlikely to be useful   In this example case, it is likely that question B is the best choice since the reduction is very significant and the join involves a small table \(300 tuples.\igure 5 shows the optimized sequence of SQL queries \(optimization based on question B\ult-equivalent to the Na\357ve SQL query shown in Figure 4   1 2 3 4 5       Figure 5: Equivalent Optimized Sequence of SQL Queries Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE INSERT INTO FrequentProductIdMonth SELECT ProductId, Month FROM Sales, Calendar WHERE Sales.CalendarID = Calendar.CalendarID GROUP BY ProductId, Month HAVING COUNT INSERT INTO ReducedSales1 SELECT <all attributes of Sales FROM Sales S, Calendar C FrequentProductIdMonth F WHERE S.ProductId = F.ProductId AND S.CalendarID = Calendar.CalendarID AND C.Month = F.Month INSERT INTO FrequentProductIdMonthZip SELECT ProductId, Month, Zip FROM ReducedSales1 S, Calendar, Customers WHERE S.CalendarID = Calendar.CalendarID AND Customers.CustomerID = S.CustomerID GROUP BY ProductId, Month, Zip HAVING COUNT INSERT INTO ReducedSales2 SELECT <all attributes of Sales FROM ReducedSales1 S, Calendar C FrequentProductIdMonthZip F, Customers D WHER S.ProductId = F.ProductId AND S.CalendarID = Calendar.CalendarID AND C.Month = F.Month AND D.Zip = F.Zip AND D.CustomerID = S.CustomerID SELECT A.ProductId, B.ProductId, Zip, Month COUNT FROM ReducedSales2 A, ReducedSales2 B Customers, Calendar WHERE A.CustomerID = B.CustomerID AND A.CalendarId = B.CalendarId AND Calendar.CalendarId = A.CalendarId AND Customers.CustomerID = A.CustomerID GROUP BY A.ProductId, B.ProductId, Zip, Month HAVING COUNT 


  Query creates a table of product-month pairs for which there are more than 00 sales transactions in the Sales table.  Query 2 creates a table ReducedSales which extracts from the Sales fact table only those sale transactions that involve product-month pairs resulting from Query The result of Query 2, a reduced Sales table ReducedSales is based on question B.  This reduced table is used in Query 3 to create a table of product-month-zip triples for which there are more 00 sales transactions in the Sales fact table and later in Query 4 to create a table ReducedSales2 that extracts all sale transactions that involve product-month-zip triples resulting from Query 3.  We did not have to use the original Sales fact table, because we know that all possible transactions that satisfy conditions of Query 3 and 4 are replicated in much smaller ReducedSales table Finally, Query 5 performs a join of the ReducedSales2 table with itself in order to find the final result  6. Experiments  In this section we describe an experimental performance study of mining real world data that uses the system and methods described in previous sections of this paper For our experiments we used Oracle 9i relational DBMS running on a 28KB Linux machine.  The data we used reflects the aggregated daily purchases for a major US-based retailer during a six-month period and conforms to the schema shown in Figure 2.  The sizes of the tables are as follows The Product table has around 5000 tuples The Location table has around 2000 tuples with about 50 different Regions The Calendar table has around 200 tuples The Sales table has around 3 million tuples As we discussed in Section 4, standard association rules are not defined for such data.  However, as we also outlined in Section 4, we can still mine association rules by approximating or redefining the meaning of 223frequently together.\224  In our experiments we mined both association rules and extended association rules.  In this paper, we present the results of two queries that are typical for our experiments The first query finds regular association rules based on the following question Find all pairs of items that appear together in more than 9000 transactions Our implementation of this query \(code available by contacting the authors\ involves a pruning of all infrequent items as described in Section 5.2.  The result of the query is thirteen pairs of items shown in Table 2  Table 2  8130 13380 64 521 3060 8130 10226 13890 15240 64 561 13890 15150 9498 8130 13380 11192 123660 123690 54 565 600 8130 48 521 PRODUCTID1 PRODUCTID2 SUM PRODUCTID1 PRODUCTID2 REGIONID SUM 7740 8130 9811 8130 8280 12473 8130 8310 13717 8130 8550 9057 8130 13890 11906 8130 15150 9642 8130 15240 11541 13890 15240 11157 15150 15240 10749  The second query finds extended association rules using the Approach Section 4\o approximating 223frequently together\224, and it is based on the following question  Find all pairs of items that appear together in more than 300 transactions in the same region Our implementation of this query \(code available by contacting the authors\volves pruning all item-region pairs that do not appear in more than 300 transactions The result of this query, shown in Table 3, is a set of fifteen rules, where each rule involves two items and a region  Table 3  15150 18420 62 562 123660 123720 53 556 123660 123780 54 507 123690 123720 53 736 123690 123720 54 803 123690 123840 53 535 123720 123840 53 1081 164490 168420 62 599 167520 167640 17 530 168120 168420 62 555 168420 169650 62 575  Comparing the two queries and their implementation in our system we make the following observations.  The extended association rule mining takes significantly less Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 600 8130 16853 


 time to produce about the same number of rules as association rule mining.  Furthermore, in order to find a similar \(manageable\ber of rules, the support threshold for the association rules had to be set significantly larger than the threshold for extended rules This fact supports our claim that association rules find coarser granularity correlations among items while extended rules discover finer patters The experiments also validate the viability of our tightly-coupled integration with the relational DBMS The running times for all of our queries are measured in seconds and minutes; and there is still a room for significant performance improvement by, for example upgrading hardware or through the addition of indexing In practice, in cases when a data warehouse is heavily utilized with OLAP and reporting requests, a separate data mart dedicated exclusively to data mining can be a good alternative in order to minimize the hits on the enterprise data warehouse and improve overall performance  7. Conclusions  In this paper, we presented a new data-mining framework that is tightly integrated with the data warehousing technology.  In addition to integrating the mining with database technology by keeping all query-processing within the data warehouse, our approach introduces the following two innovations Extended association rules using the other non-item dimensions of the data warehouse, which results in more detailed and ultimately actionable rules Defining association rules for aggregated \(nontransactional\ata We have shown how extended association rules can enable organizations to find new information within their data warehouses relatively easily, utilizing their existing technology.  We have also defined several exact approaches to mining repositories of aggregated data which allows companies to take a new look at this important part of their data warehouse We have conducted experiments that implement our approach on real-life aggregated data and the results support the viability of our integration approach as well as the appropriateness of extended association rules In our future work we plan to elaborate on the optimization algorithm.  We also intend to undertake a further performance study with larger data sets, using different hardware platforms and various types of indexes  References    A g ra wa l R., Im ie linsk i T. a n d A  Sw a m i.  Mining  Association Rules Between Sets of Items in Large Databases Proceeding of ACM SIGMOD International Conference  993\, 207-2 6  2 A g ra wa l R. a nd Srik a n t R Fa st A lg o rithm s f o r Mining  Association Rules Proceeding of International Conference On Very Large Databases VLDB  994\, 487-499  3 R. A g ra wa l, H. Ma nnila R Srik a n t, H. T o iv one n, a n d A   Verkamo.  Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining AAAI/MIT Press  996  4 Berry M. an d L i n o f f   G    Data Mining Techniques for Marketing, Sales and Customer Support Wiley 997  5 C h a u dhr i  S. a n d D a y a l, U  A n ov e r v i ew of  D a ta Warehousing and OLAP Technology ACM SIGMOD Record  26   997\, 65-74  6 H ilde r m a n, R.J Ca rte r C L H a m ilton, H J a nd Ce rc one   N.  Mining Association Rules from Market Basket Data Using Share Measures and Characterized Itemsets International Journal of Artificial Intelligence Tools 7 \(2 998 89-220  7 In m o n  W  H  Building the Data Warehouse Wiley 996  8 K i m b a ll, R., Re e v e s L Ros s M., a nd T hor nthw hite W   The Data Warehouse Lifecycle Toolkit Wiley 998  9 L e a v itt, N. Da ta Mi ning f o r the C o rp ora te Ma sse s IEEE Computer 2002\, 22-24     S  S a r a w a gi  S  Th o m a s  R  A gr a w a l  I n t e gr at i n g M i n i n g  with Relational Database Systems: Alternatives and Implications Proceedings of ACM SIGMOD Conference   998\, 343-354    S  T s ur, J. Ullm a n S. A b ite b oul C. Clif ton  R. M o tw a n i, S  Nestorov, A. Rosenthal.  Query Flocks: A Generalization of Association-Rule Mining Proceedings of ACM SIGMOD Conference  998 2   2 W a ng K H e Y  a n d H a n J  Mi ni ng Fr e que nt I t e m s e ts Using Support Constraints Proceedings of International Conference on Very Large Databases VLDB 2000\, 43-52   3 W a ts on H  J A nni no D   A a nd W i x o m  B  H  C u r r e nt  Practices in Data Warehousing Information Systems Management  8 200   Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


 11 B IOGRAPHY  Ying Chen is a Senior Consultant with Booz Allen Hamilton Inc. She has 5 years of professional experience in system design and development with J2EE and SOA technologies. She is now involved in a research and development project in support of United States Intelligence Community, designing and implementing Advanced Information Sharing and Collaboration solutions. She holds an MS degree in Computer Sc ience from Virginia Tech and BS degree in Computer Science from Fudan University in Shanghai, China  Brad Cohen is an Associate with Booz Allen Hamilton Inc He has 8 years of professional experience in developing and implementing enterprise-class systems for both commercial and government applications.  Currently he is serving as the technical manager on a research and development project in support of the United States Intelligence Community.  He holds an MBA and MS in Information Systems, and a BS in Decision and Information Scien ces from the University of Maryland, College Park   


Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 7 Conclusion and Future Work References frontier  pages 487\320499 Morgan Kaufmann 1994  W orkshop on frequent itemset mining implementations 2003 http://\336mi.cs.helsinki.\336/\336mi03  W orkshop on frequent itemset mining implementations 2004 http://\336mi.cs.helsinki.\336/\336mi04  J  Han J  Pei and Y  Y in Mining frequent patterns without candidate generation In Proceedings of 20th International Conference on Very Large Data Bases VLDB VLDB Journal Very Large Data Bases Data Mining and Knowledge Discovery An International Journal Lecture Notes in Computer Science  2004  J W ang and G Karypis Harmon y Ef 336ciently mining the best rules for classi\336cation In The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD\32504 Symposium on Principles of Database Systems 2 b Average time taken per frequent itemset shown on two scales T10I4D100K is increased and hence the number of frequent items decreases Figure 5\(c also shows that the maximum frontier size is very small Finally we reiterate that we can avoid using the pre\336x tree and sequence map so the only space required are the itemvectors and the minSup SIAM International Conference on Data Mining required drops quite quickly as ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2000 ACM SIGMOD Intl Conference on Management of Data Figure 5 Results  8\(3\3204 2000  F  P an G C ong A T ung J Y ang and M Zaki Carpenter Finding closed patterns in long biological datasets In  2121:236 2001  M Steinbach P N T an H Xiong and V  K umar  Generalizing the notion of support In a Runtime ratios T10I4D100K c Number of Itemvectors needed and maximum frontier size T10I4D100K  pages 1\32012 ACM Press May 2000  F  K orn A Labrinidis Y  K otidis and C F aloutsos Quanti\336able data mining using ratio rules  Morgan Kaufmann 2003  J Pei J Han and L Lakshmanan Pushing convertible constraints in frequent itemset mining We showed interesting consequences of viewing transaction data as itemvectors in transactionspace and developed a framework for operating on itemvectors This abstraction gives great 337exibility in the measures used and opens up the potential for useful transformations on the data Our future work will focus on 336nding useful geometric measures and transformations for itemset mining One problem is to 336nd a way to use SVD prior to mining for itemsets larger than  pages 205\320215 2005  We also presented GLIMIT a novel algorithm that uses our framework and signi\336cantly departs from existing algorithms GLIMIT mines itemsets in one pass without candidate generation in linear space and time linear in the number of interesting itemsets Experiments showed that it beats FP-Growth above small support thresholds Most importantly it allows the use of transformations on the data that were previously impossible  That is the space required is truly linear  D Achlioptas Database-friendly random projections In  2001  R Agra w al and R Srikant F ast algorithms for mining association rules In  8:227\320252 May 2004  J Pei J Han and R Mao CLOSET An ef 336cient algorithm for mining frequent closed itemsets In  pages 21\32030 2000  S Shekhar and Y  Huang Disco v ering spatial colocation patterns A summary of results 


mator from sensor 1 also shown 6. CONCLUSIONS This paper derives a Bayesian procedure for track association that can solve a large scale distributed tracking problem where many sensors track many targets. When noninformative prior of the target state is assumed, the single target test becomes a chi-square test and it can be extended to the multiple target case by solving a multidimensional assignment problem. With the noninformative prior assumption, the optimal track fusion algorithm can be a biased one where the regularized estimate has smaller mean square estimation error. A regularized track fusion algorithm was presented which modifies the optimal linear unbiased fusion rule by a less-than-unity scalar. Simulation results indicate the effectiveness of the proposed track association and fusion algorithm through a three-sensor two-target tracking scenario 7. REFERENCES 1] Y. Bar-Shalom and W. D. Blair \(editors Tracking: Applications and Advances, vol. III, Artech House, 2000 2] Y. Bar-Shalom and H. Chen  Multisensor Track-to-Track Association for Tracks with Dependent Errors  Proc. IEEE Conf. on Decision and Control, Atlantis, Bahamas, Dec. 2004 3] Y. Bar-Shalom and X. R. Li, Multitarget-Multisensor Tracking Principles and Techniques, YBS Publishing, 1995 4] Y. Bar-Shalom, X. R. Li and T. Kirubarajan, Estimation with Applications to Tracking and Navigation: Algorithms and Software for Information Extraction, Wiley, 2001 5] S. Blackman, and R. Popoli  Design and Analysis of Modern Tracking Systems  Artech House, 1999 10 15 20 25 30 35 40 45 50 55 60 2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 1 Sensor 1 Centralized Est Track Fusion 10 15 20 25 30 35 40 45 50 55 60 0 2 


2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 2 Sensor 1 Centralized Est Track Fusion Fig. 7. Comparison of the NEES for centralized IMM estimator \(configuration \(i estimators \(configuration \(ii sensor 1 also shown 6] H. Chen, T. Kirubarajan, and Y. Bar-Shalom  Performance Limits of Track-to-Track Fusion vs. Centralized Estimation: Theory and Application  IEEE Trans. Aerospace and Electronic Systems 39\(2  400, April 2003 7] H. Chen, K. R. Pattipati, T. Kirubarajan and Y. Bar-Shalom  Data Association with Possibly Unresolved Measurements Using Linear Programming  Proc. 5th ONR/GTRI Workshop on Target Tracking Newport, RI, June 2002 8] Y. Eldar, and A. V. Oppenheim  Covariance Shaping Least-Square Estimation  IEEE Trans. Signal Processing, 51\(3 pp. 686-697 9] Y. Eldar  Minimum Variance in Biased Estimation: Bounds and Asymptotically Optimal Estimators  IEEE Trans. Signal Processing, 52\(7 10] Y. Eldar, A. Ben-Tal, and A. Nemirovski  Linear Minimax Regret Estimation of Deterministic Parameters with Bounded Data Uncertainties  IEEE Trans. Signal Processing, 52\(8 Aug. 2004 11] S. Kay  Conditional Model Order Estimation  IEEE Transactions on Signal Processing, 49\(9 12] X. R. Li, Y. Zhu, J. Wang, and C. Han  Optimal Linear Estimation Fusion  Part I: Unified Fusion Rules  IEEE Trans. Information Theory, 49\(9  2208, Sept. 2003 13] X. R. Li  Optimal Linear Estimation Fusion  Part VII: Dynamic Systems  in Proc. 2003 Int. Conf. Information Fusion, Cairns, Australia, pp. 455-462, July 2003 14] X. D. Lin, Y. Bar-Shalom and T. Kirubarajan  Multisensor Bias Estimation Using Local Tracks without A Priori Association  Proc SPIE Conf. Signal and Data Processing of Small Targets \(Vol 


SPIE Conf. Signal and Data Processing of Small Targets \(Vol 5204 15] R. Popp, K. R. Pattipati, and Y. Bar-Shalom  An M-best Multidimensional Data Association Algorithm for Multisensor Multitarget Tracking  IEEE Trans. Aerospace and Electronic Systems, 37\(1 pp. 22-39, January 2001 pre></body></html 


20 0  50  100  150  200  250  300 Pe rc en ta ge o f a dd iti on al tr af fic Cache size 200 clients using CMIP 200 clients using UIR c Figure 6. The percentage of additional traf?c the cache at every clock tick. A similar scheme has been proposed in [13], which uses fv, a function of the access rate of the data item only, to evaluate the value of each data item i that becomes available to the client on the channel If there exists a data item j in the client  s cache such that fv\(i j replaced with i A prefetch scheme based on the cache locality, called UIR scheme, was proposed in [7]. It assumes that a client has a large chance to access the invalidated cache items in the near future. It proposes to prefetch these data items if it is possible to increase the cache hit ratio. In [6], Cao improves the UIR scheme by reducing some unnecessary prefetches based on the prefetch access ratio \(PAR scheme, the client records how many times a cached data item has been accessed and prefetched, respectively. It then calculates the PAR, which is the number of prefetches divided by the number of accesses, for each data item. If the PAR is less than one, it means that the data item has been accessed a number of times and hence the prefetching is useful. The clients can mark data items as non-prefetching when PAR &gt; b, where b is a system tuning factor. The scheme proposes to change the value of b dynamically according to power consumption. This can make the prefetch scheme adaptable, but no clear methodology as to how and when b should be changed. Yin et al. [19] proposed a power-aware prefetch scheme, called value-based adaptive prefetch \(VAP the number of prefetches based on the current energy level to prolong the system running time. The VAP scheme de?nes a value function which can optimize the prefetch cost to achieve better performance These existing schemes have ignored the following characteristics of a mobile environment: \(1 query some data items frequently, \(2 during a period of time are related to each other, \(3 miss is not a isolated events; a cache miss is often followed by a series of cache misses, \(4 eral requests in one uplink request consumes little additional bandwidth but reduces the number of future uplink requests. In this paper, we addressed these issues using a cache-miss-initiated prefetch scheme, which is based on association rule mining technique. Association rule mining is a widely used technique in ?nding the relationships among data items. The problem of ?nding association rules among items is clearly de?ned by Agrawal et al. in [5]. However in the mobile environment, one cannot apply the existing association rule mining algorithm [4] directly because it is too complex and expensive to use This makes our algorithm different from that of [4] in 


This makes our algorithm different from that of [4] in twofold. First, we are interested in rules with only one data item in the antecedent and several data items in the consequent. Our motivation is to prefetch several data items which are highly related to the cache-miss data item within Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE the cache-miss initiated uplink request. We want to generate rules where the antecedent is one data item, but the cache-missed data item and the consequent is a series of data items, which are highly related to the antecedent. If we have such rules, we can easily ?nd the data items which should also be piggybacked in the uplink request. Second in mobile environment, the client  s computation and power resources are limited. Thus, the rule-mining process should not be too complex and resource expensive. It should not take a long time to mine the rules. It should not have high computation overhead. However, most of the association rule mining algorithms [4, 5] have high computation requirements to generate such rules 5. Conclusions Client-side prefetching technique can be used to improve system performance in mobile environments. However, prefetching also consumes a large amount of system resources such as computation power and energy. Thus, it is very important to only prefetch the right data. In this paper, we proposed a cache-miss-initiated prefetch \(CMIP scheme to help the mobile clients prefetch the right data The CMIP scheme relies on two prefetch sets: the alwaysprefetch set and the miss-prefetch set. Novel association rule based algorithms were proposed to construct these prefetch sets. When a cache miss happens, instead of sending an uplink request to only ask for the cache-missed data item, the client requests several items, which are within the miss-prefetch set, to reduce future cache misses. Detailed experimental results veri?ed that the CMIP scheme can greatly improve the system performance in terms of increased cache hit ratio, reduced uplink requests and negligible additional traf?c References 1] S. Acharya, M. Franklin, and S. Zdonik. Prefetching From a Broadcast Disk. Proc. Int  l Conf. on Data Eng., pages 276  285, Feb. 1996 2] S. Acharya, M. Franklin, and S. Zdonik. Balancing Push and Pull for Data Broadcast. Proc. ACM SIGMOD, pages 183  194, May 1997 3] S. Acharya, R. Alonso, M. Franklin, and S. Zdonik. Broadcast disks: Data Management for Asymmetric Communication Environments. Proc. ACM SIGMOD, pages 199  210 May 1995 4] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In J. B. Bocca, M. Jarke, and C. Zaniolo editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB pages 487  499. Morgan Kaufmann, 12  15 1994 5] R. Agrawal, Tomasz Imielinski, and Arun Swami. Mining Association Rules Between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207  216, Washington, D.C May 1993 6] G. Cao. Proactive Power-Aware Cache Management for Mobile Computing Systems. IEEE Transactions on Computers, 51\(6  621, June 2002 7] G. Cao. A Scalable Low-Latency Cache Invalidation Strategy for Mobile Environments. IEEE Transactions on Knowledge and Data Engineering, 15\(5 ber/October 2003 \(A preliminary version appeared in ACM MobiCom  00 8] K. Chinen and S. Yamaguchi. An Interactive Prefetching Proxy Server for Improvement of WWW Latency. In Proc INET 97, June 1997 9] E. Cohen and H. Kaplan. Prefetching the means for docu 


9] E. Cohen and H. Kaplan. Prefetching the means for document transfer: A new approach for reducing web latency. In Proceedings of IEEE INFOCOM, pages 854  863, 2000 10] R. Cooley, B. Mobasher, and J. Srivastava. Data preparation for mining world wide web browsing patterns. Knowledge and Information Systems, 1\(1  32, 1999 11] C. R. Cunha, Azer Bestavros, and Mark E. Crovella. Characteristics of WWW Client Based Traces. Technical Report TR-95-010, Boston University, CS Dept, Boston, MA 02215, July 1995 12] D. Duchamp. Prefetching hyperlinks. In USENIX Symposium on Internet Technologies and Systems \(USITS  99 1999 13] V. Grassi. Prefetching Policies for Energy Saving and Latency Reduction in a Wireless Broadcast Data Delivery System. In ACM MSWIM 2000, Boston MA, 2000 14] S. Hameed and N. Vaidya. Ef?cient Algorithms for Scheduling Data Broadcast. ACM/Baltzer Wireless Networks \(WINET  193, May 1999 15] Q. Hu and D. Lee. Cache Algorithms based on Adaptive Invalidation Reports for Mobile Environments. Cluster Computing, pages 39  48, Feb. 1998 16] Z. Jiang and L. Kleinrock. An Adaptive Network Prefetch Scheme. IEEE Journal on Selected Areas in Communications, 16\(3  11, April 1998 17] V. Padmanabhan and J. Mogul. Using Predictive Prefetching to Improve World Wide Web Latency. Computer Communication Review, pages 22  36, July 1996 18] N. Vaidya and S. Hameed. Scheduling Data Broadcast in Asymmetric Communication Environments. ACM/Baltzer Wireless Networks \(WINET  182, May 1999 19] L. Yin, G. Cao, C. Das, and A. Ashraf. Power-Aware Prefetch in Mobile Environments. IEEE International Conference on Distributed Computing Systems \(ICDCS 2002 Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE pre></body></html 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





