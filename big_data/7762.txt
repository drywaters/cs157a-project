 L Greeshma Assistant Professor, Department of CSE VNR Vignana Jyothi Institute of Engineering and Technology Telangana, India  greeshma_l@vnrvjiet.in  
 Abstract 
  Itemset mining identifies group of frequent itemsets that signify possibly of relevant information. Unique constraints are usually forced to emphasis the analysis on most interestingness itemsets. In this paper we proposed unique constraint based mining on relational dataset. The constrainedbased mining helps us to merge all itemsets, which are interrelated to each other. Specifica lly it chooses itemsets with same consequent part of an assoc iation rule and evaluates the 
 INTRODUCTION Knowledge discovery from data and data mining are multidisciplinary domain that mainly focuses of achieving relevant association rules from un labeled data to consistent data. To perform this task we require various data mining algorithm incorporated from various data sources such as files, data warehouse or databases, World Wide Web and other information repositories such as OLAP 
 Itemset mining, data mining, association rule mining, candidate key generation 
 
highest itemsets with minimum coverage in that relational database. This paper mainly concentrates to propose a new Apriori-based algorithm, which satisfy the certain properties of constrained itemset based mining like anti-monotonicity 
Index Terms 
 I 
Online Analytical Processing which is performed by certain OLAP operations like roll-up, pivot drill-down, slice and dice data is resulted by OLAM Online Line Analytical Mining technique which integrates OLAP with data mining and mining knowledge in interdimen sional databases. Mainly it is required for iden tifying the periodic strategies, which are abstracted inside in database or data warehouse plays, a fundamental role in many data min ing tasks, such as frequent itemset mining, high utility Itemset mining and weighted itemset mi   of m o st widely use d data mining technique is Associa tion Rule Mining \(ARM  Association rule is defined as 002Y, which infers that if a 
X 
specific transaction in a dat abase contains itemset X 
then there is chance it contains itemset For instance, if X purchases a Laptop then there is a possi bility that Y purchases software The most important association ru les are estimated by using interestingness measures like support, confidence. In general the association rule contains IF THEN Conclusion. In other words the interesting measure support signifies the likelihood of IF and THEN part of an association rule may occur together in a data set and confide nce indicates the conditional possibility of happening THEN part under conditional possibility of happening IF part of rule  
 Y 
 Dr. G Pradeepini Associate Professor, Department of CSE K L University, Vaddeswaram AndhraPradesh, India  pradeepini.gera@gmail.com    
The association rules mining difficulty has become a significant research zone by conce pt of the, mining frequent itemset using candidate key generation i.e. Apriori algorithm  rst phase, finding frequent itemsets is vital for making  In Second phase, prov ing these rules is important because most of acquired rules may have same IF part, which causes a duplicates. It also distorts the end user for taking exact decision The problem to be overcome is to 
 
reduce time complexity of resulted data set. In general Information Retrieval System retrieves the huge number of rules because of it various data mining algorithm focuses on mining interdimensional association rules using static discretization of quantitative features on positive dependencies rather than negative dependen cies but in certain situations we can interrelate positive and negative dependencies Thus  time complexity for the algorithms increases which degrades the performance. Later on to represent frequent itemsets the concept of mining closed high utility itemset, compressed frequent pattern, crucial itemsets and frequent itemsets based on positive dependencies have be en proposed in the literature  9], [10], [11 The problems associated with these association rules is it con 
tain redundancy and also size of itemsets are summarized but not association rules. Those rules are often termed as multilev el association rule. Frequent itemsets signify persistent as sociations among data item  which are generally designated by considering their interestingness in the  10]. The m a nual  analysis of the data mining result is a challenging task. To overcome this problem, it em set mining with overall constraints intents to iden tify set of items Instead of estimating and retrieving item sets separately, pattern sets \(i.e sets of itemsets\d and calculated as a whole to analyze the associations among data from a topmost-level perspective. Moreover, instead of generating all the itemsets for each representation only the 
major itemset should be considered, because all the others are reduced representations of the same data. However, to estimate itemset interestingness like support and confiden ce. All previous algorithms just calculate one itemset at a single stage. Therefore, they cannot mine for each representation only the best representative itemset unlike generating all the itemsets initially and then post prune the uninterestingness pattern s. In this paper we addressed the issue of itemset min ing with overall constraints from relational database. In ord er to find out itemsets containing all the relevant in formation related to a given aspect, we proposed a new global constraint, namely the Unique Constraint Frequent Item Set Mining 
2016 6th International Advanced Computing Conference 978-1-4673-8286-1/16 $31.00 © 2016 IEEE DOI 10.1109/IACC.2016.23 66 
2016 6th International Advanced Computing Conference 978-1-4673-8286-1/16 $31.00 © 2016 IEEE DOI 10.1109/IACC.2016.23 66 
2016 IEEE 6th International Conference on Advanced Computing 978-1-4673-8286-1/16 $31.00 © 2016 IEEE DOI 10.1109/IACC.2016.23 66 
2016 IEEE 6th International Conference on Advanced Computing 978-1-4673-8286-1/16 $31.00 © 2016 IEEE DOI 10.1109/IACC.2016.23 68 
2016 IEEE 6th International Conference on Advanced Computing 978-1-4673-8286-1/16 $31.00 © 2016 IEEE DOI 10.1109/IACC.2016.23 68 


 constraint-based association mining. The constraint-based association mining chooses all the itemsets that are \(i\ects only of frequent itemsets with the same representation and \(ii illustrated by greatest size among those corresponding to that representation. To provide a concise and lossless useful representation of differe nt data sets we choi ce at most one item set per representation.  To improve efficiency of mining itemsets with constrain t-based association, we present a new Apriori-based algorithm nam ely Const raine d I t em set Mining algorithm \(CIM\which adopts a breath first search strategy to identify frequent items ets at the time. Therefore, the itemsets of interestingness pa tterns are retrieved at once without the need for post processing. Therefore, even the resultant data can directly help domain experts for advanced analyses. Section 2 reviews on related work. Section 3 outlines necessary Preliminaries for the representation of itemsets and CIM algorithms are proposed Section 4 describes about experimental results. Conclusion is given in Section 5 II. RELATED WORK There are numerous existing algorithms to find frequent itemsets by surveying datab ase for each tran saction in the database and most important among them is Apriori Algorithm Pei et al 4   pr opos ed CL OS E T usi ng t h e de nser unit of data known as Frequent Pa ttern-Tree which is achieved by mining frequent itemsets w ithout the help of a candidate key generation and applying the divide and conquer approach from Frequent Pattern-Growth algorithm 5   Gra hne a n d Zhu  6  d s ubs eque nt vers ion of CL OSE T+ term ed as FPCLOSE condenses iterative traversal over FP-Tree. These rules can be weighed by its coverage and accuracy Major drawback is it is not necessary to produced association rule sets may or may not be less than user specified threshold All these related works specified here are meant for retrieving high relevant rules with out any redundancy and lossless information to the end users based on certain constraints. The issue of choosing discrete itemsets according to their features has already been addressed in In 9  the authors first formulate the problem of discrete itemset mi ning with certain constraints In this context, constraints are aggregations of boolean predicates which impose the presence or the absence of a given itemset permutation. Simi  an attempt to constrain itemset mining according to the itemset representation in the presence of classes has also been comp leted. Unlik  work emphases on retrieving coll ection of itemsets satisfying global constraints rather than discrete itemsets according to their interestingness. However, this paper focuses on retrieving itemsets based on constrained association mining    III.  PRELIMINARIES AND PROPOSED ALGORITHMS In this section we proposed fundamental concepts and definitions, which are requi red for defining proposed algorithms  Let I={I1,I2,I3,ÉÉÉÉ..In} be set of n distinctive items in a relational database R where I1,I2 I3,ÉÉ.In represents feature names. Each relation in a data base consists of distinctive identifiers In the perspective of relational data, a dataset R is a set of records and it is illustrated by a schema 1 2 3 n} which consists of set of attributes j Each tuple t, with an identifier tid, is a set of items Item i j is a pair \(f j v j f j is an attribute that describes a given feature, and v j represents the associated value and belonging to the respective feature domain. Continuous feature values are discretized by the various preprocessing step A k-itemset I in R is a set of k item  with distinct attributes, i.e., I 1,v1 2,v2 n, vn\hat j q for all  q, vq I. In the following we represents it as R\(I\of an itemset I i.e., the set of features of an items appearing in I. Coverage of an itemset I in a given tuple t R if and only if I t. The transaction identifier \(tid\of itemset I, denoted as tid , is the set of tids is associated to the tuples which are  covered by I in R Table 1  represents some relevant data about the employee under analysis. Each tuple corresponds to a different employee and it reports the values of a subset of features. List of attributes are age, buys a computer, salary and work. To achieve their goal, data mining anal ysts mine from the input Buys: a computer, No where each itemset is categorized by a given relation \(e.g Age, Buys: a computer}\To generate relevant itemset, the mined itemsets must contain at least 30% of the employee, i.e their frequency of occurrence \(support\d satisfy min_supp = 30 Itemsets with the same relation are appeared together because they represent the same dataset. For the reason of ease, let us consider the itemsets with a pair of features. Since data analysts do not have a priori information about the most significant representations to consider, they ha ve to \(i\nding all the itemsets satisfying minimum sup port threshold value, \(ii combine the mined itemsets according to their relation, and iii\m sets by arranging them in decreasing order based on their confidence and discarding those itemsets which are not satisfying threshold value Table 2  reports the subset of mined itemsets. Among the itemsets with pair of attributes, the item set with highest confidence is; {Buys: a computer, Work} \(83.3%\For example, according to employee buys a computer and work data analysts can find out di fferent advertisin g policies for youth software employee and middle-aged professors Together, the above-mentioned sections covers 83% of the employee thus represent conceivable objectives of advertising promotions           
 
 A Preliminaries   
  
002\003 004\005 006\005 006\005 006\005 005 007\002 006\005 006\005 006\005 005 010\006\005 011\005 006\005 007 007 012 
67 
67 
67 
69 
69 


 Table 1: Employee Relational Dataset  1 Youth Yes 20K Professor 2 Youth No 25K Software Employer 3 Youth No 25K Software Employer 4 Middle Aged Yes 30K Professor 5 Senior No 15K Clerk 6 Middle Aged Yes 35K Professor  Table 2: Itemsets satisfying the relation-based and minimum confidence constraints, which are mined from Employee Relational Dataset. \(min_supp 30%, min_confidence = 60 I {Age 50 33.3 83.3 I {Buys  Buys, No\\(50 50 100 I {work  Work, Professor\\(50 Work, Software Employer 50 83.3 I {Age, Buys Age, Youth\\(Buys, No\ \(50 Age, Middle Aged\\(Buys Yes\} \(33.3 66.6 I {Age, Work Age Yo Work, Software 33.3 Work 33.3 66.6 I {Buys, Work Buys No Work Software 50 Buys, Yes\\(Work, Professor 33.3 83.3    Itemset mining from a relational dataset R involves identifying subsets of items from  Definition 1 \(Itemset\set of all itemsets in a relational dataset R. I X is a itemset in R Hereafter we will denote by I the set of all the possible itemsets in a relational dataset R, i.e., I=2 n Since this work focuses on identifying interesting itemsets, the distinct items occurring in a item set will be denoted as item sets throughout the paper. Itemsets are categorized by different interesting measures we will consi d er two conventional  interesting measures, name ly the set cardinality and coverage which is estimated particul arly for a suitable itemsets adaptable by domain expert s for handbook inspection    Definition 2 \(Itemset cardinality and coverage i  I be an arbitrary itemset. The definitions follow i The cardinality of I i denoted as c\(I i number of itemsets in I i i.e, c\(I i  ii The coverage of I i denoted as cover\(I i  percentage of tuple in R covered by an itemset in I i i.e  Cover \(I i t R X I i such that t X                \(1  R  U X I i Rid\(I\           \(2     R   Let us consider again the example itemsets reported in Table2 Itemset   I {Age, Buys: a computer r has cardinality=2 and Buys a Buys: a computer Yes with rid {2,3} and {4,6 respectively. Hence, the coverage of the itemset is 66.6 Minimum coverage and maximum cardinality constraints are unique constraints th at are usually enforce d to select the most relevant itemse Th ey choose the itemsets that represent a large sufficient percentage of the analyzed data \(i.e., cover\(I i  min_coverage\have adaptable size \(i.e., c\(I i   max_cardinality  Definition 3 \(relati on-based constraint\min_supp and X f be the set of frequent itemsets in a relational dataset R according to min_supp. An arbitrary itemset I i I satisfies the relation-based constraint if and only if i I contains only frequent itemsets in X with the same relation, i.e., for all X j X q I i such that relation \(X i X q  ii I contains all the frequent itemsets in X with the same relation, i.e., for all X j X q X f such that relation \(X i X q X j X q I i  The anti-monotonicity property should following properties hold i\The minimum coverage constraint cover\(I i  min_coverage is anti-monotone w.r.t., i.e., if cover\(I i  min_coverage then cover\(I j  min_coverage ii\aximum cardinality constraint is anti-monotone w.r.t, i.e., if c\(I i  max_cardinality then c\(I i   max_cardinality, if min_supp = 0 is required Candidate key and itemset generation: The procedure generates the k-itemsets and their resultant itemsets at the same time \(line 12\Any itemset that satisfies the relationbased constraint must contain only itemsets with the same relation. Hence, to generate a candi date keys that contains kitemsets and satisfies the relation-based constraint CIM a pairs of itemsets containing \(k-1\sets. Such items sets are 
B. Itemset mining set with conventional constraints  C. Constrained Item Set Mining Algorithm  
Rid Age Buys a Computer Salary Work  ItemSet Support Count Confidence i 
    
012 007\006 007 013 007 012 007 014\006 015 007 007 007 007 014 006 016 006 016\006 015 017 
68 
68 
68 
70 
70 


 generated at the \(k-1\h itera tion and composed into set CI k-1  line 23\mset I i CI k-1  the relation attributes are ranked in specific order. Two itemsets I i  I j CI k-1  are joined if they share th e first \(k Ö 2 attributes. The resulting itemset I t has relation where relation\(I i n\(I j  ii\set calculation and collecting the resultant data set The support of k-itemset is calculated by scanning a dataset and the infrequent itemsets are removed\(lines 13-17 Therefore, the itemsets satisfying the unique constraint C, i.e the coverage or the cardinality constraint, are selected \(lines 18Ö23\itemsets with the same relation cannot cover the same tuple, the coverage of a itemset can be simply calculated by summing the support count of its itemsets The CIM algorithm Transactional database D, minimum support user specified threshold value min_s upp, uniform constraint c Itemsets satisfying the relation-b ased and unique constraints  1 K := 1 2 Identify all frequent 1-itemsets satisfying min_supp denoted as FI 1 3 Findout the candidate key itemsets for composed 1itemset satisfying relation-ba sed constraints denoted as CI 1 4 for each Candidate Itemset  I CI 1 5  6 Calculate Itemset-1 with respect to interestingness measures 7  8 CI 1 applying unique constraint \(CI 1 C\by deleting Itemsets which are not satisfyin g unique constraints 9 While \(CI k  0 10  11 K=k+1 12 Findout the candidate key itemsets from CI k-1 to evaluate CI k  13 for each Candidate Itemset I CI k  14  15 Calculate the support count of an itemset\(I 16  17 CI K apply_support_constraint\( CI k min_supp discarding the infrequent itemsets from Candidate keys 18 for each candidate Itemset I CI k  19  20 Calculate Itemset-1 with respect to interestingness measures 21  22 CI k apply_unique Constraints \( CI k C 23 CI k applying unique constraint \(CI k C\by deleting Itemsets which are not satisfyin g unique constraints 24  25 return U k CI k    From above example, employee relation dataset in Table 1  and the items in Table 2  mined by considering min_supp = 33 and min- coverage = 50%. CIM first identifies itemsets I age}, I{buys: a computer}, I {salary} and I{work}. I {salary  is removed, along with the corresponding itemset I {salary 25K}, because it does not satisfy the minimum coverage constraint. The remaining itemsets are added to the resultant set and they are used to findout candidate sets I {Age, Buys}, I Age, Work} and I {Buys, Work} For each pair of itemsets their associated itemsets are merged. For example, itemset I Age, Buys  contains 4 candidate keys by merging the itemsets in I {age} and I{buys: a computer},. Among them Buys a computer, No\nd {\(Age Middle-aged\\(Buys a computer, Yes\re frequent and, thus they are included in I {Age, Buys}. Itemsets I {Age, Buys are selected because they satisfy the minimum coverage constraint. Finally, at the third iteration, I {Age, Buys der and I {Age, Work}are merged because they have common the first relation attribute \(i.e., Age\and the associated itemset I Age, Buys: a computer, Work } generated. At the same time frequent 3-itemsets {\(Age, You Buys a computer, No Work, Software Employer\ and Age, Middle-aged\ \(Buys Work, Professor\ are generated and included in I {Age, Buys: a computer, Work IV.  EXPERIMENTAL RESULTS We performed extensive analysis on real and synthetic dataset of CIM \(Constrained Itemset Mining\n compared with the previous approaches  TABLE 3: Characteristics of various Dataset  Vehicle 894 19 Electronic Voting 435 17 Letter_rec. 20000 17   TABLE 4: Based on CIM algorithm for var ious datasets: Number of Itemsets by imposing different coverage constraint value and min_support = 1  Vehicle 50 70 90 3.21E+06 8.73E+05 6.80E+04 64.2 79.2 93.7 67.4 91.1 99.3 Electronic Voting 50 70 90 1.83E+06 6.34E+05 4.97E+04 66.1 79.7 93.3 33.6 76.9 98.2 Letter_rec. 50 70 90 4.08E+03 2.72E+03 1.96E+02 74.9 80.1 98.3 69.8 79.8 98.5  
 
007 007 006\007 010 007 007 
                         
Algorithm Input Output   Data Set Number of tuples Number of attributes Data set Minimum coverage  Itemsets Average Coverage per Itemset Pruned Itemset 
69 
69 
69 
71 
71 


 o ----  min_support = 0.035 x ---- min_support = 0.025   Fig. 1. Vehicle dataset    o ----- min_support = 0.015 x ----- min_support = 0.01  Fig. 2. Electronic dataset      o ----- min_support = 0.025 x ----- min_support = 0.015  Figure 3: Letter_rec. dataset  V.  CONCLUSION The problems of associatio n rule mining is to retrieve relevant itemsets This paper addresses the itemset mining problem with unique constraints. It presents a new constraint, called relation-based constraints, fitted to relational data. In Constrained Itemset Mining \(CIM\ithm helps us to identify candidate key itemsets and generates frequent itemsets by satisfying anti-monoton icity property i.e., minimum coverage and maximum card inality which are restricted to a particular dataset. The experiments illustrate the choosiness of the proposed constraint as well as the algorithm increases efficiency and scalability  REFERENCES  T. Joachims, çOptimizing Search Engin es Using Click through Data,é Proc. Eighth ACM SIGKDD Intêl Conf Knowledge Discovery and Data Mining \(KDD ê02 142, 2002    Agrawal R, Imielin  003 ski T, Swami A \(1993 association rules between sets of items in large data- bases. In Proceedings of the 1993 ACM SIGMOD international conference on management of data \(SIGMOD ê93\pp. 207Ö216  PasquierN, Taoui lR,B astid eY,StummeG, LakhalL \(2005  Generating a condensed representation for association rules. J Intell Inf Syst 29Ö60   PeiJ, HanJ MaoR \(2000 L OSET: an efficient algor ithm for  mining frequent closed itemsets. In: ACM SIGMOD workshop on research issues in, data mining and knowledge discovery, pp 21Ö30  HanJ,PeiJ,YinY\(2000 a tterns withou t  candidate generation. In: Proceedings of the 2000 ACM SIGMOD international conference on management of data SIGMOD ê00\pp 1Ö12   GrahneG,ZhuJ\(2005 equent itemset mining using FP-trees. IEEETransKnowl Data Eng 17\(10 1347Ö1362  Zaki MJ \(Apr il 2002 CJ \(2002 t algorithm for closed itemset mining. In Grossman R, Han J Kumar V, Mannila H, Motwani R \(eds\ings of the second SIAM international conference on data mining Arlington, VA, pp 457Ö473   ChengJ,KeY,NgW\(2008 ive el iminatio n of redundant association rules. DataMinKnowlDisc 16\(2\1Ö249   T. Hamrou ni, çKey ro les of closed sets and minimal generators in concise representations of frequent patterns,é Intell Data Anal., vol. 16, no. 4, pp. 581Ö631, 2012   C. Lucchese, S. Orlando an d R. Perego Fast and memor y  efficient mining of frequent closed it emsets IEEE Trans Knowl. Data Eng., vol. 18, no. 1, pp. 21Ö36, Jan. 2006   Greeshma is working as an Assistant Professor in VNR Vignana Jyothi Institute of Engineering and Technology Telangana, India Main research interest includes data mining and data warehouse, Big Data. Published papers in International Journals and having 3 years of teaching experience    Dr. G Pradeepini, Associate Professor Department of Department of CSE, K L University, Vaddeswaram, Andhra Pradesh, India. Published papers in National & International Journals .She having 13 years of teaching experience and 7 years of research experience.  Main research interest includes data mining and data warehouse, Big data and Neural Networks 
 
 
 BIOGRAPHIES 
70 
70 
70 
72 
72 


j j j 
002\003 004\005\004 006\004\004\007 007\010\011\005 
A Data B Conìguration C The Experiments 
002\003\004\004\005\006\007\010\011\012\007\003\006\013\002\003\014\012\015\003\016\015\012\017\020\015\021\022\023\003\024\007\012\017\004\014\015\003\006\015\025\026\006\012\017\020\012\007\010\015\027\011\012\011 
012\013\014\013\015\016\015\017\020\016\021\021\022\023\024 025\022\015\015\016\014\013\026\027\024\013\022\014\017\025\022\030\024\017\031\013\014\017\015\013\032\032\013\022\014\030\033 011\034\011\011\004\010 011\034\011\011\004 011\034\011\011\006\010 011\034\011\011\006 011 010\011 006\011\011 006\010\011 004\011\011 004\010\011 035\011\011 035\010\011 003\011\011 036\032\037\022\023\013\024 \015  025\032\022\030"\024 036 
        
017    004 004 004 017  015 D 
Fig 1 Comparing our proposed algorithm with a MapReduce adaptations of Closet and AFOPT on the synthetic data The lines represent the communication cost of the three algorithms for each minimum support The bars present the number of closed frequent itemsets found for each minimum support 
002 002 002 
adding an item to a previously found closed itemset We denote that itemset  and the added item such that  Suppose that  In the context of the algorithm it means that would be eliminated We should show that can be produced from a different generator Consider the smallest item in  Since it is frequent and since then surely  meaning that the algorithm will add it to  creating  It is possible that  however if we keep growing with the smallest items we will eventually get  V E XPERIMENTS We have performed several experiments in order to verify the efìciency of our algorithm and to compare it with other renowned algorithms We tested our algorithm on both real and synthetic datasets The real dataset was downloaded from the FIMI repository and is called webdocs It contains close to 1.7 million transactions each transaction is a web document with 5.3 million distinct items each item is a word The maximal length of a transaction is about 71 thousand items The size of the dataset is 1.4 Gigabytes A detailed description of the webdocs dataset that also includes various statistics can be seen in The synthetic dataset was generated using the IBM data generator W e ha v e generated six million transactions with an average of ten items per transaction of a total of 100k items The total size of the input data is 600mb We run all the experiments on the Amazon Elastic MapReduce infrastructure each run w as e x ecuted on 16 machines each is an SSD-based instance storage for fast I/O performance with a quad core CPU and 15 GiB of memory All machines run Hadoop version 2.6.0 with Java 8 We used communication-cost see Section II-B as the main measurement for comparing the performance of the different algorithms the input records to each map task and reduce task were simply counted and summed up and the end of the execution This count is performed on each machine in a distributively manner The implementation of Hadoop provides an internal input records counter that makes the counting and summing task extremely easy Communication-cost is an infrastructure-free measurement meaning that it is not effected by weaker/stronger hardware or temporary network overloads making it our measurement of choice However we also measured the time of execution we run each experiment 3 times and give the average time We have implemented the following algorithms   a naive adaptation of Closet to MapReduce    the AFOPT-close adaptation to MapReduce and   our proposed algorithm All algorithms were implemented in Java 8 taking advantage of its new lambda expressions support We ran the algorithms on the two datasets with different minimum supports and measured the communication cost and execution time for each run The rst batch of runs was conducted on the synthetic dataset The results can be seen in gure 1 The lines represent the communication cost of the three algorithms for each minimum support The bars present the number of closed frequent itemsets found for each minimum support As can be seen our algorithm outperforms the others in terms of communication cost in all the minimum supports In the second batch of runs we run the implemented algorithms on the real dataset with four different minimum supports and measured the communication cost and execution time for each run The results can be seen in gures 2 and 3 The lines represents the different execution times for the different minimum supports As can be seen our algorithm outperforms the existing algorithms VI C ONCLUSION We have presented a new distributed and parallel algorithm for mining closed frequent itemsets using the popular MapReduce programming paradigm Besides its novelty using MapReduce makes this algorithm very easy to implement relieving the programmer from the wearing work of handling concurrency synchronization and nodes management that are part of a distributed environment and focus on the algorithm itself In addition as you recall from section IV-A one of the input parameters to each iteration of the algorithm is  the database This parameter is the dominant one in terms of size 
g g g k k k k k 
f i g f i i  c g c c i c g i c i  g i  f f g f i h g c g c i ii iii 
81 
82 


002\003\004\004\005\006\007\010\011\012\007\003\006\013\002\003\014\012\015\003\016\015\012\017\020\015\021\022\023\003\024\007\012\017\004\014\015\003\006\015\030\020\011\022\015\027\011\012\011 030\005\006\006\007\006\023\015\031\007\004\020\015\003\016\015\012\017\020\015\021\022\023\003\024\007\012\017\004\014\015\003\006\015\030\020\011\022\015\027\011\012\011 
Data Mining and Knowledge Discovery Knowledge and Information Systems Database Theory ICDT 99 Data Mining 2007 ICDM 2007 Seventh IEEE International Conference on Concurrency and Computation Practice and Experience Communications of the ACM Data Mining Workshops ICDMW 2012 IEEE 12th International Conference on FIMI Big Data 2013 IEEE International Conference on Proceedings of the 13th International Conference on Extending Database Technology FIMI 
004\006\010 010\007\011 010\011\004\007 006\010\010\011\005 
012\013\014\013\015\016\015\017\020\016\021\021\022\023\024 025\022\015\015\016\014\013\026\027\024\013\022\014\017\025\022\030\024\017\031\013\014\017\015\013\032\032\013\022\014\030\033 011\034\010 011\034\003 011\034\035 011\034\004 011 006\011\011\011 004\010\011\011 003\011\011\011 010\010\011\011 002\011\011\011 007\010\011\011 006\011\011\011\011 006\004\011\011\011 036\032\037\022\023\013\024 \015  025\032\022\030"\024 036 012\013\014\013\015\016\015\017\020\016\021\021\022\023\024 016\014\014\013\014\037\017'\013\015"\017\031\013\014\017\015\013\014\016\024"\030\033 011\034\010 011\034\003 011\034\035 011\034\004 011 010 006\011 006\010 004\011 004\010 035\011 035\010 003\011 003\010 036\032\037\022\023\013\024 \015  025\032\022\030"\024 036 
Fig 2 Comparing our proposed algorithm with a MapReduce adaptations of Closet and AFOPT on the real data The lines represent the communication cost of the three algorithms for each minimum support The bars present the number of closed frequent itemsets found for each minimum support Fig 3 Comparing the execution time of our proposed algorithm with a MapReduce adaptations of Closet and AFOPT on the real data Since this input is static does not change from one iteration to the next we might use some caching mechanism further increasing the efìciency R EFERENCES  J Han H Cheng D Xin and X Y an Frequent pattern mining current status and future directions  vol 15 no 1 pp 55Ö86 2007  J Cheng Y  K e and W  Ng  A surv e y on algorithms for mining frequent itemsets over data streams  vol 16 no 1 pp 1Ö27 2008  N P asquier  Y  Bastide R T aouil and L Lakhal Disco v ering frequent closed itemsets for association rules in  Springer 1999 pp 398Ö416  C Lucchese S Orlando and R Pere go P arallel mining of frequent closed patterns Harnessing modern computer architectures in  IEEE 2007 pp 242Ö251  C Lucchese C Mastroianni S Orlando and D T alia Mining home toward a public-resource computing framework for distributed data mining  vol 22 no 5 pp 658Ö682 2010  J Dean and S Ghema w at Mapreduce Simpliìed data processing on large clusters  vol 51 no 1 pp 107Ö113 2008  T  A S F oundation Hadoop  http://hadoop.apache.or g  S.-Q W ang Y B Y ang G.-P  Chen Y  Gao and Y  Zhang Mapreducebased closed frequent itemset mining with efìcient redundancy ltering in  IEEE 2012 pp 449Ö453  G Liu H Lu J X Y u W  W ang and X Xiao  Afopt An ef cient implementation of pattern growth approach in  2003  S Moens E Aksehirli and B Goethals Frequent itemset mining for big data in  IEEE 2013 pp 111Ö118  F  Afrati and J Ullman Optimizing joins in a map-reduce en vironment in  ACM 2010 pp 99Ö110  Frequent itemset mining dataset repository   http://ìmi.ua.ac.be/data  C Lucchese S Orlando R Pere go and F  Silv estri W ebdocs a reallife huge transactional dataset in  vol 126 2004  IBM Ibm dataset generator   http://sourcefor ge.net/projects ibmquestdatagen  Amazon Elastic mapreduce emr  https://a ws.amazon.com elasticmapreduce 
82 
83 


0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.48 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0   Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time 
0.1 1 10 100 3000 2000 1000 0 1000 2000 3000 4000 5000 
0.6 0.5 0.4 0.3 0.2 0.1 0.0 10 100 
1 10 100 6000 4000 2000 0 2000 4000 6000 8000 10000 
0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07 0.06 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  0 10000 5000 15000 c Webdocs Figure 5 The performance results of Apriori-AP on three real-world benchmarks DP time SR time and CPU time represent the data process time on AP symbol replacement time on AP and CPU time respectively Webdocs switches to 16-bit encoding when relative minimum support is less then 0.1 8-bit encoding is applied in other cases 
E vs Eclat Eclat et al Eclat Eclat 
0.80 0.75 0.70 0.65 0.60 0.55 0 100 200 300 400 Computation Time \(s Relative minimum support 1.0X Symbol replacement time 0.5X Symbol replacement time 0.1X Symbol replacement time Figure 7 The impact of symbol replacement time on Apriori-AP performance for Pumsb how symbol replacement time affects the total AprioriAP computation time A reduction of 90 in the symbol replacement time leads to 2.3X-3.4X speedups of the total computation time The reduction of symbol replacement latency will not affect the performance behavior of AprioriAP for large datasets since data processing dominates the total computation time 
Apriori Eclat Equivalent Class Clustering   is another algorithm based on Downward-closure uses a vertical representation of transactions and depth-ìrst-search strategy to minimize memory usage Zhang  proposed a h ybrid depth-ìrst/breadth-ìrst search scheme to expose more parallelism for both multi-thread and GPU versions of  However the trade-off between parallelism and memory usage still exists For large datasets the nite memory main or GPU global memory will become a limiting factor for performance and for very large datasets the algorithm fails While there is a parameter which can tune the trade-off between parallelism and memory occupancy we simply use the default setting of this parameter for better performance Figure 8 shows the speedups that the sequential 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 AP counting speedup Apriori-AP overall speedup    Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  b Accidents 
T40D500K Counting T40D500K Overall T100D20M Counting T100D20M Overall Webdocs5X Counting Webdocs5X Overall Speedup Relative Minimum Support Figure 6 The speedup of Apriori-AP over Apriori-CPU on three synthetic benchmarks 
1 10 100 
a Pumsb 
696 


0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 10 Pumsb Accidents T40D500K Webdocs Webdocs5X T100D20M ENWiki Speedup Relative Minimum Support 
Figure 8 The performance comparison of CPU sequential and algorithm achieved with respect to sequential Apriori-CPU Though has 8X performance advantage in average cases the vertical bitset representation become less efìcient for sparse and large dataset high trans and freq item ratio This situation becomes worse as the support number decreases The Apriori-CPU implementation usually achieves worse performance than  though the performance boost of counting operation makes Apriori-AP a competitive solution to parallelized  Three factors make a poor t for the AP though it has better performance on CPU 1 requires bit-level operations but the AP works on byte-level symbols 2 generates new vertical representations of transactions for each new itemset candidate while dynamically changing the values in the input stream is not efìcient using the AP 3 Even the hybrid search strategy cannot expose enough parallelism to make full use of the AP chips Figure 9 and 10 show the performance comparison between Apriori-AP 45nm for current generation of AP and sequential multi-core and GPU versions of  Generally Apriori-AP shows better performance than sequential and multi-core versions of  The GPU version of shows better performance in Pumsb Accidents and Webdocs when the minimum support number is small However because of the constraint of GPU global memory Eclat-1G fails at small support numbers for three large datasets ENWiki T100D20M and Webdocs5X ENWiki as a typical sparse dataset causes inefìcient storage of bitset representation in Eclat and leads to early failure of EclatGPU and up to 49X speedup of Apriori-AP over Eclat-6C In other benchmarks Apriori-AP shows up to 7.5X speedup over Eclat-6C and 3.6X speedup over Eclat-1G This gure also indicates that the performance advantage of AprioriAP over GPU/multi-core increases as the size of the dataset grows The AP D480 chip is based on 45nm technology while the Intel CPU Xeon E5-1650 and Nvidia Kepler K20C on which we test are based on 32nm and 28nm technologies respectively To compare the different architectures in the same semiconductor technology mode we show the performance of technology projections on 32nm and 28nm technologies in Figure 9 and 10 assuming linear scaling for clock frequency and square scaling for capacity The technology normalized performance of Apriori-AP shows better performance than multi-core and GPU versions of Eclat in almost all of the ranges of support that we investigated for all datasets with the exception of small support for Pumsb and T100D20M Apriori-AP achieves up to 112X speedup over Eclat-6C and 6.3X speedup over Eclat-1G The above results indicate that the size of the dataset could be a limiting factor for the parallel algorithms By varying the number of transactions but keeping other parameters xed we studied the behavior of Apriori-AP and Eclat as the size of the dataset increases Figure 11 For T100 the datasets with different sizes are obtained by the IBM synthetic data generator For Webdocs the different data sizes are obtained by randomly sampling the transactions or by concatenating duplicates of the whole dataset In the tested cases the GPU version of fails in the range from 2GB to 4GB because of the nite GPU global memory Comparing the results using different support numbers on the same dataset it is apparent that the smaller support number causes Eclat-1G to fail at a smaller dataset This failure is caused by the fact that the ARM with a smaller support will keep more items and transactions in the data preprocessing stage While not shown in this gure it is reasonable to predict that the multicore implementation would fail when the available physical memory is exhausted However Apriori-AP will still work well on much larger datasets assuming the data is streamed in from the hard drive assuming the hard drive bandwidth is not a bottleneck VII C ONCLUSIONS AND THE F UTURE W ORK We present a hardware-accelerated ARM solution using Micronês new AP architecture Our proposed solution includes a novel automaton design for matching and counting frequent itemsets for ARM The multiple-entry NFA based design was proposed to handle variable-size itemsets MENFA-VSI and avoid routing reconìguration The whole design makes full usage of the massive parallelism of the AP and can match and count up to itemsets in parallel on an AP D480 48-core board When compared with the based single-core CPU implementation the proposed solution shows up to 129X speedup in our experimental 
Apriori Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat F Normalizing for technology Eclat G Data size Eclat Eclat Eclat Apriori 
18 432 
 
697 


0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 1 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm a Webdocs\(1.4GB 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails d Webdocs5X 7.1GB Figure 10 Performance comparison among Apriori-AP Eclat-1C Eclat-6C and Eclat-1G with technology normalization on four large datasets 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm b Accidents 34MB 0.40 0.36 0.32 0.28 0.24 0.20 0.16 0.12 0.08 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm c T40D500K\(49MB Figure 9 Performance comparison among Apriori-AP Eclat1C Eclat-6C and Eclat-1G with technology normalization on three small datasets results on seven real-world and synthetic datasets This APaccelerated solution also outperforms the multicore-based and GPU-based implementations of 0.60 0.55 0.50 0.45 0.40 0.35 0.30 0.25 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails c T100D20M 6.3GB 
ARM a more efìcient algorithm with up to 49X speedups especially on large datasets When performing technology projections on future generations of the AP our results suggest even better speedups relative to the equivalent-generation of CPUs and GPUs Furthermore by varying the size of the datasets from small to very large our results demonstrate the memory constraint of parallel ARM particularly for GPU 
Eclat Eclat 
0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm 
0.20 0.15 0.10 0.05 0.00 1 10 100 1000 10000 100000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails b ENWiki\(3.0GB 
a Pumsb 16MB 
698 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


