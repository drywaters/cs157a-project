Towards Early Detection of Novel Attack Patterns through the Lens of A Large-Scale Darknet 
 Darknet monitoring provides a cost-effective way to monitor the global trend of cyber-threats in the Internet To make full use of the darknet trafìc at hand in this paper we present a study on early detection of emerging novel attacks observed in the darknet First exploration of the regularities in the communications from attacking hosts are done by feeding all observed packets in the darknet to a frequent itemset mining engine where the most frequently occurred attack 
Abstract 
Tao BAN Shaoning PANG Masashi ETO Daisuke INOUE Koji NAKAO Runhe HUANG 
patterns are automatically grouped together Second a time series which characterizes the activity level of each attack pattern is created over the observation period Then to extract the most prominent attack patterns a clustering algorithm is engaged to cluster the attack patterns into groups that carry the similar activities in a long run and dimension reduction is employed to provide visual hints about their relationship Finally attacks featured by a recent rapid increase are picked up to be further inspected by security experts for incident handling purpose The experiments show that the proposed scheme is effective and efìcient in early detection of new attack patterns from conventional approaches 
I I NTRODUCTION 
The conventional practice of cybersecurity has been focused on mitigating cyber-threats at the network edge IPS Intrusion Prevention Systems IDS Intrusion Detection Systems rewalls and web security appliances are installed at the enterprises access points to the Internet making binary yes no decisions while scanning network trafìc that passes through The implicit assumption behind this perimeter protection scheme is that the interior of our enterprise network is a trusted zone while everything outside is untrusted Nevertheless the past few years of cybersecurity has seen emerging new types of cyberattacks which can easily get through the edge without alarming the network security devices Among them are the Drive-By Download DBD attacks which happen when a user is visiting a website viewing 
an e-mail message or cheated to click on a deceptive popup window 2 and Adv anced Persistent Threat APT attacks which employs sophisticated evasion techniques to intrude and stole information from target organizations in sectors with high-value information 4 Getting across the edge gives the adversaries free reign to move laterally within organizations locate valuable intellectual properties and exìltrate them out using undetectable protocols This poses signiìcant threats to the conìdentiality integrity and availability of our data stored and communicated in the Tao Ban Masashi Eto Daisuke Inoue and Koji Nakao are with the National Institute of Information and Communications Technology Tokyo 184-8795 Japan Shaoning Pang is with Unitec Institute of Tech 
nology Auckland 92025 NZ Runhe Huang is with Hosei University Tokyo 184-8584 Japan email 
  
bantao eto dai ko-nakao nict.go.jp ppang@unitec.ac.nz rhuang@hosei.ac.jp Internet To address the concerns raised by these threats there is a pressing need for a proactive warning system which detects the emergence of new threats at its early stage and provides detailed forensic information to facilitate appropriate countermeasures The best practice to foster such an early warning scheme is by global network monitoring While the computation communication and storage costs for monitoring and analyzing a densely populated global-scale network in real 
time may render it impractical the monitoring of unused address space known as a 
 usually provides a good trade-off between the monitoring costs and global knowledge acquisition Darknets also kno wn as netw ork telescopes blackhole monitors sinkholes or background radiation monitors is a portion of routed allocated IP space that contains no advertised services 7 8 Because of the absence of legitimate hosts in the darknet any trafìc observed on a darknet is by its presence aberrant it is either caused by malicious intent or a mis-conìguration Assorted works have deployed darknets in existing networks to help identify the types and sources of malicious trafìc present on the larger network of which they form a part with darknets used to host 
darknet 
ow collectors backscatter detectors packet sniffers and so on 10 Considerable impro v ements in detection rate and cut-down in false positives are reported in related works Driven by the necessity to gain further understanding of the nature of malware-infected hosts to devise their behavioral regularities and to predict their future activities in Ban et al describe a study on beha vior analysis of the attacking hosts monitored on a darknet using association rule learning ARL 12 The disco v ered association rules are used to characterize the regularities among the scanning behaviors of attacking hosts The following ndings have motivated the study presented in this paper 1 When infected by a certain type of malware program 
network devices tend to probe the Internet in a predeìned way resulting in frequent and highly stable association rules discovered 2 Destination ports which specify the network services targeted by the attacking hosts can be used to identify the activity of a speciìc malware program In this paper based on the above discoveries we extend the work of Ban et al to fulìll a more speciìc purpose to identify the emerging types of attacks at their early stage so as to enable proactive countermeasure of these cyberthreats To do so we follow the steps below to mine the darknet trafìc and retrieve the information most relevant to 
2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress 978-1-5090-2771-2/16 $31.00 © 2016 IEEE DOI 10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.157 341 


 
  supp  for X Y I X Y   conf     supp  supp  
1 2 1 2 
N M 
Frequent attack-pattern mining stage Attack activity-level proìling stage Attack pattern clustering and visualization stage data clustering dimension reduction A Association Rule Learning association rule learning database support Frequent pattern mining minimum support association rule conìdence strong frequent pattern mining association rule generation F requent pattern mining Apriori property All nonempty subsets of a frequent itemset must also be frequent Thus for an infrequent itemset all its supersets must also be infrequent Association rule generation a 
the attacks of interest In this stage we try to group the probing activities from millions of attacking hosts into malware-speciìc factions by exploring the regularities in the communications The attack patterns obtained from the rst stage have varying levels of activity during the observation period To track the uctuations therein the activity levels of each attack pattern are measured and recorded as a time series in which each data point records the indicator with a xed time interval To extract the most prominent attack patterns a clustering algorithm is employed to cluster the attack patterns into groups that show the similar activities in a long run Nonlinear dimension reduction is used in this stage to visualize the clustering results and provide visual hints and reduce redundant information for security operators The proposed approach of a combination of human expertise and an exploratory analysis of the behavioral irregularities of the attacking hosts observed in the darknet can complement existing malware countermeasures in the following aspects First the detection of new attack patterns can be achieved by nding the abrupt changes on the time series of attack activity-levels The emergence of these new attacks could be the symptom of pandemic incidents and early detection and take-down of related hosts can help prevent heavy loss Second discovered knowledge can be used to improve the performance of an adaptive monitoring system so that more pertinent malware information can be collected using limited system and network resources For example darknet sensors can be partially updated to an interactive honey-pot system to collect more information about the attacks The efìcacy of such a hybrid monitoring system will be greatly improved if the interesting type of attacks can be deìned beforehand Last but not least prevalent attack patterns and others attacks with exceptional features may be undiscovered following the same scheme whose discovery can lead to further insights into the mechanism and current status of the attacks so as to enable better countermeasures for them This paper is organized as follows Section II introduces related work on the data mining algorithms involved in the study ARL clustering and dimension reduction Section III describes the background of darknet Section IV reports the setting and results of the experiments Finally Section V presents our conclusions II R ELATED WORKS This section briefs ARL  and  Involved in the rst stage ARL aims at nding regularities in the attacking behavior of malicious hosts in the Internet Data clustering is applied in the third stage to group the activity-level time series of all attacking hosts to form a condensed representation of the data Dimension reduction is used to visualize the result of data clustering and provide visual hints about the complicated relationship between the attacks The problem of ARL in which frequent pattern mining FPM is usually the rst key step was originally proposed in the context of market basket data in order to nd frequent groups of items that are purchased together 12 FPM has numerous recent applications to major data mining problems such as customer transaction analysis web log mining software bug analysis and chemical and biological applications Recent surveys on algorithms and applications of ARL and FPM can be found in 14 Following the original deìnition in the problem of ARL is deìned as follows Let be a set of transactions called the  Let be the universal set of items present in the database Each transaction in has a unique transaction ID and contains a subset of the items in  The of a set of items for short itemset is deìned as the number/proportion of transactions in the database is to determine all patterns that occur in at least a fraction of the transactions The fraction is referred to as the  expressed either as an absolute number or as a fraction of  An is deìned as an implication of the form 1 The of a rule is presented by the conditional probability  i.e 2 To select interesting rules from the set of all possible rules rules that meet both a minimum support threshold  and a minimum conìdence threshold  are called  In general ARL can be done in two steps and  invokes searching in a power set of all possible combinations of items for itemsets that satisfy the minimum support condition The key to an efìcient search algorithm is the so-called  Frequent Pattern growth FP-growth   s among the fastest and most popular algorithms for FPM Based on a preìx tree representation of the given database FP-growth can save the extensive amounts of memory for storing the transactions and reduce the search space Refer to for a f ast implementation of FP-gro wth which ha v e been used in our experiments can be done by manipulating the frequent itemsets in the previous step as follows For 
   
D     D 002 003 004 005 006  007  010 
T T T N I i i i M I X X P I S S N X Y    P Y X X Y P Y X X Y  X  S C 
342 


  443 465 993 995 8443 443 443 465 443 465 993 443 465 993 443 465      exp   2  exp   2  0   2  1  1  0 
1 2 2 2  2 2 2 1  2 1 
 002     002  
002 003 003 004 005 006 006 
T i i i ij i j j i i j i k i i k i i i ij j i i j i i ij i j ij i j k l k l ii 
l l s l s l s C           D   T t t t M D t D d p x x p d   002 d   002 p  p p p M  002 P u d q t q q  
b B Clustering data clustering Cluster analysis clustering dendrogram linkage complete distance cutoff C Dimension Reduction 
each frequent itemset  generate all nonempty subsets of  For each nonempty subset of  output the rule   if its conìdence is higher than the conìdence threshold  Since the rules are generated from frequent itemsets all association rules created in this way automatically satisfy the minimum support condition Due to the combinational nature of FPM discovered frequent patterns tend to form high correlated groups For example in our experiments port set is one of the frequent attack patterns when the regularities in targeted destination ports are exploited It is easy to conìrm that all its subsets such as   are also among the frequent attack patterns under the same condition As the attack targeting port set may constitute a considerable part of that targeting  the activity-level time series created in the second stage in our scheme for the frequent attackpatterns tend to form condensed clusters for these highly correlated port combinations To model these clustered data and discover the essential information we sort to methods to give a condensed view of the data or is a highly interdisciplinary eld whose goal is to divide a set of objects into homogeneous groups such that objects in the same group called a cluster are more similar in some sense or another to each other than to those in other groups clusters It is a main task of exploratory data mining and a common technique for statistical data analysis studied and used in many elds Refer to for a thorough surv e y of clustering algorithms and their applications For the sake of its simplicity and ease of implementation hierarchical clustering is one of the most widely studied clustering algorithms It solves clustering by developing a binary tree-based data structure called the  Once the dendrogram is constructed from the data one can automatically choose the right number of clusters by splitting the tree at certain levels to obtain different clustering solutions for the same dataset without rerunning the clustering algorithm again This particular feature of hierarchical clustering renders the human interaction effective in treating with complicated activity-level time series created in the previous stage The basis steps involved in an agglomerative hierarchical clustering algorithm generally known as are as follows First using a particular proximity measure a dissimilarity matrix is constructed and all the data points time series of activity-level time series are visually represented at the bottom of the dendrogram The closest sets of clusters are merged at each step and then the dissimilarity matrix is updated correspondingly This process of agglomerative merging is carried on until no clusters are closer than a predeìned cutoff parameter For concreteness we use the ubiquitous Euclidean distance to measure the proximity between two time series i.e 3 where is the length of the time series Following the convention in the eld and are normalized to have mean zero and a standard deviation of one before calling the distance function To update the distance between clusters in the dissimilarity matrix we choose the complete distance rather than the single distance The is determined by the proximity measure of the most dissimilar members in two clusters Therefore the maximal pair-wise distance between all time series in the same cluster is guaranteed to be smaller than the chosen  Therefore the so-called complete linkage algorithm can yield compact shaped clusters while being robust to noise and outliers The time series which record the activity-level of different attack patterns generally have very a high dimension based on the length of the observation These time series are inherently correlated because of the complicated composition of attacks targeting the same ports For such complicated data nonlinear dimension reduction is one commonly resorted technique to represent the relevant location of the time series in a visible layout for easy understanding In this paper we adopt SNE a popular nonlinear dimension reduction algorithm to perform the mapping from high dimensional time series to a layout in 2 dimension SNE is known to work better than existing techniques at creating a single map that reveals structure at many different scales and it tends to produce good visualizations by reducing the tendency to crowd points together in the center of the embedded layout Below is a brief review of SNE following Given data points in a dimensional space and the pairwise distances between the data points deìned by a distance function SNE nds a mapping from the dimensional input space to a dimensional space as in the following It deìnes joint probabilities that measure the pairwise similarity between objects and by symmetrizing two conditional probabilities using the Gaussian kernel 4 5 Here the bandwidth of the kernels  whose optimal value is usually determined using a simple binary search or a robust root-ìnding method is required to make the perplexity of the conditional distribution equals a predeìned perplexity  Then in the dimensional embedding the similarities between two points and are measured using a normalized Studentkernel with a single degree of freedom 6 
003                 011  011 011  011 
x y x y x y x x x x y y y y y y 
343 


    log 
 
i i j ij ij 
005 
002 
y 
The locations of the embedding points are determined by minimizing the Kullback-Leibler divergence between the joint distributions and  7 The above non-convex optimization problem can be normally solved using a steepest descent algorithm III B ACKGROUND ON D ARKNET T RAFFIC A NALYSIS Network trafìc captured on a darknet contain valuable forensic information of malware programs that are probing the Internet The results reported in this paper are based on a long term observation over a group of darknet sensors hosted in the NICTER project 20 In the NICTER the sensors are installed in a variety of network environments Darknet monitoring relies on the fact that most kinds of malware consist in a scanning phase in search for the next potential victims Thus the more IP addresses encompassed by the darknet the more essential information the darknet could gather Of course due to the limited scale of a darknet compared with the IPV4 space and its passive nature only partial information of the attacks is observable on the darknet The attacks towards the darknet are captured in the form of network packets In general a packet is consisted of two parts control information and user data also known as  Due to the passive nature of the darknet except for some special cases e.g a mis-conìgured server which connects to a presumed printer in the darknet space there will be very few persisting connections toward a darknet especially toward a single IP address Most of the observed hosts send only a couple of connection initializing packets to the darknet i.e TCP packets with SYN ag set on This renders the darknet connections more fragmented lacking application level information Therefore our analysis focus on the control information in the elds stored in packet headers including time stamp of the communication source and destination IP addresses source and destination ports used IP protocols etc The regularities in the scanning packets can be explored at multiple levels At packet level scans towards a certain vulnerability usually conìned to a speciìc destination port At target-host level attacks can be featured by the combination and order of destination ports on a targeted host At network-level precoded rules to select the next targets provide a clue for the type of the malware At meta-level the strength and frequency of the probing packets can be the indicator of certain attacks All of the above information can be helpful to determine the type of the attacks In our experiments we select to exploit the regularities in the targeted ports of the attacks In computer networking a is a software construct that serves as a communication endpoint in a network deviceês host operating system A port uniquely identiìes different applications or processes running on the device and thereby is used as a single physical connection shared in packet-switched networks The port number identiìed by a 16-bit number together with a deviceês IP address completes the destination address for a communication session By convention applications implementing common services often use speciìcally reserved well-known ports e.g the rst 1024 ports for receiving service requests from client hosts The open ports on a device are usually probed by malware to determine available services before exploitation of known vulnerability on the service As the destination ports of the scans carries the most essential information about the targeted network services attacks can be grouped by the destination ports of their scan packets Despite that scans towards a single port can be easily grouped enumeration of the minor and new combination of destination ports could only be done efìciently by advanced ARL algorithms such as FP-growth In a lar ge number of strong association rules with regards to the destination ports are discovered and conìrmed providing valuable clues for malware diagnosis Because the port combinations in the strong association rules have to satisfy the minimum conìdence condition only a small part of the port combinations are covered in the results To provide an overall view of the attacks targeting different ports we choose to perform analysis on the frequent patterns i.e port combinations in this paper which is the output of the frequent pattern mining step of FP-growth This results in increased number of time series created in the second stage nevertheless we can rely on the clustering algorithm in the next stage to create a condensed view of the analysis results IV E XPERIMENTS The experiments described in this section are based on the trafìc collected from a class B darknet which include 65536 unused IP addresses during the full year of 2015 We use the TCP SYN packets i.e TCP packets with the SYN ag set on to perform the analysis Figure 1 show the number of packets and number of unique host observed everyday in 2015 Note that due to some technical problems the observation is terminated for a few hours in late January and for a few days in early May resulting in missing values in the time series Because the observation time is chosen to be a full year to produce a long-term view of the attack activities the inîuence of these missing values is small enough to be ignored To apply the FP-growth algorithm the set of unique destination ports probed by an attacking host during a xed interval is taken as a transaction in the database The time interval is select to be 1-hour on account of the following factors First a comparatively short interval will generally enable ne-grained analysis and fast response Second an interval as long as 1 hour is proved to be long enough to 
A Characteristics of the Darknet Trafìc payload B Investigated regularities in the attacks port A Frequent patterns of destination ports 
P Q C E KL P Q p p  
011 
344 


Fig 1 Formulation of darknet sensors reduce the chance to slice one attack into multiple incoherent attacks The inîuence of dynamic IP address assignment commonly used in modern networks i.e IP addresses assigned via the DHCP protocol could be safely ignored for such a short time interval TABLE I F REQUENT ITEMSETS RELATED TO PORT 23 J ANUARY 1 ST  2015 0:00AM 0:59AM ID Port 1 Port 2 Port 3 Port 4 Occur 1 23 21374 2 23 10073 296 3 23 80 17 4 23 58455 16 5 21 23 10 6 23 443 10 7 21 23 80 10 8 21 23 443 10 9 21 23 80 443 10 10 23 80 443 10 11 23 32764 9 12 23 80 32764 7 13 23 80 58455 7 14 23 80 32764 58455 7 15 23 32764 58455 7 16 23 139 2 17 23 3389 2 18 23 445 1 19 23 8080 1 20 22 23 1 The well-known network services on involved ports are as follows 21 le transfer 22 SSH remote login protocol 23 telnet 80 hypertext transfer protocol HTTP 139 NetBIOS Session Service 443 hypertext transfer protocol over TLS/SSL HTTPS 445 Windows server message block 554 real time streaming protocol 1433 Microsoft SQL server 3389 Microsoft terminal server 8080 HTTP alternate To extract as many attack patterns as possible we have to set a small value ideally 1 to the minimum support 318 93.08 2 10 100 3 16 100 4 7 100 5 10 100 6 10 100 7 10 100 8 10 100 9 10 100 10 10 100 11 10 100 12 10 100 13 10 100 14 7 100 15 7 100 16 7 100 17 7 100 18 7 100 19 7 100 20 7 100 21 7 100 22 7 100 23 296 1.38 24 17 0.1 25 7 77.78 The last three rules which do not satisfy the minimum conìdence are not considered as strong association rules Table I shows the frequent itemsets discovered from the trafìc collected in the darknet sensor in the rst hour of 2015 20 frequent itemsets which are related to port 23 are selected from a pool of 1108 frequent itemsets with the number of their occurrences shown in the last column As shown in the table 21,374 hosts probed port 23 during the 1-hour framework on January 1st 2015 Spotted ports that are probed together with port 23 include ports 21 22 80 139 445 etc Port 23 hosts the telnet service on a PC It is a popularly probed partially because of the following facts Along with the rise of Internet of Things IoT many Linux embedded devices e.g routers web cameras network storage tend to stay 24-hour online with port 23 open for facilitating remote control These IoT devices are frequently targeted by attackers because they often lack operating system and security updates so that when compromised they can serve 
S k k S 
  
10073 23 443 23 80 58455 23 58455 80 23 21 80 23 21 23 80 21 443 23 21 23 443 443 23 21 21 443 80 23 21 443 23 80 21 80 23 443 443 80 23 21 32764 23 32764 80 23 32764 23 80 32764 58455 23 32764 23 58455 32764 58455 80 23 32764 58455 23 80 32764 80 23 58455 58455 80 23 32764 23 10073 23 80 23 32764 80  80 
002 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 003 
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Jan Number of Packets 10 7 0 1 2 3 Number of Unique Attacking Hosts 10 5 0 1 2 3  
 However one of the factors that make this impractical is the existence of a couple of port scanning hosts which is referred to applications designed to probe networked devices for open ports are often used by administrators to verify security policies of their networks and by attackers to identify services running on a host and exploit vulnerabilities For a port scanner probing up to different ports attack patterns will be generated Common network scanners seen in the darknet tend to scan more than different ports which renders the further analysis difìcult As a port scanner can be simply identiìed by the extraordinary number of destination ports it probes we choose to remove the trafìc from hosts who is probing more than ports within an 1-hour interval and preserve it for other research purpose When the port scanners are removed FP-growth returns a few thousands of frequent attack patterns for  which is signiìcantly smaller than all the possible combinations of destination ports which indicates that is an appropriate threshold to remove the negative effect of port scans TABLE II A SSOCIATION RULES RELATED TO PORT 23 ID Rule Support Conìdence 1 
 20 6 1 6 
Port scanners 
                            C 
345 


B Activity level of hosts 
Fig 2 Activity-level time series collected in 2015 hosts number of probed destination IPs and combination of above measurements For simplicity we use the number of unique attacking hosts observed in each time interval which scales to the number of compromised hosts in the Internet as the indicator of activity level To illustrate the long term characteristics of the attack hosts Fig 2 shows a few examples of the activity-level time series that are created for different attack patterns In the graphs each data point denotes the number of unique attacking hosts observed in an 1-hour period and the tags of the time series are shown on the 
23764 80 23 23 23764 80 23 21 23 80 443 23 80 443 21 23 80 443 23 80 
  C    y       
1000 2000 3000 4000 5000 6000 7000 8000  0 5 10 15 1000 2000 3000 4000 5000 6000 7000 8000  0 5 10 15 20 1000 2000 3000 4000 5000 6000 7000 8000  0 10 20 30 1000 2000 3000 4000 5000 6000 7000 8000 23 0 10000 20000 1000 2000 3000 4000 5000 6000 7000 8000  0 100 200 1000 2000 3000 4000 5000 6000 7000 8000  0 50 100 150 200 1000 2000 3000 4000 5000 6000 7000 8000  0 50 100 150 
as step stones for various attacks As can be seen in Table I ports 21 23 80 and 443 show a strong correlation they tend to be probed at the same time This is conìrmed by the association rules shown in Table II which are generated from the frequent patterns in Table I Rules 5 to 13 illustrate the correlation between ports 21 23 80 and 443 If two of the ports are probed the chance for the other ports to be probed is 100 Because of the high correlation of these ports they can be treated as the signature of the probing behavior Take the rules 15 and 25 in Table II as another example The association rule 15   has a strong conìdence of 100 Therefore probes to ports 80 and 23764 can be considered as the causal factor of the probes to port 23 e.g if packets directed to ports 80 and 23764 are observed from a host then port 23 will be also probed with a large chance by the same host On the other hand despite of the comparable high number of co-occurrence between ports 23 80 and 23764 line 15 in Table I the association rule 23   only has a conìdence of 77.78 failing to meet the minimum conìdence requirement 80 The association rules shown in Table II help to conìrm the high correlation between the ports that are often probed together Nevertheless it is not recommended to take these rules as the subject for next-step study on the their activity levels because of two reasons First strong rules are not always strong during the whole observation period Due to network issues such as packet loss and the variation of attacks over time the strong rules may change from time to time If a rule is strong for a certain period but fails to meet the minimum conìdence condition for other periods the recorded time series will subject to uncontrollable uctuations Second it is difìcult to choose an optimal minimum conìdence threshold  Because of the combinational nature of the rule creation procedure when the minimum conìdence threshold is set to a low value there will be too many similar strong association rules On the other hand if the threshold is set to a large value information of many important attacks will be lost because they are not signiìcant enough Therefore we propose to take the frequent patterns as the subject of the analysis on activity levels That is attacks are organized in the name of the frequent patterns discovered in the rst stage The strong association rules serves as a good indicator of the purity of the pattern For example the frequent pattern consists activity components of many attack patterns whilst represents the activity of a type of attack because of the high conìdence in the related rules An attack pattern obtained from the rst stage may have varying activity level during an observation period as long as one year To track the uctuations therein the activity level of an attack pattern is measured and recorded as a time series in which each data point denote the indicator in a xed time interval Common indicators to measure the activity level include the number of packets number of unique attacking axis It can be seen that the time series for and look almost the same in a long run because of the high correlation conìrmed by the high conìdence of the related association rules in Table II The time series for port set deviates slightly from the rst two This can be explained 
003 003           
346 


botnets bot master C Data clustering and visualization 
Distance between Clusters 0 10 20 30 40 50 60 70 80 Index of Clusters 25 29 37 41 10 20 26 14 2 18 31 3 24 22 80 34 36 1 23 15 48 52 69 12 66 54 56 64 73 7 21 45 19 50 44 62 55 8 77 38 43 6 40 59 75 79 4 70 74 47 68 11 28 13 60 78 17 67 51 72 39 49 57 16 32 9 58 76 27 30 46 63 33 53 61 65 71 5 35 42  
Fig 3 Dendrogram create by a completed linkage algorithm 30 clusters are generated when the cutoff is set to 64.8 The dendrogram created by a complete linkage algorithm is shown in Fig 3 When the number of clusters is selected as 30 the distance threshold to partition the clusters is 64.8 To get an insight to the data distribution in the high dimensional input space we apply the 
21 443 80 23 21 23 80 443 23 23 10073 23 20012 365 24 21 23 443 21 23 80 443 21 23 23 20012 
       t t A B A B A B        
that association rule such as does not hold with a 100 conìdence for other time intervals It can be more clear that because port 23 is involved in many type of attacks besides the attacks probing ports set  the time series for is totally different from the rst three The 5th time series in Fig 2 shows the records of an attack pattern which has reduced activities starting from mid2015 In fact port set is the signature of the Linux Moose worm which infects Linux-based routers and other Linux-based devices and turns them into social network bots Few weeks after the publication of the whitepaper  in May 2015 the Moose C&C serv ers went dark and the number of infected hosts signiìcantly decreased It is reported that an update on this atypical embedded Linux botnet is one online after a few weeks using a different port set i.e  to perform the infection This can be conìrmed on the 6th time series in the gure more than 200 unique attacking hosts are observed at peak time The last time series in the gure gives an example of temporally coordinated attack activities observed in the darknet which is of particular interest for security experts There are many abrupt changes on the graph indicating that the burst of attacks are only observable during a very short time period This agrees with the facts reported in other studies of  groups of Internet-connected computers communicating with other in an effort to complete repetitive tasks and objectives As infected PCs in a botnet is typically governed by a  they tend to carry out attacks in a coordinated fashion after they got the attack command via the C&C channels All the above cases indicate that the shape of the time series could provide valuable information about the nature of attacks associated with the attack patterns Nevertheless the complicated shape of the time series also indicate that a universal algorithm which could characterize all possible attacks in an optimal way may not exist Rather than relying on an analytical tools playing as the oracle we advocate a scheme to combine human expertise together with advanced machine learning tools to approach such a challenging task While a security expert may be able to identify attack patterns of particular interest based on extensive experience he/she may still rely on machine learning to provide valuable hints and lter out irrelevant and redundant information In the following we describe the application of the linkage algorithm and the SNE algorithm to fulìll this goal As shown in Fig 2 highly correlated attack patterns will generate very consistent activity-level time series It is ideal to pick out the representative ones and discard those without much new information The one year darknet trafìc yield more than 150,000 distinct attack patterns Before applying the linkage algorithm to form condensed clusters we use a simple ltering rule to remove the less signiìcant attack patterns patterns that have been less observed than 1000 out of the hours are removed for simplicity This results in 852 attack patterns as input to the linkage algorithm SNE algorithm to create a 2-dimensional embedding of the data which is shown in Fig 4 In the gure each colored disk denotes the location of the attack pattern in the embedding space Data points in different clusters are shown with different colors as illustrated in the color bar at the right side of the gure The size of a disk indicates the average number of hosts with the same attack pattern To conìrm that the highly correlated attack patterns fall close to each other and belong to the same cluster we connect them using line More speciìcally a point is connected to its parent when they are in the same cluster A port set is called the parent of set if and contains exact one less element than  For example set is the parent of set  whilst set is not The relationship is so deìned to create connections between related attack patterns but not to result in too many connections in the plot In Fig 4 it can be conìrmed that the attack patterns generated from the same attack are assigned to the same cluster For attack patterns that are related in two aspects high similarity in terms of the shape of the time series  indicated by the same color of the data points and the correlation in terms of the shared ports  illustrated by a connecting line segments they can be considered as the same attack A closer look into the cluster structures in the gure reveals attack patterns in the same cluster but are not connected by any lines This indicates that these attacks patterns belongs to different attacks but happen to have a similar shape of activity-level These attack patterns shall be investigated individually Attack patterns which are in the same cluster of port set will be the most interesting for they have a high chance to be related to some novel types of attacks 
003          002         
347 


Fig 4 Clustering results in the 2D embedding created by SNE V C ONCLUSION In this paper we presented a study on application of association rule learning clustering and dimension reduction on the darknet trafìc Frequent itemsets with respects to probed destination ports are reported in the experiments Some of the signiìcant association rules discovered in the experiments are proved to correspond to speciìc known attacks from IoT devices Analysis on the activity-level time series of the attack patterns has reveal emergence of new type of attacks The proposed scheme can help security experts to discover new emerging attacks at their early stage providing valuable hints for further investigation We leave the formulation of a strategic framework to countermeasure the new threats discovered by this approach for future work R EFERENCES  M Co v a C Krue gel and G V igna Detection and analysis of dri v eby-download attacks and malicious javascript code in  ser WWW 10 New York NY USA ACM 2010 pp 281Ö290  L Lu V  Y e gnesw aran P  Porras and W  Lee Blade An attackagnostic approach for preventing drive-by malware infections in  ser CCS 10 New York NY USA ACM 2010 pp 440Ö450  E Cole  1st ed Syngress Publishing 2013  Z Saud and M H Islam T o w ards proacti v e detection of adv anced persistent threat apt attacks using honeypots in  ser SIN 15 New York NY USA ACM 2015 pp 154Ö157  L In v ernizzi S Misk o vic R T orres C Krue gel S Saha G V igna S Lee and M Mellia Nazca Detecting malware distribution in large-scale networks in  2014  T  Ban M Eto S Guo D Inoue K Nakao and R Huang  A study on association rule mining of darknet big data in  2015 pp 1Ö7  M Baile y  E Cook e F  Jahanian J Nazario D W atson  The internet motion sensor-a distributed blackhole monitoring system in  2005  T  Ban L Zhu J Shimamura S P ang D Inoue and K Nakao Behavior analysis of long-term cyber attacks in the darknet in  vol 151 no 3 Elsevier 2012 pp 620Ö628 
Proceedings of the 19th International Conference on World Wide Web Proceedings of the 17th ACM Conference on Computer and Communications Security Advanced Persistent Threat Understanding the Danger and How to Protect Your Organization Proceedings of the 8th International Conference on Security of Information and Networks 21st Annual Network and Distributed System Security Symposium NDSS 2014 San Diego California USA February 23-26 2014 2015 International Joint Conference on Neural Networks IJCNN 2015 Killarney Ireland July 12-17 2015 et al NDSS Neural Information Processing 19th International Conference ICONIP 2012 Doha Qatar November 12-15 2012 Proceedings Part V 
40 30 20 10 0 10 20 30 40 40 30 20 10 0 10 20 30 40 5 10 15 20 25 30 
t 
348 


 U Harder  M W  J ohnson J T  Bradle y  and W  J Knottenbelt Observing internet worm and virus attacks with a small network telescope  vol 151 no 3 pp 47Ö59 2006  K Benson A Dainotti K Claf fy  and E Aben Gaining insight into as-level outages through analysis of internet background radiation in  IEEE 2013 pp 447Ö452  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  vol 22 no 2 ACM 1993 pp 207Ö216  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  vol 29 no 2 ACM 2000 pp 1Ö12  J Han H Cheng D Xin and X Y an Frequent pattern mining Current status and future directions  vol 15 no 1 pp 55Ö86 Aug 2007  C C Aggarw al and J Han Eds  Springer 2014  C Bor gelt Frequent item set mining   vol 2 no 6 pp 437Ö456 2012  L Kaufman and P  J Rousseeuw   9th ed Wiley-Interscience Mar 1990  C C Aggarw al  2013  L v an der Maaten and G E Hinton V isualizing high-dimensional data using t-sne  vol 9 pp 2579Ö2605 2008  K Nakao K Y oshioka D Inoue and M Eto  A no v el concept of network incident analysis based on multi-layer ovservation of malware activities in  2007 pp 267Ö279  D Inoue K Y oshioka M Eto M Y amagata E Nishino J T ak euchi K Ohkouchi and K Nakao An incident analysis system nicter and its analysis engines based on data mining techniques in  2008  O Bilodeau and T  Dupuy  Dissecting linux/moose  2015 A v ailable http://www weli v esecurity com/wpcontent/uploads/2015/05/Dissecting-LinuxMoose.pdf  O Bilodeau Linux/moose endangered or extinct 2015 A v ailable https://www.virusbulletin.com/uploads/pdf/conference slides/2015 Bilodeau-VB2015.pdf 
Electronic Notes in Theoretical Computer Science Computer Communications Workshops INFOCOM WKSHPS 2013 IEEE Conference on ACM SIGMOD Record ACM SIGMOD Record Data Min Knowl Discov Frequent Pattern Mining Data Mining Knowledge Discovery Finding Groups in Data An Introduction to Cluster Analysis An Introduction to Cluster Analysis Journal of Machine Learning Research The 2nd Joint Workshop on Information Security JWIS07 15th International Conference on NeuroInformation Processing of the Asia Pasiìc Neural Netowrk Assembly ICONIP 2008 
349 


each of which has distinct characteristics compared with the other partitions Such distinct features among the partitions allow FiDoop-DP to efﬁciently reduce the number of redundant transactions In contrast a dataset with high dimensionality has a long average transaction length therefore data partitions produced by FiDoop-DP have no distinct discrepancy Redundant transactions are likely to be formed for partitions that lack distinct characteristics Consequently the beneﬁt offered by FiDoop-DP for highdimensional datasets becomes insigniﬁcant 6.3.2 Data Correlation We set the correlation among transactions i.e corr to 0.15 0.25 0.35 0.45 0.55 0.65 and 0.75 to measure the impacts of data correlation on the performance of the two algorithms on the 8-node Hadoop cluster The Number of Pivots is set to 60 see also Section 6.1 The experimental results plotted in Fig 5c clearly indicate that FiDoop-DP is more sensitive to data correlation than Pfp This performance trend motivates us to investigate the correlation-related data partition strategy Pfp conducts default data partition based on equal-size item group without taking into account the characteristics of the datasets However FiDoop-DP judiciously groups items with high correlation into one group and clustering similar transactions together In this way the number of redundant transactions kept on multiple nodes is substantially reduced Consequently FiDoop-DP is conducive to cutting back both data transmission trafﬁc and computing load As can be seen from Fig 5c there is an optimum balance point for data correlation degree to tune FiDoop-DP performance e.g 0.35 in Fig 5c If data correlation is too small Fidoop-DP will degenerate into random partition schema On the contrary it is difﬁcult to divide items into relatively independent groups when data correlation is high meaning that an excessive number of duplicated transactions have to be transferred to multiple nodes Thus a high data correlation leads to redundant transactions formed for partitions thereby increasing network and computing loads 6.4 Speedup Now we are positioned to evaluate the speedup performance of FiDoop-DP and Pfp by increasing the number of data nodes in our Hadoop cluster from 4 to 24 The T40I10D 128 blocks dataset is applied to drive the speedup analysis of the these algorithms Fig 6 reveals the speedups of FiDoop-DP a nd Pfp as a function of the number of data nodes The experimental results illustrated in Fig 6a show that the speedups of FiDoop-DP and Pfp linearly scale up with the increasing number of data nodes Such a speedup trend can be attributed to the fact that increasing the number of data nodes under a xed input data size inevitably 1 reduces the amount of itemsets being handled by each node and 2 increases communication overhead among mappers and reducers Fig 6a shows that FiDoop-DP is better than Pfp in terms of the speedup efﬁciency For instance the FiDoop-DP improves the speedup efﬁciency of Pfp by up to 11.2 percent with an average of 6.1 percent This trend suggests FiDoopDP improves the speedup efﬁciency of Pfp in large-scale The speedup efﬁciencies drop when the Hadoop cluster scales up For example the speedup efﬁciencies of FiDoopDP and Pfp on the 4-node cluster are 0.970 and 0.995 respectively These two speedup efﬁciencies become 0.746 and 0.800 on the 24-node cluster Such a speedup-efﬁciency trend is driven by the cost of shufﬂing intermediate results which sharply goes up when the number of data nodes scales up Although the overall computing capacity is improved by increasing the number of nodes the cost of synchronization and communication among data nodes tends to offset the gain in computing capacity For example the results plotted in Fig 6b conﬁrm that the shufﬂing cost Fig 5 Impacts of data characteristics on FiDoop-DP and Pfp Fig 6 The speedup performance and shufﬂing cost of FiDoop-DP and Pfp 110 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


is linearly increasing when computing nodes are scaled from 4 to 24 Furthermore the shufﬂing cost of Pfp is larger than that of FiDoop-DP 6.5 Scalability In this group of experiments we evaluate the scalability of FiDoop-DP and Pfp when the size of input dataset dramatically grows Fig 7 shows the running times of the algorithms when we scale up the size of the T40I10D data series Figs 7a and 7b demonstrate the performance of FiDoop-DP processing various datasets on 8-node and 24-node clusters respectively Fig 7 clearly reveals that the overall execution times of FiDoop-DP and Pfp go up when the input data size is sharply enlarged The parallel mining process is slowed down by the excessive data amount that has to be scanned twice The increased dataset size leads to long scanning time Interestingly FiDoop-DP exhibits a better scalability than Pfp Recall that see also from Algorithm 1 the second MapReduce job compresses an initial transaction database into a signature matrix which is dealt by the subsequent process The compress ratio is high when the input data size is large thereby shortening the subsequent processing time Furthermore Fidoop-DP lowers the network trafﬁc induced by the random grouping strategy in Pfp In summary the scalability of FiDoop-DP is higher than that of Pfp when it comes to parallel mining of an enormous amount of data 7R ELATED W ORK 7.1 Data Partitioning in MapReduce Partitioning in databases has been widely studied for both single system servers e.g and distributed storage systems e.g BigTable PNUTS[31 The existing approaches typically produce possible ranges or hash partitions which are then evaluated using heuristics and cost models These schemes offer limited support for OLTP workloads or query analysis in the context of the popular MapReduce programming model In this study we focus on the data partitioning issue in MapReduce High scalability is one of the most important design goals for MapReduce applications Unfortunately the partitioning techniques in existing MapReduce platforms e.g Hadoop are in their infancy leading to serious performance problems Recently a handful of data partitioning schemes have been proposed in the MapReduce platforms Xie et al  developed a data placement management mechanism for heterogeneous Hadoop clusters Their mechanism partitions data fragments to nodes in accordance to the nodes processing speed measured by computing ratios In addition Xie et al  designed a data redistribution algorithm in HDFS to address the data-skew issue imposed by dynamic data insertions and deletions CoHadoop is a H a d oop s lightweight extension which is designed to identify relateddataﬁlesfollowedbyamodiﬁeddataplacement policy to co-locate copies of those related les in the same server CoHadoop considers the relevance among les that is CoHadoop is an optimization of HaDoop for multiple les A key assumption of the MapReduce programming model is that mappers are completely independent of one another Vernica et al  broke such an assumption by introducing an asynchronous communication channel among mappers T his c hannel e nables the m appers to see global states managed in metadata Such situationaware mappers SAMs can enable MapReduce to exibly partition the inputs Apart from this adaptive sampling and partitioning were proposed to produce balanced partitions for the reducers by sampling mapper outputs and making use of obtained statistics Graph and hypergraph partitioning have been used to guide data partitioning in parallel computing Graph-based partitioning schemes capture data relationships For example Ke et al applied a graphic-execution-plan graph EPG to perform cost estimation and optimization by analyzing various properties of both data and computation Their estimation module coupled with the cost model estimate the runtime cost of each vertex in an EPG which represents the overall runtime cost a data partitioning plan is determined by a cost optimization module Liroz-Gistau et al proposed the MR-Part technique which partitions all input tuples producing the same intermediate key co-located in the same chunk Such a partitioning approach minimizes data transmission among mappers and reducers in the shufﬂe phase The approach captures the relationships between input tuples and intermediate keys by monitoring the execution of representative workload Then based on these relationships their approach applies a min-cut k-way graph partitioning algorithm thereby partitioning and assigning the tuples to appropriate fragments by modeling the workload with a hyper graph In doing so subsequent MapReduce jobs take full advantage of data locality in the reduce phase Their partitioning strategy suffers from adverse initialization overhead Fig 7 The scalability of FiDoop-DP and Pfp when the size of input dataset increases XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 111 


7.2 Application-Aware Data Partitioning Various efﬁcient data partitioning strategies have been proposed to improve the performance of parallel computing systems For example Kirsten et al  developed two general partitioning strategies for generating entity match tasks to avoid memory bottlenecks and load imbalances Taking into account the characteristics of input data Aridhi et al proposed a novel density-based data partitioning technique for approximate large-scale frequent subgraph mining to balance computational load among a collection of machines Kotoulas et al built a data distribution mechanism based on clustering in elastic regions Traditional term-based partitioning has limited scalability due to the existence of very skewed frequency distributions among terms Load-balanced distributed clustering across networks and local clustering are introduced to improve the chance that triples with a same key are collocated These selforganizing approaches need no data analysis or upfront parameter adjustments in a priori Lu et al studied k nearest neighbor join using MapReduce in which a data partitioning approach was designed to reduce both shufﬂing and computational costs In Lu’s study objects are divided into partitions using a Voronoi diagram with carefully selected pivots Then data partitions i.e Voronoi cells are clustered into groups only if distances between them are restricted by a speciﬁc bound In this way their approach can answer the k-nearest-neighbour join queries by simply checking object pairs within each group FIM for data-intensive applications over computing clusters has received a growing attention efﬁcient data partitioning strategies have been proposed to improve the performance of parallel FIM algorithms A MapReducebased Apriori algorithm is designed to incorporate a new dynamic partitioning and distributing data method to improve mining performance This method divides input data into relatively small splits to provide exibility for improved load-balance performance Moreover the master node doesn’t distribute all the data once rather the rest data are distributed based on dynamically changing workload and computing capability weight of each node Similarly Jumbo adopted a dynamic partition assignment technology enabling each task to process more than one partition Thus these partitions can be dynamically reassigned to different tasks to improve the load balancing performance of Pfp Uthayopas et al  investigated I/O and execution scheduling strategies to balance data processing load thereby enhancing the utilization of a multi-core cluster system supporting association-rule mining In order to pick a winning strategy in terms of data-blocks assignment Uthayopas et al incorporated three basic placement policies namely the round robin range and random placement Their approach ignores data characteristics during the course of mining association rules 8F URTHER D ISCUSSIONS In this study we investigated the data partitioning issues in parallel FIM We focused on MapReduce-based parallel FPtree algorithms in particular we studied how to partition and distribute a large dataset across data nodes of a Hadoop cluster to reduce network and computing loads We argue that the general idea of FiDoop-DP proposed in this study can be extended to other FIM algorithms like Apriori running on Hadoop clusters Apriori-based parallel FIM algorithms can be classiﬁed into two camps namely count distribution and data distribution  For the count distribution camp each node in a cluster calculates local support counts of all candidate itemsets Then the global support counts of the candidates are computed by exchanging the local support counts For the data distribution camp each node only keeps the support counts of a subset of all candidates Each node is responsible for delivering its local database partition to all the other processors to compute support counts In general the data distribution schemes have higher communication overhead than the count distribution ones whereas the data distribution schemes have lower synchronization overhead than its competitor Regardless of the count distribution or data distribution approaches the communication and synchronization cost induce adverse impacts on the performance of parallel mining algorithms The basic idea of Fidoop-DP—grouping highly relevant transactions into a partition allows the parallel algorithms to exploit correlations among transactions in database to cut communication and synchronization overhead among Hadoop nodes 9C ONCLUSIONS A ND F UTURE W ORK To mitigate high communication and reduce computing cost in MapReduce-based FIM algorithms we developed FiDoop-DP which exploits correlation among transactions to partition a large dataset across data nodes in a Hadoop cluster FiDoop-DP is able to 1 partition transactions with high similarity together and 2 group highly correlated frequent items into a list One of the salient features of FiDoopDP lies in its capability of lowering network trafﬁc and computing load through reducing the number of redundant transactions which are transmitted among Hadoop nodes FiDoop-DP applies the Voronoi diagram-based data partitioning technique to accomplish data partition in which LSH is incorporated to offer an analysis of correlation among transactions At the heart of FiDoop-DP is the second MapReduce job which 1 partitions a large database to form a complete dataset for item groups and 2 conducts FP-Growth processing in parallel on local partitions to generate all frequent patterns Our experimental results reveal that FiDoop-DP signiﬁcantly improves the FIM performance of the existing Pfp solution by up to 31 percent with an average of 18 percent We introduced in this study a similarity metric to facilitate data-aware partitioning As a future research direction we will apply this metric to investigate advanced loadbalancing strategies on a heterogeneous Hadoop cluster In one of our earlier studies see for details we addressed the data-placement issue in heterogeneous Hadoop clusters where data are placed across nodes in a way that each node has a balanced data processing load Our data placement scheme can balance the amount of data stored in heterogeneous nodes to achieve improved data-processing performance Such a scheme implemented at the level of Hadoop distributed le system HDFS is unaware of correlations among application data To further improve load balancing 112 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


mechanisms implemented in HDFS we plan to integrate FiDoop-DP with a data-placement mechanism in HDFS on heterogeneous clusters In addition to performance issues energy efﬁciency of parallel FIM systems will be an intriguing research direction A CKNOWLEDGMENTS The work in this paper was in part supported by the National Natural Science Foundation of P.R China No.61272263 No.61572343 Xiao Qin’s work was supported by the U.S National Science Foundation under Grants CCF-0845257 CAREER The authors would also like to thank Mojen Lau for proof-reading R EFERENCES  M J Zaki Parallel and distribu ted associat ion mining A survey IEEE Concurrency  vol 7 no 4 pp 14–25 Oct 1999  I Pramudiono and M Kitsuregawa  Fp-tax Tree structure based generalized association rule mining in Proc 9th ACM SIGMOD Workshop Res Issues Data Mining Knowl Discovery  2004 pp 60–63  J De an a n d S Gh e ma wa t M ap re du ce  S i mp l i e d da ta pr o ce s si n g on large clusters ACM Commun  vol 51 no 1 pp 107–113 2008  S Sakr A Liu and A G Fayoumi The family of mapred uce and large-scale data processing systems ACM Comput Surveys  vol 46 no 1 p 11 2013  M.-Y Lin P.-Y Lee and S.-C Hsueh Apriori-based frequent itemset mining algorithms on mapreduce in Proc 6th Int Conf Ubiquitous Inform Manag Commun  2012 pp 76:1–76:8  X Li n  Mr a pr io ri  As so ci a ti o n ru le s a lg o ri th m ba se d on mapreduce in Proc IEEE 5th Int Conf Softw Eng Serv Sci  2014 pp 141–144  L Zhou Z Zhong J Chang J Li J Huang and S Feng Balanced parallel FP-growth with mapreduce in Proc IEEE Youth Conf Inform Comput Telecommun  2010 pp 243–246  S Hong Z Huaxuan C Shiping and H Chunyan The study of improved FP-growth algorithm in mapreduce in Proc 1st Int Workshop Cloud Comput Inform Security  2013 pp 250–253  M Riondato  J A DeBrabant R Fonseca and E Upfal Parma A parallel randomized algorithm for approximate association rules mining in mapreduce in Proc 21st ACM Int Conf Informa Knowl Manag  2012 pp 85–94  C Lam Hadoop in Action  Greenwich USA Manning Publications Co 2010  H Li Y Wang D Zhang M Zhang and E Y Chang PFP Parallel FP-growth for query recommendation in Proc ACM Conf Recommender Syst  2008 pp 107–114  C Curino E Jones Y Zhang and S Madden Schism A workload-driven approach to database replication and partitioning Proc VLDB Endowment  vol 3 no 1-2 pp 48–57 2010  P Uthayop as and N Benjamas Impact of i/o and execution scheduling strategies on large scale parallel data mining J Next Generation Inform Technol  vol 5 no 1 p 78 2014  I  P r a m u d i o n o a n d M  K i t s u r e g a w a  P a r a l l e l F P g r o w t h o n P C cluster in Proc.Adv.Knowl.DiscoveryDataMining  2003 pp 467–473  Y Xun J Zhang and X Qin Fidoop Parallel mining of frequent itemsets using mapreduce IEEE Trans Syst Man Cybern Syst  vol 46 no 3 pp 313–325 Mar 2016 doi 10.1109 TSMC.2015.2437327  S Owen R Anil T Dunning and E Friedman Mahout Action  Greenwich USA Manning 2011  D Borthakur  Hdfs architecture guide HADOOP APACHE PROJECT Available  http://hadoop.apache.org/common/docs current/hdfs design.pdf 2008  M Zaharia M Chowdhury M J Franklin  S Shenker and I Stoica Spark Cluster computing with working sets in Proc 2nd USENIX Conf Hot Topics Cloud Comput  2010 p 10  W Lu Y Shen S Chen and B C Ooi Efﬁcient proces sing of k nearest neighbor joins using mapreduce Proc VLDB Endowment  vol 5 no 10 pp 1016–1027 2012  T Kanung o D M Mount N S Netanya hu C D Piatko R Silverman and A Y Wu An efﬁcient k-means clustering algorithm Analysis and implementation IEEE Trans Pattern Anal Mach Intell  vol 24 no 7 pp 881–892 Jul 2002  A K Jain Data clustering 50 years beyond k-means Pattern Recog Lett  vol 31 no 8 pp 651–666 2010  D Arthur and S Vassilvitskii  k-means  The advantages of careful seeding in Proc 18th Annu ACM-SIAM Symp Discr Algorithms  2007 pp 1027–1035  J Leskovec A Rajaraman and J D Ullman Mining Massive Datasets  Cambridge U.K Cambridge Univ Press 2014  A Stupar  S Mich el and R Schen kel Rankred uce–pr ocessin g k-nearest neighbor queries on top of mapreduce in Proc 8th Workshop Large-Scale Distrib Syst Informa Retrieval  2010 pp 13–18  B Bahmani A Goel and R Shinde Efﬁcient distributed locality sensitive hashing in Proc 21st ACM Int Conf Inform Knowl Manag  2012 pp 2174–2178  R Panigrahy Entropy based nearest neighbor search in high dimensions in Proc 17th Annu ACM-SIAM Symp Discr Algorithm  2006 pp 1186–1195  A Z Broder M Charikar  A M Frieze and M Mitzenma cher Min-wise independent permutations J Comput Syst Sci  vol 60 no 3 pp 630–659 2000  L Cristofor ARtool Association rule mining algorit hms and tools 2006  S Agrawal V Narasayya  and B Yang Integrating vertical and horizontal partitioning into automated physical database design in Proc ACM SIGMOD Int Conf Manag Data  2004 pp 359–370  F Chang J Dean S Ghema wat W Hsieh D Wallach  M  Burrows T Chandra A Fikes and R Gruber Bigtable A distributed structured data storage system in Proc 7th Symp Operating Syst Des Implementation  2006 pp 305–314  B F Cooper R Ramakrishn an U Srivastava A Silberstein P Bohannon H.-A Jacobsen N Puz D Weaver and R Yerneni Pnuts Yahoo!’s hosted data serving platform Proc VLDB Endowment  vol 1 no 2 pp 1277–1288 2008  J Xie and X Qin The 19th heterogenei ty in computing workshop HCW 2010 in Proc IEEE Int Symp Parallel Distrib Process Workshops Phd Forum  Apr 2010 pp 1–5  M Y Eltabakh Y Tian F  Ozcan R Gemulla A Krettek and J McPherson Cohadoop Flexible data placement and its exploitation in hadoop Proc VLDB Endowment  vol 4 no 9 pp 575 585 2011  R Vernica A Balmin K S Beyer and V Ercegovac Adaptive mapreduce using situation-aware mappers in Proc 15th Int Conf Extending Database Technol  2012 pp 420–431  Q Ke V Prabhakar an Y Xie Y Yu J Wu and J Yang Optimizing data partitioning for data-parallel computing uS Patent App 13/325,049 Dec 13 2011  M Liroz-Gis tau R Akbarinia D Agrawal E Pacitti  and P Valduriez Data partitioning for minimizing transferred data in mapreduce in Proc 6th Int Conf Data Manag Cloud Grid P2P Syst  2013 pp 1–12  T Kirsten L Kolb M Hartung A Gro H K  opcke and E Rahm Data partitioning for parallel entity matching Proc VLDB Endowment  vol 3 no 2 pp 1–8 2010  S Kotoulas E Oren and F Van Harmelen Mind the data skew Distributed inferencing by speeddating in elastic regions in Proc 19th Int Conf World Wide Web  2010 pp 531–540  L Li and M Zhang The strategy of mining associat ion rule based on cloud computing in Proc Int Conf Bus Comput Global Inform  2011 pp 475–478  S Groot K Goda and M Kitsuregawa  Towards improv ed load balancing for data intensive distributed computing in Proc ACM Symp Appl Comput  2011 pp 139–146  M Z Ashra D Taniar and K Smith ODAM An optimiz ed distributed association rule mining algorithm IEEE Distrib Syst Online  vol 5 no 3 p 1 Mar 2004 Yaling Xun is currently a doctoral student at Taiyuan University of Science and Technology She is currently a lecturer in the School of Computer Science and Technology Taiyuan University of Science and Technology Her research interests include data mining and parallel computing XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 113 


Jifu Zhang received the BS and MS degrees in computer science and technology from the Hefei University of Tchnology China and the PhD degree in pattern recognition and intelligence systems from the Beijing Institute of Technology in 1983 1989 and 2005 respectively He is currently a professor in the School of Computer Science and Technology TYUST His research interests include data mining parallel and distributed computing and artiﬁcial intelligence Xiao Qin received the PhD degree in computer science from the University of Nebraska-Lincoln in 2004 He is currently a professor in the Department of Computer Science and Software Engineering Auburn University His research interests include parallel and distributed systems storage systems fault tolerance real-time systems and performance evaluation He received the U.S NSF Computing Processes and Artifacts Award and the NSF Computer System Research Award in 2007 and the NSF CAREER Award in 2009 He is a senior member of the IEEE Xujun Zhao received the MS degree in computer science and technology in 2005 from the Taiyuan University of Technology China He is currently working toward the PhD degree at Taiyuan University of Science and Technology His research interests include data mining and parallel computing  For more information on this or any other computing topic please visit our Digital Library at www.computer.org/publications/dlib 114 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


