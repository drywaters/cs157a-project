Coimbatore.        Coimbatore  Department of Computer Science   Department of Computer Science C.Sivamathi M.Sc., M.Phil     Dr. S.Vijayarani MCA., M.Phil/, Ph.D c.sivamathi@gmail.com      vijimohan_2000@yahoo.com  PhD Research Scholar,        Assistant Professor Bharathiar University Bharathiar University Keywords Data mining; Utility mining; High utility itemset mining; Utility mining algorithms   I  I NTRODUCTION  Data mining is the process of extraction of implicit information and knowledge which are previously unknown and potentially useful  2    Data mining analyzes large volumes of data and automatically discovers interesting and previously unknown relationships among data. This helps in better understanding of the underlying processes Data mining is a combination of statistics, artificial intelligence and machine learning  Util it y  m i ning is one of the fields in data mining. Utility mining is defined as discovery of itemsets whose utility value go beyond user mentioned minimum utility threshold The concept of utility mining comes from frequent itemset mining [3 ent ite m s et m i n i n g  reveals frequent items in a database, which does not concentrates on significant importance of that purchased item Con s ider an electron ic su per store. Assume that the profit of a mobile phone is 5000 INR and the profit of a memory card is 15 INR In a transaction database, memory card occurs in 10 transactions and mobile phone occurs in 3 transactions. The total profit of memory card is 150 INR and the total profit of a mobile phone is 15000 INR. As per frequent itemset mining memory card has high frequency. But the total profit of a mobile phone is much greater than a memory card. Hence traditional frequent itemset mining cannot discover the most important itemsets.  This is because frequent itemset mining does not consider the profit \(i.e utility\ an item, which is also highly important in decision making. As per utility mining by considering profit mobile phone has high utility The inputs of utility mining algorithms are a transaction database, utility values of items and minimum utility threshold. Utility mining algorithms work as follows   First th e ite m s et s are generated. Then Utility values of all itemsets are calculated.  Now this utility values is compared with user given minimum utility threshold. If the itemset utility is greater than minimum threshold, then such items are defined as high utility itemsets   1  Performance Analysis ofUtility Mining algorithms Utility Mining is one of the recent emerging fields in Data mining. The main objective of utility mining is to discover high utility itemsets from a database. It differs from traditional frequent itemset mining. In frequent itemset mining, frequently occurred items in database are found. But in Utility mining itemsets with high utility utility here refers profit number of items, cost of an item or any user favorite value\re retrieved.  Hence utility mining retrieves semantic correlation among items in a database. This helps in better decision making for target markets cross selling etc. In recent years numerous algorithms like Two-Phase, UP-Growth+  , EFIM algorithm, FHM algorithm , HUI-Miner algorithm , IHUP, d2HUP are proposed to find high utility itemsets. These algorithms calculate only positive utility values. There are also algorithms like FHN, HUINIV to calculate high utility with negative values. In this paper performance of FHN and HUINIV are compared and experimental results are discussed Abstract 


II  P RELIMINARY DEFINITIONS  The following are some of the preliminary definitions for discovering high utility itemsets[4     Definition 1  Definition 6 Definition 7 in_Util where Min_Util is user specified minimum utility threshold   III A. FHM - Faster High-Utility Itemset Mining using Estimated Utility Co-occurrence Pruning Philippe Fournier-Viger, Cheng-Wei Wu SouleymaneZida, Vincent S.Tseng developed FHM  It is based on HUI miner algorithm The algorithm first calculates TWU of items and arranges them in ascending order.  Then a novel structure Estimated Utility Co-Occurrence Structure i.e EUCS was build  All u t ilit y  m e as u r es are  stored in this structure and high utility itemsets are retrieved. The following is pseudo code for FHM     Pseudocode1. FHM algorithm  B. EFIM - Efficient high-utility Itemset Mining SouleymaneZida, Philippe Fournier-Viger Jerry Chun-Wei Lin,Cheng-Wei Wu, Vincent S Tseng proposed EFIM[8 Th is al g o rithm  u s e s tw o  upper-bounds - sub-tree utility and local utility.It also proposed an array-based utility counting technique called Fast Utility Counting to calculate upper bounds Definition 2:  The external utility of an item i p is transaction independent numerical value, defined by the user. It reflects importance of the item. It is common practice to choose profit as external utility External utilities are stored in a separate utility table  Definition 3:  Utility function f is a function of two utilities. In general the product of internal and external utilityis consider ed as utility function  Definition 4: The utility of item i p in transaction T is the calculated using utility function Utility of an item in a particular transaction = product of its internal utility in that transaction and its external utility  Definition 5: The utility of itemset S in   S,S The internal utility of an item i p is a transaction dependent numerical value. In general the quantity of an item in transaction is taken as internal utility The utility of itemset S in database DB is defined as  Itemset S is said to be high utility itemset if and only if  U\(S   Fig 1. Utility mining process  2     Definition 8 H IGH U TILITY M INING A LGORITHMS ip The utility of transaction T is defined as u\(T\ = Pip T   T  T u\(ip, T DB, S T 


Pseudocode2. EFIM algorithm C. HUI-Miner - High Utility Itemset Miner  MengchiLiu  andJunfengQu developed this   T h is algorithm  doe s n o t g e nerate s an y candidate itemsets. . HUI-Min er uses a structure known as utility-list, which stores utility details of an itemset  Pseudocode3. HUI miner algorithm D. UP-Growth - Utility Pattern Growth  Vincent S. Tseng, Cheng-Wei Wu, Bai-En Shie , and Philip S. Yu proposed UP Growth   T h e in f o r m ation of h i gh u til it y  itemsets are stored in a d ata structure called UP-Tree i.eUtility Pattern Tree. Here candidate itemsets are generated by two scans of the database E.D2HUP Direct Discovery of High Utility Patterns Junqiang Liu and Benjamin proposed D2HUP  T h is algorit hm is an integration of the depth-first search of the reverse set enumeration tree with some pruning techniques   Pseudocode D2HUP algorithm   III  E XPERIMENTAL E VALUATION   To evaluate the performance of various algorithms, an experiment is conducted with a RETAIL DATASET. It is a benchmark dataset. It contains retail market basket data from an anonymous Belgian retail store. It contains 541909 instances with 8 attributes. All the algorithms was implemented in Java and executed in a machine with 3.20 GHz CPU. In this experiment the algorithm was executed with 1000, 2000, 3000, 4000 and 5000 transactions.  Candidates co unt, execution time and number of Itemsets retriev ed for each algorithm are compared. Also the numbers of items with various minimum threshold values are compared The following figures shows the performance of various high utility mining algorithms.  Figure 2 shows Comparison of execution time \(in milli seconds\memory used \(in Kilobytes\y various utility mining algorithms.  From this figure, D2HUP algorithm, has less execution time and HUI miner algorithm has occupied less memory space. Figure 3 shows the comparison of number of candidates items generated at various minimum utility threshold like 1000, 2000, 3000 and 4000 


In Proceedings of the Utility-Based Data Mining Workshop 7   Vincent T., Bai-En Shie ; Cheng-Wei Wu ; Philip S YuEfficient Algorithms for Mining High Utility Itemsets from Transactional Databases    Eighth IEEE International Conference on Data Mining 6   SouleymaneZida, Philippe Fournier-Viger, Jerry ChunWei Lin,Cheng-Wei Wu, Vincent S. Tseng In book: Advances in Artificial Intelligence and Soft Computing, pp.530-546 Data mining Techniques 9   R. Agrawal and R. Srikant \(1994 1257     A text book published by University Press \(India\ Private limited, pp -42 3   2012    Intell. Syst., 8502 \(2014 pp. 83 H.F. Li, H.Y. Huang, Y.Cheng Chen, and Y. Liu and S Lee. \(2008  Fig 2. Comparison of execution time and memory used by utility mining algorithms   Fig 3. Comparison of number of candidates items generated in utility mining algorithms C ONCLUSION  In Data Mining, Utility mining is gaining more research in recent years. It is originated from frequent itemset mining only. The basic difference is frequent itemset mining does not concentrates on profit number of items purchaed or cost of an itemset. In real life applications there may be some items, which may not be frequent but may have high profit. Utility Mining mines such high utility itemsets in a transaction database. It is very beneficial in several real-life applications. In this paper a brief overview of various algorithms for high utility itemset mining was presented and those algorithms are compared with various performance factors like Execution time memory used and number of candidates generated  References 1   A Fast High Utility Itemsets Mining Algorithm 8  EFIM: A Highly Efficient Algorithm for High-Utility Itemset Mining  3324 5   Junqiang Liu and Benjamin Fung, Mining high utility patterns in one phase without generating candidates IEEE transaction on knowledge and data engineering volume 28, page no: 1245 FHM: faster high-utility itemset mining using estimated utility co-occurrence pruning in Proceedings of the 20th International Conference Very Large Databases, pp 487-499 4   Fast Algorithms for Mining Association Rules  Pattern Recognition. 3317 Arun K. Pujari \(2001 Mengchi Liu JunfengQu  Liu, Y., Liao, W., and A. Choudhary, A. \(2005 without Candidate Generation J. Hu, A. Mojsilovic \(2007 P. Fournier-Viger, C.W. Wu, S. Zida, V.S. Tseng R   Agrawal,T Imielinski,   A   Swami.   Database Mining: A Performance Perspective.  IEEE Transactions on Knowledge and Data Engineering 1993,12:914 -925 2   92 Fast and Memory Efficient Mining of High Utility Itemsets in Data Streams High-utility pattern mining: A method for discovery of high-utility item sets 


candidate k-itemset’s support count is updated and pruned from k C if support count is less than |DB´| x s.The support count of each new candidate k-itemset is estimated using the principle of maximum possible value. Since a new candidate 1-itemset is not in DB k F DB k PF its support count in an original database is at best equal to  1 where  is a minimum support count of the original database. The new candidate k-itemset, whose support count is less than |DB´| x s, are pruned from new k C because such a candidate k-itemset has no chance to be a frequent itemset  Fig. 3  Updating support count and frequent itemset procedure Then is scaned to find   x and calculate x   for each remaining candidate k-itemset, i.e. C k new k C If a remaining candidate k-itemset is the memb er of k-itemsets obtained from prior-updating procedure, its su pport count can easily be updated. The k-itemset whose support count satisfies the minimum support threshold is then a frequent itemset. For the k-itemset whose support count lower than the minimum support threshold, the probability of an itemset to be a frequent itemset in the updated database  x  P    will be calculated Infrequent itemsets that have  x  P    greater than Prob pl are treated as prospective frequent itemsets Contrarily, the remaining candidate k-itemset is a new candidate k-itemset. The support count of a new candidate kitemset is estimated. The new candidate 1-itemset, whose support count is greater than or equal to |DB´| x s, are moved to Temp_scanDB. The original database is scanned only if the number of itemsets in Temp_scanDB is greater than  F P F  k  k   015 where 015 is a threshold specified by users Otherwise, the new candidate k-itemsets in Temp_scanDB will be kept. The procedure is re peated until no candidate kitemsets can be generated. The proposed algorithm can thus find all frequent k-itemsets for the entire updated database  Fig. 4  Scanning an original database procedure IV  EXPERIMENT  We used a synthetic dataset called T10I4D100K The synthetic dataset comprises 100,000 transactions over 140 unique items. The performance of proposed algorithm is compared with Apriori and those algorithms which handle both case of record insertion and deletion, i.e. FUP2, pre-large and EDUA algorithm. However, since pre-large algorithm cannot deal with the case of insertion and deletion simultaneously, we first perform the case of deletion then follow by the case of insertion by using algorithm in  and [7 respectively The accuracy of frequent itemsets obtained from these algorithms is checked with those obtaine d from Apriori algorithm For the first experiment, 20,000 transactions are inserted into and 10,000 transactions ar e deleted from an original database of 100,000 transactions The support thresholds are varied between 0.01 and 0.05 with prob pl 0.01. The execution time versus minimum support is shown in Fig. 5 For the second experiment, the impact of the incremental size is tested using several size   and  As a preliminary experiment, the experiment is conducted only when the size of   is twice as the size of   with a support threshold 0.05. The execution time of each algorithm is shown in Fig. 6 According to Fig. 5 and 6, the execution time of the proposed algorithm is faster than that of Apriori, Pre-large and  Algorithm 2: Updating support count and frequent itemsets Input      DB k F  DB k PF s Output x   Temp_scanDB 1  scan   to find out   x for each X in db k C  2  for each itemset db k C X   3  if DB k DB k db k PF F C    4  move x to new k C  5  for each itemset DB k DB k PF F X    6  if db k C x   7          x x x  8  else x x      9  if x   DB´| x s 10  remove X from C k  11  for each itemset X  new k C  12  if   x   1\< |DB´| x s 13  remove X from new k C  14  scan  to find out   x  for each X in new k db k C C   15  for each itemset X  C k and X  DB k F DB k PF  16           x x x  17  for each itemset X  new k C  18  if      x x   1 DB´| x s 19  move X to Temp_scanDB 20   k F X  C k  x    DB´| x s 21  k k k F X  C X  F P      and pl prob  x  P      Algorithm 3 Scanning an original database Input   x      x    x   k F   k PF Temp_scanDB, s Output  k F   k PF  1  scan DB to find out x  for each X in Temp_scanDB 2  for each itemset X  Temp_scanDB 3             x x x x  4  new k F X  Temp_scanDB  and  x    DB´| x s 5  new k k new k F X  C X  PF    and pl prob  x  P      6   k F   k F new k F  7   k PF   k PF new k PF  8  clear Temp_scanDB 2016 8th International Conference on Information Technology and Electrical Engineering \(ICITEE 


EDUA algorithms but slower than FUP2 and Prob-based algorithms. This is such a case because  of  calculating probability in the proposed algorithm becomes extremely laborious as n increases due to large value of n  Besides, the proposed algorithm calculates  x  P    for each and every infrequent itemset which has poten tial to be frequent itemset in updated database. Thus, approx imation and infrequent itemsets pruning techniques are needed for further improvement  In this paper, the performance of EDUA algorithm is relatively insufficient because the algorithm is based on the assumption that the number of candidate itemsets is not much larger than frequent itemsets But in the experiments with a support threshold 0.05, the num ber of candidate itemsets is 14,023 while the number of frequent itemsets is only 199       1 10 100 1000 10000 100000 0.01 0.02 0.03 0.04 0.05 Apriori FUP2 Pre-large EDUA Prob-based Proposed  Fig. 5  The execution time \(sec   20K  10K and |DB|=100K transactions 1 10 100 1000 10000 10 20 30 40 Apriori FUP2 Pre-large EDUA Prob-based Proposed  Fig. 6  The execution time \(sec      TABLE V  THE NUMBER OF CANDIDATE ITEMSETS RESCANNED IN THE ORIGINAL DATABASE  k-itemset k Apriori FUP2 Prelarge EDUA Probbased Proposedalgorithm 1 144 6 48 75 0 0 2 4278 24 4230 2172 5 15 3 979 1 1051 169 1 2 4 128 0 9 15 0 0 5 61 0 0 6 0 0 6 17 0 0 0 0 0 7 2 0 0 0 0 0 Total 5680 31 5338 2437 6 17  However, the proposed algorithm can prune away efficiently new candidate itemsets if they ar e not potential frequent itemsets. Thus, the number of candidate itemsets need to be rescanned from the original database generated by the proposed algorithm is less than that generated by Ariori, FUP2 Pre-large and EDUA algorithm as shown in Table V In order to evaluate the predi ction accuracy, the Precision Re\ia are used. The result shows moderate accuracy of the proposed algorithm with precision=0.33, recall=1.00, and F1=0.50 V  CONCLUSIONS  AND FUTURE WORK  In this paper, we have proposed an incremental maintenance that can handle case of record insertion and deletion simultaneously. Th e proposed algorithm predicts prospective frequent itemsets by using random walk process and update frequent itemsets when transactions are added or deleted from database. The experimental results show that the execution time of the proposed algorithm is faster than that of Apriori algorithm but slower than FUP2 algorithm. It is quite obvious that calculating probab ility in the proposed algorithm becomes extremely laborious as n increases due to large value of n! Hence, work is going on for further improvement in our future work   R EFERENCES  1  Agrawal R and Srikant R \(1994\m for mining association rules in large database, Proceedings of 20th International Conference on Very Large Data Bases,  pp.487-499 2  Cheung DW, Han J, Ng VT, and Wong CY \(1996\, Maintenance of discovered association rules in large database: an incremental updating technique, Proceedings of 12th IEEE International Conference on Data Engineering, pp. 106-114 3  Cheung DW, Lee SD, and Kao B \(1997\eral incremental technique for maintaining discovered association rules, Proceedings of the 5th International Conference on Database Systems for Advanced Applications, pp. 185-194 4  Zhang S, Zhang J, and Zhang C \(2007\An efficient algorithm for dynamic database mining, Information Science Journal, Vol. 177 pp.2756-2767 5  Chen M, Han J, and Yu PS \(1996\ overview from database perspective, IEEE Transactions on Knowledge and Data Engineering, Vol. 8, pp. 866-883 6  Lee C, Lin CR, Chen MS \(2001\liding-window filtering: an efficient algorithm for incremental mining, in: Proceedings of International Conference on Information and Knowledge Management, CIKM01, pp 263–270 7  Hong TP, Wang CY, and Tao YH \(2001\, A new incremental data mining algorithm using pre-large itemsets, Intelligent Data Analysis Vol. 5, pp. 111–129 8  Hong TP and Huang TJ \(2007\, Maintenance of generalized association rules for record deletion based on the pre-Large concept, Proceedings of the 6th WSEAS Int. Conf. on Artificial Intelligence, Knowledge Engineering and Data Bases 9  Amornchewin R and Kreesuradej W \(2009\, Mining dynamic database using probability-based incremental association rule discovery algorithm, Journal of Universal Com puter Science, Vol. 15, no. 12, pp 2409-2428 10  Thusaranon PR and Kreesuradej W \(2015\ probability-based incremental association rule discovery algorithm for record insertion and deletion, Journal of Artificial Life and Robotics, Vol. 20, no. 2, pp. 115123   11  http://fimi.ua.ac.be/data  2016 8th International Conference on Information Technology and Electrical Engineering \(ICITEE 


 International Conference on Computing, Communication and Automation \(ICCCA2016   93     Fig. 7  Execution time of trie based Apriori on three different block distributions x-none In Fig   7, BD1 exhibits minimum execution time compared to BD2 and BD3 In BD1 blocks are located only on three DNs i.e two physical a nd one VMs DN So in this case Mappers are not running on both slower DNs tha t make the execution faster. Execution time for BD2 is poor than that o f BD1 due to using both VMs DNs  BD3 exhibits the worst performance since all the blocks are available on each DN MapReduce processes a data block locally on the DN where the block is present. F or block distribution BD3, all Mappers are running on the same DN since all blocks are locally available to each node In different attempt of running a job DN may be d ifferent each time but all the Mappers are being run on a same DN. All Mapper s running on the same DN does not make use of available resources which leads to increased execution time Here it can be seen that due to higher replication factor data locality may be a hurdle that slow down the execution  x-none E  Controlling Parallelism with Split Size x-none Hadoop is designed to process big datasets that does not mean one cannot be benefited for small datasets. Apriori is a CPU-intensive algorithm and consumes a significant time for smaller datasets. To reduce the execution time we need more than on e task running in parallel Split is used to control the number of map tasks for a MapReduce job A split may consist of multiple blocks and there may be multiple splits for a single block So without changing the block size user can control the number of Mappers to be run for a particular job We have used the method setNumLinesPerSplit\(Job job int numLines  of class NLineInputFormat  from MapReduce library to set the number of lines per split. In our earlier cases we were running multiple Mappers against different parts of the same block Here we set the split size 5K lines on block distribution BD3 which contains 5 data blocks This creates 12 splits i.e 12 Mappers  size for BD3 the n  blocks are considered as input s plits. Fig   8 shows the difference in execution time for these two cases  Here it can be seen that how the split size controls the parallelism Smaller split size launches more number of Mappers which consequently increase the parallelism. It does not mean that more number of Mappers always results into better performance. Increasing the number of Mappers beyond a particular point starts to degrade the performance due to unnecessary overheads and shortage of resources   To achieve the right level of parallelism it must be taken care that the map task is CPU-intensive or CPU-light as well as the size of dataset to be processed Fig. 8  Execution time of trie based Apriori with Input Split and without Input Split on BD3 x-none F  Issues Regarding MapReduce Implementation The efficiency of an algorithm running as a MapReduce job is extensively influenced by data structure used and algorithm itself A third factor that cannot be ignored is the implementation technique.  Implementation technique may be regarding to implementation of various modules of Apriori e.g candidate generation support counting of candidates against each transaction pruning of infrequent itemsets or regarding to MapReduce implementation of Apriori MapReduce implementation of Apriori is central to discussion here. A major issue in MapReduce based Apriori is to invoke candidate generation i.e apriori-gen  Algorithm 7 at appropriate place inside Mapper class. In our implementations we have invoked apriori-gen  inside customized method map of Mapper class. In Mapper class, two methods setup  and map  are customized and one method apriori-gen  is defined Method setup  is called once at the beginning of a task It is customized to read frequent itemsets of previous iteration from distributed cache and to initialize prefix tree Method apriori-gen  generates candidates using prefix tree containing frequent itemsets The map  is invoked for each line of input split of dataset If there are 100 lines of input assigned to a Mapper then map method will be invoked for 100 times Subsequently it invokes apriori-gen  repeatedly each time Since apriori-gen  method produces candidates which is independent of input instance, so need not to invoke repeatedly inside map  method The apriori-gen  metho d is  computation intensive and increases the execution time when invoked repeatedly. This repeated computation can be fixed if we invoke apriori-gen  outside of map  Theoretically it sounds good but did not work when invoked inside setup  method We have also tried another way in which apriorigen  is invoked inside overrided method run  of Mapper class but again could not achieve expected reduction in execution time VI  C ONCLUSIONS  In this paper we have investigated a number of factors affecting the performance of MapReduce based Apriori algorithm on homogeneous and heterogeneous Hadoop 


 International Conference on Computing, Communication and Automation \(ICCCA2016   94   cluster and presented strategies to improve the performance It has been shown that how hash table trie data structure and transaction filtering technique can significantly enhance the performance Factors like speculative execution physical  VMs DataNodes, data locality block distribution and split size are such that their proper tuning can directly enhance the performance of a MapReduce job even without making algorithmic optimization Approaches of MapReduce implementation of Apriori is another important factor that also influence the performance R EFERENCES  1  J. Han and M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers 2006  2  J S Ward a  b y d ata a survey of big d ata d  http://arxiv.org/abs/1309.5821v1 3  Apache Hadoop, http://hadoop.apache.org 4  Big data is useless without algorithms Gartner says http://www.zdnet.com/article/big-datais useless-withoutal gorithmsgartner-says/, Retrieved Nov. 2015 5   algorithms for mining association rules  Proceedings Twentieth International Conference on Very Large Databases, Santiago 1994 pp. 487 499 6   and distributed association mining a survey Concurrency, IEEE, vol 7, no. 4,pp. 14 25, 1999 7  K. Bhaduri, K. Das, K. Liu, H. Kargupta and J. Ryan, Distributed Data Mining Bibliography 2008  8  HDFS  Architecture Guide https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html Retrieved Sept 2015  9  MapReduce Tutorial http://hadoop.apache.org/docs/current/hadoopmapreduce-client/hadoop-mapreduce-clientcore/MapReduceTutorial.html, Retrieved Sept. 2015 10  Yahoo Hadoop Tutorial http://developer.yahoo.com/hadoop/tutorial/index.html 11   ACM SIGOPS Operating Systems Review vol 37 no 5 pp 29 43 2003  12    Commun., vol. 51, pp 107 113, 2008 13   mining using clouds an experimental implementation of apriori over mapreduce    14   Apriori: association rules algorithm based on mapreduce  IEEE 2014  15   as a programming model for association rules algorithm on hadoop  nternational Conference on Information Sciences and Interaction Sciences ICIS 2010 vol. 99, no. 102, pp. 23 25  16   implementation of apriori algorithm based on mapreduce  h ACIS International Conference on  Software Engineering Artificial Intelligence Networking and Parallel  Distributed Computing IEEE 2012 pp 236 241  17   Hadoop as a platform for distributed association rule mining   COMPUTING 2013 the Fifth International Conference on Future Computational Technologies and Applications, pp. 62 67  18  M-Y Lin P-Y Lee and S based frequent itemset mining algorithms on mapreduce in  Proceedings 6th International Conference on  Ubiquitous Information Management and  2012, Article 76 19   itemset mining on Hadoop  Proceedings IEEE 9th International Conference on Computational Cybernetics \(ICCC\Hungry, 2013, pp. 241 245  20   strategy of mining association rule based on cloud computing   Proceedings IEEE International Conference on Business Computing and Global Informatization BCGIN 2011 pp 29 31 21  Honglie Yu, Jun Wen, Hongmei Wang and Li Jun, "An improved apriori algorithm based on the boolean matrix and Hadoop Procedia Engineering 15 \(2011\1827-1831, Elsevier 22  Matteo Riondato, Justin A. DeBrabant, Rodrigo Fonseca and Eli Upfal PARMA: a parallel randomized algorithm for approximate association rules mining in mapreduce in Proceedings 21st ACM international conference on information and knowledge management 2012 pp 8594  23  Jiong Xie et al Improving mapreduce performance through data placement in heterogeneous hadoop clusters in IEEE International Symposium on Parallel & Distributed Processing,  Workshops and Phd Forum \(IPDPSW\ 2010, pp. 19  24  Matei Zaharia Andy Konwinski Anthony D Joseph Randy Katz and Ion Stoica Improving mapreduce performance in heterogeneous environments in 8th USENIX Symposium on Operating Systems Design and Implementation \(OSDI\ 2008, vol. 8, no. 4, pp. 2942  25  Hsin-Han You, Chun-Chung Yang and Jiun-Long Huang, "A load-aware scheduler for MapReduce framework in heterogeneous cloud environments in Proceedings of the ACM Symposium on Applied Computing, 2011, pp. 127-132 26  Faraz Ahmad Srimat Chakradhar Anand Raghunathan and T N Vijaykumar, "Tarazu optimizing MapReduce on heterogeneous clusters," ACM SIGARCH Computer Architecture News, vol. 40, no. 1 pp. 61-74, 2012 27  HADOOP PERFORMANCE TUNING white paper Impetus Technologies Inc October 2009 https://hadooptoolkit.googlecode.com/files/White%20paperHadoopPerformanceTuning.pdf 28  Apache Hadoop NextGen MapReduce YARN http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarnsite/YARN.html, Retrieved Sept. 2015 29  SPMF Datasets http://www.philippe-fournierviger.com/spmf/index.php?link=datasets.php 30   ata structure for data  in Mathematical and Computer Modelling, vol 38  no. 7, pp. 739-751, 2003 31  Ferenc Bodon A fast apriori implementation in Proceedings IEEE ICDM workshop on frequent itemset mining implementations  90, 2010 32  Sudhakar Singh, Rakhi Garg and P. K. Mishra  analysis of apriori algorithm with different data structures on hadoop cluster  International Journal of Computer Applications, vol. 128, no. 9, pp. 4551  2015  33  Christian Borgelt Efficient implementations of apriori and 351clat in Proceedings IEEE ICDM workshop on frequent itemset mining   34  Ferenc Bodon, "Surprising results of Trie-based fim algorithms," FIMI 2004  35  Ferenc Bodon A trie-based APRIORI implementation for mining frequent item sequences," in Proceedings 1st international workshop on open source data mining: frequent pattern mining  implementations ACM, 2005 36  Hadoop Wiki Virtual Hadoop https://wiki.apache.org/hadoop/Virtual%20Hadoop  


002 004\002 005\002 006\002 007\002 003\002 033\002 034\002 035\002 002\004\002\005\002 006\002 
002 005 007 033 035 004\002 004\005 004\007 004\005\006\007\003\033 
C Increased Rule Length 
002 005 007 033 035 004\002 004\005 004\007 004\033 004\035 005\002 020\020\030\027\011\025\021\013 026\032\025\025\011\020\021\030\032 025 011\021\012\030\017 023\004\002\002 023\004\002\004 
 
002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 
Figure 6 Throughput for increasing length rule candidates The Y-axis is measured in billion evaluations per second The X-axis shows the corresponding rule length As before the gures include the theoretical trend-line computed from amplifying naive-sc on the GPU using Eq 4 reason the observed improvement is stable depending mostly on the item number Assigning multiple candidate rule collections to a single block resulted in Figure 7 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using synthetic data   Figure 8 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using real data For both kernels we observe a similar behaviour when we increase rule candidate length to a number larger than 32 After that point we require evaluating the pre“x in two phases following a technique similar to parallel reduction although using warp vote functions This extra phase requires an additional synchronization step which increases the total execution time per iteration Additionally when we have prominent rules with item indices in sequence i.e accidents dataset as indicated by its sparsity pattern caching transactions does not provide any improvement However when there are many rules with out of sequence pre“xes the cost of uncoalesced memory accesses matches the synchronization cost as indicated by experiments on dataset 1 VIII C ONCLUSION In this paper we studied the support count operation commonly used in association rule mining problems We proposed a work-ef“cient parallel algorithm that is suitable for massively parallel architectures Furthermore we presented a data layout scheme used to enable low overhead coordination of the processing elements reduce the memory requirements and achieve high off-chip memory bandwidth utilization Furthermore we discussed in detail low level optimization strategies related to effective use of shared memory while presenting a simple strategy for resolving shared memory bank con”icts incurring minimal additional work However there is still some additional issues that we need to address Firstly we already considering resolving the issue of 
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
improvement over the default-tpsc execution time A combination of loop unrolling and increase shared memory utilization from storing more candidate rules in the same block was the reason for the observed improvement In contrast enabling caching of transactions in shared memory with kernel mrs-tpsc presented less improvement in the relative execution time compared to mr-tpsc The culprit is this case is the additional synchronization step which is required after loading the data in shared memory Finally experiments performed on dataset 2 and 4 indicate similar behavior to our previous experiments where multiprocessor underutilization was limiting the maximum possible performance increase Even in the case where we increase the workload of participating blocks interleaved execution of warps is limited since as the block size is small In this section we discuss the effects of discovering rules with length larger than 32 Due to lack of space we present the results from the execution on synthetic data 1 and accidents which are good representatives of the observed behaviour We focus on the mrs-tpsc and mr-tpsc variations which we established to be highly optimized throughout our experiments   
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
037\005\005\010 \004\012\007!\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002 006\002 011\012\012\004\005\007\010\011\012\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002\006\002 004\007\024\010$\013  014%\031&&\013 
18 
027\011$\012\014\017\021\036\021\031\013\020  025\012%&\011\036\013\020\022\036\022'\031\014  025\012%&\011\036\013\020\022\036\022\020\031\014 021\(\011\032\015\011\021\030\020    002 004 005 006 007 002\004\002\005\002 006\002 
1431 
1431 


 volume 22 pages 207…216 ACM 1993  R Agra w al R Srikant et al F ast algorithms for mining association rules In  volume 1215 pages 487…499 1994  E Ansari G Dastghaibif ard M K eshtkaran and H Kaabi Distrib uted frequent itemset mining using trie data structure  35\(3 2008  M Atzmueller and F  Puppe Sd-map…a f ast algorithm for e xhausti v e subgroup discovery In  pages 6…17 Springer 2006  C Creighton and S Hanash Mining gene e xpression databases for association rules  19\(1 2003  W  F ang M Lu X Xiao B He and Q Luo Frequent itemset mining on graphics processors In  pages 34…42 ACM 2009  K Geurts G W ets T  Brijs and K V anhoof Pro“ling of high-frequenc y accident locations by use of association rules  1840 2003  A Ghoting G Buehrer  S P arthasarathy  D Kim A Nguyen Y K Chen and P Dubey Cache-conscious frequent pattern mining on modern and emerging processors  16\(1 2007  G Grahne and J Zhu Ef ciently using pre“x-trees in mining frequent itemsets In  volume 90 2003  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation In  volume 29 pages 1…12 ACM 2000  J Hipp U G  untzer and G Nakhaeizadeh Algorithms for association rule mining„a general survey and comparison  2\(1 2000  R Jin and G Agra w al An algorithm for in-core frequent itemset mining on streaming data In  pages 8…pp IEEE 2005  R Jin and G Agra w al Systematic approach for optimizing comple x mining tasks on multiple databases In  pages 17…17 IEEE 2006  E Lindholm J Nick olls S Oberman and J Montrym Nvidia tesla A uni“ed graphics and computing architecture  2 2008  J Liu Y  P an K W ang and J Han Mining frequent item sets by opportunistic projection In  pages 229…238 ACM 2002  L Liu E Li Y  Zhang and Z T ang Optimization of frequent itemset mining on multiple-core processor In  pages 1275…1285 VLDB Endowment 2007  B Mobasher  R Coole y  and J Sri v asta v a Automatic personalization based on web usage mining  43\(8 151 2000  E  Ozkural B Ucar and C Aykanat Parallel frequent item set mining with selective item replication  22\(10 2011  J Pei J Han H Lu S Nishio S T ang and D Y ang H-mine Hyper structure mining of frequent patterns in large databases In  pages 441…448 IEEE 2001  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster  In  pages 467…473 Springer 2003  C Silv estri and S Orlando gpudci Exploiting gpus in frequent itemset mining In  pages 416…425 IEEE 2012  A T ajbakhsh M Rahmati and A Mirzaei Intrusion detection using fuzzy association rules  9\(2 2009  T  T assa Secure mining of association rules in horizontally distrib uted databases  26\(4 2014  K W ang M Stan and K Skadron Association rule mining with the micron automata processor In  2015  M J Zaki Scalable algorithms for association mining  12\(3 2000  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors In  pages 43…43 IEEE 1996  F  Zhang Y  Zhang and J D Bak os Accelerating frequent itemset mining on graphics processing units  66\(1 2013  Y  Zhang F  Zhang Z Jin and J D Bak os An fpga-based accelerator for frequent itemset mining  6\(1 2013 
002 004 005 006 007 002 004\005\035 005\003\033 006\035\007 003\004\005 002 033 004\005 004\035 005\007 002 004\005\035 005\003\033 006\035\007 003\004\005 
ACM SIGMOD Record Proc 20th int conf very large data bases VLDB IAENG International Journal of Computer Science Knowledge Discovery in Databases PKDD 2006 Bioinformatics Proceedings of the fth international workshop on data management on new hardware Transportation Research Record Journal of the Transportation Research Board The VLDB Journal FIMI ACM SIGMOD Record ACM sigkdd explorations newsletter Data Mining Fifth IEEE International Conference on Data Engineering 2006 ICDE06 Proceedings of the 22nd International Conference on IEEE micro Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining Proceedings of the 33rd international conference on Very large data bases Communications of the ACM Parallel and Distributed Systems IEEE Transactions on Data Mining 2001 ICDM 2001 Proceedings IEEE International Conference on Advances in Knowledge Discovery and Data Mining Parallel Distributed and Network-Based Processing PDP 2012 20th Euromicro International Conference on Applied Soft Computing Knowledge and Data Engineering IEEE Transactions on Proceedings of the 2015 IEEE 29th International Parallel and Distributed Processing Symposium Knowledge and Data Engineering IEEE Transactions on Supercomputing 1996 Proceedings of the 1996 ACM/IEEE Conference on The Journal of Supercomputing ACM Transactions on Recon“gurable Technology and Systems TRETS 
Figure 9 Execution time measured for increasing rule size  the Xaxis indicates the rule length multiprocessor under-utilization For dataset with low number of items we can assign individual groups of threads in the same block to different rule collections effectively increasing the block size as well as utilization Secondly we would like to adapt our solution to an architecture consisting of multiple GPUs and address challenges related to partial result sharing A CKNOWLEDGMENT This work was supported by the U.S National Science Foundation under grant ACI-1339756 R EFERENCES  Frequent itemset mining dataset repository  2015 URL http://“mi.ua.ac.be/data  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases In 
014\010\015\004\013\016!\004\005!\021\013 026\027\012\007\030\004\007\010\005\013\031\013 014\010\015\004\016!\004\005!\021\013 037\005\005\010 \004\012\007!\013 
015\036\021\031\013\020 015\013\036\021\031\013\020 
1432 
1432 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372…390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94…117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1…2:17 May 2013  P  Dlugosch  An ef“cient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Microns automata processor architecture Recon“gurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Recon“gurable Technol Syst et al IEEE TPDS Proc of IPDPS14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Ef“cient Accelerators and Recon“gurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


