html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">  1 Use of Evolutionary Computation for Isolating Surface Emissions from Orbit Michael A. Mischna, Seungwon Lee, Mark Allen and Richard J. Terrile Jet Propulsion Laboratory, California Institute of Technology 4800 Oak Grove Dr., M/S 183-401, Pasadena, CA 91109-8099 818-393-4775 michael.a.mischna@jpl.nasa.gov  Abstract  High precision targeting of localized sources of atmospheric species outgassed from planetary surfaces remains an important goal in planetary science, but one that largely remains beyond present-day observational capabilities. For disequilibrium trace species, the detectable signature of a gas in the atmosphere often is confined to a small region \(order 10-100  s km Identifying a plume source is challenging since the observed plume is likely many times larger than the surface source itself. Limited spatial coverage of the surface also may preclude direct detections of the plume surface source while the large fields of view of present-day orbital instruments tend to  blur  small-scale signals. To address these issues, we have developed a novel technique for determining the surface location of a trace gas emission source in a planetary atmosphere with an uncertainty of a few tens of km using present-day direct observational capabilities in concert with high-performance numerical modeling of atmospheric dynamics.1,2 This paper shows how evolutionary computational models ECMs of trace gas plumes in the atmosphere of Mars. We present the specific approach taken in linking an ECM to a simple idealized plume model, and gauge the overall ability of this system to return estimates of a plume source location over several generations of the model  s execution. This technique can isolate trace gas source locations with orders of magnitude greater efficiency than brute-force approaches, making identification of such sources, even with limited observational data, a plausible endeavor TABLE OF CONTENTS 1. INTRODUCTION AND MOTIVATION ..................................1 2. OBJECTIVE AND APPROACH............................................2 3. MODELING FRAMEWORK ................................................3 4. RESULTS ...........................................................................8 5. FUTURE PLANS ...............................................................11 6. CONCLUSIONS ................................................................11 7.  ACKNOWLEDGEMENTS..................................................12 REFERENCES ......................................................................12 BIOGRAPHY ........................................................................13 1 1 978-1-4244-2622-5/09/$25.00  2009 IEEE 2 IEEEAC paper #1524, Version 3, Updated January 8, 2009 1. INTRODUCTION AND MOTIVATION For most objects in the solar system, observation from orbit still remains the best means of studying active surface processes that are expressed in the atmosphere. A host of recent discoveries from the Mars Express, Galileo and Cassini spacecraft, among others, have shown that atmospheric plumes can be observed either directly \(e.g water vapor jets on Enceladus byproducts \(e.g. methane on Mars provides an excellent example of how recent discoveries of  active  behavior at the surface have triggered a whole new paradigm in planetary exploration and spacecraft design, as these behaviors introduce the possibility that Mars is not simply a passive member of the planetary family, but rather one which is presently, and continually, evolving. Because of the heightened interest in the detection of trace gases in the martian atmosphere, we have selected Mars as the model planet on which we demonstrate our method The recent putative discoveries of methane in the martian 


The recent putative discoveries of methane in the martian atmosphere [1-4] have ignited a debate over whether Mars is presently  alive  Detections of methane have been made at the parts per billion \(ppb limits of present-day observational capabilities. Coarse spatial variability in the methane signal has been tentatively reported [2,3], although such results remain unconfirmed and it is presently unclear from where the source or sources of the methane signals may originate. The present-day oxidizing atmosphere of Mars \(95% CO2 long-term stability of methane \(a reduced form of carbon the atmosphere, so it must be presently outgassed if a signal is being detected Source locations of methane, or of any disequilibrated species of sufficient abundance on Mars, should be readily detectible, yet no distinct surface sources have been unequivocally identified. This suggests that, if present, such trace gas source\(s  point   sources on the order of tens of kilometers or smaller in size below the nominal resolution of present-day orbiting instruments. Examples of these types of source features include volcanic vents, cracks or caldera where gas  plumes  can originate from within the martian crust. The small sizes of these features present challenges for their remote detection  even if directly observed, the observational  footprint  of an orbiting instrument may be 2 too large to uniquely isolate a small source location. The surface coverage presently provided by instruments designed to make trace gas detections also may be limited in both space and time, resulting in only a small chance that any one such feature will be observed directly from orbit Additionally, the unknown mechanism by which these source locations inject a trace species into the atmosphere may result in irregular  pulsed  injections over time, further reducing the chance that an active plume will be detected at its origin. Because of the conflicting practicalities of highresolution observations and regular global coverage, orbital detection of trace gas species in the martian atmosphere remains a challenging goal for the NASA Mars Exploration Program [5-7 Here, we present a novel technique, demonstrated using idealized plume models, for isolating the source locations of small-scale surface plumes from orbit with an uncertainty of a few tens of km \(within the 20-40 km search range of planned future Mars surface rovers observational capabilities. Additionally, we characterize the precision to which these idealized surface point sources can be isolated. Such precision is strongly dependent upon the atmospheric lifetime of the constituent, the structure of the plume emission, and the density of observations, but also on the specific characteristics of the observing instrument and the surface coverage obtained. While the detection of methane on Mars is the driving force behind this research this approach can be equally applied to any planet, and to gas or aerosol without distinction. Applying the present method, surface regions of the highest interest as landing sites can be identified and targeted from orbit for future missions 2. OBJECTIVE AND APPROACH Our objective is to demonstrate the feasibility of using evolutionary computational algorithms as a means of analyzing a set of observations to isolate the source location of trace gas plumes to within a few tens of kilometers with a minimum of computational effort. To achieve this objective we have established a series of tests to demonstrate the process and quantify potential sources of error in our analysis. These sources of error arise from both the numerical models employed as well as the limitations of the instruments used to make the atmospheric observations and uncertainties in the knowledge of the environmental state 


uncertainties in the knowledge of the environmental state There are two steps necessary to illustrate this evolutionary computational approach in the absence of real observational measurements 1 idealized plume shape 2 TPM estimates of the evolutionary computational model ECM plume is compared to the synthetic observations produced in step 1 Because of a lack of direct orbital observations of trace gas plumes in the martian atmosphere, the generation of synthetic plume observations \(step #1 reproduce the type of measurements that would otherwise be taken by a spacecraft as it flies over \(or through atmospheric plume. A sequence of observations is taken along the spacecraft trajectory, each centered at a different latitude/longitude, and covering a surface region \(the surface  footprint   of the observing instrument. The simulated spacecraft orbit used in this study has a high inclination \(74   of two hours \(7,200 s period of 88,775 s, this gives 12.3 spacecraft orbits per martian day, putting consecutive orbits ~30  apart in longitude. Our simulated instrument is a generic atmospheric limb sounder, operating on a 50% duty cycle, 3 seconds on, 3 seconds off, resulting in an along-track observing footprint of ~10 km, a nominal cross-track footprint of 200 km, and generating ~14,800 observations per martian day Accurately simulating a plume  s spatial distribution is not a trivial task, as the structure of a plume at any given time is driven not only by the prevailing atmospheric dynamics at the source location, but also the plume composition strength, surrounding topography, etc. General circulation models \(GCMs which can provide high fidelity representations of atmospheric motions, and can generate reasonable reproductions of a plume  s evolution over time. However simpler approaches, which are more appropriate for this initial demonstration of the ECM, can be used to simulate the plume distribution. It is these simplified approaches that we outline here, reserving more complex GCM simulations for later studies We have chosen to create simulated plumes out of simple time-invariant \(i.e. steady state easy to calculate and to visualize. In steady state, these plumes have assumed constant emission and decay rates that are equivalent, and typical of a plume that has been emitting for an extended period of time.  \(In future tests plumes can be made time varying, but this is not considered here of these plumes depend only upon the location of the spacecraft relative to the plume, and not on the time of the observation; hence, multiple orbits will enhance spatial coverage of the static plume. Because of our interest in identifying small-scale features, we can adjust the parameters of the analytic function to produce a very localized \(i.e  small   design of a specific test, we may additionally vary any one of a number of parameters in order to generate a plume that meets our specific model requirements 3 With an idealized plume and spacecraft orbit identified, we can produce a sequence of observations by flying through this synthetic plume. The plume, by its very nature \(i.e influenced by diffusion and dispersion from prevailing winds  blurs  our knowledge of its source position, which is substantially smaller than the observable plume itself. The observations further blur the apparent location of the plume 


observations further blur the apparent location of the plume source location due to the finite size of the observation footprint. Figure 1 illustrates this effect for three different periods of observation of the plume found in the lower right panel  Truth   200 x 10 km swath, it is not abundantly clear where the source location of the plume is in any of the three cases With one day of coverage \(Figure 1, upper left only five distinct observations that even suggest the presence of a plume. We may be able to make an educated guess as to the plume center from this limited sample, but it is by no means certain that such a guess will be even remotely correct.  Even as we increase the amount of surface coverage by extending our observation campaign to 100 days, the central source point may still be difficult to locate to within 10-20 km \(especially if the source were not as symmetric and well-defined as it is in this idealized case Because we have chosen our plume to be in steady state, we can be certain that the plume source is located somewhere within the detection region, but there is very little else the observations, and our intuition, can tell us In order to identify the surface source with any precision we need some means of simulating the type of atmospheric behavior that would reproduce the wind dispersion and diffusion processes implicit in the observed measurements In other words, the present approach requires us to make an a priori guess of the shape of the underlying \(actual, or  Truth    defocusing  effect of the observation footprint.  This is done via creation of a  test  plume. In the next section, we discuss in more detail how we choose the structure of the plume used to generate the observations as well as both the TPM and ECM used in this study 3. MODELING FRAMEWORK Generating Idealized Observations In this study we will consider two different plume shapes  a simple symmetric, Gaussian plume, and a second plume having a more complex, zonally asymmetric Gaussian shape. The Gaussian plume shape follows the function T\(x,y x ? x0 2 + \(y ? y0 2? 2    1 where T\(x,y position \(x,y plume center and ? is the standard deviation of the Gaussian in degrees   x0=0  and y0=0  The plume generated by Eq. 1, as seen from orbit, is shown contoured in Figure 2a \(an enlarged version of the  Truth  plume in Figure 1 such that the strength is unity at the plume center For the zonally asymmetric plume \(Figure 2b developed a compound plume shape with a zonally asymmetric tail designed to represent the type of downwind pattern that might be the result, after several days, of steady prevailing winds in the martian atmosphere. The plume is composed of the sum of Eq. 1 and an elliptical plume also following a Gaussian shape T\(x, y x ? x0 2 + \(y ? y0 2? 2    exp x?x1 C  2? 2 /C 


2? 2 /C         2 where prior values are the same as Eq. 1, x1 and y1 are the center location of the elliptical plume, and C is a factor representing the width-to-height ratio of the elliptical plume. In this study we have set C=5, x1=x0+1 and y1=y0+0 the elliptical portion of the plume is thus offset one degree to the east of the circular plume, and the elliptical  tail  is five times broader in longitude than latitude asymmetric plume, as with its circular counterpart, has a normalized peak value at \(0  0   both Eqns. 1 and 2 is initially evaluated on a uniform global grid of 1440 \(longitude latitude a gridded resolution of 0.25  x0.25  or about 15 km at the martian equator Spacecraft observations are, of course, not expected to fall on the 0.25  x 0.25  grid of our analytic solution  indeed they are dictated by the orbital geometry of the spacecraft The strength of the test plume at each of the previously determined observation points is therefore calculated by bilinear interpolation among the four grid points surrounding the observation position. The error introduced by the interpolation process is generally negligible. While an observation is said to be  centered  on a specific latitude/longitude pair, on this high-resolution grid, the observation footprint spans multiple gridpoints, hence the magnitude of an individual observation is actually averaged across the total number of gridpoints \(and fractions thereof that are spanned by the observation footprint. Gridding of the idealized plume \(rather than specifically calculating the plume strength at the observation location averaging process. Interpolation and averaging is applied at each of the observation locations. The final product of this process is a table of observation locations and the corresponding measurements at these locations Creating the Test Plume Model \(TPM The next step is to compare the observations of a test plume centered on a selected latitude/longitude pair with the  actual  observations. As stated previously, for simplicity 4 we have chosen to employ the same analytic function for the test plume as was used to generate the observations Through implementation of the ECM discussed in the next subsection, a series of  guesses  is made of the plume center x0 and y0. We then perform a  fly through  of each of these test plumes, following the same orbital pattern as for the actual observations. This allows us to perform a simple least-squares fit of each test plume to the actual observations, according to f = testn ? actualn 2 n 3 for all n observations. A better match between the test plume measurements and actual observations produces a lower value of f, with f=0 for a perfect match   Figure 1: Comparison of spacecraft coverage of circular plume \(lower right  and 100 days of observations.  As length of time increases, coverage becomes more complete, but for all panels  the true plume remains defocused  a consequence of the observation footprint size  


 The Evolutionary Computation Model An evolutionary computational model and the underlying genetic algorithms \(GAs approach to this plume source location method. The basic principles behind ECMs and GAs have been detailed elsewhere [8,9], but we can benefit from a fresh discussion of the ECM within the framework of this new planetary application. As its name suggests, the ECM applies principles that underlie evolutionary behavior to problems that require determination of an optimal or  best-fit   solution, given a series of constraining parameters. As such the ECM is a generic algorithm that can be applied to many 5 different problems that seek an optimized solution Essentially, here, our TPM becomes a customized module that gets  bolted onto  the ECM. The constraining parameters, or  genes  define, in the present study, the latitude and longitude of the plume source location Changes in these two parameters \(the genotype the TPM, with each change yielding a different model result or phenotype to extend the analogy further The proper selection of genes in the test plume model will provide the best fit to the actual observations So where does the  evolutionary  aspect of the ECM come into play? Without any knowledge of the location of the plume source, the ECM makes an initial set of arbitrary   Figure 2: \(a  strength is 1.0. \(b map of the zonally asymmetric Gaussian plume defined in Equation 2. Maximum plume strength is  1.0  guesses as to the latitude and longitude of the source location. Alternatively, the ECM may be  seeded  in areas where optimal solutions are likely to be found \(in a situation where there is some a priori knowledge of the system speed up the convergence process.  Each of these initial guesses is then sequentially used to generate a test plume centered at a latitude/longitude, following the details in the previous subsection. The fitness function, or goodness-of-fit parameter \(f to Eq. 3,. Those individual runs with the best fits are used together, to influence the next set \(or  generation   latitude/longitude guesses by the ECM. The ECM thus mimics nature  s  Survival of the Fittest  approach to evolution. In each subsequent generation of the process only the strongest guesses \(i.e., those with the best genes  the best fit help populate the next generation. This next generation will consist primarily of guesses that resemble the genotype of the parents in some way \(i.e., they will have latitudes and longitudes that are  blends  of the best-fit parents As with genetic evolution, mutations and other aberrations do occasionally occur. These manifest themselves in different ways in the ECM. Mutations have no similarity to the parents, and hence a mutated guess from the ECM will have seemingly arbitrary values for latitude and longitude Mutations serve to inject diversity into the population and increase the likelihood that the best-fit solution is truly a global best fit. In the absence of genetic mutations, over time, subsequent generations look increasingly identical to previous ones. Mutations prevent premature convergence on incorrect solutions by maintaining a large diversity in the population. The evolutionary process might converge on an incorrect solution \(i.e., a local best-fit, as with more 6  greedy  minimization algorithms diversifying the population of latitude/longitude guesses.  In our idealized scenario, we have a unimodal fitness 


our idealized scenario, we have a unimodal fitness landscape, and thus our local best fit will be the global best fit; however, this will not be the case for more complex situations, having multimodal landscapes.  Such scenarios are to be expected for implementations that are more realistic Substitutions are another way of injecting diversity into the gene pool. With substitution, values for individual genes are swapped. In the present scenario, this is akin to changing a latitude value while fixing longitude, or vice versa. Lastly a small fraction of guesses by the ECM are  clones  of the parent, having identical genetic structure. The combination of these four processes \(crossover, mutation, substitution and cloning optimal genotype while eliminating weaker genetic structures.  The ECM design allows the user to select the ratio of guesses that are produced via crossover vs. other processes Some of these genetic processes can be seen operating in Figure 3, which shows a typical example of the ECM  Figure 3:  Location of 100 plume source location guesses in each of four different generations  spacecraft observations of the circular analytical plume with 0% noise is the reference case; crossover/mutation is 50/50  see text and Figure 4 cross represents a single guess. Guesses are essentially random in Generation 1, but begin to  converge on a solution by Generation 5.  Convergence continues through Generation 10, and by Generation 20, most guesses  are within a fraction of a degree of the correct solution  behavior over several generations \(spacecraft observations of the circular analytical plume with 0% noise are assumed In each generation of this example, the ECM makes 100 guesses of the plume center location, with a 50/50 ratio of genetic crossover to the other processes. Each of the crosses in Figure 3 represents the location of a single guess \(of the center of an individual test plume region in which our steady state plume is located \(as illustrated in Figure 1 region to search only a 20  x 20  box that encompasses the observations Generation 1 shows the arbitrary distribution of the first 100 guesses. For each of these guesses, the plume model prepares an idealized plume centered at the latitude longitude of the guess, measurements of which are then compared to the set of spacecraft observations. Following the algorithm discussed above, those guesses that most closely fit the observations have their genetic information carried forward into the subsequent generation to seed a portion of the next 100 guesses, with the remaining portion being generated through mutation, substitution and cloning As quickly as in Generation 5 \(Figure 3, top right 7 significant number of guesses are seen converging on a solution. This convergence improves through Generation 10, although we still see the same fixed number of mutations \(the point at 9  latitude, -6  longitude is an example of a mutation have converged to within a fraction of a degree of the correct solution at \(0  0   substitution of a single gene on a number of guesses in Generation 20, which produce the  cross  shape apparent in the figure. For these guesses, either latitude or longitude is replaced, with the other gene held at its prior best-fit value By adjusting the fraction of mutations and substitutions correctly, one can balance the need for diversity with speed of convergence. In the absence of any mutations or substitutions, the solution will converge on the first local best fit it obtains, which may or may not be the global best fit. At the other extreme, a 100% mutation/substitution rate yields scattershot, or random, guesses, and provides no 


yields scattershot, or random, guesses, and provides no computational advantage over a brute-force solution, since determination of the global best fit only can be made after the full parameter space has been explored. A optimal medium exists somewhere between these two extremes.  A series of tests were performed to identify the best ratio to use in our ECM simulations \(Figure 4 tests was run out for 40 generations with 100 guesses in each generation, and the best-fit solution \(the lowest value of f from the 100 guesses ratio is ~50% crossover, 50% mutation \(including cloning and substitution search, but also provides efficient convergence to the global best fit  Figure 4: Demonstration of improved fitness for various ratios of crossover to mutation in a  sample ECM run \(spacecraft observations of the circular analytical plume with 0% noise is the reference case  identical plume models were run for 40 generations, but with each having the indicated percent of crossover and mutation. Lower  values of the ordinate indicate a better match between the plume model and observations. We see that a 50/50 split between  crossover and mutation provides the quickest path to a best-fit solution  We use this ratio for all the tests performed in Section 4 Generally, we obtain good convergence in 25 or fewer generations, each containing 100 test plumes \(2500 total compared with the more than one million required for a brute-force search over the 1440 x 720 points in the original grid Other minimization algorithms, such as LevenbergMarquardt \(L-M G-N significant disadvantages over the ECM for this type of problem.  First, because L-M and G-N are gradient-based algorithms, they require the landscape of the fitness function being minimized to be smooth, with a well-defined gradient. The function, f, being minimized in our tests however, has a sharp  spike  at the best-fit value, and a flat gradient most everywhere else. \(In other words, test plumes that do not exactly match the observations generally have the same poor fitness value regardless of the selection of genes global, minimizers.  If a guess is made and falls close to a 8 local minimum, the algorithm can be trapped around the local minimum and never find the global best fit.  The design of the ECM alleviates both of these problems  it functions well even with poorly defined gradients, and is designed to search for and identify the global best-fit solution There are other alternative approaches to identifying global minima such as the simulated annealing \(SA do not suffer the aforementioned problems.  The SA approach, as with the ECM, iteratively selects new solutions to the minimization problem; however, the decision to  accept  the new solution in SA is typically made in a probabilistic sense, independent of the new solution  s fitness.  With the ECM, a new solution is only accepted if the fitness is improved over the prior best fit There are two additional drawbacks to applying an approach like SA to this particular problem, although they best manifest themselves under less idealized conditions than those used here.  First, SA cannot handle problems that involve multiple competing objectives.  For example, we may be interested in identifying not only the source location of a plume, but also the surface emission flux or duration of emission. For such a problem, SA is forced to use an aggregated single objective by combining the competing objectives into a single weighted objective function, which is then iteratively optimized.  The ECM, on the other hand 


is then iteratively optimized.  The ECM, on the other hand can handle these multiple, simultaneous objectives without compromising accuracy in achieving each objective, and hence, is significantly more flexible Second, SA cannot find multiple local optimal solutions simultaneously.  In future implementations of our plume identification algorithm, we will likely have environments with multiple plumes, and minima corresponding to each solution.  The ECM is capable of efficiently handling this problem by partitioning the search space into multiple parts and having sub-populations, each of which focuses on a different part of the domain.  Since SA is a one-solution optimization algorithm, it cannot find multiple local solutions simultaneously For the present task, SA provides a viable alternative to obtaining the location of our simulated, idealized plume however it quickly becomes impractical when the problem becomes more complex and more realistic. Under those conditions, the flexibility of the ECM becomes a great asset 4. RESULTS A number of tests were performed to quantify accuracy in determining plume surface location as a function of the number of observations, level of instrument noise observation footprint size, and knowledge of actual plume shape. For most of the tests discussed in this section, we have assumed the simple, circular plume shape for both the actual and test plumes. This allows us to focus specifically on the singular objective of each test without dealing with differences in plume shape.  The exception to this broad assumption, is, of course, when we evaluate the role of our knowledge of actual plume shape itself, in which case we retain the circular shape for our actual plume and employ the zonally asymmetric one for our test plume Number of Observations We have shown previously \(Figure 1 circular plume appears like when observed for different lengths of time. We have performed a series of tests without random noise, for varying observation periods \(1 day, 10 days, 100 days, 1000 days using only observations that fall within  10  of what appears to be the plume center. Because of the Gaussian shape of the plume, tracer strengths at locations distant from the center are inconsequential and their contribution to f in Eq. 3 can be ignored.  Consequently, we can reduce our search space \(i.e. the region in which the ECM makes its guesses  x 20  box, which encloses the region with the strongest observations \(again, our steadystate approximation assures us that the plume source location must be within the region of strongest observations We have found that the accuracy in finding the correct plume center location generally increases as the number of measurements increases, as shown in Table 1. The results for the best-fit surface source location \(latitude and longitude with corresponding error in km generations. This demonstrates that, while it is not apparent from a cursory glance at Figure 1 that the source location of the true plume can be gleaned from the spacecraft coverage with any precision \(as the blurring of the original plume is quite evident amount of information to allow the best-fit latitude and longitude to determined by the ECM with high accuracy With observations of such a surface source over several tens of days, we should be able to decrease the source location uncertainty to levels that fall within the mobility range of future surface rovers \(20-40 km days or less of coverage \(about 1.5 martian years length of time of the primary science phase of most Mars missions Table 1:  Best-fit latitude and longitude positions for the circular plume model for varying lengths of observation 


circular plume model for varying lengths of observation campaigns Campaign Length days Best-Fit Latitude   Best-Fit Longitude  Error [km 1 -0.3476 -0.6190 42.0 10 -0.2522 -0.4613 31.1 100 -0.1862 -0.4960 31.3 1000 -0.1834 -0.2507 18.4 9 Instrument Noise Every observational measurement includes some level of noise in addition to the  true  signal. As such signal-to-noise problems are unavoidable, we must attempt to interpret their influence on the measurement result by introducing random noise into each observation and gauging the effect this has on our best-fit solution. To emulate, qualitatively, noise of various amounts, in separate tests we have added a noise term with a 3? distribution of either 10, 100 or 1000% of the local observation strength to each individual spacecraft observation. The impact of this noise term on the circular plume, relative to the perfect \(0% error Figure 5 There are 100 days of observations in each example. The results for the determination of the plume source location are presented in Table 2. While Figure 5 shows only one instance in which random noise at a specific amount has been applied, for Table 2  at each level of measurement uncertainty  the evaluation of plume source location was done ten times to account for the random factor and the variance among these ten cases is what is reported For low levels of measurement noise, in the tens of percent range, the results are largely indistinguishable from the noise-free standard, and uncertainty in position remains low relative to the noise-free instance from 10% to 100%, and then to 1000%, the ability to identify the plume source location becomes significantly diminished. With 100% uncertainty, individual measurements can vary by up to a factor of two, thus introducing  ghost  local maxima in the signal which are then interpreted as being close to the plume source location This phenomenon is significantly increased in the 1000 case. Note that, while Figure 5 shows only one case of random noise applied to the observations, the ghost local maxima do move around at this level of noise in different samplings, thus partly explaining the high uncertainty in source location  Figure 5:  Illustration of varying levels of instrument noise on the noise-free plume  measurement \(upper left of observations. As instrument noise increases, the plume source location is even less obvious  so that the accuracy to which the plume source location can be isolated is reduced 10 Table 2:  Variance in best-fit latitude and longitude solutions produced by the ECM for varying measurement uncertainty.  Source location uncertainty is relative to 0 measurement uncertainty value Measurement Uncertainty Source Location Uncertainty [km 0 0 10 0.6 100 31.5 1000 96.5  


 Footprint Size We performed a series of experiments demonstrating the role of measurement footprint size on source location uncertainty. As the footprint size decreases, the spatial extent over which the signal is averaged decreases and, in principle, provides better isolation of the peak signals from the adjacent, weaker  wings  of the plume \(Figure 6 footprint size is decreased from 200 km to 10 km, the retrieved shape from the observations better matches the  truth  although at the expense of a decrease in the area sampled in the same spatial domain. Table 3 bears out the competing influence of increased signal isolation and decreased spatial coverage for a 100-day observation campaign.  From a 200 km \(cross-track footprint, there is a ~45% decrease in uncertainty, but beyond this, there is no further gain. Figure 6 illustrates the cause of this strange behavior.  At 200 km \(upper left have near complete surface coverage, with substantial overlap, over the course of 100 days.  At 100 km \(upper right beginning to see small gaps in coverage, suggesting that for 100 days of observations, the amount of  repeat  coverage is small.  Hence, by reducing the cross-track footprint from 200 km to 100 km, we are provided the benefits of a smaller footprint  Figure 6:  Comparison of different observational footprint sizes.  Top left panel \(200 km  the nominal footprint size used in this study.  As footprint size is reduced, the shape of the plume is better defined, but at  the expense of spatial coverage Additionally, the peak plume strength in the observation set approaches that of the  true   plume \(cf. Figure 2b in black space indicates the fraction of the surface not observed by the instrument 11 Table 3:  Best-fit latitude and longitude positions for the derived plume source locations and consequent error for varying footprint widths Footprint Width km Best-Fit Latitude   Best-Fit Longitude  Error [km 200 -0.1862 -0.4960 31.3 100 -0.1523 -0.2485 17.2 50 -0.1178 -0.2390 15.8 20 -0.1195 -0.2414 15.9 10 -0.1276 -0.2391 16.0  but are not sacrificing overall surface coverage. For footprint sizes smaller than 100 km \(Figure 6, lower row the two effects counter each other relatively equally, and there is generally no improvement to the best fit. However as was discussed previously, for a fixed footprint size, an increase in the number of observations \(up to the point where coverage begins to substantially overlap improve the best fit. Footprint size is not an easily adjustable parameter for a spacecraft instrument and appears not to be as significant a factor in the plume source location error budget as is measurement spatial coverage Plume Shape In previous tests, we used the idealized case of a circular plume and assumed the same shape for the test plumes However, in reality, we will have no knowledge of the actual shape of the plume from our limited, defocused measurements, so we will need to make an educated guess as to the plume shape that is being observed. While knowledge of the meteorological context of the plume observations may be available, which would allow a 


observations may be available, which would allow a simulation of the plume shape using a GCM \(plume lifetime can be inferred from the plume  s spectroscopically determined composition discrepancy between the real plume shape and the GCMderived shape used for the test plumes in the ECM analysis of the observational data. We simulated this uncertainty by adopting our real plume shape from Eqn. 1 \(circular the test plume from Eqn. 2 \(zonally asymmetric expect this difference to result in a generally poorer solution. In fact, the results in Table 4 for lower surface coverage \(1 and 10 day campaigns error with coverage is not certain to be monotonic Interestingly, the errors in localizing the source when there is a reasonably high degree of surface coverage by the measurements are comparable to the results in Table 1 \(both cases assuming noise-free measurements reasonable, but not perfect, understanding of the observed plume shape, a campaign length of several tens of days or more, again, brings the uncertainty in plume source location to within the range of a landed rover Table 4:  Best-fit latitude and longitude positions for different campaign lengths, but assuming different actual and test plume shapes Campaign Length days Best-Fit Latitude   Best-Fit Longitude  Error [km 1 -0.5361 -0.2151 34.3 10 -0.1574 1.1961 71.5 100 -0.1692 0.2477 17.8 1000 -0.1389 0.2569 17.3  5. FUTURE PLANS A more ambitious implementation of this scheme would incorporate output of a full GCM in lieu of our basic analytic test plume model in the ECM framework. With this approach, the set of ECM guesses in a particular generation would spawn a suite of GCM model runs, with the  best guess  solutions from the completed GCM runs identified by the ECM and used to prepare the next generation of guesses.  Such an approach would be entirely autonomous and the best application of the GCM to the problem In the simplified demonstration herein, the importance of the ECM is somewhat understated.  Indeed, for simple plume shapes, a reasonable guess of plume location can be made directly by identifying the peak value in the observation set. This is an unfortunate side effect of this simple, two-gene demonstration.  The real value of the ECM manifests itself when we consider a parameter space of multiple genes.  Under more complex scenarios \(multiple plumes, various lifetimes, etc certainty, isolate individual plume locations. If the steadystate assumption is not valid, and the plume is evolving with time, the age of the plume becomes an additional gene we must consider.  Preliminary exploration of plume age as a third gene has begun, and we have obtained positive results from the ECM.  Further work is necessary to incorporate additional genes, and will be pursued in future work 6. CONCLUSIONS We have discussed the framework of a novel new approach to finding atmospheric plume source locations on planetary surfaces from orbit by using existing numerical analysis methods. This approach relies on the ability of genetic algorithms to quickly identify global best-fit solutions to a multivariate problem. We have found that, under idealized conditions, a source location can be identified to within tens of km on the surface within a couple dozen iterations of the ECM system using idealized plume shapes, even with 


ECM system using idealized plume shapes, even with limited spacecraft observations to constrain the system Error is minimized by increasing the number of observations, and by reducing the instrument uncertainty 12 Additionally, development of instruments with narrower footprints may contribute to a decrease in the source location error. Error due to uncertainty in the knowledge of the observed plume shape is reduced when observations are acquired in a long duration campaign. Plans for future development of this approach have been outlined, which involve the incorporation of general circulation models to supplant the idealized plumes used in the present study. The benefits of the genetic algorithms used here are maximized when multiple genes are employed, rather than just latitude and longitude. Examples of these genes would be parameters such as plume age, species lifetime and time evolution of source emission flux This approach can be of great value to planetary exploration programs, which seek high-precision identification of regions of interest on planetary surfaces to target future landed missions. By applying the methods presented here to realistic atmospheres, we can augment the direct observations made by orbiting spacecraft with a robust indirect approach, saving both time and money 7.  ACKNOWLEDGEMENTS The research described in this publication was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration REFERENCES 1] V.A. Krasnopolsky, J.P. Maillard and T.C. Owen  Detection of Methane in the Martian Atmosphere Evidence for Life  Icarus 172, 537-547, 2004 2] M.J. Mumma, G.L. Villanueva, R.E. Novak, T Hewagama, B.P. Bonev, M.A. DiSanti and M.D. Smith  Absolute Measurements of Methane on Mars: The Current Status  Proceedings of the American Astronomical Society Division of Planetary Sciences Bull. Amer. Astron. Soc. 38, 471, 2007 3] M. Mumma, G. Villanueva, R.E. Novak, T. Hewagama B.P. Bonev, M.A. DiSanti and M.D. Smith  Absolute Measurements of Methane on Mars: The Current Status   Mars Atmosphere: Modeling and Observations Workshop, #9099, 2008 4] V. Formisano, S. Atreya, Th. Encrenaz, N. Ignatiev and M. Giuranna  Detection of Methane in the Atmosphere of Mars  Science 306, 1758-1761, 2004 5] D.W. Beaty, M.A. Meyer and the Mars Advance Planning Group  2006 Update to Robotic Mars Exploration Strategy: 2007-2016  24 pp., 2006 6] D.J. McCleese and the Mars Advance Planning Group  Robotic Mars Exploration Strategy: 2007-2016  33pp 2006 7] MEPAG  Mars Scientific Goals, Objectives Investigations and Priorities: 2006  J. Grant, ed., 31pp 2006 8] R.J. Terrile, et al  Evolutionary Computation Technologies for Space Systems  IEEE Aerospace Conference Proceedings, Big Sky, MT, March 2005 9] S. Lee, C.H. Lee, S. Kerridge, C.D. Edwards and K.-M Cheung  Orbit Design and Optimization Based on Global Telecommunication Performance Metrics  IEEE Aerospace Conference Proceedings, Big Sky, MT, March 2006 13 BIOGRAPHY Michael A. Mischna is an atmospheric scientist with the Jet 


atmospheric scientist with the Jet Propulsion Laboratory focusing on the atmospheres and climates of terrestrial planets. He was the EDL Atmosphere Team Lead for the Mars Phoenix Mission. His research interests include longterm evolution of the martian climate, the role of greenhouse gases in providing habitable environments and surfaceatmosphere interactions. Dr. Mischna has a B.S. in atmospheric science from Cornell University, an M.S. in meteorology from the Pennsylvania State University and an M.S. and Ph.D. in Geophysics and Space Physics from the University of California, Los Angeles  Seungwon Lee is a member of the technical staff at the Jet Propulsion Laboratory. Her research interests include genetic algorithms, lowthrust trajectory design nanoelectronics, quantum computation, parallel cluster computation and advanced scientific software modernization techniques. Dr. Lee received her Ph.D. in Physics from the Ohio State University in 2002 Her work is documented in numerous journals and conference proceedings  Mark Allen is supervisor of the Earth and Planetary Atmospheres Group and Principal Scientist at the Jet Propulsion Laboratory. His research interests include remote detection of trace atmospheric species, and the role of atmospheric chemistry on the atmospheres of Mars, Venus and Titan. He was the PI of the MARVEL Mars Scout proposal, and is a co-investigator on the MIRO microwave instrument on the Rosetta Orbiter. Dr Allen has a B.A. from Columbia University and received a Ph.D. in Chemistry from the California Institute of Technology in 1976     Richard J. Terrile created and leads the Evolutionary Computation Group at NASA  s Jet Propulsion Laboratory. His group has developed genetic algorithmbased tools to improve on human design of space systems and has demonstrated that computer aided design tools can also be used for automated innovation and design of complex systems. He is an astronomer, the Mars Sample Return Study Scientist, the JIMO Deputy Project Scientist and the co-discoverer of the Beta Pictoris circumstellar disk. Dr. Terrile has B.S. degrees in Physics and Astronomy from the State University of New York at Stony Brook and an M.S. and a Ph.D. in Planetary Science from the California Institute of Technology in 1978 14  pre></body></html 


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





