Literature Search Tool for the Extraction of Disease-Associated Genes using Frequent Itemset Mining Ansu Francis Department of Computer Science and Engineering National Institute of Technology Calicut Kerala India Email ansu513@gmail.com K A Abdul Nazeer Department of Computer Science and Engineering National Institute of Technology Calicut Kerala India Email nazeer@nitc.ac.in Abstract Biomedical text mining is the process of extracting high quality information from biomedical text It has a lot of 
applications in genetics related studies Information about genedisease associations is very important in drug design Laboratory based methods for gene-disease association extraction need more effort and time Literature mining is a good method for generating candidate set of genes Manual methods are not suitable for literature mining as it is highly time-consuming due to the high volume of text data Even though most of the existing genedisease association extraction methods use abstracts full text articles provide more association data This paper proposes an automatic method for the extraction of disease-associated genes 
using the pr operty of fr equent itemsets fr om full text articles The method uses rule based Named Entity Recognition\(NER method for the initial extraction of candidate genes and frequent itemset mining to nd the gene-disease associations Experimental results show that this method gives better precision compared to the existing methods Index Terms biomedical text mining frequent itemset mining named entity recognition gene disease association I I NTRODUCTION With the introduction of molecular medicine the association between diseases and genes has become a very important in 
formation This can be used to correct the fundamental genetic errors that cause the diseases Text mining also referred to as text data mining roughly equivalent to text analytics refers to the process of deriving high-quality information from text As the number of articles available electronically increases effectiveness of text mining also increases Text mining has a lot of applications in different areas It is widely used in sentiment analysis A Biomedical Text Mining One of the main application areas of text mining is the biomedical domain Biomedical text mining also known as BioNLP refers to text mining applied to texts and literature 
of the biomedical and molecular biology domain W ith increase in the number of electronically available publications of the biomedical literature the scope of biomedical mining has increased The biomedical text mining is applied mainly for the identiìcation of genes and proteins nding interactions and associations between biological entities etc After the introduction of Human Genome Project the number of articles published in this domain became high So biomedical text mining is very much useful and efìcient for applications like protein-protein interaction extraction gene-gene interaction extraction drug-drug interaction extraction gene-disease association extraction etc PubMed PMC PubMed Central 4 MEDLINE 5 
etc are some of the databases that store literature related to the biomedical domain These databases are good sources of the datasets that we need for the text mining These databases allow the users to give the query and retrieve the articles The users can design the query as per their requirements like publication year publication type etc B Gene-Disease Association Extraction A disease can be caused by the irregularities in a number of genes That means there will be a set of genes associated with each disease Understanding the genetic causes behind a disease is very important in biological sciences Genedisease association extraction is the process of identifying the 
association between different diseases and genes Gene-disease association is an important information for the design of drugs to prevent or cure diseases In the earlier days laboratory based methods were employed for nding out the gene-disease associations These methods were timeconsuming and needed a lot of effort So there was practical difìculty in identifying these associations If a set of candidate genes associated with a disease/phenotype can be provided beforehand then the time and effort for the experimental analysis can be saved The genetics related literature is an important source of association information By using literature mining on genetics related articles it is possible to generate candidate 
gene sets Even though manual literature mining is possible it needs huge amount of human effort So some automatic mining methods are necessary This paper proposes a literature mining tool for extracting disease-associated genes from scientiìc literature The input to the tool is the name of the disease The output is the set of 2016 International Conference on Computing, Analytics and Security Trends \(CAST College of Engineering Pune, India. Dec 19-21, 2016 978-1-5090-1338-8/16/$31.00 ©2016 IEEE 306 


   Fig 1 Work ow of the proposed literature search tool genes associated with that disease ranked according to their signiìcance The remainder of this paper is organized as follows Section II reviews the state-of-the-art techniques for identifying genedisease interactions from the published literature Section III gives a detailed description of the proposed methodology Section IV discusses the results and section V concludes the report II R ELATED W ORKS The most efìcient method for providing candidate gene sets for a disease before experimental analysis is the literature mining The existing works related to this area use the concept of co-occurrence network based analysis and machine learning based methods But a satisfactory method is yet to be introduced The methods proposed by Jae-Yoon Jung et al in 2013 and Sune Pletscher-Frankild et al  in 2014 are based on cooccurrence Since these methods only consider the abstracts of articles the associations which are present only in the main text of the articles cannot be extracted The paper by Sreekala S et al  introduces the Hidden Mark o v Model for the identiìcation Here Hidden Markov Model is combined with rule based Named Entity Recognition approach for the identiìcation of gene symbols This paper takes the full text articles from PubMed for the analysis So it is more efìcient in nding associations that are mentioned only in the main text of the literature Xuan Liu et al  proposed the gene analysis for Li v e r Cancer based on literature mining In this method the associations are extracted using Bayesian network classiìer Network analysis is used for the gene-disease association extraction in the methods proposed by Changqin Quan et al  and Arzucan Ozgur et al  A disease-speciìc gene interaction network is employed in both these methods for extracting the associations Algorithm 1 LiteratureSearch Input Vector C  a 1 a 2  a n  of full text PMC articles disease name d Output Ranked list of genes G  g 1 g 2 g k  where Score  g i  Score  g i 1  1 Let p be the number of partitions 2 Search the corpus C and extract D  a 1 a 2 a x  where each a i is related to d 3 for a i  D do 4 Preprocess a i and extract the portions between abstract a i  and conclusion a i  5 for w i  a where a  w 1 w 2 w p  do 6 if w i matches with the gene symbol RE then 7 W i  W i  w i 8 end if 9 end for 10 W i  W i  dictionary words disease name abbreviations non-gene genetic terms  11 end for 12 Create transaction database B  a i W i  where articles a i  D are the transactions and candidate gene symbols W i  W are the items 13 L  P artition  B p  where L  L 1 L 2 L y  is the frequent gene sets  See Algorithm 2  14 for L i  L do 15 G  G  L i where L i  g 1 g 2 g r  and G is the frequently occurring set of gene symbols 16 end for 17 for g i  G do 18 Calculate Score  g i  using 1 19 end for 20 Sort G  g 1 g 2 g k  such that Score  g i   Score  g i 1  307 


Janet Pinero et al  proposed the DisGeNET  a platform that integrates the available curated and text mined genedisease association data The information contained is obtained from databases namely CTD UniProt 14 Rat Genome Database RGD and Mouse Genome Database MGD  This paper also proposes a scoring mechanism that considers the supporting evidence available for the genedisease associations The above mentioned methods mainly use the abstract of articles for nding out the associations Therefore the associations which are speciìed only in the main text of the articles cannot be identiìed by the above methods Most of these methods use a sentence by sentence approach So it is not possible to extend them to full text articles due to the large size of articles Moreover if the negative sentences are not eliminated from text the false positive rate will go high III P ROPOSED M ETHOD The proposed method considers the full text articles instead of considering only the abstract of articles Hence associations present in the main text will also be obtained Since the method uses frequent itemset mining approach 18 for nding the disease-associated genes most of the negative associations will be eliminated This method aims to obtain high precision results compared to the existing methods This method will fetch some new high accuracy associations Fig.1 shows the work ow of the proposed method Pseudocode of the proposed literature search tool is given as Algorithm 1 A Retrieving Articles The dataset used in this method is a collection of genetics related full text articles corresponding to various diseases downloaded from PubMed Central PMC The PMC provides the facility to query the dataset based on userês requirements such as publication type year etc Only research or review papers are used here as they will give the necessary information The PMC query is designed in such a way as to obtain the genetics related articles which contain the disease name or the MeSH Medical Subject Headings terms of the disease name in the title of the article The disease name or MeSH term of the disease name in the title ensures that the paper is relevant for the disease Five diseases Alzheimerês Disease Autism Spectrum Disorder Bipolar Disorder Breast Cancer Prostate Cancer are considered for the experiment Articles are collected for each of the ve diseases The pdf les are then converted to text les in order to perform the text mining B Gene Symbol Extraction Gene Named Entity Recognition NER is one of the major tasks in biomedical literature mining Most of the articles use gene symbols instead of using gene names directly Named Entity Recognition methods are of three types dictionary based methods rule based methods and machine learning based methods HGNC HUGO Gene Nomenclature Committee has provided the guidelines for assigning gene symbols According to these guidelines all the gene symbol representations should follow some particular patterns This is represented using the following regular expressions  A  Z  A  Z 0  9   and C 0  9 or f 0  9 These regular expressions are used to extract the initial set of candidate gene symbols But these regular expressions detect patterns that do not correspond to gene symbols also So dictionary words disease abbreviations non-gene genetic terms etc need to be removed from the obtained results After the gene symbol extraction a set of gene symbols are obtained corresponding to each article in the data set C Frequent Itemset Mining Frequent itemsets are itemsets that appear in a data set frequently Such frequent itemsets can be used for mining associations and correlations The main application of frequent itemset mining is in market basket analysis In this paper the frequent itemset mining is used for nding out the correlation between the gene symbols The genes associated with a particular disease will have a good correlation between them This idea is used here to extract the disease-associated genes from scientiìc literature The frequent itemset mining is done on transaction databases So a transaction database needs to be created with the individual articles as transaction IDs and the corresponding set of candidate gene symbols as items The Apriori algorithm  is found to be suitable for the task as it is capable of extracting all the frequent itemsets The Apriori algorithm is not capable of managing very large datasets To make it suitable for large datasets we can use the partitioning based method as given in Algorithm 2 which will di vide the large dataset into multiple small datasets and apply Apriori algorithm on each of the partitions  Algorithm 2 Partition Input Transaction database B  T,I  T transaction ID I Set of items number of partitions p  Output Frequent itemsets H  H 1 H 2 H y  1 Let t be the minimum support count 2 Divide B into p partitions B 1 B 2 B p 3 for B i  B do 4 F i  Apriori  B i t  p  where F is the local frequent itemsets 5 end for 6 Find Global Frequent Itemsets H  H 1 H 2 H y  using F as the candidate itemsets 7 return H  In the partitioning based method the entire database is rst divided into n partitions each containing manageable number of articles Then the local frequent itemsets are extracted for 308 


each of these partitions Then these local frequent itemsets are given as candidate sets for the global frequent itemset mining Partitioning will reduce the number of database scans needed Since the data will not be uniformly distributed in the database we need to run the partitioning based method multiple times to get good results From the result of frequent itemset mining the correlated gene symbols will be obtained These gene symbols can be considered as the set of genes associated with the given disease D Gene Ranking The obtained genes should be ranked based on their signiìcance According to the proposed method the frequency count of gene symbol and its maximum frequent itemset size are the two parameters that determine the importance of a gene symbol So a formula using the frequency count of gene symbols and the size of the maximum frequent itemset is used to nd the score of each gene symbol The score for a gene symbol g is calculated using 1 Score  g  FC  g    N 1    1 where FC  g  Frequency count of the gene symbol g N Size of the maximum frequent itemset containing g  Tuning parameter value between 0 and 1 The gene symbols obtained from frequent itemset mining were ranked using both 1 and by merely counting the frequency of the genes Both the rankings were compared using the Spearmanês correlation coefìcient The former gave better results compared to the latter one Therefore 1 was chosen to rank the genes in our proposed method The gene symbols are sorted using the Score to obtain the list of genes related to the given disease ranked according to their signiìcance IV R ESULTS AND D ISCUSSIONS Articles are obtained from PMC for each target disorder and the gene symbols associated with the diseases are identiìed The genes are ranked based on their scores The top 10 genes corresponding to each disease are listed in Table I The precision recall and F-measure of the proposed tool for Breast Cancer is checked by comparing it with the standard gene-disease association database HuGE Navigator The performance is obtained by varying the size of the dataset number of articles used Fig.2 shows the performance comparison There is a slight increase in the value of the metrics as the number of articles used is increased The results obtained for Alzheimerês Disease using the proposed method for various support values are compared against the HuGE Navigator database to nd out the precision recall and F-measure The result is given in Fig.3 It is found that the precision of the proposed method increases with the support value But as the support value increases the recall value decreases That is as the support value increases TABLE I R ANKED LIST OF GENE SYMBOLS OBTAINED  Alzheimerês Disease Autism Spectrum Disorder Bipolar Disorder Breast Cancer Prostate Cancer APOE FMR1 BDNF BRCA1 PTEN APP MECP2 CACNA1C BRCA2 TMPRSS2 PSEN1 SHANK3 ANK3 TP53 ERG CLU BDNF BP1 GAPDH GAPDH PICALM CA1 DISC1 ESR1 MYC PSEN2 PTEN MAF EGFR ETV1 CR1 NRXN1 COMT PTEN EZH2 BIN1 TSC1 SDS AT M SDS BACE1 TSC2 CA1 ERBB2 GSTP1 CD33 NLGN3 NRG1 CDH1 KLK3 Fig 2 Performance for different dataset sizes number of articles the proposed method gives a small set of genes that are actually associated results with high precision Comparison of the proposed method with the existing methods mentioned in and 6 is done by using the same data set The methods and 6 are based on abstracts only  So the abstracts of the articles are obtained for the same dataset The results are shown in Fig.4 The comparison is done by using full text articles for the proposed method and abstracts for the methods and 6 The precision of the proposed method is found to be higher than that of the existing methods Since the proposed method uses the concept of frequent itemsets associations that are present in single papers will be missed out Hence the recall value of the proposed method is less compared to the existing methods But when the size of the data set is increased the recall value improves as can be seen in Fig.2 The proposed method is tested on full text articles as well as abstracts The results of comparison is shown in Fig.5 The precision when abstracts are used is found to be better than that of full text articles But the recall value for abstract based method is much less compared to full text based method So it is always better to use full text articles than just using the abstracts After conducting the experiments using the proposed method some new genes are found to be associated with the 309 


Fig 3 Performance for various support values for Alzheimerês Disease Fig 4 Comparison with existing methods Fig 5 Performance of the proposed method when used with abstracts and full text articles TABLE II N EWLY IDENTIFIED GENE DISEASE ASSOCIATIONS  Gene Name Disease Name Supporting Reference SHANK2 Autism 25 SCN1A Autism 26 BRCA1 Alzheimerês 27 PINK1 Alzheimerês 28 E2F1 Breast Cancer 29 GREB1 Breast Cancer 30 FKBP5 Prostate Cancer 31 diseases The list of such newly identiìed associations is given in Table II Manual checking is done in such cases to nd the relevance of the newly identiìed associations and some highly scored genes mentioned in Table II are found to be relevant The supporting references are also provided in the table for the identiìed associations Laboratory based experimental analysis of these gene-disease associations needs to be performed to conìrm the validity of the associations Since the proposed method uses the concept of frequent itemsets genes that are present in single papers will be missed out even though they are relevant This method will not be able to extract those genes that are represented using their names instead of gene symbols Those gene symbols that do not follow the standard gene symbol representation will not be captured too Some gene symbol representations may also correspond to some other technical terms in biology For example CA1 and CA3 correspond to gene names and are also names of brain regions In such cases these will be identiìed as associated genes if they satisfy the support value 310 


V C ONCLUSION AND F UTURE S COPE Information about disease-associated genes is very useful in drug design Efìcient methods for generating candidate genes for a disease are much required The existing tools use the abstract of articles for this purpose but that may lead to missing the associations present in main text These methods may identify the negative associations also which will increase the false positive rate The proposed method tries to address some of the limitations of the existing methods The proposed method uses the rule based Named Entity Recognition in full text articles to generate the initial set of candidate genes Then the frequent itemsets are extracted from this initial set of genes using frequent itemset mining This method improves the precision of the disease-associated gene extraction A few new gene-disease associations that are not mentioned in the HuGE Navigator database are also identiìed Through the proposed method we could nd out that full text articles are better for mining out the associations than using the abstracts alone Since the method uses partition based frequent itemset mining local frequent itemsets for all the partitions can be found out parallelly thereby reducing the time required for frequent itemset mining Experimental results show that the proposed method extracts gene-disease associations with much better performance compared to the existing methods The proposed method can be extended to more number of diseases The results of frequent gene set mining can be used for identifying the gene-gene interactions in different diseases The genes which are associated with multiple diseases can also be identiìed The method can be extended to add drugs also Then gene-disease-drug associations can be identiìed using graph theoretic approaches These associations will be helpful in drug discovery R EFERENCES  T e x t mining  wikipedia the free enc yclopedia  https://en.wikipedia.org Online accessed 12-October  M P  Sa wicki G Samara M Hurwitz and E P assaro Human genome project The American journal of surgery  vol 165 no 2 pp 258Ö264 1993  Pubmed  http://www ncbi.nlm.nih.go v/pubmed Online accessed 3Novemeber  Pubmed central  http://www ncbi.nlm.nih.go v/pmc Online accessed 3-December  Medline  http://www medline.com/home.jsp Online accessed 3December  J.-Y  Jung T  F  DeLuca T  H Nelson and D P  W all  A literature search tool for intelligent extraction of disease-associated genes Journal of the American Medical Informatics Association  vol 21 no 3 pp 399Ö405 2014  S Pletscher Frankild A P allej  a K Tsafou J X Binder and L J Jensen Diseases Text mining and data integration of diseaseÖgene associations Methods  vol 74 pp 83Ö89 2015  S Sreekala K Nazeer  et al  A literature search tool for identifying disease-associated genes using hidden markov model in Computational Systems and Communications ICCSC 2014 First International Conference on  pp 90Ö94 IEEE 2014  X Liu W  Zou and J W ang Li v e r cancer related gene analysis based on literature mining International Journal of Bioscience Biochemistry and Bioinformatics  vol 3 no 5 p 416 2013  C Quan and F  Ren GeneÖdisease association e xtraction by te xt mining and network analysis in Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis Louhi EACL  pp 54Ö63 2014  A  Ozg  ur T Vu G Erkan and D R Radev Identifying gene-disease associations using centrality on a literature mined gene-interaction network Bioinformatics  vol 24 no 13 pp i277Öi285 2008  J Pi  nero N Queralt-Rosinach  A Bravo J Deu-Pons A BauerMehren M Baron F Sanz and L I Furlong Disgenet a discovery platform for the dynamical exploration of human diseases and their genes Database  vol 2015 p bav028 2015  A P  Da vis B L King S Mockus C G Murphy  C Saraceni-Richards M Rosenstein T Wiegers and C J Mattingly The comparative toxicogenomics database update 2011 Nucleic acids research  vol 39 no suppl 1 pp D1067ÖD1072 2011  U Consortium et al  The universal protein resource uniprot Nucleic acids research  vol 36 no suppl 1 pp D190ÖD195 2008  S J Laulederkind G T  Hayman S.-J W ang J R Smith T  F  Lowry R Nigam V Petri J de Pons M R Dwinell M Shimoyama et al  The rat genome database 2013data tools and users Brieìngs in bioinformatics  vol 14 no 4 pp 520Ö526 2013  J A Blak e C J Bult J T  Eppig J A Kadin J E Richardson M G D Group et al  The mouse genome database integration of and access to knowledge about the laboratory mouse Nucleic acids research  p gkt1225 2013  J Han and M Kamber  Data mining concepts and techniques the morgan kaufmann series in data management systems 2000  M H Dunham Data Mining Introductory and Advanced Topics  Prentice-Hall 2002  Medical subject headings  http://www ncbi.nlm.nih.go v/mesh Online accessed 15-December  U Leser and J Hak enber g What mak es a gene name named entity recognition in the biomedical literature Brieìngs in bioinformatics  vol 6 no 4 pp 357Ö369 2005  D Nadeau and S Sekine  A surv e y of named entity recognition and classiìcation Lingvisticae Investigationes  vol 30 no 1 pp 3Ö26 2007  Hgnc  http://www genenames.or g/about/guidelinesgenesymbols Online accessed 3-December  J H Zar  Signiìcance testing of the spearman rank correlation coef cient Journal of the American Statistical Association  vol 67 no 339 pp 578Ö580 1972  C for Disease Control and Pre v ention Hugena vigator phenopedia  https://phgkb.cdc.gov/HuGENavigator Online accessed 10-March2016  S Berk el C R Marshall B W eiss J Ho we R Roeth U Moog V Endris W Roberts P Szatmari D Pinto et al  Mutations in the shank2 synaptic scaffolding gene in autism spectrum disorder and mental retardation Nature genetics  vol 42 no 6 pp 489Ö491 2010  L W eiss A Escayg J K earne y  M T rudeau B MacDonald M Mori J Reichert J Buxbaum and M Meisler Sodium channels scn1a scn2a and scn3a in familial autism Molecular psychiatry  vol 8 no 2 pp 186Ö194 2003  A Nakanishi A Minami Y  Kitagishi Y  Ogura and S Matsuda Brca1 and p53 tumor suppressor molecules in alzheimers disease International journal of molecular sciences  vol 16 no 2 pp 2879 2892 2015  M M W ilhelmus S M v a n der Pol Q Jansen M E W itte P  v a n der Valk A J Rozemuller B Drukarch H E de Vries and J Van Horssen Association of parkinson disease-related protein pink1 with alzheimer disease and multiple sclerosis brain lesions Free Radical Biology and Medicine  vol 50 no 3 pp 469Ö476 2011  D W orku F  Jouhra G Jiang N P atani R Ne wbold and K Mokbel Evidence of a tumour suppressive function of e2f1 gene in human breast cancer Anticancer research  vol 28 no 4B pp 2135Ö2139 2008  J M Rae M D Johnson J O Sche ys K E Cordero J M Larios and M E Lippman Greb1 is a critical regulator of hormone dependent breast cancer growth Breast cancer research and treatment  vol 92 no 2 pp 141Ö149 2005  L Li Z Lou and L W ang The role of fkbp5 in cancer aetiology and chemoresistance British journal of cancer  vol 104 no 1 pp 19Ö23 2011 311 


The last MapReduce job aggregates the second MapReduce job’s output i.e all the frequent patterns to generate the nal frequent patterns for each item For example the output of the second MapReduce job includes three frequent patterns namely abc adc and bdc Using these three frequent patterns as an input the third MapReduce job creates the nal results for each item as a abc,adc b abc,bdc c abc,adc,bdc and d adc,bdc We pay attention to the second MapReduce job and the reason is three-fold First at the heart of FiDoop-DP is the construction of all frequent patterns which is implemented in the second MapReduce job Second this MapReduce job is more complicated and comprehensive than the rst and the third ones Third this job plays a vital role in achieving high performance of FiDoop-DP To optimize the performance of Pfp we make an improvement in the second MapReduce job by incorporating the Voronoi diagram-based partitioning idea In what follows we elaborate the algorithm for the second MapReduce job Given a set of k pivots  p 1 p 2   p k  selected in the preprocessing step we perform item grouping and data partitioning using statistical data collected for each partition Algorithm 1 is an LSH-based approach that integrates the item grouping see Step 3 and partitioning processes see Steps 4-20 In Algorithm 1 each mapper takes transactions as an input in the format of Pair h LongWritableoffset Textrecord i see Step 1 The mappers concurrently load FList to lter infrequent items of each transaction see Step 2 Meanwhile FList is divided into Q groups i.e GLists  by determining similarity among items and the given pivots  P 1 P 2   P k  each GList consists of Gid and the collection of items in the group see Step 3 Then each record including the pivots  P 1 P 2   P k  T i is transformed into a set followed by applying the minhash function to generate a column c i of signatures matrix see Steps 4-12 and algorithm 2 LSH is carried out using the above signature matrix M 0  l 005 n  see Steps 13-16 M 0 is divided into b bands each of which contains r rows where b 005 r  l  Then these bands are hashed to a number of hash buckets each hash bucket contains similar transactions see Step 15 Below we show the rationale behind applying LSH to determine similarity among transactions Given two transactions e.g T 1 and T 2  if there exists at least a pair of bands e.g b 1 2 T 1 and b 2 2 T 2  such that bands b 1 and b 2 are hashed into the same bucket then transactions T 1 and T 2 are considered similar see Step 17 Assume the similarity between two columns denoted as c 1 c 2  of a signature matrix is p  then the probability that c 1 and c 2 are exactly the same in a band is p r  the probability that c 1 and c 2 are completely different with respect to all the b bands is 1 003 s r  We show that if selecting appropriate values of b and r  transactions with a great similarity are mapped into one bucket with a very high probability If a band of T i shares the same bucket with a band of P j  we assign T i to the partition labelled as P j  We donate such an assignment in form of a pair Pair h P j T i i  see Steps 1819 At the end of the map tasks GLists are checked to guarantee the data completeness Steps 21-24 Finally the mappers emit Pair h P i T i i to be shufﬂed and combined for the second job’s reducers and reducers conduct local FP-Growth to generate the nal frequent patterns of each item see Steps 28-42 Algorithm 1 LSH-Fpgrowth Input FList  k pivots DB i  Output transactions corresponding to each Gid 1 function MAP key offset values DB i  2 load FList  k pivots 3 Glists  GenerateGlists  FList kpivots   based on the correlation of each item in FList and k pivots  4 for all T in DB i  do 5 items 010  Split  eachT   6 for all item in do 7 if item is in FList then 8 a 010  item 9 end if 10 end for 11 Add into Arrarylist sigMatrix  12 end for 13 for all  c i in sigMatrix  do 14 divide c i into b bands with r rows 15 Hashbucket  HashMap  each band of c i   16 end for 17 if at least one band of c i and pivot p j is hashed into the same bucket then 18 Gid  j  19 Output\(Gid new 20 end if 21 for all each GList t  t 6  i  do 22 if c i contains an item in GList t then 23 Gid  t 24 Output\(Gid new  guarantee the data completeness for each GList  25 end if 26 end for 27 end function Input transactions corresponding to each Gid Output frequent k-itemsets 28 function REDUCE key Gid  values DB Gid  29 Load GLists 30 nowGroup  GList Gid 31 localFptree.clear 32 for all  T i in DB Gid  do 33 insert-build-fp-tree\(localFptree T i  34 end for 35 for all  a i in nowGroup  do 36 Deﬁne a max heap HP with size K  37 Call TopKFPGrowth\(localFptree a i  HP  38 for all  v i in HP  do 39 Output v i  support v i  40 end for 41 end for 42 end function During the process of generating the signature matrix it is infeasible to permute a large characteristic matrix due to high time complexity This problem is addressed by employing the Minwise Independent permutation to speed up the process see algorithm 2 Let h\(x be a permutation function on a set X  for an element x 004 X  the value permuted is h  x  min  h  x 1  h  x 2    h  x n   When we XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 107 


obtain the signature matrix the original high-dimensional data are mapped to a low-dimensional space And the time complexity of subsequent operations is greatly reduced thanks to the above dimensions reduction 6E XPERIMENTAL E VALUATION We implement and evaluate the performance of FiDoop-DP on our in-house Hadoop cluster equipped with 24 data nodes Each node has an Intel E5-1620 v2 series 3.7gHZ 4 core processor 16G main memory and runs on the Centos 6.4 operating system on which Java JDK 1.8.0_20 and Hadoop 1.1.2 are installed The hard disk of NameNode is conﬁgured to 500 GB and the capacity of disks in each DataNode is 2 TB All the data nodes of the cluster have Gigabit Ethernet NICs connected to Gigabit ports on the switch the nodes can communicate with one another using the SSH protocol We use the default Hadoop parameter conﬁgurations to set up the replication factor i.e three and the numbers of Map and Reduce tasks Our experimental results show that over 90 percent of the processing time is spent running the second MapReduce job therefore we focus on performance evaluation of this job in our experiments To evaluate the performance of the proposed FiDoop-DP We generate synthetic datasets using the IBM Quest Market-Basket Synthetic Data Generator which can be exibly conﬁgured to create a wide range of data sets to meet the needs of various test requirements The parameters characteristics of our dataset are summarized in Table 1 6.1 The Number of Pivots We compare the performance of FiDoop-DP and Pfp when the number k of pivots varies from 20 to 180 Please note that k in FiDoop-DP corresponds to the number of groups in Pfp Fig 3 reveals the running time shufﬂing cost and mining cost of FiDoop-DP and Pfp processing the 4G 61-block T40I10D dataset on the 8-node cluster Fig 3 shows that FiDoop-DP improves the overall performance of Pfp Such performance improvements are contributed by good data locality achieved by Fidoop-DP’s analysis of correlation among the data FiDoop-DP optimizes data locality to reduce network and computing loads by eliminating of redundant transactions on multiple nodes As a result FiDoop-DP is capable of cutting mining cost see Fig 3b and data shufﬂing cost see Fig 3c Algorithm 2 Generate-signature-matrix Input  Output signature matrix of 1 function G ENERATE SIGNATURE MATRIX  2 for i=0 i  numHashFunctions i do 3 minHashValues  i 010 Integer:MAX VALUE  4 end for 5 for i=0 i  numHashFunctions i do 6 for all ele do 7 value  Integer  ele   8 byte  value   24  9 byte  value   16  10 byte  value   8  11 byte value  12 hashIndex  hashFunction  i 010 hash  bytesToHash   13 if  minHashValues  i 010  hashIndex then 14 15 end if 16 end for 17 end for 18 end function Fig 3a illustrates that the performance improvement of FiDoop-DP over Pfp becomes pronounced when the number k of pivots is large e.g 180 A large k in Pfp gives rise to a large number of groups which in turn leads to an excessive number of redundant transactions processed and transfers among data nodes As such the large k offers a great opportunity for FiDoop-DP to alleviate Pfp’s heavy CPU and network loads induced by the redundant transactions Interestingly we observe from Fig 3a that the overall running times of the two algorithms are minimized when number k is set to 60 Such minimized running times are attributed to 1 the FP-Growth mining cost plotted in Fig 3b and 2 the shufﬂing cost shown in Fig 3c Figs 3b and 3c illustrate that the mining cost and shufﬂing cost are minimized when parameter k becomes60inarangefrom20to180 The running times mining cost and shufﬂing cost exhibit a U-shape in Fig 3 because of the following reasons To conduct the local FP-Growth algorithm we need to group frequent 1-itemsets followed by partitioning transactions based TABLE 1 Dataset Parameters Avg.length Items Avg.Size/Transaction T10I4D 10 4000 17.5B T40I10D 40 10000 31.5B T60I10D 60 10000 43.6B T85I10D 85 10000 63.7B Fig 3 Impacts of the number of pivots on FiDoop-DP and Pfp 108 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


on items contained in each item group When the number of pivots increases the entire database is split into a ner granularity and the number of partitions increase correspondingly Such a ne granularity leads to a reduction in distance computation among transactions On the other hand when the pivot number k continues growing the number of transactions mapped into one hash bucket signiﬁcantly increases thereby leading to a large candidate-object set and high shufﬂing cost see Figs 3b and 3c Consequently the overall execution time is optimized when k is 60 for both algorithms see Fig 3a 6.2 Minimum Support Recall that minimum support plays an important role in mining frequent itemsets We increase minimum support thresholds from 0.0005 to 0.0025 percent with an increment of 0.0005 percent to evaluate the impact of minimum support on FiDoop-DP The other parameters are the same as those for the previous experiments Fig 4a shows that the execution times of FiDoop-DP and Pfp decrease when the minimum support is increasing Intuitively a small minimum support leads to an increasing number of frequent 1-itemsets and transactions which have to be scanned and transmitted Table 2 illustrates the size of frequent 1-itemsets stored in FList and the number of nal output records of the two parallel solutions under various minimum-support values Fig 4a reveals that regardless of the minimum-support value FiDoop-DP is superior to Pfp in terms of running time Two reasons make this performance trend expected First FiDoop-DP optimizes the partitioning process by placing transactions with a high similarity into one group rather than randomly and evenly grouping the transaction Fig 4b conﬁrms that FiDoop-DP’s shufﬂing cost is signiﬁcantly lower than that of Pfp thanks to optimal data partitions offered by FiDoop-DP Second this grouping strategy in FiDoop-DP minimizes the number of transactions for each GList under the premise of data completeness which leads to reducing mining load for each Reducer The grouping strategy of FiDoop-DP introduces computing overhead including signature-matrix calculation and hashing each band into a bucket Nevertheless such small overhead is offset by the performance gains in the shufﬂing and reduce phases Fig 4a also shows that the performance improvement of FiDoop-DP over Pfp is widened when the minimum support increases This performance gap between FiDoop-DP and Pfp is reasonable because pushing minimum support up in FiDoop-DP lters out an increased number of frequent 1-itemsets which in turn shortens the transaction partitioning cost Small transactions simplify the correlation analysis among the transactions thus small transactions are less likely to have a large number of duplications in their partitions As a result the number of duplicated transactions to be transmitted among the partitions is signiﬁcantly reduced which allows FiDoop-DP to deliver better performance than Pfp 6.3 Data Characteristic In this group of experiments we respectively evaluate the impact of dimensionality and data correlation on the performance of FiDoop-DP and Pfp by changing the parameters in the process of generating the datasets using the IBM Quest Market-Basket Synthetic Data Generator 6.3.1 Dimensionality The average transaction length directly determines the dimensions of a test data We conﬁgure the average transaction length to 10 40 60 and 85 to generate T10I4D 130 blocks T40I10D 128 blocks T60I10D 135 blocks T85I10D 133 blocks datasets respectively In this experiment we measure the impacts of dimensions on the performance of FiDoop-DP and Pfp on the 8-node Hadoop cluster The experimental results plotted in Fig 5a clearly indicate that an increasing number of dimensions signiﬁcantly raises the running times of FiDoop-DP and Pfp This is because increasing the number of dimensions increases the number of groups thus the amount of data transmission sharply goes up as seen in Fig 5b The performance improvements of FiDoop-DP over Pfp is diminishing when the dimensionality increases from 10 to 85 For example FiDoop-DP offers an improvement of 29.4 percent when the dimensionality is set to 10 the improvement drops to 5.2 percent when the number of dimensions becomes 85 In what follows we argue that FiDoop-DP is inherently losing the power of reducing the number of redundant transactions in high-dimensional data When a dataset has a low dimensionality FiDoop-DP tends to build partitions Fig 4 Impact of minimum support on FiDoop-DP and Pfp TABLE 2 The Size of FList and the Number of Final Output Records Under Various Minimum-Support Values minsupport 0.0005 0.001 0.0015 0.002 0.0025 FList 14.69k 11.6k 9.71k 6.89k 5.51k OutRecords 745 588 465 348 278 XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 109 


each of which has distinct characteristics compared with the other partitions Such distinct features among the partitions allow FiDoop-DP to efﬁciently reduce the number of redundant transactions In contrast a dataset with high dimensionality has a long average transaction length therefore data partitions produced by FiDoop-DP have no distinct discrepancy Redundant transactions are likely to be formed for partitions that lack distinct characteristics Consequently the beneﬁt offered by FiDoop-DP for highdimensional datasets becomes insigniﬁcant 6.3.2 Data Correlation We set the correlation among transactions i.e corr to 0.15 0.25 0.35 0.45 0.55 0.65 and 0.75 to measure the impacts of data correlation on the performance of the two algorithms on the 8-node Hadoop cluster The Number of Pivots is set to 60 see also Section 6.1 The experimental results plotted in Fig 5c clearly indicate that FiDoop-DP is more sensitive to data correlation than Pfp This performance trend motivates us to investigate the correlation-related data partition strategy Pfp conducts default data partition based on equal-size item group without taking into account the characteristics of the datasets However FiDoop-DP judiciously groups items with high correlation into one group and clustering similar transactions together In this way the number of redundant transactions kept on multiple nodes is substantially reduced Consequently FiDoop-DP is conducive to cutting back both data transmission trafﬁc and computing load As can be seen from Fig 5c there is an optimum balance point for data correlation degree to tune FiDoop-DP performance e.g 0.35 in Fig 5c If data correlation is too small Fidoop-DP will degenerate into random partition schema On the contrary it is difﬁcult to divide items into relatively independent groups when data correlation is high meaning that an excessive number of duplicated transactions have to be transferred to multiple nodes Thus a high data correlation leads to redundant transactions formed for partitions thereby increasing network and computing loads 6.4 Speedup Now we are positioned to evaluate the speedup performance of FiDoop-DP and Pfp by increasing the number of data nodes in our Hadoop cluster from 4 to 24 The T40I10D 128 blocks dataset is applied to drive the speedup analysis of the these algorithms Fig 6 reveals the speedups of FiDoop-DP a nd Pfp as a function of the number of data nodes The experimental results illustrated in Fig 6a show that the speedups of FiDoop-DP and Pfp linearly scale up with the increasing number of data nodes Such a speedup trend can be attributed to the fact that increasing the number of data nodes under a xed input data size inevitably 1 reduces the amount of itemsets being handled by each node and 2 increases communication overhead among mappers and reducers Fig 6a shows that FiDoop-DP is better than Pfp in terms of the speedup efﬁciency For instance the FiDoop-DP improves the speedup efﬁciency of Pfp by up to 11.2 percent with an average of 6.1 percent This trend suggests FiDoopDP improves the speedup efﬁciency of Pfp in large-scale The speedup efﬁciencies drop when the Hadoop cluster scales up For example the speedup efﬁciencies of FiDoopDP and Pfp on the 4-node cluster are 0.970 and 0.995 respectively These two speedup efﬁciencies become 0.746 and 0.800 on the 24-node cluster Such a speedup-efﬁciency trend is driven by the cost of shufﬂing intermediate results which sharply goes up when the number of data nodes scales up Although the overall computing capacity is improved by increasing the number of nodes the cost of synchronization and communication among data nodes tends to offset the gain in computing capacity For example the results plotted in Fig 6b conﬁrm that the shufﬂing cost Fig 5 Impacts of data characteristics on FiDoop-DP and Pfp Fig 6 The speedup performance and shufﬂing cost of FiDoop-DP and Pfp 110 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


is linearly increasing when computing nodes are scaled from 4 to 24 Furthermore the shufﬂing cost of Pfp is larger than that of FiDoop-DP 6.5 Scalability In this group of experiments we evaluate the scalability of FiDoop-DP and Pfp when the size of input dataset dramatically grows Fig 7 shows the running times of the algorithms when we scale up the size of the T40I10D data series Figs 7a and 7b demonstrate the performance of FiDoop-DP processing various datasets on 8-node and 24-node clusters respectively Fig 7 clearly reveals that the overall execution times of FiDoop-DP and Pfp go up when the input data size is sharply enlarged The parallel mining process is slowed down by the excessive data amount that has to be scanned twice The increased dataset size leads to long scanning time Interestingly FiDoop-DP exhibits a better scalability than Pfp Recall that see also from Algorithm 1 the second MapReduce job compresses an initial transaction database into a signature matrix which is dealt by the subsequent process The compress ratio is high when the input data size is large thereby shortening the subsequent processing time Furthermore Fidoop-DP lowers the network trafﬁc induced by the random grouping strategy in Pfp In summary the scalability of FiDoop-DP is higher than that of Pfp when it comes to parallel mining of an enormous amount of data 7R ELATED W ORK 7.1 Data Partitioning in MapReduce Partitioning in databases has been widely studied for both single system servers e.g and distributed storage systems e.g BigTable PNUTS[31 The existing approaches typically produce possible ranges or hash partitions which are then evaluated using heuristics and cost models These schemes offer limited support for OLTP workloads or query analysis in the context of the popular MapReduce programming model In this study we focus on the data partitioning issue in MapReduce High scalability is one of the most important design goals for MapReduce applications Unfortunately the partitioning techniques in existing MapReduce platforms e.g Hadoop are in their infancy leading to serious performance problems Recently a handful of data partitioning schemes have been proposed in the MapReduce platforms Xie et al  developed a data placement management mechanism for heterogeneous Hadoop clusters Their mechanism partitions data fragments to nodes in accordance to the nodes processing speed measured by computing ratios In addition Xie et al  designed a data redistribution algorithm in HDFS to address the data-skew issue imposed by dynamic data insertions and deletions CoHadoop is a H a d oop s lightweight extension which is designed to identify relateddataﬁlesfollowedbyamodiﬁeddataplacement policy to co-locate copies of those related les in the same server CoHadoop considers the relevance among les that is CoHadoop is an optimization of HaDoop for multiple les A key assumption of the MapReduce programming model is that mappers are completely independent of one another Vernica et al  broke such an assumption by introducing an asynchronous communication channel among mappers T his c hannel e nables the m appers to see global states managed in metadata Such situationaware mappers SAMs can enable MapReduce to exibly partition the inputs Apart from this adaptive sampling and partitioning were proposed to produce balanced partitions for the reducers by sampling mapper outputs and making use of obtained statistics Graph and hypergraph partitioning have been used to guide data partitioning in parallel computing Graph-based partitioning schemes capture data relationships For example Ke et al applied a graphic-execution-plan graph EPG to perform cost estimation and optimization by analyzing various properties of both data and computation Their estimation module coupled with the cost model estimate the runtime cost of each vertex in an EPG which represents the overall runtime cost a data partitioning plan is determined by a cost optimization module Liroz-Gistau et al proposed the MR-Part technique which partitions all input tuples producing the same intermediate key co-located in the same chunk Such a partitioning approach minimizes data transmission among mappers and reducers in the shufﬂe phase The approach captures the relationships between input tuples and intermediate keys by monitoring the execution of representative workload Then based on these relationships their approach applies a min-cut k-way graph partitioning algorithm thereby partitioning and assigning the tuples to appropriate fragments by modeling the workload with a hyper graph In doing so subsequent MapReduce jobs take full advantage of data locality in the reduce phase Their partitioning strategy suffers from adverse initialization overhead Fig 7 The scalability of FiDoop-DP and Pfp when the size of input dataset increases XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 111 


7.2 Application-Aware Data Partitioning Various efﬁcient data partitioning strategies have been proposed to improve the performance of parallel computing systems For example Kirsten et al  developed two general partitioning strategies for generating entity match tasks to avoid memory bottlenecks and load imbalances Taking into account the characteristics of input data Aridhi et al proposed a novel density-based data partitioning technique for approximate large-scale frequent subgraph mining to balance computational load among a collection of machines Kotoulas et al built a data distribution mechanism based on clustering in elastic regions Traditional term-based partitioning has limited scalability due to the existence of very skewed frequency distributions among terms Load-balanced distributed clustering across networks and local clustering are introduced to improve the chance that triples with a same key are collocated These selforganizing approaches need no data analysis or upfront parameter adjustments in a priori Lu et al studied k nearest neighbor join using MapReduce in which a data partitioning approach was designed to reduce both shufﬂing and computational costs In Lu’s study objects are divided into partitions using a Voronoi diagram with carefully selected pivots Then data partitions i.e Voronoi cells are clustered into groups only if distances between them are restricted by a speciﬁc bound In this way their approach can answer the k-nearest-neighbour join queries by simply checking object pairs within each group FIM for data-intensive applications over computing clusters has received a growing attention efﬁcient data partitioning strategies have been proposed to improve the performance of parallel FIM algorithms A MapReducebased Apriori algorithm is designed to incorporate a new dynamic partitioning and distributing data method to improve mining performance This method divides input data into relatively small splits to provide exibility for improved load-balance performance Moreover the master node doesn’t distribute all the data once rather the rest data are distributed based on dynamically changing workload and computing capability weight of each node Similarly Jumbo adopted a dynamic partition assignment technology enabling each task to process more than one partition Thus these partitions can be dynamically reassigned to different tasks to improve the load balancing performance of Pfp Uthayopas et al  investigated I/O and execution scheduling strategies to balance data processing load thereby enhancing the utilization of a multi-core cluster system supporting association-rule mining In order to pick a winning strategy in terms of data-blocks assignment Uthayopas et al incorporated three basic placement policies namely the round robin range and random placement Their approach ignores data characteristics during the course of mining association rules 8F URTHER D ISCUSSIONS In this study we investigated the data partitioning issues in parallel FIM We focused on MapReduce-based parallel FPtree algorithms in particular we studied how to partition and distribute a large dataset across data nodes of a Hadoop cluster to reduce network and computing loads We argue that the general idea of FiDoop-DP proposed in this study can be extended to other FIM algorithms like Apriori running on Hadoop clusters Apriori-based parallel FIM algorithms can be classiﬁed into two camps namely count distribution and data distribution  For the count distribution camp each node in a cluster calculates local support counts of all candidate itemsets Then the global support counts of the candidates are computed by exchanging the local support counts For the data distribution camp each node only keeps the support counts of a subset of all candidates Each node is responsible for delivering its local database partition to all the other processors to compute support counts In general the data distribution schemes have higher communication overhead than the count distribution ones whereas the data distribution schemes have lower synchronization overhead than its competitor Regardless of the count distribution or data distribution approaches the communication and synchronization cost induce adverse impacts on the performance of parallel mining algorithms The basic idea of Fidoop-DP—grouping highly relevant transactions into a partition allows the parallel algorithms to exploit correlations among transactions in database to cut communication and synchronization overhead among Hadoop nodes 9C ONCLUSIONS A ND F UTURE W ORK To mitigate high communication and reduce computing cost in MapReduce-based FIM algorithms we developed FiDoop-DP which exploits correlation among transactions to partition a large dataset across data nodes in a Hadoop cluster FiDoop-DP is able to 1 partition transactions with high similarity together and 2 group highly correlated frequent items into a list One of the salient features of FiDoopDP lies in its capability of lowering network trafﬁc and computing load through reducing the number of redundant transactions which are transmitted among Hadoop nodes FiDoop-DP applies the Voronoi diagram-based data partitioning technique to accomplish data partition in which LSH is incorporated to offer an analysis of correlation among transactions At the heart of FiDoop-DP is the second MapReduce job which 1 partitions a large database to form a complete dataset for item groups and 2 conducts FP-Growth processing in parallel on local partitions to generate all frequent patterns Our experimental results reveal that FiDoop-DP signiﬁcantly improves the FIM performance of the existing Pfp solution by up to 31 percent with an average of 18 percent We introduced in this study a similarity metric to facilitate data-aware partitioning As a future research direction we will apply this metric to investigate advanced loadbalancing strategies on a heterogeneous Hadoop cluster In one of our earlier studies see for details we addressed the data-placement issue in heterogeneous Hadoop clusters where data are placed across nodes in a way that each node has a balanced data processing load Our data placement scheme can balance the amount of data stored in heterogeneous nodes to achieve improved data-processing performance Such a scheme implemented at the level of Hadoop distributed le system HDFS is unaware of correlations among application data To further improve load balancing 112 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


mechanisms implemented in HDFS we plan to integrate FiDoop-DP with a data-placement mechanism in HDFS on heterogeneous clusters In addition to performance issues energy efﬁciency of parallel FIM systems will be an intriguing research direction A CKNOWLEDGMENTS The work in this paper was in part supported by the National Natural Science Foundation of P.R China No.61272263 No.61572343 Xiao Qin’s work was supported by the U.S National Science Foundation under Grants CCF-0845257 CAREER The authors would also like to thank Mojen Lau for proof-reading R EFERENCES  M J Zaki Parallel and distribu ted associat ion mining A survey IEEE Concurrency  vol 7 no 4 pp 14–25 Oct 1999  I Pramudiono and M Kitsuregawa  Fp-tax Tree structure based generalized association rule mining in Proc 9th ACM SIGMOD Workshop Res Issues Data Mining Knowl Discovery  2004 pp 60–63  J De an a n d S Gh e ma wa t M ap re du ce  S i mp l i e d da ta pr o ce s si n g on large clusters ACM Commun  vol 51 no 1 pp 107–113 2008  S Sakr A Liu and A G Fayoumi The family of mapred uce and large-scale data processing systems ACM Comput Surveys  vol 46 no 1 p 11 2013  M.-Y Lin P.-Y Lee and S.-C Hsueh Apriori-based frequent itemset mining algorithms on mapreduce in Proc 6th Int Conf Ubiquitous Inform Manag Commun  2012 pp 76:1–76:8  X Li n  Mr a pr io ri  As so ci a ti o n ru le s a lg o ri th m ba se d on mapreduce in Proc IEEE 5th Int Conf Softw Eng Serv Sci  2014 pp 141–144  L Zhou Z Zhong J Chang J Li J Huang and S Feng Balanced parallel FP-growth with mapreduce in Proc IEEE Youth Conf Inform Comput Telecommun  2010 pp 243–246  S Hong Z Huaxuan C Shiping and H Chunyan The study of improved FP-growth algorithm in mapreduce in Proc 1st Int Workshop Cloud Comput Inform Security  2013 pp 250–253  M Riondato  J A DeBrabant R Fonseca and E Upfal Parma A parallel randomized algorithm for approximate association rules mining in mapreduce in Proc 21st ACM Int Conf Informa Knowl Manag  2012 pp 85–94  C Lam Hadoop in Action  Greenwich USA Manning Publications Co 2010  H Li Y Wang D Zhang M Zhang and E Y Chang PFP Parallel FP-growth for query recommendation in Proc ACM Conf Recommender Syst  2008 pp 107–114  C Curino E Jones Y Zhang and S Madden Schism A workload-driven approach to database replication and partitioning Proc VLDB Endowment  vol 3 no 1-2 pp 48–57 2010  P Uthayop as and N Benjamas Impact of i/o and execution scheduling strategies on large scale parallel data mining J Next Generation Inform Technol  vol 5 no 1 p 78 2014  I  P r a m u d i o n o a n d M  K i t s u r e g a w a  P a r a l l e l F P g r o w t h o n P C cluster in Proc.Adv.Knowl.DiscoveryDataMining  2003 pp 467–473  Y Xun J Zhang and X Qin Fidoop Parallel mining of frequent itemsets using mapreduce IEEE Trans Syst Man Cybern Syst  vol 46 no 3 pp 313–325 Mar 2016 doi 10.1109 TSMC.2015.2437327  S Owen R Anil T Dunning and E Friedman Mahout Action  Greenwich USA Manning 2011  D Borthakur  Hdfs architecture guide HADOOP APACHE PROJECT Available  http://hadoop.apache.org/common/docs current/hdfs design.pdf 2008  M Zaharia M Chowdhury M J Franklin  S Shenker and I Stoica Spark Cluster computing with working sets in Proc 2nd USENIX Conf Hot Topics Cloud Comput  2010 p 10  W Lu Y Shen S Chen and B C Ooi Efﬁcient proces sing of k nearest neighbor joins using mapreduce Proc VLDB Endowment  vol 5 no 10 pp 1016–1027 2012  T Kanung o D M Mount N S Netanya hu C D Piatko R Silverman and A Y Wu An efﬁcient k-means clustering algorithm Analysis and implementation IEEE Trans Pattern Anal Mach Intell  vol 24 no 7 pp 881–892 Jul 2002  A K Jain Data clustering 50 years beyond k-means Pattern Recog Lett  vol 31 no 8 pp 651–666 2010  D Arthur and S Vassilvitskii  k-means  The advantages of careful seeding in Proc 18th Annu ACM-SIAM Symp Discr Algorithms  2007 pp 1027–1035  J Leskovec A Rajaraman and J D Ullman Mining Massive Datasets  Cambridge U.K Cambridge Univ Press 2014  A Stupar  S Mich el and R Schen kel Rankred uce–pr ocessin g k-nearest neighbor queries on top of mapreduce in Proc 8th Workshop Large-Scale Distrib Syst Informa Retrieval  2010 pp 13–18  B Bahmani A Goel and R Shinde Efﬁcient distributed locality sensitive hashing in Proc 21st ACM Int Conf Inform Knowl Manag  2012 pp 2174–2178  R Panigrahy Entropy based nearest neighbor search in high dimensions in Proc 17th Annu ACM-SIAM Symp Discr Algorithm  2006 pp 1186–1195  A Z Broder M Charikar  A M Frieze and M Mitzenma cher Min-wise independent permutations J Comput Syst Sci  vol 60 no 3 pp 630–659 2000  L Cristofor ARtool Association rule mining algorit hms and tools 2006  S Agrawal V Narasayya  and B Yang Integrating vertical and horizontal partitioning into automated physical database design in Proc ACM SIGMOD Int Conf Manag Data  2004 pp 359–370  F Chang J Dean S Ghema wat W Hsieh D Wallach  M  Burrows T Chandra A Fikes and R Gruber Bigtable A distributed structured data storage system in Proc 7th Symp Operating Syst Des Implementation  2006 pp 305–314  B F Cooper R Ramakrishn an U Srivastava A Silberstein P Bohannon H.-A Jacobsen N Puz D Weaver and R Yerneni Pnuts Yahoo!’s hosted data serving platform Proc VLDB Endowment  vol 1 no 2 pp 1277–1288 2008  J Xie and X Qin The 19th heterogenei ty in computing workshop HCW 2010 in Proc IEEE Int Symp Parallel Distrib Process Workshops Phd Forum  Apr 2010 pp 1–5  M Y Eltabakh Y Tian F  Ozcan R Gemulla A Krettek and J McPherson Cohadoop Flexible data placement and its exploitation in hadoop Proc VLDB Endowment  vol 4 no 9 pp 575 585 2011  R Vernica A Balmin K S Beyer and V Ercegovac Adaptive mapreduce using situation-aware mappers in Proc 15th Int Conf Extending Database Technol  2012 pp 420–431  Q Ke V Prabhakar an Y Xie Y Yu J Wu and J Yang Optimizing data partitioning for data-parallel computing uS Patent App 13/325,049 Dec 13 2011  M Liroz-Gis tau R Akbarinia D Agrawal E Pacitti  and P Valduriez Data partitioning for minimizing transferred data in mapreduce in Proc 6th Int Conf Data Manag Cloud Grid P2P Syst  2013 pp 1–12  T Kirsten L Kolb M Hartung A Gro H K  opcke and E Rahm Data partitioning for parallel entity matching Proc VLDB Endowment  vol 3 no 2 pp 1–8 2010  S Kotoulas E Oren and F Van Harmelen Mind the data skew Distributed inferencing by speeddating in elastic regions in Proc 19th Int Conf World Wide Web  2010 pp 531–540  L Li and M Zhang The strategy of mining associat ion rule based on cloud computing in Proc Int Conf Bus Comput Global Inform  2011 pp 475–478  S Groot K Goda and M Kitsuregawa  Towards improv ed load balancing for data intensive distributed computing in Proc ACM Symp Appl Comput  2011 pp 139–146  M Z Ashra D Taniar and K Smith ODAM An optimiz ed distributed association rule mining algorithm IEEE Distrib Syst Online  vol 5 no 3 p 1 Mar 2004 Yaling Xun is currently a doctoral student at Taiyuan University of Science and Technology She is currently a lecturer in the School of Computer Science and Technology Taiyuan University of Science and Technology Her research interests include data mining and parallel computing XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 113 


Jifu Zhang received the BS and MS degrees in computer science and technology from the Hefei University of Tchnology China and the PhD degree in pattern recognition and intelligence systems from the Beijing Institute of Technology in 1983 1989 and 2005 respectively He is currently a professor in the School of Computer Science and Technology TYUST His research interests include data mining parallel and distributed computing and artiﬁcial intelligence Xiao Qin received the PhD degree in computer science from the University of Nebraska-Lincoln in 2004 He is currently a professor in the Department of Computer Science and Software Engineering Auburn University His research interests include parallel and distributed systems storage systems fault tolerance real-time systems and performance evaluation He received the U.S NSF Computing Processes and Artifacts Award and the NSF Computer System Research Award in 2007 and the NSF CAREER Award in 2009 He is a senior member of the IEEE Xujun Zhao received the MS degree in computer science and technology in 2005 from the Taiyuan University of Technology China He is currently working toward the PhD degree at Taiyuan University of Science and Technology His research interests include data mining and parallel computing  For more information on this or any other computing topic please visit our Digital Library at www.computer.org/publications/dlib 114 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


