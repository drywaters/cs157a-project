An Web Content Based Data Mining for Car Consumption Preference in China Xiaoshu Hang\224 Honghua Dai\224, Youhua Zhangb School oflnformation Technology, Deakin Universiy Australia Institute ofhtelligent Machines the Chinese Academy of Sciences, Hefei,China xhan hdai adeakin edu au b Abstract This paper introduces an incremental FP-Growth approach for web content based data mining and its application in solving a real worldproblem The problem is solved in the following ways Firstly we obtain the semi-structured data from the 
web pages of Chinese car market and structure them and save them in local database Secondly we use an incremental FP-Growth algorithm for mining association rules to discover Chinese consumers\222 car consumption preference To find more general regularities, an attribute-oriented induction method is also utilized to find customer\222s consumption preference among a range of car categories Experimental results have revealed some interesting consumption preferences that are useful for the decision makers to make the policy to encourage and guide car consumption Although the current data we used may 
not be the best representative ofthe actual market in practice it is still good enough for the decision making purpose in terms of reflecting the real situation of car consumption preference under the two assumptions in the context 1 Introduction The majority of the research has been done so far in the area of data mining is mainly focused the studies on the theow and approaches of data mining only a few have explored the real applications of data mining 
that can be seen from literatures One of the bottlenecks is the difficulty of obtaining real-life data With the explosive growth of information sources available on the World Wide Web it has become increasingly significant for users to acquire knowledge from WWW Web mining can be generally classified into content-based mining and usage-based mining Content-based Web mining can he reclassified into two sub-categories agent-based and database-based Agent-based Web content mining includes intelligent search agent and information filter agent Database-based Web content mining includes multilevel database and 
Web query system Database based approach focuses on the techniques for organizing the semi-shuctured data into structured ones and save them into database for later data mining[Cooley R et al 19991 When the data for knowledge discovery are obtained from web pages by some ways then it become a dynamic data source because the web pages are usually unexpectedly updated The mining algorithm designed for such a dynamic data source should be incremental for the purpose of efficiency We present an incremental FP Growth algorithm for mining association rules 
and apply it to a Chinese car market database Car consumption is viewed as a new potential power to the growth of Chinese economy and has been paid a great attention by the Chinese govemment Many car dealers release the car market information via Internet for the convenience of the customers to acquire the updated information It is quite natural that the customers access to the web pages to find their favorite cars before they actually go to the market to purchase it Therefore it is safe to assume that the more a web page of introducing 
a type of car is accessed the higher that model of car is preferred by customers Thus we have the following two assumptions on which our research is based a Generally a potential customer can obtain car market information via internet b The more a web page of introducing a particular model of car is visited the higher preference this type of car is given by the customers The semi-structured data retrieved from Web page are distributed by Chinese largest car market in URL 
httn://www.cheshi.com.cn The web-based database includes various kinds of cars from lower price cars to much higher ones from Chinese cars from foreign cars The number of customers\222 daily access to a type of car ranges from less than one hundred to several thousands which shows customers\222 different preference The remainder of this paper is arranged as fellows In section 2 the techniquc about retrieving semi-structured data on web page is introduced In section 3 an 0-7803-8242-0/03 17.00 Q 2003 IEEE 235 


incremental FP-Growth algorithm for mining association rules are introduced An attribute-oriented induction is utilized to find some more general regularities e.g customers\222 preference among different car categories in section 4 Some experimental results and analysis are given in section 5 Section 6 is the conclusions 2 Data retrieval from web pages In this section we discuss how to retrieve the interesting data embedded in web pages such as HTML pages XML pages or text files Reorganized the semi-structured data into more structured data is the first step of Web content based mining The popular method is to write a program called\223Wrapper\224[Kushmeri ck N e tal 1997 Ade 1 berg B 19981 to extract data from web page and then store them in database 2.1 Preprocessing The preprocessing includes Input interesting URL and key words Fetching the Web page Trimming and normalizing the source file In this phase our input is interesting URL and key words and the output is the wanted and normalized data block Sometimes a web page includes more than 100 data blocks e.g http://newssina.com.cn.We usually focus on the interesting region see Fig.2.1 For instance we are interested with foreign exchange rate so we can pick up the URL and some keywords such as 223Sale Price\224 223Purchase Price\224 etc The URL and keywords are viewed as parameters of the preprocessing function which retums the wanted HTML or XML source file This raw data is then normalized to facilitate the next phase of processing Note that most of the valuable data embedded between table pairs so we can expurgate some senseless block such as javascript block head block and convert all source to lowercase or uppercase and then replace the continuous spaces with one blank character which will improve the efficiency of extraction Since some documents are irregularly written such as ASP and the HTML document and only the interesting region of the web page is retrieved a check function is needed to analyze the structure of the document and determine whether to fetch or skip the document to avoid disordering the hierarchy of the document In this paper we present a method which does not need the integrity of html document hut the wanted information should be within the fragment 2.2 Getting the wanted block A module called hook is designed to identify the interesting region or wanted block from a normalized document. The hook algorithm is described as follows Algorithm Hook Input: keywords a normalized document Output the interesting block Begin For i=to m llm is the number of keywords Calculating the frequency ni and position p i pil pil p in of ith keyword in the normalized document if q  1 then retum this table pair block else find the minimum nk=min\(ni for j=l to nk Calculating the kth keyword\222s table pair\222s positions tbegin rend which must satisfy ibegin pph<rend Test all other keywords\222 position If one of these keywords has no position with this region\(tbegin tend continue next j or if all of them bas at least one position in region\(tbegin tend return this region as the wanted block Endfor Endfor End 2.3 Generating structured data Inside the wanted data block we are mainly concemed the three kinds of tags 223table\224 223tr\224 and 223td\224 All other tags can be trimmed to obtain a skeleton of the data block These tags reflect the hierarchy of data As an example a hierarchy can be generated as follows table tr td currency td> <td buy </td td sale td tr td US td td 826.47 td td 828.95 </td tr table Then we pick up the data between td and Utd pair store them in a pre-defined structured array and save them in local database for data mining tr  3 An incremental FP-Growth algorithm 3.1 Problem description FP-Growth has been known as a highly efficient algorithm due to its striking characteristics such as using compact data structure eliminating repeated database scan and basic counting as well as FP-tree building operation FP-growth includes two-step process In the 236 


first step a FP-tree is created by scanning the database twice and I the second step the algorithm FP-Growth is used to mine iteratively the frequent patterns from the FP tree in the bottom-up order In the real world however we can hardly apply a mining algorithm directly to an application without any modification In a dynamic data environment where new data flow arrives unexpectedly the present FP-Growth can not be used to directly mine association rules if it does not re-run every time the database is updated The alternative is an incremental FP-Growth algorithm Such dynamic data environments are easily found in the area of the web-content based data mining where the data for mining frequent patterns are usually retrieved by the wrapper program Every time the web pages are updated a new date set is retrieved and he appended into the database by the wrapper program The incremental FP-Growth approach can be described as fellows Let FTo be a FP-tree built by FP-Growth algorithm from data set Do at time to and a new data flow AD are retrieved from web page in the time interval At the problem is how to update FTo to a new FP-tree FT with data flow AD at time stamp t  h+At so that FT is exactly the same as that built from data set Do+AD We have known that in FP-Growth algorithm a header table is created for keeping the each item\222s frequency in database So the first step for the incremental FP-Growth algorithm to do is to update this header table with the new coming data set AD This can be done just by scanning data set AD one time and by resorting the items\222 frequency And next the algorithm need to scan data set AD again to create an initial FP-Tree FT\222 so that we have two FP-trees in memory and the FT\222 is much smaller than FTo because its data set AD is much smaller The next is to grow FP-Tree FT\222 by dismantling FTa a Multi-traversing FP-Tree FTo to generate all the item sets and their frequencies An itemset\222s frequency is defined as the frequency of the terminal node in the itemset b Dismantling FP-Tree FT by deleting the sub trees in which the frequencies of all nodes become zero Growing FT-Trce FT\222 with the itemsets got from c a Note that the new data set AD may cause such a problem Such problem must be resolved before the mining process For a given minimal support an item that is frequent in D may become infrequent in DuAD On the other hand an item which is infrequent in D may be highly frequent in AD All the possible situations due to the new increased data set AD are described as follows An item is frequent in both D and AD therefore it is frequent in DuAD An item is frequent in D but not frequent in AD therefore it is either frequent or not in DuAD An item is not frequent in D but frequent in AD therefore it is either frequent or not in DuAD An item is neither frequent in D nor in AD therefore it is not freauent in DuAD Since the present FP-Growth can not deal with the last three cases the header table in FP-Growth is modified to hold not only the frequent items but also some infrequent ones Definition 1 The marginal frequency of an item X in dynamic data environment is defined as where rx and rD represent the growth rates of item X and database D, respectively Note that the marginal frequency of an item reflects its growth trend in the dynamic database Higher marginal frequency implies potentially higher frequency of an item in the dynamic database Definition 2 For a given marginal frequency threshold a an item X is said to be potentially frequent if X sup 2 a Lemma 1 If item X is frequent in D then it must be frequent in DuAD if its marginal frequency X,,-sup>l Proof let X,,,,and X,,p-o donate the supports of item X in D and DuAD respectively Since X is frequent in D that is for a given minimal support threshold 6 we have X,,,D>6 then in the case that X,a,-sup>l which means X grow faster that D we have Xrup_~u X,,,O therefore X is frequent in DuAD 0 We are highly concerned with whether the header table Ht rearranges its item frequency order due to the increment data set AD In the cases that Ht keeps the same item order after AD is added into D the update problem can be easily resolved simply by inserting each new item into the FP-Tree built from the previously accumulated database and in this sense the conventional FP-Growth approach is incremental But in the cases that Ht reorders its item order since the database D receives some new data the previously created FP-Tree can not be simply updated by the same conducts because the tree by this way will have the different structure from that built from the total data set The alternative is to build a new FP-tree with AD and then break up the previously built FP-Tree into frequent itemsets such that we can insert each frequent itemset into the new FP-Tree 237 


In this paper, the header table is updated based on both the minimal support threshold and the minimal marginal frequency threshold so that it holds not only the frequent items in database but also some infrequent but potentially frequent items 3.2 Algorithm Incremental FP-Growth Algorithm for Mining Association Before we introduce the algorithm description of the incremental FP-Growth we give a demonstration by a simple example As we have discussed above, we focus on how to update the header table and then update the previously created FT-Tree with the new coming data With respect to updating the header table we need scan the increment data set AD once and calculate each items both frequency in DuAD and marginal frequency If the header table does not rearrange its item order then each frequent item in AD is directly inserted into the FP-tree Otherwise a new FP-Tree FTA is built from AD and then the previously built FP-Tree FT is broken up into frequent itemsets so that each of which is inserted into the new FP-Tree FTA Example 3.1 Suppose that the initial database D contains five transaction itemsets A,C,D F B,F},{A,B,C,F}and{A,C,F After two scans of the transaction database, the FP-Tree FT and the header table Ht are generated as shown in Figure 1 IOI 4    D B   Figurel traversing a FP-Tree to get all the frequent itemsets Suppose that a new data set AD=\({D C B},\(D A C C E arrives then the frequency and marginal frequency of each item are computed The header table Ht is thus rearranged in the order of C:7 F:6 D:4 A:4,B:3 and E:l with respect to the given support threshold S=2 and the marginal frequency threshold o=l In this case a new FP-Tree FTA is constructed with AD and the updated Ht see figure 2 We now extract frequent itemsets from FP-Tree FT by traversing it twice After the first traverse we get itemsets F, C A D  F C, A B F B Their frequencies in the database are 2 I and I respectively which are subtracted from the frequencies of the corresponding nodes in the tree The tree becomes smaller by deleting the nodes whose frequency becomes zero So node D and B are deleted from FT<,,,,J after the first traverse The frequency of an itemset in the database is determined by the frequency of the terminal node in the path The tree FT now contains only three nodes F:l C:l and A:l in one the path so the itemset F C, A with frequency 1 is obtained by the second traverse See figure 1    Figure 2 FP-Tree FTA built with AD Figure 3 a new FT-tree by merging FT and FT The new created FP-Tree FTA continues to grow up by absorbing itemsets from FT,,,rcn Meanwhile the FP-Tree FT,,,,e is quickly contracting and completely disappeared when FTA ends its growth We can prove that the tree FTA built by this method is exactly the same as that built from the total dataset DuAD if both the support threshold and the marginal frequency threshold are small enough 238 


Example 3.2 Suppose the database D and incremental one AD be the sizes of 1000 and 100 respectively There are four items I 12 13 I4 and their frequencies 56,48,21,36 respectively in the header table Ht In the incremental data set AD the four items 11 12.13 I4 occur 16 28 18 and 3 respectively Assume that the given minimal support is 640 and the minimal marginal frequency is o=l We firstly calculate the marginal frequency of each item The four marginal frequencies for 11 12 13 I4 are 2.587 5.823 6.207 and 0.833 respectively and each item\222s frequency in DuAD is 11:72, 12:76 13:39 and 1,:39 The header table Hi then resort the four items in the new order of 12:76 I,:72 13:39 14:39 I3 and I4 are infrequent both in D and Dum but I3 is potentially frequent and thus kept in the header table Ht whereas I4 is deleted from Ht since it is neither frequent in D and nor infrequent in DuAD and nor potentially frequent Algorithm Incremental FP-Grow fro Mining Association Rules Input a FP-Tree FT,,,en a new arrived data set AD min-supp and min-mar-frequency Output a group of frequent patterns and association rules Begin Scan AD once to update each item\222s count in Ht Calculate each item\222s frequency in Dum Calculate each item\222s marginal frequency Re-sorting the header table Ht Delete the items from Ht that are neither frequent and nor potentially frequent If the order of items in Ht is rearranged then Delete all the links between the header table and the FP-Tree FT Create a new FP-Tree FTA from I AD 1  While  FT has child nodes  Depth-first traversing FT to get a set of itemsets IS For k=l to I IS I  for each itemset ISk in IS Rearrange the items in ISk according to the order of items in the header table Insert each item in ISk into FP-Tree FTa as the child of current node Update the frequency of the nodes in FTA with the frequency of ISx  Connect the new tree FTA to the header table Subtract the frequency of ISk from of its each node in the path of FT Delete a node and its subtree If its that frequency becomes 0 End for End while Call FP-Growth FT   Else  the order of the header table keeps as before Fori 1 to IAD Rearrange the order of items in ADi For each item in itemset ADi according to that of the header table Insert item into FP-Tree FTcu,,cn as a child of the current node Connect the new tree node to the header table Endfor j Endfor i Call FP-Growth FT   Endif End 3.3 Performance test There is no report or accurate analysis so far on the complexity of FP-Growth It is difficult to give an accurate expression of the time complexity of the above algorithm We provide an estimation of the time complexity of the algorithm Some experimental results about the performance of the above algorithm are provided in Figure 4 The data for performance test are retrieved from web pages so the database is dynamic We are concerned with the efficiency of the algorithm when compared with then conventional FP-Growth and its effectiveness to the dynamic data environment. Since the convention FP-Growth algorithm always re-run to find all the frequent patterns when new data flow arrives the first curse diagram in figure 4 shows that the conventional FP Growth algorithm is less efficient than our extending one after the database becomes item-frequency-order stable extending FP-Growth vs FP-Growth time\(seconds 25  m    2500 5700 12000 3300 8000 17500 database size FP-Growth  FP-Growth 239 


time consumption at different min-supports time\(sec elsecl Time consumption compared with FP-Growth  50  20 10  FP-Growth 0.01 0.2 0.4 0.6 0.8 1 min-support mles mined at different mi-suppoN rule number 2500 sire:26000 5 size:l775 1000 500 0.01 0.2 0.4 0.6 0.8 1 min-supportp Figure 4 Experimental results of performance test 4 Obtaining more general information Many data mining approaches search for interesting information in database at low levels That is the language of the mined rules is composed of the terms occurring in the database itself However people sometimes expect to get some high levels of knowledge abstraction which give them a clearer expression of information ahout the database. Generating a less number of the final rules by inducing the discovered rules which are similar at low level can help people better understand the domain problem In our application we pay more attention to finding the difference of customers preferences between Chinese cars and foreign cars or between expensive cars and inexpensive ones and etc than to finding the differences between each type of cars Generally an attribute-oriented induction takes as a database relation and a concept hierarchy for each attributes A concept hierarchy is a concept tree that forms a taxonomy of concepts ranging from a most general concept at the root to some representation of all attribute values at the leaves A concept hierarchy is actually the representation of background knowledge and therefore is built with the support of domain experts. In a concept tree a higher concept is formed by integrating the related lower concepts or attribute values into a symbol which may be highly abstract In this paper, three concept hierarchies see figure 5 are designed in our mining task The continuous attribute price is discreted into 9 intervals and 5 higher level concepts are designed for concept induction The attribute model-name is a category one and has a quite number of values which are classified into Chinese car and foreign car The attribute daik-access is also a continuous one that is discreted into six intervals and is further generalized three concepts Our purpose focus on finding the difference of customers preferences between Chinese cars and foreign cars the expensive cars and inexpensive cars and etc mice Figure 5 Concept hierarchies for the attributes price I I low-access mid-access high-access SO 50-100 Figure 6 Concept hierarchies for the attributes Access 240 


hinese cars Figure 7 Concept hierarchies for the attributes car-model 4 Applied to the Web-content based data mining In this section we present a real-world application of our approach In applying it to the Chinese largest car market whose Web site is located at htto://www.cheshi.com.cn  we found some interesting results about Chinese car consumption preference The data that are retrieved by a wrapper from the web pages contain 11 attributes among which the attributes model-name sub-category price pricing-date and daily-access are valuable to our mining task wramer incremental FP-Growth Figure 8 Frame of incremental FP-Growth applied to Web-content based data minine The association rules in table 1-3 are mined when the database incrementally reaches the size of about 17750 instances Table1 lists some rules that have strong association between the attributes prices daily_access and model-name The rules reflect the customers\222 higher preference to Chinese cars with price in the range of 100,000-200,000\(RMB than to those in other price ranges and Satana and Shenglong are the two major models that Chinese people prefer most Table 2 shows that Chinese customers are most interested in foreign cars of moderate price ranging from 300,000 to 500,00O\(RMB The association between price and daily_access is shown in table 3 from which we can see that inexpensive cars are highly associated with frequent daily access whereas expensive cars have strong relation with less daily access But the cars with price under 50,00O\(RMB is an exception because they show no attraction to customers Moderate price cars are moderately preferred by Chinese peoples Table I Preference to main models of Chinese cars Association rules Supp Conf Price:l&20 A Access:100-500  Santana 2.749 18.937 Price:IO-20 Access 50-100 Santana 7.489 53.415 PricclO-20 A Access 100-500 Shenglong 16S75 Price:lO-20 A Access 50.100 3 Shenglong Price:lO-20 A Access 100-500 2 FAW 2.441 32.757 0.621 22.581 Pricc-lO A Access 100-500 2 Xiali Priee:lO-20 A Access 50-100 3 Citroen 0.272 4.483 0.313 11.389 Price:5-lO A Access 100-500 a Changan 0.786 10.497 Price:IO-20 A Access 100-500 2 Chew 0.130 4.264 Price:30-50 A Access 50-100 2 Audi Table 2 Preferences to main models of foreign cars Association rules Supp Conf Price:30-~O~Access 50-100  Toyota 0.426 13.953 Price:30-5O~Access 50-100 3 Buick 0.414 13.566 Price:30-5O~Access 50-100 j Nissan 0.278 9.109 Price:30-5O~Access 50-100 3 Lexus o.102 3.295 0.012 0.388 Price:30-SO~Access 50-100  Benz 0.035 0.517 Access 50-100  BMW Table 3 Association between price and access Association rules Supp Conf Price 5 Access 100-500 0.898 1 1.428 Price 5-10 Access 100-500 2.749 83.937 Price 10-20 3 Access 100-500 7.489 53.415 Price 20-30  Access 100-500 2.353 29.926 Price 20-30  Access 50-100 2.737 39.879 Price 20-30 3 Access SO 8.884 13.519 Price 30-50 Access 50-100 3.050 80.000 Price 50-70  Access 60 11.319 17.224 Price 70-90  Access 50 5.426 8.311 Price 90-120 Access  50 1.608 2.446 Price I20  Access 50 2.994 4.416 241 


Table 4 More general rules induced with min supp=l O Association rules Supp  Conf Price low Access x mid 3.889 89.437 P\224ce\(x low-mid a Aceess\(x mid 18.648 92.875 Priee\(r mid  Access\(x mid 13.718 87.097 Price\(x mid-high Access x low 9.453 90.099 China_car\(x,\224Yes\224  mid 72.125 46.021 Foreign_car\(x,\224Yes\224 x law 26.389 89.882 Price\(x,low_mid x,\224Yes\224 17.203 61.223  x mid 12.568 94.018 Price\(x mid A Foreigr_car\(x 223Yes\224  Accessfx low   5.219 98.914 Priee\(x.high 224Y~~\222 3 Access\(x low 5 Conclusion An incremental FP-Growth algorithm for mining association rules is proposed in this paper It extends the FP-Growth algorithm to dynamic data environments. The web-content based data mining is a typical example of such kind dynamic environments Both experimental results on performance testing and the real-world application have shown its high effectiveness The web content based application discovered some interesting results about Chinese customers\222 car consumption preference This extended FP-Growth algorithm has demonstrated its great potential in discovering usual association rules from dynamic environments which placed the algorithm in a position to be a strong candidate as a good data mining tool among the other algorithms such as classification clustering sequential patterns mining etc The system is implemented with VC 6.0 under the platform of windows 98/2000 and has been put into practical use One of the important issues which can be investigated in the future is the reliability of the discovered association rules References I Cooley R Mobasher B  Srivastava, J\(1999 Web Mining Information and Pattern Discovery on the WorldWideWeb,htto://wwwusers.cs.umn.edu/-moha sher/webminer/survevsurve~.html Doorenbos,R.,\( 1997 Wrapper induction for 2 Kushmerick,N Daniel Weld  information extraction Proceeding of the 15th International Joint Conference on Artificial Intelligence. \(pp. 729-737 3 Kushmerick,N.\(1997 Wrapper induction for information extraction Ph.D Dissertation Dept of Computer Science Univ of Washington TR UW CSE-97-11-04 4 Adelberg,B.,\(1998  A tool for semi automatically extracting structured and semistructured data from text documents Proceedings of SIGMOD\22298.\(pp. 283-294 5 Liu,L Pu,C  Han,W 2000 Xwrap An XML enabled Wrapper Construction System for Web Information Sources  International Conference on Data Engineering, San Diego, CA 6 J Han J Pei and Y. Yin, \223Mining Frequent Patterns without Candidate Generation\224 SIGMOD Conference 2000 1-12 7 Webb, Geoffrey L 223Efficient search for association rules\224 In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining pages 99-107 2000 SI Zheng, Zijian, Kohavi, Ron, and Mason, Llew, \223Real World Performance of Association Rule Algorithms\224 In proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining San Francisco Califomia, August 2001 9 Ashoka Savasere, Edward Omiecinski, and Shamkant B Navathe 223An Efficient Algorithm for Mining Association Rules in Large Databases\224 VLDB 1995 432-444 10]R Agrawal and R Srikant Fast algorithms for mining association rules VLDB94 487-499 Santiago, Chile I 1]R I Bayardo Efficiently mining long patterns from databases SIGMOD98.85-93 Seattle Washington  Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket analysis SIGMOD97 255-264 Tucson, Arizona, May 1997  W Cheung  H.Y.Hwang ADA W Fu and J Han. Efficient rule-based attribute-oriented induction for data mining Journal of intelligent information systems 15,175-200,2000 14]Colin L.Carter and Howard I Hamiton Efficient Attribute-oriented generalization for knowledge discovely from large database IEEE transaction on knowledge and data engineering Vol  IO,No.2,March/April 1998 242 


 1   1   l  1 i 2  i  Hence the true frequency count of any item occurring on some input stream must be C  l  1 j  x 1  tc  j    j    where  is a small quantity 3  The number of items present in each input stream is thus n  C 4  Since synopses for d l  1  x input streams are transmitted through a node at level x  the load on the most heavily loaded link\(s is L  x  d l  2  x  n C  Clearly the maximum value of L  x  is achieved when   0  The expression for L  x  can be simpli“ed to L  x  1  l  1 j  x 1  j  d x  j 1  Now our expression for the worst-case load on any link can be reduced to W  T    max x 0  1 l  2  L  x   We desire to minimize W  T   subject to the constraints  2   l  1  0 and  l  1 j 2  j   1 It is easy to show that this minimum is achieved when L 0  L 1    L  l  2  Solving for  2   l  1  we obtain  i   1  d  1  l  2   d  1 d  2  i  l  2 and  l  1   1  d  l  2   d  1 d  Translating to error tolerances we set  i   1   l  1  i    d  1 d  l  2   d  1 d for all 2  i  l  1  This setting of  2  l  1 minimizes worst-case communication load on any link We term this strategy MinMaxLoad WC Under this strategy the maximum possible load on any link is L wc   l  2   d  1 d d   1 counts per epoch Lastly we note that MinMaxLoad WC remains the optimal precision gradient even if nodes of the same level can have different  values Informally since with worst-case inputs all incoming streams have identical characteristics maximum link load cannot be improved by using non-uniform  values for nodes at a given level we omit a formal proof for brevity 2.1.4 Good Precision Gradients for Non-Worst-Case Inputs Real data is unlikely to exhibit worst-case characteristics Consequently strategies that are optimal in the worst case may not always perform well in practice In terms of minimizing the maximum communication load on any link the worst-case inputs are ones in which the set of items occurring on each input stream are disjoint When this situation arises a gradual precision gradient is best to use as shown in Section 2.1.3 Using a gradual precision gradient some of the pruning of frequency 3 Recall that we allow the frequency of an item to be a real number 4 More precisely each stream contains  n C  items of weight 1 each and one item of weight  n C  n C   Note that each input stream contains at most one item with weight less than 1 as stipulated earlier counts is delayed until a better estimate of the overall distribution is available closer to the root thereby enabling more effective pruning In the opposite extreme when all input streams contain identical distributions of item occurrences there is no bene“t to delaying pruning and performing maximal pruning at the leaf nodes as in strategy SS2 is most effective at minimizing communication In fact it is easy to show that SS2 is the optimal strategy for minimizing the maximum load on any link when all input streams are comprised of identical distributions we omit a formal proof Note however that SS2 still incurs a high space requirement on the root node R since it sets  1    We posit that most real-world data falls somewhere between these two extremes To determine where exactly a data set lies with regard to the two extremes we estimate the commonality between input streams S 1 S m by inspecting an epoch worth of data from each stream We compute a commonality parameter   0  1 as   1 m   m i 1 G i L i  where G i and L i are de“ned over stream S i as follows The quantity G i is de“ned as the number of distinct items occurring in S i that occur at least   S i  times in S i and also at least   S  times in S  S 1  S 2  S m  where  S  denotes the number of item occurrences in S during the epoch of measurement The quantity L i is de“ned as the number of distinct items occurring in S i that occur at least   S i  times in S i  Hence commonality parameter  measures the fraction of items frequent enough in one input stream to be included in a leaf-level synopsis by strategy SS 2 that are also at least as frequent globally in the union of all input streams A natural hybrid strategy is to use a linear combination of MinMaxLoad WC and SS2 weighted by   The strategy is as follows set  i 1       1   l  1  i    d  1 d  l  2   d  1 d        for 2  i   l  2  and  l  1 1       1  d  l  2   d  1 d         We term this hybrid strategy MinMaxLoad NWC for non-worstcase Commonality parameter  1 implies that locally frequent items are also globally frequent and SS2 modi“ed to use  1   is a good choice Conversely  0 indicates that MinMaxLoad WC is a good choice For 0  1  a weighted mixture of the two strategies is best 2.1.5 Summary The precision gradient strategies we have introduced are summarized in Table 3 Sample precision gradients are illustrated in Figure 3 3 Experimental Evaluation In this section we evaluate the performance of our newly-proposed strategies for setting the precision graProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


Table 3 Summary of precision gradient settings studied  Strategy Description Section Introduced Simple Strategy 1 SS1 Transmits raw data to root node R 1.3 Simple Strategy 2 SS2 Reduces data maximally at leaves 1.3 MinRootLoad Minimizes total load on root in all cases 2.1.2 MinMaxLoad WC Minimizes worst-case maximum load on any link 2.1.3 MinMaxLoad NWC Achieves low load on heaviest-loaded link under non-worst-case inputs 2.1.4 0 0.0002 0.0004 0.0006 0.0008 0.001 43210 Tree level \(i SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC Error tolerance  i input            leaf root Figure 3 Precision gradients for  0  001   0  5  dient using the two na  ve strategies suggested in Section 1 as baselines We begin in Section 3.1 by describing the real-world data and simulated distributed monitoring environment we used Then in Section 3.2 we analyze the data using our model of Section 2.1.4 to derive appropriate parameters for our MinMaxLoad NWC strategy that is geared toward performing in the presence of non-worst-case data We report our measurements of space utilization on node R in Section 3.3 and provide measurements of communication load in Section 3.4 3.1 Data Sets As described in Section 1 our motivating applications include detecting DDoS attacks and monitoring hot spots in large-scale distributed systems For the rst type of application we used traf“c logs from Internet2 and sought to identify hosts recei ving lar ge numbers of packets recently For the second type we sought to identify frequently-issued SQL queries in two dynamic Web application benchmarks con“gured to execute in a distributed fashion The I NTERNET 2 traf c traces were obtained by collecting anonymized net”ow data from nine core routers of the Abilene network Data were collected for one full day of router operation and were broken into 288 ve-minute epochs To simulate a larger number of nodes we divided the data from each router in a random fashion We simulated an environment with 216 network nodes which also serve as monitor nodes For the web applications we used Java Servlet versions of two publicly available dynamic Web application benchmarks RUBiS and R UBBoS 10 R U BiS is modeled after eBay an online auction site and RUBBoS is modeled after slashdot an online bulletin-board so we refer them as AUCTION and BBOARD  respectively We used the suggested con“guration parameters for each application and ran each benchmark for 40 hours on a single node.We then partitioned the database requests into 216 groups in a roundrobin fashion honoring user session boundaries We simulated a distributed execution of each benchmark with 216 nodes each executing one group of database requests and also serving as a monitor node For all data sets we simulated an environment with 216 monitoring nodes  m  216  and a communication hierarchy of fanout six  d 6  Consequently our simulated communication hierarchy consisted of four levels including the root node  l 4  We set s 0  01   0  1  s  and  1 0  9    Our simulated monitor nodes used lossy counting in batch mode whereby frequency estimates were reduced only at the end of each epoch in all cases less than 64KB of buffer space was used to create synopses over local streams The epoch duration T was set to 5 minutes for the I NTERNET 2 data set and 15 minutes for the other two data sets 3.2 Data Characteristics Using samples of each of our three data sets we estimated the commonality parameter  for each data set Recall that we use  to parameterize our strategy MinMaxLoad NWC presented in Section 2.1.4 We obtained  values of 0.675 0.839 and 0.571 for the I NTER NET 2 AUCTION and BBOARD data sets respectively Hence the AUCTION data set exhibited the most commonality among all three data sets Results presented in Section 3.4 show that AUCTION indeed has the most commonality 3.3 Space Requirement on Root Node Figure 4 plots space utilization at the root node R as a function of time in units of epochs using Algorithm 2a to generate the synopsis for different values of the decay parameter   using two different strategies for the precision gradient The plots shown are for the I N TERNET 2 data set The y-axis of each graph plots the current number of counts stored in the     synopsis S A maintained by the root node R  Figure 4a plots synopsis size under our MinMaxLoad WC strategy under three different values of   0.6 0.9 and 1 As predicted by our analysis in when  1 the size of S A remains roughly constant after reaching steady-state whereas when  1 synopsis size increases logarithmically with time similar results were obtained for the Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 0 100 200 300 400 500 600 1 21416181 Time \(epoch counts 0.6 0.9 1.0 a MinMaxLoad WC 0 1000 2000 3000 4000 5000 6000 7000 8000 1 21416181 Time \(epoch counts 0.6 b SS2 Figure 4 Space needed at node R to store answer synopsis S A  non-distributed single-stream case In contrast when SS2 is used to set the precision gradient Figure 4b the space requirement is almost an order of magnitude greater This difference in synopsis size occurs because in SS2 frequency counts are only pruned from synopses at leaf nodes so counts for all items that are locally frequent in one or more local streams reach the root node No pruning power is reserved for the root node and therefore no count in S A is ever discarded irrespective of the  value The same situation occurs if Algorithm 2b is used instead of Algorithm 2a This result underscores the importance of setting  1  in order to limit the size of S A  as discussed in Section 2.1 3.4 Communication Load Figure 5 shows our communication measurements under each of our two metrics for each of our three data sets under each of the ve strategies for setting the precision gradient listed in Table 3 First of all as expected the overhead of SS1 is excessive under both metrics Second by inspecting Figure 5a we see that strategy MinRootLoad does indeed incur the least load on the root node R in all cases as predicted by our analysis of Section 2.1.2 Under this metric MinRootLoad outperforms both simple strategies SS1 and SS2 by a factor of 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 247k 10k 296k 132k 20k counts transmitted \(per epoch a Load on root node R 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 43k    21k 52k   19k 23k   7k counts transmitted \(per epoch b Maximum load on any link Figure 5 Communication measurements k denotes thousands ve or more in all cases measured However MinRootLoad performs poorly in terms of maximum load on any link as shown in Figure 5b because no early elimination of counts for infrequent items is performed and consequently synopses sent from the grand-children of the root node to the children of the root node tend to be quite large As expected MinMaxLoad NWC performs best under that metric on all data sets For the AUCTION data set even though SS2 outperforms MinMaxLoad WC to be expected because of the high  value our hybrid strategy MinMaxLoad NWC is superior to SS2 by a factor of over two For the I NTERNET 2 and BBOARD data sets the improvement over SS2 is more than a factor of three On the negative side total communication not shown in graphs is somewhat higher under MinMaxLoad WC than under SS2 increase of between 7.5 and 49.5 depending on the data set 4 Related Work Most prior work on identifying frequent items in data streams 6 8 9 19 only considers the single-stream case While we are not aware of any work on maintaining frequency counts for frequent items in a distributed stream setting work by Babcock and Olston does adProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


dress a related problem In the problem is to monitor continuously changing numerical values which could represent frequency counts in a distributed setting The objective is to maintain a list of the top k aggregated values where each aggregated value represents the sum of a set of individual values each of which is stored on a different node The work of assumes a single-le v e l communication topology and does not consider how to manage synopsis precision in hierarchical communication structures using in-network aggregation which is the main focus of this paper The work most closely related to ours is the recent work of Greenwald and Khanna which addresses the problem of computing approximate quantiles in a general communication topology Their technique can be used to nd frequencies of frequent items to within a con“gurable error tolerance The work in focuses on providing an asymptotic bound on the maximum load on any link our result adheres to the same asymptotic bound It does not however address how best to con“gure a precision gradient in order to minimize load which is the particular focus of our work 5 Summary In this paper we studied the problem of nding frequent items in the union of multiple distributed streams The central issue is how best to manage the degree of approximation performed as partial synopses from multiple nodes are combined We characterized this process for hierarchical communication topologies in terms of a precision gradient followed by synopses as they are passed from leaves to the root and combined incrementally We studied the problem of nding the optimal precision gradient under two alternative and incompatible optimization objectives 1 minimizing load on the central node to which answers are delivered and 2 minimizing worst-case load on any communication link We then introduced a heuristic designed to perform well for the second objective in practice when data does not conform to worst-case input characteristics Our experimental results on three real-world data sets showed that our methods of setting the precision gradient are greatly superior to na  ve strategies under both metrics on all data sets studied Acknowledgments We thank Arvind Arasu and Dawn Song for their valuable input and assistance References  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB  1994  I Akamai T echnologies Akamai http://www akamai.com   A Ak ella A Bharambe M Reiter  and S Seshan Detecting DDoS attacks on ISP networks In PODS Workshop on Management and Processing of Data Streams  2003  A Arasu and G S Manku Approximate quantiles and frequency counts over sliding windows In PODS  2004  B Babcock and C Olston Distrib uted top-k monitoring In SIGMOD  2003  M Charikar  K  Chen and M F arach-Colton Finding frequent items in data streams In International Colloquium on Automata Languages and Programming  2002  E Cohen and M Strauss Maintaining time-decaying stream aggregates In PODS  2003  G Cormode and S Muthukrishnan What s hot and whats not Tracking frequent items dynamically In PODS  2003  E D Demaine A Lopez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space In European Symposium on Algorithms  2003  DynaServ er R UBis and R UBBos http://www.cs rice.edu/CS/Systems/DynaServer   eBay Inc eBay  http://www.ebay.com   C Estan and G V a r ghese Ne w directions in traf c measurement and accounting In SIGCOMM  2002  M F ang N Shi v akumar  H  Garcia-Molina R Motwani and J Ulmann Computing iceberg queries ef“ciently In VLDB  1998  P  B Gibbons and Y  Matias Ne w sampling-based summary statistics for improving approximate query answers In SIGMOD  1998  P  B Gibbons and S T irthapura Estimating simple functions on the union of data streams In Symposium on Parallel Algorithms and Architectures  2001  M Greenw ald and S Khanna Po wer conserving computation of order-statistics over sensor networks In PODS  2004  J Han J Pei G Dong and K W ang Ef cient computation of iceberg queries with complex measures In SIGMOD  2001  Internet2 Internet2 Abilene Netw ork http abilene.internet2.edu   R M Karp S Shenk er  and C H P apadimitriou A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  2003  H.-A Kim and B Karp Autograph T o w ard automated distributed worm signature detection In Proceedings of the 13th Usenix Security Symposium  2004  A Manjhi V  Shkapen yuk K Dhamdhere and C Olston Finding recently frequent items in distributed data streams Technical report 2004 http://www cs.cmu.edu/˜manjhi/freqItems.pdf   G S Manku and R Motw ani Approximate frequenc y counts over data streams In VLDB  2002  Open Source De v elopment Netw ork Inc Slashdot http://slashdot.org  Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


