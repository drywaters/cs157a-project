Mining Association Rules from Relations on a Parallel NCR Teradata Database System Soon M. Chung and Murali Mangamuri Department of Computer Science and Engineering Wright State University Dayton, Ohio 45435, USA schung@cs.wright.edu Abstract Data mining from relations is becoming increasingly important with the advent of parallel database systems. In this paper, we propose a new algorithm for mining association rules from relations.  The new algorithm is an enhanced version of the SETM algorithm [3  a nd it re duc e s the  num be r of  c a ndida te  itemsets considerably.  We implemented and evaluated the new The 
new algorithm is much faster than the SETM algorithm, and its performance is quite scalable Key words data mining, association rules, parallel database system, performance analysis 1 Introduction very from databases, is the process of finding useful patterns from databases.  One of the useful patterns is the association rule, and it is formally described in [1 a s f o llow s: L e t I i 1 i  i m  be a set of items. Let D represent a set of transactions, where each transaction T contains a set of items, such that T 
001 I. Each transaction is associated with a unique identifier, called transaction identifier \(TID\A set of items X is said to be in transaction T if X 002 An association rule is an implication of the form X => Y, where X 002 I, Y 002 I and X 001\002\001\003\001 003 The rule X => Y holds in the database D with confidence c if c% of the transactions in D that contain X also contain Y. The rule X Y has a support s if s% of the transactions in D contain X U Y For example, beer and disposable diapers are items such that 
beer => diapers is an association rule mined from the database if the co-occurrence rate of beer and disposable diapers \(in the same transaction\ not less than the minimum support and the occurrence rate of diapers in the transactions containing beer is not less than the minimum confidence The problem of mining association rules is to find all the association rules that have support and confidence greater than or equal to the user specified minimum support and minimum confidence, respectively. This problem can be decomposed into the following two steps 1 Find all sets of items \(called itemsets\t have 
support above the user specified minimum support These itemsets are called frequent itemsets or large itemsets 2 For each frequent itemset, all the association rules that have minimum confidence are generated as follows: for every frequent itemset f, find all nonempty subsets of f. For every such subset a, generate a rule of the form a => \(f - a\ if the ratio of support\(f\ to support\(a\ is at least the minimum confidence Finding all the frequent itemsets is a really resource consuming task, but generating all the valid association rules from the frequent itemsets is quite straightforward There are many association rule mining algorithms proposed [1 
 H o w e v e r, m o s t of the s e  a l g o rithm s  a r e  de s i g n e d  for data stored in file systems. Considering that relational databases are widely used to manage the corporation data integrating the data mining with the relational database system is important. A methodology for tightly coupling a mining algorithm with relational database using user-defined functions is proposed in [8  a nd a de ta ile d s t udy of v a rious a r c h ite c t ura l  is presented in [9   The SETM algorithm proposed in [3 w a s e x pre s s e d in the  f o rm  of SQL queries. Thus, it can be easily applied to relations in the relational databases, and can take advantage of the 
functionalities provided by the SQL engine, such as the query optimization, efficient execution of relational algebra operations, and indexing. SETM can be also easily implemented on a parallel database system which can execute the SQL queries in parallel on different processing nodes. By processing the relations directly, we can easily relate the mined association rules to other information in the same database, such as the customer information In this paper, we propose a new algorithm named Enhanced SETM \(ESETM\hich is an enhanced version of the SETM algorithm.  We implemented both ESETM and SETM on a parallel NCR Teradata database system, and evaluated and 
compared their performance for various cases.  It has been shown that ESETM is considerably faster than SETM. Our NCR Teradata database system is described in Section 2, and the SETM and ESETM algorithms are presented in Sections 3 and 4, respectively. The results of performance analysis are given in Section 5 Proceedings of the International Conference on Info rmation Technology: Coding and Computing \(ITCC\22204 0-7695-2108-8/04 $ 20.00 \251 2004 IEEE 


2 NCR Teradata Database System The algorithms are implemented on a NCR Teradata database system. It has two nodes, where each node consists of 4 Intel 700 MHz Xeon processors, 2 GB shared memory, and 36 GB disk space. The nodes are interc onnected by a dual BYNET interconnection network supporting 960 Mbps of data bandwidth for each node. Moreove r, nodes are connected to an external disk storage subsystem configured as a level-5 RAID Redundant Array of Inexpensive Disks\ with 288 GB disk space Figure 1. Teradata system architecture The Relational DBMS used here is Teradata RDBMS \(version 2.4.1\hich is designed specifically to function in the parallel environment. The hardware that supports Teradata RDBMS software is based on off-the-shelf Symmetric Multiprocessing SMP\chnology. The hardware is combined with a communication network \(BYNET\ that connects the SMP systems to form Massively Parallel Processing \(MPP\ systems as shown in Figure 1  The versatility of the Teradata RDBMS is based on virtual processors \(vprocs\ that eliminate the dependency on specialized physical processors Vprocs are a set of software processes that run on a node within the multitasking environment of the operating system. Each vproc is a separate independent copy of the processor software, isolated from other vprocs, but sharing some of the physical resources of the node such as memory and CPUs  Vprocs and the tasks running under them communicate using the unique-address messaging as if they were physically isolated from one anothe r. The Parsing Engine \(PE\ccess Module Processor \(AMP\re two types of vprocs. Each PE executes the database software that manages sessions decomposes SQL statements into steps, possibly parallel, and returns the answer rows to the requesting client. The AMP is the heart of the Teradata RDBMS. The AMP is a vproc that performs many database and file management tasks. The AMPs control the management of the Teradata RDBMS and the disk subsystem. Each AMP manages a portion of the physical disk space, and stores its portion of each database table within that disk space. The AMPs are responsible for obtaining the rows required to process the requests \(assuming that the AMPs are processing a SELECT statement\ If a file is accessed through the primary index and the request is for a single row, the PE transmits the operation steps to a single AMP, as shown at PE1 in Figure 2. If the request is for many rows \(an all-AMP request\the PE broadcasts the operation steps to all AMPs, as shown at PE2 in Figure 2.  The Teradata RDBMS uses hashing to distribute data to disks  Figure 2. Query processing in the Teradata system 3. SETM Algorithm The SETM algorithm proposed in [3  f o r  f i nding  f r e que nt itemsets and the corresponding SQL queries used are as  SALES = <trans_id, item k := 1 sort SALES on item F 1 set of frequent 1-itemsets and their counts R 1 filter SALES to retain supported items repeat k := k + 1 sort R k-1 on trans_id, item 1 item k-1  R\220 k merge-scan R k-1 R 1  sort R\220 k on item 1 item k  F k generate frequent k-itemsets from the sorted R\220 k  R k filter R\220 k to retain supported k-itemsets until R k  In this algorithm, initially all frequent 1-itemsets and their respective counts \(F 1 item, count>\re generated by a simple sequential scan over the SALES table. After creating F 1 R 1 is created by filtering SALES using F 1 A merge-scan is performed for creating R\220 k table using R k-1 and R 1 tables. R\220 k  viewed as the set of candidate k-itemsets coupled with their transaction identifiers SQL query for generating R\220 k  INSERT INTO R\220 k SELECT p.trans_id, p.item 1 p.item k-1 q.item FROM R k-1 p, R 1 q WHERE q.trans_id = p.trans_id AND q.item > p.item k-1 Frequent k-itemsets are generated by a sequential scan over R\220 k msets that meet the minimum support constraint Proceedings of the International Conference on Info rmation Technology: Coding and Computing \(ITCC\22204 0-7695-2108-8/04 $ 20.00 \251 2004 IEEE 


SQL query for generating F k  INSERT INTO F k SELECT p.item 1 p.item k COUNT FROM R\220 k p GROUP BY p.item 1 p.item k HAVING COUNT\(*\=  :minimum_support R k table is created by filtering R\220 k table using F k R k  viewed as a set of frequent k-itemsets coupled with their transaction identifiers. This step is performed to ensure that only the candidate k-itemsets \(R\220 k lative to frequent kitemsets are used to generate  the candidate \(k+1\temsets SQL query for generating R k  INSERT INTO R k SELECT p.trans_id, p.item 1 p.item k FROM R\220 k p, F k q WHERE p.item 1 q.item 1 AND   p.item k-1 q.item k-1 AND p.item k q.item k ORDER BY p.trans_id, p.item 1 p.item k A loop is used to implement the procedure described above and the number of iterations depends on the size of the largest frequent itemset, as the procedure is repeated until F k is empty 4. Enhanced SETM \(ESETM The Enhanced SETM \(ESETM\rithm has three modifications to the original SETM algorithm 1 Create frequent 2-itemsets without materializing R 1 and R\220 2  2 Create candidate \(k+1\sets in R\220 k+1 by joining R k with itself 3 Use a subquery to generate R k rather than materializing it, thereby generating R\220 k+1 directly from R\220 k The number of candidate 2-itemsets can be very large, so it is inefficient to materialize R\220 2 table. Instead of creating R\220 2 table ESETM creates a view or a subquery to generate candidate 2itemsets and directly generates frequent 2-itemsets. This view or subquery is also used to create candidate 3-itemsets CREATE VIEW R\220 2 trans_id, item 1 item 2 S SELECT P1.trans_id, P1.item, P2.item FROM \(SELECT p.trans_id, p.item FROM SALES p, F 1 q WHERE p.item = q.item\ AS P1 SELECT p.trans_id, p.item FROM SALES p, F 1 q WHERE p.item = q.item\ AS P2 WHERE P1.trans_id = P2.trans_id AND P1.item < P2.item Note that R 1 is not created since it will not be used for the generation of R\220 k The set of frequent 2-itemsets, F 2  directly generated by using this R\220 2 view INSERT INTO F 2 SELECT item 1 item 2 COUNT FROM R\220 2 GROUP BY item 1 item 2 HAVING COUNT\(*\= :minimum_support The second modification is to generate R\220 k+1 using the join of R k with itself, instead of the merge-scan of R k with R 1  SQL query for generating R\220 k+1  INSERT INTO R\220 k+1 SELECT p.trans_id, p.item 1 p.item k q.item k FROM R k p, R k q WHERE p.trans_id = q.trans_id AND p.item 1 q.item 1 AND   p.item k-1 q.item k-1 AND p.item k q.item k This modification reduces the number of candidates \(k+1 itemsets generated compared to the original SETM algorithm For example, if {1, 2, 3} is a frequent 3-itemset and {1, 2, 3, 5 8} are the items in a transaction. The candidate 4-itemsets produced with frequent itemset {1, 2, 3} for this transaction are 1, 2, 3, 5} and {1, 2, 3, 8} in SETM if items 5 and 8 have the minimum support. In ESETM, {1, 2, 3, 5} is included into R\220 4 only if {1, 2, 5} is also frequent. Similarly, {1, 2, 3, 8} is included only if {1, 2, 8} is also frequent.   This step will also ce in the later iterations where the number of tuples in R k is less than that of R 1 The performance of the algorithm can be improved further if candidate \(k+1\msets are directly generated from candidate k-itemsets using a subquery as follows SQL query for R\220 k+1 using R\220 k  INSERT INTO R\220 k+1 SELECT P1.trans_id, P1.item 1 P1.item k P2.item k FROM SELECT p.* FROM R\220 k p, F k q WHERE p.item 1 q.item 1 AND  . . .   AND p.item k q.item k S P1 SELECT p.* FROM R\220 k p, F k q WHERE p.item 1 q.item 1 AND . . .  AND p.item k q.item k S P2 WHERE     P1.trans_id = P2.trans_id AND P1.item 1 P2.item 1 AND   P1.item k-1 P2.item k-1 AND P1.item k P2.item k R k is generated as a derived table using a subquery, thereby the cost of materializing R k table is saved Proceedings of the International Conference on Info rmation Technology: Coding and Computing \(ITCC\22204 0-7695-2108-8/04 $ 20.00 \251 2004 IEEE 


ESETM with Pruning PSETM In the ESETM algorithm candidate \(k+1\msets in R\220 k+1 are generated by joining R k with itself on the first k-1 items, as described above. For example, a 4-itemset {1, 2, 3, 9} becomes a candidate 4-itemset only if {1, 2, 3} and {1, 2, 9} are frequent 3-itemsets. It is different from the subset-infrequency based pruning of the candidates used in the Apriori algorithm, where a \(k+1\temset becomes a candidate \(k+1\temset only if all of its k-subsets are frequent.  So, {2, 3, 9} and {1, 3, 9} should be also frequent for 1, 2, 3, 9} to be a candidate 4-itemset. The above SQL-query for generating R\220 k+1 can be modified, such that all the k-subsets of each candidate \(k+1\temset can be checked. To simplify the presentation, we divided the query into subqueries. Candidate k+1\msets are generated by the Subquery Q 1 using F k Subquery Q 0  SELECT item 1 item 2 item k FROM F k Subquery Q 1  SELECT p.item 1 p.item 2 p.item k q.item k FROM F k p, F k q WHERE p.item 1 q.item 1 AND   p.item k-1 q.item k-1 AND p.item k q.item k AND p.item 2 p.item k q.item k IN \(Subquery Q 0 ND   p.item 1 p.item j-1 p.item j+1 p.item k q.item k  Subquery Q 0 ND   p.item 1 p.item k-2 p.item k q.item k Subquery Q 0  Subquery Q 2  SELECT p.* FROM R\220 k p, F k q WHERE  p.item 1 q.item 1 AND  . . . .  AND p.item k q.item k INSERT INTO R\220 k+1 SELECT p.trans_id, p.item 1 p.item k q.item k FROM \(Subquery Q 2 p, \(Subquery Q 2  WHERE p.trans_id = q.trans_id AND p.item 1 q.item 1 AND   p.item k-1 q.item k-1 AND p.item k q.item k AND p.item 1 p.item k q.item k IN Subquery Q 1  The Subquery Q 1 joins F k with itself to generate the candidate k+1\temsets, and all candidate \(k+1\temsets having any infrequent k-subset are pruned.   The Subquery Q 2 derives R k  and R\220 k+1 is generated as R\220 k+1 R k JOIN R k JOIN \(Subquery Q 1  However, it\220s not efficient to prune all the candidates in all the passes, since the cost of pruning the candidates in the Subquery Q 1 is too high when there are not many candidates to be pruned until the number of rows in F k becomes less than 1000, or up to 5 passes The difference between the total execution times with and without pruning was very small for most of the databases we tested Using the Identifiers of Candidate Itemsets  The performance of PSETM can be improved further by generating the candidate k-itemsets and storing their ids in R\220 k table instead of storing the actual candidate k-itemsets in R\220 k table. In this case, R k contains only two columns: transaction id and the id of the candidate k-itemset. As a result, the size of  R\220 k table is reduced, and the processing time decreases when the data set is large and the minimum support level is low. Candidate \(k+1 itemsets are generated by joining two frequent k-itemsets on the first k-1 items, and each candidate \(k+1\temset is assigned a unique id.  For example, frequent 3-itemsets {A, B, C} and {A B, D} are joined to form a candidate 4-itemset {A, B, C, D} if B, C, D} and {A, C, D} are also frequent. To generate R\220 k+1 efficiently from R\220 k ps the id of the generated candidate \(k+1\mset to the ids of the two joined frequent k-itemsets.  For example, suppose that F 3 contains the following frequent 3-itemsets 3-itemset_id    i1 i2 i3 1         A  B  C 2         A  B  D 3         A  C  D 4         B  C  D 5         D  E  F 6         D  E  G 7         D  F  G 8         E  F  G    4-itemset_id     i1  i2  i3  i4 1          A   B   C  D 2          D   E   F  G   Then, the mapping table MP 4 4itemset to the two joined frequent 3-itemsets is as follows 4-itemset_id    3-itemset_id1   3-itemset_id2 1                     1                      2 2                     5                      6   Using this mapping table, R k+1 can be generated directly from R\220 k by using the following query.  The resulting R k+1 table also has two columns: transaction id and \(k+1\mset_id Proceedings of the International Conference on Info rmation Technology: Coding and Computing \(ITCC\22204 0-7695-2108-8/04 $ 20.00 \251 2004 IEEE 


INSERT INTO R\220 k+1 SELECT P1.trans_id, q.\(k+1\mset_id FROM \(SELECT p.trans_id, p.k-itemset_id FROM R\220 k p, \(SELECT DISTINCT k-itemset_id1 FROM MP k+1 S q WHERE p.k-itemset_id = q.k-itemset_id1\S P1 SELECT p.trans_id, p.k-itemset_id FROM R\220 k p, \(SELECT DISTINCT k-itemset_id2 FROM MP k+1 S q WHERE p.k-itemset_id = q.k-itemset_id2\S P2 MP k+1 q WHERE q.k-itemset_id1 = P1.k-itemset_id AND q.k-itemset_id2 = P2.k-itemset_id AND P1.trans_id = P2.trans_id The overhead associated with the generation of the candidate kitemsets and the mapping table is very small compared to the performance gain due to the space reduction in R\220 k table 5. Performance Analysis In this section, the performance of the Enhanced SETM ESETM\ESETM with pruning \(PSETM\nd SETM are evaluated and compared.  However, we didn\220t use the method of storing the identifiers of the candidate k-itemsets in R\220 k table We used synthetic transaction databases generated according to the procedure described in [2    0 200 400 600 800 1000 1200 1.00 0.50 0.25 0.10 Minimum Support Ti m e  s e c  SETM ESETM PSETM The total execution times of ESETM, PSETM and SETM are shown in Figure 3 for the database T10.I4.D100K, where Txx.Iyy.DzzzK indicates that the average number of items in a transaction is xx, the average size of maximal potential frequent itemset is yy, and the number of transactions in the database is zzz in t housands ESETM is more than 3 times faster than SETM for all minimum support levels, and the performance gain increases as the minimum support level decreases. ESETM and PSETM have almost the same total execution time, because the effect of the reduced number of candidates in PESTM is offset by the extra time required for the pruning The time taken for each pass by the algorithms for the T10.I4.D100K database with the minimum support of 0.25% is shown in Figure 4. The second pass execution time of ESTM is much smaller than that of SETM because R\220 2 table \(containing candidate 2-itemsets together with the transaction identifiers and R 2 table \(containing frequent 2-itemsets together with the transaction identifiers\are not materialized. In the later passes the performance of ESETM is much better than that of SETM because ESTM has much less candidate itemsets generated and does not materialize R k tables, for k > 2 0 50 100 150 200 250 9 sses Ti m e  s e c  SETM ESETM PSETM In Figure 5, the size of R\220 k table containing candidate k-itemsets is shown for each pass when the T10.I4.D100K database is used with the minimum support of 0.25%. From the third pass, the size of R\220 k table for ESETM is much smaller than that of SETM because of the reduced number of candidate itemsets. PSETM performs additional pruning of candidate itemsets, but the difference in the number of candidates is very small in this case 0 500 1000 1500 2000 2500 R\2203 2204 R\2205 2206 R\2207 2208 R\2209 N o  of T u p l es  i n 1000s  SETM ESETM PSETM The scalability of the algorithms is evaluated by increasing the number of transactions and the average size of transactions Figure 6 shows how the three algorithms scale-up as the number of transactions increases. The database used here is T10.I4 and the minimum support is 0.5%. The number of transactions ranges from 100,000 to 400,000. SETM performs poorly as the Figure 3. Total execution times \(for T10.I4.D100K Figure 4. Per pass execution times \(for T10.I4.D100K Figure 5. Size of R\220 k for T10.I4.D100K Proceedings of the International Conference on Info rmation Technology: Coding and Computing \(ITCC\22204 0-7695-2108-8/04 $ 20.00 \251 2004 IEEE 


number of transactions increases because it generates much more candidate itemsets than others The effect of the transaction size on the performance is shown in Figure 7. In this case, the size of the database wasn\220t changed by keeping the product of the average transaction size and the number of transactions constant. The number of transactions was 20,000 for the average transaction size of 50, and 100,000 for the average transaction size of 10. We used the fixed minimum support count of 250 transactions, regardless of the number of transactions.  The performance of SETM deteriorates as the transaction size increases because the number of candidate itemsets generated is very large compared to other algorithms in the later passes \(i.e., after the second pass\ on the databases.  On the other hand, the total execution times of ESETM and PSETM are stable because the number of candidate itemsets generated in the later passes is small. When the transaction size is large PSETM is slightly better than ESETM since its additional pruning of candidates takes effect on the performance  0 200 400 600 800 1000 1200 100 200 300 400 Number of Transactions \(in 1000s Tim e  s e c  SETM ESETM PSETM 0 1000 2000 3000 4000 5000 10 20 30 40 50 Average Transaction Size Ti m e  s e c  SETM ESETM PSETM 6. Conclusion In this paper, we proposed a new algorithm, named Enhanced SETM \(ESETM\for mining association rules from relations ESETM is an enhanced version of the SETM algorithm [3  a nd its performance is much better than SETM because it generates much less candidate itemsets to count.  ESTM and SETM are implemented on a parallel NCR database system, and we evaluated their performance in various cases.  ESTM is at least three times faster than SETM in most of our test cases, and its performance is quite scalable.  Currently, we are developing an algorithm for mining association rules across multiple tables on the NCR Teradata database system 7.  Acknowledgements This research has been supported in part by NCR, LexisNexis Ohio Board of Regents \(OBR\nd Wright Brothers Institute WBI References 1 R.  A g ra w a l, T  Im ie linsk i a nd A   Sw a m i 215 M ining  in Large Databases,\216 Proc of ACM SIGMOD Int\220l Conference on Management of Data, 1993, pp. 207-216 2 R A g ra w a l a nd R. Srik a n t, \215 F a s t A l g o rithm s f o r Mining  Association Rules,\216 Proc. of VLDB Conference, 1994, pp 487-499 3  M. H outs m a a nd A  Sw a m i, \215 S e t O rie n te d Mining  f o r Association Rules in Relational Databases,\216 Proc. of Int\220l Conference on Data Engineering, 1995, pp. 25-33 4  J   S. P a rk M. S. Che n, a nd P  S. Y u 215 U s i ng a H a s h Based Method with Transaction Trimming for Mining Association Rules,\216 IEEE Trans. on Knowledge and Data Engineering, Vol. 9, No. 5, 1997, pp. 813-825 5 A   S avasere  E   Om i eci n s ki  an d S  Navat h e  215A n E f f i ci en t  Algorithm for Mining Association Rules in Large Databases,\216 Proc. of VLDB Conference, 1995, pp.432444 6 J  D  H o lt a nd S. M. Chung 215 M ultipa s s A l g o rithm s f o r Mining Association Rules in Text Databases,\216 Knowledge and Information Systems, Vol. 3, No. 2, Springer-Verlag 2001, pp. 168-183 7 J D  H o lt a nd S. M Chung  215 M ining  A s s o c i a tion Rule s  Using Inverted Hashing and Pruning,\216 Information Processing Letters, Vol. 83, No. 4, Elsevier Science, 2002 pp. 211-220 8 R A g ra w a l a nd K  Shim  215 D e v e l oping  T i g h tly Couple d Data Mining Applications on a Relational Database System,\216 Proc. of Int\220l Conference on Knowledge Discovery and Data Mining, 1996, pp. 287-290 9  S Sa ra w a g i  S T hom a s a nd R. A g ra w a l, \215 I nte g r a ting  Association Rule Mining with Relational Database Systems: Alternatives and Implications,\216 Proc. of ACM SIGMOD Int\220l Conference on Management of Data, 1998 pp. 343-354  Introduction to Teradata RDBMS NCR Teradata Division, 2002  Teradata RDBMS Database Design NCR Teradata Division, 2001 Figure 6. Effect of the number of transactions Figure 7. Effect of the transaction size Proceedings of the International Conference on Info rmation Technology: Coding and Computing \(ITCC\22204 0-7695-2108-8/04 $ 20.00 \251 2004 IEEE 


sequent conìdence thresholds having the same support are connected with lines As a result we get three precisionrecall curves  one for each investigated support The connecting lines between measured values are for sake of clarity and not for interpolation In Figure 4 ROSE achieves for a support of 1 and a conìdence of 0  1 a recall of 0  15 and a precision of 0  26  The recall of 0.15 states that ROSE s suggestion correctly included 15 of all changes that were actually carried out in the given transaction  The precision of 0.26 means that 26 of all recommendations were correctÑevery fourth suggested change was actually carried out and thus predicted correctly by ROSE  The programmer has to check about four suggestions in order to nd a correct one Figure 4 also shows that increasing the support threshold also increases the precision but decreases the recall as ROSE gets more cautious However using the highest possible thresholds does not always yield the best precision and recall values If we increase the conìdence threshold above 0  80 both precision and recall decrease Furthermore Figure 4 shows that high support and conìdence thresholds are required for a high precision Still such values result in a very low recallÑindicating a trade-off between precision and recall In practice a graph such as the one in Figure 4 is thus necessary to select the best support and conìdence values for a speciìc project In the remainder of this paper though we have chosen values that are common across all projects in order to facilitate comparison One can either have precise suggestions or many suggestions but not both 6.3 Likelihood While a precision like 26 sounds low keep in mind that this is the likelihood of each single recommendation predicting a speciìc location If some change in A results in either B  C or D being changed ROSE suggests B  C and D  but each suggestion has an average precision of only 33 To assess the actual usefulness for the programmer we checked the likelihood whether the expected location would be included in ROSE s top three navigation suggestions assuming that a programmer wonêt have too much trouble judging the rst three suggestions Formally L 3 is the likelihood that for a query q   Q  E   at least one of the rst three recommendations is correct L 3  L   apply  Q  R 3  012 E   0  where L  p  stands for the probability of the predicate p  If in the example above ROSE always suggested B  C and D as topmost suggestions L 3  100 would hold 6.4 Results Navigation through Source Code We repeated the experiment from Section 6.2 for all eight projects with a support threshold of 1 and a conìdence threshold of 0  1Ñsuch that for navigation the user gets several recommendations The results are shown in Table 2 on thenextpage\(column Navigation  For these settings the average recall is 15 the average precision is 26 these values are also found for ECLIPSE Section 6.2 The average likelihood L 3 of the three topmost suggestions predicting a correct location is 64 While KOFFICE and JEDIT have lower recall precision and likelihood values GCC strikes by overall high values The reason is that KOFFICE and JEDIT are projects where continuously many new features are inserted which cannot be predicted from history while GCC is a stable system where the focus is on maintaining existing features When given one initial changed entity ROSE can predict 15 of all entities changed later in the same transaction In 64 of all transactions ROSE s topmost three suggestions contain a correct location 6.5 Results Error Prevention Besides supporting navigation ROSE should also prevent errors The scenario is that when a user decides to commit all her changes to the version archive ROSE checks if there are related changes that have not been changed If there are it issues a pop-up window with a warning it also suggests one or more missing entities that should be considered We wanted to determine in how many cases ROSE can predict such a missing entity For this purpose we took each transaction left out one entity and checked if ROSE could predict the missing entity In other words the query was the complete transaction without the missing entity So for each single transaction   and each entity e  entities   we queried Q  entities   e   and checked whether ROSE would predict E  e   For each transaction we thus again ran   entities    tests As too many false warnings might undermine ROSE s credibility ROSE is set up to issue warnings only if the high conìdence threshold of 0.9 is exceeded Still we wanted to get as many missing entities as possible resulting in a support threshold of 3 The results are shown in Table 2 column Prevention   The average recall is about 4 This means that in only one out of 25 queries in GCC  every 5th query ROSE correctly predicted the missing entity  The average precision is above 50 This means that every second recommendation of ROSE is correct or If a warning occurs and ROSE recommends further entities the user on average has to check only one false recommendation before getting to the correct one Proceedings of the 26th International Conference on Software Engineering \(ICSEê04 0270-5257/04 $20.00 © 2004 IEEE 


Navigation Prevention Closure Support 1 3 3 Conìdence 0.1 0.9 0.9 Project R  P  L 3 R  P  R M P M ECLIPSE 0.15 0.26 0.53 0.02 0.48 1.0 0.979 GCC 0.28 0.39 0.89 0.20 0.81 1.0 0.953 GIMP 0.12 0.25 0.91 0.03 0.71 1.0 0.978 JBOSS 0.16 0.38 0.69 0.01 0.24 1.0 0.981 JEDIT 0.07 0.16 0.52 0.004 0.59 1.0 0.986 KOFFICE 0.08 0.17 0.46 0.003 0.24 1.0 0.990 POSTGRES 0.13 0.23 0.59 0.03 0.66 1.0 0.989 PYTHON 0.14 0.24 0.51 0.01 0.50 1.0 0.986 Average 0.15 0.26 0.64 0.04 0.50 1.0 0.980 Table 2 Results for ne granularity  R  recall P  precision L  likelihood Given a transaction where one change is missing ROSE can predict 4 of the entities that need to be changed On average e very second recommended entity is correct 6.6 Results Closure The nal question in the Error Prevention scenario is how many false alarms ROSE would produce in case no entity is missing We simulated this by testing complete transactions For each transaction   we queried Q  entities   and checked whether ROSE would predict E   we thus had one test per transaction As the expected outcome is the empty set the recall is always 1 To measure the number of false warnings we cannot use micro-evaluation anymore as one single false alarm results in a summarized precision of 0 We thus turn to macro-evaluation precision The precision for a single query in this setting is either 0 if at least one entity is recommended or 1 if no entities are recommended P M is the percentage of commits where ROSE has not issued a warning and 1  P M is the percentage of false alarms The results are shown in Table 2 column Closure  One can see that the precision is very high for all projects usually around 0.98 This means that ROSE issues a false alarm in only every 50th transaction ROSE s warnings about missing changes should be taken seriously Only 2 of all transactions cause a false alarm In other words ROSE does not stand in the way 6.7 Results Granularity By default ROSE recommends entities at a ne granularity level e.g variables or functions This results in a low coverage of the rules for a project as most functions are rarely changed Our hypothesis was that if we applied mining to les rather than to variables or functions we would get a higher support and thus a higher recall Navigation Prevention Closure Support 1 3 3 Conìdence 0.1 0.9 0.9 Project R  P  L 3 R  P  R M P M ECLIPSE 0.17 0.26 0.54 0.03 0.48 1.0 0.980 GCC 0.44 0.42 0.87 0.29 0.82 1.0 0.946 GIMP 0.27 0.26 0.90 0.08 0.74 1.0 0.963 JBOSS 0.25 0.37 0.64 0.05 0.44 1.0 0.980 JEDIT 0.25 0.22 0.68 0.01 0.44 1.0 0.984 KOFFICE 0.24 0.26 0.67 0.04 0.61 1.0 0.971 POSTGRES 0.23 0.24 0.68 0.05 0.59 1.0 0.978 PYTHON 0.24 0.36 0.60 0.03 0.67 1.0 0.991 Average 0.26 0.30 0.70 0.07 0.66 1.0 0.973 Table 3 Results for coarse granularity  R  recall P  precision L  likelihood Therefore we repeated the experiments from Sections 6.4to6.6witha coarse granularity e.g mining and applying rules between les rather than between entities The results are shown in Table 3 It turns out that the coarser granularity increases recall in all cases sometimes even dramatically as the factors 3Ö8 in KOFFICE show The precision stays comparable or is increased as well If ROSE thus suggests only a le rather than an entity the suggestions become more frequent and more precise However each single suggestion becomes less useful as it suggests a less speciìc locationÑnamely only a le rather than a precise entity 6 A possible consequence of this result is to have ROSE start with rather vague suggestions say regarding les or packages which become more and more speciìc as the user progresses We plan to apply and extend generalized association rules 23 s u ch that ROSE can suggest the nest granularity wherever possible When given one changed le ROSE can predict 26 of the les actually changed in the same transaction In 70 of all transactions ROSE s topmost three suggestions contain a correct location 6.8 Threats to Validity We have studied 10,761 transactions of eight open-source programs Although the programs themselves are very different we cannot claim that their version histories would be representative for all kinds of software projects In particular our evaluation does not allow any conclusions about the predictive power for closed-source projects However a stricter software process would result in higher precision and higher recallÑand hence a better predictability 6 This is a general trade-off If all entities were contained within one le then any suggestion regarding this one le would yield a precision of 100 and a recall of 100%Ñand be totally useless at the same time Proceedings of the 26th International Conference on Software Engineering \(ICSEê04 0270-5257/04 $20.00 © 2004 IEEE 


Transactions do not record the order of the individual changes involved Hence our evaluation cannot take the order into account the changes were madeÑand treats all changes equal In practice we expect speciìc orderings of changes to be more frequent than others which may affect results for navigation and prevention We have made no attempt to assess the quality of transactions ROSE learned from past transactions regardless of whether they may be desired or not Consequently the rules learned and evaluated may reîect good practices as well as bad practices However we believe that competent programmers make more good transactions than bad transactions and thus there is more good than bad to learn from history We have examined the predictive power of ROSE and assumed that suggesting a change narrowed down to a single le or even a single entity would be useful However it may well be that missing related changes could be detected during compilation or tests in which case ROSE s suggestions would not harm or may be known by trained programmers anyway who may nd ROSE s suggestions correct but distracting Eventually usefulness for the programmer can only be determined by studies with real users which we intend to accomplish in the future 7 Related Work Independently from us Annie Ying developed an approach that also uses association rule mining on CVS version archives 25 She es pecially e v aluated the us efulnes s o f the results considering a recommendation most valuable or surprising if it could not be determined by program analysis and found several such recommendations in the MOZILLA and ECLIPSE projects In contrast to ROSE  though Yingês tool can only suggest les not ner-grained entities and does not support mining-on-the-îy Change data has been used by various researchers for quantitative analyses Word frequency analysis and keyword classiìcation of log messages can identify the purpose of changes and relate it to change size and time between changes 18 V a ri ous res earchers comput ed met ri cs on t h e module or le level 3 9 11 12 or ort hogonal t o t hes e per feature 19 and in v e s tigated the change of thes e m etrics over time i.e for different releases or versions of a system Gall et al were the rst to use release data to detect logical coupling between modules 8 The CVS history allows to detect more ne-grained logical coupling between classes 10  l es and functions 27  None of these works on logical coupling did address its predictive power Sayyad-Shirabad et al use inductive learning to learn different concepts of relevance between logically coupled les 21 22  A concept is a s et of attributes like le name extension and simple metrics like number of routines deìned If two les have these attributes then they are relevant to each other Sayyad-Shirabad thoroughly evaluated the predictive power of the concepts found but none of the papers gives a convincing example of such a concept Amir Michail used data mining on the source code of programming libraries to detect reuse patterns in form of association 16 o r generalized as s o ciation rules 17  The latter take inheritance relations into account The items in these rules are re-\e relationships like method invocation inheritance instantiation or overriding Both papers lack an evaluation of the quality of the patterns found However Michail mines a single version while ROSE uses the changes between different versions To guide programmers a number of tools have exploited textual similarity of log messages 5 o r program code 2  HIPIKAT 6 i mpro v e s o n t his by taking als o other s ources like mail archives and online documentation into account In contrast to ROSE  all these tools focus on high recall rather than on high precision and on relationships between les or classes rather than between ne-grained entities 8 Conclusion and Consequences ROSE can be a helpful tool in suggesting further changes to be made and in warning about missing changes But the more there is to learn fro m history the more and better suggestions can be made  For stable systems like GCC  ROSE gives many and precise suggestions 44 of related les and 28 of related entities can be predicted with a precision of about 40 for each single suggestion and a likelihood of over 90 for the three topmost suggestions  For rapidly evolving systems like KOFFICE or JEDIT  ROSE s most useful suggestions are at the le level Overall this is not surprising as ROSE would have to predict new functions which is probably out of reach for any approach  In about 4Ö7 of all erroneous transactions ROSE correctly detects the missing change If such a warning occurs it should be taken seriously as only 2 of all transactions cause false alarms What have we learned from history and what are our suggestions Here are our plans for future work Taxonomies Every change in a method implies a change in the enclosing class which again implies changes in the enclosing les or packages We want to exploit such taxonomies to identify patterns such as this change implies a change in this package rather than in this method that may be less precise in the location but provide higher conìdence Proceedings of the 26th International Conference on Software Engineering \(ICSEê04 0270-5257/04 $20.00 © 2004 IEEE 


Sequence rules Right now we are only relating changes that occur in the same transaction In the future we also want to detect rules across multiple transactions The system is always tested before being released as indicated by appropriate changes Further data sources Archived changes contain more than just author date and location One could scan log messages including the one of the change to be committed to determine the concern the change is more likely to be related to say Fix vs New feature Program analysis Another yet unused data source is program analysis although our approach can detect coupling between items that are not even programs knowing about the semantics of programs could help separating related changes into likely and non-likely Furthermore coupling that can be found via analysis 25 need not be repeated in ROSE s suggestions Rule presentation The rules as detected by ROSE describe the factual software processÑwhich may or may not be the intended process Consequently these rules can and should be made explicit In previous work 27  w e u sed visual mining t o d etect re gularities and irregularities of logically coupled items Such visualizations could further explain the recommendations to programmers and managers We are currently making ROSE available as a plug-in for ECLIPSE  For information on download and installation see http://www.st.cs.uni-sb.de/softevo Acknowledgments This project is funded by the Deutsche Forschungsgemeinschaft grant Ze 509/1-1 Holger Cleve Carsten G org Christian Lindig Stefan Siersdorfer and the anonymous ICSE reviewers gave helpful comments on earlier revisions of this paper References  R A gra w a l and R S r ikant F a s t algorithm s f or m ining as s o ciation rules In Proceedings of the 20th Very Large Data Bases Conference VLDB  pages 487Ö499 Morgan Kaufmann 1994  D L  Atkins  V ers ion s ens iti v e editing Change his tory as a programming tool In B Magnusson editor Proceedings of System Conìguration Management SCMê98  volume 1439 of LNCS  pages 146Ö157 Springer-Verlag 1998  T  Ball J  M Kim  A A P orter  and H P  S i y  If your v e rs ion control system could talk In ICSE Workshop on Process Modelling and Empirical Studies of Software Engineering  1997  J M Bie ma n A A Andre ws and H J Y ang Understanding change-proneness in OO software through visualization In Proc 11th International Workshop on Program Comprehension  pages 44Ö53 Portland Oregon May 2003  A Chen E  Chou J  W ong A Y  Y a o Q Z h ang S Z h ang and A Michail CVSSearch searching through source code using CVS comments In ICSM 2001 14 pages 364Ö374  D  Cubrani c and G C Murphy Hipikat Recommending pertinent software development artifacts In ICSE 2003 pages 408Ö418  K  F ogel and M O  N eill cvs2cl.pl CVS-log-message-toChangeLog conversion script  Sept 2002 http://www.redbean.com/cvs2cl  H  G all K  H a jek and M J azayeri D e tection of logical coupling based on product release history In Proc International Conference on Software Maintenance ICSM 98  pages 190Ö198 Washington D.C USA Nov 1998 IEEE  H  G all M  J azayeri R K l  osch and G Trausmuth Software Evolution Observations based on Product Release History In Proceedings of International Conference on Software Maintenance ICSM 97  pages 160Ö196 1997  H  G a ll M J azayeri and J  K r aje w s ki CV S r eleas e h is tory data for detecting logical couplings In IWPSE 2003 15 pages 13Ö23  T  L  G r a v es  A  F  K arr  J  S  Marron and H  S i y  P r edicting f ault incidence using software change history IEEE Transactions on Software Engineering  26\(7 2000  A E  Has s a n a nd R Holt T h e c haos of s o ftw are de v elopm ent In IWPSE 2003 15 13 Proc 25th International Conference on Software Engineering ICSE  Portland Oregon May 2003 14 Proc International Conference on Software Maintenance ICSM 2001  Florence Ital y Nov 2001 I EEE 15 Proc International Workshop on Principles of Software Evolution IWPSE 2003  Helsinki Finland Sept 2003 IEEE Press  A  Michail D a ta m i ning library reus e patterns in us er s elected applications In Proc 14th International Conference on Automated Software Engineering ASEê99  pages 24Ö33 Cocoa Beach Florida USA Oct 1999 IEEE Press  A  Michail D a ta m i ning library reus e p atterns us ing generalized association rules In International Conference on Software Engineering  pages 167Ö176 2000  A Mockus and L  G V otta Identifying reas ons for s oftw are changes using historic databases In Proc International Conference on Software Maintenance ICSM 2000  pages 120Ö130 San Jose California USA Oct 2000 I EEE  A Mockus  D  M  W eis s  and P  Z hang Unders tanding and predicting effort in software projects In ICSE 2003 13 pages 274Ö284  C J  V  Rijs ber gen Information Retrieval 2nd edition  Butterworths London 1979  J  Sayyad-Shirabad T  C L e thbridge and S Matwin Supporting maintainance of legacy software with data mining techniques In ICSM 2001 14 pages 22Ö31  J  Sayyad-Shirabad T  C L ethbridge and S Matwin Mining the maintenance history of a legacy software system In Proc International Conference on Software Maintenance ICSM 2003 Amsterdam Netherlands  Sept 2003 I EEE  R Srikant and R Agra w a l Mining generalized as s o ciation rules  I n Proceedings of the 21th Very Large Data Bases Conference VLDB  pages 407Ö419 1995  R S r ikant Q  V u  a nd R A g ra w a l Mining as s o ciation rules w ith item constraints In Proceedings of the 3rd International Conference on KDD and Data Mining KDD 97  Newport Beach California USA Aug 1997  A T  T  Y i ng Predicting s ource code changes by m ining re vis ion history Masterês thesis University of British Columbia Canada Oct 2003  A  Z e ller and R H ildebrandt S i m plifying and is olating f ailureinducing input IEEE Transactions on Software Engineering  28\(2 Feb 2002  T  Z i m m e rm ann S  D i ehl and A  Z eller  H o w his tory jus ti es s y s tem architecture or not In IWPSE 2003 pages 73Ö83  T  Z i m m e rm ann and P  W eiﬂgerber  P reproces s i ng CVS data for ne-grained analysis Technical report Saarland University Mar 2004 Submitted for publication Proceedings of the 26th International Conference on Software Engineering \(ICSEê04 0270-5257/04 $20.00 © 2004 IEEE 


  11 could be improved by a simple modification of the feed by adding a small tuning vane to th e feed. Therefore, it can be stated that some improvement can be expected by modification of the feeds, and adaptation of the test antenna in such a way that surrounding Ku-band element are closed   Figure 28 Reflection coefficient of Ku-band stacked patch antenna element in dual-frequency antenna stack  Figure 29 shows the influence of the L-band slots on the return loss of the Ku-band antenna element. To this end, the four connectors of the L-band elements were alternately open and terminated by means of 50 loads. The deviations were measured with respect to the set-up where all connectors were terminated Apparently, the deviations are acceptable  Figure 29 Influence of L-band termination on return loss of Ku-band antenna element, with and without termination Figure 30 and Figure 31 show the isolation between the Lband and Ku-band elements in L-band and Ku-band respectively. To this end the S21-parameters have been measured. These figures reveal that the mutual coupling between the L-band and Ku-band elements is sufficiently small  Figure 30 Measured isolation between L-band and Ku-band antennas in L-band frequencies  Figure 31 Measured isolation between L-band and Ku-band antennas in Ku-band frequencies From these measurements it can be concluded that opportunities should be considered to improve the design of the dual-frequency antenna \(by optimizing the feeds of the Ku-band elements antenna and the measurement set-up \(closure of surrounding Ku-band ports and use of appropriate connectors for the open Ku-band ports 7  M ODIFIED DUAL FREQUENCY ANTENNA  In order to benefit the str ong points of the two separate designs as discussed in section 4, an alternative antenna is proposed that exploits the properties of a \221best of both worlds\222 solution employing ideas from both designs. The modified antenna possesses an aperture fed L-band patch of a similar form to first design, but situated towards the bottom of the stack. Ku band el ements are located within the L-band perforations and para sitic patches are situated above a foam spacer \(see Figure 32 and Figure 33\A measurement campaign is underway to assess the behaviour of this modified test antenna 


  12  Figure 32 Bottom view of dual frequency antenna tile with perforated L-band patc h in lower layer with Kuband patches  Figure 33 Layer stack with perforated L-band patch in lower layer with Ku-band patches  8  B EAM FORMING N ETWORK  A major keystone for the su ccess of phased array antenna onboard aircraft is the capability of steering the main beam in the direction of the geosta tionary satellites. This requires the inclusion of a broadband beam forming network. Beam steering can be realized by adding RF-phase shifters and LNA\222s to the antenna elements of the array. However traditional phase shifters in ge neral have a narrow band, and hence do not yield the re quired broadband capability Alternative technologies for broadband beam forming are switched beam networks \(using Butler matrices innovative designs for RF-compone nts such as phase shifter LNA components in \(M\IC technology, or beam forming by using opti cal ring resonators  The German SME IMST is involved in several projects for development of electronica lly steerable phased array antennas for satellite communication. In the NATALIA project \(New Automotive Track ing Antenna for Low-cost Innovative Applications\ ESA, IMST is investigating the possibility of realizing a compact costeffective solution for a recei ve-only full electronically steerable antenna for cars in Ku-band. This antenna is a planar array composed of approximately 150 patches circularly polarised by using a 90\260 hybrid, and arranged in a hexagonal fashion. Each patc h is equipped with a MMIC corechip containing a phase sh ifting unit, LNA and digital steering logic  In the Netherlands, a consortiu m \(consisting of University of Twente, Lionix BV, National Aerospace Laboratory NLR and Cyner Substrates developing in the national FlySmart project technology for a broadband optical beam forming network. For the steering of the beam of the conformal phased array a squi nt-free, continuously tunable mechanism is proposed that is based on a fully integrated optical beam forming network \(OBFN optical ring resonators \(ORRs as tunable delay elements. A narrowband continuously tunabl e optical TTD device is realized as a recirculating wa veguide coupled to a straight waveguide. This straight wave guide can behave as a bandpass filter with a periodic, bell-shaped tunable group delay response. The maximum group delay occurs at a tunable resonance frequency. A larger delay-bandwidth product can be achieved by cascading multiple ORR sections. A complete OBFN can be obtaine d by grouping several delays and combining elements in one optical circuit. Such an OBFN can be realized on a si ngle-chip. Electrical/Optical E/O O E by means of filter based single-sideband modulation suppressing the carrier lanced coherent optical detection. Further details of the optical beamforming network have been presented in Re The proof-ofconcept has been shown by manufacturing a chip for an 8x1 OBFN. Essential components of the OBFN are the optical modulators, which are used to modulate the light in the ORR system 9  C ONCLUSIONS  For enhanced communicati on on board aircraft, novel antenna systems with broa dband satellite-based capabilities are required. So far, existi ng L-band satellite based systems for communications are used primarily for passenger application \(APC\i nistrative communications AAC and now data are tending to evolve towards broadband dig ital applications \(Voice over IP\any studies are going on worldwide to employ Kuband TV geostationary sate llites for communication with mobile terminals on aircraft The inbound traffic is about 5 times higher than the outbound The inbound traffic requires the availability of a broadband Ku-band antenna in receive mode only. The outbound traffic services can be supplied by the Inmarsat SBB link, whic h requires the installation of an L-band transmit antenna. In order to avoid both the installation of L-band antenna and Ku-band antenna, the concept of a hybrid dual frequency antenna operating L 


  13 band and Ku-band with low aerodynamic profile has been investigated in this paper. Keyaspects of this research are 200  Design and testing dual-fre quency antenna elements operating in both L-band and Ku-band 200  Conformal aspects of Ku-band phased array antennas 200  Beam forming algorithms for planar and conformal phased array antennas Two designs for dual-frequency antenna tiles consisting of 8x8 Ku-band antenna elements and one L-band element The designs have been analysed by means of computer simulations. Both designs show promising performance both in L-band and Ku-band. The design with slotted Lband antenna has a resonant fre quency in receive mode with a bandwidth of about 1 GHz. The Ku-band antenna is a stacked patch configuration where a parasitic element is placed above a lower patch separated by dedicated space filler. The manufactured protot ype antennas indicate that the bandwidth is sufficiently large In order to be able to communicate with geostationary satellites also at high latitudes e.g. during inter-continental flights\stem should have sufficient performance at low elevation angles. The antenna Ku-band system is required to have a small beamwidth \(to discriminate between the satellite signals\gain 30 dB angles. The effects of these requirements on the size and positioning of the antenna on the aircraft fuselage have been investigated. These requirements can be best satisfi ed by installing two planar phased array antennas on both side s of the fuselage with at least 1600 Ku-band elements. Each element has two feed lines, one for each polarization Every feed line has to be connected to the beam formi ng network. This means that the connections cannot be routed to one of the four sides of the antenna. Instead the concept of vertical feed lines \(by means of vias in a sufficiently thick substrate recommended. These vertical f eed lines connect the L-band and Ku-band antenna elements in the upper layer with feed networks in multiple lower laye rs. This vertical feed line system was not available so far due to manufacturing problems The performances of one dua l-frequency antenna design have been investigated by manufacturing two test antennas without vertical feed line syst em. The first antenna contains only a multilayer structure with L-band slots and 8x8 Kuband stacked patches. The performances of the L-band slots and Ku-band stacked patches c ould be measured separately It was concluded that opportun ities should be considered to improve the design of the dual-frequency antenna \(by optimizing the feeds of the Ku-b and elements the dual frequency test-antenna and the measurement set-up More important, however, is the realization of a mechanically stable vertical feed line system, so that the properties of L-band and Ku-band elements can be measured adequately The second test antenna contains only a multilayer structure with 8x8 Ku-band stacked patches and a feed network with 8 combiners, where each comb iner coherently sums 8 antenna elements. In combination with a prototype 8x1 OBFN, a Ku-band phased arra y antenna is obtained of which the beam can be steered in one direction. This second test antenna is used to analyze the broadband properties of the 8x8 Ku-band antenna array and 8x1 OBFN. The measured performances of this antenna are presented in Ref   A CKNOWLEDGMENT  This work was part of the EU 6 th Framework project ANASTASIA., and the FlySmart project, supported by the Dutch Ministry of Economic A ffairs, SenterNovem project numbers ISO53030 The FlySmart project is part of the Eureka PIDEA  project SMART Cyner Substrates is acknowle dged for technical assistance during the fabrication of the prototype antennas 


  14 R EFERENCES  1  P. Jorna, H. Schippers, J. Verpoorte, \223Beam Synthesis for Conformal Array Antennas with Efficient Tapering\224 Proceedings of 5 th European Workshop on Conformal Antennas, Bristol, September 11-12, 2007 2  The Radio Regulations, editi on of 2004, contain the complete texts of the Radio Regulations as adopted by the World Radio-communication Conference \(Geneva WRC-95 tly revised and adopted by the World Radio-communication Conference WRC-97\RadioWRC2000\and the World Radio-communication Conference WRC-03 Resolutions, Recommendations and ITU-R Recommendations incorporat ed by reference 3  RECOMMENDATION ITU-R M.1643, Technical and operational requirements for ai rcraft earth stations of aeronautical mobile-satellite service including those using fixed satellite service network transponders in the band 14-14.5 GHz \(Earth-to-space 4  ETSI EN 302 186 v1.1.1 \(2004-01 Stations and Systems \(SES\onised European Norms for satellite mobile Aircraft Earth Stations AESs\the 11 12/14 GHz frequency bands covering essential requirement s under article 3.2 of the R&TTE directive 5  EUROCAE ED-14E; Environmental Conditions and Test procedures for Airbor ne Equipment, March 2005 6  F. Croq and D. M. Pozar, \223Millimeter wave design of wide-band aperture-coupled stacked microstrip antennas,\224 IEEE Trans. Antennas Propagation, vol. 39 pp. 1770\2261776, Dec. 1991 7  S. D. Targonski, R. B. Waterhouse, D. M. Pozar Design of wide-band aperture stacked patch microstrip antennas ", IEEE Transactions on Antennas and Propagation, vol. 46, no. 9, Sep. 1998, pp. 1245-1251 8  R. B. Waterhouse, "Design of probe-fed stacked patches", IEEE Transactions on Antennas and Propagation, vol. 47, no. 12, Dec. 1999, pp. 1780-1784 9  D.M. Pozar, S. D. Targonski, \223A shared aperture dualband dual-polarised microstrip array\224, IEEE Transactions on Antennas and Propagation,Vol. 49 no. 2,Feb. 2001, pp. 150-157 10  http://www.ansoft.com 11  J-F. Z\374rcher, F.E. Gardiol, \223Broadband patch antennas\224 Artech House, \(1995\N 0-89006-777-5 12  H. Schippers, J. Verpoorte, P. Jorna, A. Hulzinga, A Meijerink, C. G. H. Roeloffzen, L. Zhuang, D. A. I Marpaung, W. van Etten, R. G. Heideman, A. Leinse, A Borreman, M. Hoekman M. Wintels, \223Broadband Conformal Phased array with Optical Beamforming for Airborne Satellite Communication\224, Proc. of the IEEE Aerospace Conference, March 2008, Big Sky, Montana US 13  H. Schippers, J. Verpoorte, P. Jorna, A. Hulzinga, L Zhuang, A. Meijerink, C. G. H. Roeloffzen, D. A. I Marpaung, W. van Etten, R. G. Heideman, A. Leinse M. Wintels, \223Broadband Op tical Beam Forming for Airborne Phased Array An tenna\224, Proc. of the IEEE Aerospace Conference, March 2009, Big Sky, Montana US 


  15  B IOGRAPHIES  Harmen Schippers is senior scientist at the National Aerospace Laboratory NLR. He received his Ph. D. degree in applied mathematics from the University of Technology Delft in 1982. Since 1981 he has been employed at the National Aerospace laboratory NLR. He has research experience in computational methods for aero-eleastics, aeroacoustic and electromagnetic problems. His current research activities are development of technology for integration of smart antennas in aircraft structures, and development of computational tools for installed antenna analysis on aircraft and spacecraft  Jaco Verpoorte has more than 10 years research experience on antennas and propagation Electromagnetic compatibility \(EMC and radar and satellite navigation He is head of the EMC-laboratory of NLR. He is project manager on several projects concerning EMCanalysis and development of advanced airborne antennas    Adriaan Hulzinga received his BEng degree in electronics from the hogeschool Windesheim in Zwolle Since 1996 he has been employed at the National Aerospace laboratory \(NLR as a senior application engineer. He is involved in projects concerning antennas and Electromagnetic compatibility \(EMC  Pieter Jorna received the M.Sc degree in applied mathematics from the University of Twente in 1999 From 1999 to 2005 he was with the Laboratory of Electromagnetic Research at the University of Technology Delft. In 2005 he received the Ph.D. degree for his research on numerical computation of electromagnetic fields in strongly inhomogeneous media Since 2005 he is with the National Aerospace Laboratory NLR\ in the Netherlands as R&D engineer   Andrew Thain is a research engineer in the field of electromagnetic modelling of antennas. He specialises in the use of surface integral methods for the calculation of coupling and radiation patterns and works closely with Airbus on the topic of antenna positioning. He has experience in the field of electromagnetic modelling  Gilles Peres is head of the Electromagnetics group of EADS-IW He has a wide experience in computational EM modelling particularly the use of FDTD, integral and asymptotic techniques for antenna structure interactions. He has contributed with Airbus experts to the certification campaign of the A340/500 and A340/600. Dr Peres holds a PhD thesis from University of Toulouse \(1998\ on impulsive Electromagnetic Propagation effects through plasma   Hans van Gemeren has a BEng degree in electronics. From the beginning of Cyner substrates he is involved in development and production of prototyping and nonconventional Printed Circuit boards Working mainly for design and research centers Cyner got involved in many high tech projects and from this developed a great expertise in the use of different \(RF materials. In the FlySmart project Hans and his colleagues are able to do what they like most: In close cooperation with designers, creatively working on substrate solutions 


  16  


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


