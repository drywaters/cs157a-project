A Novel Algorithm for Associative Classification of Image Blocks Xiaoyuan Xu Guoqiang Han Huaqing Min School of Computer Science and Engineering South China University of Technology Guangzhou 510641 China xiaoyxu@yahoo.com.cn csgqhan@scut.edu.cn hqmin@sina.com Abstract Because of its accurate and robust performance association rule-based approach is recently used for image classification However the existing algorithms for associative classification suffer from inefficiency Addressing this problem a novel algorithm based on atomic association rules is presented and successfully 
used in image block classification Mining only the atomic association rules achieves fast image block classification Using the strong atomic association rules extracted under a high confidence threshold can accurately differentiate instances from the image dataset Furthermore multi-passes of partial classifications can classify the whole dataset This algorithm uses a self-adaptive confidence threshold and a dynamic support threshold both of which are important for good classification performance The 
experiments were performed on a standard dataset of image segmentation The results show the proposed algorithm can classify the image blocks faster more accurate and robust than the typical associative classification algorithm 1 Introduction In recent years with the rapid development of multimedia and computer networks the volume of digital images has dramatically increased Undoubtedly there are large amounts of valuable information in these image resources Image understanding and content-based image retrieval have 
become hot topics among image processing researchers Image segmentation is one of the most important technologies in image processing For block-based segmentation it divides an image into its constituent parts A feature vector is formed for each block by using the color and textural characteristics The subimages or image blocks can be classified by supervised or unsupervised learning algorithms There exist many classification approaches such as decision tree support vector machine genetic algorithm neural network and so on Associative 
classification is a relatively new approach Because of its accurate and robust performance it has drawn much attention in many areas such as that of artificial intelligence pattern recognition and detection of computer system intrusions Associative classification as a new classification approach has drawn the attention of researchers in data mining community 1  2   it also f i nd s a p p licatio n i n digital image processing 5 In this work we propose a novel approach CAAR that is Classification based on Atomic Association 
Rules CAAR is different from the typical algorithm for associative classification Our approach has three obviously characteristics 1 CAAR only extracts atomic rules under high and self-adaptive confidence thresholds for partial classification 2 Its classification is completed by multi-passes of partial classifications and 3 It uses a dynamic support threshold instead of a fixed one Because it mines only atomic rules the algorithm is very fast although multi-passes classifications are 
required CAAR can achieve high accuracy by using strong atomic rules with a high confidence for classifier construction The rest of this paper is arranged as follows Section 2 introduces associative classification of image blocks and the related definitions Section 3 describes the image block classification using CAAR approach Section 4 presents experiment results Finally section 5 draws our conclusions 2 Associative classification of image blocks The image block classification is an important step 
in block-based image segmentation It consists of three main steps 1 dividing the whole image into r  rpixel regions or blocks 2 extracting the features from the Proceedings of the Fourth International Conference on Computer and Information Technology \(CIT04 0-7695-2216-5/04 $20.00  2004 IEEE 


image blocks and 3 applying the algorithm in order to cluster or classify the blocks 2.1 Feature extraction To construct an image dataset for image block classification it is required to extract the color and/or textural features of each block The image dataset consists of n  m+1 cells where n stands for the number of regions m for the number of features An additional column is used for storing the class label of that region Although our paper focuses on classification it is necessary to introduce the approach of features extraction for the dataset on which our experiments were performed It is a UCI standard dataset image segmentation whose instances were drawn randomly from a database of 7 outdoor images The images were hand segmented The class of each instance was manually labeled This dataset contains 210 training instances and 2100 test instances each of which is relevant to a 3  3 pixel image region The 19 features called attributes are extracted from the sub-image Details can be found in the original datasets description For image block classification purpose the first three attributes which were found to be useless were removed The summary to this feature set is given below 1 short-line-density-5 the results of a line extraction algorithm that counts how many lines of length 5 of any orientation with low contrast less than or equal to 5  go through the region 2 short-line-density-2 same as short-line-density-5 but counts lines of high contrast greater than 5 3 features that measures the contrast of horizontally and vertically adjacent pixels in the region These attributes are used as a vertical edge detector and horizontal line detector respectively 4 intensity-mean the average over the region of R  G  B\/3 where R B and G refer to the amount of red green and blue in the pixel respectively 5 raw colormeans the average over the region of the R G and B value 6 excess color-means measure the excess red 2R G  B excess blue 2B G  R and excess green 2G R  B 7 the rest value-mean based on 3-d nonlinear transformation of RGB saturation-mean and hue-mean In this dataset there are 7 classes of image blocks including brickface sky foliage cement window path and grass 2.2 Association classification In order to perform classification on the image dataset its necessary to clarify various terms Let U={F D C be the problem discussed where F={F 1  F 2  F m F i  F j   i  j stands for the discretized feature set of cardinality m m=|F  D for the dataset each instance of which corresponds to a r  rpixel image block or region C for a class variable related to the blocks class labels Let be th e v alu e dom ain of variable X In the associative classification of image blocks the related terms are given below Definition 2.1 An item means a feature variable taking a value like F 2 3 Definition 2.2 An itemset denotes the conjunction of multiple items For example a 2-itemset can be given as F i a  F j b where    symbol represents the logical AND operator a  V[F i  b  V[F j  n d F i F j  F Definition 2.3 An association rule consists of two parts one is the left hand side LHS and the other is the right hand side RHS The form of a rule is represented as LHS  RHS Definition 2.4 For a rule r the support called sup refers to the statistical significance The confidence called conf measures the strength of the rule RHS r p\(r.LHS  r.sup  1 HS RHS\/p\(r.L  r p\(r.LHS  r.conf  2 p\(x represents the probability of an item or itemset xs occurrence in dataset D Definition 2.5 The strong rules are those satisfying the minimum support called minsup and minimum confidence called minconf thresholds A typical associative classification algorithm is CBA  i.e clas s i f i cation bas e d o n a s s o ciation ru les   CBA uses the Apriori algorithm 6 t o gener ate the class association rules at minsup=1 and minconf=50 The classifier is constructed by pruning the redundant rules from the set of extracted class association rules To avoid running out of memory CBA sets a hard limit of 80000 on the total number of mined rules Although CBA is declared to have higher classification accuracy than traditional decision tree classification algorithm C4.5 7  it co nsum es m uch system resources and generates many classification rules 3 Image block classification using CAAR Classification based on atomic rules CAAR is proposed in our study and proves to be very effective for most classification problems in the real world We tested CAAR on 26 UCI standard datasets and found that CAAR could be successfully used to classify 20 of these datasets despite its selectivity to the other 6 Proceedings of the Fourth International Conference on Computer and Information Technology \(CIT04 0-7695-2216-5/04 $20.00  2004 IEEE 


In this work for the purpose of classification only the class association rule is extracted That is the RHS of a rule is an item C=c where c  V[C A n atom ic class association rule has the form of F i a  C=c where a  V[F i  n d F i  F i  j For short the atomic class association rules are called atomic rules For human beings there are two important ways of identifying things One is using a single outstanding feature with an easy-first strategy for step-by-step classification The other is classification based on a combination of multiple features CAAR imitates a human adopting multiple passes of partial classifications This approach can overcome the overfitting of the classification mode to the training data 3.1 Introduction to CAAR CAAR generates a classifier in six steps 1 Scan the dataset and count the LHS items and the LHS-RHS 2-itemsets relevant to atomic rules through hash table hash1 and hash2 respectively  2 If the class labels of the valid non-deleted instances in the dataset are the same the algorithm generates a special rule which indicates the default class and the flow goes to step 6 3 Generation of strong atomic rule It generates strong atomic rules satisfying r.sup  minsup and r.conf  minconf The supports and confidences of the atomic rule candidates can be readily computed through hash1 and hash2  After generation of strong atomic rules the contents of hash1 and hash2 are cleared in order to consider the next pass classification 4 Pruning Sort the strong atomic rules with confidence as first key and support as second key All keys are sorted in descending order Test the strong atomic rules in sequence on each instance of the dataset If an instance contains the LHS item it is deleted logically not physically for the sake of disk-resident data mining and the match_count of the tested rule increases by 1 otherwise test the next rule until a rule matches the instance or until all rules are tested If no rule matches this instance it is regarded as a valid instance and is counted through hash1 and hash2 for the next pass The algorithm continues to test the next instance until all instances are tested 5 Select all rules with match_count  0asasubset of classifier rule set If the D is zero the flow goes to step 6 otherwise it goes to step 2 for the next pass of partial classification 6 Combine all classification rule subset in sequence to form the final classifier CAAR use a self-adaptive confidence for strong atomic rule extraction minconf=COEF  maxconf 3 In equation 3 COEF is a user-specified coefficient and maxconf is the maximum confidence of all atomic rule candidates in each pass Moreover if the support of a rule is computed relative to D 0  CAAR will have a dynamic support threshold  0 D D sup min sup min i i   4 In 4 D 0 and D i stand for the numbers of valid instances in datasets at the first pass and at the i-th pass respectively The minsup i is the relative minimum support threshold at the i-th pass of classification When i 1 D i  D 0  3.2 CAAR-based classification of image blocks CAAR is demonstrated by classifying the dataset discussed in this paper Image segmentation dataset containing 210 and 2100 instances in training set and test set respectively We combine both into one and delete the first three features which are useless to image block classification To inspect whether this dataset can be classified by CAAR the key problem is to observe if there are some strong atomic rules with high confidences in each passes of partial classification Figure 1 shows the spectrum of confidences for 695 candidate atomic rules in the first pass classification There are 8 rules with confidence 1       012\015 012\015 012\015 012\015                 012               Figure 2 indicates that in all passes of partial Proceedings of the Fourth International Conference on Computer and Information Technology \(CIT04 0-7695-2216-5/04 $20.00  2004 IEEE 


classifications the maxconf is 1.0 For some other datasets at the late passes the valid instances can be regarded as difficult to be classified In this case the maxconf may be less than 1                                      Figure 3 gives the number of valid instances D at different passes On this dataset CAAR requires 6 passes to classify all instances On the first pass 1009 instances can be differentiated So there are 1301 instances left for the second pass From the fourth pass the number of valid instances is far smaller than D 0  so CAAR is very fast even though multiple passes of partial classification are required 3.3 Theoretic analysis 3.3.1 Convergence  To assure that CAAR converges the strong atomic rules are generated in three steps 1 extracting the strong atomic rules with r.conf  COEF  maxconf and r.sup  minsup 2 if step 1 fails change support constraint r.sup  minsup to r.count  2  and 3 if step 2 fails extract all the rules satisfying r.conf  COEF  maxconf  So at each pass of partial classification theoretically there is at least one instance is classified and logically deleted from the dataset At last D converges to zero 3.3.2 Complexity Let n=|D 0 andN a stand for the mean size of attribute value lists and N c for the number of class labels In CAAR let p be the number of passes of partial classifications In each pass of CAAR counting the atomic rule related 2-itemsets requires O\(n  m time where m A and selecting the classification rules from the strong atomic rules requires O\(n  R 1  time where R 1 is the set of strong atomic rules The total time complexity is as follows T CAAR p,m,n R 1  O\(p  n  m p  n  R 1  5 Its very important that p is determined by the inherent model in the dataset and has no direct relationship with n In CBA let l be the maximum number of the rules LHS items The cost of CBA is a function of the time for mining the strong class rules plus the time for testing the strong rules to build a classifier The total time complexity is below T CBA m,n l,|R k     R  n C n  O l k k l k  R  k         1 1 2 1 6 where R k represents the set of strong class rules containing k items in LHS R 0 is the set of atomic-rule candidates and its size is m  N a  N c The R 1 ofCBA is far greater than that of CAAR because the CAARs minconf is much higher than 50 of CBA In addition in CBA the total number of mined rules increases exponentially as k becomes large 4 Further experimental results The experiments were done on a 1.5 GHz Pentium4 PC with 256MB main memory The prototype of CAAR was implemented in Java CAAR was compared with a typical associative classification algorithm in data mining That is CBA Classification Based on Association rules which was implemented in C by its authors All parameters in CBA took the default values as mentioned in 1  T he image segmentation dataset was selected from the UCI ML repository http://www.ics.uci.edu/~mlearn MLRepository.html The experimental dataset is obtained by combining the original training and test datasets of image segmentation  The first three attributes of the dataset row column of the center pixel of the region and the number of pixels in a region were removed Before associative classification all the continuous attributes were discretized 4.1 Parameter selection of CAAR In association classification the two important parameters are minsup and minconf For CAAR minconf COEF  maxconf where COEF is a coefficient and maxconf is the maximum confidence of all atomic rule candidates In parameter selection experiments the training and evaluation were all performed on the whole dataset Table 1 shows the influence of COEF on the classification performance of CAAR with a fixed minsup 1 Acc represents the accuracy and the time shows how many seconds that CAAR takes to build the model Its clear that as COEF increases the accuracy and execution time also rises In each pass of classification if only the atomic rules with the highest confidence i.e COEF=1 are extracted for partial classification CAAR achieves the highest accuracy at the cost of the longest execution time So a tradeoff between accuracy and other performance parameters is Proceedings of the Fourth International Conference on Computer and Information Technology \(CIT04 0-7695-2216-5/04 $20.00  2004 IEEE 


necessary For this dataset the COEF was chosen as 0.92 because of the relatively high accuracy and the acceptable execution time for model building With COEF fixed at 0.92 the influence of minsup on the performance of CAAR was observed Table 2 shows the results When minsup varies from 0.1 to 5 the accuracy decreases At minsup  0.5 the execution time is significantly larger than those in the other cases On the other hand at higher minsup rules with coarse granularity reduce the classification time but also lower the accuracy At minsup=5 the number of frequent LHS-RHS 2-itemsets decreases and results in less classification rules at each pass so it takes more time to complete the classification As a whole the minsup for this dataset can be chosen in the range of 0.1 to 2 In the following experiments minsup is set to 1 as default 4.2 Performance evaluation In order to evaluate the classification accuracy of the algorithms we used the standard stratified 10-fold Cross-Validation Test 10-fold CV which is widely accepted in the machine learning community The image block classification was evaluated in terms of the mean accuracy the mean execution time of ten runs in 10-fold CV and the size of classifier CAAR was closely compared with CBA and it was also compared with the decision tree algorithm C4.5 under the same running environment All algorithms were implemented by their authors Figure 4 shows the influence of minsup on the mean classification accuracies of 10 runs in 10-fold CV The parameters were set as follows COEF=0.92 for CAAR and minconf=50 for CBA Its obvious that CAAR is less dependent on the influence of minsup than CBA When minsup varies from 0.1 to 5 the accuracy of CAAR is between 88.4 and 83.9 But the accuracy of CBA varies in a wide range from 86.3 to 21.9 The reason why CBA is so sensitive to minsup is that the k-itemset k  2 candidates are generated based on k-1 frequent itemsets If minsup is relatively high there will be less frequent itemsets generated A small number of coarsely granular rules cant make fine classifications But for CAAR its different because CAAR uses a dynamic support threshold for strong atomic rule extraction in multipasses of partial classifications                                       015 015 015 015                              015   Figure 5 shows the total run time for 10-fold CV at different minsups under the conditions of minconf=50 for CBA and COEF=0.92 for CAAR The results show that CBA takes much more time than CAAR to complete the 10-fold CV At minsup=1 the time for CBA is 13.8 times more than that of CAAR At a very low minsup CBA generates a lot of strong rules satisfying minsup and minconf Because of the hard limit of 80000 on the number of mined rules association rule mining of CBA will be forced to stop when the total number of rules reaches 80000 So the k-condition rule will have small values of k at very low minsups As minsup increases k becomes large so the number of k-condition rules increases exponentially 0\015 0                              015                            Proceedings of the Fourth International Conference on Computer and Information Technology \(CIT04 0-7695-2216-5/04 $20.00  2004 IEEE 


which takes much execution time for strong rule generation and rule pruning operations At high minsup the number of frequent itemsets decreases which results in a small number of rules Rule as showed in Figure 6 so in this case the algorithm becomes faster     1 1 1 1                     1   Figure 6 indicates that the number of rules generated by CBA is 3.2 times more than that of CAAR at minsup=1 At minsup=5 the numbers of mined rules for CBA and CAAR are 20 and 56 respectively and the accuracies for CBA and CAAR are 21.9 and 83.9 as showed in Figure 4 The self-adaptive confidence and dynamic minsup of CAAR account for this big difference in accuracy Even at high minsup CAARs dynamic minsup prevents the number of rules from dramatic decreasing and the self-adaptive confidence threshold always ensures rules of high equality for classifier construction Table 3 shows the overall comparison of classification results among three algorithms for classifying the image blocks Rule denotes the number of rules in the generated classifier For C4.5 the number of rules equals to the number of tree nodes after pruning It is clear that CAAR uses the least rules and achieves the highest accuracy 88.4 for this dataset C4.5 is far faster than CAAR and CBA and CAAR is much faster than CBA Furthermore CBA uses far more memory than CAAR In the experiment we observed that CBA generates 80000 class association rules on this image dataset while CAAR generates at most 695 atomic rules because there are only 695 possible atomic rules 0.79MB memory space is required to store 695 instances of Rule class in a vector However for 80000 rules the memory consumption is up to 92.3MB 5 Conclusion In this work a new associative classification algorithm called CAAR is introduced to classify the image blocks CAAR only mining the atomic rules provides fast and concise classifier for block-based image segmentation Using a self-adaptive confidences threshold 0.92  maxconf can extract the strong atomic association rules with high confidences for classifier construction CAAR takes an easy-first strategy for multiple passes of partial classifications A dynamic support threshold can also automatically adapt the rules granularity for fine classification which can improve the accuracy The experimental were performed on a UCI dataset image segmentation The results show the proposed algorithm provides a fast accurate and robust classifier for block-based image segmentation 6 Acknowledgement This paper was supported by the National Natural Science Foundation of China NSFC grant No 10171033 and the Natural Science Foundation of Guangdong Province of China grant No 31340 7 References 1 B  L iu  W  H su  a n d Y Ma  Integrating classification and association rule mining Proceedings of 4th ACM Int Conf on KDD New York pp 80-86 August 1998 2 E len a Baralis Silv ia Ch iu san o an d P ao lo G a rza On support thresholds in associative classification In Proc of the 19th ACM symposium on Applied computing Nicosia pp 553-558 2004 3 M aria-L u iza A n t o n i e a n d etc A sso ciative C lassif i ers f o r Medical Images Lecture Notes in Artificial Intelligence Vol.2797 Mining Multimedia and Complex Data pp 68-83 Springer-Verlag 2003 4 R us hing  J A  R a n g a na th H  H ink e  T H  G r a v e s  S.J  Huntsville AL Image segmentation using association rule features IEEE Transactions on Image Processing Volume 11 pp.558567 2002 5 M a r i a L ui z a A n t o ni e  O s m a r R  Z a   a ne  A l e x a ndr u Coman Application of Data Mining Techniques for Medical Image classification Proc of the 2nd International Workshop on Multimedia Data Mining San Francisco pp 94-101 August 26 2001 6 R  A g r a w a l  a n d R Srik a n t  F a s t a lg o r ith m s f o r m in in g association rules Proceedings of the 20th Int Conf on VLDB Santiago pp 487-499 September 1994 7 J  R  Q u i n l an  C 4  5  P r o g ram s f o r m ach in e l earn in g Morgan Kaufmann San Francisco 1993 015.!\015 015   1   2     3&4     55     55 Proceedings of the Fourth International Conference on Computer and Information Technology \(CIT04 0-7695-2216-5/04 $20.00  2004 IEEE 


ered frequent 2-itemsets and the Apriori downward property is utilized to generate the minimal number of their candidate calendar patterns. Finally, all frequent itemsets and their cal endar patterns are discovered in one shot. Calendar-based temporal association rules are then obtained. Experimental results have shown that our method is more efficient than others References 11 R. Agrawal and R. Srikant. Fast Algorithms for Min ing Association Rules. In Proceedings of the Inrema tional Very Large Database Conference , pages 487 499,1994 2 ]  J. Han, G. Dong, and Y. Yin. Efficient Mining of Par tial Periodic Patterns in Time Series Databases. In Pro ceedings of the Inremational Conference on Data En gineering, pages 106-1 15, 1999 3] C. H. Lee, C. R. Lin and M. S. Chen. Sliding-Window Filtering: An Efficient Algorithm for Incremental Min ing. In Pmceedings of the ACM 10th Intemational Conference on Information and Knowledge Manage ment, pages 263-270,2001 4] Y. Li, P. Ning, X. S. Wang and S .  Jajodia. Dsicovering Calendar-based Temporal Association Rules. Data and Knowledge Engineering, Vo1.44, No.2, pages 193-21 8 2003 5] B. Ozden, S. Ramaswamy, and A. Silberscbatz. Cyclic Association Rules. In Proceedings of the 15th Inter national Conference on Data Engineering, pages 41 2 421,1998 6] S. Ramaswamy, S. Mahajan, and A. Silberschatz. On the Discovery of Interesting Patterns in Association Rules. In Proceedings of the International Very Large Database Corference , pages 368-379,1998 7] J. F. Roddick and M. Spiliopoulou. A Survey of Tem poral Knowledge Discovery Paradigms and Methods IEEE Trans. Knowledge and Data Engineering, Vol 14, Issue 4., pages 75C!-767,2002 3127 pre></body></html 


decreased. However, refer to Fig.6, it brings the following problem  110 100 010 100 100 011 001 111 100 011 100 34 33 34 DSD u u u                         u              101 100 001 100 100 010 011 111 100 011 100 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


