TSINGHUA SCIENCE AND TECHNOLOGY ISSN ll 1007-0214 ll 04/09 ll pp160\226173 Volume 22 Number 2 April 2017 Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning Zhiqiong Wang Junchang Xin 003  Hongxu Yang Shuo Tian Ge Yu Chenren Xu and Yudong Yao Abstract The Extreme Learning Machine 050ELM\051 and its variants are effective in many machine learning applications such as Imbalanced Learning 050IL\051 or Big Data 050BD\051 learning However they are unable to solve both imbalanced and large-volume data learning problems This study addresses the IL problem in BD applications The Distributed and Weighted ELM 050DW-ELM\051 algorithm is proposed which is based on the MapReduce framework To con\002rm the feasibility of parallel computation 002rst the fact that matrix multiplication operators are decomposable is illustrated Then to further improve the computational ef\002ciency an Improved DW-ELM algorithm 050IDW-ELM\051 is developed using only one MapReduce job The successful operations of the proposed DW-ELM and IDW-ELM algorithms are 002nally validated through experiments Key words weighted Extreme Learning Machine 050ELM\051 imbalanced big data MapReduce framework user-de\002ned counter 1 Introduction Machine Learning 050ML\051 as a technique for effectively predicting and analyzing data   has been widely used for information retrieval medical diagnosis and natural language understanding Many ML algorithms have been investigated including the  017 Zhiqiong Wang and Shuo Tian are with the SinoDutch Biomedical  Information Engineering School Northeastern University Shenyang 110169 China E-mail wangzq@bmie.neu.edu.cn dyhswdza@sina.com 017 Junchang Xin Hongxu Yang and Ge Yu are with the School of Computer Science  Engineering Northeastern University Shenyang 110169 China Email xinjunchang@cse.neu.edu.cn 443577693@qq.com yuge@cse.neu.edu.cn 017 Chenren Xu is with the School of Electronics Engineering and Computer Science Peking University Beijing 100871 China E-mail chenren@pku.edu.cn 017 Yudong Yao is with the Department of Electrical and Computer Engineering Stevens Institute of Technology Castle Point on Hudson Hoboken NJ 07030 USA E-mail yyao@stevens.edu 003 To whom correspondence should be addressed Manuscript received 2016-08-27 revised 2017-01-14 accepted 2017-01-18 Extreme Learning Machine 050ELM\051   which has the least human intervention high learning accuracy and fast learning speed However raw data with imbalanced class distributions which is known as the Imbalanced Learning 050IL\051 problem can be found almost everywhere   and the existing ML algorithms including ELM cannot resolve this problem effectively Some methods such as undersampling  and oversampling   can rebalance the data distributions following which standard ML algorithms can be applied Recently Weighted ELM 050WELM\051  has been proposed to improve the ELM algorithm for the IL problem by assigning different weights to different samples/classes in the training dataset Although ML algorithms including ELM and WELM are effective for various applications their implementation in Big Data 050BD\051 applications still has many challenges  because the data volume to be stored and analyzed greatly exceeds the storage and computing capacity of a single machine One feasible approach is to scale up the corresponding algorithms using parallel or distributed architectures   Several distributed ELMs 050e.g PELM   ELM 003   DKELM   E 2 LM   and A-ELM 003  051 based on the 


Zhiqiong Wang et al Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 161 MapReduce framework  have been proposed to improve the scalability of the ELM algorithm to resolve the problem of BD learning However these algorithms did not consider the IL problem A centralized WELM has been proposed to manage the imbalanced class distribution however it processes big datasets inef\002ciently Some Distributed ELMs 050DELMs\051 have been proposed to resolve the problem of BD learning but they are powerless to imbalanced data Therefore in this study considering both the large volume of data and imbalanced training samples we proposed a Distributed and Weighted ELM 050DW-ELM\051 If the algorithm which is similar to the DELM is directly adopted then two steps in the matrix multiplication need to be calculated when solving the matrices H T WH and H T WT  that is two MapReduce jobs The matrix A D H T W would be calculated in the 002rst MapReduce job and then matrix AH or AT would be calculated in the second MapReduce job Here A is an L 002 N dimensional big matrix and the amount of data transferred between the two jobs is very large therefore the training ef\002ciency is very low We skillfully apply the parameter W as a diagonal matrix then the DW-ELM algorithm is proposed such that matrices H T WH and H T WT can be calculated ef\002ciently in a single MapReduce job which reduces the amount of data transmission effectively and improves the learning ef\002ciency Therefore the DW-ELM algorithm is not a simple combination of DELM and WELM but solves a more complex problem using three matrix multiplications rather than two matrix multiplications of the DELM which is the core contribution in this study This study addresses the issue of IL in BD applications A DW-ELM based on WELM and the distributed MapReduce framework is proposed We prove that the most expensive computational part in the WELM algorithm is decomposable and can be computed in parallel The contributions of this study are as follows First we prove that the matrix multiplication operators in the centralized WELM algorithm are decomposable which indicates that its scalability can be improved by MapReduce Second a DWELM algorithm based on the MapReduce framework is proposed to learn the imbalanced BD ef\002ciently Finally an Improved DW-ELM 050IDW-ELM\051 algorithm is proposed to further improve the computational performance The rest of the study is organized as follows Section 2 brie\003y reviews the background for our work The theoretical foundation and computational details of the proposed DW-ELM and IDW-ELM algorithms are introduced in Section 3 The experimental results to show the effectiveness of the proposed algorithms are reported in Section 4 Finally Section 5 concludes this study 2 Background In this section we describe the background for our work which includes a brief overview of the WELM algorithm  and a detailed description of the distributed MapReduce framework   2.1 Weighted ELM  For N arbitrary distinct training samples  x i  t i   where x i D 214x i1  x i2  001 001 001  x i n 215 T 2 R n and t i D 214t i1  t i2  001 001 001  t im 215 T 2 R m  the ELM can resolve the following learning problem H 014 014 014 D T 0501\051 where 014 014 014 D 214\014 014 014 1  001 001 001  014 014 014 L 215 T is the output weight matrix between the hidden and output nodes T D 214 t 1  001 001 001  t N 215 T are the target labels H D 214 h T  x 1  001 001 001  h T  x N 215 T  where h  x i  D 214g 1  x i  001 001 001  g L  x i 215 are the hidden node outputs and g j  x i  is the output of the j th hidden node for input x i  To maximize the marginal distance and minimize the weighted cumulative error with respect to each sample an optimization problem is mathematically written as Minimize W 1  2 k 014 014 014 k 2 C C W 1  2 N X i D 1 k 030 030 030 i k 2 Subject to W h  x i 014 014 014 D t T i 000 030 030 030 T i 0502\051 where C is the regularization parameter to represent the trade-off between the minimized weighted cumulative error and maximized marginal distance 030 030 030 i is the training error of sample x i caused by the difference of the desired output t i and the actual output h  x i 014 014 014  Here W is an N 002 N diagonal matrix associated with every training sample x i and W i i D 1   t i  0503\051 or W i i D 050 0:618   t i  if   t i   AVG I 1   t i  if   t i  6 AVG 0504\051 where   t i  is the number of samples belonging to class t i and AVG is the average number of samples per class According to the Karush-Kuhn-Tucker theorem   the following solutions for the WELM can be obtained 014 014 014 D 022 I  025 C H T WH 023 000 1 H T WT 0505\051 


162 Tsinghua Science and Technology April 2017 22\0502\051 160\226173 when N 035 L  or 014 014 014 D H T 022 I  025 C WHH T 023 000 1 WT 0506\051 when N 034 L  2.2 MapReduce framework  MapReduce is a simple and 003exible parallel programming model initially proposed by Google for large scale data processing in a distributed computing environment   with Hadoop 050http hadoop.apache.org/\051 being one of its open source implementations The typical procedure of a MapReduce job is illustrated in Fig 1 First the input to a MapReduce job starts as the dataset stored on the underlying distributed 002le system 050e.g Google File System 050GFS\051  and Hadoop Distributed File System 050HDFS\051  051 which is split into several 002les across machines Next the MapReduce job is partitioned into many independent map tasks Each map task processes a logical split of the input dataset The map function takes a key-value pair k 1  v 1  as input and generates intermediate key-value pairs 214.k 2  v 2 215  Next the intermediate data 002les from the already completed map tasks are fetched by the corresponding reduce task following the 223pull\224 model 050similarly there are many independent reduce tasks\051 The intermediate data 002les from all the map tasks are sorted accordingly and passed to the corresponding reduce task The reduce function combines all intermediate key-value pairs k 2  214v 2 215 with the same key into the output key-value pairs 214.k 3  v 3 215  Finally the output data from the reduce task is generally written back to the corresponding distributed 002le system 3 Distributed and Weighted Extreme Learning Machine In this section we present the DW-ELM and IDW-ELM algorithms To implement the parallel computation of Fig 1 Illustration of the MapReduce framework the WELM algorithm using the MapReduce framework we 002rst need to prove the matrix decomposability in the WELM algorithm 3.1 Decomposability of the WELM algorithm In imbalanced BD learning applications the number of training records is much larger than the number of hidden nodes that is N 035 L  Therefore the size of H T WH is much smaller than that of WHH T  Equation 0505\051 is used to calculate the output weight vector 014 014 014 in WELM We 002rst analyze the properties of the WELM algorithm and formulate the matrix operations in the form of the MapReduce framework In this way we can extend the WELM algorithm for BD applications Let U D H T WH and V D H T WT  Thus we obtain 014 014 014 D 022 I  025 C U 023 000 1 V 0507\051 Following the matrix operations we have Eq 0508\051 Then we further obtain u ij D N X k D 1 W kk 002 g w i 001 x k C b i  002 g w j 001 x k C b j  0509\051 Similarly following the matrix operations we obtain Eq 05010\051 Finally we obtain Eq 05011\051 v ij D N X k D 1 W kk 002 g w i 001 x k C b i  002 t kj 05011\051 According to Eq 0509\051 we know that the item u ij in matrix U can be expressed by the summation of W kk 002 g w i 001 x k C b i  002 g w j 001 x k C b j   Here W kk is the weight of the training sample  x k  t k   and h ki D g w i 001 x k C b i  and h kj D g w j 001 x k C b j  are the i th and j th elements in the k th row h x k  of the hidden layer output matrix H  respectively Similarly according to Eq 05011\051 we know that item v ij in matrix V can be expressed by the summation of W kk 002 g w i 001 x k C b i  002 t kj  Here t kj is the j th element in the k th row t k of matrix T that is related to the training sample  x k  t k   The variables involved in the equations for the matrices U and V include W kk  h ki  h kj  and t kj  According to Eqs 0503\051 and 0504\051 to calculate the corresponding weight W kk related to the training sample  x k  t k   we must 002rst obtain the number   t k  of training samples that belong to the same class as t k  The number of training samples in all classes can be easily calculated in one MapReduce job At the same time the remaining three variables namely h ki  h kj  and t kj  are 


Zhiqiong Wang et al Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 163 U D H T WH D 2 6 6 6 6 4 h x 1  h x 2     h x N  3 7 7 7 7 5 T 2 6 6 6 6 4 W 11 0 001 001 001 0 0 W 22 001 001 001 0 0 0    0 0 0 001 001 001 W N N 3 7 7 7 7 5 2 6 6 6 6 4 h x 1  h x 2     h x N  3 7 7 7 7 5 D h h x 1  T h x 2  T 001 001 001 h x N  T i 2 6 6 6 6 4 W 11 0 001 001 001 0 0 W 22 001 001 001 0 0 0    0 0 0 001 001 001 W N N 3 7 7 7 7 5 2 6 6 6 6 4 h x 1  h x 2     h x N  3 7 7 7 7 5 D h x 1  T W 11 h x 1  C h x 2  T W 22 h x 2  C 001 001 001 C h x N  T W N N h x N  D N X k D 1 h x k  T W kk h x k  D N X k D 1 2 6 6 6 6 4 g w 1 001 x k C b 1  g w 2 001 x k C b 2     g w L 001 x k C b L  3 7 7 7 7 5 W kk h g w 1 001 x k C b 1  g w 2 001 x k C b 2  001 001 001 g w L 001 x k C b L  i D N X k D 1 W kk 2 6 6 6 6 4 g w 1 001 x k C b 1  g w 2 001 x k C b 2     g w L 001 x k C b L  3 7 7 7 7 5 h g w 1 001 x k C b 1  g w 2 001 x k C b 2  001 001 001 g w L 001 x k C b L  i 0508\051 V D H T WT D 2 6 6 6 6 4 h x 1  h x 2     h x N  3 7 7 7 7 5 T 2 6 6 6 6 4 W 11 0 001 001 001 0 0 W 22 001 001 001 0 0 0    0 0 0 001 001 001 W N N 3 7 7 7 7 5 2 6 6 6 6 4 t 1 t 2    t N 3 7 7 7 7 5 D h h x 1  T h x 2  T 001 001 001 h x N  T i 2 6 6 6 6 4 W 11 0 001 001 001 0 0 W 22 001 001 001 0 0 0    0 0 0 001 001 001 W N N 3 7 7 7 7 5 2 6 6 6 6 4 t 1 t 2    t N 3 7 7 7 7 5 D h x 1  T W 11 t 1 C h x 2  T W 22 t 2 C 001 001 001 C h x N  T W N N t N D N X i D 1 h x k  T W kk t k D N X k D 1 2 6 6 6 6 4 g w 1 001 x k C b 1  g w 2 001 x k C b 2     g w L 001 x k C b L  3 7 7 7 7 5 W kk h t k1 t k2 001 001 001 t km i D N X k D 1 W kk 2 6 6 6 6 4 g w 1 001 x k C b 1  g w 2 001 x k C b 2     g w L 001 x k C b L  3 7 7 7 7 5 h t k1 t k2 001 001 001 t km i 05010\051 only related to the training sample  x k  t k  itself and not the other training samples therefore the calculation of the U and V matrices can be accomplished in another MapReduce job To summarize the calculation process of the U and V matrices is decomposable therefore we can realize the parallel computation of these matrices using the MapReduce framework to break through the limitation of a single machine and improve the ef\002ciency in which the WELM algorithm learns the imbalanced big training 


164 Tsinghua Science and Technology April 2017 22\0502\051 160\226173 data 3.2 DW-ELM algorithm The process for the DW-ELM algorithm is shown in Algorithm 1 First we randomly generate L pairs of hidden node parameters  w i  b i  050lines 1 and 2\051 Then we use a MapReduce job to count the number of training samples contained in each class 050line 3\051 Next we use another MapReduce job to calculate the U and V matrices according to the input parameters and randomly generate the parameters 050line 4\051 Finally we solve the output weight vector 014 014 014 according to Eq 0505\051 050line We describe the speci\002c processes of the two MapReduce jobs involved in the DW-ELM algorithm The process for the 002rst MapReduce job of the DWELM algorithm is shown in Algorithm 2 The algorithm includes two classes Mapper 050lines 1\22610\051 and Reducer 050lines 11\22616\051 Class Mapper contains three methods Initialize 050lines 2 and 3\051 Map 050lines 4\2267\051 and Close 050lines 8\22610\051 whereas Class Reducer only contains one method namely Reduce 050lines 12\22616\051 In the Initialize method of the Mapper class we initialize one array c  which is used to store the intermediate summation of the training samples contained in each class 050line 3\051 In the Map method of the Mapper class 002rst we analyze the training sample s and resolve the class to which sample s belongs 050lines 5 and 6\051 Then we adjust the corresponding value in the array c 050line 7\051 In the Close method of the Mapper class the intermediate summations stored in c are emitted by the mapper 050lines 9 and 10\051 In the Reduce method of the Reducer class 002rst we initialize a temporary variable sum 050line 13\051 Next we combine the intermediate summations of the different mappers that have the same Key and obtain the 002nal summation of the corresponding element of the Key 050lines 14 and 15\051 Finally we store the results in a distributed 002le system 050line 16\051 The process for the second MapReduce job of the DW-ELM algorithm is shown in Algorithm 3 In the Initialize method of the Mapper class we initialize two arrays u and v  which are used to store the  Algorithm 1 DW-ELM  1 for i D 1 to L do 2 Randomly generate hidden node parameters  w i  b i  3 Calculate all   t k  using Algorithm 2 4 Calculate U D H T WH  V D H T WT using Algorithm 3 5 Calculate the output weight vector 014 014 014 D  I 025 C U  000 1 V   Algorithm 2 First MapReduce Job of the DW-ELM Algorithm  1 class M APPER 2 method I NITIALIZE 050\051 3 c D new A SSOCIATIVE A RRAY 4 method M AP 050sid id sample s 051 5 t D ParseT s 6 num D Class  t  7 c\214 num 215 D c\214 num 215 C 1 8 method C LOSE 050\051 9 for i D 1 to c Length\050\051 do 10 context.write\050cid i  count c\214i\215 051 11 class R EDUCER 12 method R EDUCE 050cid id counts 214c 1  c 2     215 051 13 sum D 0 14 for all count c 2 214c 1  c 2     215 do 15 sum D sum C c 16 context.write\050cid id count sum\051   Algorithm 3 Second MapReduce Job of the DW-ELM Algorithm  1 class M APPER 2 method I NITIALIZE 050\051 3 u D new A SSOCIATIVE A RRAY 4 v D new A SSOCIATIVE A RRAY 5 method M AP 050sid id sample s 051 6 h D new A SSOCIATIVE A RRAY 7  x  t  D ParseAll s 8 w D Weight\050Counts 214 Class  t 215 051 9 for i D 1 to L do 10 h\214i\215 D g w i 001 x C b i  11 for i D 1 to L do 12 for j D 1 to L do 13 u\214i j 215 D u\214i j 215 C w 002 h\214i\215 002 h\214j 215 14 for j D 1 to m do 15 v\214i j 215 D v\214i j 215 C w 002 h\214i\215 002 t 214j 215 16 method C LOSE 050\051 17 for i D 1 to L do 18 for j D 1 to L do 19 context.write\050triple 050 0 U 0  i  j 051 sum u\214i j 215 051 20 for j D 1 to m do 21 context.write\050triple 050 0 V 0  i  j 051 sum v\214i j 215 051 22 class R EDUCER 23 method R EDUCE 050triple p  sum 214s 1  s 2     215 051 24 uv D 0 25 for all sum s 2 214s 1  s 2     215 do 26 uv D uv C s 27 context.write\050triple p  sum uv 051  intermediate summations of the elements in the U and V matrices respectively In the Map method of the Mapper class 002rst we initialize a local variable h 050line 6\051 Then we resolve the input training sample s and divide s into the training feature x and its corresponding training result t 050line 7\051 Again according to the training 


Zhiqiong Wang et al Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 165 result t and the result of Algorithm 2 we obtain the corresponding weight w of s 050line 8\051 Next we calculate the corresponding hidden layer output vector h x  050lines 9 and 10\051 Finally we separately calculate the local summations of the elements in the U and V matrices and save the result to local variables u and v 050lines 11\22615\051 In the Close method of the Mapper class the intermediate summations stored in u and v are emitted by the mapper 050lines 17\22621\051 In the Reduce method of the Reducer class 002rst we initialize a temporary variable uv 050line 24\051 Next we combine the intermediate summations of the different mappers that have the same Key and obtain the 002nal summation of the corresponding element of the Key 050lines 25 and 26\051 Finally we store the results in the distributed 002le system 050line 27\051 The DW-ELM algorithm requires two MapReduce jobs where job#1 is used to count the number of samples   t k   of each t k category in m categories therefore the total amount of data that job#1 needs to transmit in the Map phase is the total number of training samples multiplied by the number of Mapper Assuming the number of Mappers is N Mapper  Then we can obtain cost basic1 D sizeof  Integer  001 m 001 N Mapper 05012\051 Job#2 is used to calculate the H T WH and H T WT matrices Therefore the total amount of data to be transferred to job#2 is the U and V matrix size multiplied by N Mapper in the Map phase Assuming that the number of hidden layer nodes is L  the number of categories is m  and the U and V matrices are L 002 L and L 002 m  respectively we further obtain cost basic2 D sizeof  003oat  001  L 001 L C L 001 m  001 N Mapper 05013\051 and cost basic D cost basic1 C cost basic2 D sizeof  Integer  001 C 001 N Mapper C sizeof  003oat  001  L 001 L C L 001 m  001 N Mapper 05014\051 3.3 IDW-ELM algorithm The DW-ELM algorithm requires two MapReduce jobs to complete the training process If we can merge the above two MapReduce jobs into a single MapReduce job the performance of the DW-ELM algorithm can be signi\002cantly improved In addition to providing several built-in counters the MapReduce framework allows users to employ their own user-de\002ned counters Using a user-de\002ned counter users can transfer some statistical information between the Mapper and Reducer classes The task of the 002rst MapReduce job in the DW-ELM algorithm is to count the number of training samples contained in each class however we propose an IDW-ELM algorithm based on user-de\002ned counters using only one MapReduce job to complete the tasks The framework is shown in Fig 2 The process for the MapReduce job of the IDWELM algorithm is shown in Algorithm 4 The Initialize method of the Mapper class is similar to Algorithm 3 In the Map method of the Mapper class 002rst we initial a local variable h 050line 6\051 Then we resolve the training sample s  which means dividing s into the training feature x and its corresponding training result t 050line 7\051 Again we resolve the class that sample s belongs to according to the training result t  and adjust the value of the corresponding user-de\002ned counter 050lines 8 and 9\051 Next we calculate the hidden layer output vector h x  corresponding to x 050lines 10 and 11\051 Finally we separately calculate the local summations of the elements in the U and V matrices and save the result to local variables u and v 050lines 12\22616\051 050the results of different class need to be stored respectively so u and v are 3-dimensional arrays.\051 In the Close method of the Mapper class the intermediate summations stored in u and v are emitted by the mapper 050lines 18\226 23\051 In the Reduce method of the Reducer class 002rst we initialize a temporary variable uv 050line 25\051 Next we calculate the corresponding weight according to the statistical information stored in the user-de\002ned counters to combine the intermediate summations of the different mappers that have the same Key and obtain the 002nal summation of the corresponding element of the Key 050lines 27\22629\051 Finally we store the results in the distributed 002le system 050line 30\051 With the IDW-ELM algorithm only one MapReduce job is required using a user-de\002ned counter to complete the calculation local accumulation of u and v need to be computed separately in each classi\002er The cost improved1 variable is the amount of counter transmission Hence we can obtain cost improved D cost improved1 C cost improved2 D sizeof  Integer  001 C 001 N Mapper C sizeof  003oat  001 C 001  L 001 L C L 001 m  001 N Mapper 05015\051 Assuming that the network bandwidth is B  the 


166 Tsinghua Science and Technology April 2017 22\0502\051 160\226173 Fig 2 The framework of the IDW-ELM algorithm transmission times of the DW-ELM and IDW-ELM algorithms are respectively T basic D cost basic  B D sizeof  Integer  001 C 001 N Mapper 016 B C sizeof  003oat  001  L 001 L C L 001 m  001 N Mapper 016 B 05016\051 and T improved D cost improved 016 B D sizeof  Integer  001 C 001 N Mapper 016 B C sizeof  003oat  001 C 001  L 001 L C L 001 m  001 N Mapper 016 B 05017\051 If the start-up time of the MapReduce job is T start and if T improved 000 T basic  T start  then the performance of the IDW-ELM algorithm is better than that of the DW-ELM algorithm else T improved 000 T basic  T start and the performance of the IDW-ELM algorithm is lower than that of the DW-ELM algorithm Through the analysis of the communication cost of the DW-ELM and IDW-ELM algorithms we can see that when C is large the communication cost of the IDW-ELM algorithm will be far greater than that of the DW-ELM algorithm Under normal circumstances the number of data classi\002cations is usually small and the network bandwidth is large therefore the performance of the IDW-ELM algorithm in the experiment is better than that of the DW-ELM algorithm 


Zhiqiong Wang et al Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 167  Algorithm 4 The IDW-ELM Algorithm  1 class M APPER 2 method I NITIALIZE 050\051 3 u D new A SSOCIATIVE A RRAY 4 v D new A SSOCIATIVE A RRAY 5 method M AP 050sid id sample s 051 6 h D new A SSOCIATIVE A RRAY 7  x  t  D ParseAll s 8 c D Class  t  9 context.getCounter\050\223Classes\224 223c\224 c 051.increment\0501\051 10 for i D 1 to L do 11 h\214i\215 D g w i 001 x C b i  12 for i D 1 to L do 13 for j D 1 to L do 14 u\214c i j 215 D u\214c i j 215 C h\214i\215 002 h\214j 215 15 for j D 1 to m do 16 v\214c i j 215 D v\214c i j 215 C h\214i\215 002 t 214j 215 17 method C LOSE 050\051 18 for k D 1 to u Length  do 19 for i D 1 to L do 20 for j D 1 to L do 21 context.write\050triple 050 0 U 0  i  j 051 value 050 k  u\214k i j 215 051\051 22 for j D 1 to m do 23 context.write\050triple 050 0 V 0  i  j 051 value 050 k  v\214k i j 215 051\051 24 class R EDUCER 25 method R EDUCE 050triple p  values 214s 1  s 2     215 051 26 uv D 0 27 for all sum s 2 214s 1  s 2     215 do 28 count  context.getCounter\050\223Classes\224 223c\224  s getClass\050\051\051.getValue\050\051 29 uv D uv C Weight\050count\051 002 s getValue\050\051 30 context.write\050triple p  sum uv 051  4 Results and Performance Evaluation In this section the performances of our proposed DWELM and IDW-ELM algorithms are evaluated in detail by considering various experimental settings We 002rst describe the platform used in our experiments in Section 4.1 Then we present and discuss the experimental results 4.1 Experimental platform All the experiments are run in a cluster with nine computers connected in a high-speed Gigabit network Each computer has an Intel Quad Core 2:66 GHz CPU 4 GB memory and CentOS Linux 5:6 operating system One computer is set as the master node and the others are set as the slave nodes We use Hadoop version 0:20:2 and con\002gure it to run up to 4 map tasks or 4 reduce tasks concurrently per node Therefore at any point in time at most 32 map tasks or 32 reduce tasks can run concurrently in our cluster In this experiment the synthetic data that was randomly generated in the 2140 1\215 d data space are used to verify the ef\002ciency and effectiveness of our proposed DW-ELM and IDW-ELM algorithms The generation procedure of the experimental data is as follows First the data center of each class is generated randomly second the data of each class are generated according to the multivariate Gaussian distribution where the former generated center point is used as the mean whereas the reciprocal of the number of classes is the variance Finally the generated data of all classes are combined into the experimental data The real-life datasets with yeast glass ecoli pima and colon are downloaded online The imbalance ratio of the dataset can be as low as 0.0304 and the highest imbalance ratio occurs at 0.6667 However both the aduit and banana datasets are from UCI where the imbalance ratios are 0.3306 and 0.8605 respectively Table 1 summarizes the parameters used in our experimental evaluation In each experiment we vary a single parameter while setting all others to their default values The imbalance ratio which quantitatively measures the imbalance degree of a dataset is de\002ned as follows   Imbalance ratio D Min    t i  Max    t i  05018\051 The DW-ELM algorithm is a MapReduce-based distributed implementation of the original or centralized WELM algorithm and it does not change the formulation in the WELM algorithm Therefore it does not have any impact on the classi\002cation accuracy We evaluate the training time and speedup of the DW-ELM and IDW-ELM algorithms The speedup achieved by an m computer mega system is de\002ned as Speedup\050 m 051 D Computing time on 1 computer  Computing time on m computers 05019\051 Table 1 Experimental parameters  Parameter Range Default  Number of nodes 1 2 3 4 5 6 7 8 8 Dimensionality 10 20 30 40 50 50 Number of hidden nodes 100 150 200 250 300 200 Number of records 9 002 10 6  12 002 10 6  15 002 10 6  18 002 10 6  21 002 10 6 21 002 10 6 Number of classes 5 10 15 20 25 15 Imbalance ratio 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.5  


168 Tsinghua Science and Technology April 2017 22\0502\051 160\226173 4.2 Experimental results Table 2 describes the accuracy of the ELM 003  and DW-ELM algorithms under the real-life datasets From Table 2 the DW-ELM algorithm outperforms the ELM 003  algorithm The next experiment veri\002es the DW-ELM algorithm training time and the acceleration ratio under the synthetic datasets First the impact of the number of slave nodes in the cluster on the running time of the ELM 003   DW-ELM and IDW-ELM algorithms is discussed As shown in Fig 3a the training times for the ELM 003   DW-ELM and IDW-ELM algorithms decrease signi\002cantly with increasing the number of slave nodes in the cluster and the training time of the ELM 003  algorithm is always lower than that of the DW-ELM and IDW-ELM algorithms Moreover the IDW-ELM algorithm is always lower than that of the DW-ELM algorithm The various speedup ratios through changing the number of slave nodes are shown in Fig 3b and the speedup ratio closely follows a linear growth trend When the number of slave nodes increases the number of map and reduce tasks that can be executed simultaneously also increases which means the amount of work that can be completed simultaneously Therefore on the premise of the same amount of work the training time for the ELM 003   DW-ELM and IDW-ELM algorithms decreases with increasing number of slave nodes Second we investigate the impact of the training data dimensionality As shown in Fig 4a with the increase of training data dimensionality the training times of the ELM 003   DW-ELM and IDW-ELM algorithms all increase slightly while the training time of the ELM 003  algorithm is signi\002cantly lower than that of the DW-ELM and IDW-ELM algorithms Also the training time of the IDW-ELM algorithm is Table 2 Performance results of the ELM 003  and DW-ELM algorithms  Datasets 050imbalance ratio\051 Testing result 050%\051  ELM DW-ELM  Yeast 0500.0304\051 80.14 94.22 Glass 0500.1554\051 93.99 95.47 Ecoli 0500.1806\051 91.09 93.81 Aduit 0500.3306\051 72.87 80.76 Pima 0500.5350\051 70.52 71.81 Colon 0500.6667\051 84.78 85.16 Banana 0500.8605\051 87.98 88.04  Fig 3 The impact of the number of slave nodes on 050a\051 time and 050b\051 speedup ratio always lower than that of the DW-ELM algorithm The variation trend of speedup ratio of three algorithms is shown in Fig 4b where the speedup ratios of three algorithms remain stable with the change of the data dimensionality and the algorithm performance of the IDW-ELM algorithm is slightly better than that of the DW-ELM algorithm Increasing the training data dimensionality leads to the running time for calculating the corresponding row h k of the hidden layer output matrix H in Mapper to slightly increase thus the training times of the DW-ELM and IDWELM algorithms also slightly increase The DW-ELM algorithm uses two MapReduce jobs to complete the calculation of the U and V matrices but the IDW-ELM algorithm only uses one MapReduce jobs to complete the same calculation Although compared to the second MapReduce job of the DW-ELM algorithm the amount of middle data transmission slightly increased but its transmission time is far less than the time required by 


Zhiqiong Wang et al Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 169 Fig 4 The impact of dimensionality on 050a\051 time and 050b\051 speedup ratio the 002rst MapReduce job of the DW-ELM algorithm Therefore the training time of the IDW-ELM algorithm was signi\002cantly lower than that of the DW-ELM algorithm illustrating the effectiveness of the proposed improved algorithm Again we investigate the impact of the number of hidden nodes As shown in Fig 5a by increasing the number of hidden nodes the training times of the ELM 003   DW-ELM and IDW-ELM algorithms all increase while the training time of the IDW-ELM algorithm is signi\002cantly lower than that of DW-ELM algorithm In addition the ELM 003  algorithm is lower than that of IDW-ELM algorithm The various of speedup ratios through changing the number of hidden nodes is shown in Fig 5b and the speedup ratio remains stable Increasing the number of hidden nodes leads to the dimensionality of the hidden layer output matrix H to increase and indirectly leads to the increase of the dimensionality of the intermediate U and V matrices This not only makes the computation time of the local Fig 5 The impact of the number of hidden nodes on 050a\051 time and 050b\051 speedup ratio accumulated sum of U and V increase but also makes the transmission time of the intermediate results in the MapReduce job increase Therefore the training times for the DW-ELM and IDW-ELM algorithms increase with the number of hidden nodes The DWELM algorithm uses two MapReduce jobs while the IDW-ELM algorithm uses only one MapReduce jobs to complete the calculation of the U and V matrices Overall the training time of the IDW-ELM algorithm is less than that of the DW-ELM algorithm which veri\002es the validity of the improved algorithm Then we investigate the impact of the number of training records As shown in Fig 6a with the increasing number of records the training times of the ELM 003   DW-ELM and IDW-ELM algorithms all increase obviously While the training time of the IDW-ELM algorithm is signi\002cantly lower than that of the DW-ELM algorithm and the ELM 003  algorithm is signi\002cantly lower than that of the IDWELM algorithm The impact of increasing the number 


170 Tsinghua Science and Technology April 2017 22\0502\051 160\226173 Fig 6 The impact of the number of records on 050a\051 time and 050b\051 speedup ratio of training records on the speedup ratio is shown in Fig 6b Although the speedup ratio does not achieve a linear change but the speedup ratio of the ELM 003  algorithm is slightly higher than that of the DW-ELM and IDW-ELM algorithms Increasing the number of records means that the numbers of Mapper and Reducer which need to be launched increase On the other hand it increases the number of corresponding locally accumulated summation of U and V that need to be transmitted leading to increased transmission time of the intermediate results Therefore the training times of the DW-ELM and IDW-ELM algorithms increase with the increasing number of training records In addition the training time of the IDW-ELM algorithm is less than that of the DW-ELM algorithm which further veri\002es the validity of the improved algorithm Next we investigate the impact of the number of classes As shown in Fig 7a along with the increasing number of classes the training times of the ELM 003  and DW-ELM algorithms are basically stable while Fig 7 The impact of the number of classes on 050a\051 time and 050b\051 speedup ratio the training time of the IDW-ELM algorithm is slightly increased and it is always less than the training time of the DW-ELM algorithm The in\003uence of changing the number of classes on speedup ratio is shown in Fig 7b and the speedup ratio remains stable The number of classes increases which only increases the number of statistical values in the 002rst MapReduce job and subsequently the number of input values in the second MapReduce job of the DW-ELM algorithm which has limited impact on the overall training time so the training time is relatively stable However the increase in the number of classes makes the number of output intermediate results of the Mapper class in the IDWELM algorithm increase and to a certain extent this leads to the transmission time of the intermediate results to increase so the training time increases slightly In addition the training time of the IDW-ELM algorithm is less than that of the DW-ELM algorithm which means that although the effects of the improved algorithm decrease slightly with the increase in the number of 


Zhiqiong Wang et al Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 171 classes the optimization effect is still good Finally we investigate the impact of imbalance ratio As shown in Fig 8a with the increasing imbalance ratio the training times of the ELM 003   DW-ELM and IDW-ELM algorithms are basically stable and the training time of the IDW-ELM algorithm is always lower than that of the DW-ELM algorithm Also the training time of the ELM 003  algorithm is lower than that of the IDW-ELM algorithm The variation in speedup ratio of changing the imbalance ratio of the training records is shown in Fig 8b and the speedup ratio remains stable Increasing the imbalance ratio did not produce any substantial effects on the calculation process of the MapReduce job so the training time is relatively stable Since the training time of the IDW-ELM algorithm is less than that of the DWELM algorithm this further veri\002es the validity of the improved algorithm From the results of the six groups the training time of the ELM 003  algorithm is less than that of the DWFig 8 The impact of the imbalance ratio on 050a\051 time and 050b\051 speedup ratio ELM algorithm in each group of experiments The reason is because in the DW-ELM algorithm training process the number of samples in each category needed to be 002rst counted that is calculate the diagonal elements of the weight matrix W in the Map phase then the H T H and H T T matrices were calculated in the MapReduce calculation while only H T H and H T T matrices needed to be counted in the Map phase of the ELM 003  algorithm However the G-mean value of the ELM 003  algorithm in managing the unbalanced data was signi\002cantly lower than that of the DW-ELM algorithm which apparently was not applicable to the imbalance BD learning 5 Conclusion Neither the WELM nor the DELM algorithms manage the imbalanced big training data ef\002ciently since they only consider either the 223big\224 or the 223imbalanced\224 aspect of imbalanced big training data and not both In this study we proposed a DW-ELM algorithm based on the MapReduce framework The DW-ELM algorithm makes full use of the parallel computing ability of the MapReduce framework and realizes ef\002cient learning of imbalanced BD Speci\002cally through analyzing the characteristics of the WELM algorithm we found that the matrix multiplication operators 050i.e H T WH and H T WT 051 within the WELM algorithm are decomposable Then we transformed the corresponding matrix multiplication operators into summation forms which suited the MapReduce framework well and proposed a DWELM algorithm to calculate the matrix multiplications using two MapReduce jobs Furthermore an IDWELM algorithm which only uses one MapReduce job to complete the tasks was proposed to improve the performance of the DW-ELM algorithm Finally in the cluster environment we performed a detailed validation of the performance of the DW-ELM and IDW-ELM algorithms with various experimental settings The experimental results show that the DW-ELM and IDWELM algorithms can learn imbalanced BD ef\002ciently Acknowledgment This research was partially supported by the National Natural Science Foundation of China 050Nos 61402089 61472069 and 61501101\051 the Fundamental Research Funds for the Central Universities 050Nos N161904001 N161602003 and N150408001\051 the Natural Science Foundation of Liaoning Province 050No 


172 Tsinghua Science and Technology April 2017 22\0502\051 160\226173 2015020553\051 the China Postdoctoral Science Foundation 050No 2016M591447\051 and the Postdoctoral Science Foundation of Northeastern University 050No 20160203\051 References   P Langley The changing science of machine learning Mach Learn  vol 82 no 3 pp 275\226279 2011   G.-B Huang Q.-Y Zhu and C.-K Siew Extreme learning machine Theory and applications Neurocomputing  vol 70 nos 1\2263 pp 489\226501 2006   G.-B Huang L Chen and C.-K Siew Universal approximation using incremental constructive feedforward networks with random hidden nodes IEEE Trans Neural Netw  vol 17 no 4 pp 879\226892 2006   G.-B Huang and L Chen Convex incremental extreme learning machine Neurocomputing  vol 70 nos 16\22618 pp 3056\2263062 2007   G.-B Huang and L Chen Enhanced random search based incremental extreme learning machine Neurocomputing  vol 71 nos 16\22618 pp 3460\2263468 2008   G.-B Huang X Ding and H Zhou Optimization method based extreme learning machine for classi\002cation Neurocomputing  vol 74 nos 1\2263 pp 155\226163 2010   G.-B Huang H Zhou X Ding and R Zhang Extreme learning machine for regression and multiclass classi\002cation IEEE Trans Syst Man Cybern Part BCybern  vol 42 no 2 pp 513\226529 2012   G.-B Huang D Wang and Y Lan Extreme learning machines A survey Int J Mach Learn Cybern  vol 2 no 2 pp 107\226122 2011   Q.-Y Zhu A K Qin P N Suganthan and G.-B Huang Evolutionary extreme learning machine Pattern Recognit  vol 38 no 10 pp 1759\2261763 2005   G.-B Huang N.-Y Liang H.-J Rong P Saratchandran and N Sundararajan On-line sequential extreme learning machine in Proc of the IASTED Int Conf on Computational Intelligence  Calgary Canada 2005 pp 232\226237   N.-Y Liang G.-B Huang P Saratchandran and N Sundararajan A fast and accurate on-line sequential learning algorithm for feedforward networks IEEE Trans Neural Netw  vol 17 no 6 pp 1411\2261423 2006   H.-J Rong G.-B Huang N Sundararajan and P Saratchandran On-line sequential fuzzy extreme learning machine for function approximation and classi\002cation problems IEEE Trans Syst Man Cybern Part B-Cybern  vol 39 no 4 pp 1067\2261072 2009   H He and E A Garcia Learning from imbalanced data IEEE Trans Knowl Data Eng  vol 21 no 9 pp 1263\226 1284 2009   X Liu J Wu and Z Zhou Exploratory undersampling for class-imbalance learning IEEE Trans Syst Man Cybern Part B-Cybern  vol 39 no.2 pp 539\226550 2006   H Han W.-Y Wang and B.-H Mao Borderline-SMOTE A new over-sampling method in imbalanced data sets learning in Proc of Int Conf on Intelligent Computing  Hefei China 2005 pp 878\226887   W Zong G.-B Huang and Y Chen Weighted extreme learning machine for imbalance learning Neurocomputing  vol 101 no 3 pp 229\226242 2013   M Chen S Mao and Y Liu Big data A survey Mobile Netw Appl  vol 19 no 2 pp 171\226209 2014   J Chen Y Chen X Du C Li J Lu S Zhao and X Zhou Big data challenge A data management perspective Front Comput Sci  vol 7 no 2 pp 157\226164 2013   Q He T Shang F Zhuang and Z Shi Parallel extreme learning machine for regression based on mapreduce Neurocomputing  vol 102 no 2 pp 52\22658 2013   J Xin Z Wang C Chen L Ding G Wang and Y Zhao ELM 003  Distributed extreme learning machine with mapreduce World Wide Web  vol 17 no 5 pp 1189\226 1204 2014   X Bi X Zhao G Wang P Zhang and C Wang Distributed extreme learning machine with kernels based on mapreduce Neurocomputing  vol 149 no 1 pp 456\226 463 2015   J Xin Z Wang L Qu and G Wang Elastic extreme learning machine for big data classi\002cation Neurocomputing  vol 149 no 1 pp 464\226471 2015   J Xin Z Wang L Qu G Yu and Y Kang AELM 003  Adaptive distributed extreme learning machine with MapReduce Neurocomputing  vol 174 no 1 pp 368\226374 2016   J Dean and S Ghemawat MapReduce Simpli\002ed data processing on large clusters in Proc Symposium on Operating System Design and Implementation  San Francisco CA USA 2004 pp 137\226150   J Dean and S Ghemawat MapReduce Simpli\002ed data processing on large clusters Commun ACM  vol 51 no 1 pp 107\226113 2008   J Dean and S Ghemawat MapReduce A 003exible data processing tool Commun ACM  vol 53 no 1 pp 72\22677 2010   C.-T Chu S.-K Kim Y.-A Lin Y.-Y Yu G Bradski A.Y Ng and K Olukotun Map-reduce for machine learning on multicore in Proc 20th Annual Conf on Neural Information Processing Systems  Vancouver Canada 2007 pp 281\226288   X Meng and M W Mahoney Robust regression on mapreduce in Proc 30th Int Conf on Machine Learning  Atlanta GA USA 2013 pp 888\226896   R Fletcher Constrained optimization in Practical Methods of Optimization  N J Hoboken Ed John Wiley  Sons Ltd 1981 pp 127\226416   S Ghemawat H Gobioff and S.-T Leung The google 002le system in Proc 19th ACM Symposium on Operating Systems Principles  New York UK USA 2003 pp 29\22643   K Shvachko H Kuang S Radia and R Chansler The hadoop distributed 002le system in Proc 26th IEEE Symposium on Mass Storage Systems and Technologies  Incline Village NV USA 2010 pp 1\22610 


Zhiqiong Wang et al Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 173 Zhiqiong Wang received the MSc degree and PhD degree in computer software and theory from the Northeastern University China in 2008 and 2014 respectively She visited the National University of Singapore in 2010 and the Chinese University of Hong Kong in 2013 as the academic visitor Currently she is an associate professor with the Sino-Dutch Biomedical and Information Engineering School of Northeastern University She served as a PI or co-PI for more than ten national research grants from NSFC the Natural Science Foundation of Liaoning Province etc She has published more than 30 papers Her main research interests are biomedical biological data processing cloud computing and machine learning Junchang Xin received the BSc MSc and PhD degrees in computer science and technology from the Northeastern University China in 2002 2005 and 2008 respectively He visited National University of Singapore as Post-Doctoral Visitor April 2010  April 2011 He is currently an associate professor with the School of Computer Science and Engineering Northeastern University China He served as a PI or co-PI for more than ten national research grants from NSFC the 863 Program Project 908 under the State Oceanic Administration SOA etc He has published more than 60 research papers His research interests include big data uncertain data and bioinformatics Yudong Yao received the BEng and MEng degrees from Nanjing University of Posts and Telecommunications China in 1982 and 1985 respectively and the PhD degree from Southeast University China in 1988 all in electrical engineering From 1989 and 1990 he was at Carleton University Ottawa Canada as a research associate working on mobile radio communications From 1990 to 1994 he was with Spar Aerospace Ltd Montreal Canada where he was involved in research on satellite communications From 1994 to 2000 he was with Qualcomm Inc San Diego CA where he participated in research and development in wireless code-division multipleaccess CDMA systems He has been with Stevens Institute of Technology Hoboken New Jersey since 2000 and is currently a professor and department director of electrical and computer engineering Dr Yao was an associate editor of IEEE Communications Letters and IEEE Transactions on Vehicular Technology  and an editor for IEEE Transactions on Wireless Communications  He served as a PI or co-PI for multiple national research grants from NSF He holds one Chinese patent and twelve U.S patents and published more than 100 research papers His research interests include wireless communications and networks spread spectrum and CDMA antenna arrays and beamforming cognitive and software de\002ned radio CSDR and digital signal processing for wireless systems He is a NAI fellow IEEE fellow and IEEE ComSoc distinguished lecturer Hongxu Yang received the bachelor degree from Dalian Jiaotong University in 2014 Currently she is a master student of the School of Computer Science and Engineering Northeastern University China Her main research interests are big data management machine learning and bioinformatics Shuo Tian received the bachelor degree from Northeast Forestry University in 2013 Currently she is a master student of the Sino-Dutch Biomedical and Information Engineering School of Northeastern University Her main research interests are machine learning digital image processing and bioinformatics Ge Yu received the BE and ME degrees in computer science from Northeastern University of China in 1982 and 1986 respectively PhD degree in computer science from Kyushu University of Japan in 1996 He has been a professor with Northeastern University of China since 1996 He served as a PI or co-PI for more than twenty national research grants from NSFC the 863 Program the 973 Program etc He has published more than 200 research papers His research interesting includes database theory and technology distributed and parallel systems big data and cloud computing cyber-physical systems He is a CCF fellow a member of IEEE and ACM Chenren Xu received the BE degree from Shanghai University in 2008 MS degree in applied mathematical statistics and PhD degree in electrical and computer engineering from the Rutgers University in 2014 He was a postdoctoral research fellow in the School of Computer Science Carnegie Mellon University from 2014 to 2015 He is currently an assistant professor in the School of Electronics Engineering and Computer Science and also a member of the Center for Energy-ef\002cient Computing and Applications CECA Peking University He was the receipt of Best Poster Award of ACM SenSys in 2011 Gold Medal of Samsung Best Paper Award Competition and Best Paper Award Nominee Award of ACM UbiComp in 2014 His current research interests are in wireless networks mobile systems and IoT health 


S Din et al  Cluster-Based Data Fusion Technique to Analyze Big Data    M Ye C Li G Chen and J Wu An energy ef\034cient clustering scheme in wireless sensor networks Adhoc Sensor Wireless Netw  vol 3 pp 99\025119 2007   M Sharma and K Sharma An energy ef\034cient extended LEACH 050EEE LEACH\051 in Proc Int Conf Commun Syst Netw Technol  2012 pp 377\025382   W B Heinzelman A P Chandrakasan and H Balakrishnan An application-speci\034c protocol architecture for wireless microsensor networks IEEE Trans Wireless Commun  vol 1 no 4 pp 660\025670 Oct 2002   H M Ammari and S K Das Centralized and clustered k-coverage protocols for wireless sensor networks IEEE Trans Comput  vol 61 no 1 pp 118\025133 Jan 2012   T Murata and H Ishibuchi Performance evaluation of genetic algorithms for 035owshop scheduling problems in Proc 1st IEEE Conf Evol Comput IEEE World Congr Comput Intell  Jun 1994 pp 812\025817   M Yu K K Leung and A Malvankar A dynamic clustering and energy ef\034cient routing technique for sensor networks IEEE Trans Wireless Commun  vol 6 no 8 pp 3069\0253079 Aug 2007   S Jabbar A E Butt N U Sahar and A A Minhas Threshold based load balancing protocol for energy ef\034cient routing in WSN in Proc 13th Int Conf Adv Commun Technol 050ICACT\051  2011 pp 196\025201   S Sharma Y Shi Y T Hou and S Kompella An optimal algorithm for relay node assignment in cooperative ad hoc networks IEEE/ACM Trans Netw  vol 19 no 3 pp 879\025892 Jun 2011   D Bol et al  Green SoCs for a sustainable Internet-of-Things in Proc IEEE Faible Tension Faible Consommation 050FTFC\051  Jun 2013 pp 1\0254   T Han and N Ansari Heuristic relay assignments for green relay assisted device to device communications in Proc IEEE Global Commun Conf 050GLOBECOM\051  Dec 2013 pp 468\025473   S Abdullah and K Yang An energy-ef\034cient message scheduling algorithm in Internet of Things environment in Proc 9th Int Wireless Commun Mobile Comput Conf 050IWCMC\051  2013 pp 311\025316   050May 4 2016\051 MICAz Wireless Measurement System  Available http://www.openautomation.net/uploadsproductos/micaz_ datasheet.pdf  Service Requirements for Machine-Type Communications  3GPP TS 22.368 V11.0.0 Dec 2010   A Ahmad A Paul and M M Rathore An ef\034cient divide-and-conquer approach for big data analytics in machine-to-machine communication Neurocomputing  vol 174 pp 439\025453 Jan 2016   E F Nakamura A A Loureiro and A C Loureiro Information fusion for wireless sensor networks Methods models and classi\034cations ACM Comput Surv  vol 39 no 3 p 9 2007   A Ahmad A Paul M M Rathore and H Chang Smart cyber society Integration of capillary devices with high usability based on cyber\025physical system Future Generat Comput Syst  vol 56 pp 493\025503 Mar 2016   A Reiss and D Stricker Introducing a new benchmarked dataset for activity monitoring in Proc 16th Int Symp Wearable Comput 050ISWC\051  2012 pp 108\025109   A Reiss and D Stricker Creating and benchmarking a new dataset for physical activity monitoring in Proc 5th Int Conf Pervas Technol Rel Assist Environ  2012 p 40   O Banos et al  mHealthDroid a novel framework for agile development of mobile health applications in International Workshop on Ambient Assisted Living 050Lecture Notes in Computer Science\051 Cham Switzerland Springer International Publishing 2014 vol 8868 pp 91\02598 SADIA DIN received the bachelor's degree in computer engineering from the COMSATS Institute of Information Technology Abbottabad Pakistan She is currently pursuing the master's leading to Ph.D degree with Kyungpook National University Daegu South Korea Her research interests include Internet of Things big data analytics wireless sensor network and 5G/4G AWAIS AHMAD received the Ph.D degree in computer science and engineering from Kyungpook National University Daegu South Korea He is currently an Assistant Professor 050Research Professor\051 with the Department of Information and Communication Engineering Yeungnam University South Korea He has authored or co-authored more than 50 research papers 050journals and conferences\051 and also several book chapters related to big data and Internet of Things His research interest includes big data Internet of Things social Internet of Things and human behavior analysis using big data He received three prestigious awards including the Research Award from the President of Bahria University Islamabad Pakistan in 2011 the Best Paper Nomination Award in WCECS 2011 at UCLA USA and the Best Paper Award in 034rst Symposium on CS  E Moju Resort South Korea in 2013 His serves as a Guest Editor in various Elsevier and Springer journals He is an invited Reviewer in the IEEE C OMMUNICATION L ETTERS  the IEEE J OURNAL of S ELECTED T OPICS in A PPLIED E ARTH O BSERVATIONS and R EMOTE S ENSING  the IEEE T RANSACTIONS on I NTELLIGENT T RANSPORTATION S YSTEMS  and several other IEEE and Elsevier journals ANAND PAUL 050SM'15\051 received the Ph.D degree in electrical engineering from National Cheng Kung University Tainan Taiwan in 2010 He is currently an Associate Professor with the School of Computer Science and Engineering Kyungpook National University Daegu South Korea He is also a Delegate representing Korea for M2M Focus Group and for MPEG His research interests include algorithm and architecture re-con\034gurable embedded computing He has Guest Edited various international journals and he is also a part of the Editorial Team for the Journal of Platform Technology and Cyber Physical Systems  He serves as a Reviewer for various IEEE/IET journals He is the Track Chair for smart human computer interaction in ACMSAC 2015 and 2014 He received the Outstanding International Student Scholarship Award in 2004 and 2010 the Best Paper Award in National Computer Symposium Taipei Taiwan in 2009 and the UWSS 2015 in Beijing China MUHAMMAD MAZHAR ULLAH RATHORE received the master's degree in computer and communication security from the National University of Sciences and Technology Islamabad Pakistan in 2012 He is currently pursuing the Ph.D degree with Kyungpook National University Daegu South Korea His research interests include Internet of Things big data analytics network traf\034c analysis and monitoring intrusion detection and computer and network security GWANGGIL JEON received the B.S M.S and Ph.D 050 summa cum laude 051 degrees from the Department of Electronics and Computer Engineering Hanyang University Seoul South Korea in 2003 2005 and 2008 respectively He was with the Department of Electronics and Computer Engineering Hanyang University from 2008 to 2009 From 2009 to 2011 he was with the School of Information Technology and Engineering University of Ottawa Ottawa ON Canada as a PostDoctoral Fellow From 2011 to 2012 he was with the Graduate School of Science and Technology Niigata University Niigata Japan as an Assistant Professor He is currently an Associate Professor with the Department of Embedded Systems Engineering Incheon National University Incheon South Korea His current research interests include image processing particularly image compression motion estimation demosaicking and image enhancement and computational intelligence such as fuzzy and rough sets theories He received the IEEE Chester Sall Award in 2007 and the ETRI Journal Paper Award in 2008 VOLUME 5 2017 5083 


TIAN et al  ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3091 optimal coverage control issue and potential failure of local communication links R EFERENCES  L  F  B ertuccelli and J  P  H o w   UAV search for dynamic targets with uncertain motion models in Proc 45th IEEE Conf Decision Control  Dec 2006 pp 59415946  P  T  M illet D  W  Cas b eer  T  M erck er  a nd J  Bis hop Multiagent decentralized search of a probability map with communication constraints in Proc AIAA Guid Navigat Control Conf  Aug 2010 pp 25 3 J  H u L  X i e K  Y  L u m  and J  X u M ultiagent inf o r m ation f us ion a nd cooperative control in target search IEEE Trans Control Syst Technol  vol 21 no 4 pp 12231235 Jul 2013  D  P  B erts ekas   A n e w clas s o f i ncrem e ntal gradient m e thods for l eas t squares problems SIAM J Optim  vol 7 no 4 pp 913926 1996  M  G  R abbat a nd R D No w a k Quantized increm ental a lgorithm s for distributed optimization IEEE J Sel Areas Commun  vol 23 no 4 pp 798808 Apr 2005  C  G  L opes a nd A H Sayed Incre mental adaptive strategies over distributed networks IEEE Trans Signal Process  vol 55 no 8 pp 40644077 Aug 2007 7 X  L in  A s chem e f or r o b u s t dis t r i b u ted s ens o r f us ion b as ed on average consensus in Proc Int Conf Inf Sensor Netw  Apr 2005 pp 6370 8 I  D  S chizas  G  M ateos  and G  B  G iannakis  D is tr ib uted L M S f or consensus-based in-network adaptive processing IEEE Trans Signal Process  vol 57 no 6 pp 23652382 Jun 2009 9 S  S  S tank o v ic M  S  S tank o v ic a nd D  M  S tipano v ic  D ecentr alized parameter estimation by consensus based stochastic approximation IEEE Trans Autom Control  vol 56 no 3 pp 531543 Mar 2011  C G L opes a nd A H Sayed Dif fusion least-mean squares over adaptive networks Formulation and performance analysis IEEE Trans Signal Process  vol 56 no 7 pp 31223136 Jul 2008  F  S  Catti v e lli and A  H  S ayed  D i f f us ion L M S s t r a te gies f o r distributed estimation IEEE Trans Signal Process  vol 58 no 3 pp 10351048 Mar 2010  J  Chen and A  H  S ayed  Dif f us ion a daptation s trate g ies f or dis t rib u ted optimization and learning over networks IEEE Trans Signal Process  vol 60 no 8 pp 42894305 Aug 2012  A H Sayed S Y  T u J  Chen X Zhao,andZ.J.Towc,Diffusion strategies for adaptation and learning over networks An examination of distributed strategies and network behavior IEEE Signal Process Mag  vol 30 no 3 pp 155171 May 2013  J  Chen C  R ichar d  A  O  H er o and A  H  S ayed  D i f f us ion L M S f o r multitask problems with overlapping hypothesis subspaces in Proc IEEE Int Workshop Mach Learn Signal Process MLSP  Sep 2014 pp 16  J  Chen S  Y  T u  a nd A  H  S a yed D is tr ib uted optim ization v ia diffusion adaptation in Proc 4th IEEE Int Workshop Comput Adv Multi-Sensor Adapt Process CAMSAP  Dec 2011 pp 281284  J  Chen and A  H  S ayed  On the b enets of dif f us ion c ooperation f or distributed optimization and learning in Proc 21st Eur Signal Process Conf EUSIPCO  Sep 2013 pp 15  Z  J  T o wc a nd A H Sayed  A da ptive penalty-based distributed stochastic convex optimization IEEE Trans Signal Process  vol 62 no 15 pp 39243938 Aug 2014  A H Sayed  A dapti v e n etw o rks   Proc IEEE  vol 102 no 4 pp 460497 Apr 2014  X Z h ao and A  H  S ayed  As ynchronous adaptation a nd learning o v e r networksPart I Modeling and stability analysis IEEE Trans Signal Process  vol 63 no 4 pp 811826 Dec 2014  A Nedic A Ozdaglar  a nd P  A Parrilo Constrained consensus and optimization in multi-agent networks IEEE Trans Autom Control  vol 55 no 4 pp 922938 Apr 2010  M Z h u a nd S Martinez On d is tr ibuted convex optimization under inequality and equality constraints IEEE Trans Autom Control  vol 57 no 1 pp 151164 Jan 2012  J  L i n Di v er gence m eas ures bas e d o n t he Shannon entrop y   IEEE Trans Inf Theory  vol 37 no 1 pp 145151 Jan 1991  S Oh L  S chenato P  Chen a nd S Sas t ry   T r acking a nd coordination of multiple agents using sensor networks System design algorithms and experiments Proc IEEE  vol 95 no 1 pp 234254 Jan 2007  P  D a m e s a nd V  K u m a r  Cooperati v e m u lti-tar g et localization w ith noisy sensors in Proc EEE Int Conf Robot Autom ICRA  May 2013 pp 18771883  C Manning and H  S chtze Foundations of Statistical Natural Language Processing  Cambridge MA USA MIT Press 1999  C E  Shannon  A m a them atical theory of com m unication  Bell Syst Tech J  vol 27 no 3 pp 379423 1948  F  Niels e n a nd R Nock Skew Jensen-Bregman Voronoi Diagrams  Berlin Germany Springer 2011  H W  K uhn and A  W  T uck e r  Nonlinear program m i ng  i n Proc 2nd Berkeley Symp Math Statist Probab  Jul 1950 pp 481492  M H e s t enes   Multiplier a nd gradient m e thods   J Optim Theory Appl  vol 4 no 5 pp 303320 1969 A v a ilable http://dx.doi.org/10.1007/BF00927673  M  J  D  P o w e ll  A m e thod f o r nonlinear cons tr aints i n m inim ization problems in Optimization Symposium of the Institute of Mathematics and Its Applications University of Keele  London U.K Academic 1969 pp 283298  M  P o w e ll O n s ear ch dir ections f o r m inim ization a lgor ithm s   Math Program  vol 4 no 1 pp 193201 1973 A v a ilable http://dx.doi.org/10.1007/BF01584660  J  E  F a lk  L a grange m u ltipliers and nonlinear program m i ng  J Math Anal Appl  vol 19 no 1 pp 141159 1967  G He  Ros e n s g radient p rojection w ith dis c rete s t eps   Acta Math Appl Sin  vol 6 no 1 pp 110 1990 A v a ilable http://dx.doi.org/10.1007/BF02014710  J  Ros e n T he gradient projec tion method for nonlinear programming Part I Linear constraints J Soc Ind Appl Math  vol 8 no 1 pp 181217 1960 O A v a ilable http://dx doi o r g 10 1137/0108011  W  Rudin Principles of Mathematical Analysis vol.3.NewYork,NY USA McGraw-Hill 1964  J  Cortes  S  M artinez T  Karatas  and F  B ullo  Co v e rage control f or mobile sensing networks IEEE Trans Robot Autom  vol 20 no 2 pp 243255 Apr 2004  A T  Y  Ster giopoulos   Spatially di stributed area coverage optimisation in mobile robotic networks with arbitrary convex anisotropic patterns Automatica  vol 49 no 1 pp 232237 Jan 2013  S Cheng G L i u L  Feng Y  W a ng and Q  G ao  Pers is tent a w arenes s coverage control for mobile sensor networks Automatica  vol 49 no 6 pp 18671873 Jan 2013  R Durre t t  Probability Theory and Examples  Cambridge U.K Cambridge Univ Press 2011 Daxin Tian M13SM16 is an Associate Professor in the School of Transportation Science and Engineering Beihang University Beijing China His current research interests include mobile computing intelligent transportation systems vehicular ad hoc networks and swarm intelligent Jianshan Zhou received the B.Sc and M.Sc degrees in trafc information engineering and control in 2013 an d 2016 respectively He is currently working toward the Ph.D degree with the School of Transportation Science and Engineering Beihang University Beijing China His current research interests are focused on wireless communication articial intelligent system and intelligent transportation systems Zhengguo Sheng is currently a Lecturer with the Department of Engineering and Design University of Sussex U.K He has authored over 50 international conference and journal papers His current research interests cover IoT/M2M vehicular communications a nd edge/cloud computing 


2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2680460, I\EEE Transactions on Big Data A 17 for images In this practical based in images  Ho w the feature selection w ould impact the classi\002cation reported  More adv anced feature selection approaches such as the in can be introduced into the frame w ork e images samin images challenge to problem  Man y dif ferent approaches ha v e been proposed to address as results the are Ho we v er  none of these that most eloping MRs 12 Metamorphic testing w as 002rst Chen al  for testing non-testable systems bioinformatics sysaultrelations A recent has compilers  Metamorphic testing has been applied for testi ng a lar ge ASA and also successfully engines Baidu Ho we v er  the quality of reported are w information In this paper  metamorphic in results w are tests xity SUT  Combinatorial technique 53 used for testing softw are for are C N the learning were to classi\002cation the for confusion learning important it data our data techniques A T King and xperi#1262933 e Corporation research R S  V  Gudi v ada R Raeza-Y ates and V  Ragha v an 223Big data Promises and 224 Computer 2015  Y  Bengio 223Learning deep architectures for ai 224 ends Learning 2009  Apache 2016 Hadoop Online A v ailable http://hadoop.apache.or g  V  Gudi v ada D Rao and V  Ragha v an 223Renaissance in database 224 IEEE Computer 2016  J Zhang Y  Feng M S Moran J Lu L Y ang al of 224 ess 2013  R M and T  Poggio 223Models of object recognition 224 oscience 2000  K Jacobs  J Lu and X Hu 223De v elopment of a dif f raction imaging 224 Lett 2009  2016 Adda project Online A v ailable https://github com/addateam adda  T  Y  Chen S C Cheung and S Y iu 223Metamorphic testing a ne w CS98and 1998  J Ding D Zhang and X Hu 223 An application of metamorphic testing in metamorphic ICSE 2016  U Kane w ala and J M Bieman 223T esting scienti\002c softw are A system\224 gy 56 2014  S Se gura G Fraser  A Sanchez and A Ruiz-Cort 264 on 224 Engineering  2016  2016 Mongodb  Online A v ailable https://www mongodb com  2016 Mongochef Online A v ailable http://3t.io/mongochef  M Y urkin and A Hoekstra 2014 User manual for the discrete 1.3b4 A v ailable https team/adda/tree/master/doc  C Hsu C.-C Chang and C.-J Lin 223 A practical guide to support v ector 2003  Y  LeCun Y  Bengio and G Hinton 223Deep learning 224 e 521 2015  R Haralick 223On a te xture-conte xt feature e xtract ion algorithm for in Society ol 650\226 657 
 


2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2680460, I\EEE Transactions on Big Data A 18  K Dong Y  Feng K Jacobs J Lu R Brock al 223Label-free 224 Biomed ess 2011  R M Haralick K Shanmug an and I H Dinstein 223T e xtural features 224 Cybern SMC-3 1973  S K Thati J Ding D Zhang and X Hu 223Feature selection and analin ion ance 2015  J Dixon and J Ding 223 An empirical study of parallel solution for glcm 2016  T  Kanungo D Mount N Netan yahu C Piatk o R Silv erman and im\224 hine ence 2012  M A Hall 223Correlation-based feat ure selection for machine learning 224 wzealand 1999  A Krizhe vsk y  I Sutsk e v er  and G E Hinton 223Imagenet classi\002cain al systems and 1097\2261105  E Gibne y  223Google ai algorithm masters ancient g ame of go 224 e   M Moran 223Correlating the morphological and light scattering prop 2013  R P an Y  Feng Y  Sa J Lu K Jacobs and X Hu 223 Analysis 224 ess  2014  X Y ang Y  Feng Y  Liu N Zhang L W ang al e fraction 224 ess 7 2014  M Zhang 223 A deep learning based classi\002cation of lar ge scale biomed2016  Y  Feng N Zhang K Jacobs W  Jiang L Y ang al 223Polarization w 224 A 2014  C.-C Chang and C.-J Lin 2016 Libsvm Online A v ailable csie.ntu.edu.tw 030 cjlin/libsvm  2016 Caf fe project Online A v ailable http://caf fe.berk ele yvision.or g  J Mayer and R  Guderlei 223 An empirical study on the selection of good in e C06 475\226484  U Kane w ala J M Bieman and A Ben-Hur  223Predicting metamorphic approach 224 and Reliability 2015  J Ding T  W u J Q Lu and X Hu 223Self-check ed metamorphic testing in on vement apore 2010  W  E W ong and A Mathur  223Reducing the cost of mutati on testing 224 e pp 1995  Y  Jia and M Harman 223 An anal ysis and surv e y of the de v elopment of 224  649\226678 2011  L Cai and Y  Zhu 223The challenges of data quality and data quality 224 ournal 1 2015  J Gao C Xi e and C T ao 223Big data v alidation and quality assurance in Service\(SOSE 433\226441  X Dong E Gabrilo vich K Murph y  V  Dang W  Horn C Lug aresi 224  938\226949 2015  X Y in J Ha n and P  S Y u 223T ruth disco v ery with multiple con\003icting 224 Data  2008  C H W u and Y  Song 223Rob ust and distrib uted web-scale near dup in IEEE Data 2606\226 2611  2016 Apache samza Online A v ailable http://samza.apache.or g  J A Saez B Kra wczyk and M W ozniak 223On the in\003uence of class 002ltering 224 ence 590\226609 2016  M Y ousef D S D M 250 223Feature for 224 bioinformatics 2016  F  Min Q Hu and W  Zhu 223Feature sel ection with test cost constraint 224 Reasoning 167\226 2014  H A L Thi H M Le and T  P  Dinh 223Feature selection in machine function 224 Learning 2015  H Liu F C K uo D T o we y  and T  Chen 223Ho w ef fecti v ely does problem?\224 on Engineering 2014  V  Le M  Afshari and Z Su 223Compiler v alidation via equi v alence in amming on Kingdom 216\226226  M Lindv all D Ganesan R rdal and R E W ie g and 223Metamorphic in 37th Engineering 129\226 138  Z Zhou S Xiang and T  Chen 223Metamorphic testing for softw are 224 e Engineering 2016  C Nie and H Leung 223 A surv e y of combinatorial testing 224 CM y 2011 CE O HERE Ding Computer has Computer in Nanjing 2004 r ed His the He by CM Hu  ada East  
 


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   19     en-US  en-US  en-US  en-US 52 en-US  en-US  en-US  en-US en-US e en-US ti en-US en-US  en-US  en-US en-US  en-US 4 en-US en-US  en-US  en-US  en-US 53 en-US  en-US  en-US  en-US en-US  en-US  en-US DA en-US en-US  en-US  en-US  en-US  en-US 54 en-US  en-US  en-US  en-US en-US e en-US n en-US  en-US en-US  en-US v en-US en-US  en-US  en-US  en-US 55 en-US  en-US  en-US  en-US en-US k en-US en-US thm en-US en-US  en-US ron en-US  en-US 0 en-US en-US  en-US  en-US 5 en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US n en-US  en-US  en-US  en-US 57 en-US  en-US  en-US  en-US en-US ti en-US en-US T en-US  en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 58 en-US  en-US  en-US  en-US en-US  en-US Pre en-US en-US  en-US t en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 3 en-US en-US 1 en-US  en-US  en-US  en-US 59 en-US  en-US  en-US t en-US  en-US  en-US n en-US en-US  en-US  en-US en-US  en-US OS en-US  en-US 2 en-US en-US  en-US  en-US  en-US 60 en-US  en-US  en-US  en-US en-US  en-US ti en-US  en-US t en-US en-US  en-US 4 en-US en-US  en-US  en-US  en-US 61 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US ess en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 62 en-US  en-US  en-US  en-US en-US X en-US en-US ng en-US en-US s en-US  en-US en-US  en-US  en-US i    en-US x en-US en-US e en-US en-US i en-US en-US r en-US en-US is en-US en-US 2 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 63 en-US  en-US  en-US i en-US  en-US en-US  en-US  en-US en-US h en-US i   Av a i l a bl e   en-US e en-US en-US is en-US en-US ng en-US en-US a en-US en-US nd en-US en-US b en-US en-US r en-US en-US the en-US en-US net en-US en-US of en-US en-US 0 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 64 en-US  en-US  en-US BI en-US en-US  en-US en-US e en-US  en-US  en-US en-US er en-US  en-US i    en-US du en-US en-US es en-US en-US new en-US en-US ai en-US en-US nd en-US en-US net en-US en-US of en-US en-US s en-US en-US ves en-US en-US 6 en-US en-US 0 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 65 en-US  en-US  en-US  en-US en-US Su en-US  en-US  en-US en-US a en-US  en-US 9 en-US en-US  en-US  en-US  en-US  en-US 66 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US s en-US  en-US  en-US  en-US  en-US 67 en-US  en-US  en-US  en-US en-US  en-US a en-US en-US ve en-US en-US a en-US  en-US 383 en-US en-US  en-US  en-US  en-US 68 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US ess en-US  en-US 7 en-US en-US  en-US  en-US  en-US 69 en-US  en-US  en-US  en-US en-US  en-US vey en-US en-US  en-US ri en-US s en-US  en-US 77 en-US en-US  en-US  en-US  en-US 70 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 71 en-US  en-US  en-US o en-US  en-US  en-US ne en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 72 en-US  en-US  en-US  en-US en-US  en-US n en-US e en-US  en-US en-US  en-US n en-US  en-US 6 en-US en-US  en-US  en-US  en-US 73 en-US  en-US  en-US  en-US en-US  en-US  en-US hy en-US  en-US en-US  en-US n en-US  en-US 6 en-US en-US  en-US  en-US  en-US 74 en-US  en-US  en-US  en-US en-US  en-US t en-US en-US I en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 75 en-US  en-US  en-US  en-US en-US  en-US ty en-US en-US  en-US en-US  en-US f en-US en-US  en-US  en-US  en-US  en-US 76 en-US  en-US  en-US a en-US  en-US en-US ti en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 96 en-US en-US  en-US  en-US  en-US 77 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US 0 en-US 8 en-US en-US  en-US  en-US ess en-US  en-US 85 en-US en-US  en-US  en-US  en-US 78 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 79 en-US  en-US  en-US hen en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 80 en-US  en-US  en-US  en-US en-US ng en-US en-US  en-US  en-US  en-US en-US s en-US  en-US  en-US 6 en-US en-US  en-US  en-US  en-US 81 en-US  en-US  en-US N en-US  en-US  en-US en-US  en-US  en-US en-US 2011 en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 82 en-US  en-US  en-US  en-US  en-US en-US o en-US  en-US  en-US en-US 2011 en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 83 en-US  en-US  en-US  en-US en-US  en-US s en-US  en-US  en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 84 en-US  en-US  en-US  en-US en-US  en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  20      en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 85 en-US  en-US  en-US F en-US  en-US en-US  en-US  en-US en-US  en-US 0 en-US  en-US 5 en-US en-US  en-US  en-US  en-US 86 en-US  en-US  en-US h en-US  en-US  en-US l en-US en-US  en-US en-US  en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 87 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US  en-US d en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 88 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US 6 en-US  en-US 263 en-US en-US  en-US  en-US  en-US 89 en-US  en-US  en-US  en-US en-US tem en-US en-US OTA en-US  en-US 6 en-US  en-US 6 en-US en-US  en-US  en-US  en-US  en-US 90 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US m en-US  en-US 87 en-US en-US  en-US  en-US  en-US  en-US 91 en-US  en-US  en-US e en-US  en-US  en-US en-US  en-US  en-US en-US  en-US m en-US  en-US 87 en-US en-US  en-US  en-US  en-US  en-US 92 en-US  en-US  en-US  en-US en-US  en-US a en-US  en-US  en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 93 en-US  en-US  en-US  en-US en-US  en-US r en-US en-US  en-US  en-US en-US  en-US 195 en-US en-US  en-US  en-US  en-US 94 en-US  en-US  en-US Wei en-US en-US xi en-US ng en-US en-US eng en-US en-US  en-US en-US  en-US k en-US en-US to en-US en-US  en-US  en-US en-US ess en-US  en-US 1 en-US en-US  en-US  en-US  en-US 95 en-US  en-US  en-US  en-US en-US  en-US c en-US r en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 96 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US r en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 97 en-US  en-US  en-US  en-US  en-US  en-US en-US e en-US en-US  en-US r en-US en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US  en-US m en-US en-US s en-US en-US 3 en-US  en-US 3 en-US en-US  en-US  en-US  en-US 98 en-US  en-US  en-US a en-US en-US  en-US en-US  en-US ter en-US en-US  en-US  en-US en-US l en-US  en-US  en-US  en-US  en-US 99 en-US  en-US  en-US a en-US  en-US en-US  en-US  en-US  en-US en-US  en-US BE en-US  en-US 1 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US e en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US e en-US en-US  en-US ter en-US en-US  en-US  en-US  en-US  en-US 2 en-US  en-US  en-US e en-US en-US  en-US ter en-US en-US the en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US  en-US ti en-US en-US n en-US en-US  en-US n en-US  en-US 3 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US th en-US  en-US  en-US ter en-US en-US  en-US  en-US en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US en en-US en-US  en-US n en-US  en-US  en-US en-US 4 en-US  en-US 4 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US e en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US  en-US tem en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US  en-US en-US  en-US a en-US en-US  en-US en-US  en-US S en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US  en-US en-US ter en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 72 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US n en-US en-US a en-US  en-US 5 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US W en-US  en-US en-US  en-US  en-US nty en-US en-US  en-US ON en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US Se en-US  en-US en-US  en-US l en-US  en-US 334 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US Int en-US en-US y en-US  en-US  en-US  en-US 249 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US d en-US en-US  en-US 1 en-US 1 en-US 52 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US te en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US en-US r en-US  en-US 34 en-US en-US  en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   21     en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US s en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US 77 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US d en-US en-US a en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US o en-US  en-US en-US  en-US  en-US en-US s en-US  en-US 0 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US s en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US a en-US  en-US 321 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US K en-US en-US t en-US en-US  en-US en-US  en-US  en-US s en-US en-US dy en-US en-US matics en-US  en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US res en-US  en-US  en-US  en-US 9 en-US  en-US  en-US R en-US n en-US en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US n en-US en-US  en-US en-US n en-US  en-US  en-US  en-US en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US man en-US en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US d en-US en-US s en-US 2 en-US  en-US 46 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ti en-US en-US ti en-US en-US  en-US n en-US en-US n en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US n en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US en-US 2 en-US  en-US ron en-US  en-US 351 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US Semi en-US en-US  en-US  en-US en-US  en-US S en-US s en-US en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US ex en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US nt en-US en-US  en-US en-US  en-US  en-US a en-US  en-US en-US  en-US r en-US  en-US 7 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US EEE en-US  en-US 6 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US n en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ene en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US 4 en-US  en-US  en-US en-US  en-US ti en-US en-US  en-US en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US dy en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US  en-US en-US IE en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US n en-US en-US  en-US l en-US en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US  en-US n en-US en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US the en-US  en-US en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  22      en-US Inte en-US  en-US  en-US 132 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US n en-US en-US  en-US I en-US en-US  en-US  en-US  en-US 69 en-US en-US 8 en-US  en-US  en-US  en-US 2 en-US  en-US  en-US r en-US en-US  en-US en-US k en-US en-US to en-US en-US  en-US  en-US en-US r en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US t en-US en-US n en-US en-US n en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US i   en-US m en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US tr en-US  en-US n en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US a en-US i   en-US d en-US en-US a en-US en-US c en-US en-US es en-US 2 en-US en-US n en-US en-US 7  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US dy en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US en-US  en-US vey en-US en-US  en-US  en-US 13 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US  en-US n en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US 2010 en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US o en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US B en-US  en-US en-US A en-US ti en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 6 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US e en-US en-US s en-US  en-US  en-US  en-US en-US  en-US n en-US  en-US 1 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US f en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US e en-US en-US  en-US  en-US  en-US 1 en-US 68 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US f en-US en-US  en-US ew en-US en-US e en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US n en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US h en-US en-US er en-US i   Av a i l a bl e   en-US p en-US en-US ten en-US en-US hn en-US y en-US en-US ends en-US en-US l en-US en-US the en-US en-US l en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US en-US re en-US  en-US 6 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US y en-US en-US  en-US e en-US en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US en-US re en-US  en-US 6 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US en-US t en-US  en-US earn en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 34 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US Lef en-US  en-US en-US  en-US  en-US ti en-US en-US  en-US en-US V en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US Pro en-US  en-US  en-US  en-US 34 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US ng en-US en-US ti en-US en-US  en-US  en-US  en-US en-US  en-US 1 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US O en-US en-US M en-US  en-US en-US l en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US N en-US s en-US  en-US en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US to en-US en-US  en-US en-US n en-US  en-US  en-US  en-US 5 en-US  en-US  en-US D en-US  en-US  en-US en-US nt en-US en-US  en-US en-US  en-US en-US  en-US  en-US  en-US  en-US 6 en-US en-US  en-US 2 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US edes en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US t en-US  en-US en-US  en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   23     en-US  en-US 22 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US t en-US en-US  en-US vey en-US en-US  en-US  en-US ne en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US  en-US tbed en-US en-US  en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US G en-US  en-US en-US o en-US en-US  en-US en-US  en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US e en-US en-US ent en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US a en-US en-US z en-US en-US  en-US dez en-US en-US z en-US en-US  en-US en-US  en-US a en-US  en-US  en-US en-US  en-US 26 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US a en-US en-US a en-US en-US o en-US en-US  en-US a en-US en-US  en-US en-US o en-US en-US F en-US en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US 2015 en-US b en-US 5 en-US  en-US 9 en-US en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US o en-US en-US  en-US en-US  en-US  en-US en-US n en-US  en-US  en-US 45 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US k en-US  en-US  en-US en-US  en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US r en-US  en-US 86 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US y en-US en-US by en-US en-US  en-US en-US  en-US s en-US  en-US 1 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US  en-US dez en-US en-US  en-US en-US n en-US en-US  en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US tem en-US en-US  en-US ti en-US en-US ve en-US en-US  en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US ty en-US en-US  en-US en-US  en-US  en-US  en-US 5 en-US  en-US 2 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US en en-US en-US  en-US  en-US  en-US en-US ess en-US  en-US 831 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US ne en-US en-US  en-US e en-US en-US  en-US  en-US 26 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US ve en-US  en-US  en-US en-US  en-US rk en-US  en-US 7 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US Sy en-US  en-US  en-US en-US  en-US  en-US  en-US es en-US  en-US 1 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US  en-US  en-US r en-US rks en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US n en-US  en-US  en-US 9 en-US 0 en-US  en-US 411 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US tems en-US en-US  en-US  en-US  en-US  en-US n en-US  en-US en-US  en-US v en-US es en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US en en-US en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US m en-US en-US  en-US  en-US th en-US en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ng en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US  en-US rk en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US t en-US dy en-US en-US n en-US s en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  24      en-US  en-US i en-US  en-US en-US  en-US  en-US 9 en-US  en-US 2 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US n en-US  en-US t en-US 7 en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US E en-US ess en-US  en-US 858 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US ess en-US  en-US 1 en-US en-US  en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US  en-US  en-US o en-US en-US  en-US en-US in en-US  en-US  en-US  en-US l en-US en-US  en-US g en-US  en-US 0 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US Ben en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US f en-US  en-US  en-US dy en-US en-US  en-US 11 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US  en-US en-US T en-US en-US  en-US o en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US z en-US en-US  en-US en-US ti en-US en-US  en-US  en-US  en-US en-US sort en-US  en-US  en-US 5 en-US  en-US  en-US  en-US 8 en-US  en-US  en-US z en-US en-US  en-US en-US  en-US f en-US en-US the en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 121 en-US en-US  en-US  en-US  


