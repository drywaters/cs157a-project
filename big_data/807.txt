Data System Design for a Hyperspectral Imaging Mission Concept   Christine M Hartzell Jennifer Carpena-Nunez Aerospace Engineering Sciences Department of Physics University of Colorado University of Puerto Rico Boulder CO 80309 San Juan PR 00931 206 915-6409 787 381-4171 hartzelc@colorado.edu jennifercarpena@gmail.com Lindley C Graham David M Racek Department of Aeronautics and Astronautics Electrical and Computer Engineering Department Massachusetts Institute of Technology Montana State University Cambridge MA 02139 Bozeman MT 59717 408 306-9247 253 205-4538 lcgraham@mit.edu 
dave.racek@gmail.com Tony S Tao Christianna E Taylor Aerospace Engineering School of Aerospace Engineering Pennsylvania State University Georgia Institute of Technology University Park PA 16802 Atlanta GA 30332 610 306-2761 617 669-6774 tonystao@gmail.com christianna.taylor@gmail.com Hannah R Goldberg Charles D Norton Jet Propulsion Lab Jet Propulsion Lab California Institute of Technology California Institute of Technology Pasadena CA 91109 Pasadena CA 91109 818 216-6856 818 793-3775 hannah.r.goldberg@jpl.nasa.gov charles.d.norton@jpl.nasa.gov Abstract\227 
Global ecosystem observations are important for Earth-system studies The National Research Council's report entitled Earth Science and Applications from Space is currently guiding NASA's Earth science missions It calls for a global land and coastal area mapping mission The mission scheduled to launch in the 2013-2016 timeframe includes a hyperspectral imager and a multi-spectral thermal-infrared sensor These instruments will enable scientists to characterize global species composition and monitor the response of ecosystems to disturbance events such as drought 003ooding and volcanic events Due to the nature and resolution of the sensors these two instruments produce approximately 645 GB of raw data each day thus pushing the limits of con 
ventional data handling and telecommunications capabilities The implications of and solutions to the challenge of high downlink data volume were examined Low risk and high science return were key design values The advantages of onboard processing and advanced telecommunications methods were evaluated This paper will present an end-to-end data handling system design that will handle the large data downlink volumes that are becoming increasingly prevalent as the 978-1-4244-2622-5/09 25  00 c r 2009 IEEE IEEEAC Paper 1375 Version 4 Updated 1/09/2009 complexity of Earth science increases The designs presented 
here are the work of the authors and may differ from the current mission baseline T ABLE OF C ONTENTS 1 M ISSION O VERVIEW                               1 2 C LOUD D ETECTION T ECHNIQUES                4 3 C OMPRESSION 
S CHEMES                          5 4 C OMMUNICATIONS A RCHITECTURE              8 5 G ROUND S TATION S ELECTION                    12 6 S CIENCE A NALYSIS 
                               14 7 D ATA S TORAGE AND D ISTRIBUTION              16 8 C ONCLUSION                                      18 A CKNOWLEDGEMENTS                            
19 R EFERENCES                                      19 1 M ISSION O VERVIEW The National Research Council's NRC report entitled Earth Science and Applications from Space provides a roadmap of Earth-science missions for the next decade The HyspIRI 1 


mission expected to launch between 2013 and 2016 is called for by the NRC to monitor global ecosystem health in order to identify the ecological effects of climate change and urbanization The nominal mission duration of 3 years will allow analysis of the temporal variation in the ecological properties as well The science objectives are achieved using two imaging spectrometers that operate in different regions of the electromagnetic spectrum Due to the high resolutions of the imagers and the global mapping nature of the mission approximately 6455 gigabytes of raw data will be collected each day Science Objectives HyspIRI aims to detect ecosystem responses to climate change and human land management It intends to monitor and enable prediction of volcanic activity landslide hazards wild\002re susceptibility drought exploitation of natural resources species invasion surface and soil alterations changes in surface temperature deforestation and ecosystem disturbance These can be detected by distinctive spectral features in the shortwave visible near infrared and infrared regions of the electromagnetic spectrum Imager Characteristics The HyspIRI mission carries two imaging spectrometers the Visible Shortwave Infra Red VSWIR hyperspectral imaging spectrometer and a Thermal Infra Red TIR multispectral imager The two instruments will address independent science objectives The VSWIR imager will capture sunlight re\003ected off the Earth's surface while the TIR collects thermal energy emitted by the Earth's surface The VSWIR instrument will have a 145 km wide swath width It is a nadir-pointing push-broom instrument with a spatial resolution of 60m and a repeat cycle of 19 days The TIR instrument will have a 600 km wide swath width It is also nadir-pointing however it has a shorter repeat cycle 5 days due to its large swath width The TIR instrument is a whisk-broom sensor with a spatial resolution of 60m The VSWIR instrument will collect re\003ected light at 214 spectral bands from 380nm violet to 2510nm near infrared with a 10nm spectral resolution The TIR collects data at 8 bands 7 bands between 7.3-12.1 026 m and 1 band between 3-5 026 m Due to the locations of their spectral bands in the electromagnetic spectrum the TIR will address science objectives concerning thermal events while the VSWIR will address science objectives related to surface composition and ecosystem information Mission Design Characteristics The preliminary mission design presented here was developed by JPL's TeamX TeamX creates integrated preliminary mission design concepts using a collaborative engineering environment The HyspIRI payload is sized to be approximately 145 kg A commercial bus the SA-200HP bus from General Dynamics will be modi\002ed for use on the HyspIRI mission Notable characteristics of this bus include upgrade options for GPS and the ability to have 3-axis pointing with 16 arcsec pointing control 0.5 arcsec knowledge and 0.1 arcsec/sec stability The bus should provide 2,000 W at end-of-life EOL at 1 AU and has enough onboard fuel capacity for a 6-year mission The launch vehicle is assumed to be a Taurus-class rocket Based on the last TeamX study conducted in July 2007 the HyspIRI satellite will orbit at an altitude of 623 km in a circular sun-synchronous orbit which translates into an inclination of 97.0 degrees The l ocal descending node of the orbit would be 11am According to the mission scientists with this local descending node time there will be the fewest clouds statistically obstructing the sensors view of the Earth Missions Operations A selective instrument resolution scheme was created to reduce the mission's data volume while retaining image detail in more scienti\002cally-signi\002cant areas By working with the mission scientists Robert Green and Simon Hook at JPL geographic areas where the data resolution could be reduced were identi\002ed Since the TIR measures the thermal energy radiated by the earth it will operate during the entire orbit regardless of the position of the sun If the sensor is taking measurements of the ocean the resolution will be reduced by averaging the data over multiple pixels The spatial resolution over the ice sheet in Antarctica is also reduced to the Ocean resolution level According to Hook 200 m x 200 m would be a desirable resolution for the TIR over the ocean Leastsigni\002cant spectral quantization data will be dropped to reduce the quantization resolution to two bits per band since exact spectral data is not as important to scientists as an overall classi\002cation and trends in the regions of lower resolution Cloud data is of interest to scientists in this spectral range so clouds will not undergo data reduction The TIR s resolution modes are enumerated in Table 1 below Using this scheme 97 of TIR data volume would be devoted to Land/Shallow Water data The VSWIR instrument's resolution modes differ from those of the TIR instrument Since the VSWIR instrument requires the re\003ected light from the sun to operate the sensor will only be active when the sun-elevation angle is suf\002cient to produce meaningful data Current sensor design puts this elevation at 20 016  Resolution modes for the VSWIR are shown below in Table 2 Clouds imaged by the VSWIR instrument are compressed spectrally since images of clouds in the VSWIR wavelengths provides little scienti\002cally usefully information Cloud pixels are not compressed spatially since the spatial unpredictability of clouds would make consistent 2 


Table 1  Spatial spectral and quantization resolutions for the TIR instrument by land classi\002cation Table 2  Spatial spectral and quantization resolutions for the VSWIR instrument by land classi\002cation spatial compression impossible Using this scheme 98 of VSWIR data volume would be devoted to Land/Shallow Water data Note that in contrast to the TIR the VSWIR keeps its full spectral and quantization resolution over Ocean 226 designated areas Data Volume Challenge Hyperspectral data is not new to earth science remote sensing missions rather what makes the HyspIRI mission unique is the amount of data being collected stored and distributed The huge data rate on the order of hundreds of gigabytes per day is due to the two instruments collecting high resolution data over a wide spectral range The spectral information in the case of the VSWIR imager is quantized into 214 bands each represented by a 14 bit sample The spectral information collected by the TIR imager is similarly quantized into 8 bands with each sample being described in 14 bits With a maximum sample rate of approximately 70Msamples/second the resulting data stream is substantial The HyspIRI mission presents a data handling challenge that will push the limits of current processing storage and communication technologies Moore's law stating that the number of transistors that can be inexpensively placed on an integrated circuit will double approximately every two years may ease the cost and technology boundaries by the mission's 2011 technology cutoff Nevertheless the downlink capabilities and on-board processing power of the HyspIRI mission limit the amount of data that can be collected and analyzed The mission utilizes hardware implementation of data processing and compression methods to reduce the stream of data coming from the instruments The on-orbit data system must be able to process and compress up to 70M samples per second in real time Different resolution modes cloud screening and lossless data compression become vital to the overall data reduction Without these data reduction methods the data volume would be well beyond the feasible volume to transmit to earth meaning a smart combination of these options must be designed and modeled The spacecraft is physically limited by the on-board solid state storage which must be able to store hundreds of gigabytes of information until the next ground station is in range The communications system will have to handle transmission of 002ve terabits of data per day yet still have a robust enough downlink margin to allow for a missed communications pass Once the data is on the ground a series of ground calibrations and processing must be done and the challenge again becomes the huge data volume to be stored The problem of making the science products readily available and accessible is not easily answered when sifting through hundreds of terabytes of data The HyspIRI mission will collect nearly two petabytes over the three year mission life Figure 1 shows the overall data reduction transferred over the communication system and the data stored on the ground over the three year mission The data reduction that is accounted for in these graphs includes reductions due to decreasing the instrument resolution when taking data over areas of little scienti\002c interest and onboard compression of data Shown on the left is the amount of data reduced on-board the spacecraft per orbit thereby reducing the volume of data that must be transmitted to the ground On the right is the total data reduction over the three year mission It can be seen that there is a substantial reduction in data volume due to the compression techniques employed and discussed in detail in this paper Figure 1  Data Reduction Graphs showing the effectiveness of the compression schemes employed by the HyspIRI mission Given the high data downlink volume a two-pronged ap3 


proach was taken in designing the mission's data system First techniques were employed to reduce the data downlink volume Once those techniques were exhausted a data system architecture was designed to accommodate the resulting data downlink volume Five topics associated with data downlink volume reduction and accommodation were investigated in further detail and presented here In attempts to reduce the data downlink volume cloud detection and hyperspectral data compression algorithms were identi\002ed In order to accommodate the resulting data downlink volume communications architectures were analyzed and ground stations were identi\002ed Additionally preliminary science products were identi\002ed and a ground-based data storage architecture was developed 2 C LOUD D ETECTION T ECHNIQUES An on-board cloud detection algorithm is required for the HyspIRI mission in order to reduce the data downlink volume While the cloudy TIR data has science value there is little bene\002t to returning cloudy data in the spectral range of the VSWIR instrument Since clouds cover nearly 30 of the globe the total mission data downlink volume can be reduced by nearly 96 GB 13 per day by applying additional compression to the cloudy pixels In order to detect clouds the raw data must 002rst be converted to the top-of-atmosphere TOA re\003ectance There are several detection algorithms that can be compared and evaluated based on the needs of the mission Finally once clouds have been detected the cloudy pixels will undergo lossy spectral and bitwise compression before being further compressed by the main compression algorithm Note that clouds will only be detected when the instrument is over land or coastal areas due to the selective resolution of the instrument A preliminary cloud detection algorithm was applied to sample hyperspectral data collected by an air-borne imaging spectrometer JPL's Airborne Visible/Infrared Imaging Spectrometer AVIRIS as a proof-ofconcept trial Preliminary On-board Processing In order to apply a cloud detection algorithm the raw VSWIR data must be converted to TOA re\003ectance First the dark level signal will be subtracted from the raw data Then the data is converted to radiance Finally the radiance is converted to TOA re\003ectance for the equivalent solar zenith angle by dividing the sensed radiance by the normal component of the solar radiance for the equivalent zenith angle see Figure 2 Similar on-board calibration was done during the Hyperion instrument's extended mission on the Earth Observing-1 satellite The equi v alent solar zenith angle is calculated on-board using the spacecraft's ephemeris data Although it is theoretically necessary to recalculate the zenith angle for each pixel it is predicted that impact of recalculating this value less frequently will be minimal Further analysis should address the implementation of this calibration process onboard the spacecraft Also it is important to note that this calibration protocol does not take into account the atmospheric scattering effects Further analysis possibly in the form of an Observing Simulated Systems Experiment OSSE could investigate the effect of this omission on the quality of the science returns Figure 2  Calculation of Re\003ectance for the Equivalent Solar Zenith Angle  022 z  Re\003ectance is the sensed radiance divided by the normal component of the solar radiance for the zenith angle Types of Cloud Detection Algorithms The cloud detection algorithm selection was driven by two main requirements the algorithm must be able to process data in real-time on-board the spacecraft and it must distinguish between cloud and non-cloud pixels with suf\002cient accuracy Speci\002cally since the cloudy pixels will be lossily compressed the cloud detection algorithm must be clear-sky conservative it is more important to avoid erroneously classifying pixels as cloudy than to detect all the cloudy pixels On 002rst glance the problem of cloud detection seems simple given the characteristic shapes and brightness of clouds However automated cloud detection poses a challenge since classi\002ers must be able to discern on a pixel-by-pixel basis cloud from non-cloud With respect to the high re\003ectance of clouds in hyperspectral imagers it is a challenge to determine at which wavelength to look for a characteristic cloud signature since smoke ash plumes snow and highly-re\003ective sand can be easily mistaken for clouds Often cloud detection algorithms are applied once the data has been sent to the ground where there are fewer constraints on processing time and memory requirements Cloud detection is a subset of current research surrounding automated pixel classi\002cation There are several types of algorithms that can be used for pixel classi\002cation A comparison of four types of pixel classi\002cation used for the purpose of autonomous event identi\002cation is given in 7  A more detailed look at cloud detection algorithms is given in 4 


On-board cloud detection was previously implemented on the Hyperion instrument of Earth Observing-1 EO-1 An algorithm containing a series of thresholds was initially implemented on the instrument see for details for cloud detection During Hyperion's extended mission a Support Vector Machine algorithm was used for pixel identi\002cation Due to the heritage of these two algorithm types on-board Hyperion they were selected for further investigation with respect to implementation on the HyspIRI mission Threshold cloud detection algorithms operate like a series of logic gates Pixels are labeled as clouds if their re\003ectances are within a given range for a given wavelength or set of wavelengths At its simplest a threshold algorithm can take the form if 032 1  67 025 0  4  then the pixel is cloudy A series of six thresholds was initially implemented on Hyperion for cloud detection  Multiple thresholds were used in order to reduce the classi\002cation error over desert snow ice and highly re\003ective water The advantage of threshold algorithms is that their simplicity results in fast processing times However threshold algorithms become impractical if attempting to label more than one class of pixels Support Vector Machines SVMs are more capable classi\002ers than threshold algorithms although this increased capability comes with a cost of increased complexity SVMs can be used to label multiple classes of pixels for example snow water ice land and cloud An SVM is a machine learning algorithm that maps data into a higher dimensional space and then classi\002es it based on individual data points positions relative to separating hyperplanes The SVM must be trained using sample data to distinguish between the different classes of pixels This can be accomplished through user-de\002ned pixel labeling or clustering methods which classify the pixels based on the desired number of classi\002cations SVMs also vary in complexity based on the way they separate the data in multidimensional space A more detailed explanation of the theory behind SVMs can be found in On EO-1 an SVM using a linear kernel was implemented to autonomously detect science events of interest such as volcanic eruptions and ice sheet breakup Similar capabilities could be implemented on HyspIRI although they are not speci\002cally required by the current science objectives A threshold cloud detection algorithm has been baselined for the HyspIRI mission due to its simplicity Since the classi\002er is only required to classify pixels into two classes cloud and non-cloud a threshold algorithm can meet the mission's needs Preliminary Algorithm Results In order to demonstrate the feasibility of the proposed preliminary calibration and threshold cloud detection algorithm the process was applied to two scenes of AVIRIS data Although AVIRIS data does differ slightly from the data that would be collect by the VSWIR instrument it is similar enough in terms of wavelengths of spectral bands to be used to test the detection algorithms that could be used by HyspIRI A simple threshold algorithm was applied to the sample AVIRIS scenes if 032 1  67 026m 025 0  5  then the pixel is cloudy where 032 x is the re\003ectance at wavelength x  The threshold was applied at a wavelength 1.67 026 m where clouds have high re\003ectance values and there is a low sensitivity to the presence of water The algorithm was 002rst applied to an AVIRIS image from South-Central Pennsylvania 39.96 016 N 78.32 016 E taken July 2 2008 This image is an obviously cloudy agricultural scene The accuracy of the cloud mapping algorithm can be seen in Figure 3 It can be seen that most of the clouds are detected by the algorithm Figure 3b a subset of Figure 3a shows that the algorithm does not very accurately detect smaller less bright clouds Unfortunately no quantitative analysis can be done on the results from this classi\002cation because there is currently no 223truth\224 data i.e the image has not been manually labeled for clouds However the preliminary qualitative results from this image indicate that the classi\002er is performing as desired that is to say it is clear-sky conservative The algorithm was also applied to a desert scene from Central Nevada 38.52 016 N 117 016 E taken on June 11 2008 As mentioned sand which can be highly re\003ective is often dif\002cult to differentiate from cloudy pixels For this scene which contained no clouds the classi\002er erroneously labeled 0.11 of the pixels as cloudy The cloud detection must be applied to more data samples in order to determine the average classi\002cation error Additionally the minimum classi\002cation accuracy that is required should be de\002ned by the science community The required algorithm accuracy may vary based on the ground cover due to the mission's science objectives For instance incorrectly classifying pixels as clouds over the desert will have a limited impact on the global plant species map science product The HyspIRI mission will use a threshold algorithm to detect clouds in data collected by the VSWIR instrument Preliminary work presented here has shown that threshold algorithms have suitable accuracy in detecting clouds Once the cloudy pixels have been detected they will be lossily compressed by reducing the spectral and radiometric resolutions of the data 3 C OMPRESSION S CHEMES Given the large volume of data to be produced by HyspIRI on-board data compression is critical to enabling the mission When both instruments are operating in full resolution mode the HyspIRI onboard processing system must be able to process up to 70Msamples/second in real time before the data can be sent to the solid state recorder and prepared for downlink by the communications system Based on the last JPL Team X design onboard data storage is limited to 1.6 Terrabits or 200 Gigabytes of solid state memory This memory constraint along with the maximum downlink rate capa5 


Figure 3  Cloud Detection Results for Pennsylvania AVIRIS Data a shows the full image swath with true color on left with the cloud detection algorithm results on the right b is a close-up from same data set It can be seen that the algorithm has some dif\002culty detecting small clouds and cloud edges bilities are the two main drivers of the need for onboard real time data compression for the HyspIRI mission The speci\002c requirements on the compression algorithm as well as a comparison between select algorithms will be presented Compression Algorithm Requirements The compression algorithm was chosen in response to three driving requirements 1 Does the algorithm effectively compress hyperspectral data in a lossless manner As expected it is desirable to implement the highest degree of compression possible without losing the scienti\002c usefulness of the data Lossless compression allows a reduction of data volume while still maintaining the ability to exactly reconstruct the original 002le In this application lossless compression is necessary to preserve the scienti\002c integrity of the data collected Among the things that set this mission apart from previous remote sensing missions is the 002ne spatial resolution of the instruments Therefore in order to produce a superior end-level data product it is important that at no point is the data compressed in a lossy manner Note that this approach is for the land and shallow water regions only Cloudy areas taken by the VSWIR instrument are lossily compressed because they are of less scienti\002c interest Since the instrument is designed to collect high resolution data the compression should not degrade that resolution 2 Does the algorithm function fast enough to be implemented in real time with a high throughput data rate The speed requirement of the algorithm is a simple result of the data rate at which the sensors will collect data An algorithm that is fast will have low computational requirements A fast algorithm will also utilize hardware implementation meaning all of its operations can be easily described in a hardware description language and implemented on an integrated circuit or 002eld programmable gate array FPGA 3 Can the algorithm be implemented on-board Since the algorithm will be implemented on-board the spacecraft it must have low computational and memory requirements Each sample will require some number of computational operations when being compressed this number should be on the order of tens of operations per sample In addition multiplication operations take a larger number of clock cycles so an ef\002cient algorithm will be optimized to use few multiplication operations and more add subtract and bit shift 6 


operations Fast Lossless Compression Algorithm Five lossless compression algorithms were compared in terms of compression ratio by Aaron Kiely at JPL see Table 3 The data compression algorithms were tested using calibrated 1997 16 bit AVIRIS data from the Jasper Ridge Lunar Lake and Moffett Field datasets The table shows the average number of bits/sample to which the data was reduced The Fast Lossless algorithm developed by Matt Klimesh at JPL had the lowest number of output bits per sample and the overall best performance The average compression ratio achieved by the Fast Lossless algorithm for these datasets was 3.1:1 Traditional algorithms that have previously been used on hyperspectral data produced a higher average bits/sample and less effective compression ratios F or detailed information on the Fast Lossless Compression algorithm please see 12 Table 3  Comparison in terms of resulting bits per sample after processing by compression algorithms Initially all samples were initially expressed with 16 bits In addition to supplying an advantage in terms of compression ratio over other algorithms the Fast Lossless Compression Algorithm is also well suited for implementation on an FPGA due to the algorithm's low number of calculations per sample The memory requirements are also low requiring only enough memory to hold one spatial-spectral slice of the data on the order of hundreds of Kbytes Figure 4 shows the data 003ow from the instrument to the solid state recorder in the case of the VSWIR instrument Data from the sensor is initially calibrated to re\003ectance values which can be analyzed by the cloud detection algorithm Calibration data is sent to the solid state recorder The cloud detection algorithm generates a cloud map that becomes an input to the compressor The cloud map can be thought of as a matrix of cells that correspond to a spatial location on the image Each cell holds one bit to tell the compressor if the current pixel is a cloud pixel or a non cloud pixel The cloud map is then sent to the solid state recorder If the compressor is compressing a cloudy pixel the pixel is initially lossily compressed then additional lossless compression is applied The initial compression works by 002rst averaging the 214 spectral bands to 20 and reducing each sample from 14 to 4 bits If the compressor is compressing a non cloud pixel only the lossless compression algorithm is applied The compressed data stream is sent to the solid state recorder where it will be stored until it can be transmitted to a ground station The compression algorithm will be implemented on a 002eld programmable gate array FPGA Figure 4  Data Flow through the compressor for the VSWIR instrument Hardware Implementation The motivation for hardware implementation of the onboard compression algorithm is speed Hardware compression may perform as much as 002ve times faster than software compression for data of this type T o implement the algorithm in hardware it is translated from C code executed as a list of software instructions to a hardware description language executed as a propagation of signals through circuits The low complexity and minimal computational requirements of the Fast Lossless compression algorithm make it very well suited to be implemented using an FPGA Work to implement the Fast Lossless Compression Algorithm on an FPGA is being done at JPL The algorithm was tested using a Xilinx Virtex 4 LX160 FPGA mounted on a custom made evaluation board and preliminary test results demonstrated a throughput data rate of 33 Msamples/sec at a system clock rate of 33MHz The algorithm used approximately ten percent of the chips available programmable logic and interconnect resources The VSIWIR instrument will collect samples at a maximum rate of 60 Msamples/sec the TIR will collect at a maximum of 10 Msamples/sec for a combined maximum sample rate of 70 Msamples/sec which should be within the capabilities of the board FPGAs also provide appealing features such as high density low cost radiation tolerance and functional evolution As science goals change or as spacecraft capabilities are limited the ability of an FPGA to be reprogrammed from earth allows for the functional evolution of hardware thought the 7 


life of the mission For the HyspIRI mission this means that if a new compression algorithm with a higher compression ratio or a more accurate cloud detection algorithm with better cloud classi\002cation is developed the new technology could be implemented on the satellite with relative ease In March 2008 Xilinx Inc the main manufacturer of space ready FPGAs has released the Radiation-Tolerant Virtex-4 QPro-V Family providing good options for radiation-tolerant FPGAs with the speed and processing power required for the HyspIRI mission Thus based on a comparison of a v ailable compression algorithms and the HyspIRI missions requirements an FPGA implementation of the Fast Lossless Compression Algorithm is the recommended compression scheme for the mission 4 C OMMUNICATIONS A RCHITECTURE Once the data has been compressed it will be transmitted to a series of ground stations using a given communications protocol X-band Ka band TDRSS and LaserComm architectures were compared in terms of their ability to accommodate the volume of data to be transmitted their cost and mass and their heritage level An X-band architecture utilizing three ground stations was chosen Data Flow Modeling In order to design the communications system it was necessary to determine the desired data volume per unit time that must be transmitted from the satellite to the ground To do this a data 003ow model was created to calculate the data rate and volume that would be created aboard the spacecraft as seen in Figure 5 As the data 003ow diagram Figure 5 shows the model takes into account the satellite orbital parameters the desired sensor con\002guration swath width trackwise and swathwise pixel resolution spectral resolution number of bands intensity resolution bits per band compression ratio and a model of Earth's geography Geography is important since the sensors data output rates will vary according to the sensors target Land/Shallow Water LSW Cloud or Ocean To determine the percentage of time the sensors will view the features a map was overlaid onto a Satellite Orbit Analysis Program SOAP model of the satellite developed by Francois Rogez at JPL The map determined the satellite's sensor mode 226 whether it should record with LSW or Ocean resolutions By running this model over a 19-day period at 5minute resolution as to account for the entire repeat cycle the percentage of time the satellite would collect data from the designated regions LSW or Ocean was identi\002ed Since the locations of clouds are unpredictable a statistical average was used to predict the time the sensor would be viewing clouds With an 11 AM local descending node the expected cloud cover is approximately 30 Estimating a 2-in-3 detection success rate for pixels that are cloudy we estimate 20 of LSW areas will be labeled as cloud Note that clouds will only be identi\002ed on the VSWIR instrument when it is viewing LSW regions Some key elements of the model are shown in Table 4 Table 4  Important values and assumptions in the Data Flow Model The bits per orbit for each of the instrument and modes were then summed over one day Note that the VSWIR is considered to be on for half the orbit the actual duty cycle would be less due to the minimum 20 016 sun angle Therefore the model will slightly overestimate the data taken over the ocean The results of the model are shown below in Table 5 with comparative data volumes shown in Figure 6 One day was chosen as the time unit because it allows for a reasonably accurate estimate over multiple orbits thereby reducing 003uctuations in data generation due to non-uniform geography The data volumes shown are data outputs for the instruments A 10 overhead amount was added to account for metadata calibration data cloud map and packetization This average downlink data volume per day of 5,097 Gbits was used to size the communication system Communications Architecture Selection The main driver in the selection of HypsIRI's communication architecture was the large data downlink rate required Because the HyspIRI mission aims to provide consistent lowrisk operations redundancy and heritage were considered to be major selection factors Cost and masses of various architectures were also considered Table 7 shows a comparison between the architectures considered Ka band frequencies between 18-40 GHz are often used in communications satellites These frequencies are chosen be8 


Figure 5  Simpli\002ed operation of the data 003ow model Table 5  Data sum over one day of normal science operations The added 10 overhead accounts for the required added metadata Table 6  Link Budget for X-band to Svalbard GS cause of their ability to provide high gain as well as high data rates with smaller physical structures The Ka band was ruled to be unfeasible because most ground stations do not support Ka band and the use of Ka would mean introducing a new standard to multiple ground stations while modem upgrades to the exisiting X-band receiving systems were ruled more 9 


Figure 6  Comparative sizes of the data volumes Note that LSW VSWIR and LSW/Cloud TIR represent 98 and 97 of the overall data volume respectively cost-effective 223TDRSS is a communication signal relay system which provides tracking and data acquisition services between low earth orbiting spacecraft and NASA/customer control and/or data processing facilities\224 TDRSS while not supporting as high rates as can be achieved by X-band is capable of continuously operating for long link times   20 minutes per orbit TDRSS was investigated due to its long link times and its convenient ground station location at White Sands However using TDRSS with the current return link data rate would require more power from HyspIRI as well as more complicated structural requirements due to the need for a larger antenna to meet Effective Isotropic Radiative Power EIRP requirements on the HyspIRI-to-TDRSS-side LaserComm is a relatively new technology under development in labs including JPL LaserComm has the potential to transmit data at extremely fast rates up to 10 Gbps making it attractive to the HyspIRI mission While the link speed has been demonstrated the link rate does not seem to be supportable due to the data rate capabilities of the Solid State Recorder SSR Faster storage media must be developed before the satellite can support such a high data rate It also remains the most expensive of the communications system options X-band was deemed to be the best solution overall for HyspIRI The X-band system is estimated to be the least expensive of the three options while having excellent heritage in the DigitalGlobe WorldView-1 satellite both for data rate and data volume X-band also pro vides HyspIRI with the ability to reduce the data rate as GeoEye-1 does to utilize un-upgraded ground stations and increase the data envelope should the upgraded ground stations at 800 Mbps fail Additionally many high-latitude ground stations such as Svalbard and Fairbanks Alaska 20 21 are compatible with the X-band frequencies X-band TDRSS LaserComm Power Required 200 W 320 W 40-60 W Data Rate 800 Mbps 300 Mbps 6 Gbps Data Margin 28.5 13.3 23.7 Cost 9.5M 11.2M 20-30M Table 7  Comparison of the different communications architectures considered Based on these characteristics an X-band-based architecture was baselined for the mission X-band Architecture Details High data rate X-band heritage\227 Two LEO imaging satellites were benchmarked to 002nd a starting point for the design of the X-band communications system The 002rst is the DigitalGlobe WorldView-1 satellite launched in September of 2007 Using two 400-Mbps X-band radios it achieves an overall data rate of 800 Mbps The other is the GeoEye-1 Satellite which will be launched in September of 2008 which is speci\002ed to be capable of 740 Mbps and 150 Mbps The DigitalGlobe WorldView-1 is of special interest because of its data rate The satellite is speci\002ed to collect 331 gigabits of data per orbit which is extremely close to the 343 gigabits per orbit expected to be collected by HyspIRI mission Spacecraft Segment for X-band\227 To achieve 800 Mbps transmission the satellite would use two 400 Mbps radios simultaneously To accomplish this at the same band the signals would be linearly and perpendicularly polarized The WorldView-1 satellite which operates at 800 Mbps uses the L-3 Communications T-722 X-band radio which operates between 8 and 8.4 GHz The radio has an output power of 7.5 W with an OQPSK modulation scheme which increases bitrate from BPSK without introducing error associated with higher-order PSK T o determine if this system w ould be feasible for HyspIRI a link budget was created as can be seen in Table 6 After determining the desired acquisition location the link geometry see Table 8 was used to conduct link budget sizing using a sample link budget from David Hansen at JPL The budget was set up to produce a minimum link margin of 6 dB with the available 7.5 W that is speci\002ed by the T-722 radio The b udget is for one radio each radio requiring data rates of 400 Mbps since the other will be identical but perpendicularly polarized the budget applies for both radios As this rough link budget shows Svalbard ground station is theoretically capable of 10 dB more gain than is necessary to 10 


GS Link Elevation 10 016 above horizon Max Slant Distance 1984 km Off-Nadir Gimballing Angle 64 016 Table 8  Link Geometry for 623 km Note the Gimballing angle must be 64 degrees this has been ruled feasible with a 0.25m-dia parabolic antenna as noted in Table 6 make the link Therefore there is room to reduce the power decrease the size of the antenna or use smaller receiver antennas on the ground and gain access to more ground stations while maintaining a robust link Physically the radio system will require precise gimballing and rotational control to align the polarization with the receiver antenna at the ground station The SA-200HP bus that is baselined for the mission has suf\002cient spacecraft pointing knowledge to supply for the 0.5 016 pointing error assumed in the link budget The mass of the X-band system is shown below in Table 9 Item Mass Radio 3.9 kg each Antenna and Gimballing 2.5 kg S-band Rx 2.5 kg Coax Switch 0.4 kg Total Mass 12.2 kg Table 9  Mass Budget for X-band Architecture Because the two radios will be downlinking from the same pool of onboard data it is necessary to create a 002le splitting scheme to downlink the data see Figure 7 Since the instrument compression will be reset every several lines for error containment the data that is processed will be separated by geographical segments These segments will each correspond to a 223\002le bundle\224 which contains the instrument data calibration data and cloud map data associated with that segment's geographical location Metadata in the 002les will be used to designate the location the 002le bundle correlates to Each 002le bundle will be assigned to a radio so that at any one moment two 002le bundles are transmitted together During transmission the next 002le bundle will then be queued for the next available radio Since each 002le bundle contains metadata about its location the data can be pieced back together at the ground station Ground Segment for X-band\227 Multiple NASA ground stations support X-band communications with current antennas These ground stations include high-latitude ground stations Svalbard Fairbanks TrollSat McMurdo TERSS and many others These ground stations have dual-polarizing parabolic antennas which would be necessary to support the downlink radio frequency structure While this band is supported at Figure 7  Each radio will downlink a single 223File Bundle at a time these stations no station supports data rates higher than 150 Mbps while most support approximately 100 Mbps Therefore to use the 800 Mbps link the ground stations must be upgraded from their current speed with new electronics and data storage For the purposes of communications system sizing it was assumed that with a combination of ground stations it would be possible to provide an average of 10 minutes of downlink time per orbit This could be accomplished by upgrading two to three high latitude ground stations As a contingency option it should be feasible to decrease the data rate of HyspIRI's communications system to 100 Mbps to utilize more ground stations for additional coverage time and data volume or as a backup in case one of the high-rate ground stations is unavailable Data Volume for X-band Communications\227 At 800 Mbps transfer rate with an average of 10 minutes of contact time per orbit the X-band system will be able to downlink 7,125 Gb while the satellite should generate 5,097 Gb on average This scenario yields an overall data margin of 28.5 Cost of X-band Communications\227 The estimated cost for Xband communications is shown in Table 10 Satellite communications costs are taken from the last TeamX report conducted in 2007 Ground station costs are estimates after consulting Joseph Smith at JPL Dedicated data line costs were estimated at 1M each over 3 years 11 


Table 10  Estimated cost of the X-band system 5 G ROUND S TATION S ELECTION As mentioned previously there are several ground stations that are compatible with X-band communications frequencies It was necessary to determine how many and which ground stations should be used in order to accommodate the HyspIRI mission's large data downlink volume Analysis Methods Twenty ground stations were considered in this analysis It was assumed that the ground stations would be upgraded to accommodate the 800 Mbs data rate required by the HyspIRI mission A driving factor in the ground station selection process was the amount of onboard data storage available currently 1.6Tb based on Team X A Satellite Orbit Analysis Program SOAP scenario was created to provide the downlink durations over the entire 19-day period for each instrument PERL scripts created by Francois Rogez at JPL translated the downlink durations into the amount of data being stored onboard the spacecraft based on the data downlink and uplink rates While the onboard storage capabilities of the spacecraft limited the possible ground station options other factors also in\003uenced the selection of the ground station con\002guration To account for these factors a decision making tool was created With respect to the onboard storage the decision making tool took into account the peak volume the slope volume vs time and the average volume of the data stored over the 19 day repeat cycle In addition to these characteristics the con\002gurations were also evaluated based on the cost associated with upgrading the ground stations the assumed reliability and the political control of each ground station The objective function is shown in Equation 1 The T subscript represents the target and the A subscript represents the actual value The objective function target values are given below in Table 11 obj f unc  013 M axP eak T 000 M axP eak A M axP eak T  f Slope T 000 Slope A Slope T  037 AvgV al T 000 AvgV al A AvgV al T  016  Reliability 000 1  017  P oliticalControl 000 1  036 numGS 000 AvgV alGS AvgV alGS 1 where 013  f  037  016  017  036  1 2 Table 11  Target Values of Objective Function Attributes Four scenarios were created by varying the values of the weighting coef\002cients used in Equation 1 see Table 12 The scenarios were run using one two and three ground stations The ground station combination that minimized the objective function was then chosen as the baseline con\002guration Results It was found that the HyspIRI mission's data volume is too large to be accommodated by just one ground station For the scenarios generated using just one ground station the amount of data stored onboard the spacecraft increased linearly over the 19 day collection cycle clearly violating the 1.6Tb onboard data storage constraint The amount of data collected by the HyspIRI mission is also too large to be accommodated by two ground stations again the amount of data stored onboard increases almost linearly over the 19 day repeat cycle With three ground stations however there are several feasible ground station combinations for this mission Of the 20 possible ground station combinations there are 11 solutions where the onboard data storage volume varies sinusoidally see Figure 8 Note that the trial numbers correspond to the ground station numbers shown in Table 13 However once the 1.6Tb onboard data storage volume constraint is included in this analysis there are only two feasible ground station con\002gurations see Figure 9 The con\002gurations that meet the onboard data storage requirements are Fairbanks McMurdo and TrollSat and Fairbanks Svalbard and TrollSat 12 


Figure 8  Close-up Time series of amount of data stored onboard for scenarios using three ground stations upgraded to 800 Mbps Figure 9  Three ground stations upgraded to 800Mbps solutions with 1.6Tb onboard storage constraint 13 


Table 12  Objective Function Coef\002cient Scenarios Sensitivity Analysis Given that only two combinations of three ground stations do not exceed the onboard data storage limits it is necessary to understand the sensitivity of our results to the onboard data storage limits This was accomplished by varying the weighting coef\002cients of the objective function see Table 12 Table 14 shows the results for each of the scenarios Table 14 shows that for every scenario TrollSat was included in the best ground station combination Fairbanks was the second-most favored ground station with McMurdo and Svalbard tied for third place Sweden and Tasmania were never chosen for the best combination This does not mean that they were not feasible solutions However with the given weighting functions they were never chosen as the best solution The optimal ground station con\002guration from Scenario 2 Fairbanks McMurdo and Trollsat was baselined for this mission due to its emphasis on the 1.6Tb maximum onboard data storage constraint Preliminary analysis was conducted into the sensitivity of the system to missing a downlink opportunity This analysis was conducted for the ground link scenario experienced on the second day of the spacecraft's 19 day repeat cycle Also this analysis does not look at the possibility of loosing contact with several different ground stations over the course of a day Preliminary results are shown in Table 15 and a more detailed discussion can be found in It can be seen that virtually all of the cases considered exceed the 1.6Tb of available onboard storage More work needs to be done in this area to adequately address the risk of missing downlink opportunities throughout the mission Table 13  Ground Station Reference Numbers More advanced HyspIRI mission design conducted at JPL by Team X has now expanded the available onboard data storage to 3Tb 6 S CIENCE A NALYSIS In order to more fully understand the requirements associated with the data throughout the mission it was necessary to de\002ne data levels both on-board the spacecraft and on the ground By de\002ning the form and scienti\002c signi\002cance of the data at each stage a greater understanding of the data system's role was gained Additionally by gaining a greater understanding of HyspIRI's data products it was possible to identify missions with which HyspIRI could produce complementary science products Data Level De\002nition De\002nitions for data levels follow see Figure 10 Level 0 Raw Data HyspIRI will collect two types of raw data the science data from the two sensors and the calibration data As the sensors collect photons they will output analog signals resulting from the light intensity levels as well as the sensors material properties The calibration data is obtained through lunar blackbody solar and dark level calibration Lunar and blackbody calibration will be conducted by satellite pointing Solar calibration will be conducted by re\003ecting solar light off the instrument cover Dark level calibration will be conducted by closing the instrument cover Ephemeris data will be obtained through the onboard GPS and star-sensors The calibration data will be needed on-board the spacecraft in order to conduct the preliminary calibration required for cloud detection Level 1A Roughly Calibrated to Re\003ectance Data Table 14  Objective Function Results 14 


Ground Station Number of Passes Missed Max Required On-board Storage Fairbanks 1 2 Tb All 8 2.7 Tb Trollsat 1 1.5 Tb All 10 3.7 Tb Svalbard 1 1.7 Tb All\(10 3.8 Tb Table 15  Amount of storage required when downlink opportunities are missed These preliminary results were computed on the second day of the spacecraft's 19 day repeat cycle All passes missed indicates that all the passes over the ground station were missed for a given day Figure 10  The data level de\002nitions are presented The chart 003ows from left to right top to bottom through levels 0 1A 1B 1C 2 3 and 4 These visual examples belong past mission imagery Visual examples for levels 1A 2 and 3 belong to AVIRIS data and calibration charts while visual examples for Level 4 belongs to ASTER data Once the data is converted to digital number values it will be calibrated using the dark level calibration values to correct for sensor offsets due to material 003aws During dark level calibration the sensors should not detect any signal thus any signals from the sensor indicate an erroneous offset This dark-level-calibrated data must then be converted from sensor readings to spectral radiance values This conversion process takes inputs from solar and satellite zenith and azimuth angles radiometric information stored onboard in coef\002cient appendices and top-of-atmosphere effects scattering The data will be tagged with universal collection time and latitude and longitude Radiance values that have been atmospherically corrected can then generate re\003ectance values This conversion must be completed onboard because re\003ectance allows for the detection of clouds for the VSWIR instrument Level 1B Compressed Packetized Data Level 1B data consists of compressed packetized data Cloudy pixels undergo a lossy compression and a lossless compression while non-cloudy land-and-shallow-water pixels undergo only lossless compression Ocean pixels will undergo lossy compression Because there is little scienti\002c value gained from VSWIR images of clouds cloud maps are generated to enable higher compression for clouds In the case of the TIR all land-and15 


shallow-water data regardless of cloud presence will be subject to lossless compression Compressed Level 1A data is then packaged alongside the ephemeris metadata to become Level 1B data which is downlinked to ground stations Packetization is done such that communications errors causing the loss of one packet will not affect the data integrity of other packets Level 1C Decompressed Depacketized Data Once data packets are received on ground the data is depacketized and decompressed The packets are then assembled to create full-resolution continuous data sets as Level 1C data This data is backed up and will be stored permanently as is described below Level 2 Atmospheric Corrected Data In order to generate Level 2 data instrument measurement values previously calibrated are resampled remapped and recalibrated with more sophisticated and accurate methods Data is also geometrically calibrated and orthorecti\002ed Further corrections may involve cosmic ray compensation and bad pixel replacement Spectral signatures are identi\002ed A series of high-level information physical optical radiance and other derived elements can be viewed at this level Spectral signatures derived from re\003ectance surface radiance temperature emissivity and composition are available at this level Level 3 Mapped Data Geophysical parameters are mapped onto uniform space grids located in space and time with instrument location pointing and sampling Volcanic eruptions landslides wild\002res natural resources drought soil and vegetation type will be mapped Gridded maps are created for each image Files may be located regionally A series of overlapping data per orbit may be encountered All 002les will be made available including overlapping 002les Level 4 Temporal Dependant Data As the instruments 002nish their repeat cycles they will produce global frames of science products 5 days per frame for the TIR and 19 days for the VSWIR With these frames it is possible to characterize the earth in terms of the science data products every repeat cycle These frames will be compiled based on collection time producing a 4-D model of the earth which can be used to view local or global changes as a function of time This 4-D model would be made available on a monthly seasonal or yearly basis Synergy with Other Missions The HyspIRI mission will produce science products for various monitoring and early warning programs If combined with science products from other missions the HyspIRI science products can serve for even greater societal bene\002t and scienti\002c interest HyspIRI's science products can be combined with missions that will be collecting data simultaneously around 2013-2016 as well as with past and future missions and ground-based measurements These will permit the creation of high-impact science products It is possible that the HyspIRI mission could collaborate with the Aerosol and Cloud Ecosystems ACE mission which is also motivated by the NRC Decadal Survey By collaborating with ACE 226 a mission scheduled launch between 2013 and 2016 226 more accurate predictions of aquatic ecosystem responses to acidi\002cation and organic budget changes can be obtained from disturbance effects on distribution biodiversity and functioning of ecosystems HyspIRI science products as well as ocean CO 2 sinking and acidity changes ACE science products Also correlation between ecosystem changes and ocean organic measurements can be studied by combining disturbance effects on distribution biodiversity and functioning of ecosystems HyspIRI science products and oceanic organic material budgets ACE science product Through collaboration with other mission's the impact of HyspIRI's data products will be increased HyspIRI's many science products produced at the various data levels will add to the large volume of data that must be stored once the data is sent to the ground Additionally clear de\002nitions of the science products and data levels will enable the creation of an ef\002cient and accessible ground data storage system 7 D ATA S TORAGE AND D ISTRIBUTION Long-term data and science product storage is becoming an increasingly relevent issue in data system design Afterall the worth of the mission is based on the accessibility and usability of the data once it has been sent to the ground The combined data collection rate of HyspIRI's two instruments exceeds 1.5TB/day and nearly 0.56 PB/year The raw data ingest rate alone would increase the current Earth Observing System Data and Information System EOSDIS daily archive growth by 48 not including the storage of science products The HyspIRI Adaptive Data Processing System HIIAPS is a proposed data processing architecture based on the Atmospheric Composition Processing System ACPS with minor modi\002cations HIIAPS would handle the challenge of archiving processing and distributing such an immense amount of data while keeping costs low Lessons Learned from EOSDIS The Earth Observing System EOS Data and Information System EOSDIS began development and implementation in the early 1990s and has been fully operational since 1999  28 29 EOSDIS is the primary information infrastructure that supports the EOS missions The responsibilities of EOSDIS could originally be divided into two sectors mission operations and science operations 27 28  Mission Operations handles satellite 003ight operations data capture and initial processing of raw data Most of the Mission Operations responsibilities are now handled by Earth Science Mission Operations ESMO 32 Sci16 


ence Operations processes distributes and archives the data  27 28 31 N ASA Inte grated Services Netw ork NISN mission services handle the data transfer between the mission operations system and the science operations systems 30 There are several valuable lessons that were learned from the operation and continued evolution of EOSDIS The Goddard Earth Sciences GES Distributed Active Archive Center DAAC and Langley Research Center LaRC Atmospheric Science Data Center ASDC evolutions emphasize reducing operations costs by maintaining only a single processing system increasing automation through the use of simpler systems reducing engineering costs through the replacement of COTS commodity-off-the-shelf products with commodity products and increasing accessibility by increasing disk usage by phasing out existing tape based systems The par allel development of Archive Next Generation ANG 257 e and Simple Scalable Script-Based Science Processing Archive S4PA at GES DAAC and LaRC ASDC DAAC demonstrate that NASA continues to encourage individual entities such as the DAACs and Science Investigator-led Processing Systems SIPSs to suggest and develop new approaches A summary of the lessons learned from the development and continued operation of EOSDIS from 223Evolving a Ten Year Old Data are brie\003y listed belo w 017 Having a program requirement for continuous technology assessment establishes a culture of innovation 017 An on-going small-scale prototyping effort within operational elements is important for exploring new technologies and hardware 017 Involvement by all affected parties is essential to evolution planning 017 It is necessary to collect and analyze system usage metrics e.g request patterns for various data products in planning for the future 017 Flexible management processes accommodating innovation speed and ef\002ciency are essential for increasing agility in development despite the higher perceived risks 017 A risk management strategy must be included in planning for evolution 017 Transitions affecting the user community need to be carefully managed with suf\002cient advance noti\002cation All of these lessons point toward an independently operated system built for continuous evolution As has been stated before HyspIRI generates extremely large amounts of data A highly scalable and adaptive system would be the most cost effective approach to handle the processing archiving and distribution needs of the mission The presence of a culture of innovation would facilitate the necessary continuous reassessment of the technology present at HIIAPS Such technology reassessments would facilitate a smooth continuous integration of new processing and storage components in both the hardware and software realms Thus initial development costs could be reduced by eliminating the need to purchase all storage media at once and instead purchasing storage media gradually as costs and performances improve and as they are needed To be able to do this in the most cost effective manner a system usage analysis should be conducted in order to properly size the storage and processing systems For such a system there should be a detailed plan in place for the migration of data from old to new storage media as the older storage media becomes obsolete or nonfunctional Keeping all affected parties involved is especially relevant to the formatting of the data which will be discussed later Involvement of these parties combined with continuous smallscale prototyping speeds the generation and integration of new ideas and also serves as a risk management measure to prevent changes being made to the system that would be detrimental to the user community Involvement of these parties also helps to determine how access to data should be kept online and how that data might be most easily accessed online HyspIRI Adaptive Processing System HIIAPS HyspIRI Adaptive Processing System pronounced 223high-apps,\224 is a proposed stand alone data processing system which would handle the data processing storage and distribution requirements of the HyspIRI mission It would be based on ACPS with only the minor modi\002cations that occur in adapting a previous system to the slightly different needs of another mission JPL's Team X states that 223Science team has responsibility for all storage and analysis    Data is made available online to full science team for providing atmospheric corrections and producing products\224 An A CPSlike processing system would ful\002ll these requirements HIIAPS would be a centralized dedicated SIPS and DAAClike system that would be co-located with the instrument and science teams The MODIS Adaptive Processing System MODAPS has demonstrated that co-locating the science team with the processing center reduces cost by saving time on the development and integration of new algorithms  It is also important to note that HIIAPS w ould ha v e no commercial or proprietary parts thus evading the issues of software licensing A diagram of the HIIAPS data processing context is shown in Figure 11 All storage and analysis for the HyspIRI mission would be done on site Level 1C data is archived and distributed at HIIAPS HIIAPS will also process and archive Level 3 and below data leaving higher level products to academia and the private sector Data products would be sent directly to the HypsIRI science team and an offsite archive This offsite archive could be a DAAC or another data processing center that serves as a risk management precaution in the event of a natural disaster at the HIIAPS site Like MODAPS HIIAPS would employ data pool technology to save on storage and processing costs Higher level smaller data products and recently created products would be kept in the data pool while more cumbersome products like Level 1C data would be produced on demand Also like previous systems HIIAPS would provide custom data products in the form of virtual products that are made to order By building upon the knowledge base 17 


Figure 11  HIIAPS Data Processing Flow of previous processing systems HIIAPS would both reduce the costs and risk of creating a new processing center Storage Media Disk vs Tape Initially EOSDIS used tape-based storage Now it is slowly transitioning to disk based storage 31 34 T ape is easily transportable has faster read/write speeds but it is sequential access storage and a nearline media 36 37 On the other hand disk has a higher storage capacity per unit makes the data easily accessible online experiences more frequent technology upgrades but has higher electrical costs and is more prone to error 36 37 Disk storage is also more convenient to use because it is random access and data can be kept online as opposed to nearline or of\003ine all the time thus shortening data retrieval times It was been shown that disk storage is more expensive in the short term but in the long term the cost difference between disk and tape systems seems to converge This combined with the f act that ACPS the Ozone Monitoring Instrument Data Processing System OMIDAPS and MODAPS are all disk based storage leads to the conclusion that HIIAPS should also be an entirely disk based system especially with the Kryder's Law disk storage densities double every 23 months taken into account Data Formats As the previous section states it is imperative that HyspIRI data is made available to as many non-remote sensing scientists as possible Part of achieving this goal will be accomplished by distributing the data in a format that is easy to use A standard format would be helpful because it would guarantee that everybody could read that 002le format However there is no single standard format for geospatial data  N ASA  s EOSDIS currently uses HDF-EOS5 which is good for processing but not everyone can easily read the HDF 002le format 40 41 Ev en though there is no standard some formats are more easily accessible than others Using a GIS Geographic Information Systems format like a 003at georeferenced JPEG or PNG image would make HyspIRI data readily accessible to an already large user base and to those who do not use a GIS program The Open GIS Consortium Inc R r OGC is working to solve this formatting issue and towards Open GIS 43 223Open GIS is the full integration of geospatial data into mainstream information technology What this means is that GIS users would be able to freely exchange data over a range of GIS software systems and networks without having to worry about format conversion or proprietary data types Working with the OGC or at least staying in touch with their progress would help to maximize interoperability and accessibility Based on this HyspIRI should have at least two standard data formats One would be HDF-EOS5 because that is the standard NASA EOS format and HIIAPS can draw on pervious NASA experience for it The second would be a GIS ready format like a georeferenced JPEG or PNG to increase accessibility to HyspIRI data This section looked at how HyspIRI might 002t into the EOSDIS framework several previous processing systems and touched on accessibility and interoperability concerns HyspIRI should take advantage of the present EOSDIS infrastructure However HIIAPS should be a stand alone processing center so that HyspIRI's large data volume does not overtax the existing EOSDIS system There are three heritage systems for HIIAPS MODAPS OMIDAPS and ACPS each an the evolution of the next There has been a steady shift from tape-based storage to disk-based storage over the years Thus HIIAPS should use a disk-based storage system In addition to storage and processing distribution is also a concern for the HyspIRI mission HyspIRI data should be made available at no charge to as many people as possible in a manner accessible to as many people as possible Some of the challenges are choosing a data format reaching non-remote sensing scientists and providing the data in an easy to 002nd manner Using Google Earth TM and supplying HyspIRI data products in more than one format will help to solve this challenge 8 C ONCLUSION An end-to-end data system has been described for the HyspIRI mission a high data volume Earth-observing mission With the HyspIRI mission ambitious science objectives are driving the development of a cutting-edge data handling system both on-board the spacecraft and on the ground 18 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





