A Maximum Likelihood Estimator for Tracking Purposes with Extended Sources Brett Monz Department of Electrical and Computer Engineering Air Force Institute of Technology Wright-Patterson AFB OH 45433-7765 937-255-6800 X4303 adrian.monz.us@afit.edu Jason Schmidt Department of Electrical and Computer Engineering Air Force Institute of Technology Wright-Patterson AFB OH 45433-7765 937-255-3636 X7224 j ason 
In many cases optical tracking systems do not have cooperative beacons available This is particularly true for the case involving tracking 
Abstract 
the reflectance from a laser illuminated target such as a missile seeker head where the object of interest is most definitely an extended source Fur\255 thermore the extended source is always combined with noise such as shot noise which further degrades the signal Consid\255 eration is also given to atmospheric turbulence This paper examines the performance of an existing projection-based maximum-likelihood technique for tilt estimation in the pres\255 ence of extended sources with particular application to the image motion tracking problem Comparison is made be\255 tween 
the performance of a traditional centroiding algorithm and a projection-based algorithm with simulated data The projection-based algorithm is shown to offer improved per\255 formance in the motion tracking problem TABLE OF CONTENTS 
1 1 2 5 
3 SIMULATION 3 4 CONCLUSIONS 4 REFERENCES 225\225\225\225\225\225\225\225\225\225\225\225\225\225\225\225\225\225\\225\225\225\225\225\225\225\225\225\225\225\225\225\225\225\225\225\225\\225 
2 
INTRODUCTION DESCRIPTION OF ALGORITHMS 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 BIOGRAPHy 5 1 INTRODUCTION The traditional tilt estimation and motion tracking 
problem is well understood and adequately addressed when the object of interest is a point source or a source whose dimensions and distance from the observing aperture allow it to be safely treated as a point source However when the dimensions of the object of interest preclude it from being safely mod\255 eled as a point source as in many real-world scenarios the u.s Government work not protected by U.S.copyright For U.S Govern\255 ment employees only IEEEAC Paper 1568 Version 3 Updated 8/12/2008 problem is not so well 
addressed The uncooperative or ex\255 tended source presents many challenges to the motion track\255 ing and tilt estimation problem The traditional centroiding method employed in most Shack-Hartmann type sensors re\255 lies on points of high contrast within the image to act as bea\255 cons in estimating the wavefront tilt When considering ex\255 tended beacons or scenes which are large compared to the aperture diameter there are often few if any high-contrast points and the centroid algorithm often performs poorly Ad\255 ditionally the centroid algorithm is sensitive 
to noise such as that due to the random arrival of photons to the sensor shot noise Cross-correlation algorithms offer improved noise re\255 jection properties as well as better performance over scenes in which there are few prominent high-contrast points such as that found with some extended sources They do have one serious drawback in that the computational burden due to the two dimensional cross correlations carried out is often heavy and renders the method slow and intensive The projection algorithm as derived in Reference 2 utilizes two se:parate sensor 
arrays to sense the tilt in each of two per\255 pendicular dimensions Optical tilt over the incoming wave\255 front is fed to each of the two sensors via a beam-splitting mechanism and produces a shift in spot position on the array The projection-based algorithm relies on the data read out from the sensors in vector form and uses the cross-correlation technique to compute tilt in each dimension This vector read\255 out mlethod is not new and has been used before by MIT Lin\255 coln Laboratory in their SWAT system 1 
The projection\255 based algorithm does depart however in the method used to estimate optical tilt cross correlation of the vector read out rather than centroiding In this report the projection-based cross-correlation algorithm for tilt estimation is compared to the traditional centroid method for both point and extended scenes with consideration given to shot noise and atmospheric turbulence 
is shown that overall the performance of the projection algorithm is comparable to the centroid algorithm even though the projection algorithm is receiving half the 
It 


Z Z N where where The centroid algorithm used in this report is the traditional al\255 gorithm whereby the center of mass is calculated by dividing the sum across both The key advantage to using the projection-based cross\255 correlation algorithm is that the two-dimensional image is re\255 duced to a one-dimensional image which preserves the tilt information present in one dimension of the original image So that tilt information in both dimensions can be measured two sensors are required The operation of both is identical and for the purposes of this report only analysis for one sen\255 sor is shown Referring to Figure 1 assuming a Lenslet Array WxW L x L y i\(x y pixel in the array N is the dimension of the array in pixels and It frame light level and    r  1 2 1 i 1 2 If:A\(k If:A\(k i';"k 2 DESCRIPTION OF ALGORITHMS A k n X y x y x P D P w r w L  dimensions The equation for the pixel sensor array and considering just one image formed on the sensor the data are read off in vector form such that each projection corresponding to the r s    Figure  Centroid algorithm S  rad/m/Detector pixel of motion where and and 2 4 X-Wavefront Sensor Z x Zpixel This tilt is in units of detector pixels To enable comparison with other algorithms we convert it into rad/m in the following manner If we take the point spread function PSF for no tilt to be is the aperture diameter in meters and in this case for vertical shift is random The estimate is given by where in the area of concern and zero otherwise In order to capture the discrete and non-negative nature of the photon-counting process a Poisson model is used for the measured projection s is a reference frame which may be the first frame captured and s are window functions of value and where r dimension is which can be separated in the exponential to give the slope of the tilt change to be 21r  rad/aperture pixel This is easily adapted to give the conversion from detector pixels to rad/m as 21r dimensions of the pixel values weighted according to their displacement from the center by the un-weighted sum across both dimensions The calcula\255 tion is similar for both the  i\(r i\(r s r s r=l 225 1    s are sensor array coordinates s is the data at coordinate 5 x x 1 Projection transformation employed on sensor lenslet array forming images onto a x y x y L Dk\(r s r=l Z x k th r Dk r d'k s f3 d'k s as previously defined Projection algorithm _ is the summation of the image across the columns of the sub\255 array being considered The projection is defined as 2 3 1 is the amplitude of the Z/W d'k\(s Ware Psf\(n k=l Psf\(n k=l th L:xL:yXi\(x,y ex Z/W ir\(s s W W is the pixel index then the PSF for motion of one pixel is is the position in the detector array from the origin and is the signal at detector coordinates is in aperture pixels The centroid algorithm is the optimal tilt estimator for Gaus\255 sian focal spots with Poisson noise 4 However as can be seen from Eq.\(l the outlying pixels are weighted more than the ones closest to the center The dimmest pixels are often found furthest from the center which means that the least im\255 portant pixels are given a higher weighting in this algorithm which contributes to its lower performance in noisy extended scenes In Reference 4 it is shown that the maximum-likelihood ML estimator is a good slope estimator for wavefronts in the presence of noise This combined with the benefits of using cross-correlation make it beneficial to use a Bayesian estimator for the tilt parameter Here the estimator is derived by forming the likelihood function for the tilt conditional on the data and then maximizing it with respect to the tilt 5 The likelihood function is the conditional probability of the tilt parameter given the measured projection and the tilt pa\255 rameter from the previous image frame is assumed that an estimate of the true projection is known and is deterministic and the associated tilt parameter jh<;:-l N D 


and equal to 1 elsewhere the natural logarithm will be equal to negative infinity for values of and updating the estimate in the direction of increasing  can be expressed as a product of the marginal densities over all pixels in Both diffraction-limited point sources and extended sources were considered The extended source was modeled as a 4 x 4 pixel source in the source plane as shown in Figure 3 No anisoplanatic effects are considered The ratio of the aperture diameter b fd k lf3k dlb to to Fried parameter for a given shift in the vertical of pixels giving the maximum likelihood function which can be expressed as a function of terms that only depend on the shift k us\255 ing an iterative approach to computing the value of the func\255 tion locally around the current estimate for 7 In order to maximize this expression in terms of the tilt pa\255 rameter 9 In this research the widowing function  In[fd k lf3k d b dl13k k d Figure 2 is an illustration of the simulation setup A 128 In s=1 fd bf The value of S S  T s poissrnd 128 pixel source is assumed to be a very large   re\255 spectively f f b max s L L w b i b d T conditioned on the tilt parameter of the previous frame being w T r  max max 8 giving the log-likelihood function to be which make the window function equal to O To avoid this the limits of integration are chosen to include only those points where the window function is non-zero making the log-likelihood function is the prob\255 ability of the random tilt being equal to is the unconditional prob\255 ability that the projection vector random process is equal to a specific realization Eq.\(6 is maximized with respect to its logarithm is defined as equal to 0 for This pdfis becomes the resolution of the tilt estimation algo\255 rithm in units of array pixels A linear interpolator is chosen to produce sub-pixel resolution for tilt estimates The linear interpolator has the form and the tilt from the previous frame and and and the second term ofEq.\(7 may be dropped The projection vector and  As previously stated the likelihood function requires knowledge of the dis\255 tribution of the tilt parameter from frame to frame This can be illustrated using Bayes rule in that Eq.\(10 can be maximized with respect to tilt parameter in steps of 3 SIMULATION b is the probability of the tilt for frame is the probability that the projection vector ran\255 dom process is equal to a specific realization of that process conditioned on is the expected maximum absolute value of the tilt parameter Then because the win\255 dow function is chosen to be smaller than the size of the projection vector by a number of 3 where is one in all cases In order to make proper comparison of results obtained the rms tillt error due to noise in units of radians per meter was calculated for all simulations This is the Y axis on all simu\255 lation plots Figure 4 shows the results for a diffraction lim\255 ited point source compared to the theoretical result from Eq 5.30 of Roggeman and Welsh 3 shown in Eq 12 and the result using the traditional centroid method discussed in Section 2 using 100 realizations at each mean received light level from 100 to 1000 photons  The Roggemann model rep\255 resents the best case scenario for a Shack-Hartmann sensor assuming the centroiding method It can be seen that the projection vector method performs better than both the tra\(6 b L\(b  1   x L\(b L\(b  given distance from the aperture of the observing instrument Only one sen\255 sor is simulated here and hence the light intensity assumed incident on the sensor in simulation is halved Atmospheric turbulence is given consideration by the use of a single phase screen generated using the Fourier method in the same man\255 ner as in Reference 3 Shot noise is also added to the re\255 ceived image frame to simulate the random arrival nature of the photons using the MATLAB\256 function captured The model has a point-wise mean of w T represents an ensemble of inde\255 pendent Poisson random variables associated with individual pixel measurements Given the initial assumption of statis\255 tical independence between measurements the pdf of a col\255 lection of samples of    s 255 In[iT\(s k k w T b s Z W s L b b b b e s=1 L  d'k b d'k d'k 13k TO conditioned on the measured projection b b P d'k where Z/W Z/W-b rnax s=b rnax Z/W T T b s ir\(s-b s Z/W b rnax f we must have some knowledge of the probability of the current tilt parameter conditioned on the tilt from the previous frame If this knowledge is available it should be used However in many cases it is not and it is common practice 4 to choose a uniform density which does not vary with where iT 13k  T s 13k bid b lf3k dlb bib f3kl d k f3k-l fd k d  ff3kl d f3k--l bid b 13k-I ff3klf3k-l bib 13k 13k-l 13k b 2b max  s d\(s b iT\(s b 10 5b 5b In[ff3klf3k-l bib d'k d\(s b d s iT s b b s d         are the integer and fractional parts of 


      e      1 k k where The traditional centroid method of tilt estimation is the ba\255 sis for many sensor systems It has been confirmed here that it performs very well for point sources and reasonably well for extended sources even in the presence of atmospheric turbulence and photon noise What was surprising was ac\255 tually how well it performed in the presence of turbulence and photon noise The less than expected performance of the projection-based approach may be due to a number of factors 4 a ns 4 5 10 15 20 X Dimension pixels 4 CONCLUSIONS 225     12  q 2 0  5 0 Projection vector nlethod 225 Roggemarm Model V-Wavefront Sensor Magnified image of extended source model full size 128    Centroiding method dK\(1/2  1 compared to the re\255 sult using the traditional centroid method discussed in Section 2 These data were collected using 1000 realizations at each mean received light level from 100 to 1000 photons Only one phase screen was generated and was used for each realization at each light level For the mean light levels simulated the performance of the the projection vector method is still gen\255 erally better than the traditional centroid method It should be noted however that this performance is being achieved with only half the light available to the centroid method V27r Figure 128 pixels ditional centroid method and the theoretical lower bound of Reference 3 given by is the average number of photons received per im\255 age Figure 5 shows the results when the source is the extended source as shown in Figure 3 and no consideration is given to atmospheric turbulence compared to the result using the traQ d TO 7 6 0.6  Figure Figure 3 Z 00   l::l   Q    Atmospheric Turbulence Z\273>f X-Wavefront Sensor 1000 200 400 600 800   0     I f 247 225 o x  0   0 eCf 2 Illustration of simulation setup 5 0.8  rn 10 15 0.4 S 20 0.2 4 Rms tilt error comparison for diffraction-limited point source ditional centroid method discussed in Section 2 These data were collected using 1000 realizations at each mean received light level from 100 to 1000 photons It can be seen that the projection vector method generally performs better than the traditional centroid method for the light levels simulated Figure 6 shows the results when the source is the extended source as shown in Figure 3 and consideration is given to atmospheric turbulence with  Average photons received per inlage 11------I------I....-------l----.l--------I o 


43\(7 1670-1681 2004 3 Michael C Roggemann and Byron Welsh c5 l Detection Estimation and Modulation Theory  Q,;i 0  0.8 2.6 2.4 2.2 k including Figure 5 Rms tilt error comparison for extended source with no atmospheric turbulence 2.6 2.4 2.2 1""4 2 0.8 e Centroiding method   Projection vector method o 200 400 600 800 1000 Imaging through Turbulence 0 0 1.2 t _ n       S 2 0 l l not included in the source model considered here However it is worth noting that although the projection method did not produce the performance improvement ex\255 pected due to the physical set up of the projection method the slight improvement was made at a light level half that which the centroid method was utilizing REFERENCES 1 H T Barclay P H Malyak W H McGonagle R K Re\255 ich G S Rowe andJ C Twichell The SWAT wavefront sensor           Wiley and Sons 1968 o 200 400 600 800 1000 f Average photons received per image Figure 6 Rms tilt error comparison for extended source with atmospheric turbulence BIOGRAPHY Brett Monz received his Associate Diploma in Engineering Electronics in 1989 and a B.Eng Hons I degree in Electrical Engineering from the Univer\255 sity of Newcastle NSW Australia in 2004  From 1987 to 2007 served firstly as an enlisted communications and sen\255 sors technician and recently as a Com\255 munications and Sensor Engineering Officer in the Royal Australian Air Force He is currently a masters student at the A.irforce Institute of Technology Wright-Patterson Air\255 force Base Dayton OH His research interests include target tracking IR sensor systems and radio control helicopters  Maj Jason Schmidt was commissioned as an officer in the U.S Air Force in 1998 Also in 1998 he received the B.S degree in Physics from Marquette Uni\255 versity where he was awarded the Kar\255 ioris Memorial Award in Physics Then in 2000 he received the M.S degree in 0 l l l 267\267\267\267\267\267\267,\2679\267\267\267\267    E Physics from The Ohio State University Thereafter Maj Schmidt was assigned as a Research Physicist at Air Force Research Laboratory's Starfire Optical Range In 2006 he received the Ph.D degree in Electro-Optics from e Centroiding nlethod   Projection vector method 1.8 00 1.2 Lincoln Laboratory Journal J Opt Soc Am A 0 Optical Engineer\255 ing 1.4 0 5 115-130 1992 2 Stephen Cain Design of an image projection correlating wavefront sensor for adaptive optics cD   1 255 a 1.8 00 1.6 1.4  Q Average photons received per image 1 Insuffil;;ient number of realizations at each average photon light le:vel  1000 may be required 2 The theoretical result may include higher order aberrations that CRC Press 1996 4 M:arcos A van Dam and Richard G Lane Wave-front slope estimation 17\(7 1319-1324 2000 5 H L Van Trees Z 0 1.6     Z 5   Q,;i  _     lIt  


University of Dayton Currently Maj Schmidt is an assistant professor in the Electrical and Computer Engineering Depart\255 ment at Air Force Institute of Technology His research inter\255 ests include adaptive optics free-space optical communica\255 tions and optical simulation Maj Schmidt is also a member of the International Society for Optical Engineering SPIE Optical Society of America and Directed Energy Profes\255 sional Society 6 


faults However value faults both symmetric and asymmetric cannot be resolved without explicit mechanisms If we consider case 2 in which TCP may time out then  F i  b s a   Next lets consider the active fault descriptions If our authentication scheme is assumed to be uncompromisable then F i  b   otherwise it is F i  b s a   The interesting case in the example above is when authentication is compromised Value faults cannot be dealt with unless the authentication mechanism is implemented to provide redundancy levels of N  2 s 1 and N  3 a 1 for symmetric and asymmetric behavior respectively Note that in order to avoid common mode faults the redundant modules should be dissimilar In order to deal with symmetric faults one needs a simple majority of unaffected modules However asymmetric faults do not only require a higher degree of redundancy but also require that agreement algorithms be used These algorithms typically work in rounds of message exchanges The result is high message overhead in addition to the high component count However since in our example the imposed and active fault descriptions both contain s and a  there is no easy way around having to deal with these faults explicitly For the system designer the choices seem clear 1 one lives with the risk of authentication compromises or 2 one pays the cost of module and message overhead But how high is that cost This depends on how many faults of type s and a one wants to tolerate In addition common mode faults need to be addresses and thus the cost of dissimilar components needs to be considered Design Changes The advantage of working with imposed fault description is that it gives insight about what the infrastructure cannot inherently deal with This allows for adaptation that can bring signi“cant simpli“cations to the application Consider the example above and assume that authentication may be compromised i.e assume that F i   F i  b s a   The largest challenge is to avoid having to deal with costly asymmetric faults However the infrastructure cannot tolerate such faults implicitly and thus explicit mechanisms such as agreement algorithms must be used Therefore lets consider what changes can be made to the infrastructure in order to avoid asymmetric faults With respect to networking this is actually quite simple As indicated before a broadcast environment cannot exhibit asymmetric behavior Therefore assume that point-to-point networking in authentication is eliminated and that broadcasting is used instead Under the broadcast paradigm every node can see the same messages so that Byzantine faults can be immediately detected Now  F i  b s   and thus the application can provide simple mechanisms to take advantage of the imposed tolerance to asymmetric faults Thus by observing the limitations of the imposed fault description an infrastructure-related change can make large improvements Adaptive Policies In the previous example the design of the authentication mechanism was motivated by low cost which resulted in an active model considering only benign faults If value faults are suspected then the high cost of dealing with value faults most signi“cantly asymmetric faults was required However in most applications the worse case behavior e.g broken authentication may be only of importance during times of high threat levels This suggest a security policy that is exible and sensitive to the threat level Such a policy would select the lowest overhead solution possible under a given threat level In our authentication example this could mean using the benign model under normal situations and augmenting value faults if the threat level is high Such gear shifting is not new and has been used in the context of agreement algorithms to reduce overhead Infrastructure Changes Lastly if the infrastructure used by functionalities f i changes then one should consider if these changes have implications on the imposed fault description If they do then perhaps one can take advantage of this change On the other hand it may mean that now the limitations of the infrastructure need to be compensated by more sophisticated solutions An example of such degeneration is when a network changes from a broadcast to a point-to-point communication primitive In all the cases above the careful analysis of F i and  F i should be undertaken Misjudging the fault model can render the application non-survivable 6 Conclusions A new view of fault models was presented that shifted away from the fault cause and instead focused on the effect of faults This allows for the use of fault models in the analysis of systems operating in hostile environments A general view of design for survivability was adopted This implied that the application and infrastructure were viewed in concert in order to determine which fault models they required and supplied By viewing a system as a collection of functionalities each functionality could be separately analyzed This simpli“ed the determination of the active and imposed fault description which was then used to determine a mapping between what the application functionality required and what the infrastructure could or could not support In the latter case explicit solutions must be used to overcome the infrastructure induced limitations With respect to system analysis the functionality-based view of the system allowed exibility in the choice of hazard functions Rather than using one model for the entire system now each functionality can be analyzed using its appropriate hazard functions The exibility was then Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


extended to the time domain thus allowing to change the model over time in response to external changes References  A A vizienis et.al Fundamental Concepts of Dependability  Information Survivability Workshop ISW2000 Boston Massachusetts Oct 24-26 2000  M.H Azadmanesh and R.M Kieckhafer  Exploiting Omissive Faults in Synchronous Approximate Agreement  IEEE Trans Computers 49\(10 pp 1031-1042 Oct 2000  Bar No y  A D Dole v  C Dw ork and H R Strong Shifting Gears Changing Algorithms on the Fly to Expedite Byzantine Agreement  Information and Computation Vol 97 pp 205-233 1992  D  R  Cox Regression models and life tables  Journal of the Royal Statistical Society Series B Methodological Vol 34 No 2 pp 187-220 1972  S Elbaum and J Munson Intrusion Detection Through Dynamic Software Measurement  Proceedings of the Eighth USENIX Security Symposium 1999  R J Ellison D A Fisher  R  C  Linger  H  F  Lipson T  Longstaff and N R Mead Survivable Network Systems An Emerging Discipline  Technical Report CMU/SEI97-TR-013 November 1997 Revised May 1999  A Krings et al A Two-Layer Approach to Survivability of Networked Computing Systems  Proc International Conference on Advances in Infrastructure for Electronic Business Science and Education on the Internet SSGRR2001 LAquila Italy Aug 06 Aug 12 pp 1-12 2001  A Krings Survivable Systems  Chapter 5 in Information Assurance Dependability and Security in Networked Systems Morgan Kaufmann Publishers Yi Qian James Joshi David Tipper and Prashant Krishnamurthy Editors in press 2008  Ax el Krings Design for Survivability A Tradeoff Space  Proc 4th Cyber Security and Information Intelligence Research Workshop Oak Ridge National Laboratory May 12-14 2008  L Lamport et.al The Byzantine Generals Problem  ACM Transactions on Programming Languages and Systems Vol 4 No 3 pp 382-401 July 1982  J.C Laprie editor  Dependability Basic Concepts and Terminology  Springer-Verlag 1992  Y  Liu and K S T r i v edi Survivability Quanti“cation The Analytical Modeling Approach  International Journal of Performability Engineering Vol 2 No 1 Jan 2006 pp 29-44  Z.S Ma New Approaches to Reliability and Survivability with Survival Analysis Dynamic Hybrid Fault Models and Evolutionary Game Theory  Ph.D dissertation University of Idaho Computer Science Department 177pp 2008  Z.S Ma and A W  Krings Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management PHM  Proc IEEE AeroSpace Conference March 1-8 Big Sky MT 2008  Z.S Ma and A W  Krings Competing Risks Analysis of Reliability Survivability and Prognostics and Health Management PHM  Proc IEEE AeroSpace Conference March 1-8 Big Sky MT 2008  Z.S Ma and A W  Krings Multivariate Survival Analysis I Shared Frailty Approaches to Reliability and Dependence Modeling  Proc IEEE AeroSpace Conference March 1-8 Big Sky MT 2008  Z S Ma and A W  Krings Dynamic Hybrid Fault Models and the Applications to Wireless Sensor Networks WSNs  to appear in The 11-th ACM International Symposium on Modeling Analysis and Simulation of Wireless and Mobile Systems ACM MSWiM 2008 2008  T  Martinussen and T  H Scheik e Dynamic Regression Models for Survival Data  Springer Verlag 466pp 2006  N R Mead R J Ellison R C Linger  T  Longstaf f and J McHugh Survivable Network Analysis Method  Technical Report CMU/SEI-2000-TR-013 Software Engineering Institute Carnegie Mellon 2000  P  Thambidurai and Y K P ark Interactive Consistency with Multiple Failure Modes  Proc 7th Symp on Reliable Distributed Systems Columbus OH pp 93100 Oct 1988  T  Therneau and P  Grambsch Modeling Survival Data Extending the Cox Model  Springer Verlag 2000  Jay J W ylie et.al Selecting the Right Data Distribution Scheme for a Survivable Storage System  Technical Report CMU-CS-01-120 Carnegie Mellon University May 2001 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


themselves the role of ?facilitator? to involve a wide range of stakeholders had been important for Mayo?s cultural focus on continuous improvement. In particular, taking a ?bottom up? approach to allowing interested practitioners to get involved had helped to gain additional stakeholder involvement  4.2.3. Governance Level Findings  Use of Contracts ? Within the Rochester, MN area, the Mayo Clinic controls almost the entire endto-end provision of service, from 911 medical phone calls, to air or ground ambulance response, to trauma care.  Outside of the Rochester area there is mixed control including first responders as well as subcontracted and volunteer ambulance providers.  As such, the larger, regional emergency medical services EMS throughout the United States, constructed of many different, loosely coupled, and sometimes competing organizations. When outside the boundaries of control, the Mayo Clinic is faced with the financial and technical aspects related to inter-organizational information sharing and the need for contracts to enforce service performance levels. End user participants discussed the importance of these contracts for ensuring information exchange across provider organizations Non-contract information sharing ? Networks of cooperating and collaborating organizations outside of the Mayo system have been established for the purpose of infusing the Mayo Clinic?s philosophy of high quality patient care. Information sharing and service cooperation often takes place without formal binding contracts in place. This is often at the expense of the Mayo Clinic. But the organizational philosophy is that care provision at the Clinic will be enhanced if inter-connected care providers work like they do ? with a focus on quality patient care  4.3. State Level Stakeholder Focus Group and Evaluation  While the local level case study provided valuable insight, researchers also sought to understand practitioner perspectives from the State level including how crash, EMS, and trauma information is currently being integrated and utilized in the State of Minnesota and challenges and benefits to further integration. Moreover, the discussion focused on how to conceptualize the ?next-generation? system that would not only facilitate the analysis of large archival data sets, but also allow for more real-time analysis Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 from a clinical standpoint. A series of 2 focus group discussions and 3 follow-on interviews were conducted with 16 state level Agency decision makers. Participants represented the following organizations: Emergency Medical Services Regulatory Board \(EMSRB State Trauma System and State Trauma Advisory Council Intelligent Transportation Systems Program and Office of Traffic Safety Traffic Safety  4.3.1 Summary of State Level Focus Group and Interview Findings  Participants discussed that in order for a complete end-to-end view of patient and incident information 


end-to-end view of patient and incident information to provide value to a wide range of consumers, there would need to be data assimilated from a wide range of existing organizations and information systems These data sources would include: telematics providers \(data from automatic crash notification systems ambulance providers, emergency departments trauma centers, public health \(e.g., rehabilitation databases e.g., crash analysis reporting systems e.g., traffic management systems illustrated in Figure 6 Participants discussed operational challenges to achieving this vision, including several data collection issues. For example, there is a lack of state level information collection policies. Paramedics also experience difficulty inputting data due to their need to focus on patient care in time-critical situations The lack of wireless infrastructure can often poses system connectivity problems as well. As such, and to the dissatisfaction of physicians, the electronic incident report from the paramedic is typically received well after a patient has arrived to the ED While many obstacles were discussed, participants agreed that there exists substantial motivation to progress towards a more open, standardized integrated, yet secure and private, information sharing environment. Potential benefits discussed included reducing emergency response times improving decision making capacities, improving patient care, reducing crashes, improving emergency response management capabilities, and reducing disability consequences, fatalities, and associated costs across the State For the purpose of research, participants agreed to share data for the advancement of a ?proof of concept? project. Furthermore, participants supported a research oriented and multi-phased approach understanding the magnitude of such an undertaking and long-term vision of integrated data sharing across the State.  Participants noted the research approach should include stakeholders from across the full spectrum of crash and emergency response organizations, integrate with existing state policy programs, such as the ?Toward Zero Deaths program \(a statewide cooperative to reduce annual traffic fatalities to zero of physicians to provide a ?clinical? perspective on data sharing, and take a multi-phased approach to development beginning with architecture development, prototype creation and testing, and demonstration of a ?proof of concept? as an example of the eventual larger imlementation In sum, focus group participants validated case study findings in terms of issues, challenges and potential solutions for constructing a patient focused end-to-end trauma information system  5. The Need for an Integrated Crash Trauma Information Network \(ICTN  Taking the literature review and Minnesota case study work together, findings indicated the need for a more integrated, clinician focused enterprise model for information system design and data sharing Figures 5 and 6 illustrate an overview of the Integrated Crash Trauma Information Network ICTIN account key features that would be included in the system design. This concept is discussed below 1. At the top level of Figure 5, there are a number 


of end-to-end operational process considerations to provide emergency medical care as seen through the eyes and experience of a patient. The top level of this diagram represents a linear work flow of a patient from emergency notification \(e.g., ACN, 911 phone call provision, and arrival and definitive care provision at an emergency department and/or trauma center Based on findings, emergency responder processes may be viewed as dynamic or sequential. But the sequential representation is meant to take into account the patient experience, an important system design characteristic as described by Schooley and Horan [18 2. As shown in the second level \(from the top Figure 5, a multi-organizational view of the system architecture is an essential consideration for the ICTIN concept. As described throughout this paper and illustrated in Figure 6, many organizations are involved in emergency response activities, from emergency notification through care provision. Many other practitioners and organizations are involved in Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 crash analysis, public health research, patient outcome research, and other analyses. Therefore, the architecture needs to accommodate the information and data needs of a wide variety of organizations including authorized access to information, while maintaining the privacy and security of patient information. Fortunately, the technology exists to enable these software quality attributes.  How to implement such attributes in a complex large-scale environment is a challenge yet to be resolved 3. Dynamic information sharing considerations are represented on the third and fourth levels of Figure 5 From this analysis, and prior research, a number of information types, or taxonomies, are captured analyzed, and distributed across EMS organizational actors and hardware and software systems. The view represented here is one that would allow for information to be fully utilized by authorized emergency responders \(and information systems downstream? points of an emergency response episode. For example, rather than waiting for a paramedic to click the ?submit report? button of an electronic patient care record, patient information would be dynamically \(and incrementally physicians at a receiving ED or trauma center. To enable such open systems, web services and related information architecture standards would need to be implemented across organizations and information systems so that both ?push? and ?pull? functionality could exist for any and all authorized users 4. End-to-end performance reporting capabilities across organizations and information systems are also enabled through such an architecture. The ability to pull any number of data elements not  Figure 5. Information Coordination across the Emergency Response Process  only benefits real-time clinical decision making, but would also allow for the creation of customized reports for real-time monitoring \(e.g., dashboards retrospective research and analysis. The idea here is that performance analysis becomes a system design consideration at the outset, as opposed to taking the traditional approach of constructing performance reports after the system has been built. As shown in Figure 6, reporting and analytics would be a key component of the ICTIN 5. Additional enterprise architecture characteristics 


5. Additional enterprise architecture characteristics are illustrated in Figure 6. Though alluded to in the above discussion, these include a. Security/Privacy: A standard suite of network software, and data security measures would be implemented to ensure safe transport of patient information in accordance with State and National privacy guidelines and laws Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 b. Patient tracking system: In order to accurately identify data from one system to another, there must be a common data identifier that could associate data with a patient, including vehicle, incident, medical information, care provision, receiving hospital medications administered, dispatched emergency resources, time of phone call, etc. The data would all be linked back to the patient for real-time and retrospective analysis. A patient tracking system would facilitate this core function of the ICTIN c. Directory and Access Services: This would be one or several secure databases that include a listing of all authorized organizations and individuals allowed to access ICTIN information. Individuals would register,,be approved by the managing organization, and then allowed access to certain information based on the information access policies prescribed by the managing body. The directory is controlled by a managing entity to be determined by the network stakeholders  5.1. Directions for Subsequent Phases  While there exists a need for an integrated crash trauma information network \(CTIN a paucity of guidance, literature, and directions on how to achieve such a complex system implementation.  As confirmed by the local and state focus group discussions in Minnesota, further exploration into the potential and feasibility of developing an integrated crash trauma information network would provide an innovative advancement from both a research and practice perspective. More specific recommendations for taking a phased and incremental research approach to move the concept forward are discussed below In our phased approach, several methods are being employed.  A first task would be to validate the Minnesota case studies by conducting a comparative case study review in another state. Findings from a cross-case comparison would be used to outline the parameters for an initial prototype of the integrated crash information system. The prototype would be a simplified ?sample? system to provide a ?proof of concept? and illustrate how crash information could be shared from the moment of impact, through emergency dispatch and response, and then into the emergency room and health treatment services. A key component of this prototype will be the integration of data around the patient. Our straw man system CrashHelp series of interviews and focus groups with significant stakeholders in the process, including departments of transportation, public safety, 911, emergency services, and healthcare. Small data sets from crash EMS, health information systems would be collected and used to develop and populate the prototype to demonstrate its utility for safety decision support and planning purposes.  Feedback analysis from policymakers, planners, public health, EMS, safety engineers, emergency planners and citizens will focus on operational, organizational, and policy 


on operational, organizational, and policy deployment challenges surrounding an enhanced EMS system, including possible benefits from its utilization  Figure 6. Organizations and Integrated Crash Trauma Information Network Services  6. Expected Benefits &amp; Conclusion  The Trauma network would organize the information around the patient, providing a user/consumer centric approach to information design and use. In doing so, the trauma network would extend current EMS systems to include a greater range of information, including information that has been requested by emergency room physicians but often not available. This information is expected to improve not only the timeliness, but also the quality of the emergency response and improve patient care An end-to-end trauma network would allow for more holistic data analysis that can be both visualized and conducted in real-time. Just as private sector operations have achieved benefits through integrated customer systems \(e.g., CRM apply to the victim of a trauma. In this case, the issue is not just better customer service but fundamental issues of life and death  8. References  1] NHTSA, Emergency Medical Services: 24/7 Care Everywhere: National Highway Traffic Safety Administration, 2007 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 2] NHTSA, "Fatality Analysis Reporting System." vol 2008: National Highway Transportation Safety Administration, 2008 3] D. Trunkey, "Trauma," Scientific American, vol. 249 pp. 28-35, 1983 4] D. C. Grossman, Kim, A., Macdonald, S.C., Klein, P Copass, M.K., Maier, R.V, "Urban-rural differences in prehospital care of major trauma," The Journal of Trauma vol. 42, pp. 723-729, 1997 5] B. Schooley, T. Horan, and M. Marich, "User Perspectives on the Minnesota Inter-organizational Mayday Information System," in AMIS Monograph Series: Volume on Information Systems for Emergency Management, Van De Valle and Turoff, Eds.: IDEA Press, 2008 6] J. Peters, Hall, B., "Assessment of ambulance response performance using a geographic information system Social Science and Medicine, vol. 49, pp. 1551-1566, 1999 7] M. Jason S. Shapiro, c, Joseph Kannry, MDb, Andre W Kushnirukd, Gilad Kuperman, MD, PhDa,e "Emergency Physicians? Perceptions of Health Information Exchange pp. 700-705, 2007 8] G. Mears, J. Ornato, and D. Dawson, "Emergency Medical Services Information Systems and A Future EMS National Database," in Turtle Creek Conference III, Dallas TX, 2001 9] T.A. Horan and B. Schooley, "Time-critical information services," Commun. ACM, vol. 50, pp. 73-78, 2007 10] Joint Advisory Committee on Communications Capabilities of Emergency Medical and Public Health Care Facilities \(JAC February 4, 2008 2008 11] M. Turoff, Chumer, M., Van de Walle, B., Yao, X The Design of a Dynamic Emergency Response Management Information System \(DERMIS Information Technology Theory and Application December 25, 2003 2004 


December 25, 2003 2004 12] S. Sawyer, Tapia, A., Pesheck, L., and Davenport, J Mobility and the First Responder," Communications of the ACM, vol. 47, pp. 61-64, 2004 13] Institute of Medicine \(IOM Services: At the Crossroads," National Academy Press Washington, D.C. 2006 14] M. T. D. Ye K, Knott JC, Dent A, MacBean CE Handover in the emergency department: Deficiencies and adverse effects.," Emergency Medicine Australasia, vol. 19 pp. 433-41, 2007 15] T. Taylor, "Information management in the emergency department," Emerg Med Clin North Am, vol. 22, pp. 24157, 2004 16] A. Andrew Stiell, Ian G. Stiell and Carl van Walraven Prevalence of information gaps in the emergency department and the effect on patient outcomes " Canadian Medical Association, p. 169, November 11th 2003 17] T. A. Horan and B. L. Schooley, "Time-Critical Information Services," Communications of the ACM, vol 50, pp. 73-78, 2007 18] B. Schooley and T. Horan, "End-to-End Enterprise Performance Management in the Public Sector through Inter-organizational Information Integration," Government Information Quarterly, vol. 24, pp. 755-784, 2007 19] P. Kensaku Kawamoto, and David F. Lobach, MD PhD, MS2 "Proposal for Fulfilling Strategic Objectives of the U.S. Roadmap for National Action on Decision Support through a Service-oriented Architecture Leveraging HL7 Services," in American Medical Informatics Association vol. 14: American Medical Informatics Association 2007 pp. 146-155 20] J. Salinas, McManus, J., Convertino, V., Holcomb, J Hospital Data Warehousing System for Mining of Patient Data," in Ninth IASTED International Conference Software Engineering and Applications, Phoenix, AZ, 2005, pp. 1318 21] L. Carver, Turoff, M., "Human-Computer Interaction The Human and Computer as a Team in Emergency Management Information Systems," Communications of the ACM, vol. 50, pp. 33-38, 2007 22] Battelle Company, "Evaluation of the MAYDAY/9-11 Field Operational Test," Battelle, Maryland July 19, 2006 2006 23] W. M. Evanco, "The potential impact of rural mayday systems on vehicular crash fatalities," Accident Analysis &amp Prevention, vol. 31, pp. 455-462, 1999 24] NHTSA, "Crash Outcome Data Evaluation Systems CODES 2008 25] D. E. Clark and B. M. Cushing, "Predicted effect of automatic crash notification on traffic mortality," Accident Analysis &amp; Prevention, vol. 34, pp. 507-513, 2002 26] NHTSA, "Next Generation 9-1-1 System Concept of Operations \(NG911 ConOps Ed., 2005 27] D. N. Hatfield, "A Report on Technical and Operational Issues Impacting The Provision of Wireless Enhanced 911 Services," Federal Communications Commission, Washington, D.C. 2002 28] L. K. Moore, "An Emergency Communications Safety Net: Integrating 911 and Other Services," Congressional Research Service, Washington, D.C. Feb 28, 2008 29] I. o. M. \(IOM crossroads. Washington, DC: National Academy Press 2006 30] N. E. N. A. \(NENA effectiveness, accessibility and future of America's 9-1-1 service. Columbus, Ohio: The RCN Commission and National Emergency Number Association, 2001 31] NEMSIS, "National EMS Information System Helping Unify EMS Data," 2008 


Helping Unify EMS Data," 2008 32] N. Mann, K. Guice, L. Cassidy, D. Wright, J. Koury Are Statewide Trauma Registries Comparable? Reaching for a National Trauma Dataset," Society for Academic Emergency Medicine, vol. 13, pp. 946-953, 2006 33] R. Fantus, Fildes, J., "Deposit the Bull's-eye," Bulletin of the American College of Surgeons, vol. 91, p. 2, 2006 34] B. Schooley, M. Marich, and T. Horan, "Devising an architecture for time-critical information services: interorganizational performance data components for emergency medical service \(EMS 8th annual international conference on Digital government research: bridging disciplines \\&amp; domains Philadelphia Pennsylvania: Digital Government Research Center, 2007  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


 21] T.A. Pardo, H.J. Scholl., ?Walking Atop the Cliffs Avoid Failure and Reducing Risk in Large Scale EGovernment Projects, Proceeding of the 35th Hawaii International Conference on System Sciences-2002, IEEE Computer Society  22] T. L. Dinh and T.H. Le, ?Toward an Approach for Modeling Interoperability of Information Systems?, IEEE Research, Innovation and Vision for the Future, March 5 2007, pp.22-28  23] UN/CEFACT Core Components Technical Specification, Part 8 of the ebXML Framework, Version 2.01, Retrieved January 25, 2007 from http://www.unece.org/cefact/ebxml/CCTS_V2-01_Final.pdf  24] UN/CEFACT, ?UN/CEFACT Modeling Methodology UMM  25] UN/CEFACT ?Recommendation 34 Data Simplification and Standardization for International Trade?, Working DraftVersion 6.0, May 21, 2008 26]   UN/CEFACT ?XML Naming and Design Rules \(XML NDR   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 pre></body></html 


6] Eckerson, W.W., Data Quality and the Bottom Line Achieving Business Success through a Commitment to High Quality Data. TDWI, Chatsworth, 2002  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 7] Elliott, T., Implementing Business Intelligence Standards. BusinessObjects, 2004  8] English, L.P., Improving Data Warehouse and Business Information Quality: Methods for Reducing Costs and Increasing Profits, Wiley Computer, New York et al., 1999  9] Foshay, N., Best Practices in Business Intelligence Strategy. Blue Hammock, 2006  10] Friedman, T. and B. Hostmann, Management Update The Cornerstones of Business Intelligence Excellence Gartner Research G00120819, 2004  11] Gonzales, M., Creating a BI Stragey Document. DM Review, 2004\(November  12] Henderson, J.C. and N. Venkatraman, Strategic alignment: Leveraging information technology for transforming organizations. IBM Systems Journal, 32\(1  13] Hoffmann, O., Performance Management - Systeme und Implementierungsans  tze. 3 ed, Haupt, Bern et al 2002  14] Klesse, M. and R. Winter, Organizational Forms of Data Warehousing: An Explorative Analysis. in: IEEE Computer Society, Proceedings of the 40th Hawaii International Conference on System Sciences \(HICSS-40 Alamitos, 2007  15] Laudon, J. and K. Laudon, Management Information Systems: Managing the Digital Firm. 10 ed, Prentice Hall 2006  16] Losey, R., Enterprise Data Warehouse Strategy: Articulating the Vision. Dm Review, 2003\(January  17] Luftman, J.N. and R. Kempaiah, Key Issues For IT Executives 2007. MISQ Executive, 7\(2  18] MAIS and AIMS, A Business Intelligence Strategy Proposal for The University of Michigan. 2005  19] Melchert, F., Metadatenmanagement im Data Warehousing. Ergebnisse einer empirischen Studie. Institut f  r Wirtschaftsinformatik, Universit  t St. Gallen, 2004  20] Mosley, M., DAMA-DMBOK Functional Framework Version 3. DAMA International, 2008  21] Olszak, C.M. and E. Ziemba, Business Intelligence as a Key to Management of an Enterprise. in: Informing Science Institute, Informing Science + Information Technology Education, Pori, Finland, 2003  22] R  egg-St  rm, J., The New St. Gallen Management Model: Basic Categories of an Approach to Integrated Management, Palgrave Macmillan, Basingstoke, NY, 2005  23] Sommer, T., et al., Business Intelligence-Strategie bei der Volkswagen AG. in: Integrierte Informationslogistik B. Dinter and R. Winter, Editors, 2008, Springer, Berlin Heidelberg. pp. 261-284  


 24] Subramaniam, A., et al., Strategic planning for Data warehousing. Information &amp; Management, 33, 1997, pp 99-113  25] Totok, A., Entwicklung einer Business-IntelligenceStrategie. in: Analytische Informationssysteme - Business Intelligence-Technologien und -Anwendungen, P. Chamoni and P. Gluchowski, Editors, 2006, Springer, Berlin et al pp. 51-70  26] Vaduva, A. and T. Vetterli, Metadata Management for Data Warehousing: An Overview. International Journal of Cooperative Information Systems, 10\(3 298  27] Watson, H.J., D.L. Goodhue, and B.H. Wixom, The benefits of data warehousing: why some organizations realize exceptional payoffs. Information &amp; Management 39\(6  28] Watson, H.J., C. Fuller, and T. Ariyachandra, Data warehouse governance: best practices at Blue Cross and Blue Shield of North Carolina. Decision Support Systems 38\(3  29] Winter, R. and M. Meyer, Organization Of Data Warehousing In Large Service Companies: A Matrix Approach Based On Data Ownership and Competence Centers. Proceedings of the Seventh Americas Conference on Information Systems \(AMCIS 2001  30] Winter, R., Enterprise-wide Information Logistics Conceptual Foundations, Technology Enablers, and Management Challenges. ITI2008, 2008  31] Zarnekow, R., W. Brenner, and U. Pilgram, Integrated Information Management. Applying Successful Industrial Concepts in IT. 1 ed, Springer, Berlin, 2006  32] Zeid, A., Your BI Competency Center: A Blueprint for Successful Deployment. Business Intelligence Journal 11\(3    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207–216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Int’l Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Int’l Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





