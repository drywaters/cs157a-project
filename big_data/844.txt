A Comparison of Performance between Two Cluster Algorithms A pplied to Mineral Spectra  Robert Hogan Bay Area Environmental Research Institute c/o NASA Ames Research Center MS 245-3 Moffett Field CA 94035-1000 650-604-0780 Robert.C.Hogan@nasa.gov Giuseppe A Marzo NASA Post-Doctoral Program NASA Ames Research Center MS 245-3 Moffett Field CA 94035-1000 650-604-0780 Giuseppe.A.Marzo@nasa.gov Ted L Roush NASA Ames Research Center MS 245-3 Moffett Field CA 94035-1000 650-604-3526 Ted.L.Roush@nasa.gov 
Abstract\227 The K-means  KM  and Self-Organizing Map  SOM  are two popular and very different techniques for clustering data Both techniques require a set of training data which are used in an iterative process to 002nd clusters in this set For the KM  the number of clusters must be preassigned before a training session begins whereas the number of SOM clusters is determined after a single training is completed In this paper we compare the clustering performance of these two methods using data from three mineral spectral libraries whose samples have been hierarchically labeled with 
Class Subclass and Group names These names are used to determine the overall mineralogical purity of the clusterings as a function of cluster number The degree of cluster overlap is also determined as a function of cluster number using the Davies-Bouldin  DB  index We show that in general the purity and overlap of KM and SOM derived clusters differ signi\002cantly for cluster numbers small compared to the number of training samples The KM clusters are less pure and overlap more than SOM clusters The rami\002cations of these results on the accuracy of classi\002cation of spectra not used 
for training is discussed T ABLE OF C ONTENTS 1 I NTRODUCTION                                    1 2 A PPROACH                                         2 978-1-4244-2622-5/09 2 5  00 
c r 2009 IEEE IEEEAC Paper 1451 Updated 12 January 2009 3 D ATA U SED IN THE A NALYSIS                     3 4 E VALUATION C RITERIA AND R ESULTS            3 5 S UMMARY AND D ISCUSSION                      
6 A CKNOWLEDGEMENTS                            7 R EFERENCES                                      7 B IOGRAPHY                                        7 1 I NTRODUCTION 
The KM and SOM are examples of unsupervised clustering algorithms which have been applied to spectroscopic measurements 2 W ith lar ge spectral data sets compositional interpretative efforts can be greatly facilitated by an automated classi\002cation and identi\002cation scheme Such a scheme would be an essential element of intelligent remote sensing systems that must be autonomously capable of responding to the scienti\002c content of sensor data Clustering is an integral part of this scheme A detailed comparison of the clustering produced the KM and SOM would therefore be an important contribution to the development of intelligent 
software to organize recognize and interpret spectral data In this study we compare the clustering abilities of the KM and SOM when applied to mineral spectra from three libraries that provide detailed information on the chemistry and grain size of the samples measured This information is used to label the spectra with names that describe the samples in different levels of detail The clustering of these spectra is evaluated in a scienti\002cally meaningful way by assessing the 1 


distribution of these labels within the resulting clusters An additional evaluation uses the Davies-Bouldin  DB  cluster validation index as a more objecti v e means to measure the degree of cluster overlap from their separations and radii 2 A PPROACH Given a set of objects we de\002ned a K clustering of this set as one instance of a partitioning of this set into K disjoint subsets or clusters A clustering algorithm attempts to 002nd the optimal partition usually by minimizing some variance criterion applied to the cluster objects In this paper the set of objects studied are spectra They are represented as vectors in a N dimensional space 2 V N  where N is the number of spectral channels and the value of a vector component is the spectral measurement  re\003ectivity or emissivity  The KM and SOM can be viewed as neural networks with an input and output layer of nodes They both have a training phase where a set of N T vectors  training vectors  are used to modify an output set of M vectors 2 V N assigned to M output nodes A training vector is presented to N nodes in the input layer and associated with an output node through a process called activation The corresponding output node vector is then updated according to some rule The clustering algorithms differ in two respects 1 the number and structure of the output nodes and 2 the update rules KM Clustering The KM clustering scheme used in is study is described in  and its applications to remote sensing spectra can be found in A training v ector presented at the N input nodes activates the output node whose associated vector is closest in the Euclidean sense to the training vector The activated node vector is updated with the rule V new i  V old i  021  V T 000 V old i  1 where V i is the activated node vector V T is the training vector that activated the node and 021 is a small positive quantity called the learning rate which is decreased in a linear fashion as the training progresses The clustering scheme is performed in the following steps 017 1 De\002ne M output node vectors 2 V N  017 2 Determine the output nodes activated by the training vectors 017 3 Update the activated node vectors with the rule 1 017 4 Repeat steps 2 and 3 while decreasing 021 in a linear fashion until the output vectors have equilibrated 017 5 Associate the M equilibrated output vectors with vectors in the training set The subset of training vectors associated with a single output vector is regarded as a cluster 2 V N SOM Clustering A description of the SOM scheme used in this study and it applications to several spectral libraries can be found in  and An e xtensi v e bibliograph y of articles describing the SOM and its applications can also be found at The SOM has an input layer of N nodes and an output layer of M nodes usually arranged in a 1-D or 2-D regular array Initially each output node is assigned a vector 2 V N  A training vector presented to the N input nodes activates the output node whose vector is closest to this input vector During the training phase the entire training set is presented to the input nodes many times At a given time the activated node vectors and their surrounding node vectors are updated with the rule V new i  V old i  003 ij  V T 000 V old i  2 where V i is the vector associated with node i  V T is a training vector and 003 ij is 003 ij  013 exp 000 d 2 ij 2 033 2  3 where d ij is the distance between node i and an activated node j in the output layer and 013 and 033 are decreased in a linear fashion as the training progresses After the training phase is complete the separation between adjacent nodes is represented by a wall whose height is the distance between the node vectors The output layer is then partitioned into regions where each is surrounded by walls higher than a speci\002ed height and contains at least one node that is activated by the training set The number of regions is inversely related to the wall height The SOM clustering scheme is accomplished in the following steps 017 1 De\002ne M node vectors arranged in a square lattice where M 031 2 N T  017 2 Determined the output vectors which are activated by the input vectors 017 3 Update the activated node vectors and their neighbors with the rule 2 017 5 Repeat 3 and 4 decreasing 013 and 033 in a linear fashion until the output vectors equilibrate 017 6 Calculate the wall heights 017 7 Partition the output layer into K regions  where K is determined by a speci\002ed wall height The region vectors are associated with K clusters in V N In order to 002nd all K clusterings where K  1 to N T clusters the KM needs to be retrained because M  K  The 2 


SOM  however requires only a single training session because each K clustering is the result of partitioning the equilibrated output layer 3 D ATA U SED IN THE A NALYSIS The RELAB ASTER 10 and ASU 11 spectral data bases  circa 2004  of various minerals were used in this study to evaluate the performance of the KM and SOM  The RELAB and ASTER samples are re\003ectance measurements and the ASU samples are measured in emissivity Only the medium grained  45 125 026 m  RELAB and ASTER samples were used and the spectral range was limited to 4 6 026 m  The ASU samples are primarily coarse grained  710 1000 026 m  and measured in the spectral range 550 026 m The ASU was included in order to explore the clustering of spectra of minerals similar to the RELAB and ASTER samples but measured at wavelengths in the thermal infrared ASU Class Sub-class Group Oxide 3 X2O3 2 Hematite 2 XY2O4 1 Spinel 1 Salt 40 Carbonate 31 Aragonite 2 Calcite 19 Dolomite 7 Hydrous carbonate 3 Halide 1 Halide 1 Phosphate 3 Apatite 3 Sulfate 5 Anhydrous Sulfate 1 Hydrous Sulfate 4 Silicate 124 Inosilicate 58 Alkali Amphibole 1 Amphibole 2 Calcic Amphibole 9 Clinopyroxene 24 Orthopyroxene 15 Pyroxenoid 4 Sodic Amphibole 3 Nesosilicate 9 Al2SiO5 4 Garnet 2 Olivine 3 Phyllosilicate 37 Chlorite 2 Clay 29 Mica 3 Serpentine 3 Sorosilicate 1 Epidote 1 Tectosilicate 19 Alkali Feldspar 4 Plagioclase Feldspar 14 Quartz 1 Figure 1  ASU Sample distribution RELAB Medium Class Sub-class Group oxide-hydroxide 6 hydroxide 3 fe 3 oxide 3 hematite 1 spinel 2 salt 30 carbonate 25 anhydrous-carbonate 17 hydrous-carbonate 8 sulfate 5 anhydrous-sulfate 1 hydrous-sulfate 4 silicate 66 inosilicate 7 clinopyroxene 4 orthopyroxene 3 nesosilicate 11 olivine 11 phyllosilicate 41 chlorite 2 clay 30 mica 2 serpentine 7 tectosilicate 7 plagioclase-feldspar 1 sio2 6 Figure 2  RELAB Sample distribution Figures 1 2 and 3 summarize the breakdown of the library samples into a hierarchical labeling scheme The label names were taken partly from the database assignments to the Class and Sub-class keywords The label names are more descriptive of the sample's chemical and structural variations as one moves from the Class to Group label types All of these libraries are dominated by the primary rock forming silicate minerals In the next section we describe how this hierarchical labeling is used to evaluate the mineralogical content of the clusters in order to compare the performance of the KM and SOM clusterings 4 E VALUATION C RITERIA AND R ESULTS The distribution of label names in each cluster is tabulated in matrix form with one matrix for each label type We refer to this matrix as the CID matrix An example CID matrix for the Sub-class labels derived from the SOM clustering of the ASTER spectra is shown in Figure 4 The 002rst column is the cluster identi\002cation number and the next eleven columns indicate the frequency of occurrence for each label name The frequency distributions for each row are quanti\002ed with a measure of label purity constructed from the entropy function used in information theory This function e xpresses the degree of uncertainty in the distribution It is tabulated in the last column of the matrix shown in Figure 4 This measure for a single cluster is calculated as follows 3 


ASTER Medium Class Sub-class Group Oxides-hydroxides 10 hydroxides 1 mg 1 oxides 9 hematite 2 rutile 5 spinel 2 Salts 33 Carbonates 15 anhydrous-carbonate 12 hydrous-carbonate 3 Halides 5 chlorides 3 ruorides 2 Sulfates 13 anhydrous-sulfate 6 hydrous-sulfate 7 Silicates 61 Cyclosilicates 4 cyclosilicate 4 Inosilicates 12 amphibole 6 clinopyroxene 3 orthopyroxene 1 pyroxenoid 2 Nesosilicates 6 al2sio5 1 garnet 2 humite 1 olivine 1 zircon 1 Phyllosilicates 19 chlorite 6 clay 5 mica 7 serpentine 1 Sorosilicate 4 epidote 2 hemimorphite 1 idocrase 1 Tectosilicates 16 alkali-feldspar 2 feidspathsia 1 plagioclase-feldspar 6 sio2 5 zeolite 2 Figure 3  ASTER Sample distribution P urity  1  1 log N N X i 1 022 N i N s 023 log 022 N i N s 023 4 where N is number of names represented N i is the number of samples labeled with name i  and N s is the total number of samples in the cluster The ratio N i N s is the probability of occurrence of name i in the cluster and 4 is 1 the normalized entropy of the label probabilities When all names occur with the same frequency the purity is zero  e.g CID 16 and 25 in Figure 4  and when only one name is represented the purity is unity  e.g CID 27 in Figure 4  In the discussion that follows purity at a given cluster number CID Carbonates Cyclosilicates Halides Inosilicates Nesosilicates Phyllosilicates Sorosilicate Sulfates Tectosilicates hydroxides oxides Purity 1 1 4 2 3 2 1 0.07 2 1 1.00 3 9 1 1 0.45 4 1 1.00 5 1 1.00 6 1 1.00 7 3 1.00 8 2 2 2 0.00 9 3 1 1 10 0.31 10 3 1.00 11 1 1.00 12 1 1.00 13 1 1.00 14 1 1.00 15 2 1 2 7 1 0.19 16 1 1 0.00 17 1 1.00 18 1 1.00 19 1 1.00 20 1 1.00 21 1 1.00 22 2 1 1 1 6 0.20 23 1 1.00 24 2 1.00 25 1 1 1 1 0.00 26 1 1.00 27 2 1.00 28 1 1.00 29 2 1.00 30 1 1.00 Figure 4  CID Matrix derived from the clustering of the ASTER library is de\002ned as the average of the cluster purities In 002gure 5 we compare the purities derived from the KM clusters with those derived from the SOM clusters as a function of cluster number Results for the three label types and three spectral libaries are shown In general at a given cluster number the purities decrease as the label type gets more descriptive This trend was also observed in previous studies at optimal cluster numbers The KM and SOM purities converged as the cluster numbers approach their maximum values The differences are more signi\002cant at low cluster numbers where the SOM purities are higher than KM purities In an attempt to understand the purity differences more objectively we used DB to evaluate the quality of clustering For a given cluster i its center is de\002ned as the vector average of the cluster's vectors The separation S ij between two clusters i and j is then the distance between their centers The cluster radius R i is de\002ned as the average of the distances between the cluster's vectors and its center The DB is de\002ned 4 


Figure 5  Purity distributions for Class  top row  Sub-class  middle row  and Group  bottom row  labels 1 K K X i 1 m ax i 6  j 022 max  R i  R min   max  R j  R min  S ij 023 5 where R m in is a minimum radius introduced to give 002nite weight to single member clusters that would otherwise contribute nothing to the numerator in eqn 5 In practice it can be determined from an estimate of the measurement uncertainty per spectral channel or the systematic variations due to viewing geometry 003uctuations For this study we assumed a measurement uncertainty of 2 percent for the ASTER and RELAB and 1 percent for the ASU Small values of DB correspond to clusters that are compact and well separated In the upper three panels of 002gure 6 we plot the DB as a function of cluster number for the KM and SOM clusters derived from all three spectral libraries For all three libraries at low cluster numbers the KM clustering indicates a greater degree of overlap relative to the SOM clustering This difference is reduced as the cluster number is increased to its maximum When compared with Figure 5 it is clear that the SOM and KM purity differences are related to their DB differences as a function of cluster number In order to explore the underlying reason for the purity differences we computed the occurrence frequency of single member clusters as a function of cluster number The purity value for these clusters is unity regardless of the label type They would therefore signi\002cantly in\003uence the average purity of 5 


Figure 6  DB distributions upper row and frequency distributions of single member clusters lower row the clustering The frequency results are plotted in the lower 3 panels of Figure 6 where the differences are once again related to the purity differences The larger number of single member SOM clusters is in part what pushes the SOM purities above the KM values 5 S UMMARY AND D ISCUSSION We have compared the performance of two different clustering algorithms applied to mineral spectra taken from the RELAB and ASTER spectral libraries A hierarchical labeling scheme was used to characterize the compositional distribution in the clusters and a cluster validation index was used to evaluate the quality of the clustering We 002nd 017 1 The SOM and KM cluster differently when the number of clusters is small compared to the size of the spectral library 017 2 Measures of mineralogical content and overlap reveal that at low cluster numbers KM clusters are less pure and overlap more than SOM clusters 017 3 The higher SOM purities are primary due to the tendency of the SOM to to 002nd more single member clusters at low cluster numbers The implications of the results found in this study relate more to the role of clustering in observing schemes that autonomously classify and identify spectra of surfaces from a remote location In this application clustering is the 002rst step towards identi\002cation and classi\002cation is a intermediate step that associates unknown unlabeled spectra with clusters derived from a set of well-known labeled spectra the training set The mineralogical meaning of the clusters must be unambiguous in order to insure a high degree of accuracy in the 002nal identi\002cation of remotely measured spectra In a real world scenario a given spectrum is classi\002ed by associating it with the cluster center that resembles it the most The association of a spectrum with a low purity cluster is not very meaningful since the cluster center is not likely to be representative of a real spectrum In this sense the KM should perform poorly as a classi\002er relative to the SOM at cluster numbers small compared to the size of the training set The results shown in 002gure 5 suggest that at high cluster numbers the classi\002cation accuracy of the two algorithms should be comparable The SOM s ability to isolate more single member clusters in the small cluster number regime suggests that this algorithm may be better suited at detecting anomalous and perhaps scienti\002cally interesting spectra Such spectra would more likely be contained within the larger less pure KM clusters and would therefore escape detection if only the cluster centers are processed On the other hand the KM has been shown to separate high and low re\003ectance spectra at low cluster numbers The SOM would be capable of making this division only at high cluster numbers where regions in 6 


the output layer are surrounded by low wall heights The low re\003ectance spectra would presumably be divided into many clusters rather than just a few For large training sets and low cluster numbers the KM is clearly more computationally ef\002cient than the SOM because the number of output node vectors to update during the training phase is smaller Whether or not this computational advantage makes the KM more practical in an autonomous remote sensing application will depend on the required degree of classi\002cation accuracy The comparisons of performance between the KM and SOM presented in this paper help to address the tradeoff between speed and accuracy A CKNOWLEDGMENTS This research has been made possible via resources from NASA's Planetary Geology and Geophysics Mars Exploration and Intelligent Systems programs whose support is appreciated R EFERENCES  G A Marzo T  L Roush A Blanco S F onti and V Oro\002no 223Cluster analysis of planetary remote sensing spectral data,\224 Journal of Geophysical Research  vol 111 no E10 p 3002 Mar 2006  T  L Roush and R Hog an 223 Automated Classi\002cation of Visible and Near-Infrared Spectra Using SelfOrganizing Maps,\224 in IEEE 2004 Aerospace Conference  Big Sky Montana Mar 2006 paper 1456  D L Da vies and D W  Bouldin 223A Cluster Separation Measure,\224 IEEE Trans Pattern Anal Mach Intell  vol 1 pp 224\226227 1979  G A Marzo T  L Roush A Blanco S F onti and V Oro\002no 223Statistical exploration and volume reduction of planetary remote sensing spectral data,\224 J Geophys Res Planets  vol in press 2008  R C Hog an and T  L Roush 223Mineral emittance spectra Clustering and classi\002cation using self-organizing maps,\224 in IEEE 2009 Aerospace Conference  Big Sky Montana Mar 2009 paper 1459  T  L Roush J Helbert R C Hog an and A Maturilli 223Self-organizing map classi\002cation of the berlin emissivity data base,\224 in 38th Lunar Planet Sci Conference  Lunar and P Institute Eds Houston Texas 2008 CDROM abstract 2042  R C Hog an and T  L Roush 223SOM classi\002cation of TES data,\224 in 33th Lunar Planet Sci Conference  Lunar and P Institute Eds Houston Texas 2002 CD-ROM abstract 1693  W ebsite http://www cis.hut.\002/research/sombibl  C M Pieters and T  Hiroi 223RELAB Re\003ectance Experiment Laboratory A NASA Multiuser Spectroscopy Facility,\224 in Lunar and Planetary Institute Conference Abstracts  ser Lunar and Planetary Institute Conference Abstracts S Mackwell and E Stansbery Eds vol 35 Mar 2004 p 1720  W ebsite http://speclib jpl.nasa.go v  W ebsite http://speclib asu.edu  E T  Jan yes Probability Theory The Logic of Science  Cambridge U.K Cambridge University Press 2003 B IOGRAPHY Robert Hogan is a computer programmer at the Bay Area Environmental Research Institute working at NASA Ames He has over 30 years of experience applying numerical methods analysis and visualization techniques to a variety of theoretical and experimental data His educational background is in physics and mathematics and current research efforts are in particle/gas turbulent processes and data mining techniques for very large data sets Giuseppe Marzo is a research associate at NASA Ames His current research focuses on analysis of the surface and the atmosphere of Mars using multivariate statistical techniques He has a MS and PhD in Physics from the University of Salento in Italy Ted Roush is a space scientist at NASA Ames His research focuses on using re\003ectance and emittance spectra for compositional information of surfaces of solid objects throughout our solar system He has a BS in Geology from the University of Washington and MA and PhD in Geology and Geophysics from the University of Hawaii 7 


shapes We will also present more control surface analysis and control experiments to investigate more the suggested method A CKNOWLEDGMENT The author would like to acknowledge the support of ChristopherLynchforhishelpinperformingthe experiments reported in this paper R EFERENCES 1 H  H a g ra s   A h i e r a r c h i c a l ty p e 2 f u z z y l o g i c c o n tr o l a r c h it e c tu re fo r autonomous mobile robots IEEE Transactions on Fuzzy Systems  vol 12 no 4 pp 524Ö539 August 2004 2 H  H a g r a s  F  D o ct or  A  L o p ez a n d V  C a l l a gha n  A n i n c r e m e nt a l adaptive life long learning approach for type-2 fuzzy embedded agents in ambient intelligent environments IEEE Transactions on Fuzzy Systems  vol 15 no.1 pp 41-55 February 2007 3 N  K ar ni k J Mend el and Q  L i ang   T ype2 f uzzy l o g i c s ys t e m s   IEEE Transactions on Fuzzy Systems  vol 7 pp 643-658 December 1999 4 P L i n C  H s u a n d T  L e e  T y p e 2 f u z z y l o g i c c o n t r o l l e r d e s i g n f o r buck DC-DC converters Proceeding of the 2005 IEEE International Conference on Fuzzy Systems  Reno USA May 2005 pp 365-2370 5 F  L i u  a nd J M e nd el   A n i n t e r v al app r o ach t o F u zz i s t i cs f o r i n t e r v a l type-2 fuzzy sets Proceedings of the 2007 IEEE International Conference on Fuzzy Systems  London UK pp 1030-1035 6 C  L ync h H  H a g r a s and V  C a l l a gh an   E m be dde d t ype2 F L C f o r the speed control of marine and traction diesel engines Proceedings of the 2005 IEEE International Conference on Fuzzy Systems Reno USA May 2005 pp.347-353 7 C L y n c h H H a g r a s a n d V  C a l l a g h a n  E m b e d d e d i n t e r v a l t y p e 2 neuro-fuzzy speed controller for marine diesel engines Proceedings of the 2006 Information Processing and Management of Uncertainty in Knowledge-based Systems conference  Paris France July 2006 pp 1340-1347 8 C Lynch H Hagras and V Callaghan Using uncertainty bounds in the design of an embedded real-time type-2 neuro-fuzzy speed Controller for marine diesel engines Proceeding of the 2006 IEEE International Conference on Fuzzy Systems Vancouver,Canada,July 2006 pp 7217-7224 9 P  M e l in a n d O  C a s t i l l o   F u z z y l o g ic f o r p l a n t m o n i to r i n g a n d diagnostics Proceedings of the 2003 NAFIPS International Conference  July 2003 pp 20-25  J M e nde l  U n ce r t ai n R ul eB a s e d F uzzy L o g i c S ys t e m s  I n t r o d u c t i o n  New Directions Upper Saddle River New Jersey Prentice-Hall 2001  J  Me nde l a n d R  J o h n  T ype2 f u z z y s e t s m a de s i m p l e   IEEE Transactions on Fuzzy Systems  vol 10 pp 117-127 April 2002  J  M e nde l  F u z z y s e t s f o r w or ds  a n e w b e g i n n i ng   Proceedings of the 2003 IEEE International Conference on Fuzzy Systems St.Louis USA vol 1 May 2003 pp 37-42 13 J  M e nd el   C e n t r o i d a n d d e f u z z i f i e d va l u e o f a n i nt er va l t y p e 2 f u z z y set whose footprint of uncertainty is symmetrical Proceedings of the International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems IPMU 2004  July 2004  J  Mend el  H  W u C en t r o i d unce r t ai n t y b o u nds f o r i n t e r v al t y pe2 fuzzysets:forwardandinverseproblems Proceedings of the 2004 IEEE International Conference on Fuzzy Systems  Budapest Hungary vol 2 July 2004 pp.947-952  D  W u an d W  T an   A t y p e2 f uzzy l o g i c c on t r o l l e r f o r t h e l i q ui d level process Proceedings of the 2004 IEEE International Conference on Fuzzy Systems Budapest  Hungary vol 2 July 2004 pp 953-958  H  W u and J  Me nde l   U n ce r t ai n t y b o u nds a n d t h e i r u s e i n t h e d es i g n of interval type-2 fuzzy logic systems IEEE Transactions on Fuzzy Systems  vol.10 pp 622-639 October 2002  2008 IEEE International Conference on Fuzzy Systems FUZZ 2008 155 


  Final PU Checkout Data PU Product Initial PU Product Category PU Login PU Login Error locked shoppingCart.count\(products checkout=true checkout=true shoppingCart.count\(products checkout=true bound\(productcategory check\(username, password Figure 10 Maximum presentation unit state machine for the online shop action Thus interaction follows a step-by-step fashion as no parallelism of input or output is possible A multi-dimensional interface may be transformed to a serialized form in case there is not enough real estate As mentioned in serialized interaction has tw o problems First when the interaction path splits there is no simple choice which interaction object to activate next Second backward references in a dialogue may lead to confusion when using the interface To avoid this a clean composition of the interface is necessary The interface elements have to be sorted topologically This ordering fulìlls all dependencies as long as there are no cycles The rendering process of a concrete implementation of speech or a GUI based on Figure 6 must take the possible parallelization of the physical representation of the modality into account It usually partly serializes all parallel behavior to a modality-speciìc amount Different heuristics and the semantics of RST relations with respect to content support this serialization Interactions for a speech interface always have to be serialized When two or more dialogues are in parallel and are not serializable because they are RST alternatives  a menu presents all possible alternatives to the conversation partner in a serialized form 6 Discussion In this paper we have improved upon the variable binding approach presented in which we subsequently found to be unnatural to model with in speciìc modeling situations such as modeling login We have recognized that one of the root causes of this modeling problem lies in the variables and their binding being present only in the leaf nodes of the discourse model trees This does not make sufìcient use of the tree structure For example the whole shopping post-login subtree can only be instantiated if there is a user logged in and the only way to express this with the binding approach would have been to use the user variable in each and every leaf of the shopping subtree However few of these nodes actually need the user identity variable directly Introducing procedural constructs and assigning procedural semantics to RST relation nodes as illustrated in this paper allows the modeler to gain more powerful and generic control of the behavior of the resulting interface In the example above modeling in the root node of the shopping subtree has an impact on the whole subtree so the additional mentioning of a variable in each subtree leaf node can be avoided We also pursue a more generic approach than leaf binding annotations and relational node semantic assignment This approach involves assigning information to the tree branches in the form of conditional expressions We have began using such expressions already in this paper since procedural semantics of relations in themselves are not enough e.g a Condition relation needs a conditional expression and we are currently investigating more generic use of conditional expressions in the form of constraints on any tree branch Working with such conditional expressions is also more exible both because it allows for more complex conditions than the is bound vs is not bound distinction from the binding approach and because it needs fewer procedural constructs and less procedural interpretation of the RST relations As mentioned above we allow for an arbitrary degree of dialogue parallelization This includes completely speech or partially GUI serialized interfaces for input and output When a system based on our discourse engine is coupled with a number of rendering schemes for input or output serialized interfaces may be interpreted together to allow parallel behavior to a higher degree Thus a more exible communication ow is provided 7 Related Work A main part of any user interface is its behavior Many approaches have recognized that it is advantageous to go beyond programming of the user interface behavior and to use some kind of dynamic models for its speciìcation and Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


execution Most of these approaches utilize some kind of state machinery for this Book and Gruhn present an approach to describe the dialogue by dialogue graphs that are enhanced state machines They present the formal semantics of the Dialogue Flow Notation DFN that provides constructs for the design of modular navigation models These dialogue graphs are on the same level as our composite statecharts describing all possible dialogue paths The dialogue graphs have to be speciìed by the UI designer whereas our composite statecharts are automatically generated from the high-level discourse model ConcurTaskTrees are a w ay of hierarchical task modeling where different temporal relations among tasks can be speciìed e.g enabling disabling concurrency order independence and suspend-resume Out of such task models the user interface can be created through several steps of model transformations CTT also allows the derivation of presentation units consisting of a set of tasks and their transitions but their transition graph is more abstract than our presentation unit state machine skipping detailed trigger events for the transitions Elkoutbi et al present an approach that generates a user interface prototype from scenarios Scenarios are enriched with UI information and are automatically transformed into UML statecharts which are then used for UI prototype generation In contrast to this approach we model classes of dialogues supporting a set of scenarios We are not aware of any approach that would generate the behavior of user interfaces automatically from a largely declarative high-level discourse model In addition to taking a communication-centric approach and modeling humanmachine communication in the form of discourses we generate user interfaces automatically including their behavior 8 Conclusion In this paper we address the issue of automatic generation of the behavior of a user interface from a high-level discourse model While the generation seems to be possible from a purely declarative model the complete behavioral operationalization is facilitated by introducing behavioral constructs into the otherwise declarative model We deìned procedural semantics for both the procedural and the declarative constructs of the discourse model using statecharts Based on these building blocks for operationalization we showed how composite statecharts for whole discourse trees can be generated These directly deìne all possible behaviors of a fully serialized user interface e.g using speech input and output In contrast for a graphical user interface with potentially inìnite screen size we showed a derivation of a nite-state machine from a composite statechart for deìning its behavior The behavior of multimodal interfaces can be deìned in the spectrum between these extremes In this way we have deìned how the behavior of user interfaces can be automatically generated from a largely declarative high-level discourse model Taking this together with generating structural UI models enables the automatic generation of user interfaces from such discourse models 9 Acknowledgements This research has been carried out in the CommRob project  http://www.commrob.eu  and is partially funded by the EU contract number IST-045441 under the 6th framework programme References  C Bogdan J F alb H Kaindl S Ka v aldjian R Popp H Horacek E Arnautovic and A Szep Generating an abstract user interface from a discourse model inspired by human communication In Proceedings of the 41th Annual Hawaii International Conference on System Sciences HICSS-41  Piscataway NJ USA 2008 IEEE Computer Society Press  M Book and V  Gruhn Modeling Web-based dialog o ws for automatic dialog control In Proceedings of the 19th IEEE/ACM International Conference on Automated Software Engineering ASEê04  pages 100Ö109 Washington DC USA 2004 IEEE Computer Society  M Elk outbi I Khriss and R K K eller  Automated prototyping of user interfaces based on UML scenarios Automated Software Engg  13\(1 2006  S Ka v aldjian C Bogdan J F alb and H Kaindl T ransforming discourse models to structural user interface models In Models in Software Engineering LNCS  volume 5002/2008 pages 77Ö88 Springer Berlin  Heidelberg 2008  S K ost Dynamically Generated Multi-Modal Application Interfaces  PhD thesis TU Dresden 2006  P  Luf f N Gilbert and D Frohlich Computers and Conversation  Academic Press 1990  W  C Mann and S Thompson Rhetorical Structure Theory Toward a functional theory of text organization In Text  pages 243Ö281 1988  G Mori F  P aterno and C Santoro Design and de v elopment of multidevice user interfaces through multiple logical descriptions IEEE Transactions on Software Engineering  30\(8 8 2004  J R Searle Speech Acts An Essay in the Philosophy of Language  Cambridge University Press Cambridge England 1969 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


 11 This model specifies that covariates act multiplicatively on time t r than on the hazar d function.  That is, we assume a baseline hazard function to exist and that the effects of the covariates are to alter the rate at which an individual proceeds along the time axis.  In other words, the covariates z accelerates or decelerates the time to failure Kalbfleisch and Prentice 2002, Lawless 2003  It should be pointed out that the distribution-based regression models for exponential and Weibull distributions in the previous section are th e special cases of both PHM and AFT.  This correspondence is not necessarily true for models based on other distribu tions. Indeed, two-parameter Weibull distribution has the uniq ue property that it is closed under both multiplication of fa ilure time and multiplication of the hazard function by an arbitrary nonzero constant Lawless 2003, Kalbfleisch & Prentice 2002, Klein Moeschberger 2003  2.6. Counting Process and Survival Analysis   In the previous sections, we introduced censoring and survival analysis models th at can handle the censored information; however, we did not discuss how the censored information is processed.  Accommodating and maximally utilizing the partial information from the censored observations is the most challenging and also the most rewarding task in survival anal ysis.  This also establishes survival analysis as a unique fiel d in mathematical statistics Early statistical inferences for censored data in survival analysis were dependent on asymptotic likelihood theory Severini 2000\ Cox \(1972, 1975\ proposed partial likelihood as an extension to classical maximum likelihood estimation in the context of hi s proportional hazards model as a major contribution. Asymptotic likelihood has been and still is the dominant theory for developing survival analysis inference and hypothesis testing methods \(Klein and Moeschberger 2003, Severini 2000\. There are many monographs and textbooks of survival analysis containing sufficient details for applying survival analysis \(Cox and Oakes 1984, Kalbfleisch and Prentice 1980, 2002, Lawless 1982, 2003, Klein and Moeschberger, 2003\. A problem with traditional asymptotic lik elihood theory is that the resulting procedures can become very complicated when handling more complex censoring mechanisms \(Klein Moeschberger 2003\. A more elegant but requiring rigorous measure-theoretic probability theo ry is the approach with counting stochastic processes and the Martingale central limit theorems.  Indeed, this approach was used by Aalen 1975\ to set the rigorous mathematical foundation for survival analysis, and later further developed and summarized by Fleming and Harrington \(1991\, Andersen et al. \(1993\several research papers.  In reliability theory Aven and Jensen \(1999\ dem onstrated such an approach by developing a general failure model, which we briefly introduced in Section 1.2. However, the counting process and Martingale approach require measure theoretic treatments of probability and st ochastic processes, which is often not used in engineering or applied statistics.  A detailed introduction of the topic is obviously beyond the scope of this paper, and we only present a brief sketch of the most important concepts involved.  Readers are referred to the excellent monographs by Andersen et al. \(1993 Fleming and Harrington \(1991\ Aven and Jensen \(1999\ for comprehensive details, and Kal bfleisch and Prentice \(2002 Klein and Moeschberger \(2003\, Lawless \(2003\ for more applicationñoriented treatments The following discussion on this topic is drawn from Klein and Moeschberger \(2003  A counting stochastic process N  t  t 0 possesses the properties that N  0 ro and N  t   with probability one. The sample paths of N  t ht continuous and piecewise constant with jumps of size 1  step function In a right-censored sample, \(we assume only right censoring in this section N i  t  I  T i t  i   which keep the value 0 until individual i fails and then jump to 1  are counting processes. The accumulation of N i  t ocess     1 t N t N n i i is again a counting process, which counts the number of failures in the sample at or before time t   The counting process keeps track of the information on the occurrences of events,   for instance, the history information such as which individual was censored prior to time t and which individual died at or prior to time t as well as the covariates information This accumulated history information of the counting process at time t is termed filtration at time t denoted by F t For a given problem F t  rests on the observer of the counting process.  Thus, two observers with different recordings at different times will get different filtrations.  This is what Aven and Jensen 1999\ referred to as different information levels or the amount of actual available information about the state of a system may vary  If the failure times X i and censoring times C i  are independent,  then the probability of an event occurs at time t given the history just prior to t  F t\n be expressed as  t T if dt t h t C t X dt t C dt t X t P F dt t T t P i i i i i i r t i i r          1     t T if F dt t T t P i t i i r 0   1    51  Let dN  t be the change in the process N  t over a short time interval    t t t Ignoring the neglig ible chance of ties 1   t dN if a failure occurred and 0   t dN otherwise  Let Y  t denote the number of individuals with an observation time T i t Then the conditional expectation of dN  t   dt t h t Y F dt t C dt t X t with ns observatio of number E F t dN E t i i i t              52 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es ó the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollob·s, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSSí03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et MathÈmatiques AppliquÈes de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


