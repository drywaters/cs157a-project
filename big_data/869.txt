1  Standardized Failure Signature for a Turbofan Engine J\351r\364me Lacaille Snecma Rond-Point Ren\351 Ravaud \226 R\351au 77550 Moissy-Cramayel CEDEX - France 33 1 60 59 70 24 jerome.lacaille@snecma.fr  Abstract 227The capacity to master engines behavior is fundamental for a manufacturer to prove its efficiency in conception and maintenance capability. This understanding goes through the capacity to acquire and treat data flows produced by sensors for monitoring purposes One of many issues encountered by engine experts when 
reading measurements is that a given engine never works under exactly the same conditions. However, a practical abnormality detector must be able to check for failures in any possible context. Maintaining a monitoring application is expensive and one never wants to build specific computations for each flight ph ases and external conditions The purpose of this article is to present a normalization pretreatment which leads to the conception of standardized signatures. Those signatures continue to carry a clear meaning for experts but are independent of flight conditions With such a data pretreatment a classification of failures 
becomes possible and may be interpreted in physical terms by engineers. This will help to produce a degradation identification solution and adapt maintenance workflows This work is mainly illustrated by the failure classification and critical component identification for the start system of a civil engine 12  T ABLE OF C ONTENTS  1  I NTRODUCTION  1  2  M 
EASUREMENT ACQUISITION  1  3  D EPENDENCY NORMALIZATION  2  4  C RUDE ESTIMATION OF INDICATORS  2  5  A BNORMALITY DETECTION  4  6  F AILURE IDENTIFICATION  6  
7  C ONCLUSIONS  7  R EFERENCES  8  B IOGRAPHY  8  1  I NTRODUCTION  In this article we present a methodology and a set of algorithms to identify failures modes and defective components of a turbofan engine. This solution uses mathematic models, but also an important amount of 
expertise   1 978-1-4244-2622-5/09/$25.00 \2512009 IEEE 2 IEEEAC paper #1027, Versi on 2, Updated October 1 st 2008 From their knowledge the expert engineers define the inputs and configurations of two main patented algorithms 
200  CRN \(Context Removal and Normalization\: a standardization process that normalize observations to make them as if they were acquired always in the same context conditions \(points 1 \226 to 11 \226 
200  FDI \(Failure Detection and Identification\: a detection and classification algorithm that first diagnoses abnormalities and then identifies failures among a predefined list of possible degradation causes \(points 12 \226 to 18 \226  This low-level to high level diagnosis schema also proposes a methodology to help experts in the analysis process of each possible failure The two following graphics summarize briefly the main algorithm\222s phases with labels corresponding to the article paragraphs numbers 
 Figure 1: Synoptic of the two algorithms 2  M EASUREMENT ACQUISITION  1 \226  Sensor identification The first step in a monitoring application is the data acquisition. To supervise a specific system or a fundamental phase of the engine, one first capitalizes data flows acquired at different frequencies by many sensors The number of sensor is never huge in operational conditions. For example on a civil engine one may register a 


 2 dozen of performance sensors that read pressures, flows and, temperatures at a low frequency and even less high frequency measurements reading shaft speeds and vibrations 2 \226  Building indicators from raw measurements From those physical measurement experts will define specific indicators for each gi ven system. This work is formalized by a document ca lled FMECA \(Failure Modes Effects and Criticality Analysis\point 16 \226. In this document failures specific to a given system are listed and organized by main target components, with information about possible causes and effect s. Moreover each failure is described by the visible effect on specific results computed from measurements. Those results define methods \(also called pointers\ild the interesting indicators In the example of the start capability analysis, the experts will extract different durations and specific values or computations that have a clear meaning. Among those indicators one will find the delay between the opening of a fuel valve and the time the HP shaft reach a given speed One also will register the maximum acceleration of the HP core and many others similar indicators Figure 2 below shows some examples of characteristic points and the respective indicators built from input measurements  Figure 2: Computation of indicators for the start sequence \(time on the x-axis 3 \226  Experts define specific set of indicators The same kind of computation can be realized for each logical or physical system of the engine. A physical system is a real macro component like the HP core, the fan\205 A logical system refers to a group of components that may realize some specific task, like the start system, the lube system, the performance and so on For performance analysis, experts will focus on temperatures, pressures, and fuel flow. Then they build indicators of wear computing energy consumption, and trends from flight to flight For bearing analysis, the vibration specialists will isolate specific frequencies in the order domain \(time corresponding to rotation speed\at depend on the dimensions of the different el ements: radius of each rings diameters of the balls\205 3  D EPENDENCY NORMALIZATION  4 \226  Explore the context data During a flight each measurem ent is acquired with specific given conditions. Those conditions that may impact the reading of the indicators must also be registered External conditions may be the external temperature and pressure, the attitude and relative speed of the plane, but also the location, weather cond itions, hygrometry and so on But one may also register engine specific use, as a condition during the acquisition \(shaft speeds, exhaust temperatures fuel type, etc.\easurements will be quoted as context data 5 \226  Example of context data The graph below \(Figure 3\ shows the oil temperature just before an engine start. One immediately sees two kinds of temperatures that represent cold starts and hot starts. This is typically a case were an internal information \(the oil temperature\ must be considered as a context data. It clearly differentiates two classes of start procedures that the analysis algorithm must deal with  Figure 3: The oil temperature before start process. Each 223cycle\224 corresponds to a new start sequence 4  C RUDE ESTIMATION OF INDICATORS  6 \226  Observe indicator dependencies Indicators are dependent to c ontext but there exist also relations between indicators themselves. One cannot suppress directly dependency to context, indicator by indicator, because the result will certainly be the destruction of any information contained in the relations between the followed indicators. Engine experts are aware of the content of information contained in relations and know also the difficulty to build independent indicators  


 3 On the following 2D projection \(Figure 4\ one can observe the relation between the delay used to start the engine using the APU and the time need by the ignited engine to reach its nominal ground-idle speed There exists clearly a small relation between those two indicators, and this relation may certainly depend on context data like external temperature or the APU output pressure  Figure 4: Dependency between delay to gain enough speed to start from the APU and time to reach a given shaft speed \(ground idle speed\ignition The following process will suppress context dependency with no degradation of indicator relation. The cost will be a little change in the indicator meaning 7 \226  Build a series of dependent estimations For each indicator one will build a specific estimation with a regression projection on the other indicators and the context data. As the relations between indicators and context are definitely not linear, experts must give advices about the space where one will pr oject each indicator Let  1 m yyY 002   built from functions of context information X and of the other indicators Y except j y Then j y is projected onto  j E using a mathematic regression method. It can be a linear regression or any neural networks-like model. The result j y 210 is a crude estimation of j y taking context data and interdependent relations into account 8 \226  Specify projection surface for each indicator To build the preceding estimation one must at first define the space  j E on which to project the indicator. This is clearly the result of expert knowledge that identify from physical meaning of measurements the way data are related together. For example, a measurement may depend logarithmically of another one. Another classical example if state variables are known like energy or enthalpy, the way it may be computed helps to identify relations. If energy comes from the product of two indicators then the inverse of one may help to predict the other at a constant energy level Another way to define this projection space is to automatically build a feature space using a kernel representation. In that case a least-square support vector machine may be an adequate an swer.  This automatic neural solution is applied when no knowledge is available or if the prediction is really bad. In that case, most of the time, the interpretation of this new estimator helps experts to understand the physical meaning of the dependencies Moreover one also considers non-linear transformations of the indicator to regress because it may sometimes be easier to predict such transformation than the original value Experts will select each transformation from a list of invertible transformations \(log arithm, inverse, saturation\205 Once again, if no knowledge is available all acceptable transformations are tested and a selection is done according to a robustness criterion \(see below point 11 \226 9 \226  Practically build the crude estimation from a dataset To implement such estimation for each indicator one need a set of observations Y indicators\ and X context data good observations. \223Good\224 refer to measurements done on engine that have no problem. In fact this is an easy task because most of the time there is no problem when using an engine. Thus building a regression on normal conditions is easily feasible. As we can get a lot of such observations, the quality of the model can be optimized 10 \226  Observe normalized indicators Once the crude estimation done it is possible to look at a normalized observation  210   jjjj yyyy 212 built from the difference of the real observation to the prediction. The whole result is readjusted to normal mean conditions \(or defined standards This computation gives a new observation in the same dimensions than the initial indicators but it shows only the difference between what\222s observed and what\222s should be observed knowing the context data and other relations between indicators This way, if everything is normal, the difference between estimation and target remains small and the signal stays flat around its mean. \(Small corresponds to something under some multiple of the variance of the estimation error, which eventually may be important but is known anyway.\ The result is independent to context and local relations If something is not normal: context or relations don\222t explain it and some signals may look far from their nominal values The following graph \(Figure 5\hows on the top axis the mean gradient of the original exhaust gas temperature  be the vector of all indicators and X be the vector of context data. For each indicator j between 1 and m let  j Y represents all indicators but j We ask the experts to help define a spanned space   XYE j j 


212 obtained from least square minimization process Thus, they may reasonably be normalized according to a multivariate Gaussian hypothe sis. If their mean vector  4 measured for successive f lights. The result of the normalization process is on the bottom axis. This plot use only an engine in order of work and the disparities observed on the top graph are essentially the cause of different acquisition conditions  Figure 5: Original and normalized mean gradient of the exhaust gas temperature \(same y-scale for both graphs The green lines give the +/- 3-sigma levels around the mean value and the red ones give the +/- 6-sigma levels. In normal operation condition the probability to observe something outside 3 003   13 \226  Building some meaningful signatures The standardized indicators are computed from residuals yy 210  265 212\004 212 yz   2 1 which under good hypothesis may follow an approximately Gaussian distribution 0 I If the residual is clearly non-Gau ssian like, one may, after identification of the indica tor distribution replace the 002 is less than 3.10 3 and the probability to observe data outside 6 002 is lower than 2.10 9 The top plot clearly shows that data are not normalized which is no more the case on the bottom graph The next similar graph \(Figure 6\presents the delay to reach a maximal acceleration of the LP shaft when starting the engine. Five cycles between number 80 and 90 were a little slower to start. The normalization process erases this phenomenon because it is linked to extern conditions only On the other hand, for start number 67 which also was slower, the causes was not linked to the registered external conditions. The normalized delay is even higher than the original one and if the engine experts really identified the reasonable external condition of the start sequence, one clearly detects an abnormality  Figure 6: Time to reach maxi mum acceleration of the LP shaft during start process \(same y-scale for both graphs 11 \226  Ensure robustness in crude estimation Often experts building the dataset of normal observations forget some outliers. Start 67 was one of them. To ensure robustness of computation for the crude estimators one accept a small amount of outliers This is simply done in a two phases estimation process, the first step identify the outliers as the observations with lower likelihood according to each monovariate model distribution. The observations are sorted in descending order of the likelihood and one removes all points after a given percentile of the dataset. The second step computes the regression coefficients again but without the selected lowlevel of likelihood points The second way to ensure robust ness for a learnt model is to use a cross-validation process. In our case this is easy because of the amount of ava ilable normal operations and because our algorithm only models the normal behavior This method randomly selects a subset of operations for calibration and tests the computed results on other observations. This process is repeated for a lot of randomly selected calibration sets. The test must just ensure that the residuals are almost normal their distributions must be approximately gaussian. If not, the model is re-evaluated by an automatic selection of indicators or kernel templates or with a new expertise. The distribution of the residuals is estimated on test data only, so the mean and correlation coefficients are usable on new observations Note that a high value of re siduals, just says that the corresponding indicator will not be very sensible, but the whole analysis always remains valid 5  A BNORMALITY DETECTION  12 \226  Catching temporal behavior Sometime the successive obse rvation of indicators has a temporal meaning. Because th e standardized indicators can be compared, \(which was not th e case of the original ones it is possible to add some new indicators with computations involving past and present combinations of the standardized indicators. This is a simple way to catch some temporal behavior of the engine system. Hence we build a new set of indicators 265 and variance matrix 004 are learnt on a normal operations dataset this leads to the computation of a standardized z-score        tsyfyy stt 


 5 original difference by the difference of values readjusted to normal distribution. \(One empirically computes the cumulative distribution function \(cdf\he indicator and then applies the reverse Gaussian cdf If the input indicators y are linearly dependant, the covariance matrix may not be invertible. However in practical applications all characteristic values \(the eigenvalues of the covariance matrix\e positive but some can appear too small so the inverse is difficult to compute. A clear attention is taken to this computation that may use a pseudo-inverse algorithm or a partial compression onto the largest eigenvectors The following graph \(Figure 7\hows the signature of start number 75. One immediately identifies small problems revealed by two indicators that appear to be lower than expected. \(The green and red lines again correspond to the 3 265 265 212\004\212 212 y yzd 1 where z is the vector of scores  1 m zzz  is main part of the Gaussian likelihood and resolve the first order correlations between scores. Moreover d 2 has a known distribution  2 m 002 and 6 002 levels  Figure 7: Score signature 14 \226  Signature interpretation The observed signatures are not independent between indicators but they conserve a meaning for the experts However, experts must eventually be careful because observing a high signature may identify high value for the parameter but also a too small prediction from the other indicators. Then an abnormality, which is detected on a specific indicator, may refer to something relevant to the others Anyway, the individual scores are like some chemical tests that reveal problems linked to each indicator. The description of the signature do es not immediately reflect the feeling of the expert but the standardization process is able to transform caricatural signat ures build by expertise on simulators into score signatures The graph below \(Figure 8\ shows some abnormal start behaviors. It is the result of an experiment based on real measurements distorted using a physical failure simulator The x-abscise is a time delay between the declutching of the starter and the time the HP shaft reaches its normal groundidle speed. The y-abscise is the max value of the shaft acceleration during the start proc ess. On this specific 2D projection one can see at least two directions that in fact represent two different failures  Figure 8: 2D projection of normalized signature in original dimensions 15 \226  Compute a robust abnormality indicator The signature was built in a way to conserve input dimension and direction. Thus some dependency between monovariate scores remains but may be partially resolved by the correlation analysis The Mahalanobis distance         1 2 2 002 and 6 002 levels may directly be obtained by analytic computation The graph below \(Figure 9\ shows this global score for normal starts and the corresponding thresholds. One immediately sees the start numb er 67 that was previously identified as an outlier 005 and the 3 


007 2 helps to select among templates Because of a problem well k nown in high dimensions one may not directly use this distance as a distribution parameter. However the comparisons between pairs of computed distances continue to have a meaning. Hence it is always possible to select th e main nearest templates and limit our probability model to the corresponding sub-sphere  6  Figure 9: Global score co mputed from Mahalanobis distance. The green and red lines correspond to the equivalent probabilities obtained for the 3 007 between signature vector z and normalized template t  computed as  002 and 6 002  quantiles computed on symmetric Gaussian distributions 6  F AILURE IDENTIFICATION  16 \226  Expertise all possible failures In most industrial cases failure events are unusual and we never have a lot of real examples. Back to back bench tests help engineers to understand the engine behavior with some defective components, but the cost of such experiment is expensive. Physical numeric models also give ideas of the variations one may observe on measurements. But in all cases we don\222t have real information with corresponding context data Anyway the experts are able to list a lot of possible failures giving them an a priori occurrence probability and a caricatural signature description. This signature is defined as informal description like \223this value is too high\224, \223this one increase to slowly\224, \223this other may be low when the last one is high\224, and so on. A description table called FMECA Failure Modes, Effects and Criticality Analysis summarizes the analysis \(Table 1 Table 1: Failure mode anal ysis built by experts Prior Precision Failure Probable Cause Known Consequence I 1  I 2  I 3 205 006  002 or 6 002 and asks the experts for a codification in score scale of those failure modes This leads to a signature desc ription that caricatures the failures. It is built under the assumption of standard context and generally no understanding of the relations between indicators. To be comparable to our scores signatures built on point 12 \226 one needs to apply our normalization process using standard conditions as input context information The result is a standardized matrix with each line that corresponds to a normalized signa ture of a failure. Figure 10 shows a 2D projection of the normalized templates in cyan circles and the observed degradations by blue dots. \(New observations come from a test set of bad start measurements created by simulation  Figure 10: 2D projection of standardized degradation circles 17 \226  Classify signatures into failure probability As the failure description by e xperts is a caricature, one can only justify the direction of a signature template t It means that the comparisons of score signatures z to templates must be done on the m-dimension sphere. The norm of the signature is an indication of the abnormality level, it is the global score d 2 defined in point 15 \226. The geodesic distance 2 002   High Low N/A 205  Such table is a help to identify usable indicators; eventually relations; some temporal effects that may be translated in intermediate computed indicators for the definition of the signature Once the indicators converted into scores, such document is very easy to translate in signat ure templates. One eventually defines main thresholds like 3 b 212 zt zt t  12 2 


212  F cf c fPq cP t 3 where  fP is computed and represented by the preceding image c 212 t tt t 2/exp 22 002\007\006 where each t 002 are weight and scale parameters adjusted according to the prior occurrence probability of the failure The image \(Figure 11\pres ents the probability of each failure for a set of abnormal starts. The failures where first identified by experts and the classification is automatically done by the algorithm  Figure 11: Failure probabilities. The x-axis corresponds to the different starts and the y-axis to each failure identified by the experts. Green represents low probability and red is high probability of detection For each \223bad\224 start a set of signatures switches on Generally there is more than one signature that may appear but the corresponding vector \(v erticals lines on the image identifies a component The dataset provided for this classification test was built from progressive apparitions of different faults. On the image one clearly retrieves each of those tests detectable by vertical bands, which corresponds to faulty components 18 \226  Detect guilty component At this point we have a computed probability for each failure. To identify the gu ilty system component the relations from failures to component are defined by a set of Bayesian rules. For each observed failure, we ask the experts for a probability for each component c to be faulty when a failure f is detected   fcPq cf  7 For the start sequence we use a theoretical Gaussian mixture parameterized by a mix like  In general, as a first try, experts define diffe rent failures for each component so the matrix   cf qQ  is almost diagonal with ones for a fault that corresponds to a component and zeros elsewhere But after a second pass the experts acknowledge that failures may resemble and that some confusion is possible This can be easily resolved with an adaptation of this matrix With a set of defective examples it is also possible to help the experts showing them the coefficients of the regression of an expected a priori  xcP  vector \(where x is an observation index\ and  xfP the result computed just previously. But this automatic procedure needs a new set of defective observations and some amount of observed failures to optimize the robustness of the regression. It explains why we prefer using expert knowledge and compare to the mathematical estimation Finally for each component we may estimate a posterior probability  007 correspond to the geodesic distance to the corresponding template t 006 and 2 t t is a normalization coefficient that corresponds to the prior occurrence of guilty component c The result is truncated between 0 and 1 The image below is obtained af ter application of this rule This time each failure is replaced by a real component name  Figure 12: Component probabilities. The x-axis is always the start numbers, and the y-axis represent specific components. For each test the guilty component appears in red 7  C ONCLUSIONS  The application of CRN Context Removal and Normalization\owed by FDI algorithm \(Failure Detection and Identification provides a meaningful interpretation of the detection and identification of guilty component for an turbofan system, even when the acquisition is subject to contex t variations. The main force of this algorithm is its interp retability and the control that experts may apply on the computation. This way the code is easier to transfer in a real operational environment without loosing much in effectiveness   1 0  12 


 8 This same couple of algorithms is also used 200  for module wear identification, hence helping the maintenance workflow 200  for the sensor failure analysis 200  and for any other system that need to get rid of dependencies or external conditions  R EFERENCES  1  H. Hotelling, \223The generaliza tion of Student\222s ratio\224, Ann Math. Statist., vol. 2, pp 360-378, 1931 2  G. E. P. Box, \223A general dist ribution theory for a class of likelihood criteria\224 Biometrika, vol. 36, pp 317-346, 1949 3  T. W. Anderson, \223Asymptotic theory for principal component analysis\224, Ann. Ma th. Statist., vol. 34, pp 122148,  1963 4  K.V. Mardia, J.T. Kent, and J.M. Bibby.. \223Multivariate Analysis\224, Academic Press, 1979 5  B. R. Upadhyaya at al., \223Multivariate Statistical Signal Processing Technique for Faul t Detection and Diagnosis\224 ISA Transactions, vol. 29 n\2604, pp 79-85, 1990 6  V. N. Vapnik, \223The Nature of Statistical Learning\224, Springer Verlag \(NY\95 7  A. Smola and B. Sch\366lkopf, \223O n a kernel based method for pattern recognition regression approximation and operator inversion\224, Algorithmica, 1998 8  R. Azencott, \223Proc\351d\351 de surveillance d\222un syst\350me\224, Patent EP01170650A1, Miriad Technologies, 2002 9  R. Azencott, \223A method for monitoring a system based on performance indicators\224, U S. Patent 6594618B1, Miriad Technologies, 2003 10  J. Lacaille, H. Dubus 223Defectivity Analysis by a Swarm of Intelligent Distributed Ag ents\224, AEC/APC 2005, Palm Spring \(CA 11  J. Lacaille, \223Mathematical Solution to Identify the Causes of Yield Deterioration - A defectiv ity data based solution with an emergent computing tec hnology\224, ISMI 2005, Austin TX 12  M. Zagrebnov, J. Lacaille 223Building a Robust Model for Process Control Using Advanced Mathematical Techniques\224 AEC/APC tutorial 2006, Aix en Provence \(France 13  J. Lacaille, \223Advanced Fault Detection\224, AEC/APC tutorial 2006, Denver \(CO 14  J. Lacaille, M. Zagrebnov, \223A statistical approach of abnormality detection and its applications\224, AEC/APC 2006 Denver \(CO 15  J. Lacaille, \223How to automatically build meaningful indicators from raw data\224 AEC/APC Palm Spring 2007 16  J. Lacaille, M. Zagrebnov, \223A n Unsupervised Diagnosis for Process Tool Fault Detection: the Flexible Golden Pattern\224 IEEE Transactions on Semi conductor Manufacturing Volume 20, Issue 4, Nov. 2007 Page\(s\ \226 363 17  J. Lacaille, \223Global Predictive Monitoring System for a Manufacturing Facility\224 U.S. Patent 20080082197A1  B IOGRAPHY  J\351r\364me Lacaille is senior expert in algorithms for Snecm a. He joined the company in 2007 with responsibility for developing a health monitoring solution for jet engines. J\351r\364me has a PhD from the Ecole Normale Sup\351rieure, France in Mathematics. J\351r\364me has held several positions including scientific consultant and professor. He has also co-founded the Miriad Technologies Company, entered the semiconductor business taking in charge the direction of the Innovation Department for Si Automatio n \(Montpellier - France\ and PDF Solutions \(San Jose CA\. He developed specific mathematic algorithms that where integrated in industrial process. Over the course of his work, J\351r\364me has published several papers on integrating data analysis into industry infrastructure, including neural methodologies and stochastic modeling. J\351r\364me can be reached at jerome.lacaille@snecma.fr    


ogy Results of this paper can be directly useful to the Objective Gateway program of the Air Force where several Battlefield Airborne Communication Nodes BACNs and their ground counterparts the Rapid Attack Information Dissemination Execution Relay RAIDERs The goal of the Objective Gateway program is to provide networking technology to the forward edge of the battlefield and create a high-capacity airborne communication backbone Future Work This paper is the first step towards developing a mission planning toolbox for the airborne network deployment problem A few further research directions could be accounting for link disruptions due to airborne platform banking expanding the optimization search space to include non-circular or tilted loiter orbits accounting for terrain effects providing coverage to mobile airborne nodes that are involved in a mission tactical edge networks planning so as to optimally use satellite communication when certain airborne links are inactive developing multiple tiers of airborne backbones located at different elevations and incorporating topology control such that the airborne nodes can change power levels thus radio range during periods where airborne topology is sparse ACKNOWLEDGMENTS The authors would like to thank Maneesh Varshney and Prof Mario Gerla for insightful discussions on the subject matter of this paper This work was supported under Air Force phase I SBIR grant number FA8750-07-C-0158 7 K Schroth and D Kiwior Interdomain Routing for Mobile Nodes Proceedings of the Military Communications Conference 2007 to appear 8 E G I D Kiwior and S V Pizzi Quality of Service QoS Sensitivity for the OSPF Protocol in the Airborne Networking Environment Proceedings of the Military Communications Conference 2005 9 L V J Cooley 0 Huang and S McGarry Mobile Airborne Networking Experience with Paul Revere Proceedings of the Military Communications Conference 2005 10 K Q Weinberger and L K Saul Unsupervised Learning of Image Manifolds by Semidefinite Programming International Journal of Computer Vision vol 70 no 1 pp 77-90 2006 11 S Boyd and L Vandenberghe Convex Optimization Cambridge University Press 2006 12 J Lfberg Yalmip A toolbox for modeling and optimization in MATLAB in Proceedings of the CACSD Conference Taipei Taiwan 2004 Online Available http://control.ee.ethz.ch joloef/yalmip.php 13 R A Horn and C R Johnson Matrix Analysis Cambridge University Press 1985 14 N Megiddo and A Tamir New results on the complexity of p-center problems SIAM Journal on Computing vol 12 no 4 pp 751-758 November 1983 REFERENCES 1 T A Kostas and T G Macdonald A Methodology for Evaluating and Planning Future Airborne Networks Proceedings of the Military Communications Conference 2004 2 R Ramirez Link Management in the Air Force Airborne Network Proceedings of the Military Communications Conference 2005 3 Y Wang and Y J Zhao Fundamental Issues in Systematic Design of Airborne Networks for Aviation Proceedings of the IEEE Aerospace Conference 2006 4 B Epstein and V Mehta Free Space Optical Communications Routing Performance in Highly Dynamic Airspace Environment Proceedings of the IEEE Aerospace Conference 2004 5 M Dehkordi K Chandrashekhar and J S Baras A placement algorithm for enhanced connectivity and reliability in wireless ad-hoc networks in Conference on Future Networking Technologies CoNEXT Toulouse France October 2005 6 D Kiwior and L Lam Routing Protocol Performance over Intermittent Links Proceedings of the Military Communications Conference 2007 to appear 9 


 10 D e L one  W  H  a n d Mc L e a n E.R   T he D e L one a nd McLean Model of Information Systems Success: A Ten-Year Update Journal of Management Information Systems Vol. 19, No. 4, pp. 9-30, 2003 11 D e nz i n N  K  a nd L i nc o l n, Y  S Handbook of Qualitative Research Sage, Thousand Oaks, CA 1994 1 Di n een  B  L i n g  J  A s h  S  an d Del V ecch i o  D   Aesthetic properties and message customisation Navigating the dark side of web recruitment  Journal of Applied Psychology Vol. 92, No. 2, pp. 356-372 2007 13 Fe l d m a n, D  a nd K l a a s B Inte rne t j o b h unti n g  A  field study of applicant experiences with online recruitment Human Resource Management Vol. 41 No. 2, pp. 175-201, 2002 14 G a la nk i, E The decision to recruit online: A descriptive study  Career Development International  Vol. 7, No. 4, 2002 1 G u eu t a l  H G  an d S t on e D  L    The Brave New World of eHR: Human resources management in the digital age Jossey-Bass, San Francisco, 2005 16 H a da y a  P  a nd Et hie r J  Online purchasing of simple goods: the impact of e-service quality as provided by electronic commerce functionalities  Proceedings of the 41 st Hawaii International Conference on System Science 2008 1 G r  n r oo s C   Service Management and Marketing Customer Management in Service Competition 3rd Edition, Wiley & Sons, Chichester, 2007 18 Ho llow a y  B.B. a nd Be a tty S.E Satisfiers and Dissatisfiers in the Online Environment: A Critical Incident Assessment  Journal of Service Research  Vol. 10, No. 4, pp. 347-364, 2008 1 Kl ei n H K and M y ers M  D  A S e t o f P r i n ci p l es for Conducting and Evaluating Interpretive Field Studies in Information Systems MIS Quarterly Vol 23, No. 1, pp. 67-93, 1999 20 Kri ppe nd orf f  K Reliability in content analysis some common misconceptions and recommendations  Human Communication Research Vol. 30, pp. 411433, 2004 21 L e e  I The evolution of e-recruiting: A content analysis of Fortune 100 career web sites  Journal of Electronic Commerce in Organizations Vol. 3, No. 3 pp. 57-68, 2005 22 Le e  T W   Using qualitative methods in organizational research Sage, Thousand Oaks, 1999 2 Ngw e n g a m a O K and A  S  L ee  Communication Richness in Electronic Mail: Critical Social Theory and the Contextuality of Meaning  MIS Quarterly  Vol. 21, pp. 145 167, 1997 2 L i even s F  an d Harris  M   Research o n I n tern et  recruiting and testing: current status and future directions, In: Cooper, C., Robertson, I. \(Eds International Review of Industrial and Organizational Psychology Vol. 18, pp. 131-165, 2003 25 L i n, C S. a nd W u  S Ex pl oring t h e im pa c t of on line  service quality on portal site usage Proceedings of the 35th Hawaii International Conference on System Science 2002 26 Mc Cu ne J  C Good help is hard to find  Management Review Vol. 86, No. 6, pp. 30-31, 1997 27  Mile s   M.B. a nd H ube rm an, A  M Qualitative data analysis: An expanded sourcebook Thousand Oaks CA, Sage, 1994 28  P a rasu ram a n  A Zeith am l, V  A  an d Mal h o t ra A ES-Qual: A Multiple-Item Scale for Assessing Electronic Service Quality Journal of Service Research Vol. 7, No. 3, pp. 213-233, 2005 29 P a rry  E Drivers of the adoption of online recruitment an analysis using diffurion of innovation theory In Bondarouk, T.V., Ruël, H.J.M. \(Eds\E-HRM in theory and practice. Elsevier, Amsterdam, 2008 30 a tto n  M.Q  Qualitative research and evaluation methods Thousand Oaks, CA, Sage, 2002 31 R odg e r s  W N e g a s h S. a n d Suk  K  The moderating effect of online experience on the antecedents and consequences of online satisfaction  Psychology Marketing Vol. 22, No. 4, pp. 313-331, 2005 32 R o w l e y J An Analysis of the E-Service Literature Towards a Research Agenda  Internet Research Vol 16, No. 13, pp. 339-359, 2006 3 Ru st  R T  an d Kann an   P  K   E-Service: A New Paradigm for Business in the Electronic Environment  Communications of the ACM Vol. 46, No. 5, pp. 3742. 2003 34  Sa c h a f P  a n d Oltm a nn, S M Equality and e-service quality  Proceedings of the 40 st Hawaii International Conference on System Science 2007 35 Sa n t os, J E-service quality A model of virtual service quality dimensions  Managing Service Quality Vol.13, No. 3, pp. 233-246, 2003 3  S m i t h  A  D an d Rup p W  T    Managerial challenges of e-recruiting  Online Information Review Vol. 28 No. 1, pp. 61-74, 2004 37 St one D  L Sto n e Rom e r o, E. F  a nd L u k a s z e w s k i  K Factors affecting the acceptance and effectiveness of electronic human resource systems  Human Resource Management Review Vol. 16, No. 2, pp 229-244, 2006 38 T ong J D u f f y  V Cros s  G  T s ung F. a nd Y e n, B Evaluating the industrial ergonomics of service quality for online recruitment websites  International  Journal of Industrial Ergonomics Vol. 35, pp. 697711, 2005 39  Va n Ma a n e n J  A n End of I nnoc e n c e T h e Ethnography of Ethnography," In: Representation in Ethnography, J. Van Maanen \(ed.\ge, Thousand Oaks, CA, pp. 1-35, 1995 40 W o lf inba rg e r M. a nd G ill y  M.C eTailQ Dimensionalizing, measuring and predicting etail quality  Journal of Retailing Vol. 79, No. 3, pp. 183198, 2003 41 W o lte rs M The Effectiveness of job board Internet Recruitment Proceedings of the First European Academic Workshop on e-HRM, The Netherlands 2006 4 Z u sm an  R an d L a n d i s R   Applicant preferences for web-based versus traditional job posting  Computers in Human Behaviour Vol. 18, pp. 285-296, 2002  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


Alan Little is the MEDLI Project Manager at NASA's Langley Research Center He previously served on a variety of earth remote sensing missions and recently served as the NASA-CNES interface manager and the payload assembly integration and test manager on the joint NASAICNES CALIPSO Mission that was launched in April 2006 He has a MS in Optics from the University of Rochester Neil Cheatwood earned B.S MS and Ph.D degrees in aerospace engineering from NC State University He has played key roles in a number of NASA's planetary atmospheric flight programs and is a nationally recognized expert in aerosciences and flight mechanics for planetary entry systems He is currently serving as the Hypersonics Project Scientist for the Fundamental Aeronautics Program with NASA's ARMD Dr Cheatwood is also the Principle Investigator for the Mars Science Laboratory Entry Descent and Landing Instrumentation MEDLI project In recent years he led NASA LaRC efforts to develop inflatable aeroshell technologies He served as CoInvestigator to Claude Graves of NASA JSC on the NASA ESMD ESR&T Inflatable Aeroshell and TPS Development IA TD Project He served as the Principle Investigator for NASA LaRCs Inflatable Reentry Vehicle Experiment IRVE as well as the follow-on Program to Advance Inflatable Decelerators for Atmospheric Entry PAI-DAE Dr Cheatwood was responsible for aerodynamic databases of Stardust Mars Microprobe Genesis and Mars Exploration Rovers He has also contributed to the Mars Global Surveyor and Mars Sample Return flight projects Dr Cheatwood is an AIAA Associate Fellow and the principle author or co-author of 60 technical publications in the fields of fluid dynamics atmospheric entry and systems engineering Jeff Herath serves as the Lead Systems Engineer and Chief Engineer for the Mars Science Laboratory Entry Descent and Landing Instrumentation MEDLI He also serves as the Assistant Head of the Atmospheric Flight and Entry Systems Branch AFESB to plan direct and coordinate Branch activities in the areas offlight and entry systems research and development Mr Herath previously was the Acting Assistant Branch Head for Electronic Systems and served as the branch leadfor new business activities proposals and their development He was also the Principal Investigator PI for the Radiation Tolerant Intelligent Memory Stack RTIMS Project which successfully developed and demonstrated an in-flight reconfigurable radiation tolerant stacked memory array He co-founded Vianix LC a company developing and licensing voice compression technology and served as its Chief Technology Officer He developed the company's voice compression technology and was responsible for all research  development engineering personnel and production efforts He has 6 patents As Manager of Hardware Development at Arc Second Inc he designed and built a unique laser based three-dimensional positioning system which opened new markets for the company At E-Systems he successfully completed several military avionics programs and payload that were classified and consisted of system box and board level designs Michelle Munk has been a NASA employee for nearly 20 years first at the Johnson Space Center then at the Langley Research Center She has been involved in Mars advanced mission studies for many years both robotic and human contributing interplanetary trajectory analysis and entry and descent analysis She has managed the delivery of International Space Station hardware and was on the Mars Odyssey aerobraking operations team In 2002 Ms Munk accepted a detail assignment to become the Lead Engineer for Aerocapture Technology Development under In-Space Propulsion at Marshall Space Flight Center She managed the technical work of ISP Aerocapture for nearly 5 years before becoming the Project Area Manager and returning to Langley in 2007 Ms Munk is also a subsystem leadfor the Mars Science Laboratory Entry Descent and Landing Instrumentation MEDLI project and contributes to other NASA projects developing entry system technologies She has a BSAE from Virginia Tech and completed graduate coursework at the University of Houston Frank Novak is an Assistant Branch Head for Remote Sensing Flight Systems Branch RSFSB at the NASA Langley Research Center LaRC in Hampton VA He serves as the MEDLI Subsystem Manager for the Sensor Support Electronics SSE system Mr Novak has over 20 years of experience in the design development and test of spaceflight electronics He served as the lead development manager for the EVA IR Camera Project lead engineer for the visible imager for the GIFTS project lead electronics engineer for the pointing spectrometer for the Mars ARES project lead integration and test engineer for the SAGE III project and lead engineer for the interface adaptor module for SAGE III He earned a BS in Physics from Christopher Newport University in 1999 11 


Ed Martinez is a Project Manager/Lead Scientist/Project Engineer with 20 years experience in the aerospace and electromechanical field He is responsible for leading the Thermal Protection System TPS instrumentation programs for the NASA Ames Research Center As Project Manager he simultaneously managed multiple teams of scientists engineers and engineering technicians responsible for TPS projects including test analysis and technology advancement As Lead Scientist he was engaged in the characterization and operations of the world's largest shock tube This facility produced simultaneous overpressure and thermal environments at shock speeds up to Mach 5 As a Project Engineer his experience included project initiation coordination and providing design and instrument criteria for operating multi-100 million dollar DoD facilities Mr Martinez also managed data handling performed analysis reporting of test results and maintained technical proficiency in shockwave phenomenology 12 


  13 B IOGRAPHY  Brian Paczkowski is currently the Deputy Section Manager of the Planning and Execution Section within the Systems and Software Division at JPL. Prior to that he spent 9 years as the Cassini Science Planning Manager responsible for the development and implementation of the Science Operations Plan. Prior to Cassini, he was the Science Planning and Operations Team Chief for the Galileo Mission to Jupiter. He has also been involved with the pre-launch development of the science instruments on Galileo, Comet Rendezvous and Asteroid Flyby \(CRAF\ and Cassini missions. He has a BS in Astronomy from Villanova University and did graduate studies in Astronomy at Ohio State University  Barbara Larsen  is the Mission Operations System Engineer for the Cassini Mission. She is also on the science planning staff and previously worked in system engineering for the Mission Sequence Subsystem. She has a MS in Mathematics from California State University Long Beach and a BS in Mathematics from USC Trina Ray  is currently the Titan Orbiter Science Team \(TOST\ co-chair and the Science System Engineer for the Project Scientist for Cassini. She has been working on the Cassini Mission since before launch as an instrument operations lead for the Radio Science Team, and then as part of the Science Planning Team supporting Titan integrati on and sequence development She has a MS in Astronomy from San Diego State University and a BS in Physics, Astronomy option from CSUN  


  14  


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


