Investigation on Multi-Grain Parallelism in Chip Multiprocessor for Multimedia Application Xiaoping Huang Computer school Northwestern Polytechnical University Xi\220an, 710072, China Xiaoya Fan, Shengbing Zhang, Liwen Shi Computer school Northwestern Polytechnical University Xi\220an, 710072, China 
Abstract 
204with the advent of chip multiprocessor \(CMP architecture, programmer must tune the program to the architecture in order to fully utilize the hardware resource. How to parallel program multimedia application in the CMP is a big obstacle. In this paper, we introduce the potential parallelism in 
the multimedia application and the multi-grain parallelism architecture in the CMP; also we make a systematic analysis on how to exploit multi-grain parallelism to accelerate the multimedia application computing. A method based on the multigrain parallelism in the CMP is proposed to instruct the programmer to effectively layout the multimedia-related program. The method exploits the parallelisms from data-level instruction-level, thread-level and memory-transfer level. The Experimental results indicate that the method is practical and effective  
Keywords- Multi-Grain parallelism; Chip Mult-iprocessor multimedia application 
I I NTRODUCTION With the fast development of the information technology multimedia applications are becoming increasing extensive Contemporary multimedia application are more and more complicated and high real-time, such as video conference virtual-reality, three-dimensional graphics and image processing. A distinctive feather of these multimedia applications is that they have significant parallelism, including data-, instruction-and thread level parallelism [1  T h er e is an  urgent demand to accelerate the multimedia-related computing In the superscalar processor, introducing the multimedia instruction sets and integrating a short-vector SIMD unit into 
the processor is a common method [2  Su ch as th e M M X S S E   of Intel, the VIS of Sun, the 3DNow of AMD and the AltiVec of IAM [3  T h e  m e th o d can ex pl oit  th e d a ta an d in s t ru c t i o n level parallelism. In the CMP, it provides another new approach and we can also exploit the thread parallelism in the multimedia application. Many parallel threads can run simultaneously on the CMP. There are some emerging difficulties that how to split the multimedia application into multi-grain parallelism and how to efficiently schedule them in the CMP In this paper, we discuss the potential parallelism in the multimedia application and the CMP; we have an in-depth analysis on how to exploit the multi-grain parallelism in the 
CMP architecture. A method which can exploit the multi-grain parallelism in the CMP is proposed to instruct the programmer to effectively parallel program. We provide one example to verify the method The rest of the paper is organized as follows. Section 2 analyzes the potential parallelism in the Multimedia application Section 3 investigates the Multi-Grain Parallelism Computing in the CMP. Section 4 proposes parallel programming method based on the multi-grain parallelism in the CMP and test the method. Section 5 concludes the paper II T HE ANALYSIS  OF THE M ULTIMEDIA APPLICATION Multimedia applications have increasing dominated the 
mainstream applications in the commercial computer and customers have come to expect high quality multimedia computing. Multimedia application, which differs from traditional control-intensive applications, is arithmetic intensive. A large number data must be processed in the limited time with the same operation. There are several distinguishing characteristics in the multimedia applications. The first is that there are a lot of loop operations, and each loop operation just computes different multimedia data elements by the same operation, in other words, there is abundant data-parallelism The second is that the length of multimedia data is eight or 
sixteen bits and the computing precision is low. The third is that the data has poor spatial locality and is rarely used again As a result, the Cache organization in the superscalar processor is not suitable for the multimedia program. The fourth is that the computational complexity is relative regular but huge.  It means that the processor must be quality of high throughput to perform the task in the limited time. The fifth is that the multimedia program is relative easy to split into different parallel sub-tasks as a result of the arithmetic-intensive Based on the above analysis, we can find three potential forms of parallelisms in the multimedia application. They are data-level, instruction-level and thread-level parallelism. The 
60573107, No. 60773223 .the Sci-Tech innovation foundation of NWPU2008KJ02027and the China 863 program  with the N0.2009AA01Z110 
Supported by China National Science Foundation with the No. 60736012, No  
CMP can exploit fully the parallelisms to accelerate the multimedia computing ALPBench[4 is a p u b l i c ly r e le ase d  b e n c h m ark su it th at  pulls together five complex multimedia applications, including speech recognition, face recognition, ray tracing etc 978-1-4244-4994-1/09/$25.00 \2512009 IEEE 


III I NVESTIGATING THE M ULTI G RAIN P ARALLELISM C OMPUTING IN THE CMP To efficiently perform multimedia computing, the hardware can speed up from multi-grain parallelism. For datalevel, it can use SIMD technology [5 r i n str u ctio n le v e l it can use superscalar technology readlev e l, it can  u s e  multi-core technology o allev ia te t h e m e m o r y late n c y, it can use memory-orient optimization technology, like stridebased prefetch o f t w a re a s s i s t ed pref etch 9], DMA a n d  local- storage A The data level parallelsim Multimedia application is arithmetic-intensive and the operations are regular, so data-level parallelism is the basic parallel form. To exploit data-level parallelism, multiple data elements \(scalar elements\an be packed into one register to form a vector, and then the processor can perform operation on the vectors at the same time in the ALU. The processor can also provide multiple ALUs to further exploit parallelism. This parallel form is also called short-vector parallelism. Changing form scalar to vector, data-level parallelism can efficiently improve the amount of computations In Cell processor 10  th e s h o r tv ect o r A L U c o n t ain s  1 2 8 bit data path and it can be divided sixteen bytes, eight halfwords and four words according to the operation kind B The instruction level parallelism In case that there is no data-dependence or controldependence in sequential instructions, multiple instructions can be executed parallel; the processor can dynamically exploit the instruction parallel using superscalar technology. But when dependence occurs, the cost to maintain the program sequence is very high so the width of the parallel issued instructions is almost two instructions, no more than four instructions. In addition to speeding executing timing, instruction-level parallelism also can alleviate average memory latency by allowing multiple pending memory requests C The thread  level parallelism Multimedia application is arithmetic-intensive and the OS and other system task are almost control-intensive, so the heterogeneous CMP for multimedia application is more preferable and it is composed of multiple control-cores and multiple arithmetic-cores. The two kinds of cores have different architecture and organization optimized to multimedia application.  In the heterogeneous CMP, the arithmetic-cores are responsible for performing parallel multimedia arithmeticintensive threads and the control-cores are responsible for performing the OS, spawning the parallel thread, managing and coordinating multiple threads. The arithmetic-core is SIMDlike architecture and has independent instruction-fetch unit dynamic instruction scheduling unit and memory. Multiple cores can be connected to communicate through MESH network or bus topology Cell processor[11   a ty p i c a l th r e a d l ev e l  p a r a ll elism  heterogeneous CMP aimed for multimedia application, consists of a PPE\(Primary Processing Entity\and eight helper units called SPE\(Synergistic Processing Elements\PPE is a PowerPC-like superscalar processor to run OS and the SPE is SIMD-like processor to run multimedia application, composed SPU\(Synergistic Processing Unit\,MFC\(Memory Flow Controller\ A SPE can operate on sixteen 8-bit integers, eight 16-bit integers, four 32-bit integers, or four single-precision floating-point numbers in a single clock cycle.the Cell is showed in Fig.1 Fig. 1 Cell Block Diagram D The memory-transfer  level parallelism Memory wall has become a bottleneck in superscalar processor and the situation becomes more serious in multimedia application as it is often high bandwidth [12  In  the multimedia application, because of the memory wall arithmetic-core must wait the operands to be ready. To prepare for the data before computing, one method is hardwareÖbased perfetch. The other method is software-based prefetch. In the multimedia application, the latter is more effective Programmer should explicitly arrange the memory location of the multimedia data and transfer the data in advance to feed the arithmetic-core. Before computing, multiple memory-transfer requests can be issued parallel. In the Cell, DMA is used to move data between local storage and main memory IV PARALLEL PROGRAMMING METHOD BASED ON THE MULTI GRAIN PARALLELISM IN THE CMP Compared to the superscalar, CMP computing platform to multimedia applications is not free lunch. Not only hardware fully exploits the parallelism, but also programmer should be explicit parallel programming from data-level, instructionlevel, thread-level, and memory-transfer level. For parallel optimization, programmer should take the core architecture, the number of cores, the connection relationship, and the communication protocol into consideration. We propose a parallel programming method based on the multi-grain parallelism in the CMP. The method can be divided into four steps. In the first step, based on the hardware feature of the CMP, programmer performs p arallel a lgorithm a nalysis  explicitly divides the whole task into subtasks, draw taskgraph, and exploit thread-level parallelism. In the second stage according to the subtask dependence relationship, map each subtask to the unique core, design the scheduling strategy, and manage the data locations and data movement. In the third 


stage, in each subtask, exploit the instruction-level parallelism In the fourth stage, exploit the data-level parallelism. The method is a kind of èdivide and conquer strategy. It can instruct the programmer to perform parallel programming and exploit the parallelism in different levels.Fig.2 is the method flow chart Fig. 2 the proposed method flow chart A A  practical parallel program using the method We use the method to design a multi-channel FIR filter The task is that four parallel FIR filters process four sets independent input sequences and the final result is the average of the five FIR filterês output. The CellSim [13-14 is u s e d  to  simulate the parallel program. CellSim is a modular simulator for heterogeneous multiprocessor architectures based on the STI Cell processor Under the guidance of the method, we divide the task into six sub-tasks, which are main thread, four independent FIR filter thread, and one average calculating thread. The main thread, responsible for applying for hard resource, managing other threads, and releasing resource when the task finished, is control-intensive operation running on the PPE. The other five threads are arithmetic-intensive operation running on the SPE The four FIR filter thread can parallel run and the average calculating thread must be keeping synchronous with them. In each thread, instruction-level and data-level are also being exploited. DMA transfer data between local storage and main memory Based on the CellSim simulator, we build the whole simulation environment and parallel program the task. We have made three configurations to simulate the program. The result is show in table 1 and Fig. 3. \(C is short for configuration Table 1 the five simulator configurations C.1 C.2 C.3 C.4 C.5  1PPE 1PPE+1SPE 1PPE+2SPE 1PPE+4SPE 1PPE+6SPE Fig. 3 the consuming timing/C.1 under different conifgurations From Fig.3, it is found that the change form C.1 to C.2 is significant, because C.1 has no arithmetic-core and all work is finished by the control-core. At the same time, there is no thread-level parallel or data-level parallelism in C1. From C.2 to C.5, the number of the arithmetic core is increased gradually and multi-grain parallelisms are fully exploited, so the consuming time is decreased accordingly. The result and our real experience show that the method is practical and effective V CONCLUSION In this paper, we analyze the potential parallelism in the multimedia application. Programmer can optimize the multimedia program from data-level, instruction-level, and thread-level. Also it is found that the heterogeneous CMP is suitable to multimedia application. In the heterogeneous CMP processor hardware provides short-vector SIMD ALUs to accelerate the data-level parallelism and adopts superscalar technology to accelerate instruction-level parallelism. Controlcore and arithmetic-core are assigned to parallel accomplish control-intensive thread and arithmetic-intensive thread independently. A programming method based multi-grain parallelism in the CMP is proposed and the method can help the programmer to effectively layout the multimedia-related program. Experimental results show that the method is practical and effective A CKNOWLEDGMENT Thanks to Zhang meng and Zheng Qiaoshi who offer me plenty help to accomplish the paper R EFERENCES 1 M a n L ap L i Ruc hir a S c s c n k a,S ar it a V  A d v e Y e n k u a ng C h e n  E r i d  Debes, çThe ALPBench Benchmark Suite for Complex Multimedia Applicationé,Proceeding of the 2005 international symposium on workload characterization,October, 2005 2 X i a opi n g hu an g, xi a o y a F a n  S h en g b in g Z h a n g, çT h e in t e gra ti on of multimedia process unit into an embedded processoré,Proceeding of the 2007 IEEE international conference on integration technology,pp:492495,March 2007,China 3 I B M t ech ni ca l r e p o r t  P ow e r P C Mic r o p r oc es s o r F a m i l y  A l t i V ec TM  Technology Programming Environments Manualé, 2003 4 M an L a p  Da t a level a n d th rea d level p r a lll e l i s m in th e em ergi n g  


multimedia appliactioné, A Dissertation for the degree of B.S. ,University of Califorrnia,Berkeley, 2001 5 M ot orola t e c h n i c a l rep ort  M P C 7410 M PC74 00  R I SC M i c r op roc es s o r Family userês manualé, 2003 6 J oh n L  Hen n e s s y  D a v i d A  Pa tt ers on  C om pu t e r A r c h it ec tu re: A Quantitative Approach,third Editioné,Chinese version by Zheng Wei min,Tang Zhizhong,Wang Dongsheng,pp113-158,2004 7 S Bo r kar   T ho us an d Co r e Ch ips A te ch no l o gy  Perspectiveé,Proc.ACM/IEEE 44 th Design Aution Conf.ACM press,2007,pp.746-749 8 D  M  Pres s e l  F u n d a m en ta l li m i t a tion s on  t h e u s e of p r efet c h i n g a nd stream buffers for scientific appliaction,éin Proc.of the 10 th ACM Symposium on Applied computing,March 2001,pp.554-560 9 B  C a h oon K  S M c K i n l ey  Da t a flo w a n a l y s i s ofr s o ft w a r e p r efet c h i n g  linked data structures in Java,éin Proc. Of the 2001 international Conf.on PACT,2001,pp.52-63 10 J  A Ka h l  M  N D a y H  P  H o f s t e e  C  R  J o h n s  T  R   M a e u r e r   a n d D  Shippy, çIntroduction to the Cell Multiprocessoré,IBM J.Reaerach and Development,Vlo.49,No.4,2005,pp.589-604 11 D  p h am S A s a n M Bo l l i e r  T he de s i g n a n d im pl e m e n tatio n o f  a f i r s tgeneration cell processoré,ISSCC Dig.Tech,pp.184-185,2005  S c ot t Ri xn er  S t r ea m P r oc es s o r  A r c h i t ect u r e Ri c e  university,U.S.A,Kluwer Academic Publishers,pp.1-4,2002 13 F e l i pe Cabar cas A l e jandr o R i co D av i d Ro de nas X av ie r Mar to r e ll A le x Ramires,Eduard Ayguade. çA module-based Cell Processor Simulatoré,The Senond International Summer school on Advanced Computer Architecture and Complication for Embedded Stystem,2006,Italy http://www.hipeac.net/acaces2006  h t t p  pc s o s t res  a c upc  e du  c ells i m d ok u php  


one relationship between are getting smaller and 0.11317 0.12090 0.10844 0.09526 0.00268 0.03162 The uniformly low P-values for all tests indicate a violation of the null hypothesis of homoskedasticity i.e a constant variance error term This is not surprising given the large differences in scale between missions in the data set It does not mean that coefficients developed using ordinary least squares regression a method we frequently will use will be biased but that their standard errors may be underestimated thus potentially inflating the significance of some variables It is something we need to be aware of though not necessarily control increasing that variable X led to a proportionally faster or slower increase in costs For example assume that costs were found to be proportional to Xy i.e X raised to Y power If Y is equal to 1 then as X increases costs increase at the same rate If Y is greater than 1 as X increases costs increase more quickly The greater the value of Y the higher the increase in costs associated with the same increase in variable X Conversely of course if Y is less than 1 as X increases costs increase more slowly We also considered whether the rate of change of each variable X with respect to costs changed over time That is we considered the Y which produces the greatest correlation between XY and cost changes over time by looking at the correlation between cost and various XY throughout various windows of time Correlation Matrix Table 1 offers basic insight into which factors tend to drive others with correlations above 2 underlined An example of this approach for mass is shown in table 2 It seems that in the beginning of the sample the best correlations were achieved with Mass raised to low powers Over time the optimal power on Mass has risen albeit with COST mass maxwatts designlife instruments launch Planetary envelope heogeo deployables maxdata bands buslegacyweaker payloadlegacyweaker 1.00 0.60 0.37 0.35 0.12 0.13 0.22 0.42 0.05 0.06 0.34 0.22 0.09 0.24 1.00 0.48 1.00 0.19 0.60 1.00 0.02 0.11 0.08 0.04 0.22 0.23 0.16 0.18 0.20 0.59 0.39 0.22 0.11 0.09 0.04 0.08 0.02 0.00 0.32 0.12 0.02 0.12 0.24 0.30 0.05 0.10 0.03 0.06 0.02 0.04 Table 1 Correlation matrix for data set The matrix shows pair-wise correlations between variables shown versus bus legacy There is we began systematically inspecting the correlation between cost and each of the variables a sufficient number of observations Raising variables to various powers provided an indication not only about whether cost and were related but also whether mass tended to increase much faster than cost increased Toward the end of the sample cost still increased as slowly Ceteris paribus mass appears to be increasing in importance as a driver of cost One explanation is that while materials are getting more expensive It is possible that was essentially replicated with log-transformed variables 5 1.00 0.27 0.15 0.05 0.12 0.13 0.17 0.11 0.20 0.27 1.00 0.18 0.02 0.26 0.02 0.13 0.12 0.08 0.15 1.00 0.10 0.07 0.03 0.11 0.03 0.12 0.12 1.00 0.05 0.03 0.04 0.07 0.03 0.13 1.00 0.07 0.05 0.04 0.10 0.19 1.00 0.23 0.10 0.11 0.11 1.00 0.13 0.11 0.03 1.00 0.11 0.08 1.00 0.42 1.00 CHI-SQUARE TEST STATISTIC E**2 ON YHAT 2.509 E**2 ON YHAT**2 2.406 E**2 ON LOG\(YHAT**2 2.577 E**2 ON LAG\(E**2 ARCH TEST 2.783 LOG\(E**2 ON X HARVEY TEST 26.918 ABS\(E ON X GLEJSER TEST 19.759 D.F P VALUE 1 1 1 1 10 10 more slowly than mass but not nearly more powerful they we possessed in mass and cost will exist within over time on either of its axes Among the findings above are getting lighter and electronics are strong associations between cost and mass cost and size envelope power maxwatts and design life and weak payload I a smaller negative association between launch year and cost Single Variable Correlations with Cost To explore the explanatory power of the data a given variable X a slight decline post 2000 From these results it may be concluded that early on a one to a decade This finding 


mass and cost is high in almost all periods studied and seems to be rising over time However its power term is always less than same tendencies over time the power mass.3 mass.4 mass.5 0.3083 0.2991 0.2895 0.6261 0.6185 0.6062 0.5897 0.5808 0.5687 0.6329 0.6202 0.6063 0.8747 0.8754 0.8702 0.8924 0.9078 0.9156 0.8721 0.8853 0.8934 0.7967 0.7955 0.7924 0.7179 0.7233 0.7233 mass.6 0.2797 0.5901 0.5541 0.5914 0.8610 0.9169 0.8973 0.7879 0.7183 mass.7 mass.8 0.2699 0.2603 0.5711 0.5503 0.5376 0.5198 0.5756 0.5590 0.8492 0.8364 0.9130 0.9056 0.8980 0.8960 0.7820 0.7750 0.7093 0.6970 Table 2 Time-phased power vs correlation table for mass Particularly significant variables Mass is the most relevant variable when it as a cost estimating variable being uncertain about causality it could be argued that duration is strictly driven by cost and mass and costs is positive and fairly high which remains true when taking into account volume and watts Additionally  The linear effect of were generally ten years long in increments of five years 0.6546 0.6444 1985 1995 0.8524 0.8672 1990 2000 0.8375 0.8690 1995 2005 0.8261 0.8527 2000  0.7925 0.7958 on watts a bigger driver of the cost of construction of we chose not to include development months as an input Less significant variables Design life is seems to be declining When design life is increased costs increase although by a lesser proportion Data rate has increased roughly in proportion to cost throughout much of the sample period and this correlation has been quite high Toward the end of the sample data rate has ceased to have much explanatory power Pointing accuracy has had seems to have increased faster than cost However this hasn't been true of all time periods and the explanatory or a regular pattern of increase a series of regressions were run over various time periods using these variables The time periods chosen so that enough observations would be included The previous correlation study indicated that on them both We also tried varying the power term in each window a  MASSX  b  VOLY  c  WATTSZ However we did not find patterns worth reporting APPROACHES SURVEYED Given the challenging nature of this work several estimating methods a breadth first effort evaluating each approach to over time though beyond this we evaluated widely varied methods 6 mass.1 mass.2 1964-1975 were pursued in some explanatory power with respect to costs throughout much of the sample and accuracy was achieved For example COST  mass and volume seems to have been rising excepting the last chronologic window tested  Watts's explanatory power is increasingly uncertain when both mass is increasing This indicates that while power generation is becoming are accounted for Mass is the primary explanatory variable to over time the correlation between case The correlation between us to judge whether it merited further investigation All involved 0.3244 0.3169 1970 1980 0.6254 0.6285 19751985 mass upon adjusted cost or decrease in importance These include  Apogee  Number of instruments  Newtons in-orbit propulsion impulse  Deployables number of  Communication Bands number of  Degree of autonomy  Number of participating science organizations Variable Changes Over Chronologic Windows Since single-variable correlations indicated that Mass Volume and Watts so that a best fit seems to have are that this will continue to be the one This means that though an increase in mass leads to an increase in costs the increase is proportionally less However the difference between their rates of change is disappearing This is not surprising given the increased sophistication of materials and miniaturization of components Judging by correlations alone size envelope is probably the next most important variable in explaining costs and it so not appropriate all years 0.6901 0.7068 as Mass albeit less pronounced Watts also appears to be extremely important but were the most important variables a spacecraft the cost of providing that power per watt is decreasing rapidly Development Months also appear to be strongly correlated with costs However a variable which a strong correlation a one unit change in a one unit change in volume upon adjusted cost a lesser extent volume and watts a degree sufficient for was highly correlated with costs early in the sample but whose explanatory power seems to have been dropping excepting the last chronologic window tested  The linear effect of seems to be dependent some manner of trending to anticipate change comes to explaining cost and indications seems to decrease yet the correlation between watts and some of the 0.5961 0.5949 1980 1990 power of pointing accuracy has been dropping since the 1970s No regular patterns found Other tested factors did not show either 


 In 1  Accuracy jn\(1  Accuracy 2 In 1  Accuracy Tech Years TechYears1 4 x ccdh 1 X ccdh2  development months development months1.2 ddevelopment months1.8 In\(development month 2 In deve opment month 1.8 planetary number of science orgs x number of sci orgs.8  number of sci orgs1.2  number of sci orgs1.6 jIn number of sci orgs In number of sci L u.u C Figure 6 Coefficient values when stepped in for a stepwise regression conducted over several time windows Coefficient trend determination through stepwise regression Several attempts were made to manually develop explanatory models and then determine whether coefficients in those models changed over time No strong trend in coefficient values could be discerned based on our manual efforts We also conducted a stepwise analysis where the full range of variables was exposed to a stepwise regression method within progressive chronologic windows of the data The stepwise algorithm chose particularly significant variables from among those offered We then examined the result to determine not only which variables were regularly stepped weighted average of various spacecraft technical characteristics 3 We produced a variation on this to see whether weightings might regularly vary over time in such a way that they could be trended to estimate missions lying in the future The variables we chose for this were DryMass Envelope Watts Impulse in Physical Size Power Newtons  of Bands CC&DH GN&C GN&C Redundancy Redundancy Sensors Design Autonomous  of  of Science Life Design Instruments Organizations Type of Mission Comm Surveill Science Mgt 7 9 In\(1  Accuracy orgs 4 In number of sci orgs 8 In number of sci orgs A cap 1  cap.2 cap.6 cap.8  capl.4 cap2 intIO.4 intl.8  intl1.2 A intl1.6 comm Mgtl 4 In\(1  Mgt In 1  M gt 8 ln\(1  Mgt nni We will present the following methods with particular attention given to the most successful one  Coefficient trend determination through static models and stepwise regression  Complexity index by automatically weighted inputs  Complexity index by analogy  Neural networks  Automated windowed coefficient trending  Boosting using binomial logit classifiers  Weighted combinations of simple models in but also whether a trend could be discerned for their coefficients Figure 6 shows the result of this analysis Again no strong trends could be inferred Complexity Index by Automatically Weighted Inputs For our next approach we adopted a common formula for estimating spacecraft cost cost a  mass complexity One published complexity metric has been formulated as a 10000.00 1000.00 100.00 Cu 40 _ 100 _ 0 1 00 1 0.10 mass.6 deployables design lifel.8 Complexity watts wattsO.4 watts 1.6 Inwatts newtons newtonsO.9 newtonsA1 1 newtons2 In\(1  newtons In\(1  newtons ln\(1  newtons ln\(1  newtons data data.6 ln\(1  data ln\(l  data x Accuracy^O.3 X AccuracyA1.7 I 


Data Rate Pointing accuracy Year 10 _I  Ill on_ _X5 4 31 Deployables Figure 7 shows OD Figure 7 Weights some extent is means were unlikely to be accurate Complexity By Comparison A metric capturing overall spacecraft complexity is was carried out a complexity index solved to minimize predictive was normalized by percent ranking error for the test set As time varies seen that optimal weights change according to seemingly predictable trends This to error was minimized The optimization algorithm also required starting points for weights and or any other a very seductive goal For cost estimating purposes spacecraft a spacecraft can be are listed in the z axis The heights of the bars represent the optimal weights found to minimize forecast were used to calibrate models that then forecast missions so that forecast a variety of settings so that data from previous chronologic windows a deterministic path For this reason a simple LEO orbit to missions requiring complex flight dynamics pointing knowledge etc The payload an artifact of the calibration process which used prior periods solved weights over the 2001-2005 timeframe this process across chronologic windows it one such outcome of this sliding calibration of weights Chronologic windows progress CD OC o C           r 0 O~~~~~~~~D00 O over time for factors contributing to error Each of these variables across the full prior sample Weightings were determined using a Generalized Reduced Gradient nonlinear optimization algorithm available in Microsoft Excel's Solver Weightings were solved were tested including a naive beginning setting all weights to 1 and using prior periods solved weights as the starting point The method was repeated with progressively more recent chronologic windows The first window covered missions from 1964-1975 and the last from 1991-2000 In this way we hoped to observe trends that could be extrapolated across the x axis as the starting point The chart above shows the smoothest transition of weights over time while other weight starting points resulted in less smooth transitions Over any 10-15 year period as can be observed the weights are highly unlikely to maintain we felt that future estimates using a complexity adjustment derived from reduced gradient methods are otherwise fairly difficult to describe Complexities in the mission profile can vary from can also vary from essentially off-the-shelf equipment to those having a low technology readiness level at time of conception A single metric summarizing the relative complexity of versus 8 Developme Apogee Miles Apogee Planetary nt Months Mission Dummy  of  of Primes Budget Int'l Project Customers Capped Adjusted Final Cost CD rI I I across the width of the page while factors 


others could carry critical information going beyond relatively blind factors such as mass power and number of instruments We therefore made several attempts to generate a complexity metric The complexity by comparison method requires a human analyst's assessment of the relative complexity of a mission versus others We used a tool that facilitates relative assessments within a relatively automated framework A relatively naive analyst made the assessments as we wanted to develop this approach on a worst-case basis to gauge the minimum estimating performance of this approach Comparisons were made against past missions only to replicate what an analyst would face when trying to predict the complexity of a future mission Through trial and error a model was obtained of this form ln\(cost  constant  ln\(months since 1960  attenuated variable 1  attenuated variable 2 Attenuated Variable 1  sum of two smallest of ln\(mass ln\(watts and ln\(complexity Attenuated Variable 2  sum of two largest of ln\(lifetime number of deployables and number of instruments Where complexity was estimated using the comparisons method Results were produced within chronologic windows starting in 1989-1994 using regressions calibrated Forecast Start Year 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 As can be seen in table 3 as time goes on an increasingly reasonable estimate band was obtained so that by the 20042008 timeframe using outside authorities predicted cost for the last few missions 5000 of estimates lay within 24%60%.6 While the results were fairly good versus other methods we were concerned with the average analyst's ability to make relative complexity assessments particularly given the uncertain nature of future projects We also were concerned that the better recent results were unwittingly due to the analyst's even passing knowledge of the current state of the art For a model intended to estimate contemporary missions these results are good news but our focus is on the future Neural Analysis Neural networks are an estimating method based on a network of interconnected nodes which adaptively reweight so that a set of inputs best approximates an expected output From the Wikipedia entry In more practical terms neural networks are non-linear statistical data modeling or decision making tools They can be used to model complex relationships between inputs and outputs or to findpatterns in data In testing this approach we divided our sample into a training set for calibrating the network and a test set for evaluating its predictive accuracy Neural networks train or adapt over epochs The wisdom of the neural network community is to avoid overtraining by stopping the neural Absolute  Error End Year 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2008 Observations 23 25 27 29 36 45 47 45 46 46 40 34 31 30 25 19 mm 0.090o 4.00o 4.340o 0.430o 0.36 0.3 1 4.20 5.70o 0.23 0.990o 0.02 4.16 14.98 0.070o 0.90o 4.0 1 25th pct 23.63 79.4000 136.83 15.78 10.21 27.610 68.72 276.85 55.63 19.311 18.84 28.52 56.02 26.32 32.55 23.9900 median 35.9000 141.44 187.30 46.510 47.12 67.19 120.99 395.87 87.25 38.10 46.95 43.350 67.61 46.65 47.63 40.3500 75th pct 87.84 256.36 327.70 101.60 101.21 132.111 217.12 626.50 194.111 86.45 96.95 57.94 74.990 63.34 67.49 59.3700 max 740.16 2453.47 3033.5300 1106.22 1258.14 760.75 1050.42 5529.24 2386.48 2586.72 1753.64 1626.94 522.98 220.73 177.75 438.33 average 85.48 294.15 364.01 107.97 98.85 110.77 196.02 656.36 211.42 158.28 125.177 108.24 77.32 56.57 55.45 62.110 Table 3 Results for an estimating method including user comparisons of relative spacecraft complexity over a ten year period ending 15 years before 6 For comparison with another analogy-based study though conducted with a different time horizon and using subsystem rollup rather than purely system-level estimating the reader may wish to consult Kellogg Mahr and Lobbia 4 9 


our normalization process which basically embedded the effect of time within the factors being input to Neural networks normally so an important step network run based on the test set's root mean squared error RMSE An illustration of the process is given in figure 8 RMS Error vs Training Time 0.22 0.16 0.11 Training Error 2"""I  Testing Error I I I I I I I I I I I I I I I I I I 200 400 600 800 10 Epochs Figure 8 Root mean square of prediction error over training epochs for both training and test sets Our empirical analysis indicated that a resilient propagation or RPROP network worked best Qwiknet is a commercial software implementation of this method Its help states This is an adaptive learning rate method where weight updates are based only on the sign of the local normalizing our data essentially converting it from an extrapolative problem to an interpolative one The normalization we undertook was transforming each field to its percentage rank vis-a-vis the observed minimum and maximum for that factor For example a design life of 24 months might lie in the 25th percentile for all observed design life and so on Training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 We tried to allow as much time as possible to elapse between the latter part of the training set and the latter part of the test set to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 9 and chronologically in figure 10 The graph above shows that MREs are large and negative below approximately 50M actual mission cost but then rapidly come within 5000 and closer perhaps again rising in the high cost range somewhere above 500M The correlation between estimates and actual values is approximately 0.82 In producing an actual model for use it may be possible to introduce an artificial trend correction indexed to cost which offsets the moderate trend observed above Sorted chronologically virtually no trend can be seen as Neural Network MRE By Cost Cost Figure 9 was in 10 o LM M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 was good used entirely logistic activation functions within three news for being able predict future missions and may have layers been due to Neural network magnitude of relative error sorted by actual cost gradients not their magnitudes Our network topology estimates go farther out up to 11 years This as classification including pattern recognition or function approximation They may tend to over-fit and we took seem better suited to the model interpolative applications such 


Automated Windowed Coefficient Trending Earlier we attempted to find explanatory models whose coefficients changed over time in a predictable way This would then let us predict future coefficient values for these models and apply these trended models towards future estimates The problem was that we were not able to find such stable models at least manually We automated our search for stable models by building a system we refer to as the Automated Coefficient Trend Search ACTS tool A patent is currently pending for the method implemented in this software Against the Far Out data set ACTS is designed to automatically permute fields run chronologically-windowed regressions with the chosen fields establish linear and nonlinear trends for the resulting coefficients and then use these trended coefficients to estimate a future set of data ACTS also automatically tries up to seven different transformations on any given dependant or independent variable The length of calibration windows and the increment between them are automatically varied ACTS establishes most promising coefficient trends and also rates the performance of predictions done using trended-coefficient models With so many parameters transformations windows length and intervals to evaluate ACTS needed to search a huge problem space Despite assembling a cluster of 8 computers to run ACTS if necessary for months the problem still required pruning Each run of ACTS was therefore restricted to producing models with a different number of variables between 3 and 15 We quickly found that the best models contained very few parameters due to over-fitting on wider models permitting us to pare down the problem Neural Netv 1.00 0.75 0.50 0.25 I  M The system produced literally thousands of forecasts for each project If the distribution of project costs and project costs estimates were known a confidence interval could be computed using the cost estimates and a method such as Bayesian model averaging used to integrate them 5 Not having a priori knowledge of this distribution we chose instead to use the inter-quartile range of the estimates that is we looked at whether the estimate fell between the first and third quartile of the estimates The top and bottom 25 were disregarded because there are always outliers and it was presumed the system could work well even when many of the individual estimates were not very good Given the very large number of estimates produced by the system literally thousands or tens of thousands depending on the settings we could afford to squander a few Results were best with only three variables per equation In eight out of 31 projects 26 of the projects the project's actual costs fell within the inter-quartile range of the forecasts of costs for that project produced by the ACTS models For 30 out of 31 projects the inter-quartile range of estimates was made up of over a thousand estimates while for the 3 1st project only 446 estimates were part of the inter-quartile range The significance of these numbers is revealed in the next paragraph When equations with four variables were used only one of the true project costs fell within the inter-quartile range of forecasts produced by ACTS When more than 4 variables were used none of the true project costs fell within the interquartile range From this we concluded that ACTS performed best when only 3 variables were included in each model Using more variables appears to produce over-fitted models Aork MRE By Lauch Month Chronologic 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Launch Month Since 1960 Figure 10 Neural network magnitude of relative error sorted chronologically 11 space 


incapable of accurate forecasts Of course the wider the inter-quartile range of forecasts the more likely it is that the true forecast will fit within that range But how parsimonious is the range As figure 11 shows in about 55\2600 of the inter-quartile ranges created by ACTS the 3rd quartile of estimates is 50%0 larger than the Ist quartile of estimates For about 90\2600 of the ranges created the difference between the I't and 3rd quartile is less than 75\2600 and for 97\2600 of the projects the top of the range is 100 or less than the bottom of the range 45\2600 of the true project costs falling into the new range When the top and bottom of the range were changed by 50\2600 the range estimates encompassed 77\2600 of the true project costs In the end however we felt that the adjusted ranges were too wide for practical use Adaboosting using binomial logit classifiers Short for adaptive boosting the Adaboost algorithm 6 is a type of machine learning algorithm where a set of classifiers are adjusted to favor previous misclassifications These classifiers are actually other estimating algorithms Percentage of Time 3rd Quartile no more than XO/o Greater than 1st Quartile 120 100 80 60 40 20 0 25 50 75 100 Figure 11 Variation in ACTS-produced estimates measured between 3rd and 1st quartile Percentage of time True Cost Falls in Range Produced by ACTS 3 variable ersion 90 8070 60 5040 30 20 10 l 0 Interquartile  or10  or25  or50  or75 Figure 12 The effect of widening the estimating range measured by  of estimates now lying within Of the eight projects that fell within the average percentage difference between the 1st and 3rd quartile was 55 with median of 52 This indicated that the inter-quartile range might be too restrictive We therefore proceeded to widen the range of cost estimates for each project multiplying the ISt quartile estimate by 1 X%0 and the 3rd quartile estimate by 1  X using various Xs In each instance we compared actual project costs to estimates produced and determined how frequently the actual fell into the range of estimates Results are summarized in figure 12 When the top and bottom of the range were changed by 10 32 of the actual estimates fell within the new range Extending each of the range boundaries by 25 resulted in Adaboost works by iteratively weighting observations so that poorly classified ones are given more weight during the next iteration With the algorithm focused on improving estimates for exceptional cases it is somewhat more sensitive to noisy data and outliers though less likely to over fit We chose to test this approach due to the latter property The boosting algorithm seemed structured particularly for binary decisions though we are seeking to estimate a continuous variable cost We therefore staged the method by estimating cost bands so that the estimate for each band reverted to a binary decision The algorithm would separately estimate whether an observation was 12 


our evaluation method tested potential models using either ordinary least squares least absolute above or below X dollars then Y dollars then Z such that instead namely that we were examining a proportion rather X  Y  Z than a binary outcome but this was impractical to use and we judged that the problem was not strongly biased towards The boosting algorithm still required a classifier and model either context For a classifier we implemented binary logit78 a discrete Adaboost MRE By COST Lu was similar to ACTS but with 0 1.00 0.75 0.50 0.25 0.00 0.25 0 50 0 75 1 00 1 25 1 50 1 75 2 00 Cost Figure 13 Adaboost magnitude of relative error sorted by actual cost Adaboost MRE CHRONOLOGICALLY Year Figure 14 Adaboost magnitude of relative error sorted chronologically choice model specifically intended for a binary dependant variable is the cost above or below this point There may have been some support for using the probit model 7 This was done by a statistical programmer with output checked against the statistics program STATA 8 we obtained from this first stage system We also attempted to use OLS as a classifier but our experiments showed no significant effect from adding relatively orthogonal i.e unrelated models Since using more than one model did not make a difference boosting using OLS was little more than a single OLS model with estimates divvied up in bands no dynamic re-weighting of classifiers and thus no boosting effect to speak of To obtain models some changes and improvements and again put were discarded and resolved by the latter 13 Lu 1.00 0.75 0.50 0.25 0.00 0 0.50 0.75 1 00 1.25 1 50 1 75 2 00 error which better discounts outliers and weighted least squares The models which we built another windowed test system that our computing cluster to use trying to find good ones from were then used to bootstrap the binary logit method although coefficients a sea of potential transformations This time 


This exercise required a number of decisions about how many classifiers to use how many bands to estimate and how many variables should be used in the models All these choices were made empirically Models with 3 variables were marginally more accurate than those with 4 More models seemed generally better to a point so we used 99 models We also used 22 cost bands checking whether an estimate was above or below 12M 35M 44M and so on through 519M It is interesting to note that there was no data scarcity-induced limit on the number of bands we could have used since each was estimated using all observations lying above and below and not just between bands As with the neural network training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 The trend in estimate deviation would again permit us to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 13 and chronologically in figure 14 Figure 13 shows that MREs are large and negative for missions at 56M actual mission cost and below Above this point results are mixed although not as poor as at the very low range There seems to be no degradation in predictive accuracy as projects increase in scale The correlation between estimates and actual values is approximately 0.66 Sorted chronologically in figure 14 no trend can be seen as estimates go farther out up to 11 years as was seen with the neural network Again this may have been due to the normalization process in which we mapped each variable to its percentage ranking based on future and past minimum and maximum values basically embedding the effect of time within the factors being input to the model It also may be due to a relatively stable era Weighted combinations of simple models A persistent problem in multivariate estimation methods which has been mentioned is that a model may be produced that over fits the observations used for calibration It would thus not be adequately generalized to estimate out of that sample Techniques such as jack-knifing and outlier removal can mitigate this We chose another approach developing models that were little more than simple rules of thumb and then combining these in a weighted manner so each would have a say in the estimate The weightings were formulated using Excel's Solver to minimize estimation error on a set observations lying 15 to 25 years before the test set We refer to this method as continuous boosting because as with the Adaboost method it too involves a mix of classifiers that are combined according to their performance against a set of test observations Unlike Adaboost which solves a binary problem the algorithms are here estimating on a continuous scale Also unlike Adaboost the models are re-weighted rather than the observations to maximize prediction Before presenting results it is worth noting that we ran a control test where all variables from the simple rules of thumb were combined into a single model and run on the same windows performance was nowhere near as good We chose the models by hand guided by the criteria that they should be relatively orthogonal rely on differing parameters so they could each contribute a unique influence to the combined result Figure 15 shows that MREs are mostly positive though for lower cost missions errors are regularly negative and large Continuous Boost MRE By COST M M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Cost Figure 15 Continuous boosting magnitude of relative error sorted by actual cost 14 


CONCLUSIONS Given that this project was intended to estimate missions lying 10-15 years out we structured it differently than one intended to estimate contemporary projects A variety of conventional techniques were not used as we felt they would over fit training observations and thus not be suitable for prediction We also were not sure which approach would work so we tried many We heavily favored ensemble methods where models are combined because we surmised that any one model could not be guaranteed to have the best view of the future However a single neural network ultimately yielded the most competitive results Another finding was that methods built upon simple models such as with three variables generally did work best not surprisingly because they were less likely to over fit calibration data A graph of the three best methods is shown in figure 18 with estimates for the same missions sorted by cost For the neural network calibration occurred until just before the results shown for continuous boosting calibration occurred no more recently than 15 years prior to the results and for Adaboost calibration also occurred up until the results shown A graph sorted by year is not shown we think it instructive that no results seemed to degrade over time As mentioned but obvious from figure 17 the neural network performs best followed by Adaboost and then continuous boosting It is interesting to note that the three cases in which the neural network goes haywire and predicts too low also correspond to worst performances for the Adaboost method pointing to exceptional data points We will be further investigating these regularly errant results and other outliers which may result in estimating improvements ACKNOWLEDGEMENTS This work was carried out under Small Business Innovation Research contract FS9453-05-C-0023 with the Air Force Research Lab The authors wish to gratefully acknowledge Judy Fennelly and later Ross Wainwright our technical points of contact at AFRL for their continuous support and encouragement Our contract officer Timothy Provencio also provided invaluable assistance Our critical seed stock of data was provided by Joseph Hamaker who previously was Director of NASA Headquarters Cost Analysis Division and now is Senior Cost Analyst with SAIC Ainsley Chong and Dale Martin USAF Ret also lent considerable assistance with data gathering APPENDIX A MISSIONS COLLECTED Active Cavity Radiometer Irradiance Monitor Satellite Active Magnetospheric Particle Tracer Explorer Adeos Advanced Communications Technology Satellite Advanced Composition Explorer Alexis Amos-I AMSC-1 Anik El Anik E2 Applications Technology Satellite-I Applications Technology Satellite-2 Applications Technology Satellite-5 All MREs Lu 1.00 0.75 l 0.50l 0.25 l 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75-2.00oi v 1   Cost o N X  X~~~~~Cl Figure 16 Comparison of estimating error for three best methods 15 


Applications Technology Satellite-6 Aqua Argos Atmospheric Explorer AURA Aurora 2 Calipso Cassini Cassini Spacecraft  Huygens Probe Chandra X Ray Observatory CHIPSat Clark Clementine CloudSat COBE Columbia 5 Contour CRRES DART Dawn DBS-1 Deep Impact Flyby Spacecraft  Impactor Deep Space 1 Deep Space 2 Defense Meteorological Satellite Program-5D Defense Meteorological Satellite Program-5D3 Defense Support Program DSCS 3 FIO DSCS 3 F7 DSCS I DSCS-II DSCS-IIIA DSCS-IIIB Dynamics Explorer-I Dynamics Explorer-2 Earth Observing Satellite 1 Earth Radiation Budget Experiment EchoStar 5 Extreme Ultraviolet Explorer Far Ultraviolet Spectroscopic Explorer FAST FLTSATCOM 6 Galaxy 5 Galaxy 11 Galaxy Evolution Explorer Galileo Orbiter  Probe Gamma Ray Large Area Space Telescope GE 1 GE 5 Genesis GFO 1 Globalstar 8 Glomr GOES 3 GOES 9 GOES N GPS-1 GPS-IIR GPSMYP GRACE Gravity Probe-B GRO/Compton Gamma Ray Observatory GStar4 Hayabusa HEAO-1 HEAO-2 HEAO-3 HESSI-II High Energy Transient Explorer-II HETE HST ICESat Ikonos IMAGE IMP-H Inmarsat 3-F5 Intelsat K INTELSAT-II INTELSAT-IV International Ultraviolet Explorer Iridium James Webb Space Telescope Jason 1 JAWSAT KEPLER KOMPSAT LANDSAT1 LANDSAT-4 LANDSAT-7 Lewis Lunar Orbiter Lunar Prospector Magellan Magsat Mariner-4 Mariner-6 Mariner-8 Mariner1 0 MARISAT Mars Exploration Rover Mars Express/Beagle 2 Mars Global Surveyor Mars Observer Mars Odyssey Mars Surveyor 2001 Orbiter Mars Pathfinder  Sojourner Rovers Mars Polar Lander Mars Reconnaissance Orbiter Mars Telecommunication Orbiter Mars Climate Orbiter Messenger Meteor Mid-course Space Experiment MightySat Milstar 3  Adv EHF Model-35 Morelos NATO III Near Earth Asteroid Rendezvous NEAR Shoemaker New Horizons 16 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


