Proceedings of The IEEE International Conference on Industrial Technology 1996 Design of a Neural-Fuzzy Controller Based on Fuzzy Differential Competitive Learning Ge Ming Institute of Industrial Process Chtrol Hangzhou 3 10027 CHINA Abstraa In this paper a novel neural-fuzzy cuntroller based on fuzzy difFeretrtial competitive learning is proposed Since one of the most impoitant parts is the generation of the fuzzy rules in the design of the fuzzy control system a fast leaming algorithm-Fuzzy Merentid COntpeMive Leaming\(FDCL for the generation of the rules is applied 
in the fuzzy control system The FDCL algorithm adopts a principle of leam according to how well it wins Unlike the previous competitive learning algorithm such as ciiSp competitive learning algorithms where only one neuron will win ad leam at each competition step every neuron in the neural network based on FDCL algciithm will along with its different distance to the input pattern and leams the pa accordingly compared with the ordmary competitive learning algorithm the propod FDCL algorithm has the various distinguishing features The 
FDCL algorithm is implanted in the neural network based fuzzy system and the network adopted is fuzzy associative memory system\(FAMS which simulates the knowledge 7 221on and inference procas by using fuzzy notation and by asoociation in neural networks In FAMS the fuzzy rules will be generated by clustering the input output training data through the FDCL paradigm By using the FDCL algorithm the neural network can highly refine knowledge and represead the expert elgxxience I  Introduction During the past decade fuzzy logic control 
has been widely used in many various fields such as industrial process control Fuzzy logic control is usually based on a set of fuzzymles that sum up expert\222s common sense and experience But sometimes people maybe find it is difficult to get adequate fuzzy rules especially when certain complicated dynamic processes are comemed Now there are many approaches to obtain the fuzzy rules of some specrfc processes[l][2][3 and the fuzzy logic system designed based on these methods can also been successfully applied however these ways are quite problem dependent, i.e a method may work well for one problem but is not suited for another problem The other drawback of current fuzzy logic control is that there is no systematic procedure for the design 
of fuzzy logic control system and thus make it very difficult for people to analyze the properties of the fuzzy logic system In recent years the deal neural network has been widely combined with the fuzzy logic system for it\222s learning capability and parallel structure But in these research[4][5 on the neural-based fuzzy logic control system some problems exist: \(1\The fuzzy rules identified by the neural networks are hard to mderstand because these rules are implicitly acquired in the networks 2 The learning of the networks is time consuming 3 Some informating concerning the process are mising or not used in the identification of the fuzzy rules This paper presents a new 
kind of clustering meth Sun YiuXian Institute of Industrial Process Control Hangzhou 3 10027 CHINA Fuzzy-Differential-Competitive-learning\(FJlCL to obtain the fuzzy rules based on fuzzy associative memories\(FAM system The FAM system provided by Kosko which integrates neural and fuzzy logic and some learning laws is used to learn the causal structure of the system[6 In the designing of the FAM system  it is most important to select some efficient learning law to clustering the fuzzy rules The clustering methods we proposed here is based on a kind of competitive learning and it\222s effectiveness is verified through the control of the inverted pendulum system In Section 11 
we propose a four step proceduae for generating fuzzy rules from sampled input-output data pairs by\221the FDCL method In section m the new clustering method is applied to an inverted pendulum control system and this approach is compared with the FAM system using other clustering methods such as the Differential-Competitiveompetitive-learning method Conclusions are giyen in SectionIV 11  Fuzzy-Differential-Competitive-Learning The goal of the merential competitive learning is to cluster or categorize the training paneins into some representative groups so that patterns within a cluster are more similar to each other than patterns belonging to different clusters The differential competitive learning has been suggested as an alternative approach to various sophisticated problems 
such as the control of nonlinear time-varying ill-defined systems The model is usually associated with a layered neural feedforwad network in which hidden neurons compete according to some sort of distance metric usually the Euclidean one to learn the current input pattern  If the jth neuron wins it\222s parametric vector is updated additively by some propoition of the difference vector  6 The differential competitive learning dgorithm is described below and then we will make some modification which lead to the gew version of the competitive learning algorithm named F.Jzzy-D~erentia-~o~~titive Algorithm Learning FDCL algorithm Differential Competitive Learning law CL mirJ  I,AS,\(Y  4 0-7803-3 104-4 697 


or mJ\(t  l f ct~~,\(y,\(t mdt where ct is the learning rate and AS y t denotes the time change of the jth neuron's competitive signal s y t  in the competition field Fy  1 mi\(t  1 7 mi\(t  z*J In practice we often use only the sign of the signal difference 2 or sgn Ay the sign of the activation Werence[6 Kosko has pointed out that the Fy neuronal activations y can be updated with an additive model yj t   y J I gsZ l t Sk\(yk 3 z=l 1=1 The fixed competition matrix W defines a symmetric lateral inhibition topology within Fy  In the simplest case wy=-l and wJl=l for distinct i andj  The differential competitive learning adaptively quantities the input pattern space R and it has been proved that the competitive synaptic vectors mJ capl cowkrge exponentially quickly to pattemclass centroid 6 The clustering algorithm using the differentid learning law is described below 1.Set the number of competing neurons and initialize the neuron's synaptic vectors mz\(0 i m 2 For random sample x\(t the closest\("winning synaptic vedtor mJ{t 4 where llxll is the distances between competing neurons there are many different distance metric can be employed and the most common distance metric is the squared Euclidean norm 3 Update the winning neuron's parametric vector using the differential competitive learning law 1 Although there are many successful applications using the DCL algorithm the exclusive learning mech,?nism has two drawbacks  one is the neuron underutilizing problem 7][8 and the other is that the information concerning the closeness of input patterns and competing neurons is wasted during the winner-take-all training process because the winner takes all the responsibility for learning the current input pattern in the DCL algorithm The differential competitive learning law can be rewritten as other form mJ      m J  Ct 1 A SI y J    m J t 0 d 1'1 6 If Z=J I As shown in 6 the indicator function 4 is an crisp\(hard\function so that in the competing process only one neuron will win and learn the current training pattern Obviously the concept of win in this setting is a crisp one and has a very clear-cut boundary By considering win as a fuzzy set every neuron to a certain degree wins depending on its distance to the current training pattern Therefore it has to learn according to its win membership during the competition In this way  we can replace the none-fuzzy indicator function with a fuzzy indicator function that means the indicator function is a fuzzy scaling function smng the sign and magnitude of the difference vector 9 Below we will derive the fuzzy differential competitive learning via minimization of an objective function Let us study a collection of n patterns constituting vectors in the p-dimensional space of real numbers, namely XI 2  xn CAP  We assume that the structure contains c clusters The Objecting function will be introduced as the following sum 7 SllbJected to iUik=l Vk I urt E[0,11 Vk;z n O<xu,k<n Vk k=l where  u,k E 0,1 denotes the grade of membership of the k-th pattern in the i-th cluster The parameter m is used to control the influence of intermediate membership values on the performance index l<m 03  The function 6 Ik is usually defined as the distances between the k-th pattern and the generic structure of the clusters which involves both points and linear varieties where The function Dlk involves dzk and adds also a linear variety of dimension r expressed by a scalar prokct      This variety goes through the vector mJ and is spanned by the collection of r linearly independent vectors sy[lO The role of the parameter g is to keep the balance between these two components\(d,k and Dlk  By applying the gradient descent method to the objective function J a fuzzy differential competitive learning law can be obtained 


9 Since the derivation of these formulae is the same as that of the fuzzy clustering algorithm that has already been studied in lo  so the procedure of this derivation is not discussed in detail From the derived algorithm, it can be seen that every neuron is responsible for learning the current training pattem rather than only a winning neuron has the responsibility It is already mentioned in the front of this section that normal Werentid competitive learning has two disadvantages and the disadvantages maybe lead to the failure of the learning however the fuzzy differential competitive learning will reduce the probability of the learning failure and its effectheness will be demonstrated in the section ID From the equation 9 ad 10 we can see that the membership U,k is changed with the distance between the neuron and the pattern that is the closer the neuron to the pattern the larger its win membership UIk is and the more it has to learn However the far-away neurons has also the chance to update its synaptic vector and can be moved to some pattern regions Therefore the problem of the old differential competitive learning that it waste the idormation concerning the closeness of input patterns and competing neurons is solved In the section ID we will integrate this new clustering algorithm with the FAM system and develop a fuzzy contrcl system Although it has been proved that the synaptic vectors converge to decision-class centroids that correspond to local of the sampled but unknown probability density function Ax through the Werentid competitive learning 6 the hzzy differential comwtive learning is not yet proved and will re studied In the following discus sion we will only study the centroid theorem because it is very important for the fuzzy clustering process and the others are discussed sidarly by Ksoko[6 Suppose that the decision classes D1 D2  4 partition R\223 into k classes Rn=D1UD2  U Dk 1 1 DlnD2zo ifi*j 12 Centroid defines the deterministic 223center of mass\224 of pattem class Dj 161 13 J4Asj\(y a  I,P\(X The centroid theorem will be first proved that if a fuzzy differential competitive learning system converges it con verges to the centroid of the sampled decision class Centroid theorem where q is the centorid of the sampled decision class Prob mi 1 at equilibrium 14 proo Suppose thejth neuron in Fy wins the activation   699  competition during the training interval and the jth syna@.ic vector aj codes for decision cl&s Dp The stochastic fuzzy differential competitive learning equations is adopted in the proof for In practice we actually use the random process for studymg The stochastic fuzzy differential competitive learnihg law 9 can be rewritten as i As,\(~,k-m,]+n 15 where nj represents a zero-mean Gaussian white-noise random process 4 is defined in 9 Suppose the synaptic vector mj has reached equilibrium mj  16 which holds probability one. Take expectatiops of both sides of 15 use the zero-mm property of the noise process eliminate the synaptic vector hj with the stochastic fuzzy Werentid competitive leaning 14 and expand to give  221 qk  lps,\(Y,p mJ x Pendulum System with the Fuzzy DitTerential-Competitive-~ng-Algorithm\(F\222DCL and the normal Differential-Competitive-Learning-Algorithm CL Controlling of the Inverted Pendulum System is a hard task becapse e pendulum system is a non-linear system for which there are no traditional control system design methods Ksoko has proposed the fuzzy control strategy for the pendulum system with the DCL algorithm Now we use the FDCL algorithm to control the same problem and compared it with the DCL algorithm The Inverted Pendulum System is composed of a rigid pole and a cart on which the pole is hinged The cart moves on the rail tracks to its right or left depending on the force exeFed on the cart The control aim is to balance the pole starting from nonzero conditions by supplying appropriate force the cart The dynamics of the Inverted Pendulum System are characterized by four state variables 8 angle of the pole with respect to vertical axis 8 222\(angular velocity of the pole z position of the cart on the track and z\222\(vel0City of the cart The relationship between these variables is determined by two second-order Merentid equations described by 9 


IV.CONCLUSIONS  m+m We use the equations to generate the desired Input Output data pairs 8 8  F  F is the force that balance of the pendulum system Because the detailed process of controlling the pendulum system and the constructing of the FAM@uzzy Aslsociative Memory system is similar to the previouS methd proposed by Ksoko except for the implanted learning algorithms thus we only give the results of simulation of these two laming algorithms\(FDCL and DCL Figure1 shows the results of the two Merent control methods 15 10 hgle5 0 5 01 2 3 4 5.6 7 Titne\(Sec a 10  I 30 I 01234567 15 10 Force5 0  5 L O 12 3 4 5 6 7 Time\(Sec i c Fig 1 a Pole angle  pole angular velocity c balance force Solid dashed curves with regard to difFerent leaming methods DcL mL respectively In this paper we have proposed a Fuzzy-Differential Competitive-Leaming-Algorithm\(FDCL and implanted it in the FAM system for co"cting a Neural-Fuzzy controller By comparing of the results of the two simulations with two learning algorithms we can find that there are some impravements by using the FDCL algorithm V  REFERENCE 1 E.M Scharf and N.J Mandic The application of a fuzzy controller to the control of a multidegree-freedom robot arm Industrial Application of Fuzzy Control M.Sugeno Ed Amsterdam North Holland 1985, pp 41 62 2 S Shao Fuzzy self-organizing controller and its application for dynam~c processes Fuzzy Sets Syst 3 R Tanscheit and E.M Scharf experiments with the use of a rule-based self-organizing controller for robetics applications Fuzzy Sets Syst Vo1.26 pp 195-214, 1988 4 A.G Barto RS Sutton and C.W Anderson Neuronlike adaptive elements that can solve difficult leaming control problems JEEE Trans Syst Man Cybern Vol SMC-13,nOS pp.834-847 1983 5 F.C Chen Back-propagation neural network for nonlinear self-tuning adaptive control Proc IEEE Intelligent Machine pp 274-279, 1989 6 B Kosko Neural Networks and Fuzzy Systems Englewd Cliffs NJ Rentice-Hall 1992 7 Ahalt S.C Krishnamurthy A.K Chen P Melton D.E Competitive Learning algorithms for vector quantization  Neural Networks 3 pp.277-290 1990 8 Grossberg S Adaptive pattern classification and universal reaxling 1. Parallel development and coding of neural feature detectors Biological Cybemetics vol 23 9 A.G.Barto RS.Sutton, and C.W.Anderson, "Neronlike adaptive elements at can solve difficult leanking control problems IEEE Trans Syst. Man Cybern vol SMC-13 no. 5, pp.834-846 1983 10 Y.Yoshinari W.pedrycz and K Hirota Construction of Fuzzy models through clustering techniques Fuzzy Sets and Systems Vo1.54 pp 157-165 1993 V01.26 p 151-164 1988 121-134  700 


then for each non-empty subset a off whether the association rule a 3 If  a holds or.inot is determined by computing the confidence  supportv a If confidence 2 minimum confidence then the rule holds a is called the ascendant of the rule and If a is called the consequent of the rule The candidate ascendant are all possible subsets of the frequent itemset whose length are from 1 to the length of the frequent itemset minus 1 and the corresponding candidate consequents are those left in the frequent itemset 4.5 Example of Mining Association Rules For this specific project the end users are assumed to be doctors specialists or anyone interested in skin cancer who have knowledge about skin'cancer and know what kind of associations they are looking for Therefore to find association rules among which attributes is totally dependent on the end user The interface is designed such that user can find association rules among any combination of the attributes In the interface every attribute in the target table is listed as shown in Figure 7. Except the Malignant attribute a list box is used to select the number of intervals to discretize the attribute values The user can select a value between 1 and 10 for the number of intervals Different attributes can be discretized with different number of intervals The attribute Malignant in the table has only three values malignant benign and pre malignant so there is no need to discretize it Two sliders allow the user to determine the minimum support and the minimum confidence After the user specifies hidher requirements and clicks the Mining button the program begins Valid association rules are displayed to the user as shown in Figure 7 Mining Skin Cancer Database Boundary-Irregularity 1.0791 to 1.1707  Malignant  pre-malignant  Boundary-Irregularity 1.1707 to 1.708 1   Malignant  malignant  Asymmetry 0.0999 to 0.1467  Malignant  benign  Asymmetry 0.2318 to 0.3857  Malignant  malignant  From the rules above we can observe that high boundary irregularity or high asymmetry of the tumor is associated with malignant This coincides with the experts experience 5 Summary In this project we developed a web-based data browsing and content-based retrieval system for a skin cancer database Users can query the database by any combination of the feature attribute values or by synthesized image colors Range queries exact match queries and similarity-based queries are allowed based on any image feature Another contribution of this project is the implementation of an association rule mining algorithm which was originally developed for mining transaction databases Users can find association rules between different skin cancer feature values which can be every useful for skin cancer diagnosis and study References l National Cancer Institute NCI What You Need to Know about Skin Cancer 1995 2 L Xu M Jackowski A Goshtasby C Yu D Roseman and S Bines, "Segmentation of Skin Cancer Images," Image and Vision Computing, 17\(1\1999 pp. 65-74 3 J E Golston W V Stoecker R H Moss and I P S Dhillon Automatic Detection of Irregular Borders in Melanoma and Other Skin Tumors Computerized Medical Imaging and Graphics 16\(3\1992 pp 188-203 4 W V Stoecker W W Li and R H Moss Automatic Detection of Asymmetry in Skin Tumors Computerized Medical Imaging and Graphics 16\(3 1992 pp I91  197 SI R Jain R Kasturi and B G Schunck Machine Vision McCraw-Hill 1995 6 D. H. Ballard and C M. Brown Computer Vision Prentice Hall 1982  P Adriaans and D Zantinge Data Mining Addison Wesley 1996 8 R Agrawal and R Snkant Fast Algonthms for Mining Association Rules Proc of VLDB Conference 1994 pp 487-499 9 M Houtsma and A Swami Set-oriented Mining for Association Rules in Relational Databases Proc of Int Conference on Data Engineering 1995 pp. 25-33 Figure 7. Valid association rules are displayed Some examples of mined association rules are listed below when the minimum support is 12 and the minimum confidence is 60 Boundary-Irregularity 0.9022 to 1.0178  Malignant  benign  615 


4.2.1 The Round Robin Algorithm The main idea behind the Round Robin Algorithm denoted by RRA is rather than selecting a unique victim item per given restrictive association rule we select different victim items in turns starting from the rst item then the second and so on in each sensitive transaction The process starts again at the rst item of the restrictive rule as a victim item each time the last item is reached The rationale behind this selection is that by removing one item at a time from the sensitive transactions it would alleviate the impact on the sanitized database and the legitimate association rules to be discovered since this strategy tries to balance the decreasing of the support of the items in restrictive association rules Selecting the sensitive transactions to sanitize is simply based on their degree of con”ict Given the number of sensitive transactions to alter based on   this approach selects for each restrictive rule the sensitive transactions whose degree of con”ict is sorted in descending order The rationale is that by sanitizing the con”ict sensitive transactions that share a common item with more than one restrictive rule this optimizes the hiding strategy of such rules in one step and consequently minimizes the impact of the sanitization on the discovery of the legitimate association rules The sketch of the Round Robin Algorithm is given as follows Round Robin Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i thei th item is assigned to item v mod k in round robin fashion Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of con”ict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End The four steps of this algorithm correspond to the four steps described above in the beginning of this section The rst step builds an inverted index of the items in D in one scan of the database In step 2 the victim item Victim rr i is selected in a round robin fashion for each restrictive associationrule.Line1instep3showsthat  is used to compute the number NumbTrans rr i of transactions to sanitize This means that the threshold  is actually a measure on the impact of the sanitization rather than a direct measure on the restricted association rules to hide or disclose Indirectly  does have an in”uence on the hiding or disclosure of restricted association rules There is actually only one scan of the database in the implementation of step 4 Transactions that do not need sanitization are directly copied from D to D   while the others are sanitized before copied to D   In our implementation the sensitive transactions to be cleansed are rst marked before the database scan for copying The selection of the sensitive transactions to sanitize T ransT oSanitize is based on their degree of con”ict hence the sort in line 1 of step 4 When a transaction is selected for sanitization only the victim items are removed from it line 3.1 in step 4 4.2.2 The Random Algorithm The intuition behind the Random Algorithm denoted by RA is to select as a victim item for a given restrictive association rule one item of such rule randomly Like the Round Robin Algorithm the rationale behind this selection is that removing different items from the sensitive transactions would slightly minimize the support of legitimate association rules that would be available for being mined in the sanitized database Selecting the sensitive transactions to sanitize is simply based on their degree of con”ict We evaluated the sanitization through the Random Algorithm by selecting sensitive transactions sorted in ascending and descending order The approach based on descending order in general yielded the best results That is why we have adopted such an approach for our algorithm The sketch of the Random Algorithm is given as follows Random Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i  the item assigned to item v is random\(k Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of con”ict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00 © 2003 IEEE 


The four steps of this algorithms correspond to those in the Round Robin Algorithm The only difference is that the Random Algorithm selects the victim item randomly while the Round Robin Algorithm selects the victim item taking turns 5 Experimental Results We performed two series of experiments the rst to measure the effectiveness of our sanitization algorithms and the second to measure the ef“ciency and scalability of the algorithms All the experiments were conducted on a PC AMD Athlon 1900/1600 SPEC CFP2000 588 with 1.2 GB of RAM running a Linux operating system To measure the effectiveness of the algorithms we used a dataset generated by the IBM synthetic data generator to generate a dataset containing 500 different items with 100K transactions in which the average size per transaction is 40 items The effectiveness is measured in terms of the number of restrictive association rules effectively hidden as well as the proportion of legitimate rules accidentally hidden due to the sanitization We selected for our experiments a set of ten restrictive association rules from the dataset ranging from two to ve items in length with support ranging from 20 to 42 and con“dence ranging from 80 to 100 in the database We ran the Apriori algorithm to select such association rules The time required to build the inverted le in main memory was 4.05 seconds Based on this inverted le we retrieved all the sensitive transactions in 1.02 seconds With our ten original restrictive association rules 94701 rules became restricted in the database since any association rule that contains restrictive rules should also be restricted 5.1 Measuring effectiveness In this section we measure the effectiveness of our algorithms taking into account the performance measures introduced in Section 3.3 We compare our algorithms with a similar one proposed in 4 t o h i d e r ul es by reduci ng s upport called Algo2a The algorithm GIH designed by Saygin et al 9 i s s imilar to Algo2a The bas ic dif ference is that in Algo2a some items are removed from sensitive transactions while in GIH a mark  unknowns is placed instead of item deletions Figure 4 shows a special case in which the disclosure threshold  is set to 0 that is no restrictive rule is allowed to be mined from the sanitized database In this situation 30.16 of the legitimate association rules in the case of RRA and RA 24.76 in the case of Algo2a and 20.08 in the case of IGA are accidentally hidden While the algorithms proposed in 4 9 h i d e r ul es reducing their absolute support in the database in our frame        0 5 10 15 20 25 30 35 IGA RRA RA A l g o2a Sanitizing Algorithms Misses Cost   IGA  RRA  RA  Al g o2a  Figure 4 Effect of  on misses cost work the process of modifying transactions satis“es a disclosure threshold  controlled by the database owner This threshold basically expresses how relaxed the privacy preserving mechanisms should be When  0  no restrictive association rules are allowed to be discovered When   100  there are no restrictions on the restrictive association rules The advantage of having this threshold is that it enables a compromise to be found between hiding association rules while missing legitimate ones and nding all legitimate association rules but uncovering restrictive ones Figure 5 shows the effect of the disclosure threshold  on the hiding failure and the misses cost for all three algorithms considering the minimum support threshold   5  Notice that RRA and RA yielded basically the same results That is why their curves are very identical at the scale of the gure As can be observed when  is 0 no restrictive association rule is disclosed for all three algorithms However 30.16 of the legitimate association rules in the case of RRA and RA and 20.08 in the case of IGA are accidentally hidden When  is equal to 100 all restrictive association rules are disclosed and no misses are recorded for legitimate rules What can also be observed is that the hiding failure for RA is slightly better than that for the other approaches On the other hand the impact of IGA on the database is smaller and the misses cost of IGA is the lowest among all approaches before   75  After this value all the algorithms yield similar results Regarding the third performance measure artifactual patterns one may claim that when we decrease the frequencies of some items the relative frequencies in the database may be modi“ed by the sanitization process and new rules may emerge However in our experiments the problem artifactual pattern AP was always 0 with all algorithms regardless of the values of   Our sanitization indeed does not remove any transaction The same results can be observed for the algorithms presented in 4 9 We could measure the dissimilarity between the original and sanitized databases by computing the difference between their sizes in bytes However we believe that this Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 0255075100 Disclosure Threshold Hiding Failure IGA RRA RA 0 5 10 15 20 25 30 35 0255075100 Disclosure Threshold Misses Cost IGA RRA RA Figure 5 Effect of  on the hiding failure and misses cost dissimilarity should be measured comparing their contents instead of their sizes Comparing their contents is more intuitive and gouges more accurately the modi“cations made to the transactions in the database To measure the dissimilarity between the original and the sanitized datasets we simply compare the difference of their histograms In this case the horizontal axis of a histogram contains all items in the dataset while the vertical axis corresponds to their frequencies The sum of the frequencies of all items gives the total of the histogram So the dissimilarity between D and D denoted by dif  D D   isgiven by dif  D D   1  n i 1 f D  i   n  i 1  f D  i   f D   i  where f X  i  represents the frequency of the i th item in the dataset X 0 1 2 3 4 5 6 7 IGA RRA RA A l g o2a Sanitizing Algorithms Dissimilarity IGA RRA RA Al g o2a Figure 6 Difference in size between D and D Figure 6 shows the differential between the initial size of the database and the size of the sanitized database when the disclosure threshold  0  To have the smallest impact possible on the database the sanitization algorithm should not reduce the size of the database signi“cantly As can be seen IGA is the one that impacts the least on the database In this particular case 3.55 of the database is lost in the case of IGA 6 in the case of RRA and RA and 5.24 in the case of Algo2a 0 1 2 3 4 5 6 7 0 25 50 75 100 Disclosure Threshold Dissimilarity IGA RRA RA Figure 7 Difference in size between D and D Figure 7 shows the differential between the initial size of the database and the size of the sanitized database for our three algorithms with respect to the disclosure threshold   Again IGA is the one that impacts the least on the database for all values of the disclosure threshold   Thus as can be seen the three algorithms slightly alter the data in the original database while enabling exibility for someone to tune them 5.2 CPU Time for the Sanitization Process We tested the scalability of our sanitization algorithms vis a-vis the size of the database as well as the number of rules to hide Our comparison study also includes the algorithm Algo2a We varied the size of the original database D from 20K transactions to 100K transactions while xing the disclosure threshold  and the support threshold to 0 and keeping the set of restrictive rules constant 10 original patterns Figure 8A shows that IGA RRA and RA increase CPU time linearly with the size of the database while the CPU time in Algo2a grows fast This is due the fact that Algo2a requires various scans over the original database while our algorithms require only two Note that our algorithms yield almost the same CPU time since they are very similar Although IGA sanitizes less sensitive transactions it has an Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00 © 2003 IEEE 


overhead to group restrictive association rules that share the same items and optimizes this process We also varied the number of restrictive rules to hide from approximately 6000 to 29500 while xing the size of the database to 100K transactions and xing the support and disclosure thresholds to   0 Figure 8B shows that our algorithms scale well with the number of rules to hide The gure reports the size of the original set of restricted rules which varied from 2 to 10 This makes the set of all restricted rules range from approximately 6097 to 29558 This scalability is mainly due to the inverted les we use in our approaches for indexing the transactions per item and indexing the sensitive transactions per restrictive rule There is no need to scan the database again whenever we want to access a transaction for sanitization purposes The inverted le gives direct access with pointers to the relevant transactions The CPU time for Algo2a is more expensive due the number of scans over the database 6 Related Work Some effort has been made to address the problem of privacy preservation in association rule mining The class of solutions for this problem has been restricted basically to randomization data partition and data sanitization In this work we focus on the latter category The idea behind data sanitization was introduced in 1 Atallah et al considered the problem of limiting disclosure of sensitive rules aiming at selectively hiding some frequent itemsets from large databases with as little impact on other non-sensitive frequent itemsets as possible Specifically the authors dealt with the problem of modifying a given database so that the support of a given set of sensitive rules mined from the database decreases below the minimum support value The authors focused on the theoretical approach and showed that the optimal sanitization is an NPhard problem In 4 th e a u t h o r s i n v estig ated co n  d e n tiality issu es o f a broad category of association rules and proposed some algorithms to preserve privacy of such rules above a given privacy threshold Although these algorithms ensure privacy preservation they are CPU-intensive since they require multiple scans over a transactional database In addition such algorithms in some way modi“es true data values and relationships by turning some items from 0 to 1 in some transactions In the same direction Saygin et al 9 i nt roduced a method for selectively removing individual values from a database to prevent the discovery of a set of rules while preserving the data for other applications They proposed some algorithms to obscure a given set of sensitive rules by replacing known values with unknowns while minimizing the side effects on non-sensitive rules These algorithms also require various scans to sanitize a database depending on the number of association rules to be hidden Oliveira and Za ane 8 i nt roduced a uni  e d frame w o rk that combines techniques for ef“ciently hiding restrictive patterns a transaction retrieval engine relying on an inverted le and Boolean queries and a set of algorithms to sanitize a database In this framework the sanitizing algorithms require two scans regardless of the database size and the number of restrictive patterns that must be protected The work presented here differs from the related work in some aspects as follows First we extended our previous work presented in 8 b y addi ng t w o n e w al gori t h ms Round Robin and Random to the set of sanitizing algorithms Second the hiding strategies behind our algorithms deal with the problem 1 and 2 in Figure 3 and most importantly they do not introduce the problem 3 since we do not add noise to the original data Third we study the impact of our hiding strategies in the original database by quantifying how much information is preserved after sanitizing a database So our focus is not only on hiding restrictive association rules but also on maximizing the discovery of rules after sanitizing a database Another difference of our algorithms from the related work is that our algorithms require only two scans over the original database while the algorithms presented in 4 9 requi re v a ri ous s cans depending on the number of association rules to be hidden This is due the fact that our sanitizing algorithms are built on indexes and consequently they achieve a reasonable performance 7 Conclusions In this paper we have introduced two algorithms for balancing privacy and knowledge discovery in association rule mining Our sanitizing algorithms require only two scans regardless of the database size and the number of restrictive association rules that must be protected This rst scan is required to build the index inverted le for speeding up the sanitization process while the second scan is used to sanitize the original database This represents a signi“cant improvement over the previous algorithms presented in the literature 4 9 Our algorithms are integrated to the framework presented in 8 which combines three adv ances for ef ciently hiding restrictive rules inverted les one for indexing the transactions per item and a second for indexing the sensitive transactions per restrictive association rule a transaction retrieval engine relying on Boolean queries for retrieving transaction IDs from the inverted le and combining the resulted lists and a set of sanitizing algorithms The experimental results revealed that our algorithms for sanitizing a transactional database can achieve reasonable Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 140 20 40 60 80 100 DB Size \(transactions in thousands CPU Time \(sec IGA RRA RA Al g o2a 0 10 20 30 40 50 60 70 246810 Set of Restrictive Rules CPU Time \(sec IGA RRA RA Al g o2a AB Figure 8 Results of CPU time for the sanitization process results when compared with the other approaches in the literature Such algorithms slightly alter the data while enabling exibility for someone to tune them In particular the IGA algorithm reached the best performance in terms of dissimilarity and in terms of preservation of legitimate association rules On the other hand the results suggested that RA is slightly better than the other algorithms for hiding failure Although our algorithms guarantee privacy and do not introduce false drops to the data an extra cost is payed because some rules would be removed accidentally since there are functional dependencies between restricted and non-restricted rules The rationale behind this is that privacy preserving association rule mining deals with a tradeoff privacy and accuracy which are contradictory i.e improving one usually incurs a cost for the other It is important to note that our sanitization methods are robust in the sense that there is no de-sanitization possible The alterations to the original database are not saved anywhere since the owner of the database still keeps an original copy of the database intact while distributing the sanitized database Moreover there is no encryption involved There is no possible way to reproduce the original database from the sanitized one Currently we are investigating new optimal sanitization algorithms that minimize the impact in the sanitized database while facilitating proper information accuracy and mining In addition we are working on the optimization of the algorithms RRA and RA specially in terms of preservation of legitimate association rules since their results revealed they are promising 8 Acknowledgments Stanley Oliveira was partially supported by CNPq Conselho Nacional de Desenvolvimento Cient co e Tecnol ogico of Ministry for Science and Technology of Brazil under Grant No 200077/00-7 Osmar Za ane was partially supported by a Research Grant from NSERC Canada We would like to thank Y ucel Saygin and Elena Dasseni for providing us the code of their respective algorithms for our comparison study References 1 M  A tallah  E  Bertin o  A Elmag armid  M  I b r ah im an d V Verykios Disclosure Limitation of Sensitive Rules In Proc of IEEE Knowledge and Data Engineering Workshop  pages 45…52 Chicago Illinois November 1999  C  C l i f t on Usi ng S ampl e S i z e t o L i m i t E xposure t o Dat a Mi ning Journal of Computer Security  8\(4\:281…307 November 2000 3 C  C lifto n a n d D M a rk s Secu rity an d P ri v a c y Imp licatio n s o f Data Mining In Workshop on Data Mining and Knowledge Discovery  pages 15…19 Montreal Canada February 1996 4 E D a s s e n i  V S V e r y k i o s  A K E l m a g a r m i d  a n dE B e r t i n o  Hiding Association Rules by Using Con“dence and Support In Proc of the 4th Information Hiding Workshop  pages 369 383 Pittsburg PA April 2001 5 M  D ietzfelb in g e r  A R Karlin  K  M eh lh o r n  F  M  au f d er Heide H Rohnert and R E Tarjan Dynamic Perfect Hashing Upper and Lower Bounds SIAM Journal on Computing  23\(4\:738…761 1994  D E  OL eary  Kno wledge Disco v e ry as a T hreat to Database Security In G Piatetsky-Shapiro and W J Frawley editors Knowledge Discovery in Databases AAAI/MIT Press pages 507-516 Menlo Park CA 1991 7 S  R M O l i v e i r a a n d O  R  Z a  ane A Framework for Enforcing Privacy in Mining Frequent Patterns Technical report TR02-13 Computer Science Department University of Alberta Canada June 2002 8 S  R M O l i v e i r a a n d O  R  Z a  ane Privacy Preserving Frequent Itemset Mining In Proc of the IEEE ICDM Workshop on Privacy Security and Data Mining  pages 43…54 Maebashi City Japan December 2002 9 Y  S aygi n V  S  V e r yki os and C  C l i f t on U s i n g U nkno w n s to Prevent Discovery of Association Rules SIGMOD Record  30\(4\:45…54 December 2001 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEAS03 1098-8068/03 $17.00 © 2003 IEEE 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


