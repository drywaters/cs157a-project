1997 IEEE International Conference on Intelligent Processing Systems October 28  31 Bei~tng China Efficient Parallel Mining of Association Rules on Shared-Memory Multiple-Processor Machine Kan Hut David W Cheungl Shaowei Xiat t Department of Automation, Tsinghua University, Beijing 100084 China Email hukan@cs.hku.hk swxia8mail.tsinghua.edu.cn Department of Computer Science The University of Hong Kong, Hong Kong Email dcheung@cs.hku.hk China Abstract  In this paper we consider the problem of parallel mining of association rules on a shared memory multiprocessor system Two efficient 
algo rithms PSM and HSM have been proposed PSM adopted two powerful candidate set pruning techniques distributed pruning and global pruning to reduce the size of candidates HSM further utilized an 1/0 reduc tion strategy to enhance its performance We have implemented PSM and HSM on a SGI Power Chal lenge parallel machine The performance studies show that PSM and HSM out perform CD-SM which is a shared-memory parallel version of the popular Apriori algorithm I INTRODUCTION Mining associiition rules in large databases has attracted a lot of attention in data mining 
research l 2 4 81 The mining process needs to scan all the transactions in the database which introduces a significant amount of I/Os In addition it has to search through a large number of candidates for large itemsets which demands a lot of CPU computation Therefore the development of parallel algorithms for mining association rules is an im portant problem In this work we attempt to solve this problem on shared-memory multiple-processor machines such as the SGI Power Challenge Most proposals in parallel mining have been focused on distributed 
or shared-nothing model 5 6 7 9 lo In those models the database is partitioned and dis tributed in the local disk of the processors The memory limitation and I/O cost are the dominating performance factors The shared-memory multiprocessor parallel ma chine is another important computing model But very few parallel mining works have been carried out on this model A direct extension of the Apriori algorithm to the shared-memory model has been presented in ll Some important characteristics of the candidate sets in a parti tioned database 
have been discovered 5 They have been used to design two effective two pruning techniques the distributed pruning and global pruning which can reduce the amount of candidates effectively in the distributed or 0-7803-4253-4/97/$10.00 0 1997 IEEE parallel environment One of the first parallel algorithm in shared-nothing model is the CD Count Distribution algorithm 3 For comparison purpose we have realized the CD algorithm on a SGI Power Challenge shared-memory multiprocessor machine This version of CD is called CD-SM CD on Shared-memory Model In this work we 
propose two efficient algorithms on the shared-memory multi-processor machine The first algo rithm is PSM Parallel mining on Shared-memory Model which adopts the two pruning techniques to reduce the number of candidates in each iteration PSM remedies the problem of large number of candidate sets in the CD SM algorithm So PSM consumes less computing time than the CD-SM However PSM still needs to perform the same number of scannings on the database In gen eral the number of candidates after iteration two would reduce drastically Therefore we have reduced 
the num ber of scannings by combining the computations of all the iterations after iteration two This enhanced version is the HSM Hybrid parallel mining on Shared-memory Model algorithm Therefore HSM can reduce the 1/0 cost at most situations i.e when need to mine more than two iterations in CD-SM and has less CPU cost compared with CD-SM We have implemented the above algorithms on a SGI Power Challenge shared-memory muti-processor machine with 8 processors All algorithms base on the framework of common candidate partitioned database One single candidate hash tree or Trie is used by all 
the processors while the database is partitioned among them Each pro cessor traverses its local database and stores the support for itemsets separately on the shared hash tree Finally a master process computes the large itemsets according to the given threshold Extensive performance studies have been carried out It was observed that both PSM and HSM performed faster than CD-SM In particular HSM enjoys a very good reponse time due to its 1/0 reduction The performance studies also showed that PSM and HSM have better speed up property than CD-SM  1133  


The rest of this paper is organized as follows Sec tion 2 overviews the parallel mining of association rules Two candidate pruning techniques distributed pruning and global pruning are described in Section 3 In Section 4 we present the PSM and HSM algorithms Section 5 reports the result of the performance study Finally we conclude in Section 6 11 PA4RALLEL MINING OF ASSOCIATION RULES A Assotzataon Rules Let I  il i2   im be a set of items and D be a database of transactions where each transaction T con sists of a set of items such that T GI An associatzon rule is an implication of the form X j Y where X 5 I Y c I and X n Y  4 An association rule X  Y has support s in D if the probability of a transaction in D contains both X and Y is s The association rule X  Y holds in D with confidence c if the probability of a transaction in D which contains X also contains Y is c The task of mining association rules is to find all the association rules whose support is larger than a given minimum sup port threshold and whose confidence is larger than a given minimum confidence threshold For an itemset X we use X sup to denote its support count in database D which is the number of transactions in D containing X An item set X s I is large or frequent if X sup 2 minsup x lDJ where minsup is the given minimum support threshold For the purpose of presentation we sometimes just use support to stand for support count of an itemset It has been shown that the problem of mining associ ation rules can be decomposed into two subproblems l  1 find all large itemsets for a given minimum support threshold, and 2 generate the association rules from the large itemsets found Since 1 dominates the overall cost the current research has been focused on how to efficiently solve the first subproblem itemsets Lk are computed independently by each proces sor CD repeats steps 1  4 until no more candidate is found 1 Ck  apriori-gen\(Lk-1 2 scan partition D to find the local support count XsuP for all S E Ck 3 exchange X sup\(i 1 X E Ck with all other processors to get global support counts X sup for all X E Ck 4 Lk  X E Cn I Xsup 2 minsup x IDl Fig 1 Count Distribution Algorithm 111 CANDIDATE PRUNING TECHNIQUES Suppose the entire database D is partitioned into D1 D2  D and distributed over n processors Let X be an itemset and Xsup be the support of X in D We call X sup the global support of X Also we use'X Sup\(r to denote the local support of X at processor i which is the support of X in D X is globally large if X 2 minsup x ID Similarly X is locally large at processor i if X sup\(t 2 rninsup x ID We also call X gl-large at processor 2 if X is globally large and locally large at processor i For convenience we use the term k-itemset to stand for size-k itemset and use Lk GLb to denote the set of all globally large k-itemsets and the set of all gl-large k-itemsets n t processor i respectively CD only applies tunction aprzorz-gen on the set Lk-1 to generate the candidate sets Ck in the Ic-th iteration In fact, after the support counts exchange in the k  1 iteration each processor can find out not only the large itemsets Lk-1 in CkPl but also the processors at which an itemset X is gl-large for any X E LkPl By using this information, many candidates in Ck can be identified to be small and hence pruned away before the next scan of the database B Count Distribution Algorithm for Parallel Mining A Distributed pmning Aprion is the most well known serial algorithm for min ing association rules 2 It relies on the cspriori-gen func tion to generate the candidate sets at each iteration CD Count Distribution is a parallel version of Apriori for parallel mining basing on shared-nothing multiprocessor 3 The database D is partitioned into D1 D2   D and distributed across n processors The program frag ment of CD at processor i 1 5 i 5 n for the Ic-th it eration is outlined in Fig 1 In step 1 every proces sor computes the same candidate set Ck by applying the aprior-gen function on Lk-1 which is the set of large itemsets found at the k  1 iteration In step 2 local support counts of candidates in Ck are found In steps 3  4 local support counts are exchanged with all other processors to get global support counts and globally large  The distributed pruning technique is derived from the ob servation that all subsets of any large itemsets must be gl-large simultaneously on at least one processor For ex ample, suppose the database is partitioned into D1 and Dz on processors 1 and 2 Further assume that both A and B are two size-1 globally large itemsets In addition A is gl-large at processor 1 but not processor 2 and B is gl-large at processor 2 but not processor 1 It can be shown that AB E C2 can never be globally large If AB is globally large it must be globally and locally large gl large at some processor Assume it is gl-large at proces sor 1 then B must also be gl-large at processor 1 which is contradictory to the assumption Similarly AB cannot be gl-large at processor 2 Hence AB cannot be globally large at all In other words if AB was globally large  1134  


then A and B must be gl-large at the same time on pro cessor 1 or processor 2 or both of them This observation can be genera1i.zed to the k-th iteration Therefore the candidates can be generated by applying function apri ori-gen on each GLk-l\(i 1 5 i 5 n independently The set of size-k candidates generated with this technique is equal to CGg  UY=lCGg\(i where CGg\(i  apri ori..gen\(GLk-l\(i Note that the function apriori-gen is the same as that in the Apriori algorithm but it is ap plied on subsets of Lk-1 rather than the whole Lk-1 Due to the combinatorial effect the size of Cg  apri ori-gen\(lk-1 could be much larger than that of CGg The above observation can be summarized by the follow ing theorem proved in 5 Theorem 1 For k  1 the set of all globally large k iternsets Lk is a subset of CGk  UY=lCGk\(i where CGk  apriori-gen\(Gll,-l Based on Theorem 1 we can prune away any size-k candidate such that there does not exist any processor at which all its size 1 subsets are gl-large This pruning technique is called distributed pruning B Global Pruning In the counting process each processor keeps its local support counts for etery candidates The local support counts are exchanged or shared after each iteration As a result the local support counts X Fnp\(t for all processor i 1 5 i 5 n are also available at every other pro cessor With this information another powerful pruning technique called global pruning can be developed Let X be a candidate k-itemset 4t each processor i X sup\(r 5 Ysup\(z where Y c X Therefore X s.up\(l is bounded by thci value rnin{YSTLp I Y c X and IYl  k  1 Hence the value n x mazsup  mazsup\(t t 1 where X rnazsup\(z  min{YsUp I Y C X IYI  k  1 is an upper bound of the global support of X If X  minsup x ID then X can be pruned away This technique is called global prunzng Note that global pruning requires no additional information except the local support counts resulted from count exchange or sharing in the previous iteration We can apply global pruning to the survivals of candidates after going through the distributed pruning to get the smaller candidate itemsets That is if the upper bound of an itemset X is found to be smaller than the support threshold X cannot be globally large and should be removed from the set of candidate sets IV PSM AND HSM ALGORITHMS A Parallel Maning on Shared-Memory Model Algorithm PSM The PSM is an enhancement of CD-SM The main differ ence between them is that both the distributed pruning and global pruning are incorporated in the PSM algorithm to reduce the candidate set size The first iteration of PSM is the same as CD-SM Each processor scans its partition to find out local support counts of all size-1 itemsets and the master process is in charge of computing the global support counts At the end in addition to L1 each processor also find out the gl-large itemsets GLl for 1 5 i 5 n For the k-th iteration of FPM k  1 the program fragment at processor i 1 5 i 5 R is described in Fig 2 1 compute candidate sets CGL  Aprioii_gen\(GLr_l 2 prune candidates in CGk by global pruning 3 build CGq into the common hash tree HI 4 scan partition D to find the local support 5 compute GLgc  X E HT\(k I Xsup 2 minsup x 1D1 6 return Lk  Ur=lGLk\(z distributed pruning count S sup\(r for any X E HTp  X sup\(r 2 7nZ7LSUp x 1D,1 for all i 1 5 i 5 16 Fig 2 The PSM Algorithm The PSM algorithm is designed on the model of com mon candidate hash tree and partitioned database Every processor can visit the shared hash tree for looking up the candidates while the database split among them Local counter airay with length of ICkl is kept by every pro cessor to record the local support counts At the end of each iteration these local counter arrays are shared by all processors The data structure of the common hash is exactly same as used in Apriori 121 B Hybrid Parallel Mining on Shared-Memory Model Algorithm\(HSM Although PSM has much less candidates than CD-SM they have the same 1/0 cost i.e they scan database with the same number of passes Under the shared-memory computing model most machines only have serial I/o ability up to now However we have to face the hugh volume database in the data mining task Thus the cost of 1/0 becomes the bottleneck The HSM algorithm is designed to reduce the 1/0 cost HSM retains the pruning techniques in PSM In details the first and second iterations of HSM is the same as PSM Then we can get the results of L1 Lz Obviously C3 can be generated by using Apriori-gen and pruning techniques on Lz The subsequent steps are different from that in PSM We continue to generate C4 from C3 by  1135  


using Apriori-gen only and from C to C5 and so on until no candidate is generated A Trie as described in 4 is used to store the support counts after each processor performs one scan on its partition Therefore with only one pass we can compute all large itemsets of size larger than two Since HSM only scan database at most three passes it incures much less 1/0 comparing with CD-SM The other more flexible strategy in HSM is to assign a threshold for the Ck When the size of Ck is less than the threshold the algorithm then switchs from the hash tree approach to the Trie approach 1 08 08 04 02 0 V PERFORMANCE EVALUATION All the experiments were performed on a 8-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS RlOOOO processor There is a total of 512MB of main memory All processors run IRIX 6.2          ____ A Synthetic Databases Generation We use the synthetic test data generator introduced in 2 The database partition of each node is about 33MB in size and the number of partitions is 8 i.e n  8 The number of items N  1000 and the number of maxi mal potentially large itemsets ILI  1000. Table 1 shows the databases used and their properties In it D is the number of transactions in each partitions T is the av erage size of the transactions and I is the average size of the itemsets The minimum support threshold is 1 while 2 at the last two cases We ran all CD-SM PSM and HSM on the 5 databases Experiments were repeated multiple times to obtain stable values Table 1 DATABASE PROPERTIES Name I Dj TI1 Dl000K.T5.12 1 lOOOK 1 5 1 2 D700K.Tl0.12 I 700K 1 10 12 D700K.Tl0.14 1 k00K I 10 I 4  I  1 D40OK.T20.14 I 400K I 20 1 4 D400K.T20.16 I 400K I 20 I 6 B Relative Performance Fig 3 shows the response times for the three parallel al gorithms on the five databases Both HSM and PSM are faster than CD-SM in all cases It seems that the response time of the HSM is near a constant value But this is a mere coincidence due to the adjustment on the experi ment parameters including transaction number transac Fig 4 shows the pruning effects It is the ratio of the number of candidate sets with pruning over that gener ated by Apriori-gen only There are much less candidate  tion average size and the minimum support threshold Relalw Performans 8 6000 CD-SM c PSM  HSM 0 000 a ___.__._____ ___.....___ m _._..,.._ Q1 0 I D1000K.T5.IZ D700K.T10.12 D70OK.T10.14 D400K.TZ0.14 D4OOK.T20.IB Detebsses Fig 3 Relative Performance itemsets generated when distributed pruning and global pruning techniques are used It is obviously that the pruning effect is related to the data distribution among the database partitions The expected results is that the more data skewness among the partitions the better the pruning effect For an extreme example let 2 database partitions for 2 processors if itemsets AB AC BC are all gl-large at both 2 processors then neither distributed pruning nor global pruning can prune the itemset ABC out Extensive studies on the skewness as a parameter on the pruning effect has been performed and will be re ported in the future C Parallel Performance We have investigated the performance speedup on a fixed size database with increasing number of processors and partitions The database D700K.Tl0.14 was chosen as the dataset withthe minimum support threshold 1.0%. Fig 5 presents the relative speedup The result is very encour aging Both HSM and PSM performed better speedup than CD-SM Especially HSM has achieved a superlin ear speedup The reason is that the pruning effect is augmented when the number of partitions is increased Although there was the same pruning effect in PSM al gorithm it didn't present a superlinear property because there is no optimization on 1/0 reduction Another phenomenon in Fig 5 is that speedup of the three algorithms are not linear The reason is that the  1136  


Fig 5 Speepup 1/0 mechanism of SGI Power challenge is not parallel The 1/0 contention among processors increase when the number of processor increases and hence has a negative impact on the performance VI CONCLUSIONS In this paper we proposed two parallel algorithms PSM and HSM for mining association rules on the SGI Power Challenge shared-memory multi-processor PSM is in corporated with two candidate pruning techniques dis tributed pruning and global pruning HSM further en hances PSM by utilizing an 1/0 reduction strategy The experiments showed that both algorithms performed bet ter than CD-SM In the future work we are interested in using dynamic candidates generation approach to further reduce the 1/0 cost and adopt an asynchronous mecha nism on shared-momory parallel system to speedup the response time 3 R Agrawal and J.C Shafer Parallel mining of as sociation rules Design implementation and experi ence. Special Issue in Data Mining IEEE Trans. on Knowledge and Data Engineering IEEE Computer Society V8 N6 December 1996 pp 962-969 4 S Brin R Motwani J Ullman S Tsur Dynamic itemsets counting and implication rules for market basket data In Proc of 1997 ACM-SIGMOD Int Conf. On Management of Data 1997 5 D W Cheung J Han V T Ng A W Fu Y Fu A fast distributed algorithm for mining association rules In Proc of 4th Int Conf on Parallel and Dis tributed Information Systems Miami Beach Florida December 1996 pp 31-43 6 D W Cheung V T Ng A W Fu and Y Fu Ef ficient Mining of Association Rules in Distributed Databases Special Issue in Data Mining IEEE Trans. on Knowledge and Data Engzneering IEEE Computer Society V8 N6 December 1996 pp 911 922 7 E Han G Karypis and V Kumar Scalable parallel data mining for association rules In Proc of 1997 ACM-SIGMOD Int Conf On Management of Data 1997 8 J S Park M  S Chen and P S Yu An effective hash-based algorithm for mining association rules In Proc of 1995 ACM-SIGMOD Int Conf on Manage ment of Data San Jose CA May 1995 pp 175-186 9 J S Park M S Chen and P S Yu, Efficient parallel mining for association rules. In Proc of the 4th Int Conf. on Information and Knowledge Management Baltimore Maryland 1995 pp 31-36 References R Agrawal T Imielinski and A Swami Min ing association rules between sets of items in large databases In Proc of 1993 ACM-SIGMOD Int Conf On Management of Data Washington D.C 1993 pp 207-216 Systems 1996 R Agrawal and R Srikant. Fast algorithms for min ing association rules In Proc of the 20th VLDB Conference Santiago Chile 1994 pp 487-499 lo T Shintani M Kitsuregawa Hash based parallel al gorithms for mining association rules In Proc of 4th Int Conf on Parallel and Distributed Information I11 M J Zaki M Ogihara S Parthasarathy and W Li Parallel data mining for association rules on shared memory multi-processors Supercomputing 96 Pitts burg PA Nov 17-22 1996  1137  


limited Hence w e use a pre\014x-tree T to store all candidates in C all and then coun t supp orts for them After that L can b e obtained immediately  If maxcliqsize  maxcansize w e call GEN CPI\(X to do CPI generation call INTRA MINER to obtain L intr a  call INTER MINER to generate L inter  and 014nally w eha v e L  L intr a  L inter  The correctness of the algorithm is straigh tforw ard 4 P erformance Study W eha v e implemen ted our algorithms MINER\(I and MINER\(II for ev aluating their p erfermances In b oth implemen tations w e set maxcansize  10 F or comparison w e ha v e also implemen ted t w o Apriori v ersions One v ersion denoted b y ApriHash uses hashtr e e structure suggested b y to store candidate sets The other v ersion denoted b y ApriPr ef uses our pr e\014xtr e e structure to store candidate sets T o assess the p erformances of MINER\(I  MINER\(II  ApriHash and ApriPr ef w e p erformed sev eral exp erimen ts on a SUN UL TRA-1 w orkstation with 64 MB main memory running Sun OS 5.5 T ok eep the comparison fair w e implemen ted all the algorithms using the same basic data structures except that ApriHash used a differen t hash-tr e e structure to store candidate sets In addition for a fair comparison all our test results do not con tain the execution time for generating L 1 and L 2  b ecause Apriori uses a m uc h slo w er metho d to get L 1 and L 2 than our arra y tec hnique The syn thetic datasets used in our exp erimen ts w ere generated b y a to ol describ ed in W e use the same notation T x:I y D z to denote a dataset in whic h x is the a v erage transaction size y is the a v erage size of a maximal p oten tially frequen t itemset and z is the n um b er of transactions In addition w e generated all datasets b y setting N  1000 and j L j  2000 where N is the n um ber of all items j L j is the n um ber of maximal p oten tially frequen t itemsets Please refer to  for more details on the dataset generation Figure 2 sho ws the exp erimen tal results for four syn thetic datasets to compare the p erformances of these algorithms W e observ e that b oth our algorithms MINER\(I and MINER\(II outp erform b oth ApriHash and ApriPr ef  b y factors ranging from 2 to 4 in most cases Ho w ev er the di\013erence bet w een MINER\(I and MINER\(II is minor In theory  there are three reasons for that our methods outp erform the Apriori approac h First our metho ds only need at most 2 scans of databases for obtaining all frequen t itemsets with size greater than 2 while Apriori requires uncertainly m ultiple scans Hence the I/O cost in our metho ds is usually m uc h smaller than that in Apriori  Second w e design effectiv e metho ds to mak e the size of our candidate set reasonable so that the computation cost is also w ellcon trolled Third using pre\014x-tree structure to store candidates also has t w o adv an tages 1 its space requiremen tisv ery lo w 2 the supp ort-incremen t operations can b e made at eac h visited no de to reduce the computation cost while these op erations can only b e made at leaf no des when hash-tr e e is used T o sum up with these adv an tages our algorithms are more e\016cien t than Apriori  5 P arallelization Since mining frequen t itemsets requires lots of computation p o w er memory and disk I/O it is not only in teresting but also necessary to dev elop parallel algorithms for this data mining task Here w e study ho w to extend our algorithm MINER\(X for parallelization where X 2 f I II g  W e assume a sharednothing arc hitecture where eac hof n pro cessors has a priv ate memory and a priv ate disk The pro cessors are connected b y a comm unication net w ork and can comm unicate only b y passing messages The main idea of our parallelization is as follo ws First the transaction database D is ev enly distributed on the disks attac hed to the pro cessors W e use D i to denote the set of transactions at pro cessor P i  for all i 2 f 1 030 n g  Let c be an itemset The lo c al supp ort c ount of c at pro cessor P i refers to the n um ber of transactions con taining c in D i  W e generate all required candidates and coun t their lo cal supp orts at eac h pro cessor All the obtained lo cal supp ort coun ts will b e sen t to all other pro cessors Th us when a processor completes its lo cal supp ort-coun ting and also receiv es all other lo cal supp ort coun ts from other processors the global supp ort coun ts for all candidates can b e accum ulated and all frequen t itemsets can b e obtained at this pro cessor No w w e in tro duce some concepts to help our discussion Let T 1 and T 2 be t w o pre\014x-trees W e sa y that 1 T 1 is a subtr e e in shap e of T 2  denoted b y T 1 v T 2  if for eac h t 1 2 T 1 there exists t 2 2 T 2 suc h that t 1 item  t 2 item and the p osition or o ccurrence of t 1 in T 1 is the same as that of t 2 in T 2  2 T 1 and T 2 are in the same shap e if T 1 v T 2 and T 2 v T 1  W e in tro duce the follo wing pro cedure to do supp ort accum ulation for T 1 and T 2 in the same shap e and sa v e the result in T 1  Algorithm AccumSup T 1  T 2  for eac h t 1 2 T 1 do Let t 2 2 T 2 suc h that the p ositions of t 1 in T 1 and t 2 in T 2 are the same t 1 sup  t 1 sup  t 2 sup  Before giving our parallel solution w e 014rst parallelize INTRA MINER and INTER MINER as follo ws where w e assume that all required input data can b e a v ailable Algorithm P AR INTRA MINER Ap ar al lelization of INTRA MINER for i 1 to n at pro cessor P i do in parallel C intr a  f c j c 022 cl iq  cl iq 2 Q  c is in tra-CPI and j c j\025 3 g  Let T i b e an empt y pre\014x-tree and for eac h c 2 C intr a do instree T i  c  Coun tSup T i  D i  and send T i to all other pro cessors Receiv e T j s from all other pro cessors P j s  Synchr onization p oint for j 2 to n do AccumSup T 1  T j   T 1 030 T n ar e in the same shap e Collect all frequen tin tra-CPI k itemsets for k 025 3to L intr a b y a scan of T 1  L intr a  L intr a  L 1 f x 2 L 2 j x is in tra-CPI g  


          0 2 4 6 8 10 12 14 16 18 Minimum Support \(in T5.I2.D100K.data ApriHash ApriPref MINER\(I MINER\(II Time\(sec 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05          0 20 40 60 80 100 120 140 Minimum Support \(in T10.I4.D100K.data 0.25 0.15 0.45 0.40 0.35 0.20 0.30 0.60 0.50 0.55 Time\(sec ApriHash ApriPref MINER\(I MINER\(II          0 50 100 150 200 250 Minimum Support \(in T10.I6.D100K.data ApriHash ApriPref MINER\(I MINER\(II Time\(sec 0.60 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.55          0 50 100 150 200 250 300 350 400 450 500 Minimum Support \(in T20.I6.D100K.data ApriHash ApriPref MINER\(I MINER\(II 0.95 1.00 0.90 0.85 0.75 0.65 0.55 0.60 0.70 0.80 Time\(sec Figure 2 P erformance comparison Algorithm P AR INTER MINER  A p ar al lelization of INTRA MINER for i 1 to n at pro cessor P i do in parallel C inter  f c j c 022 cl iq  cl iq 2 Q  j c j\025 3 c is in ter-CPI all in tra-CPI subsets of c are in L intr a g  Let T i b e an empt y pre\014x-tree and for eac h cl iq 2 Q do instree T  c  Coun tSup T i  D i  and send T i to all other pro cessors Receiv e T j s from all other pro cessors P j s  Synchr onization p oint for j 2 to n do AccumSup T 1  T j   T 1 030 T n ar e in the same shap e Collect all frequen tin ter-CPI k itemsets for k 025 3to L inter b y a scan of T 1  L inter  L inter f x 2 L 2 j x is in ter-CPI g  Both algorithms ab o v e emplo y the same idea First eac h pro cessor P i for all i 2 f 1 030 n g indep enden tly inserts all in tra-CPI or in ter-CPI candidates in to a local pre\014x-tree T i  Next P i uses lo cal transaction data D i to obtain the lo cal supp orts for all candidates and sends the result tree T i to all other pro cessors Then it w aits for receiving all T j s from all other pro cessors P j s After all T 1 030 T n are a v ailable P i do es supp ort accum ulation for all candidates and store the global supp orts in to T 1  Finally  L intr a or L inter can b e generated easily at eac h P i  It is not hard to see that the ab o v e pro cedure is complete Th us w e can presen t the parallelization of MINER\(X as follo ws Algorithm P AR MINER\(X  X 2f I II g for i 1 to n at pro cessor P i do in parallel Coun t lo cal supp orts for all 1itemsets and 2-itemsets send the results to all other pro cessors receiv e lo cal supp orts from all other pro cessors accum ulate all lo cal supp orts to get global supp orts and then generate L 1 and L 2  GEN CLQ  Get Q  S maxcliqsize i 3 Q i if maxcliqsize 024 maxcansize then for i 1 to n at pro cessor P i do in parallel C all  f c j c 022 cl iq 2 Q j c j\025 3 g  Let T i b e an empt y pre\014x-tree and for eac h c 2 C all do instree T i  c  Coun tSup T i  D i  and send T i to all other pro cessors Receiv e T j s from all other pro cessors P j s  Synchr onization p oint for j 2 to n do AccumSup T 1  T j   T 1 030 T n ar e in the same shap e Collect all frequen t k itemsets for k 025 3in to L b y a scan of T i  L  L  L 1  L 2  else for i 1 to n at pro cessor P i do in parallel GEN CPI\(X  Get a CPI P AR INTRA MINER  L intr a gener ation at P 1 030 P n P AR INTER MINER  L inter gener ation at P i 030 P n for i 1 to n at pro cessor P i do in parallel L  L intr a  L inter  


It is easy to see that P AR MINER\(X is a natural parallelization of MINER\(X on a shared-nothing arc hitecture b y using our main idea men tioned b efore where X ma y be I or II This parallelization uses a simple principle of allo wing redundan t computations in parallel on otherwise idle pro cessors to a v oid comm unication whic h is also emplo y ed b y Count Distribution algorithm 3 Note that Count Distribution is the fastest parallelization of Apriori  Ho w ev er Count Distribution con tains uncertainly m ultiple sync hronization p oin ts due to the sum-reduction at the end of eac h pass to construct the global coun ts This fact greatly limits the dev elopmen t of parallelism Our P AR MINER\(X has o v ercome this dra wbac k b ecause it con tains at most 3 sync hronization p oin ts F urthermore di\013eren t from the metho ds in 11  all of our database partitions are non-o v erlapping whic h guaran tees there is no redundan t supp ort-coun ting op eration in our metho d Hence our metho d dev elops the parallelism more su\016cien tly than b oth the ab o v e existing metho ds 6 Conclusions W eha v e prop osed new algorithms for e\016cien t mining of asso ciation rules Di\013eren t from all existing algorithms w ein tro duce a concept of CPI Complete P artition of Items and divide all itemsets in to t w ot yp es in tra-CPI and in ter-CPI After obtaining all frequen t 1-itemsets and 2-itemsets w e generate and main tain a set Q of item cliques maximal p oten tially frequen t itemsets F urthermore w eha v e designed t w o metho ds for generating an e\013ectiv e CPI b y using Q  Then w e can use Q and our CPI to get a set C intr a of in traCPI candidates and coun t supp orts for them so that all frequen t in tra-CPI itemsets can be obtained Finally  w e use Q  our CPI and all frequen t in tra-CPI itemsets to generate a set C inter of in ter-CPI candidates and also coun t supp orts for them so that all frequen tin ter-CPI itemsets can b e obtained Our algorithms ha v e sev eral adv an tages First their I/O costs are quite limited b ecause they only require at most 3 scans o v er database Second they can mak e b oth sizes of C intr a and C inter reasonable so that their computation costs are also e\013ectiv ely con trolled Third they use a pre\014x-tree structure to store candidates whic h can also reduce computation cost As a result they app ear to be more e\016cien t than Apriori  one of the b est algorithms for asso ciation disco v ery  T o con\014rm that W e ha v e done the exp erimen ts to compare the p erformances of our algorithms together with Apriori  The test results sho w that our algorithms outp erform Apriori consisten tly  b y factors ranging from 2 to 4 in most cases Another adv an tage of our algorithms is that they are easy to be parallelized W e ha v e also presen ted a p ossible parallelization for our algorithms based on a shared-nothing arc hitecture W e observ e that the parallelism can b e dev elop ed more su\016cien tly in our parallelization than t w o of the b est existing parallel algorithms References 1 R Agra w al H Mannila R Srik an t H T oiv onen and A I V erk amo F ast Disco v ery of Association Rules A dvanc es in Know le dge Disc overy and Data Mining  Chapter 12 AAAI/MIT Press 1996 2 R Agra w al T Imielinski A Sw ami Mining Asso ciations bet w een Sets of Items in Massiv e Databases Pr o c of the A CM SIGMOD Int'l Confer enc e on Management of Data W ashington D.C Ma y 1993 207-216 3 R Agra w al J.C Shafer P arallel Mining of Association Rules IEEE T r ansactions on Know le dge and Data Engine ering  V ol 8 No 6 Decem ber 1996 4 D.W Cheung and Y Xiao E\013ect of Data Sk ewness in P arallel Mining of Asso ciation Rules Pr o c The Se c ond Paci\014c-Asia Conferenc e on Know le dge Disc overy and Data Mining P AKDD-98  Melb ourne Australia April 1998 48-60 5 Dao-I Lin and Zvi M Kedem Pincer-Searc h A New Algorithm for Disco v ering the Maxim um F requen t Set EDBT'98  Marc h 1998 6 Heikki Mannila Hann uT oiv onen and A Ink eri V erk amo E\016cien t algorithms for disco v ering asso ciation rules AAAI Workshop on Know le dge Disc overy in Datab ases KDD-94  pages 181 192 July 1994 7 G D Mulligan and D G Corneil Corrections to Bierstone's Algorithm for Generating Cliques J Asso ciation of Computing Machinery  19\(2 247 Apr 1972 8 Jong So o P ark Ming-Sy an Chen and Philip S Y u An E\013ectiv e Hash-Based Algorithm for Mining Asso ciation Rules Pr o c 1995 A CMSIGMOD Int Conf Management of Data  San Jose CA Ma y 1995 9 Jong So o P ark Ming-Sy an Chen and Philip S Y u E\016cien tP arallel Data Mining for Asso ciation Rules A CM CIKM 95  Baltimore MD USA 10 Ashok Sa v asere Edw ard Omiecinski Shamk an t Na v athe An E\016cien t Algorithm for Mining Asso ciation Rules in Large Databases Pr o c e e dings of the 21st VLDB Confer enc e  Zuric h Switzerland 1995 11 Mohammed Ja v eed Zaki Sriniv asan P arthasarath y  Mitsunori Ogihara W ei Li New P arallel Algorithms for F ast Disco v ery of Asso ciation Rules Data Mining and Know le dge Disc overy Sp e cial Issue on Sc alable High-Performanc e Computing for KDD  pp 343373 V ol 1 No 4 Decem b er 1997 12 Mohammed Ja v eed Zaki Sriniv asan P arthasarath y  Mitsunori Ogihara W ei Li New Algorithms for F ast Disco v ery of Asso ciation Rules T e chnic al R ep ort UR CS TR 651  Univ ersit yof Roc hester 1997 


                 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


