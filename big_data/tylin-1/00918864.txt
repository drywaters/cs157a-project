Content-based Retrieval and Data Mining of a Skin Cancer Image Database Soon M Chung and Qing Wang Department of Computer Science and Engineering Wright State University Dayton Ohio 45435 USA schung a Wright edu Abstract Skin cancer is the most common type of cancer in the United State A large, shared skin cancer image database on the Internet will be quite valuable to the medical professionals and consumers 
In this paper a skin cancer image database is created using a three-tier system a client application implemented in Java applets a web server and a backend database server JDBC-ODBC is used for the web server to communicate with the database server Various browsing and content-based retrieval methods are supported for the skin cancer image database through web-based graphic user interfaces A data mining algorithm for finding association rules between different features of the skin cancer images is also implemented Key words data mining association rules Web database multimedia, skin cancer images 1 
Introduction Each year, about a million people in the United States learn that they have skin cancer According to current estimates 40 to 50 percent of Americans who live to age 65 will have skin cancer at least once l Skin cancer information on the Internet will benefit the skin cancer research and thus greatly benefit a lot of people The skin cancer database created in this project has been designed to be accessible through the Internet This is consistent with mainstream technology and makes it relatively easy for people to browse query visualize images and get data mining 
results A three-tier architecture is used in this project It involves the front-end layer a client application in Java applets accessible to a web browser the second layer which is a web server and the third layer, which is a backend database server. The Java applet communicates with the web server application via Internet The web server communicates with the database server through the Internet and JDBC-ODBC driver All user needs to have are a connection to the Internet and a Java enabled browser Application libraries are downloaded at runtime The browser is playing the role of 
a graphic user interface CUI and the execution environment The browser is a client of the web server The local operating system does not matter much 2 Database Design An automatic segmentation method for the images of skin tumors is developed in 2 This method first reduces a color image into an intensity image and then finds an approximate segmentation by intensity thresholding Finally it refines the segmentation using image edges One table is designed for this skin cancer database to store the features of 
the tumors Besides the tumor features some other attributes are added into the table These include record number as a primary key of this table, patient id number, the date that the image was taken the image id number to identify the image, and the image file name Image file names are stored in the database instead of image file themselves Although images can be stored in the database as BLOB type our approach is more flexible because image files can be stored elsewhere like on a multimedia server A DBMS can be easily integrated with multimedia servers One advantage is that it 
is easy to integrate multimedia files with existing databases Another advantage is that other non-database applications can access those multimedia files without going through the database While performing browsing or content based retrieval Java applets will try to find and display the images using their file names stored in the database The skin cancer database can be used for medical information retrieval, expert diagnosis and medical pattern discovery 2.1 Image Feature Definitions Irregularity is associated with skin malignancies including malignant melanoma but it remains undefined up to now other than with some subjective terms such as jagged 
notched not smooth or not round One common way to measure irregularity 0 is pz 4z4 wherep and A are the perimeter and area respectively 3 Asymmetry is determined about the near-axis of symmetry by comparing absolute area differences to the total area of the tumor shape 4 Entropy a feature which measures the randomness of gray-level distribution It is defined as 611 0-7695-1062-0/01 10.00 0 2001 IEEE 


P[i J is the gray-level co-occurrence matrix It is defined by first specifying a displacement vector d  dx dy and counting all pairs of pixels separated by d having gray levels i and J Entropy is highest when all entries in P[i j are equal  1 I Energy is defined as 11 Homogeneity is defined as Inertia is defined as z\(i-j Database Browsing and Retrieval 3.1 Architecture This project is implemented in a three-tier architecture The applets run in browser is the front layer the web server is the middle layer and the backend database server is the third layer JDBC-ODBC is used for the communication between web server and database server The Java Database Connectivity JDBC provides a set of API objects and methods to interact with the underlying database It issues SQL statements to request information from local data sources or remote databases It is the layer between the low level database connection and the high level application interface When a JDBC driver does not exist for a specific application JDBC can access an ODBC driver through the JDBC-ODBC bridge So now JDBC has the capability to access almost all databases as ODBC drivers are widely available 3.2 Database Browsing The interface for the database browsing is shown in Figure 1 The goal of this interface design was to make it clear and easy to browse The records are displayed in a form format instead of a table format When the applet is executed the first record in the database is displayed The feature values in the record except the color values are displayed in separate text fields with their feature names The values of average RGB dominant RGB average HIS and dominant HIS are displayed in the same text field respectively The corresponding image is also displayed at the upper right comer The record number is displayed at the upper left comer of the features' frame The displayed record is referred to as the current record To browse the database sequentially two buttons are introduced The Previous button is used for locating the record before the current record in the database while the Next" button is used for locating the record after the current record The combo box beside the two buttons is for displaying records randomly All the record numbers in the database are included in this combo box The user can jump to any record by choosing the record number Figure 1 The interface for database browsing 3.3 Database Retrieval Users can retrieve images by their content i.e by specifying the attribute values or by using a synthesized color 1 Query by Content Users can retrieve the records according to any attribute As shown in Figure 2 all attributes are listed to be available for selection In Figure 2 eleven attributes are selected The Clear button will reset all the check boxes to their unselected state so that a new retrieval can begin The Back" button will bring the user back to the first query interface After the user selects some attributes and the Next button is pressed a new interface is displayed as shown in Figure 3 Only those selected attributes are displayed with two text boxes for each The user can specify a value range for each attribute by filling the boundary values in the corresponding text boxes Figure 2 The interface for content-based retrieval 612 


Although each attribute has two'text boxes so that the user can do range queries exact queries are also possible If the user wants to do exact query only the From" box the left one of the two boxes, needs to be filled Figure 3 The interface for specifying the values of selected attributes The query result is displayed as shown in Figure 4 The records are displayed in a table format instead of a form format This is because all these records meet the search conditions and table format is convenient for comparison and is easy to display arbitrary number of rows The record numbers of all the result records are stored in the combo box beside the image The user can display any of the result records images by selecting the record number in the combo box The bottom part is for similarity-based search queries All image feature attributes are listed in the list box All record numbers of the result records are in the combo box right to the attribute list box The left combo box contains the numbers from 5 to 95 with an interval of 5 The user can retrieve any records that are similar to a record contained in the result table above The similarity between two records is measured by their relative differences in one or more of their attributes values 2 Query by Color As shown in Figure 5 users can specify the color component values using three sliders A color window displays the specified color to the user The real RGB values of the color displayed in the color window are the sliders values multiplied by 255 In order to be consistent with the RGB values in the database fraction numbers are displayed beside each slider The Percent slider allows the user to specify a query range Assume a user selects a R value x and a percentage value y, then the query will look for records whose R value is in the range of x 1-y x 1 Y Figure 5 Query by color Query Result Figure 6 Result of a query by color Figure 4 Result of a retrieval query 613 


4 Data Mining Data mining, which is also referred to as knowledge discovery in databases means a process of nontrivial extraction of implicit previously unknown and potentially useful information from databases 7 It is an emerging research area whose goal is to extract significant patterns or interesting rules from databases Data mining can be broadly classified into three categories Classification Clustering rules that partition the database into finite disjoint and previously known unknown classes Sequences---extracting commonly occurring sequences in ordered data Association rules a form of summarization find the set of most commonly occurring groupings of items 7 In this project mining association rules in a skin cancer database has been implemented 4.1 Association Rule The formal statement of finding association rule is introduced in 8 It can be described as follows Let I  il i2   im be a set of literal called items Let D be a set of transactions, where each transaction Tis a set of items such that T c I Note that the quantities of items bought in a transaction are not considered, meaning that each item is a binary variable representing if an item was bought Each transaction is associated with an identifier, called TID Let X be a set of items A transaction T is said to contain X if and only if X c T An association rule is an implication of the form X a Y where X c I Y c I and X n Y  0 The rule X Y holds in D with confidence c if c of transactions in D that contain X also contain Y The rule X a Y has support s in D if s of transactions in D contain X U Y A set of items is called an itemset and an itemset with k items is called a k-itemset The data mining task for association rules can be broken into two steps Discover the frequent itemsets also called large itemsets that have transaction support above the pre-determined minimum support Use the frequent itemsets to generate the association rules for the database Data Preparation Since each attribute has a value in every record in the skin cancer database association rules are based on which attribute value ranges frequently appear together in the database There are a few methods to partition each attribute value range Equal-population partitioning is used in our implementation Equal-population means the total value range is divided into a certain number of intervals such that almost same number of records are in each interval regardless of the size of the interval For example, assume that there are twenty records and attribute A has a value range of 1 to 100 If the range needs to be partitioned into five intervals they can be 1 to 35 36 to 48, 49 to 52 53 to 73 and 74 to 100 if each of these intervals has four records In the skin cancer database, the number of intervals is decided by the user while the minimum and the maximum number of intervals are predefined The number of intervals is selected for each attribute independently The number of intervals for different attributes may be different since they have different value domains Each attribute value is replaced by the corresponding interval number Such an operation is called discretization After the intervals are determined selected attributes are discretized To keep the original database intact a new table is created for each selected attributes for discretization 4.3 Find Frequent Itemsets Assume table T is the target table and association rules among attributes AI A   A in T are interesting to the user The primary key in T is Record-No n new tables tl t2   t are created with the original table T intact Record-No and AI columns are inserted into tl Record-No and Az columns are inserted into tzr   and Record-No and A columns are inserted into t AI Az   and A values are then discretized in the new tables according to the user's specifications Sort and join operations are used as in the SETM algorithm 191 to find all the frequent itemsets as follows 1 Find frequent I-itemsets For k  1 to n sort to group the records in table tk based on their Ak values Remove those records whose Ak values don't have the minimum support in tk So tk contains only frequent values of Ak Join tl with tz based on the Record-No Sort to group the records in the result of Step 2 based on the values of AI and A2. Find frequent 2-itemsets of AI A values whose supports are higher than the minimum support Remove the records whose Al A values don't have the minimum support 2 3 fork=2ton-1 4 Join the table with frequent k-itemsets of AI A   Ak\values with table tk+I on Record-No to generate a table with Record-No AI Az   Ak Ak+I attributes Sort to group the records in the result of Step 4 based on Al A   Ak Ak values Threshold the result according to the minimum support to get frequent k+l of A1 A   Akr Ak+l values 5 End of for-loop In this algorithm we don't need to join tl and t2 tables first An optimal way is to join two smallest tables containing frequent 1 itemsets first Then join the result with the third smallest table and so on until all the tables are joined 4.4 Generate Association Rules The next step is to use the frequent itemsets to generate the desired association rules Suppose that f is a frequent itemset 614 


then for each non-empty subset a off whether the association rule a 3 If  a holds or.inot is determined by computing the confidence  supportv a If confidence 2 minimum confidence then the rule holds a is called the ascendant of the rule and If a is called the consequent of the rule The candidate ascendant are all possible subsets of the frequent itemset whose length are from 1 to the length of the frequent itemset minus 1 and the corresponding candidate consequents are those left in the frequent itemset 4.5 Example of Mining Association Rules For this specific project the end users are assumed to be doctors specialists or anyone interested in skin cancer who have knowledge about skin'cancer and know what kind of associations they are looking for Therefore to find association rules among which attributes is totally dependent on the end user The interface is designed such that user can find association rules among any combination of the attributes In the interface every attribute in the target table is listed as shown in Figure 7. Except the Malignant attribute a list box is used to select the number of intervals to discretize the attribute values The user can select a value between 1 and 10 for the number of intervals Different attributes can be discretized with different number of intervals The attribute Malignant in the table has only three values malignant benign and pre malignant so there is no need to discretize it Two sliders allow the user to determine the minimum support and the minimum confidence After the user specifies hidher requirements and clicks the Mining button the program begins Valid association rules are displayed to the user as shown in Figure 7 Mining Skin Cancer Database Boundary-Irregularity 1.0791 to 1.1707  Malignant  pre-malignant  Boundary-Irregularity 1.1707 to 1.708 1   Malignant  malignant  Asymmetry 0.0999 to 0.1467  Malignant  benign  Asymmetry 0.2318 to 0.3857  Malignant  malignant  From the rules above we can observe that high boundary irregularity or high asymmetry of the tumor is associated with malignant This coincides with the experts experience 5 Summary In this project we developed a web-based data browsing and content-based retrieval system for a skin cancer database Users can query the database by any combination of the feature attribute values or by synthesized image colors Range queries exact match queries and similarity-based queries are allowed based on any image feature Another contribution of this project is the implementation of an association rule mining algorithm which was originally developed for mining transaction databases Users can find association rules between different skin cancer feature values which can be every useful for skin cancer diagnosis and study References l National Cancer Institute NCI What You Need to Know about Skin Cancer 1995 2 L Xu M Jackowski A Goshtasby C Yu D Roseman and S Bines, "Segmentation of Skin Cancer Images," Image and Vision Computing, 17\(1\1999 pp. 65-74 3 J E Golston W V Stoecker R H Moss and I P S Dhillon Automatic Detection of Irregular Borders in Melanoma and Other Skin Tumors Computerized Medical Imaging and Graphics 16\(3\1992 pp 188-203 4 W V Stoecker W W Li and R H Moss Automatic Detection of Asymmetry in Skin Tumors Computerized Medical Imaging and Graphics 16\(3 1992 pp I91  197 SI R Jain R Kasturi and B G Schunck Machine Vision McCraw-Hill 1995 6 D. H. Ballard and C M. Brown Computer Vision Prentice Hall 1982  P Adriaans and D Zantinge Data Mining Addison Wesley 1996 8 R Agrawal and R Snkant Fast Algonthms for Mining Association Rules Proc of VLDB Conference 1994 pp 487-499 9 M Houtsma and A Swami Set-oriented Mining for Association Rules in Relational Databases Proc of Int Conference on Data Engineering 1995 pp. 25-33 Figure 7. Valid association rules are displayed Some examples of mined association rules are listed below when the minimum support is 12 and the minimum confidence is 60 Boundary-Irregularity 0.9022 to 1.0178  Malignant  benign  615 


4.2.1 The Round Robin Algorithm The main idea behind the Round Robin Algorithm denoted by RRA is rather than selecting a unique victim item per given restrictive association rule we select different victim items in turns starting from the rst item then the second and so on in each sensitive transaction The process starts again at the rst item of the restrictive rule as a victim item each time the last item is reached The rationale behind this selection is that by removing one item at a time from the sensitive transactions it would alleviate the impact on the sanitized database and the legitimate association rules to be discovered since this strategy tries to balance the decreasing of the support of the items in restrictive association rules Selecting the sensitive transactions to sanitize is simply based on their degree of conîict Given the number of sensitive transactions to alter based on   this approach selects for each restrictive rule the sensitive transactions whose degree of conîict is sorted in descending order The rationale is that by sanitizing the conîict sensitive transactions that share a common item with more than one restrictive rule this optimizes the hiding strategy of such rules in one step and consequently minimizes the impact of the sanitization on the discovery of the legitimate association rules The sketch of the Round Robin Algorithm is given as follows Round Robin Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i thei th item is assigned to item v mod k in round robin fashion Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of conîict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End The four steps of this algorithm correspond to the four steps described above in the beginning of this section The rst step builds an inverted index of the items in D in one scan of the database In step 2 the victim item Victim rr i is selected in a round robin fashion for each restrictive associationrule.Line1instep3showsthat  is used to compute the number NumbTrans rr i of transactions to sanitize This means that the threshold  is actually a measure on the impact of the sanitization rather than a direct measure on the restricted association rules to hide or disclose Indirectly  does have an inîuence on the hiding or disclosure of restricted association rules There is actually only one scan of the database in the implementation of step 4 Transactions that do not need sanitization are directly copied from D to D   while the others are sanitized before copied to D   In our implementation the sensitive transactions to be cleansed are rst marked before the database scan for copying The selection of the sensitive transactions to sanitize T ransT oSanitize is based on their degree of conîict hence the sort in line 1 of step 4 When a transaction is selected for sanitization only the victim items are removed from it line 3.1 in step 4 4.2.2 The Random Algorithm The intuition behind the Random Algorithm denoted by RA is to select as a victim item for a given restrictive association rule one item of such rule randomly Like the Round Robin Algorithm the rationale behind this selection is that removing different items from the sensitive transactions would slightly minimize the support of legitimate association rules that would be available for being mined in the sanitized database Selecting the sensitive transactions to sanitize is simply based on their degree of conîict We evaluated the sanitization through the Random Algorithm by selecting sensitive transactions sorted in ascending and descending order The approach based on descending order in general yielded the best results That is why we have adopted such an approach for our algorithm The sketch of the Random Algorithm is given as follows Random Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i  the item assigned to item v is random\(k Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of conîict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


The four steps of this algorithms correspond to those in the Round Robin Algorithm The only difference is that the Random Algorithm selects the victim item randomly while the Round Robin Algorithm selects the victim item taking turns 5 Experimental Results We performed two series of experiments the rst to measure the effectiveness of our sanitization algorithms and the second to measure the efìciency and scalability of the algorithms All the experiments were conducted on a PC AMD Athlon 1900/1600 SPEC CFP2000 588 with 1.2 GB of RAM running a Linux operating system To measure the effectiveness of the algorithms we used a dataset generated by the IBM synthetic data generator to generate a dataset containing 500 different items with 100K transactions in which the average size per transaction is 40 items The effectiveness is measured in terms of the number of restrictive association rules effectively hidden as well as the proportion of legitimate rules accidentally hidden due to the sanitization We selected for our experiments a set of ten restrictive association rules from the dataset ranging from two to ve items in length with support ranging from 20 to 42 and conìdence ranging from 80 to 100 in the database We ran the Apriori algorithm to select such association rules The time required to build the inverted le in main memory was 4.05 seconds Based on this inverted le we retrieved all the sensitive transactions in 1.02 seconds With our ten original restrictive association rules 94701 rules became restricted in the database since any association rule that contains restrictive rules should also be restricted 5.1 Measuring effectiveness In this section we measure the effectiveness of our algorithms taking into account the performance measures introduced in Section 3.3 We compare our algorithms with a similar one proposed in 4 t o h i d e r ul es by reduci ng s upport called Algo2a The algorithm GIH designed by Saygin et al 9 i s s imilar to Algo2a The bas ic dif ference is that in Algo2a some items are removed from sensitive transactions while in GIH a mark  unknowns is placed instead of item deletions Figure 4 shows a special case in which the disclosure threshold  is set to 0 that is no restrictive rule is allowed to be mined from the sanitized database In this situation 30.16 of the legitimate association rules in the case of RRA and RA 24.76 in the case of Algo2a and 20.08 in the case of IGA are accidentally hidden While the algorithms proposed in 4 9 h i d e r ul es reducing their absolute support in the database in our frame        0 5 10 15 20 25 30 35 IGA RRA RA A l g o2a Sanitizing Algorithms Misses Cost   IGA  RRA  RA  Al g o2a  Figure 4 Effect of  on misses cost work the process of modifying transactions satisìes a disclosure threshold  controlled by the database owner This threshold basically expresses how relaxed the privacy preserving mechanisms should be When  0  no restrictive association rules are allowed to be discovered When   100  there are no restrictions on the restrictive association rules The advantage of having this threshold is that it enables a compromise to be found between hiding association rules while missing legitimate ones and nding all legitimate association rules but uncovering restrictive ones Figure 5 shows the effect of the disclosure threshold  on the hiding failure and the misses cost for all three algorithms considering the minimum support threshold   5  Notice that RRA and RA yielded basically the same results That is why their curves are very identical at the scale of the gure As can be observed when  is 0 no restrictive association rule is disclosed for all three algorithms However 30.16 of the legitimate association rules in the case of RRA and RA and 20.08 in the case of IGA are accidentally hidden When  is equal to 100 all restrictive association rules are disclosed and no misses are recorded for legitimate rules What can also be observed is that the hiding failure for RA is slightly better than that for the other approaches On the other hand the impact of IGA on the database is smaller and the misses cost of IGA is the lowest among all approaches before   75  After this value all the algorithms yield similar results Regarding the third performance measure artifactual patterns one may claim that when we decrease the frequencies of some items the relative frequencies in the database may be modiìed by the sanitization process and new rules may emerge However in our experiments the problem artifactual pattern AP was always 0 with all algorithms regardless of the values of   Our sanitization indeed does not remove any transaction The same results can be observed for the algorithms presented in 4 9 We could measure the dissimilarity between the original and sanitized databases by computing the difference between their sizes in bytes However we believe that this Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 0255075100 Disclosure Threshold Hiding Failure IGA RRA RA 0 5 10 15 20 25 30 35 0255075100 Disclosure Threshold Misses Cost IGA RRA RA Figure 5 Effect of  on the hiding failure and misses cost dissimilarity should be measured comparing their contents instead of their sizes Comparing their contents is more intuitive and gouges more accurately the modiìcations made to the transactions in the database To measure the dissimilarity between the original and the sanitized datasets we simply compare the difference of their histograms In this case the horizontal axis of a histogram contains all items in the dataset while the vertical axis corresponds to their frequencies The sum of the frequencies of all items gives the total of the histogram So the dissimilarity between D and D denoted by dif  D D   isgiven by dif  D D   1  n i 1 f D  i   n  i 1  f D  i   f D   i  where f X  i  represents the frequency of the i th item in the dataset X 0 1 2 3 4 5 6 7 IGA RRA RA A l g o2a Sanitizing Algorithms Dissimilarity IGA RRA RA Al g o2a Figure 6 Difference in size between D and D Figure 6 shows the differential between the initial size of the database and the size of the sanitized database when the disclosure threshold  0  To have the smallest impact possible on the database the sanitization algorithm should not reduce the size of the database signiìcantly As can be seen IGA is the one that impacts the least on the database In this particular case 3.55 of the database is lost in the case of IGA 6 in the case of RRA and RA and 5.24 in the case of Algo2a 0 1 2 3 4 5 6 7 0 25 50 75 100 Disclosure Threshold Dissimilarity IGA RRA RA Figure 7 Difference in size between D and D Figure 7 shows the differential between the initial size of the database and the size of the sanitized database for our three algorithms with respect to the disclosure threshold   Again IGA is the one that impacts the least on the database for all values of the disclosure threshold   Thus as can be seen the three algorithms slightly alter the data in the original database while enabling exibility for someone to tune them 5.2 CPU Time for the Sanitization Process We tested the scalability of our sanitization algorithms vis a-vis the size of the database as well as the number of rules to hide Our comparison study also includes the algorithm Algo2a We varied the size of the original database D from 20K transactions to 100K transactions while xing the disclosure threshold  and the support threshold to 0 and keeping the set of restrictive rules constant 10 original patterns Figure 8A shows that IGA RRA and RA increase CPU time linearly with the size of the database while the CPU time in Algo2a grows fast This is due the fact that Algo2a requires various scans over the original database while our algorithms require only two Note that our algorithms yield almost the same CPU time since they are very similar Although IGA sanitizes less sensitive transactions it has an Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


overhead to group restrictive association rules that share the same items and optimizes this process We also varied the number of restrictive rules to hide from approximately 6000 to 29500 while xing the size of the database to 100K transactions and xing the support and disclosure thresholds to   0 Figure 8B shows that our algorithms scale well with the number of rules to hide The gure reports the size of the original set of restricted rules which varied from 2 to 10 This makes the set of all restricted rules range from approximately 6097 to 29558 This scalability is mainly due to the inverted les we use in our approaches for indexing the transactions per item and indexing the sensitive transactions per restrictive rule There is no need to scan the database again whenever we want to access a transaction for sanitization purposes The inverted le gives direct access with pointers to the relevant transactions The CPU time for Algo2a is more expensive due the number of scans over the database 6 Related Work Some effort has been made to address the problem of privacy preservation in association rule mining The class of solutions for this problem has been restricted basically to randomization data partition and data sanitization In this work we focus on the latter category The idea behind data sanitization was introduced in 1 Atallah et al considered the problem of limiting disclosure of sensitive rules aiming at selectively hiding some frequent itemsets from large databases with as little impact on other non-sensitive frequent itemsets as possible Specifically the authors dealt with the problem of modifying a given database so that the support of a given set of sensitive rules mined from the database decreases below the minimum support value The authors focused on the theoretical approach and showed that the optimal sanitization is an NPhard problem In 4 th e a u t h o r s i n v estig ated co n  d e n tiality issu es o f a broad category of association rules and proposed some algorithms to preserve privacy of such rules above a given privacy threshold Although these algorithms ensure privacy preservation they are CPU-intensive since they require multiple scans over a transactional database In addition such algorithms in some way modiìes true data values and relationships by turning some items from 0 to 1 in some transactions In the same direction Saygin et al 9 i nt roduced a method for selectively removing individual values from a database to prevent the discovery of a set of rules while preserving the data for other applications They proposed some algorithms to obscure a given set of sensitive rules by replacing known values with unknowns while minimizing the side effects on non-sensitive rules These algorithms also require various scans to sanitize a database depending on the number of association rules to be hidden Oliveira and Za ane 8 i nt roduced a uni  e d frame w o rk that combines techniques for efìciently hiding restrictive patterns a transaction retrieval engine relying on an inverted le and Boolean queries and a set of algorithms to sanitize a database In this framework the sanitizing algorithms require two scans regardless of the database size and the number of restrictive patterns that must be protected The work presented here differs from the related work in some aspects as follows First we extended our previous work presented in 8 b y addi ng t w o n e w al gori t h ms Round Robin and Random to the set of sanitizing algorithms Second the hiding strategies behind our algorithms deal with the problem 1 and 2 in Figure 3 and most importantly they do not introduce the problem 3 since we do not add noise to the original data Third we study the impact of our hiding strategies in the original database by quantifying how much information is preserved after sanitizing a database So our focus is not only on hiding restrictive association rules but also on maximizing the discovery of rules after sanitizing a database Another difference of our algorithms from the related work is that our algorithms require only two scans over the original database while the algorithms presented in 4 9 requi re v a ri ous s cans depending on the number of association rules to be hidden This is due the fact that our sanitizing algorithms are built on indexes and consequently they achieve a reasonable performance 7 Conclusions In this paper we have introduced two algorithms for balancing privacy and knowledge discovery in association rule mining Our sanitizing algorithms require only two scans regardless of the database size and the number of restrictive association rules that must be protected This rst scan is required to build the index inverted le for speeding up the sanitization process while the second scan is used to sanitize the original database This represents a signiìcant improvement over the previous algorithms presented in the literature 4 9 Our algorithms are integrated to the framework presented in 8 which combines three adv ances for ef ciently hiding restrictive rules inverted les one for indexing the transactions per item and a second for indexing the sensitive transactions per restrictive association rule a transaction retrieval engine relying on Boolean queries for retrieving transaction IDs from the inverted le and combining the resulted lists and a set of sanitizing algorithms The experimental results revealed that our algorithms for sanitizing a transactional database can achieve reasonable Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 140 20 40 60 80 100 DB Size \(transactions in thousands CPU Time \(sec IGA RRA RA Al g o2a 0 10 20 30 40 50 60 70 246810 Set of Restrictive Rules CPU Time \(sec IGA RRA RA Al g o2a AB Figure 8 Results of CPU time for the sanitization process results when compared with the other approaches in the literature Such algorithms slightly alter the data while enabling exibility for someone to tune them In particular the IGA algorithm reached the best performance in terms of dissimilarity and in terms of preservation of legitimate association rules On the other hand the results suggested that RA is slightly better than the other algorithms for hiding failure Although our algorithms guarantee privacy and do not introduce false drops to the data an extra cost is payed because some rules would be removed accidentally since there are functional dependencies between restricted and non-restricted rules The rationale behind this is that privacy preserving association rule mining deals with a tradeoff privacy and accuracy which are contradictory i.e improving one usually incurs a cost for the other It is important to note that our sanitization methods are robust in the sense that there is no de-sanitization possible The alterations to the original database are not saved anywhere since the owner of the database still keeps an original copy of the database intact while distributing the sanitized database Moreover there is no encryption involved There is no possible way to reproduce the original database from the sanitized one Currently we are investigating new optimal sanitization algorithms that minimize the impact in the sanitized database while facilitating proper information accuracy and mining In addition we are working on the optimization of the algorithms RRA and RA specially in terms of preservation of legitimate association rules since their results revealed they are promising 8 Acknowledgments Stanley Oliveira was partially supported by CNPq Conselho Nacional de Desenvolvimento Cient co e Tecnol ogico of Ministry for Science and Technology of Brazil under Grant No 200077/00-7 Osmar Za ane was partially supported by a Research Grant from NSERC Canada We would like to thank Y ucel Saygin and Elena Dasseni for providing us the code of their respective algorithms for our comparison study References 1 M  A tallah  E  Bertin o  A Elmag armid  M  I b r ah im an d V Verykios Disclosure Limitation of Sensitive Rules In Proc of IEEE Knowledge and Data Engineering Workshop  pages 45Ö52 Chicago Illinois November 1999  C  C l i f t on Usi ng S ampl e S i z e t o L i m i t E xposure t o Dat a Mi ning Journal of Computer Security  8\(4\:281Ö307 November 2000 3 C  C lifto n a n d D M a rk s Secu rity an d P ri v a c y Imp licatio n s o f Data Mining In Workshop on Data Mining and Knowledge Discovery  pages 15Ö19 Montreal Canada February 1996 4 E D a s s e n i  V S V e r y k i o s  A K E l m a g a r m i d  a n dE B e r t i n o  Hiding Association Rules by Using Conìdence and Support In Proc of the 4th Information Hiding Workshop  pages 369 383 Pittsburg PA April 2001 5 M  D ietzfelb in g e r  A R Karlin  K  M eh lh o r n  F  M  au f d er Heide H Rohnert and R E Tarjan Dynamic Perfect Hashing Upper and Lower Bounds SIAM Journal on Computing  23\(4\:738Ö761 1994  D E  OêL eary  Kno wledge Disco v e ry as a T hreat to Database Security In G Piatetsky-Shapiro and W J Frawley editors Knowledge Discovery in Databases AAAI/MIT Press pages 507-516 Menlo Park CA 1991 7 S  R M O l i v e i r a a n d O  R  Z a  ane A Framework for Enforcing Privacy in Mining Frequent Patterns Technical report TR02-13 Computer Science Department University of Alberta Canada June 2002 8 S  R M O l i v e i r a a n d O  R  Z a  ane Privacy Preserving Frequent Itemset Mining In Proc of the IEEE ICDM Workshop on Privacy Security and Data Mining  pages 43Ö54 Maebashi City Japan December 2002 9 Y  S aygi n V  S  V e r yki os and C  C l i f t on U s i n g U nkno w n s to Prevent Discovery of Association Rules SIGMOD Record  30\(4\:45Ö54 December 2001 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


