Fuzzy Data Mining: Effect of Fuzzy Discretization Hisao Ishibuchi, Takashi Yamamoto, and Tomoharu Nakashima Department of Industrial Engineering Osaka Prefecture University Gakuen-cho 1-1 Sakai, Osaka 599-8531 JAPAN hisaoi, yarna, nakashi} @ie.osakafu-u.ac.jp Abstract When we generate association rules continuous at tributes have to be discretized into intervals while our knowledge representation is rwt always based on such dis cretization For example we iwcally use some linguistic terms e.g young middle age and old for dividing our ages into somejiuzy categories In this paper we describe the 
extraction of linguistic association rules and examine the peformance of extracted rules First we modi the dejinitions of the two basic measures i.e conjidence and support of association rules for extracting linguistic asso ciation rules The main digerence between srandard and linguistic association rules is the discretization of continu ous attributes We divide the domain interval of each at tribute into some jiizy regions \(i.e., linguistic terms when we extract linguistic a.~sociarion rules Next we compare jizzy discretization with standard non-jidzzy discretization through computer simulations on a pattern classijcation problem with many 
continuous attributes The classGcation peformance of extracted rules on unseen test patterns is ex amined iinder various conditions Simulation results show that lingiiisiic association rules with rule weighls have high generalizalion ability even when rhe domain of each conlin uous attribute is homogeneously partitioned 1 Introduction When our knowledge extraction task involves numerical data with continuous attributes, each attribute is usually dis cretized into several intervals r1.21 The discretization into intervals is used in many machine learning techniques such as decision trees 3 In some situations, human knowledge exactly corresponds to such discretization of continuous at tribute For example the domain of 
our ages is divided into two intervals by the threshold age 20 in the following knowledge 223People under 20 are not allowed to smoke\224 In othcr situations thc discrctization into intcrvals is not appropriate for describing human knowledge For example 0-7695-1 119-8/01 17.00 0 2001 IEEE we may have the following knowledge 223Tallpeople are not comfortable in small cars\224 We cannot appropriately repre sent this knowledge using the discretization of the domain of our height into intervals This is because the linguistic term 223tall\224 cannot be appropriately represented by an inter 
val A mathematical framework for representing linguistic terms is fuzzy logic Fuzzy logic has been recognized as a convenient tool for handling continuous attributes by nile based systems in a human understandable manner 4 This recognition is supported by many successful applications of fuzzy control methods 51 For limction approximation problems with n inpub we use linguistic rules of the following form 1 where q is a nile index x  11     t is an n dimensional input vector Aqi is an antecedent linguistic tcrm such as 
223small\224 and 223large\224 y is an output vari able and B is a consequent linguistic term The above mentioned linguistic knowledge on the comfortableness in small cars can be represented in the form of 1 as 223If 2 is tall then y is low\224 where t is the height and y is the comfort ableness The linguistic rule in 1 can be viewed as an as sociation rule A 3 B where A 
 4,l      The main difference between our linguistic association rules and the standard formulation of association rules 6 is that the domain of each input \(and output variable is fuzzily divided into linguistic terms in our linguistic association rules For example one may divide the domain of our height into two linguistic terms 223tall\224 and 223not tall\224 as shown in Fig 1 The vertical axis of this figure shows the extent to which a particular value of the height on the horizontal axis is com patible with each linguistic term Of course, discretization into linguistic terms depends 
on the situation When we talk about the height of professional basketball players we irn plicitly assume different discretization from the case of the height of college students On the other hand we use linguistic rules of the follow ing form for pattern classification problems If tl is nql and  and I is A then Class C 2 If 21 is qql and  and tn is Aqn then y is E 24 1 


not tall X 140 150 160 170 180 190 200 1 Ileight cm 150 I Comfortable Figure 1 An example of fuzzy discretization 7 where x  21   x is an n-dimensional pattern vec tor and Cy is a class label The above-mentioned linguis tic knowledge on the comfortableness can be represented in the form of 2 as 223If E is lull then Class 2\224 where z is the height and Class 2 is the class label corresponding to 223not comfortable\224. This linguistic nile may be obtained from ex perimental results where a number of examinees are asked whcthcr they fccl cornfortable or not in a small car Sup pose that we have responses in Table 1 from ten examinees on the comfortableness in the small car From Fig 1 and Table 1 we can extract two linguistic rules 223If z is not tall then Class 1 i.e comfortable and 223If z is tall then Class 2 i.e not comfortabie These rules are much more intu itive than interval representation rules such as 223If 2 5 175 then Class 1\224 and 223If E  175 then Class 2\224 177 I Comfortable Table 1 Artificial data for illustration purpose 8 178 I Not comfortable I 2 I 158 I Comfortable I 9 Comfortable Not comfortable 174 Comfortable Not comfortable 185 1 Not comfortable 10 19 1 I Not comfortable The linguistic rules in 1 and 2 can be viewed as as sociation rules A  B and A  C respectively The confidence and the support of these association rules can be defined by extending their standard definitions 6 to the case of fuzzy discretization Through computer sim ulations we compare standard interval discretization with fuzzy discretization We also examine three rule selection criteria i.e the confidcnce the support and their product and two rule types i.e rules withlwithout rule weights 2 Function Approximation Let us assume that we have m input-output pairs xP yp p  1,2    m where xp is an n-dimensional input vector i.e xp  zl,l    xpn and yp is the cor responding output value Our data set D consists of these nz input-output pairs i.e ID1  m For calculating the confidence and the support of the linguistic association rule A j R we have to calculate the number of input-output pairs that arc compatiblc with A and By Such calcu lation is trivial for standard association rules with interval discretization. For example when A is the inequality con dition t  175, we can see that five examinees in Table 1 are compatible with this condition In our rule extraction task the compatibility grades of input-output pairs with A are different from each other For example, each examinee in Table 1 has a different com patibility grade with the linguistic term 223call\224 Such a com patibility grade is mathematically described by a member ship function in fuzzy logic Thc mcmbcrship function of the linguistic term 223tall\224 in Fig 1 is written as 0 ifr 5 170 x  170 if 1TO  G  180 3 1 if 180 5 c A fuzzy set of examinees compatible with 223tall\224 in Table 1 is explicitly written as p 1,2  lo 0.0 0.0 0.0 150\222 158\222 lGl\222\222.\224 where the denominator shows the height of each examinee and the numerator shows its membership value Each ele ment in 4\should not be viewed as a fraction number but a pair of xP and pLtall\(.rp The total number of examinees compatible with 223tall\224 is calculated from 4 as 10 I-WWI  CPLlall\(4 p=l  0.0  0.0  0.0  0.3  222.\222 1.0  4.8 5 In gcncral a fuzzy sct of input-output pairs compatiblc with A is written as where PA  is the membership function of A which is usually defined from the membership function of each lin guistic term A,i by the product operation as PA,\(xp PLdq1\(2pl x  x PAq,,\(xpn 7 242 


The number of input-output pairs compatible with A i.e cardinality of D\(A is defined as A fuzzy set of input-output pairs compatible with both A and R is defined as where pBq  is the membership function of R The cardi nality of D\(A rl D B is dcfincd as m lD\(Aq oq xPA,\(Xp x PB,\(Yp 10 p=l Now wc can define thc confidencc and thc support of the linguistic association rule A 3 B as follows m rn 3 Pattern Classification Let us assume that we have m labeled patterns xp  tp p  1,2    m where xp is an n-dimensional pattern vec tor i.e xp  zpl  xpn and t is the class label of xp Our data set D consists of these m labeled patterns As in the previous section we can define the confidence and the support of the association rule A j C The differ ence between A 3 E and A 3 C is that R is a linguistic term while C is a class label. Since C is a class label the compatibility grade of 1 with C is 0 or 1 1 if t  C PC 1  0 otherwise Thus the cardinality of D\(A n D\(C is defined as The confidence c\(A j C and the support s\(A j Cy of the association rule A j C are calculated from 1 I 12\using ID\(A n D\(Cq instead of ID\(A n D\(B As an example let us calculate c\(tall 3 Class 2 and s tall 3 Class 2 from the ten training patterns in Table 1 where Class 2 corresponds to 223not comfortable\224 Since the data set in Table 1 includes ten examinees ID1  10 As shown in the previous section ID tull I is calculated as ID\(tall  4.8 From Table 1 and 14 ID\(tall fl D\(C1ass 2 I is calculated as ID\(tall Class2  0.3+0.6+0.8+1.0+1.0  3.i 15 Thus the confidence and the support are calculated as c\(tall 3 Class 2  3.7/4.8  0.77 16 s\(tu2l j Class 2  3.7/10  0.37 17 In the same manner, the confidence and the support of the linguistic rule 223fall j Class 1 i.e comfortable are cal culated as c\(tall 3 Class 1  1.1/4.8  0.23 18 tall 3 Class 1  1.1/10  0.11 19 Thus we choose the linguistic association nile 223fall 3 Class 2\224 rather than 223tall Class 1\224 4 Computer Simulations 4.1 Rule Extraction and Pattern Classification In our computer simulations, we used the wine recog nition database in the UCI Machine Learning Repository http://www.ics .uci edu~mleam/MLSummary.html The wine data set is a three-class pattern classification problem with 178 patterns and 13 continuous attributes As aprepro ccssing proccdurc wc normalizcd cach attributc value into a real number in the unit interval O 11 The domain of each attribute was discretized into some linguistic terms For ex ample five linguistic terms are shown in Fig 2 i.e S small MS medium small M medium ML medium large and L large Using those linguistic lems we generated linguistic association rules of the following type with two antecedent conditions If xi is  and xj is 11 then Cia C with CF 20 243 


A 0.0 1.0  Figure 2 Five linguistic terms whcrc GF is a rulc wcight i.c ccrtainty factor For generating linguistic association rules of the form in 20 we examined all the combinations of two antecedent conditions x 5 combinations in the case of five lin guistic terms The consequent class C was specified for each combination A  A A,j  as c\(A j C  mas{c\(A j Class l c\(A a Class a c\(A j Class 3 21 The confidence c\(A j C can be directly used as the nile weight C F  C'F  c\(A  Cq 22 As shown by computer simulations we obtained better re sults from the following definition L'F  c\(A 3 C  C 23 where F is the average confidence for the other two classes In this manner, we generated a number of linguistic as sociation rules of the form in 20 Let S be the set of the generated linguistic association rules We used a single winner-based classification method for classifying a new pattern xp  xpl xp   xpn by the rule set S The winner rule R for the new pattern xp was defined as A,,\(x  max{p,.t,\(xp  R E 3 25 When multiple linguistic rules with different consequent classes had the same maximum value in 25 the classifi cation of the new pattern xp was rejected 4.2 Effect of Fuzzy Partitions Through computer simulations on the wine data set we compared fuzzy partitions with interval partitions First we extracted linguistic association rules of the form in 20 us ing all the 178 patterns in the wine data set as training data We used the two methods for specifying the rule weight CF of each rule We also examined the case of no rule weight This case was examined by assigning the same rule weight to all rules i.e CF  1.0 for Vq Next the same 178 patterns were classified by the extracted linguistic as sociation rules In this manner the classification rate on training data was examined On the other hand we used the leaving-one-out LVI technique for calculating the classi fication rate on test data In the LV1 technique 177 pat terns were used for generating linguistic association rules and the remaining single pattern was used for examining the classification ability of the generated rules This procedure was itcratcd 178 timcs so that all thc 178 pattcrns werc cho sen as test data For comparing fuzzy partitions with inter val partitions we examined four fuzzy partitions in Fig 3 We also examine the corresponding interval partitions As shown in Fig 3 each threshold value in the interval parti tions was specified by the crossing point of the neighboring membership functions Simulation results are summarized in Table 2 N Table 5 Table 2 shows classification rates on training data when fuzzy partitions were used Table 3 shows classification rates on test data On the other hand Table 4 and Table 5 show classification rates when interval partitions were used   0.0 0.0  i 0.0 o olo 1 o A i.i I 1 I I   i 0.0 1.0 0.0 I 0 Figure 3 Four fuzzy and interval partitions Table 2 Results on training data fuzzy of linguistic terms I 2 I 3 I 4 I 5 Direct use of confidence I 90.4 I 94.9 1 96.6 1 94.4 24 I 94.9 I 96.6 I 97.2 I 97.2 No rule weight 1 84.8 I 70.2 I 71.9 I 74.7 From simulation results in Table 2  Table 5 we can 244 


 of intervals 2345 Direct use of confidence 84.8 75.3 74.7 Definition in 23 24 84.8 75.3 74.7 No rule weight 0.0 0.0 1.7 61.2 61.2 0.0 aining data  of linguistic terms 2 3 4 Direct use of confidence 90.4 93.3 93.3 Definition in 23 24 92.7 95.5 94.9 No rule weight 80.3 68.0 68.5 221interval 99.4 99.4 4j51  5 89.9 93.3 69.1   No rule weight I 0.0 I 0.0 I 1.7 I 0.6 t  of intervals Direct use of confidence Definition in 23 24 observe the following 1 The use of nile weights improves the classification per formance of extracted rules 2 The performance of standard association rules with no nile weights is terribly poor 3 While standard association niles have high classifica tion performance on training data their generalization ability is poor especially when the interval partitions are fine \(i.e., when the number of intervals is large 4 The generalization ability of linguistic association niles is better than that of standard association rules We examine each of the above observations in detail First let 11s consider the effect of rule weights 8 In the case of fuzzy partitions nile weights can adjust the classifi cation boundary because we use the product of the compat ibility and thc nilc wcight for classifying ncw patterns scc 25 For visually illustrating the effect of nile weights on the classification boundary, let us consider the four linguis tic rules in Fig.4 Various classification boundaries can be generated from those four linguistic niles as shown in Fig 5 90.4 I 0.0 1 O X1 Figure 4 Four linguistic rules d CF,=I.O CF2d.O CF3~0.2 CF4d.3 Figure 5 Classification boundaries Next let us consider the classification with inkrval discretization In Table 4 and Table 5 classification rates were very low when we used no rule weights In this case the classification of almost all patterns was rejected because they were compatible with many rulcs with diffcrcnt conscqucnt classcs This sitriation is illustrated in Fig 6 where the following two niles are depicted 223If xI is in the interval 4 then Class 1\224 and 223If x2 is in the interval B then Class 2\224 When these two rules have the same rule weight we cannot classify any pat terns in the overlapping area i.e the square in the center of Fig 6 In our computer simulations, many rules overlap with each other in the 13-dimensional pattern space Thus the classification of almost all patterns was rejected When we used a different rule weight for each rule, such an unde sirable situation was resolved Another interesting observation in Table 4 and Table 5 is that the same results were obtained from the two differ ent specifications of rule weights This is because the rule weight of each rule was used only for determining the win ncr rulc in thc casc of interval discrctization On thc con trary rule weights modify cla$sification boundaries in the 245 


1.0 I x2 B  1.0 A 0.0 XI Figure 6 Two overlapping rules case of fuzzy discretization as shown in Fig 5 Thc poor gcneralization ability of association nilcs in thc case of interval discretization i.e Table 5 is due to the following reasons 1 The discretization was not adjusted The classification performance can be improved by appropriately parti tioning the domain of each continuous attribute This will be discussed in Section 5 2 All the association rules generated from training pat tcms wcrc uscd for classifying tcst pattcrns Thc rcla tion between the number of rules and their generaliza tion ability will be examined in the next subsection While the above two reasons also apply to the case of fuzzy discretization, the generalization ability of linguistic associ ation rules is much higher than that of standard association rules This is because the threshold values for discretization are not crisp but fuzzy The classification boundaries can bc adjusted by thc usc of rulc wcights as wc havc alrcady shown in Fig 5 In Fig 7 we compare the fuzzy discretization with the interval discretization from the viewpoint of the covered re gion by a single rule In the case of the fuzzy discretization in Fig 7 a the shaded region can be classified by a sin gle linguistic rule located in the center of that region On the other hand, the corresponding standard association nile can classify the much smaller shaded region in Fig 7 b Thus the classification of much more new patterns may be rejectcd in thc casc of intcrval discrctization than fuzzy dis cretization especially when the interval partitions are fine The deterioration in the generalization ability by the use of fine partitions is not so severe in the case of fuzzy partitions because each linguistic association rule can cover a much larger region compare Table 3 with Table 5 43 Effect of Rule Selection In our computer simulations in the previous subsections we used all the extracted rules for classifying new patterns In this section wc examine the relation bctwccn the num ber of rules and their generalization ability We first divided  S S MMLL a FULLY partition b Interval partition Figure 7 Classifiable region by a single rule the extracted niles from training data into three groups ac cording to their consequent classes Then the niles in each group were sorted in a descending order of their confidence values. When nultiple niles had the same confidence value they were randomly sorted For decreasing the effect of such randomization, we averaged simulation results over 20 independent runs Finally we chose the first iV niles from cach group to classifying tcst data iV  I 2     30 As in the previous sections we used the leaving-one-out LV 1 technique for evaluating the generalization ability of the ex tracted niles from training data In addition to the confi dence we also examined two different nile selection crite ria the support and the product of the confidence and the support In such computer simdations we used 23 23 for specifying nile weights and the fiizzy partition by three linguistic terms For comparison we also used the interval partition by three intervals Simulation results are summarized in Fig 8  Fig 10 As we have already shown, higher classification rates on test data were obtained from linguistic association niles than standard association niles We can see from those figiires that thc product of thc confidcncc and thc support is thc bcst nile selection criterion among the examined three criteria When we use the confidence as a nile selection criterion we tend to select association niles with high confidence but low support On the other hand we tend to select asswi ation niles with high support but low confidence when we use the support as a nile selection criterion 5 Inhomogeneous Partitions In computer simulations in the previous section we used homogeneous fuzzy partitions using symmetric triangular membership functions While such partitions have been of ten used in many applications of fuzzy rule-based systems those partitions are not optimal from the viewpoint of the classification pcrformancc of cxtractcd linguistic rulcs In the previous section we also used interval partitions ob 246 


Linguistic r Number of tules for each class Figure 8 Results by the confidence 100 I Number of rules for each class Figure 9 Results by the support tained from homogeneous fuzzy partitions Such interval partitions arc not optimal either In this scction wc dis cretize the domain of each attribute into some intervals us ing the entropy measure \(for details see Quinlan 31 The domain of each attribute was discretized independently of the other attributes Let S be the number of intervals for each attribute That is the domain interval 0 11 was dis cretized into A intervals using A  1 threshold values The threshold values were selected from m 1 candidates each of which is the mid-point of a pair of neighboring at tribute values in the given m patterns All the m-~C~-l combinations for selccting A7  1 thrcshold valucs from rn  1 candidates were examined by calculating the cor responding entropy for each combination The discretiza tion with the minimum entropy was selected for each at tribute We performed this discretization as a preprocessing procedure before the rule extraction For comparison we specified the corresponding fuzzy partition from the inter val partition as shown in Fig 11 The specification of the fuzzy partition in Fig 11 is based on the following two con ditions 1 The sum of neighboring membership functions is always 1 and 2\Crossing points of neighboring mem bership functions coincide with the threshold values in the interval partition I 30 Yumber of rules for each class Figure 10 Results by the product  I I I I1 0.0 1 o Figure 11 Inhomogeneous fuzzy partitions Using inhomogeneous partitions based on the entropy criterion we compared fiizzy partition with interval parti tions in the same manner as Subsection 4.2 Simulation results are summarided in Table 6  Table 9 From the comparison between the simulation results in this section and those in Subsection 4.2 we can see that the discretiza tion based on the entropy measure improved the classifica tion performance of standard association rules on training data \(comparc Tablc 4 with tablc 8 Wc can also obscrvc from the comparison between Table 3 and Table 7 that the generalization ability of linguistic association rules does not strongly depend on the choice of a fuzzy partition Table 6 Results on training data fuzzy oflinguistic terms 1 2 I 3 I 4 I 5 Directuseofconfidence I 93.7 I 93.3 I 97.2 1 98.3 Definition in\(23 24 I 97.8 I 94.9 I 97.2 I 98.3 No rule weight I 68.0 1 50.0 I 45.5 I 28.7 In the same manner as in Subsection 4.3 we performed rule selection using the three criteria the confidence the support and their product We observed from simulation results that the generalization ability of standard association rulcs was drastically improvcd by the use of thc cntropy based discretization and the rule selection In Fig 12 we 247 


Table 7 Results on test data Ifuuvh 224 Direct use of confidence Definition in 23 24 No nile weight Table 9 Results on test data fintervall 93.3 91.6 95.5 94.4 94.9 92.7 96.6 94.4 67.4 50.0 43.8 24.7 I of linguistic terms I 2 I 3 I 4 I 5 I Direct use of confidence Definition in 23 24 No rule weight I of intervals 121314151 92.7 76.4 64.0 52.8 92.7 76.4 64.0 52.8 0.0 3.9 3.4 2.8  of intervals Direct use of confidence Definition in 23 24 No rule weight Table 8 Results on trainina data \(interval 2345 97.8 98.9 99.4 100 97.8 98.9 99.4 100 0.0 3.9 3.9 2.8 demonstrate the relation between the number of standard association rules and their generalization ability for several specifications of the interval discretization where Ii shows the number of intervals From this figure we can see that high generalization ability was obtained by appropriately choosing standard association rules e K=2 0 K=3 m K=4 4 K=5 0 30 Number ofrules for each class 50 Figure 12 Results by the product interval 6 Concluding Remarks In this paper we extended the definitions of the two basic measures i.e confidence and support of association rules to the case of linguistic rules As we explained, linguis tic des are much more intuitive than the standard associ ation rules in some situations We demonstrated through computer simulations that the generalization ability of lin guistic rules is less sensitive to the choice of a discretization method and the number of rules than that of standard associ ation rules Thus linguistic rules have two advantages over thc standard association rulcs intuitivc intcrprctability and robust performance Of course the usefulness of linguis tic rules is problcm-dcpcndcnt Linguistic rulcs are morc intuitive in some situations, and standard association rules are more appropriate in other situations We also demon strated through computer simulations that we can obtain high generalization ability from standard association rules by appropriately specifying the discretization of continuous attributes and the number of rules References U M Fayyad and K B Irani 223Multi-interval dis cretization of continuous-valued attributes for classi fication learning,\224 Proc of 13th International Joint Conference on Artijicial Intelligence pp 1022-1027 1993 J Dougherty R Kohavi and M Sahami 223Super vised and unsupervised discretization of continuous fcaturcs,\224 Proc of 12th International Conference on Machine Learning pp 194-202 1995 J R Quinlan C4.5 Programs for Machine Learning Morgan Kaufmann 1993 S J Russell and P Nonfig Art$cial Inlelligence A Modern Approach Prentice-Hall 1995 C T Leondes Ed Fuuy Theory Systems Tech niques and Applications Academic Press 1999 R Agrawal and R Srikant 223Fast algorithms for min ing association rules.\224 Proc of 20th International Conference on Very Large Data Bases Santiago Chile\pp 487-499 September 1994 Expanded ver sion is available as IBM Research Report RJ9839 June 1994 H Ishibuchi T Nakashima and T Morisawa 223Voting in fuzzy rule-based systems for pattern classification problems,\224 Fuzzy Sets and Systems vol 103 no.2 pp 223-238 1999 H Ishibuchi and T Nakashima 223Effect of rle weights in fuzzy rule-based classification systems,\224 IEEE Trans on Fuzzy Systems vol 9 no 4 August 200 1 248 


                 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


