Analyzing the Interestingness of Association Rules from the Temporal Dimension Bing Liu, Yiming Ma School of Computing National University of Singapore 3 Science Drive 2 Singapore 117543  liub, maym} @comp.nus.edu.sg Ronnie Lee Department of Statistics and Applied Probability National University of Singapore 3 Science Drive 2 Singapore 1 17543 Abstract Rule discovery is one of the central tasks of data mining Existing research has produced many algorithms for the purpose. These algorithms, however, often generate too many rules In the past few years rule interestingness techniques were proposed to help the user find interesting rules These techniques typically employ 
the dataset as a whole to mine rules and then filter and/or rank the discovered rules in various ways In this paper we argue that this is insufficient. These techniques are unable to answer a question that is of critical importance to the application of rules i.e can the rules be trusted In practice, the users are always concerned with the question They want to know whether the rules indeed represent some true and stable or reliable\underlying relationships in the domain If a rule is not stable, does it show any systematic pattern such as a trend Before any rule can be used these questions must be answered This 
paper proposes a technique to use statistical methods to analyze rules from the temporal dimension to answer these questions. Experimental results show that the proposed technique is very effective 6 Introduction The objective of data mining is to find interestinghseful knowledge for the user Rules are an important form of knowledge Existing research has produced many algorithms for rule mining These techniques however often generate too many rules and most of the rules are of no use to the user 22 23 151 In the past few years, a number of rule interestingness techniques were proposed to deal with the problem [12 13 14, 15, 21, 22 23,4 16 24, 25 
281 They typically use the whole dataset to mine rules and then filter andlor rank the discovered rules in various ways to help the user identify interestinghseful ones These techniques however have a major shortcoming They are unable to answer a question that is crucial to the application of rules i.e can the rules be trusted or will the rules hold in the future A rule has no use if it does not hold in the future or is not predictive In this paper we argue that an important aspect of the rules has not been studied i.e the temporal aspect Analyzing rules from the temporal dimension is important for real-life applications 
In the past few years we used a number of interestingness techniques to help our users find interesting rules 13 14 151 Although these techniques helped a great deal our users often still felt uncomfortable The reason is that the users are concerned with the reliability of the rules They want to know whether the rules will hold in the future i.e whether the rules indeed represent some stable or reliable underlying relationships in the domain If a rule is not stable does it show any systematic trend The inability of the existing approaches to answer these questions lies with the fact that they do not study the dynamic behavior of rules In this research we analyze the interestingness of 
rules from the temporal dimension to solve the problem This approach makes sense as most datasets are collected over time and represent the behaviors of the application domains in different time periods In this research we focus on analyzing association rules Association rule mining is commonly stated as follows 3 Let I   i   in be a set of items and D be a set of data tuples \(records Each data tuple consists of a subset of items in I An association rule is an implication of the form X  Y where X c I Y c 
I and X n Y  0 The rule X  Y holds in D with confidence c if c of data tuples in D that support X also support Y The rule has support s in D ifs of data tuples in D contains X U Y Given a set of data tuples D the database the problem of mining association rules is to discover all rules that have support and confidence greater than or equal to the user-specified minimum support called minsup 
and minimum confidence called minconj This paper presents a set of statistical methods to analyze the behavior of rules over time The basic idea is as follows The dataset is first partitioned into a few blocks or sub-datasets corresponding to the time periods e.g years months or weeks in which they were collected The granularity of the time period is application dependent We then mine rules from each block Since the amount of data available is usually large the partitioning does not lose significance of the rules discovered After all the rules are found we analyze the supports and 0-7695-1 119-8/01 17.00 0 2001 IEEE 377 


confidences of the rules in these time periods which allow us to answer the above questions and to find various types of important rules, e.g 0 Stable rules These rules do not change a great deal over time Stable rules are more reliable and can be trusted That is they can be safely used in real-world performance tasks Trend rules These rules indicate some underlying systematic trends If the trend rules are known the user can take necessary actions to exploit desirable trends and to reverse/delay undesirable trends Our experiments and practical applications show that the proposed technique is very effective. Testing on unseen future data confirms that the stable rules and the trend rules identified by our method are indeed reliable Our experiments also indicate that although the number of rules generated from the data can be large the number of stable rules and those rules that show some trends is actually quite small It is possible to manually inspect them to identify those truly useful/actionable ones 0 2 Related Work Existing research on rule interestingness focuses on a few directions namely template matching unexpected rule identification rule summarization and organization We discuss them below in turn  121 proposes a template-based approach for finding interesting rules This approach first asks the user to specify what rules he/she wants The system then finds those matching rules 15 23 211 propose to used user expectations or beliefs to help himher find unexpected rules In this approach the user first input his/her existing knowledge about the domain and the system then finds those conforming and unexpected rules  141 presents a technique to summarize the discovered association rules using a small subset of the rules 13 presents a technique to organize the discovered rules in such a way that they can be easily browsed by the user 4 25 18 101 report a number of methods for ranking the discovered rules using some statistical interestingness measures All these techniques are different from our work as none of them analyzes rules over time Thus they are unable to answer the rule stability or reliability question 2 proposes to mine and to monitor rules in different time periods The discovered rules from different time periods are collected into a rule base Ups and downs in support or confidence over time called history are represented and defined using shape operators The user can then query the rule base by specifying some history specifications This is related to our work However there is a major difference We analyze rules generated over different time periods using statistical tests rather than simply matching user specifications By using statistical tests we can formally analyze the significance of ups and downs of support and confidence. User specifications lack statistical foundation One will not know whether the changes are significant and worth investigating 6 presents a framework for measuring changes in two models The difference between the two models e.g two sets of itemsets one generated from dataset D1 and one generated from dataset D2 is quantified as the amount of work e.g difference in supports\required to transform one model into the other. In a related work finds the support differences of association rules mined from two datasets and uses the differences to detect emerging patterns These are different from our work as we analyze rules over a number of time periods while 6 51 only compare the supportskonfidences of rules from two periods Our method uses statistical methods to identify significant changes, trends and stable rules Both 6 51 do not perform these tasks 3 Basic Steps of the Proposed Approach This section gives an overview of the proposed approach which consists of 3 steps 1 Partitioning the dataset The original dataset D is first partitioned vertically into a number of blocks or sub-datasets DI D2  D according to the time periods TI T2  T in which they were collected e.g years or months 2 Mining rules from sub-datasets We then mine rules from each sub-dataset D Let the set of rules mined from D be RI The set R of rules that will be analyzed in the third step below is defined as follows This means that if a rule appears in any rule set RI it is considered a potentially interesting rule The reason that we consider all such rules will be clear later Clearly a rule r in R may appear in RI but not in RI i  j because r may not satisfy minsup and/or minconf in DJ For our analysis later we need the supports and confidences of every rule in R in all time periods or all 0 Thus the missing support and confidence information in certain time periods for each rule needs to be obtained This can be done quite easily We can first mine rules from each sub-dataset D to produce R We then scan the data D once to obtain all the missing supports and confidences Note that in rule mining, pruning is also performed to remove those insignificant rules We use the technique given in Analyzing rules over time After all the necessary information about supports and confidences of each rule r in R in all time periods is obtained we analyze R   r I r E RI U R2  U R  3 378 


the rules to give the user different types of interesting rules A number of rankings of rules according to different statistical measures are performed to enable the user to view those most interesting rules first In the next section we discuss step 3 which is the focus of this paper Step 1 and 2 will not be discussed further as they are fairly straightforward 4 Analyzing Rules Over Time We now present a number of statistical tests to analyze the discovered rules in R from different perspectives We assume that the confidence value denoted by coni and the support value denoted by sup of each rule in R in each time period T has been obtained by a mining algorithm Our analysis aims to identify semi-stable rules stable rules rules that exhibit trends Note that the statistical tests presented here are by no means the only tests that can be performed on rules In fact many other tests may also be used for various purposes We also point out that the kinds of datasets and the problems dealt with here are different from time series data In the latter a given population is tracked over time For our case although the behavior of rules over time is of interest each time period typically corresponds to a different population 4.1 Semi-stable rules Intuitively, a rule r in R is a semi-stable rule if none of its confidences or supports in the time periods TI T2  Tq is statistically below minconf or minsup If some observed confidence conJ or support sup of the rule is less than minconf or minsup it may be due to chance Semi-stable rules are in contrast with stable rules which have more stringent conditions \(see the next sub-section Definition semi-stable confidence rules Let minsup and minconf be the minimum support and the minimum confidence confD and supD be the actual support and the actual confidence of a rule r obtained from the whole dataset D coni be the actual confidence of the rule in the time period T and a be a specified significance level The rule r is a semi-stable confidence rule over the time periods TI T2  Tq if the following two conditions are met 1 supD 2 minsup and confD 2 minconf 2 For each time period or sub-dataset we fail to reject the following null hypothesis at the significance level y Ho conJ 2 minconf Note that the first condition is to ensure that the rule r satisfies the user specified minsup and minconf thresholds in the whole dataset To test the hypothesis in the second condition we can use the statistical test on a single proportion 26 171 Test statistic Consider testing the following null hypothesis Ho against the alternative hypothesis HI Ho P 2 Po H1:pcpo where po is the given proportion The test statistic z with standard normal distribution for the sample proportion b is where n is the sample size If z is less than its critical value at the significance level a we _reject the null hypothesis In our case po is minconf ad is coni Example 1 Assume in a mining session we set minconf to 50 There is a time period Ti in which the confidence conJ of a rule A  B is 45 There are 300 data tuples that satisfy the condition A and out of these 300 tuples 135 tuples also satisfy the consequent B i.e coni 135/300  45 The question is whether conh is statistically below 50 We solve the problem as follows From the problem description we obtain the population size n  300  45 and po  50 We use a  5 The null and alternative hypotheses are Ho p 2 0.50 Hi p  0.50 The critical value for z is  1.65 at a  5 which can be obtained from statistical tables Let us compute z fi  PO  0.45 0.5 z  1.73 0.0288675 The observed value of the test statistic z  1.73 which is less than the critical value of z  1.65 Therefore we reject Ho and accept the alternative hypothesis HI In other words the difference between the sample proportion and the hypothesized value of the population proportion is large and this difference is unlikely to have occurred due to chance alone We now define semi-stable support rules Definition semi-stable support rules Let minsup and minconf be the minimum support and the minimum confidence confD and SUPD be the actual support and the actual confidence of a rule r obtained from the whole dataset D sup be the actual support of the rule in the time period T and a be a specified significance level The rule r is a semi-stable support rule over the 379 


time periods TI T2  Tq if the following two conditions are met 1 supD 2 minsup and confD 2 minconf 2 For each time period or sub-dataset we fail to reject the following null hypothesis at the significance level a Ho sup 2 minsup Here again we can use the statistical test on a single proportion to test the null hypothesis Computation complexity Let the number of rules in R be IRI and the number of time periods be q The complexity of semi-stability test is O\(qlRI Since q is normally very small, and IRI is very large \(in thousands or more the computation is thus linear in IRI 4.2 Stable rules A semi-stable rule only requires that its confidences or supports\over time are not statistically below minconf or minsup However the confidences or supports of the rule may vary a great deal Hence the behavior can be unpredictable In practice the user often wants rules that are stable over time i.e with predictable behaviors We call such rules stable rules Intuitively a stable rule is a semi-stable rule and its confidences or supports over time do not vary a great deal i.e they are homogeneous Since both confidence and support are population proportions we need a statistical test for population proportions The Chi-square test 26 171 is a popular choice for testing homogeneity of multiple proportions Definition stable confidence rules Let minsup and minconf be the minimum support and minimum confidence con be the actual confidence of a rule in the time period T and a be a specified significance level The rule r in R is a stable confidence rule over the time periods TI T2  Tq if the following two conditions are met 1 r is a semi-stable confidence rule 2 We fail to reject the following hypothesis at the significance level a Ho con5  con     confq Test of homogeneity A test of homogeneity involves testing the null hypothesis Ho that the proportions pI p2    Pk in two or more different populations are the same against the alternative hypothesis HI that these proportions are not the same That is Ho pi  p2    pk HI the population proportions are not all equal We assume that the data consists of independent random samples of size nl n2  nk from k populations The data is arranged in a 2xk contingency table Figure 1 The numbers XI x2   xk nl-xl n2-x2    nk-xk listed inside Successes Failures I I Samde I 1 2  k x1 X2  xk nl-xl nl-xz  nl-xk 3 80 


TI T2 T3 satisfy A A B 450 454.5 400 334.2 420 481.3 satisfy A A 7B 230 225.5 100 165.8 300 238.7 test statistic 2 is 62.7 I If we use the significance level of 5 the critical value for 2 is 5.99 with 2 degree of freedom df  2-1 3-1  2 The observed 2 value is much larger than the critical value Thus we reject the null hypothesis and conclude that the confidences of the rule over the three time periods are significantly different The above example tests the homogeneity of confidences of a rule over time We can also use the same method to test the supports of a rule Definition stable support rules Let minsup and minconf be the minimum support and minimum confidence supi be the actual support of a rule r in R in the time period T and a be a specified significance level The rule r is a stable support rule over the time periods TI T2  T if the following two conditions are met 1 2 r is a semi-stable support rule We fail to reject the following hypothesis at the significance level a Ho SUP  SUP    supy After a set of stable confidence or support rules are found ranking can be performed according to the mean confidence or the mean support of each rule over time This ranking allows the user to see rules that have higher average confidences or supports first Computation complexity without considering the final ranking Since the number of cells in a contingency table is 2q the computation complexity of stability test is O\(qlRI Since q is small and IRI is very large, the computation is linear in IRI Col Total 1270 630 4.3 Rules that exhibit trends In many applications users are interested in knowing whether changes in support or confidence of a rule over time are random or there is an underlying trend A statistical test called the run test  111 is often used to test whether a sequence of data is from a random process or exhibits trend Below we give a brief description of the run test in our application context Assume we have a rule that has the following confidences for TI   T6  One of the main assumptions of the 2 test is that the expected frequencies are not be too small 8 The rule of thumb is that the 2 test is appropriate only when no expected frequency is less than 5 This seldom happens in our case as we normally deal with large datasets When an expected frequency is below 5 extensions of the Fisher's exact test may be used which however is quite cumbersome and not popular 8 In our application, we find that for stability test the user is willing to give a range such that if the confidences or supports are within the range the rule is considered stable Time confidence change TI 70 75  1  Run 1 T2 T3 79 T4 82  7-5 78  3 Run2 T6 81  3 Run3 The direction of change in successive observations is shown by plus and minus signs A run is a succession of plus or minus signs, surrounded by the opposite sign If we read from top to bottom we would say that we have a run of three s followed by a run of one  and followed by a run of one  In our example there are 3 runs one run of length three and two runs of length one The above sequence may suggest a trend toward increasing values of confidence That is these values of confidence may not reasonably be looked upon as being the items of a random sample The exact run test for trends is based on the probability density function p.d.f of W the number of runs in a combined ordered sample consisting of two types of items X and Y say. Let m be the number of X items and n be the number of Y items Then as shown in  1 I the p.d.f of W is given by when W is odd when W is even Pr w  2 k     1    I    k-1 k-1 where 2k and 2k+l are elements that belong to the space of W and k is a positive integer When k  0 i.e W  l we need special treatment There are two cases 1 If m  0 orn  0 Pr\(W 1  1 That is if m=O or n  0 one run is the only possibility This case clearly indicates a trend If m  0 and n  0 Pr\(W  1  0 That is if m  0 and n  0 it is not possible to be one run 2 We are interested in testing Ho sequence generated by a random process HI sequence exhibits trend The critical region of this test is of the form W I v v is determined by computing a  Pr\(W I v Ho where a is the significance level normally 5 or 10 Let WO denote the observed number of runs e.g in our above 381 


example WO  3 If WO is greater than v we fail to reject the null hypothesis In the computation we do not need to find the value of v We can simply do the following We compute P  Pr W  1  Pr W  2    Pr W  WO If P I a we reject Ho and accept the alternative hypothesis HI sequence exhibits trend When m and n are small it is easy to compute the probability When they are large the distribution of W can be approximated by a normal distribution  111 with mean mn m+n p 2 1 P  1 P  2 and variance cr\222  We can then use the standard normal approximation to test our hypothesis m+n-1 w P I 0 The run test above finds those rules that exhibit trends However it does not tell the types of trends We rank the rules according to the number of s in each of them to show whether a rule is more likely to have an upward or a downward trend Thus the rules on the top are more likely to show upward trends while the rules at the bottom are more likely to show downward trends Computation complexity without considering the final ranking Since 223m or n choose k\224 and finding the numbers m and n can be done in linear time in q the complexity of the trend analysis is O\(qWoIRI Since both q and WO WO S q are small the whole computation is linear in IRI When q is large \(more than 20 l l we can use the normal approximation Then the complexity becomes O\(qlRI as z can be computed in O 1 5 Evaluation We now report our experiment results The objective is to show the effectiveness and efficiency of the proposed technique We used 4 real life datasets one from a vehicle insurance company, one from an education institution, and two from another education institution We did not use public domain datasets such as those in UCI machine learning repository  191 because most of these datasets are not time or sequence related For those datasets that have sequence we do not understand them and thus do not know how to partition them meaningfully a Finding different types of rules Table 1 next page shows the datasets and the results of rule generation and statistical tests Column 1 gives the name of each dataset Column 2 gives the number of tuples in each dataset Column 3 shows the number of partitions of each dataset In our case each partition contains one year of data. Column 4 gives the number of rules generated from all partitions or blocks i.e the size of R Column 5 gives the number of rules generated from the original dataset D Note that the minsup and minconf values were suggested by our users From column 4 and 5 we can see that the number of rules generated from each dataset is very large Column 6 gives the number of rules left after pruning from all partitions or blocks. Column 7 gives the number of rules after pruning for the whole dataset D we used the pruning method in 14 From column 6 and 7 we observe that the number of rules after pruning is substantially smaller for each data Column 8 gives the execution time in seconds for rule generation Column 9 gives the execution time for statistical tests running on Pentium I1 350 with 128MB RAM We see that statistical tests can be done very efficiently The following two tables Table 2 and 3 show the statistical test results  We used the significance level of 5 a commonly used level 26 171 in all our statistical tests In column 2 of both Table 2 and Table 3 we reproduce the number of rules to be analyzed for each dataset which is shown in column 6 of Table 1 Table 2 gives the results of the analysis on rule confidence We observe that although the number of discovered rules from each dataset is large the number of stable confidence rules column 3 and the number of rules that exhibit trends column 5 are actually quite small Column 4 gives the number of semi-stable rules which does not include the stable rules We observe that the education domains are quite stable as most of the rules are either stable or semi-stable The insurance domain is, however much more volatile A large majority of rules are unstable Rule support analysis given in Table 3 shows similar results Since the number of stable rules and trend rules are not that large, manual analysis is possible 5.2 Accuracy tests The main aim of our technique is to find rules that can be trusted and used in the future Thus we need to see whether the stable rules identified will still be stable and trend rules will still maintain their trends in the future For these experiments we make use of unseen test sets Since we partition each dataset into q blocks we use the first q 1 blocks to find stable rules semi-stable rules and trend rules We then test them on the unseen qth block For comparison we use mean squared error and mean absolute error to evaluate stable rules semi-stable rules and the rest of the rules \(called unstable rules We will use the mean expected confidence or support\of each In Chi-square test it is assumed that the populations are independent For the 3 education datasets this is hue as the data from different years represent different student populations In the insurance case the dataset records those insurers who had accidents and made claims It is also reasonable to assume that the data From different years are independent 382 


Table 1  Application datasets and the generated rules 1 CONF 2 3 4 5 all rules stable stable trend semi lnsur 1075 Edu2-2 1 2 3 4 semi SUP all rules stable stable rule over the q-1 time periods as the predicted confidence or support of the rule in the new qth block data We want to see the error produced by each type of rules on the test data Mean squared error MSE and mean absolute error MAE are commonly used in statistics to evaluate model accuracy They are defined as follows 5 trend c I Y  l I MAE  c Y  f S MSE  S where Y is the confidence or support of a rule in the new data and 2 is the estimated confidence or support of the rule, which is the mean value of the confidences \(or supports of the rule in the q-1 blocks S is the total number of rules tested Table 4 and 5 show the accuracy results on confidence and support respectively We discuss Table 4 first Column 2 of Table 4 gives the percentage of stable confidence rules that remain stable after considering the qth block the final year of data That is our statistical tests now consider one more year In the case of insurance data 96.4 of stable rules remain stable 2.4 of the lnsur Edu2-2 stable rules become semi-stable For the other three datasets the percentages of stable rules remaining stable are slightly smaller but are still very large We believe the reason is that these datasets are smaller and thus tend to have bigger variations over time However the remaining rules are almost all semi-stable Column 4 shows that almost all semi-stable confidence rules are still semi stable after the qth block is considered note that the semi stable rules here do not include stable rules Column 5 shows that most trend rules still exhibit trends The proportions are slightly less than those for stable rules and semi-stable rules because the number q of time periods in our experiments is small and thus the trend test is less stable Column 6 7 and 8 show the mean squared errors for each dataset when we use the mean confidences of stable rules semi-stable rules and unstable rules to predict the confidences of these rules on the qth data block respectively Column 9 10 and 11 give the respective mean absolute errors for each dataset We can see that stable rules commit least error The error of unstable rules can be very large For example in the case of Edul for Table 4 Accuracy results on confidence analysis Table 5 Accuracy results on support analysis 383 


unstable confidence rules the error in confidence on the qth block is 16.8 on average while the error in confidence for stable confidence rules is only 3.5 The errors with the insurance data are smaller because this dataset is large and thus the variance is small Note that the MSE and MAE values are not computed for trend rules as it is not reasonable to use the mean confidence as the predicted confidence in the next time period due to the trend Our trend test is mainly to give the user an indication that trends exist rather than predict the confidence of the rule in the future for which we need a larger sample \(large q and further statistical analysis Table 5 gives the corresponding results on the analysis of supports From column 2 and 3 we see that almost all stable support rules are still stable A large majority of semi-stable support rules are also still semi-stable column 4 Column 5 shows that most trend rules still exhibit trends From column 6-11 we observe that the stable support rules commit least error The results from both Table 4 and 5 confirm that stable rules and trend rules identified by our technique are indeed reliable and can be trusted in the future 6 Conclusions Association mining is an important data mining task Its application is however hampered by the fact that it often generates a large number of rules In this paper we argue that the large number of rules is only part of the problem Lack of systematic analysis procedures to help the user understand the behavior of rules is also a major issue Without understanding the behavior of a rule over time the rule cannot be used in practice This paper presented a number of statistical tests to analyze the behavior of rules Using these tests we are able to identify stable rules and trend rules and at the same time remove those unreliable unstable rules Experiment results showed that the proposed technique is very effective and efficient References Aggarwal C and Yu P 223Online generation of association rules.\223 ICDE-98,402-4 1 1 Agrawal R and Psaila G 223Active data mining.\224 Agrawal R and Srikant R 223Fast algorithms for mining association rules.\224 VLDB-94 1994 Bayardo R and Agrawal R 223Mining the most interesting rules\223 KDD-99 1999 Dong G and J Li 223Efficient mining of emerging patterns discovering trends and differences.\224 KDD 99 1999 Ganti V Gehrke J and Ramakrishnan R 223A framework for measuring changes in data characteristics\224 POPS-99 Ganti V Gehrke J and Ramakrishnan R KDD-95 1995 223DEMON Mining and Monitoring Evolving Data.\224 ICDE-2000 8 Hamilton L C Modern data analysis a first course in applied statistics Brooks/Cole, 1990 9 Han J and Fu Y 223Discovery of multiple-level association rules from large databases.\224 VLDB-95 lo Hilderman R and Hamilton H 223Principles for Mining Summaries Using Objective Measures of Interestingness.\223 ICTAI-00 2000  111 Hogg R  Craig A Introduction to mathematical statistics Macmillan Publishing 1970 12 Klemetinen M Mannila H Ronkainen P Toivonen H and Verkamo A.I 223Finding interesting rules from large sets of discovered association rules.\224 CIKM-94 1994  Liu B Hu M and Hsu W 223Multi-level organization and summarization of the discovered rules.\224 KDD-2000 14 Liu B Hsu W and Ma Y 223Pruning and summarizing the discovered associations.\224 KDD-99 15 Liu B Hsu W Mun L and Lee H 223Finding interesting patterns using user expectations.\224 IEEE Trans on Knowl  Data Eng vol 11\(6\1999  Liu H Lu H., Feng F and Hussain F 223Efficient search of reliable exceptions.\224 PA KDD-99 1999  Mann P S Introductory statistics John Wiley  Sons 1998  Megiddo M and Srikant R 223Discovering predictive association rules.\224 KDD-98 1998 19 Merz C J and Murphy P UCI repository of machine learning databases 1996 http://www.cs.uci.edu/-mlearn/MLRepository.htd 20 Ng R Lakshmanan L Han J 223Exploratory mining and pruning optimizations of constrained association rules.\224 SIGMOD-98 1998  Padmanabhan B and Tuzhilin A 223A belief driven method for discovering unexpected patterns.\224 22 Piatesky-Shapiro G and Matheus C 223The interestingness of deviations.\224 KDD-94 1994  Silberschatz A and Tuzhilin A 223What makes patterns interesting in knowledge discovery systems.\224 IEEE Trans on Know and Data Eng 24 Suzuki E 223Autonomous discovery of reliable exception rules.\224 KDD-97 1997 25 Tan P-N. and Kumar V 223Interestingness measures for association patterns a perspective.\224 KDD-2000 Workshop on Post-processing in Machine Learning and Data Mining 2000  Walpole R  Myers R Probability and statistics for engineers and scientists Prentice Hall 1993  Zaki M 223Generating non-redundant association rules.\224 KDD-2000 2000 28 Zhong N Ohshima M  Ohsuga S 223Peculiarity Oriented Mining and Its Application for Knowledge Discovery in Amino-acid Data.\224 PA KDD-2001 KDD-98 1998 8\(6\1996 pp 970-974 3 84 


                 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


