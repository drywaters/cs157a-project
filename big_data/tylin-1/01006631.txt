Granulating Data Non-Scalar Attribute On Values Lawrence Mazlack Sarah Coppock Computer Science University of Cincinnati Cincinnati Ohio 45220  mazlack, coppocs}@uc.edu Abstract Data mining discouvers interesting informa tion from a data set Mining incorporates differ ent methods and considers different kinds of in formation Granulation is an important aspect of mining The data sets can be extremely large with multiple kinds of data in high dimensional ity 
Without granulation large data sets often are computationally infeasible and the generated results may be overly fine grained Most available algorithms work with quantita tive data However many data sets contain a mixture of quantitative and qualitative data Our goal is to group records containing multiple data varieties quantitative discrete continuous and qualitative ordinal nominal Grouping based on different quantitative metrics can be difficult Incorporating various qualitative elements is not simple There are partially successful strategies as well as several 
differential geometries We ex pect to use a mixture of scalar methods and soft computing methods rough sets fuzzy sets as well as methods using other metrics To cluster whole records in a data set it would be useful to have a general similarity met ric or a set of integrated similarity metrics that would allow record to record similarity compari sons There are methods to granulate data items belonging to a single attribute 
Few methods ex ist that might meaningfully handle a combina tion of many data varieties in a single metric This paper is an initial consideration of strate gies for integrating multiple metrics in the task of granulating records GROUPING RECORDS TOGETHER Granulation helps data mining accomplish association rule discouvery classification, partitioning clustering and sequence discouvery Without granulation large data sets often are computationally infeasible and the generated re sults may be overly fine grained Data mining a data set 
composed of varied data stored in records can focus on either: granulating individual attributes data extracted from the records making up a data set, or whole records To cluster whole records in a data set it would be useh1 to have a general similarity metric that would allow record to record similarity comparison There are methods to granulate data items belonging to a single attribute Unfor tunately few methods exist that meaningfully account for a combination of many data varieties Clustering groups objects into clusters so that the simi larity among objects within the same cluster intra-cluster 
similarity is maximized and the similarity between objects in different clusters inter-cluster similarity is minimized Clustering increases granule size and is useful in data min ing Clustering can discouver the general data distribution and aid in the discouvery of similar objects described in the data set A good characterization of the resulting clusters can also be a valuable data mining product There are two types of hierarchical approaches to cluster ing agglomerative and divisive. Agglomerative begins with all objects in their own cluster and combines clusters to gether for 
which the similarity is the largest This is done repeatedly until all objects are in the same cluster Con versely, divisive begins with all objects in the same cluster and does the reverse Because most well understood ap proaches use a similarity metric an similarity appropriate metric or a way to integrate diverse metrics is desirable Any mix of data varieties without losing the meaning behind the metric\222s is necessary Another approach to grouping records is partitioning Sometimes the term partitioning is used as if synonymous with clustering However partitioning can also be ap 
proached as a purification process \(Coppersmith 1999 where partitions progressively become more pure Increasing gran ule of small partitions is then a matter of relaxing partition boundaries through either rough sets or fuzzy values A data set can have millions of records with hundreds of attributes. The attributes may have many disparate kinds of data Some algorithms offer promise in handling multiple kinds of data Unfortunately they are not scalable as their complexity is geometric. They are only useful for small data 0-7803-7280-8/02/$10.00 82002 IEEE 944 


sets In addition, some approaches lose the meaning of the metric when trying minimize algorithmic complexity DATA VARIETIES Data can be classified by scale and kind \(i.e qualitative or quantitative\Most current clustering algorithms deal with quantitative data It includes continuous values such as a person\222s height and discrete values such as the number of cars sold Qualitative data are simply symbols or names with no natural scale between the values This includes nominal data such as the color of a car and ordinal data such as the doneness of a burger rare medium well For measuring the similarity of two quantitative values, a function of the difference in magnitudes is used For evaluat ing similarity between qualitative values it is common to use simple matching; i.e if the two values match then the similarity is I otherwise, the similarity is 0 Generalizing distance clustering algorithms to handle a mix of data varieties often loses the meaning of a metric in an effort to restrain complexity An example of this is sim ply dividing the attributes into quantitative and qualitative Then apply a different metric to each Finally add every thing together for the overall similarity measure Unfortu nately, the resulting metric loses the consistency found in individual measures of the same nature. It is open to question if this will be successful in obtaining a good picture of the overall similarity between records For example, one quanti tative attribute can contribute infinitely many possible val ues while simple matching on a qualitative attribute may only contribute two possible values 0 or I Modification to make individual metrics consistent with each other would introduce significant computational complexity A consequence of the lack of scalar metric is the difficulty in quantitatively measuring the similarity between two val ues Ordinal data provides more information than nominal Ordinal data\222s progressively increasing values allow the ma tion of an index that ranks the ordinal data The increasing index values supply some relative information. A temptation to be avoided is to use the ranked index as a scalar measure An index built on nominal data provides no information use ful for clustering purposes Sometimes naive workers at tempt to apply nominal indices as a scalar measure Many data sets contain multiple varieties of data When clustering records it is important that this is considered in evaluating record similarity or conversely the dissimilarity Developed clustering methods work sufficiently well for quantitative data where the dissimilarity between two records could be a Euclidean distance measure  An unanswered ques A variety of other distance metrics are possible; e.g the Minkowski metric The Minkowski metric is defined as c Ix,-y,I 222 Euclidean geometry is the case where p=2 0-7803-7280-8/02/$10.00 02002 IEEE 945 tion is whether it would be useful to use a different geometry or a transformation into a different space analogously to a dual transformation in operations research If the data to be clustered is ordinal the values can be mapped into rank index values, e.g l..m according to their natural order Some authors have tried to use the ranked index as a distance measure Han 2001 344-5 Li 1998 The difficulty with mapping ordinal data is that an assumed arti ficial scale is imposed on the data. For example, if the values are large medium small then mapping them as large I medium 2 small 3 has the inference that the similarity between medium and small and medium and large are equal The same problem occurs when mapping nominal values except an artificial ordering is also imposed We believe that these approaches are unsatisfactory as a general approach due to their inherent artificiality An effort offering greater prom ise might be to express a non-equal increment scale using fuzzy distributions Data can vary in magnitude whether it is quantitative or qualitative Sometimes data is normalized so attributes with different maximum magnitudes can be grouped together by relative size. For example, the range for human male height might be asserted as being up to 220 cm; while the range for grasshoppers might be asserted as being up to 10 cm If we wanted to group critters as: tiny, small, medium or large we might first normalize the data to a maximum of a conven ient value, say 1.0 If the distinction between sizes is linear it usually is not then grouping by size is relatively straightforward. If not then other techniques have to be used Often the range of values to be included in a particular group is uneven and the discrimination point between them unclear For example if we are grouping humans into child adolescent adult elderly the dividing points both depend on perception Even potential definition items vary with location and situation such as school entry date re quired age before leaving school legal age to vote age at which it is first possible to retire with full benefits required age of retirement legal age to have sex*, legal age to drive elderlv 0 25 50 75 Sometimes data is expressed across a distribution such as time How to best normalize data for magnitude, uniformity and distribution is an active research question in several areas including genomics Normalization of ordinal data is more  Age of consent varies widely. Worldwide the range is none to 21 In the USA depending on state the range is 14 to 18 Reference http://www.ageofconsent.com/ageofconsent.htm 


problematic as the ordinal labels may not be uniform or complete Data is not always represented linearly For example it is common to show data on a logarithmic scale Clustering logarithmically scaled data is often done The decision to representkluster data logarithmically is usually done by a human. For the purposes of granulation little work has been done to computationally decide when data should be repre sented non-linearly e.g logarithmically There are many differential geometries that may be ap plied to the data besides Euclidean. Mathematicians have dB veloped many geometries that are primarily theoretical play things However non-Euclidean differential geometries are commonly used in physics astronomy chemistry and biol ogy for example Riemann space Various theories are de pendent on the non-Euclidean differential geometries e.g string theory and space-time There are transformations that can be used to change Euclidean values to non-Euclidean e.g Lorentz transformation Some of the geometries have higher dimensionality e.g Minkowski space unifies Euclid ean 3-space with time in Einstein's theory of special relativ ity There does not appear to be any work done using differ ential geometries to partition records SIMILARITY/DISSIMILARITY IN MIXED DATA Many current similarity metrics use pair-wise compari sons in the measurement of the similarity between two E ords. For example if the two records are ml then the similarity between the two records would be defined as sim\(aI,a2 6 sim\(bI,b2  sim\(cI,c2 where 8 indicates some combination operator and sim\(x,y is a measurement of similarity between the attribute values x and y We suggest that a more useful inter-record similarity measure would be w sim\(a 1 a2 8 w sim bl b2 8 w sim\(c 1 c2 where w represents weights for each attribute pair How to determine these weights especially if done computationally is unclear Although the similarity measure between records can be either quantitative or qualitative many current metrics at tempt to derive a quantitative scalar measure This is desir able in the clustering task because many clustering methods utilize scalar distances 0-7803-7280-8/02/$10.00 02002 lEEE 9 There are metrics such as those based on simple matching e.g the Jaccard coefficient\that work well for when all at tributes are categorical Sneath 1973 Wang 1999 It is important to note the difference between the simple matching coefficient and simple matching. Simple matching results in a match or no match result 0 or I as two qualitative values are different or the same The simple matching coefficient Sneath 1973 is the proportion of number of matching val ues to the total number of values compared When qualitative values are mapped into a form appropriate for metrics suit able for quantitative data, the utility of the measure is lost For example starting with a data set containing three records r wit,, color bag size fruit Let one arbitrary mapping a be fruit  orange I apple 2 color  red 1 orange 2 green 3 and another arbitrary mapping R be hit  orange 2 apple 1  color  red 0 orange 1 green 6 In these cases the values I 2 etc are arbitrarily assigned numeric integers In this example bag size is already a nu meric character. Whether it is a quantitative value or an arbi trary qualitative value ordinal or nominal is not described Assume we are using Euclidean distance defined as xi yi]2 where X and Y are the records being compared and xi and yi are the values for the ith attribute of X and Y With the mapping xi record r has equal distance to both r and r3 With the mapping a this is not the case Let d\(x,y repre sent the distance between records x and y but The difficulty is that Euclidean distance is defined for quanti tative values, but we are imposing an ordering and a scale not be reflected in the real world Consequently it is unlikely that the result is useful This example demonstrates that we can construct an arbitrary mapping but we cannot be sure about the utility of the resulting measure d\(ri rz  d\(ri,rJ with ai d\(rl r  d\(r,,r3 with a The difficulty of applying metrics commonly used with quantitative data is not limited to extensions to qualitative data. Guha 2000 provides an example of a problem that can occur when using distance metrics such as Euclidean s tance on binary data with centroid-based clustering It is pos sible to reach a situation where a given record is calculated as closer to one cluster's mean when it is really not If all di mensions are Boolean i.e there are no quantitative 46 


attributes then a distance computation such as Euclidean does not discriminate A record could be considered close to a mean when it actually has no values in common with the mean or less values in common than it has with another record This occurs regardless of whether the Boolean values were developed from qualitative matchho-match testing or from other sources Conversely to the situation of trying to apply quantitative methods to qualitative data, using the metrics developed for qualitative data causes loss of information when applied to quantitative data Li 1998 For example if using simple matching between quantitative values, it is possible to obtain the same pair-wise similarity between values For example consider the values 3.4 3.5 and 4.2 They will have the same pair-wise similarity between them 0 The fact that the value 4.2 is more dissimilar to 3.4 than to 3.5 is lost Even if simple matching were used on quantitative values that have been mapped to discrete intervals such as 3.0,3.5 informa tion loss is still likely In this case, much like the last the loss is the ordering of objects imposed by similarity Con sidering the same values with 3.1 in addition 3.1 3.4 and 3.5 will now be pair-wise considered to be the same i.e their pair-wise similarity will be 1 In both cases, the order ing imposed by a similarity measure will be lost Some qualitative clustering approaches cluster values ex tracted from the records rather than the records themselves Gibson 2000 Han, 1997\\(Zhang 2000 Most qualitative clustering methods simply group items together They do measure of closeness for any two particular values For ex ample if it is discovered that a b  c belong to the same cluster how can we decide whether a closer to b or to c If this could be derived then it could be used in the clustering of whole records Gibson 2000 and Zhang 2000 use a dynamic system to propagate weights from an initial value The weight is propagated according to frequency and co-occurrences The weights are then used to consider two clusters, one contain ing the initial value and the other its complement It is not clear whether the resulting weights can be used to compare pair-wise similarity Hank 1995 approach uses a hyper graph to represent the frequencies and weights the graph edges according to the co-occurrences The metrics used in these approaches are based on the frequency of the values and their occurrences together in the records This is directed towards the problem discussed by Kana1 1993 when he discussed Watanabe\222s \(1969 1985\\223Theorem Of The Ugly Duckling;\224 namely the need to have weighted memberships One way of achieving weighted memberships is to use soft computing tools Wang \(1999 and Ganti 1999\also use the value\222s fk quency to find the clustering Wang uses the metric in a func tion to evaluate clustering goodness. Values are termed 1-e if they are present within a cluster above a threshold \(human specified If values are not found to be large in a cluster they are termed small By determining two sets for each clus ter one for large values and the other small values an evaluation function can be defined The goal is to minimize this function over any possible clustering The evaluation function is a sum of the inter-cluster measure and intra-clus ter metrics A weight is provided to allow for more emphasis on either the inter or intra-cluster similarity. The intra-cluster similarity measure is the count of small values over all of the clusters. The inter-cluster similarity measure is the num ber of values that are large in multiple clusters i.e overlap between clusters An approach that does not use fkquency for the metric is the extension to the k-means algorithm by Huang 1997 1999 and Ralambondrainy 1995 Originally the distance between records is a quantitative metric such as Euclidean distance. They modify the distance based k-means algorithm to handle categorical data by combining two measures one for the quantitative values and one for the qualitative values The qualitative measure is the sum of the reverse of simple matching for categorical attributes I or 0 indicating non matching and matching respectively. \221The distance between two records is then a weighted sum of the quantitative value\222s measure and the Categorical value\222s measure In Huang 1999\similarity is computed as the sum of square differ ences for the numerical attributes added to a weighted sum mation of matches for the categorical attributes This approach does not attempt to fit one type of metric to all kinds of data present. It attempts to find a meaningll way to combine multiple metrics to obtain the overall dis tance. The similarity for two records is a combination of two metrics one for quantitative and one for categorical Huang weights only the qualitative measure while Ralambondrainy 1995\weights both measures. Ralambondrainy offers a pos sible way to weight attributes In both cases \(Huang, Ralambondrainy\the discovery of an appropriate weighting adds complexity to the algorithm With a meaningful weighting, the numerical values maintain and contribute their magnitude while the categorical values with no magnitude to contribute can contribute proportion ally to the measure. Without a good weight parameter a  ficulty arises because the metric loses the meaning of 223similar.\224 This point was previously made by Goodall 1966\The following example shows the difficulty with the added weight\(s\parameter. If the data set is 0-7803-7280-8/02/$10.00 02002 IEEE 947 


The respective dissimilarity using Huang\222s approach with weight of 1 is displayed in the following matrix Notice d\(t,.t2 and d\(t,,t where d\(r is the distance dissimilarity\for objects ti and ti Clearly this is a case of the quantitative measure dominating the qualitative measure An algorithmic question is How should the mismatching qualitative values between t and tz contribute to the measure while the quantitative difference between t and t contributes according to its magnitude? One option is to find a quantita tive metric consistent across all of the attributes being con sidered in the measure. This follows from the idea of normal izing the data before the computation of similarity Everitt 1993\Another option finding appropriate attribute weights Huang 1997 suggests that the weight be selected according to the distribution of the quantitative attributes In the ex ample above, using the standard deviation as the weight gives minimal change to the distances in the example Li 1 998 developed their method using the Goodall simi larity metric Goodall 1966 This metric measures the amount of weight that a categorical value contributes to the overall similarity measure For example if two records have the same value for a qualitative attribute k then the similar ity is not necessarily the values returned from matchhon match 0 I that most qualitative similarity metrics assign The given value assigned for a match between qualitative values is therefore a real number between zero and one. This similarity value that is assigned is decided according to the frequency of the value within the data Let s\(u,b,c,d represent the fact that c is more similar to d than U is to b Then when computing the measure for two quantitative values, say x and y the probability of other pairs of values s and t such that s\(x,y,s,t is used This use of the fiquency distribution of values allows for a more meaningful measure. The difference in magnitude for quantitative values is used in the deciding of s\(a,b,c,d The chi-squared 01\222 statistic is used for computing the measure between records When using this statistic there is an assumption of independence among the attributes Often independence among attributes cannot be guaran teed. Many methods have the underlying requirement of at tribute independence However often there is dependence How to handle dependent attributes is a significant unsolved question The distributions of the quantitative values offer added in formation in the computation of similarity It is not un common that data sets are biased. That is, they are not repre sentative of the population as a whole e.g a data set of diabetic patients In this case if two records have matching qualitative values, if the value is commonly found with dia betics, then it should not be as significant as matching val ues that are not common The above discussion includes two metrics one based on dissimilarity or distance Huang\222s metric and the other based on similarity, Li\222s metric We cannot directly compare the two metrics but we can attempt to compare them in an indirect way To begin with if we think about dissimilarity as the complement of similarity, then it is quite possible that the two metrics are not consistent with each other. It is diffi cult to dlscover whether the metrics actually correspond to each other If we are given one metric say similarity we cannot easily compute its complement dissimilarity\Rich ter \(1992 refers to some properties common to similarity and dissimilarity metrics EPILOGUE Our goal is to group records containing multiple data va rieties quantitative discrete continuous and qualitative ordinal, nominal This paper is an initial consideration of strategies for integrating multiple metrics in the task of granulating records Data mining a data set composed of varied data stored in records can focus on either: granulating individual attributes data extracted from the records making up a data set, or whole records To cluster whole records in a data set it would be useful to have a general similarity metric that would allow record to record similarity comparison There are methods to granulate data items belonging to a single attribute Few methods exist that meaninghlly account for a combination of many data varieties in a single metric 0-7803-7280-8/02/$10.00 02002 IEEE 948 


Most available algorithms work with quantitative data However, many data sets contain a mixture of quantitative and qualitative data Our goal is to group records containing multiple data varieties: quantitative \(discrete, continuous and qualitative \(ordmal, nominal This is a difiicult task Even grouping based on different quantitative metrics can be diffi cult There are several known partially successful strategies Incorporating qualitative elements is not simple We expect to use a mixture of scalar methods soft computing rough sets fuzzy sets as well as methods using other metrics Potentially there are also several possible differential geometries that might be applied For a metric to cluster records in a data set it would be useful to have a single similarity measure Unfortunately very few exist that can handle combinations of different kinds of data The meaningful multi-modal metrics are so far re stricted to particular scientific domains It appears that modification to make individual metrics consistent with each other may introduce significant compu tational complexity How to best do this is an open question Finding a way to integrate or combine a different metrics offers more promise than developing a metric general enough to use on all types of data The lack of magnitude and scale in nominal data creates a difficulty in discovering the weight ing needed to develop a useful metric So far methods to develop generalized distance clustering algorithms to handle a mix of data varieties often lose the meaning of a metric in an effort to restrain complexity To cluster whole records in a data set it would be useful to have a general similarity metric or a set of integrated simi larity metrics that would allow record to record similarity comparisons. Our research seeks to accomplish this BIBLIOGRAPHY G. Biswas J Weinberg D Fisher 1998 223ITERATE: A Conceptual Clus tering Algorithm For Data Mining,\224 IEEE Transactions on Systems Man And Cybernetics-Part C Applications and Reviews v 28 n 2 May p 219 230 D Coppersmith S.J Hong J Hosking 1999 223Partitioning Nominal At tributes In Decision Trees,\224 Data Mining And Knowledge Discovery v 3 n B. Everitt 1993 Cluster Analysis 3ded Hodder  Stoughton, London V Ganti I Gehrke R Ramakrishnan 1999 223CACTUS: Clustering Cate gorical Data Using Summaries,\224 Knowledge Discovery and Data Mining p D Gibson J Kleinberg P Raghavan 2000 223C$stering Categorical Data An Approach Based On Dynamical Systems Proceedings of the 24Ih VLDB Conference v 8 n 314 p 222-236 D Goodall 1996 223A New Similarity Index Based On Probability,\224 Bio metrics v 22 n 4 p 882-907 S Guha R Rastogi K Shim 2000 223ROCK: A Robust Clustering Algo rithm For Categorical Attributes,\224 Information Systems v 25 n 5 p 345 366 E Han G Karypis V Kumar B Mobasher 1997 223Clustering Based On Association Rule Hypergraphs,\224 Proceedings of SIGMOD 22297 Work shop on Research Issues in Data Mining and Knowledge Discouvery DMKD\22297 May, p 9-13 2 p 197-217 73-83 J. Han M Kamber 2001 Data Mining Concepts and Techniques Mor gan Kaufmann Publishers San Francisco Z Huang M Ng 1999 223A Fuzzy k-Modes Algorithm For Clustering Categorical Data,\224 IEEE Transactions on Fuzzy Systems v 7 n 4 August 2 Huang 1997 223Clustering Large Data Sets With Mixed Numeric And Categorical Values,\223 Proceedings Of I\224 Pacific-Asia Conference on Knowledge Discouvery And Data Mining p 21-34 A Jain R Dubes 1988 Algorithms For Clustering Data Prentice Hall New Jersey L Kana1 1993 223On Pattem Categories And Altemate Realities,\224 Pat tern Recognition Letters 14 p 241-255 C Li G Biswas 1998 223Conceptual Clustering With Numeric-And Nominal Mixed Data  A New Similarity Based System,\224 IEEE Transac tions on KCE H Ralambondrainy 1995 223A Conceptual Version of the K-Means Algo rithrn,\224Pattem Recognition Letters 16 p 1 147-1 157 M.M Richter 1992 223Classification and Leaming of Similarity Meas ures,\224 Proceedings der Jahrestagung der Gesellschaji fur Klassifikation Studies in Classification Data Analysis and Knowledge Organisation Springer Verlag P. Sneath R Sokal\(l973\Numerical Taxonomy Freeman and Company San Francisco K Wang C Xu B Liu 1999 223Clustering Transactions Using Large Items,\224 CIKM, p 483-490 S Watanabe 1960 Knowing And Guessing Wiley New York S Watanabe 1985 Pattern Recognition  Human And Mechanical Wiley New York Y Zhang A Wai-chee Fu, C. Cai P Heng 2000 223Clustering Categori cal Data,\224 16Ih International Conference on Data Engineering p 446-452 0-7803-7280-8/02 10.00 a2002 IEEE 949 


Comparielon of Seif-Similarity CUNOS Gazallt 0 10 20 30 40 50 BO 70 80 Sample size a Gazelle Camperision Ot Sell-Similarity Curves T1014D5M s 0.8 E 0.75 I 0.7 0.55 0.55  0.1 RC-S  0.6 O.l%A-S  O.l%A-SS  r 0.1 RC.SS 0 10 20 30 40 50 BO 70 80 Sample size b VBwk Cornparision 01 Self-Similatity Curves: TBISDZSM 0.05 RC-S  0.05 A-S  0.05 A-SS D  0.75 0.7 0.65 0.05 RC-SS  0 10 20 30 40 50 BO 70 80 Sample Sire tc T1014D5M  I 0.75 0.08 RC-S  0.08 A-S  0.08 A.SS 0  0.6 0.55 0.5 0.08 RC-SS   0 10 20 30 40 50 BO 70 Bo Sample size d TBI5D25M Figure 3 Evaluation of Proposed Method Similarity plots PerbmanceCawkkn RCn A VT!d P&manceCanpamlon UCn A:T815025M IC f F s 9 Y if E 5 0.1  C 9  0.1XA  k _ o.l%Rcs  wm 0.lXRCSS  0.IXRC-S  I  IO M B U M MI 70 80 WIW 0 10 M 30 10 M MI 70 80 90100 saw IO sanp1e sa0 Sam sa a VBook b T1014D5M c T815D25M Figure 4 Impact of Representative Set on Performance Sampling MethOdolcgy Performance T1014D5M:0.05 sup SBrnpling Melhodobgy Breakdown T1014D5M:0.05 0 10 20 30 40 50 80 70 80 90 1W 0 10 20 30 40 50 60 70 80 90 100 Sample Sue Sample Size a Overall Performance b Breakdown Figure 5 Sampling Methodology on Performance Breakdown 360 


of 73.17 125 for RC-SS seconds to compute the results The approach using the entire association set requires 140 seconds to compute the same result When compared with the baseline execution time \(1730s the representative class approach\222s improvement factor is around 25 Impact of Sampling Methodology In Figure 5 we con sider the performance of the sampling methodology pre sented in Section 3 Figure 5a compares the cumulative execution time performance of our the approach using the representative class to identify the ideal sample size while overlapping the sampling and U0 operations of the next stage with the computation of the current stage In Figure 5a we compare the performance of ideal situation where there is no sampling overhead TC-ideal with the actual perfor mance with overlapping \(TC-overlap and without overlap ping \(TC-nooverlap For the overlapping computation we used two scenarios, one where the processor handling the WO and sampling was a 300Mhz Pentium 113 i.e a more realistic scenario for active disks with a much slower pro cessor than the compute processor\and the other where the U0 processor was lGHz ideal baseline On viewing the graphs it is clear that the TC-ideal graph and the TC-overlap \(1GHz\are almost identical, reflecting the fact that almost all of the sampling overhead is over lapped with useful computation \(computing the association set for the previous sample size On relaxing the assump tion and allowing for the fact that the processor doing the sampling and WO is often much slower 300 Mhz we ob serve a marginal drop in performance \(slightly under 4 so still most of the sampling overhead is overlapped with useful computation Note that if we did not overlap the sampling overhead with computation then we perform 10 12% worse than the ideal \(comparing the TC-Ideal and TC noverlap graphs This experiment assumes the best pos sible sampling algorithm the Sample A algorithm We further broke down the performance of the sampling over heads in Figure 5b for the two configurations \(1GHz and 300Mhz The performance of the naive algorithm \(see Sec tion 3 is much worse than the Sample A algorithm 5 Conclusions and Future Work We have presented an efficient method to progressively sample for association rules Our approach relies on a novel measure of model accuracy \(self-similarity of associations across progressive samples\the identification of a represen tative class of frequent itemsets that mimic extremely accu rately\the self-similarity values across the entire set of asso ciations and an efficient sampling methodology that hides 3We did not have a dual processor system where one processor was fast and the other was slow We timed the sampling overheads for each of the different sample sizes for his dataset on the slower machine and we used these times by precomputing the samples and busy waiting for the necessary amount of time while evaluating the performance on this configuration the overhead of obtaining progressive samples by overlap ping it with useful computation We evaluated the results on a set of real and synthetic datasets We extensively benchmarked each aspect of our algorithm and obtained uniformly good performance several factor-fold execution time improvements across both real and synthetic datasets In the current work we have considered each sample to be independent of the other We would like to see if the pro posed method can be improved by using adaptive sampling techniques[S Other directions of future work have been outlined already in the text References I Anurag Acharya et 01 Active disks Programming model algorithms and evaluation In ASPLOS 1998 2 C Aggawal and P Yu Online generation of association rules In ICDE 1998 3 R Agrawal and R Srikant Fast algorithms for mining asso ciation rules In 20th VLDB Conf September 1994 4l Doug Burdick Manuel Calimlim and 1 E. Gehrke Mafia A maximal frequent itemset algorithm for transactional databases In ICDE 2001  5 Jason Catlett Megainduction A test flight In Machine Learning pages 59k599 1991 61 W G Cochran Sampling Techniques I Wiley  Sons-1977 171 G Das H Mannila and P Ronkainen Similarity of at tributes by external probes In KDD 1998 SI Carlos Doming0 and Osamu Watanabe Scaling up a boosting-based learner via adaptive sampling In PAKDD 2wO 9 1 Han and 1 Pei Yiwen yin Mining frequent patterns with out candidate generation In SIGMOD 2ooO IO Haussler Kearns Seung and Tishby Rigorous leaming curve bounds from statistical mechanics In COLT 1994 1 I George H John and Pat Langley Static versus dynamic sam pling for data mining In KDD 1996 I21 F Olken and D. Rotem Random sampling from database files  a survey In 5th Intl Conf Staristical and Scientific Dorubose Manugement April 1990 I31 S Parthasarathy Efficient progressive sampling for associ ation rules OSU CIS Technical Repon Number TR-OSU CISRC-5/02-TR13 November 2001 revised March 2002 I41 Foster 1 Provost David lensen and Tim Oates Efficient progressive sampling In KDD 1999 I51 Foster 1 Provost and Venkateswarlu Kolluri A survey of methods for scaling up inductive algorithms Duta Mining and Knowledge Discovery 3\(2 1999 I61 E Riedel G Gibson and C Faloutsos Active storage for large-scale data mining and multimedia In VLDB 1998 I71 Pradeep Shenoy et al Turbo-charging vertical mining of large databases In SIGMOD pages 22-33.2wO 1181 H Toivonen Samding large databases for association rules  In 22nd VLDB c 1996 1191 I S Vitter An efficient algorithm for sequential random sampling In ACM Trans Mathematical Siftware volume 13\(1 pages 58-67 March 87 ZO M I Zaki S Parthasarathy er al Evaluation of sampling for data mining of association rules In RIDE 1997 211 M 1 Zaki S Parthasarathy et al New algorithms for fast discovery of association rules In 3rd Inrl Conf on Knowl edge Discovery and Dora Mining August 1997 361 


  9 operation. In actuality the control of the hardware elements extends even further back to the TSX-5 and STEP heritage busses. These have over a dozen years more on-orbit success for the Air Force. Figure 9 shows several of these heritage satellites   The Glory hardware draws its heritage from the same series of satellites, namely the Defense Systems Inc. \(DSI CTA Space Systems \(CTASS\P bus architecture. TSX5 was an evolution from the STEP heritage, upgrading the electronics for radiation hardness. The TSX-5 mission is flying in a highly elliptical orbit that passes through the Van Allen belts. It has been flying for over 4 years in this environment. ACRIMSAT, OV-3 and VCL made only minor improvements to the TSX-5 designs  The reaction wheels are built by Orbital and are flying on the OV-3 satellite. The propulsion system was assembled and tested by Orbital and is identical to the OV-3 system More than 75% of the bus co mponents were built by Orbital at either the Dulles campus, or previously at the McLean or Germantown facilities. Orbital also manufactured the bus structure elements, including the aluminum honeycomb decks. The same methods were used on the extremely successful ACRIMSAT structure  In terms of the purchased components, most also have flight heritage. The GPS receivers have more than 100 years of combined on-orbit satellite years of operations on the Orbcomm constellation alone. The LN-200S is flying on the TSX-5 mission, and flew briefly on QuikToms \(launch vehicle problem\ RF transmitters and receivers are from L3-Conic and have significant flight heritage on both Orbital satellites and others across the industry. The Orbital-built antennas have extensive flight heritage. The torque rods are the industry standard units from Ithaco. The Solar Array Drive Assemblies SADAs\re the standard Type 2 gimbals from Moog. Orbital has flown these same drives on our geo-synchronous satellites    Figure 9 - ACRIMSAT, STEP-0 and VCL Satellites  The arrays are populated with single junction GaAs cells from Emcore, which was Tecstar when the cells were made and laid down on the panels. These cells are of the same vintage as those flying on the Orbcomm and GALEX satellites. The NiH2 battery is an SPV from Eagle-Picher. It is the same technology, although a smaller capacity, as the batteries flying on the Iridium constellation. The baseline star trackers are the ESA flight heritage Advanced Stellar Compass from the Danish Technical University \(DTU This unit has more than 5 years on orbit with the Oersted satellite, and has flown on more than 8 missions to date with no known failures  The Taurus launch vehicle has had 6 successful flights in various configurations. ACRIMSAT was successfully 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


