Influence and Conditional Influence\222 New Interestingness Measures in Association Rule Mining Guoqing Chen School of Economics and Management Tsinghua University Beijing 100084 China De Liu*, Jiexun Li  Center for Research on E-Commerce, University of Texas at Austin TX 78712 USA  School of Economics and Management, Tsinghua University Beijing 100084 China A&UU This paper diprusses the issues of interestingness in association rule mining Fii a rule is possibhl redundant or misleading even if it possesses high degrees of confidence and support Seeond Bssociation rules do not rekt the ef\200 of negatively influential faetn Such problem are related to confidence deviation In this paper therefore two new measures of 
interestingness namely influence and conditional influence are introduced to represent the ef\200 of the antecedent on the consequent Furthennore the mining algorithm are extended accordingly sueh that certain redundant rules can be eliminated and negatively iaauential des may be dipeovered Kqrwora3 association rule interestingness influence conditional influence 1 INTODUCTION Associon rule mining as one of the important aceas of data mining first introduced by Agrawal et al l and is meant to discover the association between daerent attributes in large databases An association rule reflects the presence of some sets of items given the presence of other items and is of the firm XsY where 
X and Y are two sets of items XnY=0 Usually a rule XsY can be obtained if its degree of support supp\(XaY and degree of confidence Dcox@bY are equal to or greater than the given thresholds minsupp and mincod That is Dsupp\(XsY  where X is a set of items or mterchangeably referred to as itemset in the database T is the number of transactionS that contain X and m is the total number of mnsmions in T In recent years many e have been made on association rule mining in various ways 2-71 It has been noted that the rules discovered upon the above 
tiuesholds may not be all usell or meaninfl Intenzhgness measures as well as domain knowledge are deemed necessary in further filtering the rules so as to discover the knowledge for decision-making 8 Interedngness covers a spectrum of semantics and existing studies have addressed a variety of issues and concerns such as redundancy conflict novelty simplicity, improvement etc 9-111 This paper, however focuses on two issues related to confidence deviation and introduces new interestingnessmeasuresintennsofinfluence In the first place a rule may be regarded redundant even if it has high degrea of confidence and support For instanm consider AaB hn.+80 which meets minsupp and minconf 
If Dsupp\(B is also SO then rule AsB Dco&80 can be regarded as a dundant and unintemthg rule for B is independent IIWI 114  yP and ea=w  llxyll I llXll2 minconf of A Siarly with rule A-B Dconf;80 rule AWB Dco&80?h may not surprise us either On the other hand the rules like A-B Dm+lOO or AaB Dconf=40\222 iook more interesting In many cases such a kind of conliden deviation is more liiely to intrigue us Second association rules do not reflect the effect of negatively influential facts A negatively influential rule reflects the decrease in hnf due to the existence of certain items in the 
antecedent of the rule For example if Dconf\(AWB  DcoflAsB one may think of C to have a negative effed on the degree of conference for rule AbB as Dconfcould be greater without C otherwk Confidence deviation in association rules has been observed and studied for years For instance P-S defined as Dsupp\(XaY  Dsupdx Y is an interestingness measure for rules defined by Shapii in 1991 12 which represents the difFerence between the real number of transactions containing XY in the database and the expected number based on the assumption of independent events Lift Mt\(XqY  hni7,XsY  DsuppO an inmestinpess measure used in Intelligent Minerof IBM  131 also called interest or strength 
in some litem represents the difference between kquences of Y with and without the condition of X A-V Added Value A-V\(X=Y  Dconf\(X~Y  DsupPO is a similar measure to Lift 14 Bayado and others intmdud another interesthgness measure improvement to eliminate the rules that can be substituted for simpler and stronger ones 15 Improvement dehed as impmvement@*Y  min\(VX\222cX Dmr@sY  Dconqx\222~Y is meant to promote the strength for all rules Though dealing with the cofidence deviation the presented Influence measures distinguish themselves from others by representing the em of the antedent as well as of the subset of the items in the 
andent on the consequent of a rule in both positive and negative directionS 2 INFLUENCE AND CONDITIONAL INFLUENCE 2.1 Notions In order to describe the effect of the antecedent on the consequent two new interestingness measures namely influence and conditional influence are introduced as follows Definition 1 Letm  IlX@l,ficylx jio\222\222 where X,Yd and XnY=0 the influence of association rule XsY is defined as Partly supported by 221Tiation\222s Outstanding Young Scientists Funds\224 ofChina No 79925001 the Bilateral Scientific and Technological Cooperation Between China and Flanders A2 and Tsinghua\222s Soft Science Key Project on Commerce 0-7803-7293-X/011$17.00 0 2001 lEEE 1440 2001 IEEE International Fuzzy Systems Conference 


whmMfi-Y can be reganled as the non-mnditional contrast between positive and negative facts whilem?Q"-W as the contmst with condition X The change of the conbast caused by X can reflect the influence of X on Y It can be easily seen that the antecedent lacks association with the consequent when influence equals 0 the antecedent is positively associated with the consequent positive influence\when influence is positive and the antecedent is negatively associated with the consequent \(negative influence when influence is negative Definition 2 On the condition of X conditional influence of itemset Z on itemset Y is defined as InIuenceG YJX  InJuence\(xz r r  lnJuence\(xz r r Lkonf\(X2 r 3 U  log Dconffl a r X 3 Definition 2 states that 1 conditional influence of Z on Y is the difference between influence of XZ on Y and that of X on Y 2 hm conditional influence it is clear and intuitive to see that influence can reflect confidence deviatiow and 3 especially when X=0 given Il0ll ITl conditional influence degenerates to influence That is to say, influence is a special form of conditional influence Similar to influence Z has positive influence on Y with condition X when influence\(Z,qw  0 Z has negative influence on Y with condition X when influence\(z;y1X  0 and Z has no influence on Y with condition X when influence\(Z,qX  0 Notably based on conditional influence which measures the influence of an item or itemset in a rule, decision-makers will be able to identi& items with positive, negative and no influence, and then screen out interesting des 2.2 Properties of Influence Property 1 Negathe Symmetry the influence of the antecedent on the negation of the consequent is the negation of the influence of the antecedent on the consequent i.e Intluen*3~-Muen**Y Proof Influence Infuenceps7u  log f\(ylx y I m  log f\(~v\(x y I x f\(V lfr\(-Y f\(-U 1 f\(Y  logl 0 U Property 2 The influence of X on Y changes in accordance with IWI IlXll IMI or 1'11 in the following manner 1 With others unchanged influence haeases when llxyll in 2 With others unchanged influence decreases when 3 With others unchanged influence decreases when yl 4 With others unchanged influence increases when I'll imxeases Proof increases increases By definition we have Apparently IlXYll t Mwce t  IN1 t Influence 4  llyll t a~uence I  11 t Muen t  Wl llxll IMI llxrll all change in the same proportion Muence keeps unchanged U Furthermore it can be observed that influence is a non-linear function of Dcod In Fig 1 the thick curve and the thin one represent respectively, the change on condition of DsuppO  0.5 and that on condition of DsuppO  0.3 The change of influence is non-hear with the DcomaY When influence is positive the higher the confidence the the influence incp on the contrary when influence is negative the lower the confidence the faster the influence decreases Especially when coddence reaches 1 i.e the rule becomes a positive logic rule, influence reaches k when contldence reaches 0 i.e the rule becomes a negative logic rule, influence reaches  This change is easy to understand in that an inmaw of fhquency hm 80 to 90 is often considemi more interedng than that hm 50 to 60 for the former is more seldom encounted and therefore more significant to decision makers From the shape of the curves we can see that the curve of Dsupp\(X=3W.5 is symmetric with gard to DcOnqX~Y which means that if Y is distributed in the database with frequency 500/4 with condition X the change fhm 50 to 40 and that hm 50 to 60 are to the same extent but in the opposite direction Furthermore it can be pmved that the difkrence of influence between the curve of Dsupp\(x==Y  0.5 and that of Dsupfl=Y is ked In this case the difkence is 0 3/\(1-0.3 0.510-0.5 1og 2 1 I 2. -2 5 t Dconf X=>Y Fig 1 Change of Influence with Dco@aY 1441 


2.3 Influence and Other Measures Muence and Conditional Influence reflect a very different semantics from other inkzestingness measures While Dsupp and Damf respectively, reflect the significance and strength of itemsets in the database influence describes the deviation of confidence An association rule with high Dsupp and high hnf may not possess high influence Like P-S lift etc influence can reflect the association between the antecedent and the consequent of a rule If thm does not exist any association then the rule is considered to be of no interest But an advantage of conditional\influence is that it can measure the influence of not only the antecedent but also the subsets of the items in the antecedent on the consequent Furthem conditional influence is advantageous over improvement Firsf impmvement is a linear hction of confidence while influence is nonlinear which can reflect different effects of confidence deviation from di&mt original values e.g a change in Dconf fiam 0.5 to 0.6 may be viewed differently than that hm 0.8 to 0.9 NeqMfl7Y rather thanJ?o is used in influence so as  to reflect symmetry 3 ASSOCIATION RULE MINING BASED ON INFLUENCE 3.1 Influence in Association Rule Mining Consider a rule with three measures in form of X*Y support confidence influence Without loss of generality positive influence is discussed in the mining process That is given the thresholds minsupp minconf and inflevel iuence level a rule will be obtained with its Dsupp 2 minsupp and hnf2 mincoe and will be regarded interesting with its Muence 1 inflevel. Correspondingly, conventional algorithms can be extended by incoprating the influence measures into the mining process as shown in algorithm 1 Algorithm 1 forall large k-itemsets lb k22 do begin consequent  end Hi={consequents of rules fiom Ik with one item in the call ap-gemles\(lbH procedure ap-gemles\(lk large k-itemset H set of m-item consequents if&>m+l\then begin H,,,+i=apriori-gen\(H forall h,,,+l E H,+i do begin DconeDsupp I@hUpp Ik-h,+i influence=logwconP 1 Dsupp\(hmcl  1  iwconf 2 minconf  influence 2 inflevel then output rule Ik-h,+J  hi+i with support=Dsupp\(lk confidence=Dconfi D~nf h else if \(Dconf minconf delete h,,,+l fiom H,+I end call ap-gemles\(lk H,+i en 3.2 Conditional Influence in Association Rule Mining Based upon conditional influence a rule with a low influential subset of the antedent is regarded unintehg and will be filtered out More concretely given a threshold cinflevel conditional influence level the de XaY whose itemsets of the antecedent are of suificient influence will be regarded as interesting where R is the rule set Sice Dmnf\(=Y  DsuppolyDsupP\(0 Dsupp\(Y and if S  X then intluence\(S,\\rlX  influence&Y  influence\(X3Y This means that Influence is a special case of Conditional Muence Furthenno the mining algorithm can be extended in an analogous manner to that with Influence It is worth mentioning that conditional influence enables us to hd negatively influential rules which flect the absence of some itemsets given the presence of other items The rule XsY whose itemsets of the antecedent are of sufficient negative influence will be regarded as interesting mm inJuence\(S YIX  s I cinfevel I o SCXY-S*YER where R is the rule set 4 EMPLE AND PRELIMINARY EXPERIMENTS 4.1 An Example Consider a transaction dataset as shown in Table 1 On one hand given minsupp=25 and minconHO0/0 eighteen association rules can be discovered hm the dataset Further if inflevel4.1 is imposed eleven rules such as pBaD Dsupp=42.860/4 DcmF60.00?4 influence=;-O.22 WD Dsupp=42.860/6 h1+75.00?/0 influend.OS etc are eliminated. Furthermore given cinflevel=O.l another two rules AB-E Dsupp=28.57 Dcot+lOo.OO min\(influence\(S,\\rlX-S and hD Ds~pp-42.86Y Doot+lOo.W/o min\(infl~ence\(S,\\rlX-S are filtered out if DconflJ=Y where X'cX conditional influence is 0 On the other hand with threshold cinflevel minimal negative influence  0.15 we can disoover four negatively influential rules such as PaD Dsupp=42.86 DConf-60.00?/0 max\(iiuence\(S,\\rlX-S  0.221 etc Table 1 Dataset TID I Item Set I IADE I ACDE 1 1442 


Experiment 1 Experiment 2 Experiment 3 1000 10000 100000 number of transaction 0.01 0.03 0.05 high middle low density of data A inflevel  cinflevel Fig 2 Experiment Results 4.2 Preliminary Experiments Preliminary experiments have been carried out to examine the measures in relation to the degree of support density of data and number of transactions see Fig 2 The results have revealed that cinflevel can reduce the number of rules rernarkabl that the denser the data the higher the percentage of the eliminated rules and that the computational complexity of conditional influence does not increase as the number of transactions increases 5 CONCLUDINGREMARKS In order to describe confidence deviation in association des two interestingness measures namely influence and conditional influence have been introduced in this paper Respectively they can desaibe the influence of the antecedent as well as subsets of the items in the antecedent on the consequent of a rule By incorporating the two measures in the mining process unintemting rules can be eliminated so as to avoid redundant and less influential rules Moreover influence can be repFesented in a non-hear manner REFERENCES El Rakesh Agrawal Tomasz Imielinski Arun Swami Mining Association Rules between Sets of Items in Large Databases In Proc of the ACM-SIGMOD 1993 Int\222l Conference on Management of Data, Washington D.C May 1993,207-216 2 Agrawal R Srikant Fast Algorithms for Mining Association Rules In Proc of the 20th Int\222l Conference on Very Large Databases, Santiago Chile, Sept. 1994 Expanded version available as IBM Research Report FU9839 June 1994 3 R Srikant R Agrawal Mining Quantitative Association Rules in Large Relational Tables SIGMOD\22296 6/96 Montreal, Canada, 1996 4 Guoqing Chen Qiang Wei Etienne E Kerre Fwy Data Mining Discovery of Fuzzy Generalized Association Rules in Recent Research Issues on Management of Fuzziness in Databases in the Physica Verlaf Series \223Studies in Fuzziness and Soft Computing\224, Springer-Verlag 200 1 5 Qiang Wei Guoqing Chen Mining Generalized Association Rules with Fuzzy Taxonomic Structures in 18th Int\222l Conf ofNAFIPS New York, June 1999 6 De Liu Guoqing Chen Association Rule Mining Based on a Simple Rule Set 16th FIP WCC2000,2000 7 M J Zaki S Parthasarathy M Ogihara W Li New Algorithms for Fast Discovery of Association Rules This work was supported in part by an NSF Research Initiation Award CCR-9409120 and ARPA contract F19628-94-C-0057\America Association for Artificial Intelligence 1999 8 Rakesh Agrawal Invited talk at the 5th ACM SIGKDD Int\222l Conference on Knowledge Discovery and Data Mining \(KDD-99 San Diego California 1999 9 Liu B Hsu W Chen S Using Generalimpressions to Analyze Discovered Classification Rules In Proc Of the 3rd International Conference on Knowledge Discovery and Data Mining 1997.3 1-36  101 Piateski-Shapiro G Matheeus C.J The Interestingness of Deviations In Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases 1994 25-36  1 13 Pedro Gago Carlos Bento A metric for Selection of the Most Promising Rules In Procs of PKDD-98 1998. 19 27  121 Piatesky-Shapiro G Discovety Analysis, and Presentation of Strong Rules In Chapter 13 of Knowledge Discovery in Databases AAAVMIT Press 1991  131 International Business Machines IBM Intelligent Miner User\222s Guide, Version 1 Release 1,1996  Sigal Sahar and Yishay Mansour An Empirical Evaluation of Interest-Level Criteria DW  KD Theory Tools and Technology 1999 15 Roberto J.Bayardo Jr Rakesh Agrawal Mining the Most Interesting Rules In Proc of the Fifth ACM SIGKDD Int\222l Conf on Knowledge Discovery and Data Mining, 1999.145-1 54 1443 


TimeSleuth into an unsupervised tool, as the user simply has to run the program with minimal instructions as to the target attribute and the values it can have There are two time windows in the classification panel The first one, called "Flattening Time Window" is as the name implies. Its value is used to flatten the input data for both c4.5 and c4.5rules. This value is then provided to c4.5rules so that it can sort the output temporally However, this value is not provided to the tree generation program, c4.5. So the decision tree is generated with no regard to any time window. The value "c4.5Time Window" is meant to affect the decision tree, as explained in  Thi s val u e, i f  di ffe rent t h a n 1  s h oul d be the same as the Flattening time window. The rules generated by c45rules have a confidence level. In order to filter the rules, a user can specify a minimum confidence level, and only rules will higher confidence values will be presented Finding a suitable window size for the data under investigation can be a challenge. For this reason TimeSleuth allows the user to run it in a batch mode wherein it tries consecutive values for the time window Training and testing accuracy can be employed to guide the search. As shown in Figure 7, the user can decide to explore all values, or stop after the accuracy has reached a threshold, or after the accuracy has stopped improving The running time is determined by C4.5's speed, which in our experiments has been good, even for fairly large values of the window size Figure 7. Exploring different window sizes The user can perform the acausality test by choosing the backward flow of time. If the results are better than in the forward flow, then the relationship is acausal user interface provides help in making sense of the rules and relations among different attributes in the rules. Using the Analysis panel the user can instruct c4.5rules to output data necessary for tabular display of rules and the relevant statistics. This changes C4.5's normal output from text-based to tabular In the analysis panel, the user can see how the selected time window has affected the resulting rules. As shown in Figure 8, the user can opt to see the rules laid out according to the time step s in which the attributes appear. This display shows how important each attribute has been in forming the rules Figure 8.  Temporal layout of rules In Figure 9, many of the condition attributes used to determine the value of the soil temperature originalDecision\ come from previous time steps. In other words, the current temperature of the soil depends on attributes measured prev iously. Using standard C4.5 with such data obviously would not be as revealing. As seen in Figure 9, the previous value of soil temperature appears in 84.3% of the rules, which supports the common sense guess that the current temperature is determined mostly by the corresponding observation an hour ago Figure 9.  Statistics about the attributes In Figure 10, TimeSleuth shows the frequency of attribute usage in rules that were actually fired. In other words, the more a rule has been used \(on test data or on training data\e more important those attributes will be.  Both training and testing results are displayed. The two values are separated by Figure 10.  Frequency of attribute usage in rules Figure 11 shows some addition information about the rules and how they were used. The column headers are self-explanatory Proceedings of the 14th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI\22202 1082-3409/02 $17.00 \251 2002 IEEE 


Figure 11. Rule usage and other related data In TimeSleuth, to determine the quality of a set of rules and the strength of the relations among the attributes that is implied by the rules, we use the training or testing accuracy of the set of rule 4. Concluding Remarks We introduced TimeSleuth an unsupervised, learning tool based on C4.5 that is targeted for discovering causal and temporal relations. We saw that discovering causal relationships using associations is the norm in the literature. TimeSleuth does the same, but instead of using statistical methods it employs the common sense notion of temporal order to go from association relationships to temporal ones. The option to set the flow of time backward or forward allows the user to determine the causality or acausality of the ru les. Often the exact nature of a temporal relationship is not known. TimeSleuth helps the user to experiment with different scenarios and see what kind of rules are discovered when different time window values are used. The tabular output of information about the rules and the attributes helps the user assimilate the discovered relations TimeSleuth helps the user to analyze the rules produced by C4.5 by showing how important each attribute is at different times in determining the value of the decision attribute. This ability makes TimeSleuth a data mining tool \(with output suitable for a domain expert\as well as a machine learning tool \(with output consisting of ready-to-execute rules TimeSleuth is freely ava ilable for download from http://www.cs.uregina.ca/~kar imi/downloads.html or by contacting the authors. The package includes on-line help, example files \(including the weather data used in figures in this paper\d the patch files for C4.5. Also available on the same web address are c4.5.exe and c4.5rules.exe, patched and compiled for Microsoft Windows 95 and up References   Berndt D  J. and Clifford J  Finding Patterns in Time Series: A Dynamic Programming Approach Advances in Knowledge Discovery and Data Mining U.M. Fayyad, G Piatetsky-Shapiro, P. Smyth et al. \(eds.\, AAAI Press/ MIT Press, pp. 229-248, 1996  Bowes, J., Neufeld   E., Gr eer, J. E. and Cooke, J., A Comparison of Association Rule Discovery and Bayesian Network Causal Inferenc e Algorithms to Discover Relationships in Discrete Data Proceedings of the Thirteenth Canadian Artificial Intelligence Conference \(AI'2000  Montreal, Canada, 2000, pp. 326-336  Freedman, D. and Humphrey s P Are There Algorithms that Discover Causal Structure Technical Report 514 Department of Statistics, University of California at Berkeley 1998  Karimi, K a nd Hamilton, H.J Finding Temporal Relations: Causal Bayesi an Networks vs. C4.5 The Twelfth International Symposium on Me thodologies for Intelligent Systems \(ISMIS'2000 Charlotte, NC, October 2000  K  and Ha milton, H.J., Learning With C4.5 in a Situation Calculus Domain The Twentieth SGES International Conference on Knowledge Based Systems and Applied Artificial Intelligence \(ES2000 Cambridge, UK, December 2000  Karimi, K and Hamilton, H.J., Di scovering Temporal Rules from Temporally Ordered Data The Third International Conference on Intelligent Data Engineering and Automated Learning \(IDEAL 2002 Manchester, UK, August 2002, pp 25-30  Karimi, K and Hamilton, H.J., RFCT An AssociationBased Causality Miner The Fifteenth Canadian Conference on Artificial Intelligence \(AI'2002 Calgary, Alberta, Canada May 2002  Keogh, E. J. and Pazzani M. J., Scaling up Dynamic Time Warping for Data Mining Applications The Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining \(KDD 2000 August 2000, pp. 285-289  Korb, K. B and W a ll ac e C S   In S e arch of P h ilos oph er s Stone: Remarks on Humphreys and Freedman's Critique of Causal Discovery British Journal of the Philosophy of Science 48 pp. 543-553, 1997  Mannila, H., To ivonen   H. and Verkamo, A. I Discovering Frequent Episodes in Sequences Proceedings of the First International Confer ence on Knowledge Discovery and Data Mining pp. 210-215, 1995  Nadel, B  A., Constr ai nt Satisfaction Algorithms Computational Intelligence 5, pp. 188-224, 1989  Pearl J   Causality: Models, Reasoning, and Inference  Cambridge University Press. 2000  Quinlan, J  R C4.5: Programs for Machine Learning  Morgan Kaufmann, 1993  Scheines, R., Spirtes, P., Gly m our, C. and  Meek  C   Tetrad II: Tools for Causal Modeling Lawrence Erlbaum Associates, Hillsdale, NJ, 1994  Silverstein  C B r in S Motwani  R  and Ul lm an J  Scalable Techniques for Mining Causal Structures Proceedings of the 24 th VLDB Conference pp. 594-605, New York, 1998  Spirtes, P. and Sch e in es, R  Reply  to Freedman   In McKim, V. and Turner, S. \(editors Causality in Crisis  University of Notre Dame Press, pp. 163-176, 1997 Proceedings of the 14th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI\22202 1082-3409/02 $17.00 \251 2002 IEEE 


                


                            


                 


  10 launched on the same 2110 Taurus vehicle configuration in 1999 with the KOMPSAT satellite  Orbital has flown numerous missions from the Mission Operations Center \(MOC\lles using the MAESTRO ground software for command and telemetry. The MOC is shown below in Figure 10. This software is hosted on Sun workstations running the Solaris operating system. The most recent missions are OV-3 and GALEX. Both programs are running smoothly and operating as planned. ACRIMSAT has recently converted over to using the MAESTRO system for operations as well. USN is supporting the GALEX mission, including X-band pass services at Hawaii and Australia. Glory will follow this proven path. USN has also supported most of Orbital’s geo-synchronous satellite launches and on orbit checkouts    Figure 10 – Orbital’s Mission Operations Center  6  S UMMARY   In the case of the Glory program, it is concluded that reuse of many components from the VCL program not only significantly reduces the cost of the Glory mission but helps to make something of what could be shelved for an indeterminate amount of time.  By employing existing resources and knowledge from a program that was very mature, the Glory program is doing its part to contribute to cost saving measures and maximizing benefit to NASA. The Glory program provides savings of nearly $15M over a ground-up bus design effort, while mitigating significant risk on the bus portion of the mission  Glory is an ideal mission for ear th sciences in the realm of climate change research. It draws from significant flight heritage elements in both th e instruments and the spacecraft bus. It uses an existing NASA asset with minor refurbishment to save significant development costs. It uses a LV that has flown several NASA and DOD missions to date. The ground system employs flight proven hardware and software with a robust plan for backup coverage   7  R EFERENCES   NASA Facts Aerosols June 1999   NASA Godda rd Space Flight Center Glory Program Science and Mission Requirements Document  Greenbelt, Maryland, February 2, 2004  8  B IOGRAPHIES   Darcie A. Durham is an Engineer in the Advanced Programs Group at Orbital Sciences Corporation in Dulles, Virginia.  Currently she is supporting the Glory program in the Systems Engineering Division as well as supporting the Concept Exploration and Refinement Program.  Her experience is both on hardware and paper study programs ranging from Crew Preference Items for Lockheed Martin to Crew Exploration Vehicle Design for Orbital Sciences Corporation.  She graduated with a B.S. in Aerospace Engineering from Texas A&M University.  Past experience includes flight certification of ISS hardware at Johnson Space Center in Houston and work with the Advanced Missions Architecture Group at the Jet Propulsion Laboratory in Pasadena, California   Thomas J. Itchkawich is a Program Director working for the VP of Science and Technology Programs in the Space Systems Group at Orbital Sciences Corporation in Dulles Virginia. Tom is currently managing the operations support for ACRIMSAT managing the Glory program at Orbital, and managing an internal ERP conversion in his spare time. He has successfully launched the Mighty Sat I satellite on the Shuttle Endeavour, and the ACRIMSAT satellite on Taurus. He has supported numerous other programs at both CTA Space Systems and Orbital Sciences. In a previous incarnation he was the lead engineer for the Pegasus payload fairing from inception through design, fabrication, qualification testing and the first four flights. A true Fighting Blue Hen, Tom graduated with a Bachelor’s of Mechanical Engineering from the University of Delaware. The first ten years of his career were spent at Hercules Aerospace working on composite rocket motor cases, including Titan IVB, Delta GEM strap-ons, and Filament Wound Case Shuttle boosters. Tom was part of the Pegasus Team that was awarded the National Medal of Technology, and the MightySat team that was awarded the Program of the Quarter by AFRL. He has several other published papers on composite structures and low-cost satellite missions  


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


