An Association Rule Algorithm Generated By Minimal Head-Item Set Based On Set-Enumeration Tree  Qing Wei School of Computer and Information Engineering Henan University of Finance and Economics HNUFE Zhengzhou City, China weiqing.hnufe@gmail.com Li Ma Department of Information Technology Engineering Yellow River Conservancy Technical Institute Kaifeng City, China Songyang Ding School of Computer and Information Engineering Henan University of Finance and Economics HNUFE Zhengzhou City, China 
Huichao Mi School of Computer and Information Engineering Henan University of Finance and Economics HNUFE Zhengzhou City, China 
204A lot of association rules may be generated in the process of association rules mining  
and much time may be wasted when a user analyzes the association rules. To solve this problem, an improved algorithm which based on the setenumeration tree for mining the association rules is introduced and the algorithm reduces the number of association rules remarkably and generates a subset of association rules without any information loss. The number of association rules is 
Abstract 
decreased very much, so the space consumption in rules\220 storage will be declined greatly, and the efficiency of analysis for association rules will be enhanced  
002 
Keywords- frequent itemset ; association rule ; head-item set set-enumeration ; key rule 
I I NTRODUCTION Association rule is a very important branch in the area of data mining. The mining of association rule is implemented by two steps, the first one is mining the frequent items; the second one is generating association rules that meet the condition of min-confidence \(mc\[1-3  Th e f i rs t s t ep is u s u ally th e c o re  step in the mining process of association rules, and the efficiency of data mining is affected mainly by finding of 
frequent item In experience, there are a great number of association rules that generated via frequent items. For instance, there are more than 10 thousands of association rules results in one mining case on medical treatment, therefore, users have to spend a great deal of time on analyzing so many rules if they want to analyze an association relation on an illnesses case,  that means, it waste not only  time but also space. Actually, it is not necessary to generate all the rules, on the contrary, it is enough for us to generate some key rules only, which contain all the others information, that is to say, those key association rules are able to deduce all rules without any data loss    
Supported by the Fund of Program for Tackling Key Problems in Science and Technology of the Department of Science and Technology of Henan Government \(Grant No. 072102210066 GRSET is a kind of algorithm based on set-enumeration its function is to generate all the association rules which based on one frequent item, and this method generates large numbers of association rules. For example, in table 1, the item I3I2I8I5 is frequent item because it meets the condition of minconfidence \(mc\ and min-support \(ms\. And according to GRSET algorithm, only one frequent item generates 10 association rules, they are I5 
I3I2 I8I5 
I3I2I8 
I2I5 I3I8 I3I5 I2I8 I5 
a head-item, and the string after the \215 
I2I8 
I3 But in fact, it is a huge waste to generate so many rules Each expression is a rule, and here, we call the string before the \215 I8I5 we call I3, I2 the head-item of the rule 'I3I2 
I8I5 I3I2 
a tailitem. For example, in the second line of the set, 'I3I2 
I3I5 I3I2I8 I8 I3I8I5 I2 I2I8I5 
I3I2I5 
I3I2 I8I5 
I3I5}                        \(2 For instance, the rule 'I5 
I2I8I5 
I5I8', and I5, I8 the tail-item of the rule 'I3I2 
I2I8 
I2I5 I3I8 I3I2I8 
I2I8 I8I5 I3I2 I3I2I8 I5 I3I2I5 I8 I3I8I5 I2 
I5I8'.[4 Th e r ef or e th e w hole  set I3I5 
I3}                                 \(1 can be deduced by a set only includes 3 rules via a tail-to-head method , the set is I5 
I2' can be obtained via moving I3,I8. So, it is not difficult to find that all the rules in \(1\ can be deduced by its subset \(2 That indicates that no information will be lost in the moving process, and any rule in \(1\ is implied in the subset key rules set GSHS \(Generate Subset of the Head-item Set\algorithm introduced in this paper is to reduce the number of association rules without any data loss, so as to save more storage consumption and time for analysis 
I3I2I8',  'I5I3 I2I8' can be obtained via moving I3 to head; and 'I3I8I5 
978-1-4244-4994-1/09/$25.00 \2512009 IEEE 


II C ONCEPTS AND THEOREMS Some new concepts, theorems and symbols are introduced here to make it easy to explain the new algorithm, besides, we also inherit some concepts in GRSET algorithm in [1    Definition 1 The support of itemset A is denoted as sup \(a which means the number of transactions that include the itemset. Take quasi association rule [4 l-c\ as an example, l is a frequent itemset, and c is the subset of l Definition 2 If quasi association rules c 1-c\eets the condition of the min-confidence Theorem 1 If itemset C is the subset of itemset D, then sup\(c sup\(d Theorem 2 If quasi association rule c l-c\eets the condition of min-confidence \(mc\, then all the quasi association rules that head-item include C and be the subset of L meets the condition of mc Proof Let c be a proper subset d, and d is the subset of l, if c l-c\ is true, then association rule d l-d is true c l-c is true sup\(l\/sup\(c mc, and because c is the proper subset of d sup c sup d  sup\(l\/sup\(d mc quasi association rule d l-d is true too. QED Therefore, by the theorem 2, given a frequent itemset l, if one of its proper subset c, as a head-item, meets the condition of association rule c l-c\hen for any itemset d, which is the subset of l and includes c, as a head-item, meets the condition of association rule d l-d By the property of Theorem 1 and 2, given an association rule, if move some tail-items to head-items, then the new quasi association rules meet the condition of mc. For example, let abcdeŽ be a frequent itemset, if ab cde meets the condition of mc, and move the tail-item c to head, then the new association rule abc de meet the condition of mc as well Given a frequent itemset l, x denotes the head-item of quasi association rules that generated by l, if quasi association rule x l-x\is tenable, then sup\(l\/sup\(x mc, namely  sup\(x sup\(l\/mc. According to the analysis above, the problem to find the head-item that makes the association rules which generated by frequent itemset l tenable, in fact, is the problem to find the subset of item set sub-item set for short whose support is less than or equal to sup\(l\/mc according to theorem 1 and 2 Therefore, in the process of finding the sub-itemset, if the support of a certain sub-itemset less than or equal to sup l\/mc, then it is not necessary to judge whether the support of the superset of sub-itemset meets the condition. However, if the support of the sub-itemset is bigger than sup \(l\mc, it is still necessary to judge the support of the superset of the subitemset by depth first algorithm Example 1 in table 1, I3I2I8I5 is a frequent itemset, and its support is 9, because mc=80%, the condition meet x I3I2I5I8-x is just sup\(x sup\(l\c, and sup\(l\c 9/80% = 11.5. So sup\(x 11.5 is necessary and sufficient condition in this case The GSHS algorithm in this paper is based on the theorem 1 and 2, and its main idea is: firstly, generates the subset of the head-item set that makes quasi association rules that generated by frequent itemset l tenable; and secondly, adds the subset of the head-item set to the Subset of the Head-item Set \(SHS III G ENERATE S UBSET OF THE H EAD ITEM S ET A LGORITHM GSHS is acronyms of Generate Subset of the Head-item Set, and it is a recursive algorithm for the function of generating a minimal expression-a subset of the head-itemset Algorithm 1 GSHS Input: frequent itemset l, min-confidence mc Output: Subset of the Head-item Set \(SHS\of association rules Method 1 m=||l 2 SHS   3 get SHS    procedure getSHS\(h s Parameters H: substring of l, the quasi association rule with the headitem h, and the rule meets the condition of mc S: the beginning location to be connected in l, and p \(l h[||h s m Method 1 n=||h 2  if n < m-1 3  if sup \(h+l[s  sup \(l\c 4  if \(! \(Superset Check \(SHS, h+l[s  th en  5  SHS=SHS  h+l[s  6  else SHS=SHS  SuperSet of h+l[s h+l[s   7  else 8  j=s 9  h=h+l[s  10  do 11  j=j+1 12  while sup \(h+l[j l   m c  13  if \(! \(Superset Check \(SHS, h+l[j 14  SHS=SHS  h+l[s  


15  else SHS=SHS  SuperSet of h+l[s   h+l  s   16  Procedure supsetcheck\(SHS, a Parameters SHS: Subset of the Head-item Set A: an itemset found just recently Method 1 If   Superset \(a SHS 2 3 Sus=Superset \(a 4 Prune \(sus\ according to Theorem 1, 2 5 Return 1 6  7\ Else 8 Return 0 Explanation: In the line 4,5,6,13,14,15 of algorithm GSHS the Superset Check\(SHS, a\ is run because if the sub-itemset in a certain recursive procedure meets the condition of sup\(x sup\(l\/mc, it needs to judge whether SHS has superset, if it is true, then the superset will be pruned according to Theorem 1 and 2, and at the same time, adds the new head-item to the set SHS, and a new set SHS will be obtained IV E XAMPLE AND A NALYSIS Example 2 Given a data set, table 1 Let ms=50 mc=80%. Sup \(I3I2I8I5 Question: output the minimal expression of association rules generated by frequent item sets f=I3I2I8I5 The mining process is as follows TABLE I T RANSACTIONAL D ATA S ET TID Item  T1 I1I2I3I4I5I7I8 T2 I2I3I5I6I7I8 T3 I3I4I6 T4 I1I2I3I4I5I6I7I8 T5 I1I2I3I5I6I7I8 T6 I1I3I4I5I7I8 T7 I2I3I4I6I7I8 T8 I1I2I3I5I6I8 T9 I1I2I3I4I5I6I7I8 T10 I3I5I7I8 T11 I1I2I4I6 T12 I1I2I4I6 T13 I3I4I7 T14 I1I2I3I4I5I6I7I8 T15 I2I3I4I5I6I7I8 T16 I2I3I5I6I8 1 m=4 2 SHS  3 Sup\(f\mc=11 4 Sup\(I3  11 5 Sup\(I3I2\0  11 SHS= SHS  I3I2}={I3,I2 6 Sup\(I3I8\2  11 7 Sup\(I3I8I5 11 SHS= SHS   I3I8I5}={I3I2,I3I8I5 8 Sup\(I3I5\1 11  SHS= SHS  I3I5}{I3I8I5}={I3I2,I3I5 9 Sup\(I2  11 10\Sup\(I2I8\0 11 SHS= SHS   I2I8}={I3I2,I3I5,I2I8 11\Sup\(I2I5  11 SHS= SHS   I2I5}={I3I2,I3I5,I2I8,I2I5 12 Sup\(I8  11 13 Sup\(I8I5\1 11 SHS= SHS   I8I5}={I3I2,I3I5,I2I8,I2I5,I8I5 14 Sup\(I5 11 15 SHS=SHS  I5}-{I3I5,I2I5,I8I5} ={I3I2,I2I8,I5 16 According to GSHS, the subset {I3I2,I2I8,I5} of head-item set of association rule can be generated via the frequent itemset I3I2I8I5, and the association rules with the head-items in {I3I2,I2I8,I5} are I3I2 I8I5 I2I8 I3I5 I5 I3I2I8 Algorithm analysis In example 2, in regard to the frequent itemset {I3I2I8I5 if by GRSET algorithm, then all the 10 association rules should be generated, namely I5 I3I2I8 I3I2 I5I8 I3I5 I2I8 I2I8 I3I5 I2I5 I3I8 I8I5 I3I2 I3I2I8 I5 I3I2I5 I8 I3I8I5 I2 I2I8I5 I3 So as to the whole database, the number of association rules will be huge, and the consumption of time, space and power will be enormous However, the GSHS algorithm here just generates 3 rules with head-item in set {I3I2 I2I8 I5}, the rules are I3I2 I8I5 I2I8 I3I5 I5 I3I2I8 and other 7 rules generated by I3I2I8I5 in GRSET can be obtained by moving tail-items to head-items via the 3 rules above. For example, 2 rules {I3I2I8 I5 I3I2I5 I8} can be obtained by tail-to-head method based on rule I3I2 I8I5; and I2I8I3 I5, I2I8I5 I3} can be deduced by I2I8 I3I5; and rules I5I3 I2I8 I5I2 I3I8 I5I8 I3I2 I5I3I2 I8 I5I3I8 I2 I5I2I8 I3} can also be deduced by I5 I3I2I8 So it is not difficult to find that the 3 key rules I3I2 I8I5 I2I8 I3I5 I5 I3I2I8} implies totally all other 7 rules without any data loss A Hasse diagram, Fig.1, is able to indicate the structure of Boolean association rules set at generated by the frequent itemset I3I2I8I5, the first line in brackets indicates the head-items of association rules, and the second line in brackets means the tail-items of association rules. It is not 


Figure 1 Hasse diagram difficult to get a conclusion that the association rules with head-items {I3I2 I2I8 I5} constitute the maximal element in Fig.1 V PERFORMANCE ANALYSIS The performance of the GSHS algorithm was tested comparing with the GREST and Apriori algorithm with the same data set. The algorithm is implemented by Java, and the running environment mainly includes: a CPU of P4 2.0GHz storage memory of 1.0G and the Windows XP system. We have tested the number of rules that generated via GRSET algorithm and GSHS algorithm respectively, the data set used in this case are shown in table 1 Let ms=50% ,and mc=80%, then Table1 and Fig.2 show the number of rules in GRSET and GSHS algorithm for the data in Table 1 As it shown from the experimental results in Table 2 and Fig.2, for the same dataset and the minimum support threshold, the number of the key rules generated by GSHS TABLE II N UMBER OF A SSOCIATION R ULES Apriori GRSET GSHS  Number of Rules 258 140 86          Figure 2 Number of Association Rules algorithm are much less than the rules that generated by GRSET algorithm, so the storage usage are decreased than that of GRSET, the GSHS algorithm almost saves 40 storage consumption compared with the GRSET, and saves 66% space to Apriori. Therefore, the experiment indicates that the GSHS algorithm is more effective in saving storage consumption than the GRSET and Apriori algorithm VI C ONLUSIONS In this paper, a new algorithm named GSHS is introduced it only generates some key rules of association rules comparing with GRSET algorithm. It is shown on theory and experiments that GSHS has three advantages. Firstly, the key rules generated by GSHS are much less than that generated by GRSET, it can save more space consumption for user Secondly, the subset built by key rules is a maximal element of all association rules which generated by the same frequent itemset, so it implies all the rules. Thirdly, the key rules can deduce all the association rules base on the same frequent itemset without any information loss Because the property of frequent closed itemset is helpful for reducing the number of redundant rules in the process of generating the association rules, the studies on generating association rules based on frequent closed itemset will be carried out A CKNOWLEDGMENT The authors are grateful to the referee for their valuable comments and suggestions. This work has been supported by the Supported by the Fund of Program for Tackling Key Problems in Science and Technology of the Department of Science and Technology of Henan Government. \(Grant No 072102210066 R EFERENCES 1 R A g ra w a l M a n n i l a H , Srik an t  R  et  a l F a s t di s c over y ru les  A    Fayyad U. Advances in Knowledge Discovery and Data Mining  Menlo Park: AAA I Press , 1996 307-328 2 C e r co ne V T s u c h i y a M L u e s y  e d ito r  s i n tr o d u c t i o n   J   I EEE Transaction on Knowledge and Data Engineering , 1993 , 5\(6\ : 901902 3 B r i n S  Mo tw an i R U l l m an J  e t al D y nam ic i t e m se t co u n t i n g a n d  implication rules for market basket data[EB/OL ht tp  ci t e s eer  njnec.com/ brin97dynamic html, 1999-05-23/2002-05-09 4 K u n W u  Ba oq in g J i a n g, Qin g W e i  A Dep t h F i r s t A l go ri t h m of F i nd in g All Association Rules Generated By A Frequent Itemset. 2006 International Conference on Intelligent Systems And Knowledge Engineering, April 6-7,2006, Shanghai, China 5 J i a n g  B a oqi n g L i J i a n  X u Yan g A St r u c t u r e of B oolea n A s s o c i a t i on Rules Set \(in Chinese\. Natrual Science Edition, Journals of Henan University. 2006 36\(1\-90 6 P as q uie r N   Bas t i d e Y  T a o u il R  e t al D i s c o v e r i ng f r e que nt cl o s e d  itmesets for association rules [EB/OL  ht tp  ci te se e r  N j ne c.co m    pasquier99 discovering html, 1999-11-23/2002-04-18 7 J W  H a n a nd M K a m b e r D a ta m i n i ng co nce p t s a nd te ch n i q u e s F i r s t edition, Beijing: Higher Education Press,2001 8 X F   L i and J  L i D a ta m i n i ng  an d kno w l e dge dis co v e ry  i n Ch ine s e    Fist Edition, Beijing: Higher Education Press, 2003 9 Y J  Y a n, Z  J L i a n d H  W C h e n F r e que nt i t e m s e ts m i n i ng  algorithms \(in Chinese\, Computer Science, 2004, 112-114 


TABLE V  E XAMPLES OF ASSOCIATION RULES         The municipality performance is not proper in handling the complaints which refer to 'asphalt settlement', 'asphalt layer', 'garbage collection' and 'installation of garbage ban subjects The probability of being satisfied is 70% in the last month of winter The unit 82 which refers to the section 2 of region 1 has the best performance in the third month of winter in comparison with the other months in handling the 'garbage collection subject The performance of municipality in handling the complaints of 'installation of garbage ban' subject in the first month of winter is very perfect and citizens feel satisfied with the probability of 100 The unit 83 which refers to the geographical section 3 of this region has the best performance and can be seen as a benchmark for the other sections Citizens feel dissatisfied with the unit 87 which refers to the section 5 of this region with the probability of 95%. This unit has a good performance just in handling the complaints which refer to the 'asphalt settlement' subject The subjects of 'asphalt layer' and 'garbage collection cause a high number of complaints in the first month of winter while the municipality performance is worse  V  C ONCLUSTION  Governments have to consider convenient channels and services to connect between the governmental managers and citizens.  Furthermore, data mining tools are needed to manage citizens' requirements and process which collect analyze, reflect, and evaluate their needs In this research, we have used clustering and association rules on the data of the urban service management system in Iran to find the subjects that cause complaint and the factors that affect the rate of satisfaction. The results show that the municipality should give top priority to the 'snow', 'garbage and waste', 'cleaning' and 'asphalt' requirements and specially to the 'asphalt settlement', 'asphalt layer', 'Garbage collection and 'Installation of garbage ban' subjects during the winter in geographical region1 Analyzing the rules, make it possible to understand the impact of factors such as time and responsible units on the rate of satisfaction. Besides, units with a perfect or worse performance in providing services and handling complaints are identified The results of the research are very beneficial in providing improved urban services and the development of citizens' satisfaction.  This study could be notable as one of the first studies on using data mining tools in CiRM R EFERENCES  1  Ahn. J. And Young, S., "Customer pattern search for after-sales service in manufacturing". Journal of Expert Systems with Applications, vol.36, pp.5371–5375, 2009 2  Cheng, Ch. and Chen, Y.,  "Classifying the segmentation of customer value via RFM model and RS theory", journal of Expert Systems with Applications, vol.36, pp.4176–4184, 2009 3  Cock, M. D., Cornelis, C. and Kerre, E. E., "Elicitation of fuzzy association rules from positive and negative examples. Journal of Fuzzy Sets and Systems", vol.149, pp.73–85, 2005 4  Han, J. and Kamber, M., Data Mining: Concepts and Techniques Second ed., Morgan Kaufman Publisher, 2006, pp. 383-407 5  Jukic, N. And Nestorov, S., "Comprehensive data warehouse exploration with qualified association-rule mining", Journal of Decision Support Systems, vol.42, pp.859–87, 2006 6  Ngai, .E.W.T., Xiu, L. and Chau, D.C.K., "Application of data mining techniques in customer relationship management: A literature review and classification", journal of Expert Systems with Applications, vol 36, pp. 2592–2602, 2008 7  Pan,S., Tan, C., Lim, E., "Customer relationship management \(CRM in e-government: a relational perspective", Decision Support Systems vol. 42, pp. 237– 250, 2006 8  Sasaki,T., A.Watanabe,Y. and Minamino,K, "An Empirical Study on Citizen Relationship Management in Japan", Proc. PICMET conference of IEEE Xplore., Aug. 2007, pp. 2820-2823 9  Tan, P., Steinbach, M. And Kumar, V, Introduction to Data Mining Addison Wesley, ISBN: 0-321-32 136-7, 2006, pp. 171-180             Lift Confidence Support Consequent Antecedent ID 1.089 75.214 28.947 satisfied month = 12 1 3.234 95 0.987 dissatisfied unit = 87 2 1.079 75.51 12.5 satisfied subject = 379 3 3.234 100 0.987 dissatisfied month = 10 and subject = 413 4 3.234 100 0.329 dissatisfied month = 10 and unit = 83 5 1.448 100 0.658 satisfied unit = 83 and subject = 524 6 3.234 100 0.329 dissatisfied month = 10 and subject = 380 and  unit = 250 7 1.184 81.818 7.237 satisfied subject = 380 and month = 12 and unit = 250 8 
281 


  4öÕ«Wâââ›6m²}}}Ñd7oÞìççw  2ª+ ´··SbÂHGo}~ýùo€± 5§î~9tóËÓ÷¿p 3Täéû_7žý¸óê$ÆL!nMiN%Q6M 9àó×Ÿ~»ûú×µ§ß¾þøÏÂÌH¤Ff  zðà×Ó'×®fbe%RË¿ß¿‰QÆÄÉÅ*%d x…xX¹˜¥Y¹Ø™å„ÙX   svv>~ü¸……Å«W¯ÄÅÅgÍš•ššŠ¬†‘‘HöôôãQQ@‰c yöôçƒ@öß×¯~¿zùÌÆ˜XYI0ÿ÷ok ƒ‘‡‹Q<¼lØu2    R¯F H<@`	¨!GÀ€#Ap <‹CƒDÁ	ØŸ4išÍvõŠ>1™™ÎüÎEAI’„h4ó<#zž':]×EQ„äyÄº®cªò+|Æ÷ý¦i,Ë:Žš|Ë0¡°ï;^ïû®ª*Ïst®ë’@š¦a"AgGhÃ€•mÛÇA¿ï{×u¡#-ÎË²Œ+¯ëjÛö4MAˆ;eGq·m‹ò<OÓ4‘,ËB H§Ói°,‹sŽs.„!@Ek­çy@÷}—R¶®¯œç9ÏsUL 001 5\006\010\031\030\020\001 1+<2\001 002\020\033\031\007\033\003\007\017\001 027\003\017\017\020\030\007\016\001 030\020\033\031\037\017\006\013\007\001 034\006\017\011\001 037\032\013\016\020 033\001 006\017\020\014\016\020\017\016\001'\013\030\001 \033\031\032\017\001\033\003\017\003\016\020\017+\001 001 b¼ŒfV-°0‚W¯^‰‹‹Ã ÓXDD1ÎÞ·o b¼ŒfV-°0‚W¯^‰‹‹Ã ÓXDD1ÎÞ·o O¯ùÙû‹°,‡a (*II’Öâ4MeY.Ë’ã8(«ª:¨1’$4M£ª*tC×uÃœN'žçasžç¢(<ÏÓ4ÍqdA@Ê,ËAX¿» GšMËN|<Ë›šÇ·dY¢”°Ë˜W RÌÈÈØÓÓS  wï^'''4Ó ŒÿÿÿãWT@‰O FV  DFFT dÀÅq©A$´··SbÂ(XAÙŠ'íPx~Â@;$;{Ú	£`¸³gÏFFFT edd a € ’=í*ƒAÝ#3®Ïž=IP a € ’=í*ƒAÝ#3®Ïž=IP 3fÌurrÚ·oŸ³³óÃ‡åääÚ¥# ³‰¼¼<0v€‘ráÂ}}}4·nÝRWW¯©©iiir©Y£ñ>œÀŠ+–-[¶yóf`"inn&¨’äöîÝ,ˆ´k‚add bO• eYAð-®ª*Ãu]eY~nÅ·þ"`Áƒëºî<Ó4™¦Ç1‚©(Š˜\–EUUœRŽ]×¹®Áyž°Ä¶m]×9tª»ü[T tz¢#QHn`_2ÙÉÛ»±‰jã+^~ÏŸ72ÿš$	!$Cr   Xkqï»uÎI 001 5\006\010\031\030\020\001 1+12\001 020\030\037\020\007\017\001 0203\020\037\031\017\006\013\007\001 017\006\014\020\001 006\014\027\030\013\023\020\014\020\007\017\001 034\006\017\011 001 037\032\013\016\020\033\001\006\017\020\014\016\020\017\016\001 001 5\006\010\031\030\020\0011+1\001\016\011\013\034\016\001\017\011\020\001\0203\020\037\031\017\006\013\007\001\017\006\014\020\001'\013\030\001 &\036!"!\001\003\007\033\001 036!"!%&\032\013\016\020\033\001 013\030\001 003\032\032\001 017\011\030\020\020\001 033\003\017\003\016\020\017\016\001 034\006\017\011\001 003\032\032 037\013\007'\006\033\020\007\037\020\001 017\011\030\020\016\011\013\032\033\001 023\003\032\031\020\001 0200\031\003\032\001 017\013\001 N+N=+\001 012\011\020\001 030\020\016\031\032 017\016\001 016\011\013\034\007\001 006\007\001 017\011\006\016\001 006\010\031\030\020\001 037\032\020\003\030\032\022\001 006\032\032\031\016\017\030\003\017\020\001 017\011\003\017\001 036  032\013\016\020\033\001\013\031\017\027\020\030'\013\030\014\020\033\001 &\036!"!\001\034\006\017\011\001\030\020\016\027\020\037\017\001\017\013\001\003\032\032\001\017\011\030\020 020\001 033\003\017\003\016\020\017\016+\001 001 033\010\027\014\006\004\012\034\021\003\014\006\010 001 007\001\013\031\030\001\027\030\020\023\006\013\031\016\001\034\013\030,\001\034\020\001\016\011\013\034\001\034\006\017\011\001\017\011\020\001\011\020\032\027\001\013'\001\0203\027\020\030 006 014\020\007\017\003\032\001\030\020\016\031\032\017\016\001\017\011\003\017\001\034\011\020\007\001\033\003\017\003\016\020\017\001\006\016\001\033\020\007\016\020\035\001\017\011\020\001\007\031\014\020\030\001\013'\001 030\0200\031\020\007\017\001 006\017\020\014\016\020\017\016\001 010\020\007\020\030\003\017\020\033\001 006\016\001 023\020\030\022\001 032\003\030\010\020+\001 012\011\006\016\001 003 020\037\017\016\001 017\011\020\001 027\020\030'\013\030\014\003\007\037\020\001 013'\001 017\011\020\001 006\007\017\020\030\020\016\017\006\007\010\001 027\003\017\017\020\030\007\016\001 033\006\016\037\013\023\020 030\022\001 003\032\010\013\030\006\017\011\014\016+\001\026\020\001\006\014\027\030\013\023\020\033\001\017\011\020\001\027\020\030'\013\030\014\003\007\037\020\001-\022\001\031\016\006\007\010\001\003\032 032 037\013\007'\006\033\020\007\037\020\001\014\020\003\016\031\030\020+\001W\023\020\007\001\017\011\013\031\010\011\001\003\032\032%\037\013\007'\006\033\020\007\037\020\001-\003\016\020 033\001 003\032\010\013\030\006\017\011\014\001 013\031\017\027\020\030'\013\030\014\020\033\001 017\011\020\001 016\031\027\027\013\030\017\001 003\016\020\033\001 003\032\010\013\030\006\017\011\014\001 034\020\001\033\006\016\037\013\023\020\030\001\017\011\003\017\001\017\011\020\030\020\001\006\016\001\016\017\006\032\032\001\007\020\020\033\001'\013\030\001'\031\030\017\011\020\030\001\006\014 027\030\013\023\020 014\020\007\017+\001 007\001 017\011\006\016\001 027\003\027\020\030\001 034\020\001 016\011\013\034\001 017\011\003\017\001 017\011\020\001\027\020\030'\013\030\014\003\007\037\020\001 037\003 007\001 020\001\006\014\027\030\013\023\020\033\001\034\006\017\011\001\017\011\020\001\011\020\032\027\001\013'\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\016\001-\022\001\030\020\033 031\037 006\007\010\001 030\020\033\031\007\033\003\007\017\001 006\017\020\014\016\020\017\016+\001 026\020\001 027\030\013\027\013\016\020\001 037\011\003\007\010\020\016\001 017\013\001 036 001\003\032\010\013\030\006\017\011\014\001-\022\001\006\007\017\030\013\033\031\037\006\007\010\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\001\010\020\007\020\030\003\017 006\013\007\001 003\007\033\001 034\020\001 030\020'\020\030\001 006\017\001 036!"!%&\032\013\016\020\033+\001 W3\027\020\030\006\014\020\007\017\003\032\001 030\020\016\031\032\017\016 001 016\011\013\034\001 017\011\003\017\001 034\020\001 003\030\020\001 003-\032\020\001 017\013\001 006\014\027\030\013\023\020\001 017\011\020\001 027\020\030'\013\030\014\003\007\037\020\001 013 001 036!"!\001\034\006\017\011\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\016+\001 &\036!"!%&\032\013\016\020\033\001\013\031\017\027\020\030 013\030\014\016\001 036!"!\001 006\007\001 017\020\030\014\016\001 013'\001 030\020\033\031\007\033\003\007\017\001 006\017\020\014\016\020\017\001 030\020\033\031\037 017\006\013\007\035\001 030\020\033\031\007\033\003\007\017\001 027\003\017\017\020\030\007\016\001 030\020\033\031\037\017\006\013\007\035\001 003\007\033\001 006\014\027\030\013\023\020\033\001 0203  020\037\031\017\006\013\007\001\017\006\014\020+\001 001 033\010\036\005\002\005\015\005\006\004\005\021\010 001 6 001 010\030\003\034\003\032\035\001 002\035\001 003\007\033\001 005\030\006,\003\007\017\035\001\002+\001\\5\003\016\017\001 \032\010\013\030\006\017\011\014\016\001'\013\030\0017\006 007 006\007\010\001 016\016\013\037\006\003\017\006\013\007\001 002\031\032\020\016 001 012\006 7\022\004\002\011\011$\010\012\021\027\006 004\007\006 5 030\003 006 012\0306\005\006 014\004\012\007\017\006\004\012\006O\011\022-\006\032\031\022\021\011\006/\031\030\031\(\031\027\011\027 035\0016//<+\001\001  001 P\003\016\017\006\033\020\035\001 024\035\001 003\0160\031\006\020\030\035\001 035\001 012\003\013\031\006\032\035\001 002\035\001 005\017\031\014\014\020\035\001 U\035\001 003\007\033\001 G\003,\011\003\032\035\001 G+\001 7\006\007\006\007\010\001 7\006\007\006\014\003\032\001 013\007%\002\020\033\031\007\033\003\007\017\001 016\016\013\037\006\003 017\006\013\007\001 002\031\032\020\016\001 4\016\006\007\010\001 5\030\0200\031\020\007\017\001 032\013\016\020\033\001 017\020\014\016\020\017\016 001 012\006 7\022\004 002\011\011$\010\012\021\027\006 004\007\006 3 027\030 006 012\0306\005\006 014\004\012\007\017\006 004\012\006 014\004\015\016\026\030\031\030\010\004\012\031\005\006 032\004\021\010\002 035\001 NNN+\001  001 P\003\022\003\030\033\020\013\035\001 002\035\001 010\030\003\034\003\032\035\001 002\035\001 003\007\033\001 U\031\007\013\027\031\032\013\016\035\001 036+\001 013\007 016\017\030\003\006\007\017%P\003\016\020\033\001\002\031\032\020\0017\006\007\006\007\010\001\006\007\001G\003\030\010\020\035\001\036\020\007\016\020\001\036\003\017\003-\003\016\020\016  001 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\0063 030\003 006%\012\0306\005\006\014\004\012\007\017\006\004\012\006/\031\030\031\006\020\012\021\010\012\011\011\022\010\012\021 035\001 6///+\001  001 R\031\016\016\003\006\007\035\0015+\035\001G\006\031\035\001R+\035\001\005\0319\031,\006\035\001W+\035\001\003\007\033\001G\031\035\001R+\001\\W3\037\020\027 017\006\013\007\001 002\031\032\020\001 7\006\007\006\007\010\001 034\006\017\011\001 003\001 002\020\032\003\017\006\023\020\001 007\017\020\030\020\016\017\006\007\010\007\020\016\016\001 7\020\003\016\031\030\020  017\006 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\0067'0 035\001:NNN+\001 1 001 014\006\020\037\006\007\016,\006\001 W+\001 032\017\020\030\007\003\017\006\023\020\001 006\007\017\020\030\020\016\017\001 014\020\003\016\031\030\020\016\001 013\030\001 014\006\007\006\007\010\001\003\016\016\013\037\006\003\017\006\013\007\016 001 020\020\020\006\035\022\031\012\027\017\0060\012\0041\005\011$\021\011\006\031\012$\006 031\030\031\006\020\012\021\010\012\011\011\022\010\012\021 035\001\023\013\032+61\035\001\007\013+\0016\035\001\027\027+\0011\\001%\001=/\035\001:NN 001  001 003\0160\031\006\020\030\035\001>\035\001P\003\016\017\006\033\020\035\001\024\035\001\012\003\013\031\006\032\035\001\002\035\001\003\007\033\001G\003,\011\003\032\035\001G+\001 032\013\016\020\033\001\005\020\017\001P\003\016\020\033\001\036\006\016\037\013\023\020\030\022\001\013'\001\005\014\003\032\032\001&\013\023\020\030\016\001'\013\030\001 016\016\013\037\006\003\017\006\013\007\001 002\031\032\020\016 001 C\011\0301\004\022\033\010\012\021\006 031\012$\006 012\007\004\022\015\031\030\010\004\012\006 001-\027\030\011\015\027\006P\004\026\022\012\031\005 035\001\023\013\032+\001;\035\001\007\013+\001:\035\001:NNN 001  001 005\006\007\010\011\035\001 002\035\001 015\013\011\007\016\017\020\007\035\001 012\035\001 002\003\010\011\003\023\003\007\035\001 021\035\001 003\007\033\001 025\006\020\035\001 024+\001 007 001 W''\006\037\006\020\007\017\001 \032\010\013\030\006\017\011\014\001'\013\030\001\036\006\016\037\013\023\020\030\006\007\010\001!\013\016\006\017\006\023\020\001\003\007\033\001>\020 010\003 017\006\023\020\001 003\017\017\020\030\007\016 001 012\006 7\022\004\002\011\011$\010\012\021\027\006 004\007\006 020\020\020\006 012\030\017\006 014\004\012\007\017\006 004\012\006 2\022\031\012\026\005\031\022\006\014\004\015\016\026\030\010\012\021 035\001>\003\007\037\011\003\007\010\035\001&\011\006\007\003\035\001:NN/+\001 S 001 005\0319\031,\006\035\001 W+\001 023\012$\010\022\011\002\030\011$\006 010\027\002\004\024\011\022-\006 004\007\006 012\030\011\022\011\027\030\010\012\021 001 0204\002\011\016 030\010\004\012\006 026\005\011\027\017\006 007\017+\001 015\013\031\030\007\003\032\001 013'\001 003\017\017\020\030\007\001 002\020\037\013\010\007\006\017\006\013\007\001 003\007\033\001 030 017\006'\006\037\006\003\032\001"\007\017\032+6=26N=1%6NS=\035\001:NN:+\001  001 025\006\020\035\001 024\035\001 015\013\011\007\016\017\020\007\035\001 012\035\001 002\003\010\011\003\023\003\007\035\001 021+\021\035\001 003\007\033\001 002\003\014\003\037\011\003\007 033\030\003\007\035\001_+\001\\^\007\001\036\006\016\037\013\023\020\030\006\007\010\001\\!\013\017\020\007\017\006\003\032\032\022\0014\016\020'\031\032 001  003 017 017 020 030\007\016\001 030\013\014\001\036\003\017\003-\003\016\020\016 001 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\006%\020\020\020\006%\012\030\017\006\014\004\012\007\017\006\004\012\006 2\022\031\012\026\005\031\022\006\014\004\015\016\026\030\010\012\021\034\0065 001 6N 001 011\013\007\010\035\001 035\001 024\003\013\035\001 024+\024+\035\001 011\016\011\006\014\003\035\001 7+\001 7\011\002\026\005\010\031\022\010\030\001 Q\022\010\011\012\030\011$\006 026\005\030\010$\031\030\031\(\031\027\011\006 010\012\010\012\021\017\006 WWW\001 012\030\003\007\016+\001 007\001 _\007\013\034\032\020\033\010\020\001\003\007\033\001\036\003\017\003\001W\007\010\006\007\020\020\030\006\007\010\035\001612/1:%/=N\035\001:NN;+\001 
419 


only, and concepts with words The corpus of the PubMed abstracts that used in the experim ents is consists of 10000 biomedical abstracts with keyword search breast cancer treatments and side effects   All experim e nts are applied on the 10000 docum ents after divided them into six documentsets 50, 100, 500, 1000 5000, and all 10000 documents. The systems are implemented by using VS .Net 2005 \(C#\a nd the experiments were performed on Intel Core2 Duo, 1.8 GHz system with Windows XP and 2 Giga of RAM A large number of association rules can be extracted by sel ecting the values of minimum support and confidence in the mining process.  The D-EART system gives the best results by using low support and high confidence values Moreover, the number of concepts that entered to the mining process is fewer by using the fuzzy weighting schema. Table V shows the experiments that are applied on various documentsets by different threshol d values. It noticed that the number of extracted association rules in D-EART system is useful and always less than that in Apriori-concept system The reason returns to the strong effect of using the fuzzy weighting schema in D-EART system Fig. 9 and Fig. 10 show that the execution time of Aprioriconcep t system is increased regular ly when the documentsets are increased compared to D-EART system. The mining process in Apriori-word system takes more time for less number of concepts in the documents. The reason is that the mining process in Apriori algor ithm depends on the size of documents rather than the number of concepts. The results show that the execution time of Apriori-concept system is about seventh fold of D-EART system. The D-EART system scans the documents only one time as the number of documents increased. Therefore the size of documents does not influence in the mining process. Finally, the results reveal that the execution time for D-EART system is much better than that of the Apriori-concept system in all cases  TABLE  V   T HE  N UMBER  OF  A SSOCIATION  R ULES  FOR  A PRIORIC ONCEPT  AND  D EART  S YSTEMS Minimum Support s  Minimum Confidence c  No. of Documents Systems s 1 c 50 3 50 7 60 10 50 500 Apriori-concept D-EART 183 71 76 31 17 5 10 2 1000 Apriori-concept D-EART 227 86 91 34 11 4 8 3 5000 Apriori-concept D-EART 239 92  75 27 20 4 15 2 10000 Apriori-concept D-EART 345 135 102 39 37 10 30 7   D5000 0,00 5,00 10,00 15,00 20,00 25,00 30,00 Apriori-concept D-EART Time in minutes s=1, c=50 s=3, c=50 s=7, c=60 s=10, c=50 Fig. 9 Execution time of Apriori-concept and D-EART systems at D=5000 D10000 0,00 5,00 10,00 15,00 20,00 25,00 30,00 35,00 40,00 Apriori-concept D-EART Time in minutes s=1, c=50 s=3, c=50 s=7, c=60 s=10, c=50 Fig. 10 Execution time of Apriori-concept and D-EART systems at D=10000 V  C ONCLUSIONS  AND  F UTURE  W ORK This paper presented a new text mining system for extracting ass o ciation rules based on concepts representation from online textual documents. This system overcame some of the problems in the prev ious EART system and the drawbacks of the Apriori algorithm by using the data structure hash table in the mining process. The results of comparing DEART and Apriori-concept syst ems reveal that the number of extracted association rules in D-EART system is always less than that in Apriori-concept system. Moreover, the execution time for D-EART system is mu ch better than that of Aprioriconcept system in all cases. So concept technique would be suitable to apply to any large corpus of medical text such as portions of the web. The future work will apply D-EART on PDF full text document with figures and images instead of using only the abstract part R EFERENCES  Fast algorithms for mining association rules,” In Jorge B Bocca, Matthias Jarke, and Carlo Zaniolo, editors Proc. 20 th Int. conf. of very Large Data Bases, VLDB Santigo, Chile 1994, pp. 487-499  T. I m ielinski, and A. Swa m i, “Mining association rules between Sets of items in large databases,” In Buneman, Peter and 


Jajodia, Sushil \(Eds Proc. of the ACMSIGMOD Int. Conf. on Management of Data, Washington D.C., 1993, pp. 207–216  e m ettinen, and A. Verka m o, “Applying data mining technique for descriptive phrase extraction in digital document collections,” in Proc. of IEEE Forum on Research and technology Advances in Digital Libraries Santa Barbra CA, 1998  m adzadeh, M. Rahgozar and A. Zarnani, “A new model for discoveri ng XML association rules from XML documents,” in Proc. 3 rd Int. Conf. on Knowledge Mining, ICKM Prague, Czech Republic, 2006 Aug. 25-27, pp. 365-369  i r, Y. Aum a nn, R Feldman, and M. Fresko Maximal association rules: A tool for mining associations in text Journal of Intelligent Information Systems 25:3, pp. 333-345, 2005  A  Ca m p i   M. Kl e m ettinen, and P  L   Lanzi M i n ing association rules fro m XML data,” in Proc. of the 4 th Int. Conf.  on Data Warehousing and Knowledge Discovery Aixen-Provence, France September 4-6, 2002  a m p i, S. Ceri, M. Kl emettinen, and P. L. Lanzi, “A tool for extracting XML as sociation rules,” in Proc. of the 14 th IEEE Int. Conf. on Tools  with Artificial Intelligence \(ICTAI’02 2002, pp. 57–64  and E. Meglio A Text M ining Strategy based on local contexts of words JADT 2004: 7 th Journées internationales d’Analyse statistique des Données Textuelles, 2004  r own Della Piet ra V J deSouza, and P V. Lai, “Class-based ngram models of natural language Computational Linguistics vol. 18 pp. 467–479, 1992  A. Napoli  and Y. T oussaint, “Towards a text mining methodology using association rule extraction,” Published online: 31 May 2005 © Springer-Verlag 2005  i cords and J. Lumpkin, “Der iving general association rules from XML data in Proc. of Fourth ACIS Int. Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel Distributed Computing SNPD'03\Lübeck, Germany, October 16-18 2003  m a n and I. Dagan, “Knowledge discover y in textual databases KDT\ in Proc. 1 st Int. Conf. on Knowledge Discovery and Data Mining 1995  R. Feld m a n, and H. Hir s h, “Mini ng associations in text in the presence of backgr ound knowledge,” in Proc. 2 nd Int. Conf. on Knowledge Discovery and Data Mining Portland, USA, 1996  m a n and I. Dagan and H Hirs h, “Mining text using keyword distributions Journal of Intelligent Systems 10, pp. 281-300, 1998  H. Zhang Q Qiu, and Z. Wang, “PCAR an ef ficient approach for mining association rules 5 th Int. Conf. on Fuzzy Systems and Knowledge Discovery, IEEE 2008  Fürnkranz, “A study using n-gram features for text categorization Austrian Research Institute for Artificial Intelligence Technical Report  OEFAI-TR-98-30 Schottengasse 3 A-1010 Wien, Austria, 1998  Bauer, J Mostafa M. Palakal, and S. Mukhopadhyay C oncept extraction and association from cancer literature WIDM’02  Mclean, Virginia, USA, November 8, 2002  J. Han, J. Pei, and Y Yin, “Mining frequent patt erns without candidate generation,” In W. Chen, J. Naughton, and P. A. Bernstein, editors, 2000 ACM SIGMOD Intl. Conf. on Management of Data ACM Press, 05 2000, pp. 1-12  W  Jin, R. K. Sr ihar i, and X Wu, “Mining concept associations for knowledge discov ery through concept chain queries,” Z.-H. Zhou, H. Li and Q. Yang \(Eds.\2007 LNAI 4426, pp. 555–562 2007.Springer-Verlag Berlin Heidelberg 2007 20  R. Joshi, X. Li , S. Ramachandaran and T. Leong \(2004\. “Automatic Model Structuring from Text using BioMedical Ontology Available http://www.aaai.org/Papers/Workshops/2004/WS-0401/WS04-01-013.pdf   Agrawal, and R. Sr ikant, “Discovering Trends in Text Databases,” in Proc of KDD, Int. Conf. on Knowledge Discovery  NewPort Beach, CA, , August 14-17, 1997, pp. 227-230  A. Dasigi, R. Dingledine, and B Ciliax, “T ext analysis of Medline for discovering functional relationships among genes: evaluation of keyword extraction weighting schemes Int J. Data Mining and Bioinformatics Vol. 1, No 1, 2006  i ve s, and J. Oliveira Concept-based knowledge discovery in texts extracted from the web ACM SIGKDD pp.29-39, July 2000  u b and D. R s n er, “Mining as sociation rules fro m  unstructured documents,” in Proc. 3 rd Int. Conf. on Knowledge Mining ICKM Prague, Czech Republic, Aug. 25-27, 2006, pp. 167-172  D. Rösner, N Is m a il, and F. Torkey  A text m i ning  technique using a ssociation rules extraction Int. J. of Computational Intelligence WASET, Vol. 4, Nr.1, 2007  a ju m d er, M  M i tra, and B. Chaudhuri, “N-gram: a language independent appr oach to IR and NLP Int. Conf. on Universal Knowledge and Language  ICUKL India November 2002  K. Ober m a y e r \(2 009\of concept based keyw ord extraction for tag recomm Available http://www.kde.cs.unikassel.de/ws/dc09/papers/paper_17.pdf   2009 a l library of Medi cine website [Online Available http://www.nlm.nih.gov   a k, “Discovering know le dge from XML documents,” In Wong John, Eds. Encyclopedia of Data Warehousing and Mining. Idea Group Publications 2005  onstrained association rules to predict heart disease,” in Pr oc. IEEE Int. Conf. on Data Mining, ICDM 2001, San Jose, CA, USA , 2001, pp. 433–440  Yong Youn, and U Kim, “A new method for mining association rules from a collection of XML documents ICCSA 2005 LNCS 3481, pp. 936–945, 2005 Springer-Verlag Berlin Heidelberg 2005  I W itten, S  Cunningha m  and G. Buchanan S calable browsing f or large collections: a case study 5 th Conf. digital Libraries  Texas, pp.215-218, 2000   M. Roche J´erom e Az´e, O. Matte-Tailliez, and Y. Kodratoff  Mining texts by association rules discovery in a technical corpus  Intelligent Information Processing and Web Mining Proc. of the Int. IIS: IIPWM'04  Conf held in Zakopane, Poland, May 17-20, 2004      M ining association rules fro m a collection of XML documents using cross filtering algorithm Int. Conf. on Hybrid Information Technology \(ICHIT'06 IEEE, 2006    W   W a n, and G. Dobbie Extr acting association rules from XML documents using XQuery,” in Proc. of the 5th ACM Int. Workshop on Web Information and Data Management \(WIDM’03 2003, pp.94–97  e iss, N Indurkhya, T. Zhang and F. Damerau TEXT MIN ING Predictive Methods for Analyzing Unstructured Information Springer Science-business Media, Inc. 2005  Li a nd T. Leong, “Automated kno wledge extraction for decision model construction: A data mining approach AMIA  Annual  Symposium Proc pp. 758-762, 2003  2009 bMed website [Online]. Available http://www.ncbi.nlm.nih.gov/pubmed  


To resolve this problem, we proposed a new KDD model. It consists of two steps: the first organizes the database records in homogeneous clusters having common properties which permit to deduce the data’s semantic. This step consists of TAH’s and MTAH generation of relieving attributes. The second permits to Discovering Knowledge. It consists to deduce the Fuzzy  Cluster Lattice corresponding to MTAH lattice generated in the first step, then traverse this lattice to extract the Meta Knowledge \( Set of fuzzy associations meta-rules on the clusters \, and in end deduce the rules modeling the Knowledge \(Set of fuzzy associations rules on the attributes\While basing on the hierarchical structure offered by the lattices, we proceed to discover the Knowledge in a hierarchical way. Thus, according to the degree of detail required by the user, this approach proposes a level of knowledge and different views of this knowledge Moreover, this solution is extensible; the user is able to choose the fuzzy method of classification according to the domain of his data and his needs This solution reduced considerably the number of generated rules, offered a better interpretation of the data and optimized both the space memory and the execution time As futures perspectives of this work, we mention 1\o test our approach on several the large data set, and 2\ to define a new intelligent method of evaluation of requests which takes into account the Meta knowledge and/or the knowledge base generated by our KDD model XI  R EFERENCES  1  P. Berkhin, “Survey of clustering data mining techniques“, Technical report, Accrue Software, 2002 2  M. Zaki, “Mining Non-Redundant Association Rules”, Data Mining and Knowledge Discovery, No 9, 2004, p. 223–248 3  G. Stumme, R.Taouil, Y. Bastide, N. Pasquier, and     L. Lakhal Intelligent structuring and reducing of association rules with formal concept analysis”, Proceedings of KI’2001 Conference, Vienna Austria, Lecture Notes in Artificial Intelligence 2174, SpringerVerlag, September 2001, p. 335–350 4  N. Pasquier “Data Mining : Algorithmes d'Extraction et de Réduction des Règles d'Association dans les Bases de Données”, Thèse Département d’Informatique et Statistique, Faculté des Sciences Economiques et de Gestion, Lyon, 2000 5  R. Agrawal, T. Imielinski, and Swami A., “Mining Association Rules between sets of items in large Databases”, Proceedings of the ACM SIGMOD Intl. Conference on Management of Data, Washington USA, June 1993, p. 207-216 6  R. Agrawal, and R. Skirant. “Fast algoritms for mining association rules”. In Proceedings of the 20th Int'l Conference on Very Large Databases, pages 478-499, June 1994 7  N. Pasquier, Y. Bastide, R.Taouil, and L. Lakhal,          “ Efficient Mining of Association Rules Using Closed Itemset Lattices Information Systems Journal, vol. 24, no 1, 1999, p. 25-46 8  M. J. Zaki, and C. J. Hsiao, “ CHARM : An Efficient Algorithm for Closed Itemset Mining ”, Proceedings of the 2nd SIAM International Conference on Data Mining, Arlington, April 2002, p. 34-43 9  G. Stumme, R. Taouil, Bastide Y., Pasquier N., and L. Lakhal, “Fast Computation of Concept Lattices Using Data Mining Techniques BOUZEGHOUB M., KLUSCH M., NUTT W., SATTLER U., Eds Proceedings of 7th Intl. Workshop on Knowledge Representation Meets Databases \(KRDB’00\Berlin, Germany, 2000, p. 129-139 10  G. Stumme, R.Taouil, Y. Bastide, N. Pasquier, and     L. Lakhal Computing Iceberg Concept Lattices with TITANIC”, J. on Knowledge and Data Engineering \(KDE\ vol. 2, no 42, 2002, p. 189222 11  S. Ben Tekaya, S. Ben Yahia, and Y. Slimani. “Algorithme de construction d`un treillis des concepts formels et de détermination des générateurs minimaux”, ARIMA journal, Novembre 2005, Numéro spécial CARI'04, pages: 171-193, 2005 12  T. Hamrouni, S. Ben Yahia, and Y. Slimani. “Prince : Extraction optimisée des bases génériques  de règles sans calcul de fermetures In Proceedings of the Intl. INFORSID Conference, Editions Inforsid Grenoble, France, pages : 353--368, 24-27 May 2005 13  B. Ganter, and R. Wille, Formal Concept Analysis: mathematical foundations. \(translated from the German by Cornelia Franzke Springer-Verlag, Berlin-Hei delberg 1999 14  T.Thanh, H.Siu Cheung, and C. Tru Hoang, “A Fuzzy FCA-based Approach to Conceptual Clustering for Automatic Generation of Concept Hierarchy on Uncertainty Data.” ,CLA 2004, pp. 1–12 ISBN 80-248-0597-9 15  L. Zadeh. Fuzzy sets. Inform ation and Control, \(69\338-353, June 1965 16  M. Sassi, M., A. Grissa Touzi, and H. Ounelli, “ “Interpretting Fuzzy Clustering Results based on Fuzzy Formal Concept Analyis”, IEEE International Conference on Fuzzy Systems. Imperial College London, UK, 2007 17  A. Grissa Touzi, M. Sassi, and H. Ounelli,  “Using Formal Concept Analysis for Flexible Querying Optimization”, 23nd International Conference on Computers and Their  Applications, \(CATA’08 Mexico, Avril 2008 18  A. Grissa Touzi, M. Sassi, and H. Ounelli, “An innovative contribution to flexible query through the fusion of conceptual clustering, fuzzy logic, and formal concept analysis”, International Journal of Computers and Their Applications. Volume. 16, N°. 4, pp 220-233, December, 2009 19  M. Sassi, A. Grissa Touzi, and H. Ounelli, “A Fuzzy Linguistic Database Summarization Approach”, Fuzzy Systems Conference IEEE International Conference on Fuzzy Systems.   Hong Kong, Juin 2008 20  J.C,  Bezdeck,  R.Ehrlich,  and  W.Full,  "FCM: The Fuzzy  C-Means Clustering Algorithm", Computers and Geoscience, vol. 10, no. 2-3 pp. 191–203, 1984 21  N. Pasquier, Y. Bastide, R.Tou il, and L.Lakhal, “Pruning closed itemset lattices for association rules”, Proceedings of 14th International Conference Bases de Données Avancées, Hammamet Tunisia, 26–30 October 1998, p. 177–196 22  M. J. Zaki, “Generating Non-Redundant Association Rules Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Boston, MA, August 2000, p 34-43 23  Y. Bastide, R.Taouil, N. Pasquier, G. Stumme, and L.Lakhal Mining frequent patterns with counting inference”, SIGKDD Explorations, vol. 2, no 2, 2000, p. 66-75 24  B. Ganter, “Two basics algorithms in concept analysis”, Technical report, Darmstadt, 1984 
134 


   


                        





