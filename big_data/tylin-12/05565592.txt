An Improved Hebbian Neural Network with Dynamic Neuronal Life and Relations  Cornelia Gy r\366di 1 Robert Gy r\366di 1 George Pecherle 3 Livia Bandici 2 Mihai Dersidan 4  1 professor Phd  Eng Department of Computer Science 2 Associate professor  Phd Eng Department of Electrical Engineering 3 Phd student Department of Computer Science 4 Student Department of Computer Science 
Faculty of Electrical Engineering and Information Technology, University of Oradea Str. Universitatii 1, 410087 Oradea, Romania cgyorodi@uoradea.ro, rgyorodi@uoradea.ro, gpecherle@uoradea.com, lbandici@uoradea.ro, m_dersidan@yahoo.com    Abstrac t In the last years, artificial intelligence has been an important field as the environments in which human-made devices have to operate become more and more complex, and designing a new algorithm for each environment can be very time and resources consuming. Neural networks have been successful in a lot of applications, since the same basic implementation can be used in a virtually unlimited number of 
situations, by modifying the structure of the network and the tests that are used for constructing it. In this paper we present an improved hebbian neural network that has the capability of adding new neurons to it and can connect neurons using an association rule. Since the main problem in neural network design is the actual construction of the inter-neuronal relations we try to solve this issue at least partially by allowing the network to modify itself depending on its response to different stimuli I  I NTRODUCTION  In complex and highly dynamic environments, controlled 
by a large number of state variables, normal human-designed algorithms are usually either ineffective, or very hard and time-consuming to build. Artificial intelligence tries to overpass these difficulties by creating computer-based structures and algorithms that imitate at a small level some aspects of the human behavior. Neural networks are a special type of structures that try to imitate the human brain in order to create a structure that is self-built \(under the control of a feedback algorithm\ in the environment where it is supposed to run. After it has been built, it remains unmodified and is used as is The process of designing the structure of the neural network and of building it with the optimal environment 
conditions is more of an art than a science, since the number of possible environments is theoretically infinite, and the possible state variables might occur with different frequencies in the environment. However, neural networks have been used successfully in performing several simple tasks In the rest of the paper, we will define the problem \(what the Hebbian theory is\we will describe our modifications and implementation for a more advanced Hebbian Neural Network, then we will describe some experimental results we have obtained using our method, and some final conclusions  II  P ROBLEM 
D EFINITION  The Hebbian theory describes a mechanism by which the repeated and persistent stimulation of the postsynaptic cell results in an increase in synaptic efficacy. This theory was introduced by Donald Hebb, therefore the theory took his name \(the Hebbian theory\Other names for this theory are also: Hebb\222s rule, Hebb\222s postulate and cell assembly theory This theory says the following We assume the following situation takes place: an axon of cell X is near enough to excite another cell Y and it fires it in a persistent and repeated way. In this case, a metabolic change or growth change takes place in either one or both cells. The efficiency of cell X is consequently increas  
This theory could be summarized as cells that fire together. This is somehow similar to the nervous system, as an oversimplification of it. This theory explains the process of associative learning. In this process, the simultaneous activation of cells leads to produce increases in synaptic strength o called Hebbian learn i ng  Since the rules of hebbian learning are simple, they can be easily encoded in a computer neural network, in a very easy weight-selection program. However, because of this simplicity, hebbian neural networks \(HNN\n be used in a limited range of applications   III  O UR M ODIFICATIONS AND 
I MPLEMENTATION  The purpose of this paper is to create a more advanced HNN that uses the advantages of this kind of association learning, but also adds some complexity to the neural interactions, allowing these networks to be used in a wider range of applications e are also tr y i n g to b u ild a m o re dynamic network, that adds new neurons and new connections to itself. While this capacity does not bring many significant improvements, it is interesting to observe the 233 


transformations that appear in the network, to be able to further improve this self-transformation The changes appear in several parts of the network design First of all, an arbiter is us ed to offer feedback to the network, depending on its output \(see Fig. 1\This arbiter is only active during initial training and, depending on the inputs, the outputs of the HNN and the optimal outputs, the arbiter returns a real number in the [n terv al F r o m n o w on, we will call this number alpha. The function used to determine this feedback number is built such as a maximum value -1 is returned when the highest one-space distance appears between the two output values: the one returned by the HNN, and the optimal one. A value of 1 is returned when the two outputs are equal, and a 0 when the HNN\222s output is neutral \(neither bad, nor good\ests with different feedbacks have been performed, by using non-linear functions that were stricter or more relaxed, but in average, the linear function worked without any problems Depending on the feedback from the arbiter, the network updates its connections \(change, creation, deletion\ depending on the activation states of each neuron During the execution of the network, each neuron has an activation value in the inter n eu ron\222s s t ate h e threshold is set at 0.5: above this value the neuron is active below it, it is inactive. The threshold is thus only used during the running of the HNN. The activation value however is going to be used when updating the weights of the connections    Fig. 1. Schematic representation of the feedback circuit   The updating function is run for each pair of neurons that are connected, and it depends on the feedback value, on the activation values of both neurons and on the learning speed this can be used as a variable and be set higher when the current learning set has a higher priority\which is constant This function is also run when the HNN has produced a good output \(alpha > goodOutputThreshold, where goodOutputThreshold is a constant set at 0.5 \226 remember that alpha has a range of [w i t h  v a l u es  f o r g ood ou tputs  in   ee n pairs of un conn ected n e u rons th at h a v e  activated at the same time. The value returned by the function is multiplied with a probability constant, probNewConnection and the resulting number is used as the probability of creating a new connection between the two neurons. If a connection is added between the two neurons, its initial weight is set to the value returned by the updating function The inter-neuronal connections are deleted when they reach a critical lower value, killConnectionThreshold. This value has been set at 0.0012 The updating function is given in the following algorithm  TABLE  I   UPDATING  FUNCTION LINE OBSERVATIONS If \(alpha > 0 Correct output If  \(state1 > 0.5 OR state2 > 0.5  Return alpha * \(state1-0.5\ * \(state2-0.5\ * K  Else  Return -alpha * \(state1-0.5\ * \(state2-0.5\ * K  Else Incorrect output If \(state1 > 0.5 OR state2 > 0.5  Return alpha * \(state1-0.5\ * \(state2-0.5\ * K  Else  Return -alpha * \(state1-0.5\ * \(state2-0.5\ * K   Where  state1  state2 are the activation states of the neurons values between 0 and 1 inclusive  K is a constant representing the learning speed Analyzing all of the cases, if the feedback is positive 225 If both neurons activated, a positive value is returned proportional with the value of the feedback, and the activation values of the neurons \(state1, state2\ince the states are greater than 0.5 \(as the neurons activated\e total product will be a positive value 225 If just one neuron activated, a negative value is returned, since one of the terms state1-0.5, state2-0.5 will be negative, and the other positive. Since the neurons activated differently, the strength of the connection must decrease 225 If neither neuron activated, a negative value is returned, reinforcing their behavior \(non activation Likewise, if the feedback is negative \(bad outputs 225 If both the neurons activated, the strength of their connection is decreased \(a negative number is returned 225 If just one of the neurons activated, a positive value is returned, reinforcing the communication between neurons with different activation states 225 If neither of the neurons activated, a positive value is returned 234 


On each run, depending on the number of the neurons, on the absolute value of the feedback and on a probability constant, a new neuron might be constructed, adding to it some weak connections to the most active \(positive feedback or most inactive neurons \(negative feedback\ [4  8   In the following algorithm, P\(x\ represents a probability function, where x is an expression whose value is between 0 and 100. P returns a Boolean value, which will be true in x out of 100 cases, and false in the other 100-x cases x| represents the absolute value of x If \( P\( |feedback| * probNewNeuron * networkSize addNewNeuron  for \( each neuron in HNN  if \(  \( stateNeuron-0.5 \* alpha >= 0 AND  P\( | stateNeuron-0.5 | * probNewConnection   connectNeurons\( current, new end if end for end if The network is run as a normal neural network, and the input and output neurons cannot be killed. They are connected to the external world, so they cannot be changed. A visual representation of this aspect is shown below, with the input/output neurons fixed, and the internal neurons and inner-connections dynamic   Fig. 2. The input and output neurons are fixed and can\222t be removed. The rest of the network is dynamic   IV  E XPERIMENTAL R ESULTS  A number of tests have been designed for testing the functionality of the network and several networks with different initial structures and numbers of neurons were used for each test. The tests and their results are presented below 1. The networks had to compute the function XOR between three input variables. The output was given on four output neurons: the XOR between the three pairs formed by the input variables, and the total XOR. The reason for using four outputs instead of just one was to allow the arbiter to return a wider range of numbers, instead of just +/-1. The weights of the output neurons were: +/- 0.5 for the total XOR and +/- 0.1\(6\or the other three outputs and the sum of these weights gives the feedback from the arbiter. 10 different initial networks were used, with the first two being built with a hierarchical structure- after a structure was set for computing the XOR between two variables it was repeated for computing the intermediary XORs, and the final one After different runs of the networks, 6 out of 10 \(including the two networks with the pre-designed structure\ced the correct results, and the other 4 had a correctness average of 74 2. Another set of tests was designed using randomly-built polynoms of the 3rd degree, with the coefficients being selected to be smaller than 15. For each polynom, 30 neural networks with 15 hidden \(neither input, nor internal\eurons were built with random connections between the neurons. The inputs and outputs of the network were encoded in binary code. The total number of input and output neurons was selected such that, if the largest value of the polynom \(with X in th terv al w as N log  N tpu t n eu r ons  w er e present. The same principle was used for the input \(since the largest input value was 30, 5 input neurons were present\he arbiter returned after each run a feedback value that depended linearly on the response of the network, the value of the polynom for the current X, and the maximum distance in the direction of the neuron\222s output of the polynom in the [1 interval. For example, for an output of 12 from the network, a correct value of 6, and a maximum value of 34, the arbiter returned the value 8/14: a 1 would be returned for 6, -1 for 34 Drawing a line in the XoY plane between these points, the intersection with X=12 would be at 8/14. After multiple runs an average correctness of 68% was obtained for this test 3. The third and final test was a practical implementation of the neural network on a simple line-following robot that received information from two light sensors, each encoded on 8 bits. The output of the network was encoded on 6+6 bits each representing the speed of the engine from the respective side \(with stop at 0 and maximum speed at 63 developing a program for this line-following robot, its output was used by the arbiter to compute the feedback value. This feedback value depended on angle difference between the two directions \(the one chosen by the robot\222s program, and the one suggested by the neural network\he robot was controlled only by its program during the building of the network. After the end of the building period, the control was given to the neural network. Different tests were made on this robot, with different reduction values for the two engines, to control the maximum speed of the robot mechanically. At slow and medium speeds, the network was able to keep the robot on the track, but at higher speeds, after 6-7 seconds, it got off tracks. Another test was made, when the arbiter 235 


remained active during the period when the network was controlling the robot. No significant improvement was observed in the behavior of the robot    V  C ONCLUSIONS  Our neural network is an improved version by allowing the possibility to add new neurons to it. Also, our network can connect neurons using an association rule Another advantage is the fact that we allow our network to modify itself depending on its response to different stimuli This solves a problem in neural network design, more exactly the construction of the inter-neuronal relations This type of network can further be improved by adding a sub graph based analysis of the neuronal connections, and updating the weights depending on the activity in each sub graph. In choosing these sub graphs, a good technique could be to select the dense sub graphs that are isolated from the rest of the network \(working separately\nd by using a more advanced arbiter, a feedback could be generated for the respective sub graph Another improvement can be the inclusion of genetic algorithms in the building of the networks. The genetic algorithm will be used to modify the different parameters that appear in the network self-construction \(the probabilities for creating new neurons, new connections etc.\, or to change the function of the arbiter for each network, thus controlling the curve of the positive-negative response to a non-linear one                             R EFERENCES  1 H e bbia n  t h e o ry   h tt p e n.w ik i pe dia o r g w iki/H e bb ia n  2  N e ur al N e t w o r ks and L e ar ning M a c h i n e s  3 r d E d itio n S i m o n H a y k in   McMaster University, Canada, 2008, Prentice Hall 3  A r tif icial  I n te l l i ge nce A Mo de r n A ppr o ach 3 r d E d i tio n S t uar t  Russell, 2009, Prentice Hall 4  P a tte r n  Re co g n i t io n a n d Mac h i n e L e ar ning  I nf o r m a tio n S c ie nce a n d  Statistics\- Christopher M. Bishop, 2007, Springer 5  P r acti cal G e ne tic A l g o r ithm s R a n d y  L  H a up t, W i l e y I n t e r s cie nce   2004 6 A s e lf orga n i si n g n e t w ork t h a t g r o w s w h e n re q u i r e d St e p h e n Marsland, Jonathan Shapiro, Ulrich Nehmzow - Neural Networks Volume 15, Issue 8-9 \(October 2002\, pp. 1041-1058, ISSN: 0893-6080 7   A Ve ry  F a s t  L e ar ning  Me t ho d f o r N e ur al Ne tw o r ks Bas e d o n  Sensitivity Analysis" - Enrique Castillo, Bertha Guijarro-Berdi\361as Oscar Fontenla-Romero, Amparo Alonso-Betanzos - The Journal of Machine Learning Research - Volume 7 \(December 2006\, pp. 1159 1182, ISSN: 1532-4435 8  P a tte r n  Re co g n it io n an d N e ur al N e tw o r ks Br ian D   Ri pl e y   Cambridge University Press, 1st edition \(January 28, 2008        236 


    the respective weightings of tags. This ensures that a resource overlapping with important tags of a concept be ranked higher than a resource overlapping with less important ones. The concept-tag weight denoted as w\(t,C is defined as C t if Coup Inv cohesion C t if C t weight   0       4  C v v t W cohesion      5  C u u t W Coup Inv  2      6  The cohesion measures the strength of connectivity of a tag with other tags in the same concept, the Inv.Coup inverse coupling\easures the strength of connectivity of a tag with other tags not in the same concept. This tag-concept similarity measure is developed based on the classic TFIDF measure in IR lots of internal links should rank higher in this concept than another ta g with only a few internal links but many external ones. With the concept-tag weight information, the similarity between a resource and a concept is adjusted to  r t C t C r t r t w C t w C t w C r Sim               2     7    The resource-concept similarity value is also in the range  w\(t,r denotes the weight of tags in each resource. It is determined by its weight in the concept it joins. If a tag does not belong to any cluster, its resour ce-tag weight will be zero  C t C t C t w r t w  0           8  Once resources are associated and ranked in concepts, it is worthwhile to rank the concepts itself. Intuitively, concepts representing large number of resources in the result set should have higher rank. However, we should also evaluate the quality of such big concept. Concepts are ranked by its internal cohesion and the number of resources it represents Average tag weight in a concept is a simple way of measuring the quality of internal cohesion of a concept. Formally the ConceptRank is defined as   N N C C t w R c C t c         9  where N denotes the total number of resources in the result set and N C denotes the number of resources in concept C The first part of formula \(9\omputes the average tag weight in the concept. The second part normalizes the number of resources in the concept by total result number Table 3 shows the tag-concept weight, resour ce-concept similarity and ConceptRank values for the simple example in Table 2 The new metrics preserve most of the observed orders we described. However, there are a few exceptions. For instance resource r 4 is ranked higher in co ncept C1 than resource r 7  Such discrepancy is a result of applying tag weights. In this particular example t 2 has higher weight in C1 than t 3 has indicating that r 4 is better represented by C 1 than r 7 is.  In addition, the new metrics is able to precisely rank resources such as r 6  and r 8 in concept C 2  r 6 is ranked higher because it overlaps with more important tags in C 2  Table 3 Similarity measure C 1  C 2  weight\(t 1 C 1 2.92 weight\(t 2 C 1 1 75 weight\(t 3 C 1 1.17 weight\(t 5 C 2 1.17 weight\(t 6 C 2  2.84 weight\(t 7 C 2 1.67 Sim\(r 1 C 1 1.00 Sim\(r 2 C 1 0.10  Sim\(r 4 C 1   Sim\(r 5 C 1 0.64  Sim\(r 7 C 1 0.70 Sim\(r 2 C 2 0.10 Sim\(r 3 C 2 1 00 Sim\(r 5 C 2 0.0 4 Sim\(r 6 C 2  0.79 Sim\(r 8 C 2 0.71 ConceptRank = 1.22 ConceptRank = 1.83 IV  EXPERIMENT AND RESULTS  The proposed algorithms are evaluated on several data sets obtained from CiteULike www.citeulike.org  an online bibliography organize r. It allows users to save bibliographic information about published research papers online. Users can tag those papers with keywords. A paper can be retrieved through all tags collectively assigned by various users. We collected several data sets using query terms including algorithm”, “software”, “web” and so on  Table 4 shows the clustering results on the data set obtained by query term “web We only show the top concepts whose ConceptRank values are greater than 0.1. For each concept, we list the represe nting tags in decreasing order of their weights in the concept We also show the top 5 papers based on resource-concept simila rity values. The result clearly distinguishes several research areas related with keyword web”. The first and the third concepts are in the area of web information retrieval with differ ent focuses. The first concept has a focus on query log analysis and user behavior study while the third one represents more traditional approach based on hyperlink analysis. The second concept has a focus on semantic web and web service s research. The fourth has a focus on more recent research on web 2.0 and social web   103 103 


    Table 4 web” query re sult Concept Members R: 0.92 t: 20 r:96   search \(24.21\, query_log_analysis \(14.47\, queries\(14.47 web_search \(13.15\ \(12.81\, search_behaviour \(12.47  6 Searching the Web: the public and their queries \(0.79 Vox Populi: the public searching of the Web \(0.79  U.S. versus European web searching trends \(0.79  Analysis of a very large web search engine query log\(0.77  An analysis of web searching by E uropean AlltheWeb.com users \(0.72 R: 0.73 t: 9  r: 153  service \(8.21\, semantic \(6.01\ ,com position \(5.14\y\(4 coordination \(3\y \(2.73\, matching 2.22\, discovery \(1.71\, rdf \(1.6 Composition-oriented Service Discovery \(0.69 Automated Composition of Semantic Web Services into Executable Processes 0.69  A software framework for matchmaking base d on semantic web technology \(0.69 Combining RDF and OWL with SOAP for Semantic Web Services \(0.67  An ontology driven framework for data tr ansformation in scientif ic workflows 0.63 R: 0.28 t: 5  r: 92 information-retrieval \(3.0\, prodei \(1.67 1.0 SALSA: the stochastic approach for link-structure analysis 1 What is this page known for? Com puting Web page reputations \(0.92  Enhanced hypertext categoriz ation using hyperlinks \(0.92  Measuring index quality using random walks on the Web 0.92 Efficient crawling through URL ordering \(0.79 R: 0.17 t:7  r: 57  3.83 Eni \(3.33\ksonomies \(2 1\ \(1 Collaborative tagging as a tr ipartite network \(0.82 Social bookmarking in the enterprise \(0.71  Referral Web: Combining Social Networks and Collaborative Filtering \(0.61  The Tipping Point: How Little Things Can Make a Big Difference \(0.55 Collaborative thesaurus taggi ng the Wikipedia way \(0.53  Table 5 shows the clustering re sult on dataset obtained using query term algorithm”. This dataset overlaps partially  with the web dataset. They both have papers in the web information retrieval research area. Interestingly, because our algorithm dynamically clusters query results, the papers in the shared part are put into slightly different concepts with relations to the query and to ot her results. The first concept in algorithm result has similar focus with the third concept in “web” data set.  Both have   informationretrieval”, “web and “algorithm” as keywords. The complete sets of keywords are different which reflect slightly different focuses of the groups. The first concept in “algorithm” data set has “google” and “pagerank” as keywords The original PageRank paper by S.Brin and L Page is ranked fourth in that concept. This paper has a large number of tags assigned by different users which dilutes the focus and makes it ranked lower than a few other papers with less tags. The third concept in “web” data set does not include specific words like “google” or “pageRank” as its keywords and it focuses on general theme on link based web ranking. The Brin&Page paper belongs to the concept. It is not one of the top 5 results though. The rest of the concepts in “algorithm datasets are about gene analysis, graphic algorithm and social network analysis V  CONCLUSION This paper reported a novel algorithm for presenting the web resource search result on the social tagging sites. It adopts a two step clustering approach to organize and rank resources based on their relative similarities with each other Initial similarities are co mputed using user and tag information. The query results are then organized as a group of concepts represented by a few semantically related terms The resources that are related with each concept are rank within the concept. In additi on, concepts are also ranked by its representing terms and the number of resources they represent. We run experiment on data sets obtained from an online bibliography organizing sites allowing people to share their favorite academic papers. The results show that our algorithm can effectively disti nguish various research areas in query results and associate papers in those areas   104 104 


    Table 5 algorithm” result  Concept Members R: 0.44 t: 9  r: 64 5.41\ch \(5.04 information retrieval \(4.83\, web \(3.94 informationretrieval \(3.89 pagerank \(3.05\, information \(1 Inside PageRank \(0.88 The anatomy of a large scale hypertextual Web search engine \(0.84 Authoritative sources in a hyperlinked environment \(0.65  The PageRank Citation Ranking Bringing Order to the Web 0.57 Understanding Search Engines: Mathematic al Modeling and Text Retrieval \(Software Environments, Tools\on 0.48 R:0.34 t:9  r:55  ica \(11.2\, brain \(6.2 3.69\, fmri \(2 eog \(2\, erp \(2\, statistics  \(1.11 Recovering EEG brain signals: Artifact suppression with wavelet enhanced independent component analysis  0.92  Temporally constrained ICA: an application to artifact rejection in electromagnetic brain signal analysis. \(0.89 R:0.33 t:9  r:51  Microarray \(8.34\Expression \(5.49\gene _set_analysis \(4.74\, comparison \(2.50 functional_annotation \(4.58 gene_list_analysis \(1.43 Group testing for pathway analysis improves comp arability of different microarray data sets. \(0.90  GenePattern 2.0 0.85  Significance analysis of functional categorie s in gene expression studies: a structured permutation approach \(0.85 R:0.28 t:8  r: 74 social_networks \(4.77\-net works \(3.77\etworks \(3.70 2.55 1 Finding community structure in very large networks 0.77 Finding and evaluating community structur e in networks 0.71 Fast algorithm for detecting community structure in networks 0.62 The Small World Phenomenon: An Algorithmic Perspective 0.62 A measure of betweenness centrality based on random walks 0.60 R: 0.22 t: 6  r: 44  3.5\ank \(2\, graph \(1.88 Ordering by weighted number of wins gi ves a good ranking for weighted tournaments 1.0  Ranking Tournaments \(1.0  Aggregating inconsistent information ranking and clustering \(0.75  A Min-Max Theorem on Feedback Vertex Sets \(0.71 A CKNOWLEDGMENT  This research is supported by MSRA ISAR Research Grant REFERENCE 1  Agrawal, R., Imieli ski, T., and Swami, A., Mining association rules between sets of items in large databases. In Proceedings of the 1993 ACM SIGMOD Washington, D.C., USA 2  Cai, D., et al. Hierarchical clustering of WWW image search results using visual, textu al and link information. In Proceedings of the 12th annual ACM international conference on Multimedia New York, NY, USA, 2004\ 952-959 3  Ding, C., et al. A minmax cut algorithm for graph partitioning and data clustering. In Proceedings of IEEE Internal Conference on. Data Mining San Jose, CA, USA, 2001\7-114 4  Dong, X., et al. Similarity search for web services. In Proceedings of the 30th VLDB Conference Toronto, Canada 2004 5  Fonseca, B. M., et al. Concept-based interactive query expansion. In Proceedings of the 14th ACM international Conference on information and Knowledge Management Bremen, Germany, 2005\. CIKM '05. ACM Press, 696-703 6  Nie, Z., et al., Object-level ranking: bringing order to Web objects, in Proceedings of the 14th international conference on World Wide Web. 2005, ACM Press: Chiba, Japan 7  Qiu, Y. and Frei, H. 1993. Concept based query expansion. In Proceedings of the 16th Ann ual international ACM SIGIR Conference on Research and Development in information Retrifeval Pittsburgh, PA, USA, 1993\93., 160-169 8  Salton, G., Wong, A. and Yang C.S., A vector space model for automatic indexing Communications of the ACM Vol. 18, No 11 \(November 1975 9  Sanderson, M. and Croft, B. Deriving concept hierarchies from text. In Proceedings of the Annual international 22nd ACM SIGIR Conference in Information Retrieval Berkeley, CA USA. 1999\ Press, 206 213 10 Tibshirani, R., Walther, G. and Hastie,T. Estimating the number of clusters in a dataset via the gap statistic. Journal of the Royal Statistical Society: Series B \(Statistical methodology\. \(2001 63\(2  11 Xu, J. and Croft, W.B.  Query ex pansion using local and global document analysis. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval Zurich, Switzerland 1996 11  105 105 


026\027\201\231\026\027\245\202\231\027\233\235\235\301<\027 255\233\210\256 023\027 252\031\021\031 031\020\031\022\034\017\026\027 f\231\021\027 226\037\037\023\032\023\017\021\022\027 231\030{\024\016\023\022\034\\\027 037\024\016\027 205\023\021\023\021{\027 231##\024\032\023\031\022\023\024\021\027 030\017#\027 023\021\027 251\031\016{\017\027 212\031\022\031 232\024\032\034\017\021\027 223\023 016\023\024\016\023\027 037\024\016\027 231##\024\032\023\031\022\023\024\021\027 030\017\027 205\023\021\023\021{\027 226\037\037\023\032\023\017\021\032\035\025\026\027 226!\016\024 231#\034\024  255\264\256 016\023#\034\021\031\021\027\202\016\023 222\222<\027 202 016\023\021{\017\016\246\303\017\016\030\031{\026\027\227\017\016\030\023\021\026\027\273\017\016\\\031\021\035\027\217\206\207\233\207\211<\027 255\233\301\256 031\\\031 031\\\031 205<\027 241\031 031 031\021\022\026\027=\031 031\021\022\027 031#\017\025\026\027 263\016\024\032\017\017\033\023\021{\027 024\037\027 022\034\017\027 206\233 022 027 303\251\212\227\027 201\024\021\037\017\016\017\021\032\017\026\027 273\024\017\022\034\031\030#\026\027\227<\027\031\021\033\027\303\031\021\027\033\017\021\027\227!##\032\034\017\026\027\232<\027 233\235\235\235<\027\231\027\263\016\023\024\016\023\027\303\017\016#!#\027\231\027 263\024#\022\017\016\023\024\016\023\027 023\030\022\017\016\023\021{\027 024\037\027 231##\024\032\023\031\022\023\024\021\027 030\017#<\027 253\021\027 263\016\024\032\017\017\033\023\021{<\027 024\037\027 022\034\017\027 233\235\235\235\026\027 231\201\205\027 202\253\273\205\240\212\027 261\024\016 034\017\023\224\031\033\017\034\026\027 f\231\030{\024\016\023\022\034\\#\027\037\024\016\027\231##\024\032\023\031\022\023\024\021\027=!\030\017\027\205\023\021\023\021{\027 023\026\027 202\034\031 022 027 255\302\256 022\023\\\023\224\017\033\027 231##\024\032\023\031\022\023\024\021\027=!\030\017#\234\027\202\032\034\017\\\017\026\027\231\030{\024\016\023\022\034\\#\026\027\031\021\033\027\303\023#!\031\030\023\224\031\022\023\024\021<\027\253\021\027 263\016\024\032\017\017\033\023\021{<\027 024\037\027 022\034\017\027 233\235\235\210\027 231\201\205\027 246\027 202\253\273\205\240\212\027 253\021\022\017\016\021\031\022\023\024\021\031\030\027 201\024\021\037\017\016\017\021\032\017\027\024\021\027\022\034\017\027\205\031\021\031{\017\\\017\021\022\027\024\037\027\212\031\022\031\026\027 273<\253<\027 261\017 034\024 205\031#\031\031 024\026\027 205\031#\031\037!\\\023\027 223\031{\023\036\031\016\031\026\027 311\202 031#\017\025\026\027\027\205\240\212\027 206\207\207\026\027\212\031\030\030\031#\026\027|\242\027 245\202\231<\027 255\233\222\256 030\017\027\231\030{\024\016\023\022\034\\#\027\037\024\016\027\016\017\254!\017\021\022\027\253\022\017\\\027\202\017\022\027 205\023\021\023\021{\025\026\027\202 202<\263\016\031 027 227\031\036\031\026\027 212\017\020\031\020\016\031\022\027 202\034\031\034\026\027 f|!\016 027 212\023#\032\024\020\017\016\023\021{\027 202\023{\021\023\037\023\032\031\021\022\027 263\031\022\022\017\016\021#<\027 205\031\032\034\023\021\017\027 251\017\031\016\021\023\021{\027 210\302\217\233\211\234\233 027\231\027\273\017\021\017\016\031\030\027\202!\016\020\017\035\027 031\021\033\027\201\024 003 003 003 003 003 003 003 003 003 003 003 003 003 003 027\264\222\206\246\264\264\264\026\027\241!\016\023\032\034\026\027\202\036\023\224\017\016\030\031\021\033\026\027\233\235\235\262<\027\027 027 255\301\256 031#\034\026\027 205<\202<\263\031\016\020\031\022\034\023\027 026\f\231\021\027 226\021\034\031\021\032\017\033\027 202\032\031\030\023\021{\027 231 263\016\017\033\017\017 027 024\021\027 017#\017\031\016\032\034\027 253##!\017#\027 023\021\027 212\031\022\031\027\205\023\021\023\021{\027\031\021\033\027\252\021\024\036\030\017\033{\017\027\212\023#\032\024\020\017\016\035\026\027 016\023#\034\021\031\021\027\202\016\023 024<\206\027\217\206\207\233\207\211\026\027 017\031\021\027 232\024!\016\021\031\030\027 024\037\027\202\032\023\017\021\022\023\037\023\032\027=\017#\017\031\016\032\034\026\027\253\202\202 035\031\\\031\026\027|<\027 233\235\235\210<\027 212\031\022\031\027 205\023\021\023\021{\027 023\021{\027 036\024\246\212\023\\\017\021#\023\024\021\031\030\027 240 026\027 245\030\016\023\032\034\027 273!\021\022\224\017\016\026\027 273\034\024\030\031\\\016\017\224\031\027 027 202\031\020\031#\017\016\017\026\027 226\033\036\031\016\033\027 240\\\023\017\032\023\021 245\033\024\\\027 237\031\023\032\034\016\024\017\021\026\027 f\231 017\017\033\246 030\023\017\033\027 227\024\024\030\017\031\021\027 027 231\030{\017 206\235\210<\027 231\231\231\253\027 263\016\017##\026\027 205\017\021\030\024\027\263\031\016 005\005 005\005 005\005 005\005 002\002 005\005 027\233\264\262\207\246\206\233\210\242\027\303\024\030<\222\235\027 016\031\025\026\027 233\235\302\235\026\027 227\031\021 026\027|\034\031\023\030\031\021\033 027 255\210\256 027 202\034\017\021\024\035\026\027 232\031\035\031\021\022\027 027 223\031\016\023#\022\031\026\027 202<\027 202!\033\031\016#\034\031\021\026\027 273\031!\016\031\020\027 227\034\031\030\024\022\023\031\026\027 205\031\035\031\021 005 003 201\034\016\023#\022\023\031\021\027\227\024\016{\017\030\022\026\027\f\202\023 024  027 017\032\034\021\023\254!\017\027 037\024\016\027 231##\024\032\023\031\022\023\024\021\027 030\017\027 205\023\021\023\021{\027 227\031#\017\033\027 024\021\027 031\021\027 231\016\022\023\037\023\032\023\031\030\027 251\023\037\017\027 231\030{\024\016\023\022\034\\\311\027 207\246\301\210\235\262\246\222\207\222\206\246\242\274\207\301\027 027 206\207\207\301\027 253\226\226\226\027 212\240\253\027\233\207<\233\233\207\235\274\273\016\201<\206\207\207\301<\233\207\222<\217\206\207\207\301\211\027 255\233\264\256 024\246\032\034\031\021{\023\021{\027 303\017\016\022\023\032\031\030\027\205\023\021\023\021{\027\024\037\027\251\031\016{\017\027\212\031\022\031 027\233\222\246\206\222 027 255\235\256 206\262\301\246\206\210\264<\027 027 222 \003 003 003 005 002 003 003 003 003 004 002 005 003 003 005 003 005 005 004 007 005 003 003 003 002 005 003 002 002 003 003 005 005 007 005 004 003 004 004 005 007 003 005 005 003 005 005 004 004 030\017#\025\026\027 202\253\273\205\240\212\027 233\235\235\210\026\027\205\024\021\022\016\017\031\030\026\027\201\031\021\021\031\033\031 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 027 255\233\206\256 017\016\027\222<\027 255\233\207\256 017#\034\027\231{\031\036\031\030\026\027\f\205\023\021\023\021{\027\273\017\021\017\016\031\030\023\224\017\033\027 231##\024\032\023\031\022\023\024\021\027 030\017#\025\026\027 016\023\021{\017\016\026\027 231\\#\022\017\016\033\031\\\026\027 017\022\034\017\016\030\031\021\033#\027 217\206\207\207\301\211<\027 255\233\262\256 017\036\027 311\231\030{\024\016\023\022\034\\#\027\037\024\016\027\031#\022\027\212\023#\032\024\020\017\016\035\027\024\037\027\231##\024\032\023\031\022\023\024\021\027=!\030\017#\311<\027\263\016\024\032<\027 222\016\033\027 253\021\022<\027 201\024\021\037<\027 024\021\027 252\021\024\036\030\017\033{\017\027 212\023#\032\024\020\017\016\035\027 031\021\033\027 212\031\022\031\026\027 205\023\021\023\021{\027 217\252\212\212^\235\301\026\027 017\036 024\016\022\027 227\017\031\032\034\026\027 201\231\211\026\027 206\302\222 031\016\023#\024\021\025\026\027\231\201\205\027\202\253\273\252\212\212\027\232!\030\035\027\206\207\207\207<\027 255\233\233\256 023\026\027 202<\027 263\031\016\022\034\031#\031\016\031\022\034\035\026\027 205<\027 240{\023\034\031\016\031\026\027 031\021\033\027 261<\027 251\023<\027 033\031\026\027 260\027 205\024\016\023\\\024\022\024\026\027 237<\260\027 205\024\016\023#\034\023\022\031\026\027 202<\260\027 031\021\033\027 024 031\021\022\026\027=\031 031 205<\223<\205\031\016{\031\034\021\035\027 031\021\033\027 231<\231<\205\023\022\036\031\030\035\026\027 f\031#\022\027 231\030{\024\016\023\022\034\\\027 037\024\016\027 205\023\021\023\021{\027 231##\023\032\023\031\022\023\024\021\027 030\017#\025\026\027 231\253\205\251\027 207\262\027 201\024\021\037\017\016\017\021\032\017\026\027 201\031\023\016\024\026\027 226{\035 016\024\032\017\017\033\023\021{\027 024\037\027 206\233\022\034\027 303\251\212\227\027 201\024\021\037\017\016\017\021\032\017\026\027 241!\016\023\032\034\026\027\202\036\023\224\017\016\030\031\021\033\026\027\233\235\235\262 027 255\262\256 002 003 017#\034\027\231{\016\031\036\031\030\026\027\205\023\021\023\021{\027\270!\031\021\035\023\022\031\022\023\020\017\027 f\231##\024\032\023\031\022\023\024\021\027 030\017#\027 023\021\027 251\031\016{\017\027 017\030\031\022\023\024\021\031\030\027 031 


              


   


                        





