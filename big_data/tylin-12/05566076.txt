Sampling Based N-Hash Algorithm for Searching Frequent Itemset  Chen Yong-ming    Zhu Mei-ling College of Mathematics Chengdu University of Information Technology Chengdu, China E-mail: chenym@cuit.edu.cn   Abstract 227Searching frequent itemsets is the critical problem in generating association rules in data mining, classic Hash-based technique, put forward by J. S. Park, for searching frequent itemsets has two shortcomings: one is that it is difficult to choose an appropriate hash function; the other is that it is liable to cause hash colliding. In order to solve the two problems, Chen Y.M proposed N-Hash algorithm which needn\222t to choose hash function and avoided hash colliding. In this paper, the sampling technique is employed to improve the efficiency of N-Hash algorithm Keywords- data mining; association rule; frequent itemset; N 
Hash algorithm;  sampling I   I NTRODUCTION  Association rule 1 is widely used in business management for example, it can help a supermarket manager to plan marketing or advertising strategies, so association rule mining has become an important data mining task and a focused theme in data mining research. In general, association rule mining can be viewed as a two-step process 2  
200  Step 1 Find all frequent itemsets By definition, each of these itemsets will occur at least as frequently as a predetermined minimum support count min_sup  200  Step 2 Generate strong association rules from frequent itemsets By definition, these rules must 
satisfy minimum support and minimum confidence Searching frequent itemsets is the critical problem in generating association rules, there are several algorithms for the task, such as the Apriori algorithm 3 Hash-based technique  4 In 2007, Chen Y.M 5 pointed out that the classic Hashbased technique has two shortcomings: one is that it is difficult to choose an appropriate hash function; the other is that it is liable to cause hash colliding. In order to solve the two problems, Chen Y.M. \(2007 5 proposed N-Hash algorithm which needn\222t to choose hash function and avoided hash colliding. But the efficiency of N-Hash method still has room to be improved, we will employ sampling technique to make N-Hash algorithm more efficient 
II  P RELIMINARIES  For convenience, we firstly list some basic concepts for later use, and then we give the Hash-based technique and NHash algorithm a brief introduction A  Basic Concepts 2  Let     2 1 m i i i I 
 be a set of items, Let D the taskrelevant data, be a set of database transactions where each transaction T is a set of items such that I T 
002 Each transaction is associated with an identifier, called TID. Let A  be a set of items. A transaction T is said to contain A if and 
only if T A 
003  I B 003 and 
004  005 B A The rule B A holds in the transaction set D with support s sometimes is referred to as relative support where s is the percentage of transactions in D that contain B A 
002 An association rule is an implication of the form B A where I A 
006  i.e   s upport A B P A B 
006 The rule B A has confidence  c in the transactions set D where c is the percentage of transactions in D containing A that also contain B  i.e   confidence A B P B A 
 Rules that satisfy both a minimum support threshold min_sup d a minimum confidence threshold min_conf e called strong A set of items is referred to as an 
itemset An itemset that contains k items is a k itemset The occurrence frequency  or support count of an itemset is the number of transactions that contain the itemset. If the relative support of an itemset I  satisfies a prespecified minimum support threshold, then I is a frequent itemset The set of frequent k itemsets is commonly denoted by k L  B  Hash-based Technique In 1995, by experiments Park discovered that the computation of searching the frequent itemsets is mainly dominated by searching the 2-itemsets 2 
L they proposed Hash-based technique 4 which means hashing itemsets into corresponding bucket. The Hash-based technique can be used to reduce the size of the candidate k itemsets k C for 1 k 978-1-4244-5143-2/10/$26.00 \2512010 IEEE 
  It performs as follows: When scanning each transaction in the database, we can generate all of the 2-itemsets for each transaction, by a hash function, hash \(i.e. map\ those 2Supported by the Scientific Research Foundation of CUIT under Gran t  KYTZ201001 and by the Folk Culture Research Centre of Philosophy an d Social Science Key Research Institute in Sichuan Province \(MJ09-03 


itemsets into the different buckets of a hash table structure and increase the corresponding bucket counts. The following example shows its usage Table 1 gives out the transactional data we considered TABLE I  T RANSACTIONAL DATA  TID List of item_IDs 1 i 1 i 2 i 5  2 i 2  i 4  3 i 2 i 3  4 i 1 i 2 i 4  5 i 1 i 3  6 i 2 i 3  7 i 1 i 3  8 i 1 i 2 i 3 i 5  9 i 1 i 2 i 3  Here the function    10  mod 7 hxy x y  is used as the Hash function, as for the transaction 1, we can generate 2itemsets i 1 i 2  i 1 i 5 and i 2 i 5 As for the 2-itemset i 1  i 2 calculate 1 2 10 1 2 mod 7 4 h   then 4 is the bucket address of the 2-itemset i 1 i 2 the 2-itemset i 1 i 2 is sometimes called bucket content of bucket address 4. After the other transactions and 2-itemsets have been dealt with, we can get a Hash table \(see table 2 TABLE II   H ASH TABLE GOT BY CLASSIC H ASH TECHNIQUE  Address 0 1 2 3 4 5 6 Count 2 2 4 2 2 4 4 Content  i 1  i 4  i 1  i 5  i 2  i 3  i 2  i 4  i 2  i 5  i 1  i 2  i 1  i 3    i 3  i 5  i 1  i 5  i 2  i 3  i 2  i 4  i 2  i 5  i 1  i 2  i 1  i 3    i 2  i 3   i 1  i 2  i 1  i 3    i 2  i 3   i 1  i 2  i 1  i 3  From table 2, we see that in address 0, there are two different bucket contents, content 14   ii and content 35   ii  this phenomenon is Hash colliding To avoid Hash colliding, Chen Y.M 5 put forward N-Hash Algorithm to generate Hash table C  N-Hash Algorithm 1  The main idea of N-Hash Algorithm The kdimensional vector is employed to stand for k itemset as for the bucket content which need to be put into a bucket address, if the bucket address to contain it had been existed this bucket is called old bucket, we put the content into corresponding old bucket address, at the same time, the bucket count increased by 1; if the bucket address to contain it had not been existed, a new bucket address is added to contain it at the same time, the bucket count of the new bucket address is 1. The idea comes from our daily life: a technique to check whether a playing card is lost or not. So the method is called Hash algorithm based on naturally assigning bucket, for brevity, it is called N-Hash Algorithm 2  Process  of N-Hash Algorithm Step 1 Denote the set of existed bucket address as A firstly let A   Step 2 Scan the transaction database, take out the transaction one by one, when got the 2-itemset  s t ii from the transaction that is being dealt with, use 2-dimensional vector  s t as the bucket address of  s t ii let s t n  be the count of  s t ii check A if  s tA  then let  s tA  and 1 st n  if  s tA  then let 1 st st nn  Repeat step 2 until all transaction have been dealt with, then we obtain a Hash table Steps 3 According to prespecified minimum support, frequent itemsets are generated 3  Example of N-Hash Algorithm Take the data in Table 1 as an example, we will show how N-Hash Algorithm works Step 1 Set A   Step 2 Scan the transaction database, take out the transaction one by one. As for transaction 1, generate the following 2-itemsets 12   ii  15   ii and 25  ii As for 12   ii since 1 2 A  then let 1 2 A  and 12 1 n  As for 15   ii since 1 5 1 2 A  then let 1 2 1 5 A  and 15 1 n  As for 25  ii since  2 5 1 2 1 5 A  then let 25 1 n  and 1 2 1 5  2 5 A   As for transaction 2, generate the 2-itemset 24   ii as for 24   ii since 2 4 1 2 1 5 2 5 A  then let 1,2\,\(1,5\,\(2,5\,\(2,4 A  and 24 1 n   As for transaction 3, generate the 2-itemset 23  ii as for 23  ii since 2,3 1 2 1,5 2,5 2 4 A   then let 1,2\,\(1,5\,\(2,5\,\(2,4\,\(2,3 A  and 23 1 n   As for transaction 4, generate the following 2-itemsets 12   ii  14   ii and 24   ii As for 12   ii since 1 2 A   then let 12 11 2 n  As for 14   ii since 1 4 1 2 1 5 2,5 2 4 2,3 A  then let 1,2\,\(1,5\,\(2,5\,\(2,4\,\(2,3\,\(1,4 A  and 14 1 n   As for 24   ii since  4  1   3  2   4  2   5  2   5  1   2  1   4  2    A then let 24 11 2 n   As for the other transactions and 2-itemsets, we can deal 


with them similarly. Finally, we obtain a Hash table shown in Table 3 TABLE III   H ASH TABLE GOT BY N-H ASH A LGORITHM  Address 1, 2 1, 5 2, 5 2, 4 2, 3 1, 4 1, 3 3, 5 Count 4 2 2 2 4 1 4 1 Content i 1  i 2  i 1  i 5  i 2  i 5  i 2  i 4  i 2  i 3  i 1  i 4  i 1  i 3  i 3  i 5   i 1  i 2  i 1  i 5  i 2  i 5  i 2  i 4  i 2  i 3  i 1  i 3   i 1  i 2    i 2  i 3  i 1  i 3   i 1  i 2    i 2  i 3  i 1  i 3  III  I MPROVEMENT ON N-H ASH A LGORITHM  A  Aadvantages and disadvantages of N-Hash Algorithm N-Hash algorithm has some advantages. Its rationale is simple and clear, it can be carried out without any Hash function and will not encounter Hash colliding problem, it absorbed the merits of 2-dimensional Hash algorithm  which employ 2-dimensional vector to represent bucket address. NHash algorithm generate the appropriate bucket address just for the bucket content, the number of address is the most but necessary and the least but sufficient number needed to avoid Hash colliding, that is it is the best value However, N-Hash algorithm also has disadvantages, there is still room for it to be improved. When we put some bucket content i.e a itemset, into its corresponding bucket address we need to check those existed addresses to see whether one of them matches or not, the process to do this is fairly timeconsuming. Take Table 3 as example, suppose that we have a 2-itemset on hand to be handled, say 13   ii we have to check the previous 6 addresses 1 2  1 5  2,5  2 4  2,3 and 1 4 the 7 th address 1 3 is the one we want, to find the address 1 3 for 13   ii cost us 7 times of matching so much time is wasted. In the next, we use sampling methods  to solve the problem and to further improve the efficiency of N-Hash algorithm B  Sampling method Sampling method is a popular method in computational statistics; two important terminologies related to it are population and sample.  The population is defined in keeping with the objectives of the study, a sample is a subset of population. Usually, when the population is large, if the sample is scientifically chosen, it can be used to represent the population, because the sample reflects the characteristics of the population from which it is drawn C  Main idea of  sampling based N-Hash algorithm Here, sampling method is employed to improve N-Hash algorithm. Usually, in data mining, the population is large, so the sampling method is appropriate. Take the above database as an example, suppose that the transaction data in Table 1 is a carefully chosen sample of some population P \(Certainly the sample here is too small, in practice, a sample will often be larger, we just use a small sample to demonstrate our idea after the sample has been dealt with, we got the Hash table see Table 3\. From table 3, we discovered that the occurrence frequency of 13   ii is high, but there are so many addresses before 2-itemset 13   ii s address 1 3 so much time is wasted in finding 13   ii s corresponding address 1 3 If there are less addresses before the address 1 3 not so much time will be needed. Therefore using sampling method can save much time, if the sample is carefully chosen, the sample can represent the population, and then the Hash table that comes from the sample can represent that comes from the population, the 2-itemsets with high frequency in sampleís Hash table are liable to be the one with high frequency in populationís Hash table. So we propose a method that combines sampling method and N-Hash algorithm to improve the efficiency of N-Hash algorithm, the method is called sampling based N-Hash algorithm To search for frequent itemsets, the sampling based N-Hash algorithm works as follows Step 1 Carefully draw a sample S from the population P  usually by random sampling Step 2  Use N-Hash algorithm to deal with the sample S  to  get a Hash table, denoted as Hash table S H  Step 3  Rank the Hash table S H with respect to the frequency of bucket content in order to make the bucket address with high frequency lie in the former and that with low frequency the latter, then we get a new Hash table SR H  Step 4 Based on SR H Use N-Hash algorithm to deal with the rest sample of the population P  i.e  P S  when finished, get a Hash table denoted as P H  Step 5 Obtain frequent itemsets according to predetermined minimum support count D  Demonstration of Sampling Based N-Hash algorithm In the above step 1, in practice, draw sample usually by random method, but our sample is rather special here, the reason why we do like this is that our main purpose is to make it convenient to demonstrate how our algorithm works in a simple way, and convenient to show our algorithmís efficiency After our above explanation, suppose we have database to be dealt with shown in Table 4 


TABLE IV   D ATABASE USED TO SHOW SAMPLING BASED N-H ASH ALGORITHM  TID List of item_IDs 1 i 1 i 2 i 5  2 i 2  i 4  3 i 2 i 3  4 i 1 i 2 i 4  5 i 1 i 3  6 i 2 i 3  7 i 1 i 3  8 i 1 i 2 i 3 i 5  9 i 1 i 2 i 3  10 i 1 i 2 i 5  11 i 2  i 4  12 i 2 i 3  13 i 1 i 2 i 4  14 i 1 i 3  15 i 2 i 3  16 i 1 i 3  17 i 1 i 2 i 3 i 5  18 i 1 i 2 i 3  We use sampling based N-Hash algorithm to get Hash table denoted as P H with respect to database in Table 4 Step 1 Draw a sample S from the population P shown in Table 4 Step 2  Use N-Hash algorithm to deal with the sample S  to  get a Hash table, denoted as Hash table S H shown in Table 3 Step 3  Rank the Hash table S H with respect to the frequency of bucket content in order to make the bucket address with the high frequent content lie in the former and that with low frequent content the latter, get a new Hash table SR H shown in Table 5 Step 4 Based on SR H Use N-Hash algorithm to deal with the rest sample of the population P  i.e  P S  when finished, get a Hash table denoted as P H shown in Table 6 Step 5 Obtain frequent itemsets according to predetermined minimum support count. If we set support count as 6, we find that 2-itemset 12   ii  23  ii and 13   ii  are frequent In this example, comparing with N-Hash, approximately 24.6% matching times \(\(86-69\69 24.6%\as been saved in dealing with the transactions in P-S TABLE V   H ASH  TABLE SR H  Address 1, 2 2, 3 1, 3 1, 5 2, 5 2, 4 1, 4 3, 5 Count 4 4 4 2 2 2 1 1 Content i 1  i 2  i 2  i 3  i 1  i 3  i 1  i 5  i 2  i 5  i 2  i 4  i 1  i 4  i 3  i 5   i 1  i 2  i 2  i 3  i 1  i 3  i 1  i 5  i 2  i 5  i 2  i 4    i 1  i 2  i 2  i 3  i 1  i 3   i 1  i 2  i 2  i 3  i 1  i 3  TABLE VI   H ASH  TABLE P H G O T BY S AMPLING BASED N-H ASH ALGORITHM Address 1, 2 2, 3 1, 3 1, 5 2, 5 2, 4 1, 4 3, 5 Count 8 8 8 4 4 4 2 2 Content i 1  i 2  i 2  i 3  i 1  i 3  i 1  i 5  i 2  i 5  i 2  i 4  i 1  i 4  i 3  i 5   i 1  i 2  i 2  i 3  i 1  i 3  i 1  i 5  i 2  i 5  i 2  i 4  i 1  i 4  i 3  i 5   i 1  i 2  i 2  i 3  i 1  i 3  i 1  i 5  i 2  i 5  i 2  i 4    i 1  i 2  i 2  i 3  i 1  i 3  i 1  i 5  i 2  i 5  i 2  i 4    i 1  i 2  i 2  i 3  i 1  i 3   i 1  i 2  i 2  i 3  i 1  i 3   i 1  i 2  i 2  i 3  i 1  i 3   i 1  i 2  i 2  i 3  i 1  i 3  R EFERENCES  1  R. Agrawal, T. Imielinski and A. Swami. ìMining association rules between sets of items in large databasesî, In Proceedings of the ACMSIGMOD International Conference on Management of Data pages 207-216, Washington D.C.,  May 1993 2  Jiawei Han and Micheline Kamber Data Mining Concepts and Techniques 2 nd edition\, Beijing: China Machine Press, 2006 3  R. Agrawal, R.Srikant. ìFast algorithms for mining association rules In Proc. 1994 Int. Conf. Very Large Data Bases pages 487-499 Santiago, Chile, Sept. 1994 4  J. S. Park, M. S. Chen, and P. S. Yu. ìAn effective hash-based algorithm for mining association rulesî. In Pro. 1995 ACM-SIGMOD Int. Conf. Management of Data SIGMODí95\, pages 175-186, San Jose, CA, May 1995 5  Chen Yong-Ming, Xie Hai-Ying, ìHash Algorithm Based on Naturally Assigning Bucket for Searching Frequent Item-set Journal of Data Analysis vol. 3, No. 5, pp. 47-54, 2008. \(in Chinese 6  HE Xiao-wei, ìTwo Dimension Hash Algorithm of Large Itemsets of Apriori Algorithm Computer and Modernization No. 4, pp. 10-12 2003. \(in Chinese  Sample Population 


468 


469 


34 P  D a y a n a n d T  S e j n o w s k i  223 T h e v a r i a n c e o f c o v a r i a n c e r u l e s for associative matrix memories and reinforcement learning.\224 Neural Computation  vol 5 pp 205\226209 1993 35 G  P a l m a n d F  S o m m e r  223 A s s o c i a t i v e d a t a s t o r a g e a n d r e t r i e v a l i n neural nets.\224 in Models of Neural Networks III  E Domany J van Hemmen and K Schulten Eds New York Springer-Verlag 1996 pp 79\226118 36 G  C h e c h i k  I  M e i l i j s o n  a n d E  R u p p i n  223 E f f e c t i v e n e u r o n a l l e a r n i n g with ineffective hebbian learning rules.\224 Neural Computation  vol 13 pp 817\226840 2001 37 D  S t e r r a t t a n d D  W i l l s h a w  223 I n h o m o g e n e i t i e s i n h e t e r o a s s o c i a t i v e memories with linear learning rules.\224 Neural Computation  vol 20 pp 311\226344 2008 38 A  L a n s n e r a n d O  E k e b e r g  223 A n a s s o c i a t i v e n e t w o r k s o l v i n g t h e 224 4 bit adder problem\224.\224 in Proceedings of the IEEE First International Conference on Neural Networks  M Caudill and C Butler Eds San Diego CA 1987 pp II\226549 39 227 227  223 A o n e l a y e r f e e d b a c k a r t i 002 c i a l n e u r a l n e t w o r k w i t h a B a y e s i a n learning rule.\224 International Journal of Neural Systems  vol 1\(1 pp 77\22687 1989 40 I  K o n o n e n k o  223 B a y e s i a n n e u r a l n e t w o r k s  224 Biological Cybernetics  vol 61\(5 pp 361\226370 1989 41 227 227  223 O n B a y e s i a n n e u r a l n e t w o r k s  224 Informatica Slovenia  vol 18\(2 pp 183\226195 1994 42 A  L a n s n e r a n d A  H o l s t  223 A h i g h e r o r d e r B a y e s i a n n e u r a l n e t w o r k with spiking units.\224 International Journal of Neural Systems  vol 7\(2 pp 115\226128 1996 43 A  S a n d b e r g  A  L a n s n e r  K  P e t e r s s o n  a n d O  E k e b e r g  223 A p a l i m p s e s t memory based on an incremental Bayesian learning rule.\224 Neurocomputing  vol 32-33 pp 987\226994 2000 44 A  K n o b l a u c h  223 N e u r a l a s s o c i a t i v e n e t w o r k s w i t h o p t i m a l b a y e s i a n learning.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 09-02 May 2009 45 S  W a y d o  A  K r a s k o v  R  Q u i r o g a  I  F r i e d  a n d C  K o c h  223 S p a r s e representation in the human medial temporal lobe.\224 Journal of Neuroscience  vol 26\(40 pp 10 232\22610 234 2006 46 A  K n o b l a u c h  223 C o m p a r i s o n o f t h e l a n s n e r  e k e b e r g r u l e t o o p t i m a l bayesian learning in neural associative memory.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 10-06 April 2010 47 227 227  223 N e u r a l a s s o c i a t i v e m e m o r y w i t h o p t i m a l b a y e s i a n l e a r n i n g  224 submitted  pp 226 2010 48 227 227  223 O n t h e c o m p u t a t i o n a l b e n e 002 t s o f i n h i b i t o r y n e u r a l a s s o c i a t i v e networks.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 07-05 May 2007 49 A  K n o b l a u c h a n d G  P a l m  223 P a t t e r n s e p a r a t i o n a n d s y n c h r o n i z a t i o n in spiking associative memories and visual areas.\224 Neural Networks  vol 14 pp 763\226780 2001 50 T  S e j n o w s k i  223 S t o r i n g c o v a r i a n c e w i t h n o n l i n e a r l y i n t e r a c t i n g n e u rons.\224 Journal of Mathematical Biology  vol 4 pp 303\226321 1977 51 227 227  223 S t a t i s t i c a l c o n s t r a i n t s o n s y n a p t i c p l a s t i c i t y  224 Journal of Theoretical Biology  vol 69 pp 385\226389 1977 52 D  W i l l s h a w a n d P  D a y a n  223 O p t i m a l p l a s t i c i t y i n m a t r i x m e m o r i e s  what goes up must come down.\224 Neural Computation  vol 2 pp 85\226 93 1990 53 G  P a l m a n d F  S o m m e r  223 I n f o r m a t i o n c a p a c i t y i n r e c u r r e n t McCulloch-Pitts networks with sparsely coded memory states.\224 Network  vol 3 pp 177\226186 1992 54 B  G r a h a m a n d D  W i l l s h a w  223 I m p r o v i n g r e c a l l f r o m a n a s s o c i a t i v e memory.\224 Biological Cybernetics  vol 72 pp 337\226346 1995 55 H  M a r k r a m  M  T o l e d o R o d r i g u e z  Y  W a n g  A  G u p t a  G  S i l b e r b e r g  and C Wu 223Interneurons of the neocortical inhibitory system.\224 Nature Reviews Neuroscience  vol 5 pp 793\226807 2004 56 H  Z h a n g  223 T h e o p t i m a l i t y o f n a i v e b a y e s  224 i n Proceedings of the 17th Florida Arti\002cial Intelligence Research Society Conference  V Barr and Z Markov Eds AAAI Press 2004 pp 562\226567 57 P  D o m i n g o s a n d M  P a z z a n i  223 O n t h e o p t i m a l i t y o f t h e s i m p l e Bayesian classi\002er under zero-one loss.\224 Machine Learning  vol 29 pp 103\226130 1997 58 G  P a l m  223 L o c a l s y n a p t i c r u l e s w i t h m a x i m a l i n f o r m a t i o n s t o r a g e capacity.\224 in Neural and synergetic computers  ser Springer Series in Synergetics H Haken Ed Berlin Heidelberg New York Springer Verlag 1988 vol 42 pp 100\226110 59 D  G o l o m b  N  R u b i n  a n d H  S o m p o l i n s k y  223 W i l l s h a w m o d e l  A s s o ciative memory with sparse coding and low 002ring rates.\224 Phys Rev A  vol 41 pp 1843\2261854 1990 60 A  H o l t m a a t a n d K  S v o b o d a  223 E x p e r i e n c e d e p e n d e n t s t r u c t u r a l s y n a p tic plasticity in the mammalian brain.\224 Nature Reviews Neuroscience  vol 10 pp 647\226658 2009 61 A  K n o b l a u c h  223 S y n c h r o n i z a t i o n a n d p a t t e r n s e p a r a t i o n i n s p i k i n g associative memory and visual cortical areas.\224 PhD thesis Department of Neural Information Processing University of Ulm Germany  2003 62 227 227  223 O n c o m p r e s s i n g t h e m e m o r y s t r u c t u r e s o f b i n a r y n e u r a l a s s o ciative networks,\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 06-02 April 2006 63 227 227  223 N e u r a l a s s o c i a t i v e m e m o r y a n d t h e W i l l s h a w P a l m p r o b a b i l i t y distribution.\224 SIAM Journal on Applied Mathematics  vol 69\(1 pp 169\226196 2008 64 227 227  223 T h e r o l e o f s t r u c t u r a l p l a s t i c i t y a n d s y n a p t i c c o n s o l i d a t i o n f o r memory and amnesia in a model of cortico-hippocampal interplay.\224 in Connectionist Models of Behavior and Cognition II Proceedings of the 11th Neural Computation and Psychology Workshop  J Mayor N Ruh and K Plunkett Eds Singapore World Scienti\002c Publishing 2009 pp 79\22690 65 227 227  223 Z i p n e t s  E f 002 c i e n t a s s o c i a t i v e c o m p u t a t i o n w i t h b i n a r y synapses.\224 in Proceedings of the International Joint Conference on Neural Networks IJCNN 2010  2010 


   Table 4. Normalized Criteria Comparison Table In AHP   Reusability Meeting Operational Requirements Meeting project Deadline Reusability 0.157 0.148 0.272 Meeting Operational Requirements 0.789 0.744 0.636 Meeting Project Deadline 0.052 0.106 0.090  Table 3 and Table 4 show the weight values of the three criterions as compared to each other using the AHP process. These weights have been decided by the stakeholders after discussions among themselves Average weights can be derived from Table 4 as follows Reusability- 0.193 Meeting Operational Requirements- 0.724 Meeting Project Deadline- 0.083 These weights represent the priority of each criterion on a scale of 0 to 1  5.3. Argumentation Tree  We develop argumentation tree for each and every alternative separately. The ar guments are stated by stake holders and assembled under the alternative but they target a specific cr iterion. These arguments can either be supporting or attacking each other or their respective alternative nodes. We present three figures, where each figure represents the argumentation hierarchy for one alternative. Rectangular boxes represent the alternatives with the name of the alternative under it. Ovals represent the criteria with their descr iption. The arguments are specified by labels èAê, èBê, èCê for alternative çAdobe flashé, çAdobe Directoré and çOpen GLé respectively Along with the labels, the arguments also have indexes associated with them. Beneath the labels are two boxes The box on left shows the weight of the argument whereas the box on right shows the priority of the stakeholder who specifies the argument  Once the argument has been sp ecified, the user enters its weight. We first reassess the weights of the arguments using priority reassessment discussed in h e n us ing the techniques specified in [11 w e red u ce t h e arg u m e n t s  to a single level. Finally, the weighted summation of the arguments with the criteria weights helps us evaluate the final weights for the decision matrix. It is important to note here that, the aggregation method used for calculating the favorability is a weighted summation  The three argumentation hierarchies for the three alternatives are presented in the Figures 7, 8, and 9. The diagrams contain arguments, their weights and the stakeholderês priorities     Figure 7. Argumentation Tree For Adobe Flash   Figure 8. Argumentation Tree For Adobe Director 150 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnêt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnêt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnêt create sound clips  A5.1 We donêt need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIês   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





