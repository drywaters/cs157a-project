ANALYSIS AND IMPLEMENTATION OF ASSOCIATION RULE MINING KARTHIYA BANU.R Master of Computer Applications Loyola Institute of Technology Palanchur, Chennai, Tamil Nadu, India karthiyabanu@gmail.com Dr. RAVANAN. R Department of Statistics Presidency College Chennai, Tamil Nadu, India  ravananstat@gmail.com GOPAL.J Master of Computer Applications Loyola Institute of Technology Palanchur, Chennai, Tamil 
Nadu, India jgopalmca@in.com  Abstract Data mining Build models of the world regression, decision trees, neural networks association rules, fuzzy systems,..  \m data that represent snippets of information about the world. Use these models to understand and discover patterns of int erest that may provide knowledge deployable in improvingbusiness processes. The non-trivial extraction of novel implicit, and actionable knowledge from large databases and in a timely manner. The APriori Data Mining Algorithm is used to create 
association rules from sets of items The algorithm finds patterns of items Algortihm uses knowledge from previous iteration phase to produce frequent itemsets that are frequently associated together. A confidence measure is created for each rule generated from the frequent itemsets Keywords Data mining, Apriori Algorithm Assocaition rules  1. Introduction A set of items is defined as an itemset and represents items found in a dataset The datasets can be obtained from real world databases representing shopping carts, retail 
transactions, data warehouses, sales, orders and purchases database tables or created artificially The APriori algorithm is used to analyze a list of transactions for items that are frequently purchased together. Considering a tr ansaction where the sale of software is increased by the sale of e-books Support and Confidence are two measures used to describe market based analysis association rules created with an APriori algorithm 2 Association rule mining Association rule mining finds interesting associations and/or correlation relationships among 
large set of data items. Association rules show attribute value conditions that occur frequently together in a given dataset. A typical and widelyused example of association rule mining is Market Basket Analysis The associations that Apriori finds are called Association rules An association rule has two parts. The Antecedent is a subset of items found in sets of data. The Consequent is an item that is found in combina tion with the antecedent Two terms describe the significance of the 
association rule. The Confidence is a percentage of data sets that contain the antecedent. The Support is a percentage of the data sets with the antecedent that also contain the consequent The input to association rule mining consists of a database T of N transactions T  t 1 t 2 tN over a set of m items I  fI 1 I 2 Img Each transaction is a subset of I Let ¢ denote the size of the largest transaction2 A subset 
X µ I is called an itemset The frequency of an itemset is the ratio of the number of transactions that contain the itemset to the total number of transactions in the database. We say that an itemset X is p frequent if its frequency is at least p The goal of association rule mining is to discover association rules of the type W \ I where W µ I and I 2 I nW The support of a rule W \ I 
is equal to the frequency of the itemset W [I The of a rule W \ I is equal to the ratio of the frequency of W I to the frequency of W Given two parameters  1 \(called as support threshold d  1 \(called as confidence threshold the goal of association rule mining is to discover association rules that have a 475 978-1-4244-8594-9/10/$26.00 c  2010 IEEE 


support of at least  and a confidence of at least   The intuitive meaning of such a rule is that transactions that contain W[I occurs frequently and a large fraction of the transactions that contain W also contain I  3. Problem Formulation Typically, the task of association rule mining is carried out in two steps. The  fiirst step consists of finding all  frequent  itemsets in the database The second step consists of forming the association rules among the frequent itemsets a s been  observed that the rst step of identifying the frequent itemsets is the most computation and I/O intensive Therefore, we first  consider the problem of frequent itemset mining where the goal is to mine all  frequent itemsets 4. APriori Data Mining Algorithm Its Developed by Agrawal and Srikant 1994. It gives an Innovative way to find association rules on large scale, allowing implication outcomes that consist of more than one item Apriori algorithm in pseudocode L 1 frequent items for k= 2; L k-1  k do begin C k candidates generated from L k-1 that is cartesian product L k-1 x L k-1 and eliminating any k-1 size itemset that is not frequent for each transaction t in database do increment the count of all candidates in C k that are contained in t L k candidates in C k with min_sup end return  k L k  5. Sets of database Transactional database D All products an itemset I = {i 1 i 2 i m  Unique shopping event T  I T contains itemset X iff X  T Based on itemsets X and Y an association rule can be X  Y It is required that X  I, Y  I and X   Y   up\(1  2\   3/5 conf\(\(1,2\--------------- = ------ = 3/4   sup\(1\    4/5 relation of number of events containing both itemsets X a and X b to number of events containing an itemset X a  Each set of data has a number of items and is called a transaction. The output of Apriori is sets of rules that tell us how often items are contained in sets of data There are thousands \(or more\ total items to search and possibly a much larger number of combinations of those items that make up all the data sets. Apriori is a fairly efficient way to iterate through the sets of data and finds association rules depending on a particular confidence and support provided as input 5.1 Large Item Set  A set of items that is contained in enough transactions such that the percentage of transactions that contain this item set is greater than or equal to the minimum support 476 2010 International Conference on Signal and Image Processing 


5.2 Candidate Set A set of items that is currently being tested to see if it is a large item set or not. Candidate sets have a reference count and a current count. The reference count is the number of transactions \(or sets of items\ in the entire data set that contain this candidate set. The current count is used for calculating a probability of this set being a large item set as the algorithm iterates and adds candidate sets to the frontier set 5.3 Frontier Set  The set of all Candidate Sets. This is reset at the end of each iteration of the algorithm to be the set of candidate sets that are expected to be large but are not yet known if they are really a large item set or not 6. Limitations of Apriori algorithm 1 Needs several iterations of the data 2 Uses a uniform minimum support threshold 3 Difficulties to find rarely occuring events 4 Alternative methods \(other than appriori can address this by using a non-uniform minimum support thresold 5 Some competing alternative approaches focus on partition and sampling Results Table-1 Total No.of  Items =5  Table-2                     012\015     Graphical Representation 5. Conclusions The association rules are simply taken from subsets of the large item sets. First of all, if there is a Large Item Set that has 4 items, then all the subsets of items of length \( 3, 2, and 1\ are also large item sets. The association rule is found by taking the reference count of those item sets of length n-1 where n is the length of the item set, that are contained in the item set, and dividing by the reference count of the item set. The result is the confidence of the association rule. If the confidence meets your minimum confidence requirement, then the rule is output The APriori Algorithm basically finds the support count and confidence of itemsets eliminating those itemsets that do not meet a minimum support count and confidence measure from a final list of rules created 012\015             012        012       012\015\015 012\015\012\012\015               2010 International Conference on Signal and Image Processing 477 


6. References 1 R. A g r a w a l, T   I m ie lin s k i, and A  S w a m i  Mining      associcion rules between sets of items in large databases. In SIGMOD Conference  pages 207{216, 1993   A g ra w a l an d R  S r i k a n t  F a s t al g o ri t h m s  for miningassociation rules in large databases. In VLDB pages 487{499, 1994  l o n an d J  Spe n cer The Probabilistic Method JohnWiley, 1992   C e g l ar a n d J. R oddi ck As s o ci at i o n mining ACM Computing Surveys 38\(2\, 2006   C h en P  Haas an d P  Sch e u e r m a n n   A new two-phase sampling based algorithm for discovering association rules. In Proceedings of ACM-SIGKDD Conference on Knowledge Discovery and Data mining KDD pages 462{468, 2002  n ila, H. T o iv on e n an d A   Verk a m o  Efficient  algorithms for discovering association rules. In Proceedings of the AAAI workshop on Knowledge Discovery in Databases pages 181{192, 1994  T o i v on en S a m p l i n g l a rg e dat a bas e s  f o r association rules. In VLDB pages 134{145 1996  Z a k i S  P a rt h a s a rat h y  W. L i an d M Ogihara. Evaluation of sampling for data mining of association rules. In RIDE 1997  P  Krieg e l, J  San d er, X. Xu A density-based algorithm for discovering clusters in large spatial databases with noise, in:Proc. of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, Oregon 1996, pp. 226… 231 10 Fa yya d U  P i a t e t s k y-S h a p ir o G  S m yt h P   The KDD process for extracting useful knowledge from volumes of data. Communications of the ACM, 1996 , 39\(11\27 35 11 E K n o r r  R N g  Find i ng a g gr e g a t e  proximity relationships and commonalities in spatial data mining, IEEE Transactions on 1996 884…897 12Jiangping Chen, An algorithm about spatial association rule mining based on cell pattern Geoinformatics2006,wuhan 478 2010 International Conference on Signal and Image Processing 


TABLE V  E XAMPLES OF ASSOCIATION RULES         The municipality performance is not proper in handling the complaints which refer to 'asphalt settlement', 'asphalt layer', 'garbage collection' and 'installation of garbage ban subjects The probability of being satisfied is 70% in the last month of winter The unit 82 which refers to the section 2 of region 1 has the best performance in the third month of winter in comparison with the other months in handling the 'garbage collection subject The performance of municipality in handling the complaints of 'installation of garbage ban' subject in the first month of winter is very perfect and citizens feel satisfied with the probability of 100 The unit 83 which refers to the geographical section 3 of this region has the best performance and can be seen as a benchmark for the other sections Citizens feel dissatisfied with the unit 87 which refers to the section 5 of this region with the probability of 95%. This unit has a good performance just in handling the complaints which refer to the 'asphalt settlement' subject The subjects of 'asphalt layer' and 'garbage collection cause a high number of complaints in the first month of winter while the municipality performance is worse  V  C ONCLUSTION  Governments have to consider convenient channels and services to connect between the governmental managers and citizens.  Furthermore, data mining tools are needed to manage citizens' requirements and process which collect analyze, reflect, and evaluate their needs In this research, we have used clustering and association rules on the data of the urban service management system in Iran to find the subjects that cause complaint and the factors that affect the rate of satisfaction. The results show that the municipality should give top priority to the 'snow', 'garbage and waste', 'cleaning' and 'asphalt' requirements and specially to the 'asphalt settlement', 'asphalt layer', 'Garbage collection and 'Installation of garbage ban' subjects during the winter in geographical region1 Analyzing the rules, make it possible to understand the impact of factors such as time and responsible units on the rate of satisfaction. Besides, units with a perfect or worse performance in providing services and handling complaints are identified The results of the research are very beneficial in providing improved urban services and the development of citizens' satisfaction.  This study could be notable as one of the first studies on using data mining tools in CiRM R EFERENCES  1  Ahn. J. And Young, S., "Customer pattern search for after-sales service in manufacturing". Journal of Expert Systems with Applications, vol.36, pp.5371–5375, 2009 2  Cheng, Ch. and Chen, Y.,  "Classifying the segmentation of customer value via RFM model and RS theory", journal of Expert Systems with Applications, vol.36, pp.4176–4184, 2009 3  Cock, M. D., Cornelis, C. and Kerre, E. E., "Elicitation of fuzzy association rules from positive and negative examples. Journal of Fuzzy Sets and Systems", vol.149, pp.73–85, 2005 4  Han, J. and Kamber, M., Data Mining: Concepts and Techniques Second ed., Morgan Kaufman Publisher, 2006, pp. 383-407 5  Jukic, N. And Nestorov, S., "Comprehensive data warehouse exploration with qualified association-rule mining", Journal of Decision Support Systems, vol.42, pp.859–87, 2006 6  Ngai, .E.W.T., Xiu, L. and Chau, D.C.K., "Application of data mining techniques in customer relationship management: A literature review and classification", journal of Expert Systems with Applications, vol 36, pp. 2592–2602, 2008 7  Pan,S., Tan, C., Lim, E., "Customer relationship management \(CRM in e-government: a relational perspective", Decision Support Systems vol. 42, pp. 237– 250, 2006 8  Sasaki,T., A.Watanabe,Y. and Minamino,K, "An Empirical Study on Citizen Relationship Management in Japan", Proc. PICMET conference of IEEE Xplore., Aug. 2007, pp. 2820-2823 9  Tan, P., Steinbach, M. And Kumar, V, Introduction to Data Mining Addison Wesley, ISBN: 0-321-32 136-7, 2006, pp. 171-180             Lift Confidence Support Consequent Antecedent ID 1.089 75.214 28.947 satisfied month = 12 1 3.234 95 0.987 dissatisfied unit = 87 2 1.079 75.51 12.5 satisfied subject = 379 3 3.234 100 0.987 dissatisfied month = 10 and subject = 413 4 3.234 100 0.329 dissatisfied month = 10 and unit = 83 5 1.448 100 0.658 satisfied unit = 83 and subject = 524 6 3.234 100 0.329 dissatisfied month = 10 and subject = 380 and  unit = 250 7 1.184 81.818 7.237 satisfied subject = 380 and month = 12 and unit = 250 8 
281 


  4öÕ«Wâââ›6m²}}}Ñd7oÞìççw  2ª+ ´··SbÂHGo}~ýùo€± 5§î~9tóËÓ÷¿p 3Täéû_7žý¸óê$ÆL!nMiN%Q6M 9àó×Ÿ~»ûú×µ§ß¾þøÏÂÌH¤Ff  zðà×Ó'×®fbe%RË¿ß¿‰QÆÄÉÅ*%d x…xX¹˜¥Y¹Ø™å„ÙX   svv>~ü¸……Å«W¯ÄÅÅgÍš•ššŠ¬†‘‘HöôôãQQ@‰c yöôçƒ@öß×¯~¿zùÌÆ˜XYI0ÿ÷ok ƒ‘‡‹Q<¼lØu2    R¯F H<@`	¨!GÀ€#Ap <‹CƒDÁ	ØŸ4išÍvõŠ>1™™ÎüÎEAI’„h4ó<#zž':]×EQ„äyÄº®cªò+|Æ÷ý¦i,Ë:Žš|Ë0¡°ï;^ïû®ª*Ïst®ë’@š¦a"AgGhÃ€•mÛÇA¿ï{×u¡#-ÎË²Œ+¯ëjÛö4MAˆ;eGq·m‹ò<OÓ4‘,ËB H§Ói°,‹sŽs.„!@Ek­çy@÷}—R¶®¯œç9ÏsUL 001 5\006\010\031\030\020\001 1+<2\001 002\020\033\031\007\033\003\007\017\001 027\003\017\017\020\030\007\016\001 030\020\033\031\037\017\006\013\007\001 034\006\017\011\001 037\032\013\016\020 033\001 006\017\020\014\016\020\017\016\001'\013\030\001 \033\031\032\017\001\033\003\017\003\016\020\017+\001 001 b¼ŒfV-°0‚W¯^‰‹‹Ã ÓXDD1ÎÞ·o b¼ŒfV-°0‚W¯^‰‹‹Ã ÓXDD1ÎÞ·o O¯ùÙû‹°,‡a (*II’Öâ4MeY.Ë’ã8(«ª:¨1’$4M£ª*tC×uÃœN'žçasžç¢(<ÏÓ4ÍqdA@Ê,ËAX¿» GšMËN|<Ë›šÇ·dY¢”°Ë˜W RÌÈÈØÓÓS  wï^'''4Ó ŒÿÿÿãWT@‰O FV  DFFT dÀÅq©A$´··SbÂ(XAÙŠ'íPx~Â@;$;{Ú	£`¸³gÏFFFT edd a € ’=í*ƒAÝ#3®Ïž=IP a € ’=í*ƒAÝ#3®Ïž=IP 3fÌurrÚ·oŸ³³óÃ‡åääÚ¥# ³‰¼¼<0v€‘ráÂ}}}4·nÝRWW¯©©iiir©Y£ñ>œÀŠ+–-[¶yóf`"inn&¨’äöîÝ,ˆ´k‚add bO• eYAð-®ª*Ãu]eY~nÅ·þ"`Áƒëºî<Ó4™¦Ç1‚©(Š˜\–EUUœRŽ]×¹®Áyž°Ä¶m]×9tª»ü[T tz¢#QHn`_2ÙÉÛ»±‰jã+^~ÏŸ72ÿš$	!$Cr   Xkqï»uÎI 001 5\006\010\031\030\020\001 1+12\001 020\030\037\020\007\017\001 0203\020\037\031\017\006\013\007\001 017\006\014\020\001 006\014\027\030\013\023\020\014\020\007\017\001 034\006\017\011 001 037\032\013\016\020\033\001\006\017\020\014\016\020\017\016\001 001 5\006\010\031\030\020\0011+1\001\016\011\013\034\016\001\017\011\020\001\0203\020\037\031\017\006\013\007\001\017\006\014\020\001'\013\030\001 &\036!"!\001\003\007\033\001 036!"!%&\032\013\016\020\033\001 013\030\001 003\032\032\001 017\011\030\020\020\001 033\003\017\003\016\020\017\016\001 034\006\017\011\001 003\032\032 037\013\007'\006\033\020\007\037\020\001 017\011\030\020\016\011\013\032\033\001 023\003\032\031\020\001 0200\031\003\032\001 017\013\001 N+N=+\001 012\011\020\001 030\020\016\031\032 017\016\001 016\011\013\034\007\001 006\007\001 017\011\006\016\001 006\010\031\030\020\001 037\032\020\003\030\032\022\001 006\032\032\031\016\017\030\003\017\020\001 017\011\003\017\001 036  032\013\016\020\033\001\013\031\017\027\020\030'\013\030\014\020\033\001 &\036!"!\001\034\006\017\011\001\030\020\016\027\020\037\017\001\017\013\001\003\032\032\001\017\011\030\020 020\001 033\003\017\003\016\020\017\016+\001 001 033\010\027\014\006\004\012\034\021\003\014\006\010 001 007\001\013\031\030\001\027\030\020\023\006\013\031\016\001\034\013\030,\001\034\020\001\016\011\013\034\001\034\006\017\011\001\017\011\020\001\011\020\032\027\001\013'\001\0203\027\020\030 006 014\020\007\017\003\032\001\030\020\016\031\032\017\016\001\017\011\003\017\001\034\011\020\007\001\033\003\017\003\016\020\017\001\006\016\001\033\020\007\016\020\035\001\017\011\020\001\007\031\014\020\030\001\013'\001 030\0200\031\020\007\017\001 006\017\020\014\016\020\017\016\001 010\020\007\020\030\003\017\020\033\001 006\016\001 023\020\030\022\001 032\003\030\010\020+\001 012\011\006\016\001 003 020\037\017\016\001 017\011\020\001 027\020\030'\013\030\014\003\007\037\020\001 013'\001 017\011\020\001 006\007\017\020\030\020\016\017\006\007\010\001 027\003\017\017\020\030\007\016\001 033\006\016\037\013\023\020 030\022\001 003\032\010\013\030\006\017\011\014\016+\001\026\020\001\006\014\027\030\013\023\020\033\001\017\011\020\001\027\020\030'\013\030\014\003\007\037\020\001-\022\001\031\016\006\007\010\001\003\032 032 037\013\007'\006\033\020\007\037\020\001\014\020\003\016\031\030\020+\001W\023\020\007\001\017\011\013\031\010\011\001\003\032\032%\037\013\007'\006\033\020\007\037\020\001-\003\016\020 033\001 003\032\010\013\030\006\017\011\014\001 013\031\017\027\020\030'\013\030\014\020\033\001 017\011\020\001 016\031\027\027\013\030\017\001 003\016\020\033\001 003\032\010\013\030\006\017\011\014\001 034\020\001\033\006\016\037\013\023\020\030\001\017\011\003\017\001\017\011\020\030\020\001\006\016\001\016\017\006\032\032\001\007\020\020\033\001'\013\030\001'\031\030\017\011\020\030\001\006\014 027\030\013\023\020 014\020\007\017+\001 007\001 017\011\006\016\001 027\003\027\020\030\001 034\020\001 016\011\013\034\001 017\011\003\017\001 017\011\020\001\027\020\030'\013\030\014\003\007\037\020\001 037\003 007\001 020\001\006\014\027\030\013\023\020\033\001\034\006\017\011\001\017\011\020\001\011\020\032\027\001\013'\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\016\001-\022\001\030\020\033 031\037 006\007\010\001 030\020\033\031\007\033\003\007\017\001 006\017\020\014\016\020\017\016+\001 026\020\001 027\030\013\027\013\016\020\001 037\011\003\007\010\020\016\001 017\013\001 036 001\003\032\010\013\030\006\017\011\014\001-\022\001\006\007\017\030\013\033\031\037\006\007\010\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\001\010\020\007\020\030\003\017 006\013\007\001 003\007\033\001 034\020\001 030\020'\020\030\001 006\017\001 036!"!%&\032\013\016\020\033+\001 W3\027\020\030\006\014\020\007\017\003\032\001 030\020\016\031\032\017\016 001 016\011\013\034\001 017\011\003\017\001 034\020\001 003\030\020\001 003-\032\020\001 017\013\001 006\014\027\030\013\023\020\001 017\011\020\001 027\020\030'\013\030\014\003\007\037\020\001 013 001 036!"!\001\034\006\017\011\001\037\032\013\016\020\033\001\006\017\020\014\016\020\017\016+\001 &\036!"!%&\032\013\016\020\033\001\013\031\017\027\020\030 013\030\014\016\001 036!"!\001 006\007\001 017\020\030\014\016\001 013'\001 030\020\033\031\007\033\003\007\017\001 006\017\020\014\016\020\017\001 030\020\033\031\037 017\006\013\007\035\001 030\020\033\031\007\033\003\007\017\001 027\003\017\017\020\030\007\016\001 030\020\033\031\037\017\006\013\007\035\001 003\007\033\001 006\014\027\030\013\023\020\033\001 0203  020\037\031\017\006\013\007\001\017\006\014\020+\001 001 033\010\036\005\002\005\015\005\006\004\005\021\010 001 6 001 010\030\003\034\003\032\035\001 002\035\001 003\007\033\001 005\030\006,\003\007\017\035\001\002+\001\\5\003\016\017\001 \032\010\013\030\006\017\011\014\016\001'\013\030\0017\006 007 006\007\010\001 016\016\013\037\006\003\017\006\013\007\001 002\031\032\020\016 001 012\006 7\022\004\002\011\011$\010\012\021\027\006 004\007\006 5 030\003 006 012\0306\005\006 014\004\012\007\017\006\004\012\006O\011\022-\006\032\031\022\021\011\006/\031\030\031\(\031\027\011\027 035\0016//<+\001\001  001 P\003\016\017\006\033\020\035\001 024\035\001 003\0160\031\006\020\030\035\001 035\001 012\003\013\031\006\032\035\001 002\035\001 005\017\031\014\014\020\035\001 U\035\001 003\007\033\001 G\003,\011\003\032\035\001 G+\001 7\006\007\006\007\010\001 7\006\007\006\014\003\032\001 013\007%\002\020\033\031\007\033\003\007\017\001 016\016\013\037\006\003 017\006\013\007\001 002\031\032\020\016\001 4\016\006\007\010\001 5\030\0200\031\020\007\017\001 032\013\016\020\033\001 017\020\014\016\020\017\016 001 012\006 7\022\004 002\011\011$\010\012\021\027\006 004\007\006 3 027\030 006 012\0306\005\006 014\004\012\007\017\006 004\012\006 014\004\015\016\026\030\031\030\010\004\012\031\005\006 032\004\021\010\002 035\001 NNN+\001  001 P\003\022\003\030\033\020\013\035\001 002\035\001 010\030\003\034\003\032\035\001 002\035\001 003\007\033\001 U\031\007\013\027\031\032\013\016\035\001 036+\001 013\007 016\017\030\003\006\007\017%P\003\016\020\033\001\002\031\032\020\0017\006\007\006\007\010\001\006\007\001G\003\030\010\020\035\001\036\020\007\016\020\001\036\003\017\003-\003\016\020\016  001 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\0063 030\003 006%\012\0306\005\006\014\004\012\007\017\006\004\012\006/\031\030\031\006\020\012\021\010\012\011\011\022\010\012\021 035\001 6///+\001  001 R\031\016\016\003\006\007\035\0015+\035\001G\006\031\035\001R+\035\001\005\0319\031,\006\035\001W+\035\001\003\007\033\001G\031\035\001R+\001\\W3\037\020\027 017\006\013\007\001 002\031\032\020\001 7\006\007\006\007\010\001 034\006\017\011\001 003\001 002\020\032\003\017\006\023\020\001 007\017\020\030\020\016\017\006\007\010\007\020\016\016\001 7\020\003\016\031\030\020  017\006 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\0067'0 035\001:NNN+\001 1 001 014\006\020\037\006\007\016,\006\001 W+\001 032\017\020\030\007\003\017\006\023\020\001 006\007\017\020\030\020\016\017\001 014\020\003\016\031\030\020\016\001 013\030\001 014\006\007\006\007\010\001\003\016\016\013\037\006\003\017\006\013\007\016 001 020\020\020\006\035\022\031\012\027\017\0060\012\0041\005\011$\021\011\006\031\012$\006 031\030\031\006\020\012\021\010\012\011\011\022\010\012\021 035\001\023\013\032+61\035\001\007\013+\0016\035\001\027\027+\0011\\001%\001=/\035\001:NN 001  001 003\0160\031\006\020\030\035\001>\035\001P\003\016\017\006\033\020\035\001\024\035\001\012\003\013\031\006\032\035\001\002\035\001\003\007\033\001G\003,\011\003\032\035\001G+\001 032\013\016\020\033\001\005\020\017\001P\003\016\020\033\001\036\006\016\037\013\023\020\030\022\001\013'\001\005\014\003\032\032\001&\013\023\020\030\016\001'\013\030\001 016\016\013\037\006\003\017\006\013\007\001 002\031\032\020\016 001 C\011\0301\004\022\033\010\012\021\006 031\012$\006 012\007\004\022\015\031\030\010\004\012\006 001-\027\030\011\015\027\006P\004\026\022\012\031\005 035\001\023\013\032+\001;\035\001\007\013+\001:\035\001:NNN 001  001 005\006\007\010\011\035\001 002\035\001 015\013\011\007\016\017\020\007\035\001 012\035\001 002\003\010\011\003\023\003\007\035\001 021\035\001 003\007\033\001 025\006\020\035\001 024+\001 007 001 W''\006\037\006\020\007\017\001 \032\010\013\030\006\017\011\014\001'\013\030\001\036\006\016\037\013\023\020\030\006\007\010\001!\013\016\006\017\006\023\020\001\003\007\033\001>\020 010\003 017\006\023\020\001 003\017\017\020\030\007\016 001 012\006 7\022\004\002\011\011$\010\012\021\027\006 004\007\006 020\020\020\006 012\030\017\006 014\004\012\007\017\006 004\012\006 2\022\031\012\026\005\031\022\006\014\004\015\016\026\030\010\012\021 035\001>\003\007\037\011\003\007\010\035\001&\011\006\007\003\035\001:NN/+\001 S 001 005\0319\031,\006\035\001 W+\001 023\012$\010\022\011\002\030\011$\006 010\027\002\004\024\011\022-\006 004\007\006 012\030\011\022\011\027\030\010\012\021 001 0204\002\011\016 030\010\004\012\006 026\005\011\027\017\006 007\017+\001 015\013\031\030\007\003\032\001 013'\001 003\017\017\020\030\007\001 002\020\037\013\010\007\006\017\006\013\007\001 003\007\033\001 030 017\006'\006\037\006\003\032\001"\007\017\032+6=26N=1%6NS=\035\001:NN:+\001  001 025\006\020\035\001 024\035\001 015\013\011\007\016\017\020\007\035\001 012\035\001 002\003\010\011\003\023\003\007\035\001 021+\021\035\001 003\007\033\001 002\003\014\003\037\011\003\007 033\030\003\007\035\001_+\001\\^\007\001\036\006\016\037\013\023\020\030\006\007\010\001\\!\013\017\020\007\017\006\003\032\032\022\0014\016\020'\031\032 001  003 017 017 020 030\007\016\001 030\013\014\001\036\003\017\003-\003\016\020\016 001 012\0067\022\004\002\011\011$\010\012\021\027\006\004\007\006%\020\020\020\006%\012\030\017\006\014\004\012\007\017\006\004\012\006 2\022\031\012\026\005\031\022\006\014\004\015\016\026\030\010\012\021\034\0065 001 6N 001 011\013\007\010\035\001 035\001 024\003\013\035\001 024+\024+\035\001 011\016\011\006\014\003\035\001 7+\001 7\011\002\026\005\010\031\022\010\030\001 Q\022\010\011\012\030\011$\006 026\005\030\010$\031\030\031\(\031\027\011\006 010\012\010\012\021\017\006 WWW\001 012\030\003\007\016+\001 007\001 _\007\013\034\032\020\033\010\020\001\003\007\033\001\036\003\017\003\001W\007\010\006\007\020\020\030\006\007\010\035\001612/1:%/=N\035\001:NN;+\001 
419 


only, and concepts with words The corpus of the PubMed abstracts that used in the experim ents is consists of 10000 biomedical abstracts with keyword search breast cancer treatments and side effects   All experim e nts are applied on the 10000 docum ents after divided them into six documentsets 50, 100, 500, 1000 5000, and all 10000 documents. The systems are implemented by using VS .Net 2005 \(C#\a nd the experiments were performed on Intel Core2 Duo, 1.8 GHz system with Windows XP and 2 Giga of RAM A large number of association rules can be extracted by sel ecting the values of minimum support and confidence in the mining process.  The D-EART system gives the best results by using low support and high confidence values Moreover, the number of concepts that entered to the mining process is fewer by using the fuzzy weighting schema. Table V shows the experiments that are applied on various documentsets by different threshol d values. It noticed that the number of extracted association rules in D-EART system is useful and always less than that in Apriori-concept system The reason returns to the strong effect of using the fuzzy weighting schema in D-EART system Fig. 9 and Fig. 10 show that the execution time of Aprioriconcep t system is increased regular ly when the documentsets are increased compared to D-EART system. The mining process in Apriori-word system takes more time for less number of concepts in the documents. The reason is that the mining process in Apriori algor ithm depends on the size of documents rather than the number of concepts. The results show that the execution time of Apriori-concept system is about seventh fold of D-EART system. The D-EART system scans the documents only one time as the number of documents increased. Therefore the size of documents does not influence in the mining process. Finally, the results reveal that the execution time for D-EART system is much better than that of the Apriori-concept system in all cases  TABLE  V   T HE  N UMBER  OF  A SSOCIATION  R ULES  FOR  A PRIORIC ONCEPT  AND  D EART  S YSTEMS Minimum Support s  Minimum Confidence c  No. of Documents Systems s 1 c 50 3 50 7 60 10 50 500 Apriori-concept D-EART 183 71 76 31 17 5 10 2 1000 Apriori-concept D-EART 227 86 91 34 11 4 8 3 5000 Apriori-concept D-EART 239 92  75 27 20 4 15 2 10000 Apriori-concept D-EART 345 135 102 39 37 10 30 7   D5000 0,00 5,00 10,00 15,00 20,00 25,00 30,00 Apriori-concept D-EART Time in minutes s=1, c=50 s=3, c=50 s=7, c=60 s=10, c=50 Fig. 9 Execution time of Apriori-concept and D-EART systems at D=5000 D10000 0,00 5,00 10,00 15,00 20,00 25,00 30,00 35,00 40,00 Apriori-concept D-EART Time in minutes s=1, c=50 s=3, c=50 s=7, c=60 s=10, c=50 Fig. 10 Execution time of Apriori-concept and D-EART systems at D=10000 V  C ONCLUSIONS  AND  F UTURE  W ORK This paper presented a new text mining system for extracting ass o ciation rules based on concepts representation from online textual documents. This system overcame some of the problems in the prev ious EART system and the drawbacks of the Apriori algorithm by using the data structure hash table in the mining process. The results of comparing DEART and Apriori-concept syst ems reveal that the number of extracted association rules in D-EART system is always less than that in Apriori-concept system. Moreover, the execution time for D-EART system is mu ch better than that of Aprioriconcept system in all cases. So concept technique would be suitable to apply to any large corpus of medical text such as portions of the web. The future work will apply D-EART on PDF full text document with figures and images instead of using only the abstract part R EFERENCES  Fast algorithms for mining association rules,” In Jorge B Bocca, Matthias Jarke, and Carlo Zaniolo, editors Proc. 20 th Int. conf. of very Large Data Bases, VLDB Santigo, Chile 1994, pp. 487-499  T. I m ielinski, and A. Swa m i, “Mining association rules between Sets of items in large databases,” In Buneman, Peter and 


Jajodia, Sushil \(Eds Proc. of the ACMSIGMOD Int. Conf. on Management of Data, Washington D.C., 1993, pp. 207–216  e m ettinen, and A. Verka m o, “Applying data mining technique for descriptive phrase extraction in digital document collections,” in Proc. of IEEE Forum on Research and technology Advances in Digital Libraries Santa Barbra CA, 1998  m adzadeh, M. Rahgozar and A. Zarnani, “A new model for discoveri ng XML association rules from XML documents,” in Proc. 3 rd Int. Conf. on Knowledge Mining, ICKM Prague, Czech Republic, 2006 Aug. 25-27, pp. 365-369  i r, Y. Aum a nn, R Feldman, and M. Fresko Maximal association rules: A tool for mining associations in text Journal of Intelligent Information Systems 25:3, pp. 333-345, 2005  A  Ca m p i   M. Kl e m ettinen, and P  L   Lanzi M i n ing association rules fro m XML data,” in Proc. of the 4 th Int. Conf.  on Data Warehousing and Knowledge Discovery Aixen-Provence, France September 4-6, 2002  a m p i, S. Ceri, M. Kl emettinen, and P. L. Lanzi, “A tool for extracting XML as sociation rules,” in Proc. of the 14 th IEEE Int. Conf. on Tools  with Artificial Intelligence \(ICTAI’02 2002, pp. 57–64  and E. Meglio A Text M ining Strategy based on local contexts of words JADT 2004: 7 th Journées internationales d’Analyse statistique des Données Textuelles, 2004  r own Della Piet ra V J deSouza, and P V. Lai, “Class-based ngram models of natural language Computational Linguistics vol. 18 pp. 467–479, 1992  A. Napoli  and Y. T oussaint, “Towards a text mining methodology using association rule extraction,” Published online: 31 May 2005 © Springer-Verlag 2005  i cords and J. Lumpkin, “Der iving general association rules from XML data in Proc. of Fourth ACIS Int. Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel Distributed Computing SNPD'03\Lübeck, Germany, October 16-18 2003  m a n and I. Dagan, “Knowledge discover y in textual databases KDT\ in Proc. 1 st Int. Conf. on Knowledge Discovery and Data Mining 1995  R. Feld m a n, and H. Hir s h, “Mini ng associations in text in the presence of backgr ound knowledge,” in Proc. 2 nd Int. Conf. on Knowledge Discovery and Data Mining Portland, USA, 1996  m a n and I. Dagan and H Hirs h, “Mining text using keyword distributions Journal of Intelligent Systems 10, pp. 281-300, 1998  H. Zhang Q Qiu, and Z. Wang, “PCAR an ef ficient approach for mining association rules 5 th Int. Conf. on Fuzzy Systems and Knowledge Discovery, IEEE 2008  Fürnkranz, “A study using n-gram features for text categorization Austrian Research Institute for Artificial Intelligence Technical Report  OEFAI-TR-98-30 Schottengasse 3 A-1010 Wien, Austria, 1998  Bauer, J Mostafa M. Palakal, and S. Mukhopadhyay C oncept extraction and association from cancer literature WIDM’02  Mclean, Virginia, USA, November 8, 2002  J. Han, J. Pei, and Y Yin, “Mining frequent patt erns without candidate generation,” In W. Chen, J. Naughton, and P. A. Bernstein, editors, 2000 ACM SIGMOD Intl. Conf. on Management of Data ACM Press, 05 2000, pp. 1-12  W  Jin, R. K. Sr ihar i, and X Wu, “Mining concept associations for knowledge discov ery through concept chain queries,” Z.-H. Zhou, H. Li and Q. Yang \(Eds.\2007 LNAI 4426, pp. 555–562 2007.Springer-Verlag Berlin Heidelberg 2007 20  R. Joshi, X. Li , S. Ramachandaran and T. Leong \(2004\. “Automatic Model Structuring from Text using BioMedical Ontology Available http://www.aaai.org/Papers/Workshops/2004/WS-0401/WS04-01-013.pdf   Agrawal, and R. Sr ikant, “Discovering Trends in Text Databases,” in Proc of KDD, Int. Conf. on Knowledge Discovery  NewPort Beach, CA, , August 14-17, 1997, pp. 227-230  A. Dasigi, R. Dingledine, and B Ciliax, “T ext analysis of Medline for discovering functional relationships among genes: evaluation of keyword extraction weighting schemes Int J. Data Mining and Bioinformatics Vol. 1, No 1, 2006  i ve s, and J. Oliveira Concept-based knowledge discovery in texts extracted from the web ACM SIGKDD pp.29-39, July 2000  u b and D. R s n er, “Mining as sociation rules fro m  unstructured documents,” in Proc. 3 rd Int. Conf. on Knowledge Mining ICKM Prague, Czech Republic, Aug. 25-27, 2006, pp. 167-172  D. Rösner, N Is m a il, and F. Torkey  A text m i ning  technique using a ssociation rules extraction Int. J. of Computational Intelligence WASET, Vol. 4, Nr.1, 2007  a ju m d er, M  M i tra, and B. Chaudhuri, “N-gram: a language independent appr oach to IR and NLP Int. Conf. on Universal Knowledge and Language  ICUKL India November 2002  K. Ober m a y e r \(2 009\of concept based keyw ord extraction for tag recomm Available http://www.kde.cs.unikassel.de/ws/dc09/papers/paper_17.pdf   2009 a l library of Medi cine website [Online Available http://www.nlm.nih.gov   a k, “Discovering know le dge from XML documents,” In Wong John, Eds. Encyclopedia of Data Warehousing and Mining. Idea Group Publications 2005  onstrained association rules to predict heart disease,” in Pr oc. IEEE Int. Conf. on Data Mining, ICDM 2001, San Jose, CA, USA , 2001, pp. 433–440  Yong Youn, and U Kim, “A new method for mining association rules from a collection of XML documents ICCSA 2005 LNCS 3481, pp. 936–945, 2005 Springer-Verlag Berlin Heidelberg 2005  I W itten, S  Cunningha m  and G. Buchanan S calable browsing f or large collections: a case study 5 th Conf. digital Libraries  Texas, pp.215-218, 2000   M. Roche J´erom e Az´e, O. Matte-Tailliez, and Y. Kodratoff  Mining texts by association rules discovery in a technical corpus  Intelligent Information Processing and Web Mining Proc. of the Int. IIS: IIPWM'04  Conf held in Zakopane, Poland, May 17-20, 2004      M ining association rules fro m a collection of XML documents using cross filtering algorithm Int. Conf. on Hybrid Information Technology \(ICHIT'06 IEEE, 2006    W   W a n, and G. Dobbie Extr acting association rules from XML documents using XQuery,” in Proc. of the 5th ACM Int. Workshop on Web Information and Data Management \(WIDM’03 2003, pp.94–97  e iss, N Indurkhya, T. Zhang and F. Damerau TEXT MIN ING Predictive Methods for Analyzing Unstructured Information Springer Science-business Media, Inc. 2005  Li a nd T. Leong, “Automated kno wledge extraction for decision model construction: A data mining approach AMIA  Annual  Symposium Proc pp. 758-762, 2003  2009 bMed website [Online]. Available http://www.ncbi.nlm.nih.gov/pubmed  


To resolve this problem, we proposed a new KDD model. It consists of two steps: the first organizes the database records in homogeneous clusters having common properties which permit to deduce the data’s semantic. This step consists of TAH’s and MTAH generation of relieving attributes. The second permits to Discovering Knowledge. It consists to deduce the Fuzzy  Cluster Lattice corresponding to MTAH lattice generated in the first step, then traverse this lattice to extract the Meta Knowledge \( Set of fuzzy associations meta-rules on the clusters \, and in end deduce the rules modeling the Knowledge \(Set of fuzzy associations rules on the attributes\While basing on the hierarchical structure offered by the lattices, we proceed to discover the Knowledge in a hierarchical way. Thus, according to the degree of detail required by the user, this approach proposes a level of knowledge and different views of this knowledge Moreover, this solution is extensible; the user is able to choose the fuzzy method of classification according to the domain of his data and his needs This solution reduced considerably the number of generated rules, offered a better interpretation of the data and optimized both the space memory and the execution time As futures perspectives of this work, we mention 1\o test our approach on several the large data set, and 2\ to define a new intelligent method of evaluation of requests which takes into account the Meta knowledge and/or the knowledge base generated by our KDD model XI  R EFERENCES  1  P. Berkhin, “Survey of clustering data mining techniques“, Technical report, Accrue Software, 2002 2  M. Zaki, “Mining Non-Redundant Association Rules”, Data Mining and Knowledge Discovery, No 9, 2004, p. 223–248 3  G. Stumme, R.Taouil, Y. Bastide, N. Pasquier, and     L. Lakhal Intelligent structuring and reducing of association rules with formal concept analysis”, Proceedings of KI’2001 Conference, Vienna Austria, Lecture Notes in Artificial Intelligence 2174, SpringerVerlag, September 2001, p. 335–350 4  N. Pasquier “Data Mining : Algorithmes d'Extraction et de Réduction des Règles d'Association dans les Bases de Données”, Thèse Département d’Informatique et Statistique, Faculté des Sciences Economiques et de Gestion, Lyon, 2000 5  R. Agrawal, T. Imielinski, and Swami A., “Mining Association Rules between sets of items in large Databases”, Proceedings of the ACM SIGMOD Intl. Conference on Management of Data, Washington USA, June 1993, p. 207-216 6  R. Agrawal, and R. Skirant. “Fast algoritms for mining association rules”. In Proceedings of the 20th Int'l Conference on Very Large Databases, pages 478-499, June 1994 7  N. Pasquier, Y. Bastide, R.Taouil, and L. Lakhal,          “ Efficient Mining of Association Rules Using Closed Itemset Lattices Information Systems Journal, vol. 24, no 1, 1999, p. 25-46 8  M. J. Zaki, and C. J. Hsiao, “ CHARM : An Efficient Algorithm for Closed Itemset Mining ”, Proceedings of the 2nd SIAM International Conference on Data Mining, Arlington, April 2002, p. 34-43 9  G. Stumme, R. Taouil, Bastide Y., Pasquier N., and L. Lakhal, “Fast Computation of Concept Lattices Using Data Mining Techniques BOUZEGHOUB M., KLUSCH M., NUTT W., SATTLER U., Eds Proceedings of 7th Intl. Workshop on Knowledge Representation Meets Databases \(KRDB’00\Berlin, Germany, 2000, p. 129-139 10  G. Stumme, R.Taouil, Y. Bastide, N. Pasquier, and     L. Lakhal Computing Iceberg Concept Lattices with TITANIC”, J. on Knowledge and Data Engineering \(KDE\ vol. 2, no 42, 2002, p. 189222 11  S. Ben Tekaya, S. Ben Yahia, and Y. Slimani. “Algorithme de construction d`un treillis des concepts formels et de détermination des générateurs minimaux”, ARIMA journal, Novembre 2005, Numéro spécial CARI'04, pages: 171-193, 2005 12  T. Hamrouni, S. Ben Yahia, and Y. Slimani. “Prince : Extraction optimisée des bases génériques  de règles sans calcul de fermetures In Proceedings of the Intl. INFORSID Conference, Editions Inforsid Grenoble, France, pages : 353--368, 24-27 May 2005 13  B. Ganter, and R. Wille, Formal Concept Analysis: mathematical foundations. \(translated from the German by Cornelia Franzke Springer-Verlag, Berlin-Hei delberg 1999 14  T.Thanh, H.Siu Cheung, and C. Tru Hoang, “A Fuzzy FCA-based Approach to Conceptual Clustering for Automatic Generation of Concept Hierarchy on Uncertainty Data.” ,CLA 2004, pp. 1–12 ISBN 80-248-0597-9 15  L. Zadeh. Fuzzy sets. Inform ation and Control, \(69\338-353, June 1965 16  M. Sassi, M., A. Grissa Touzi, and H. Ounelli, “ “Interpretting Fuzzy Clustering Results based on Fuzzy Formal Concept Analyis”, IEEE International Conference on Fuzzy Systems. Imperial College London, UK, 2007 17  A. Grissa Touzi, M. Sassi, and H. Ounelli,  “Using Formal Concept Analysis for Flexible Querying Optimization”, 23nd International Conference on Computers and Their  Applications, \(CATA’08 Mexico, Avril 2008 18  A. Grissa Touzi, M. Sassi, and H. Ounelli, “An innovative contribution to flexible query through the fusion of conceptual clustering, fuzzy logic, and formal concept analysis”, International Journal of Computers and Their Applications. Volume. 16, N°. 4, pp 220-233, December, 2009 19  M. Sassi, A. Grissa Touzi, and H. Ounelli, “A Fuzzy Linguistic Database Summarization Approach”, Fuzzy Systems Conference IEEE International Conference on Fuzzy Systems.   Hong Kong, Juin 2008 20  J.C,  Bezdeck,  R.Ehrlich,  and  W.Full,  "FCM: The Fuzzy  C-Means Clustering Algorithm", Computers and Geoscience, vol. 10, no. 2-3 pp. 191–203, 1984 21  N. Pasquier, Y. Bastide, R.Tou il, and L.Lakhal, “Pruning closed itemset lattices for association rules”, Proceedings of 14th International Conference Bases de Données Avancées, Hammamet Tunisia, 26–30 October 1998, p. 177–196 22  M. J. Zaki, “Generating Non-Redundant Association Rules Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Boston, MA, August 2000, p 34-43 23  Y. Bastide, R.Taouil, N. Pasquier, G. Stumme, and L.Lakhal Mining frequent patterns with counting inference”, SIGKDD Explorations, vol. 2, no 2, 2000, p. 66-75 24  B. Ganter, “Two basics algorithms in concept analysis”, Technical report, Darmstadt, 1984 
134 


   


                        





