 Towards Recon\002gurable Processors for Power-Proportional Computing Andrey Mokhov Maxim Rykunov Danil Sokolov Alex Yakovlev School of Electrical and Electronic Engineering Newcastle University UK Abstract 227Today we witness a major shift in microelectronics engineering priorities The cost of computation is no longer dominated by the cost of building computation hardware Almost every consumer electronic device nowadays contains a powerful processor with one or increasingly often multiple computation cores that are cheap to manufacture Growing demand for highde\002nition content and associated processing capabilities revealed the true cost of computation 226 energy Big data companies face large electricity bills in other applications such as healthcare and infrastructure monitoring the energy constraint has a different form instead of a large bill one faces limited energy availability and has to resort to such measures as energyharvesting In both cases it is essential for the computation system to be adaptable to the in\003ow of energy and computation load which often vary dramatically during runtime In this paper we discuss an approach to building processors with recon\002gurable computation pipeline capable of changing the way they fetch and execute instructions depending on energy availability and application requirements We show how to use Conditional Partial Order Graphs to model the microarchitecture of such a processor discuss implementation details and present preliminary experimental results on an example of Intel 8051 microcontroller I I NTRODUCTION From 2007 our society uses more energy for browsing the Internet than for air travel It is also predicted that the energy and environmental footprint of computation and data traf\002c will steadily increase in the near future Data centres will grow and so will the network infrastructure together with the number of terminal nodes of the global information network such as computers phones gadgets and other connected cyber-physical devices so called Internet of Things Energy-ef\002ciency of components at all levels of the computation hierarchy is thus becoming a major concern for the microelectronics industry A serious factor impeding the progress in addressing this concern is a wide gap between the ways of how energy ef\002ciency is approached by hardware and software engineers and this gap is matched by the lack of common understanding between the two communities In this paper we discuss an approach to bridging this gap by developing a shared design criterion called powerproportionality  on the basis of which both electronics and programming solutions can be judged A computing system for it to be considered power-proportional has to keep power consumption and computation load proportional to each other That is an idle system w ould ideally consume no power whereas given a small energy budget the system would adapt itself by reducing its computation 003ow and lowering the delivered Quality-of-Service QoS and still remain functional see Section II for a detailed discussion The state-ofthe-art systems have generally poor power-proportionality for example the servers used in data centres typically consume 50-60 of peak power under 10 of peak load Wh y are modern systems not power-proportional We hypothesise that because they are designed to operate in a narrow scope of conditions typically optimised for either high performance or low power consumption This approach is inherently 003awed because all the design effort is focused on one particular operation mode disregarding others Figure 1 Cross-layer power proportionality Another issue that makes it dif\002cult to handle unpredictable environmental conditions is the absence of a cross-layer feedback-control loop between energy supply components in hardware and an operating system or an active application in software see Figure 1 As a result software is unable to steer or modulate computation with the aim of maximising QoS by prioritising tasks trading accuracy for energy and in general by strategic planning of energy resource utilisation In this paper we discuss cross-layer hardware-software mechanisms for online energy monitoring and recon\002guration with particular focus on design and implementation of a powerproportional microprocessor capable of adapting to varying operating conditions such as low or even unstable voltage level and application requirements in runtime II R ECONFIGURABILITY AND POWER PROPORTIONALITY How can one build a recon\002gurable and adaptable processor The 002rst thought could be to implement it in the 002eldprogrammable gate array FPGA technology allowing static and sometimes dynamic recon\002guration Such a processor would be capable of adjusting its internal structure or behaviour by rewiring interconnections between its components 


 Figure 2 Power proportionality power supply/consumption and service demand/provision interplay or even by changing functionality at the level of individual gates This technology can provide a very 002ne-grain control over a system at runtime however the associated overheads are extremely high In particular in terms of energy consumption FPGAs are typically 10-15 times more expensive than application-speci\002c integrated circuits ASICs Since the 002ne-grain recon\002gurability offered by FPGAs is overly costly one shall consider the coarse-grain recon\002gurable architectures in the ASIC realm They signi\002cantly lower the overheads by dropping recon\002gurability in datapath components and utilising their custom designed versions instead The control logic and interconnect fabric however retain the capability to recon\002guration In this setting the key design and implementation challenge is to formally describe and synthesise the controller whose task is to coordinate hard system resources the datapath components and interconnects according to the runtime information on availability of soft system resources energy time the latter can also include information on hardware faults in a system thereby allowing the controller to bypass faulty components whenever possible The conventional approach to control logic speci\002cation and synthesis is to employ Finite State Machines FSMs or interpreted Petri Nets PNs as an underlying modelling formalism Within this approach the designer explicitly describes the controller's behaviour for each combination of available resources and operating conditions The number of such combinations and corresponding behaviours grows extremely fast with the size and degree of adaptability of the system This leads not only to the state space explosion problem  but also to explosion of the speci\002cation size thus slowing down the synthesis tools reducing productivity and increasing the overall cost of ASIC development Our approach is based on the crucial observation that the controller's behaviours are strongly related to each other in different operating conditions Indeed when a system con\002guration is changed incrementally e.g a datapath component goes of\003ine and another one is used in its place the overall behaviour of the controller is affected in the same incremental manner therefore one would want to have a joint behaviour speci\002cation for these two con\002gurations It has been demonstrated that the FSM and PN formalisms are not well-suited to describing families of many related behaviours and the design m ethodologies based on them have poor scalability in the context of recon\002gurable systems As an appropriate alternative the Conditional Partial Order Graph model was introduced by Mokhov et al   The model was devised to allow implicit description of families of related behaviours in a compact form 226 see Section III Figure 2 illustrates the concept of power-proportionality and its duality On the leftmost chart one can see dependency of power consumption on service demand Ideally the former should grow proportionally to the latter when there is no service demand the system should consume near zero power while any increase in service demand should cause a proportional increase in power consumption until the point of saturation when the system can no longer serve all the incoming requests Our design goal is to reduce the interval of wasted energy as much as possible On the right hand side of Figure 2 one can see the dual chart illustrating dependency of service provision on power supply Indeed an ideal computation system should provide service proportionally to the in\003ow of energy From this perspective our design goal is to build computation systems that can start delivering certain level of QoS on as low power supply as possible thus minimising the interval of unutilised energy The duality of power-proportionality creates a four dimensional space of power-service interplay a computation system can be considered a black box which has two inputs service demand and power supply and two outputs service provision and power consumption as illustrated in the centre of Figure 2 Our combined design goal is therefore to achieve proportionality between the inputs and outputs of the system III C ONDITIONAL P ARTIAL O RDER G RAPHS A Conditional Partial Order Graph  further referred to as CPOG or graph  is a quintuple H   V E X 032 036  where 017 V is a set of vertices which correspond to events or atomic actions in a modelled system 017 E 022 V 002 V is a set of arcs representing dependencies between the events 017 Operational vector X is a set of Boolean variables An opcode is an assignment  x 1  x 2      x j X j  2 f 0  1 g j X j of these variables An opcode selects a particular partial order from those contained in the graph 017 032 2 F  X  is a restriction function where F  X  is the set of all Boolean functions over variables in X  032 de\002nes the operational domain of the graph X can be assigned 


 a Full notation b Simpli\002ed notation Figure 3 Graphical representation of CPOGs only those opcodes  x 1  x 2      x j X j  which satisfy the restriction function i.e 032  x 1  x 2      x j X j   1  017 Function 036   V  E   F  X  assigns a Boolean condition 036  z  2 F  X  to every vertex and arc z 2 V  E in the graph Let us also de\002ne 036  z  df  0 for z  2 V  E for convenience CPOGs are represented graphically by drawing a labelled circle for every vertex and drawing a labelled arrow for every arc The label of a vertex v consists of the vertex name a colon and the vertex condition 036  v   while every arc e is labelled with the corresponding arc condition 036  e   The restriction function 032 is depicted in a box next to the graph operational variables X can therefore be observed as parameters of 032  Fig 3\(a shows an example of a CPOG with j V j  5 vertices and j E j  7 arcs There is a single operational variable x  the restriction function is 032  x   1  hence both opcodes x  0 and x  1 are allowed Vertices f a b d g have constant 036  1 conditions and are called unconditional  while vertices f c e g are conditional and have conditions 036  c   x and 036  e   x respectively Arcs also fall into two classes unconditional arc c  d  and conditional all the rest As CPOGs tend to have many unconditional vertices and arcs we use a simpli\002ed notation in which conditions equal to 1 are not depicted in the graph see Fig 3\(b The purpose of conditions 036 is to switch off some vertices and/or arcs in a CPOG according to a given opcode thereby producing different CPOG projections  An example of a graph and its two projections is presented in Fig 4 The leftmost projection is obtained by keeping in the graph only those vertices and arcs whose conditions evaluate to 1 after substitution of variable x with 1 such projections are conventionally denoted by H j x 1  Hence vertex e disappears shown as a dashed circle  because its condition evaluates to 0  036  e   x  1  0  Arcs f a  d a  e b  d b  e g disappear for the same reason they are shown as dashed arrows  The rightmost projection is obtained in the same way with the only difference that variable x is set to 0  it is denoted by H j x 0  respectively Note that although the condition of arc c  d evaluates to 1 in fact it is constant 1  the arc is still excluded from the resultant graph because one of the vertices it connects viz vertex c  is excluded and naturally an arc cannot appear in a graph without one of its vertices Each of the obtained projections can be regarded as speci\002cation of a particular behavioural scenario Figure 4 CPOG projections H j x 1 left and H j x 0 right of the modelled system e.g as speci\002cation of a processor instruction Potentially a CPOG H   V E X 032 036  can specify an exponential number of different instructions each composed from atomic actions in V  according to one of 2 j X j different possible opcodes IV I MPLEMENTATION In this section we give a brief outline for our powerproportional implementation of Intel 8051 microcontroller designed using the CPOG model A Design overview The instruction set of Intel 8051 processor contains 255 instructions and our implementation supports all of them We generally follow the standard 8051 architecture with two onchip RAMs the program is stored in a reprogrammable offchip ROM However several important changes were made 017 Our implementation is asynchronous which gives certain advantages over clocked design in case of varying operating conditions The communication between the control logic and the datapath units is arranged by means of request and acknowledgement signals 017 We use adjustable delay lines to achieve robust operation in a wide range of supply voltages Subsection IV-B 017 We can choose how to execute an instruction depending not only on application or environmental aspects but also on the functional correctness of individual components thus addressing the issues of fault tolerance The overall CPOG speci\002cation of the microcontroller is shown in Figure 5 As explained in Section III vertices correspond to datapath components and arcs model dependencies between them see for details on speci\002c components B Adaptability features The most power and time consuming datapath components are adders multipliers and dividers To enable powerproportionality in a wide range of power supply and service demand conditions we designed two sets of these arithmetic units one optimised for low energy consumption and the other one optimised for high performance Depending on whether 


 Figure 5 CPOG model of Intel 8051 control logic there is a shortage of energy or on any other restrictions imposed by an active application we can choose the most appropriate datapath unit to be used during an instruction execution The decision can be made at different levels of system control software level sensors external signals etc Figure 6 Adjustable delay lines Our implementation of self-timed datapath components is based on bundled-data approach where each component is accompanied with a matching delay line to signal completion of its computation In order to correctly function in a wide range of operating conditions e.g supply voltage or temperature a component needs to adjust the latency of its completion signal We propose to address this issue by use of adjustable delay line  whose latency is selectable by the delay code in runtime as shown in Figure 6 C Experiments We implemented the design in STMicroelectronics 130nm technology process and conducted preliminary experiments to estimate the level of adaptability we can achieve using the discussed approach Here we brie\003y report the results The nominal voltage is 1.2V but the chip is capable of operating in the range from 0.2V to 1.5V of supply voltages 017 0.89V to 1.5V  full capability mode 017 0.74V to 0.89V  at 0.89V the RAM starts to fail so the chip can only operate using internal registers 017 0.22V to 0.74V  at 0.74V the program counter starts to fail so the chip gets stuck at executing the same instruction forever Surprisingly the rest of the control logic synthesised using the CPOG model continues to operate correctly down to 0.22V In terms of performance we measured the range from 67 millions of instructions per second on nominal voltage 1.2V down to 2700 instructions per second at 0.25V V C ONCLUSIONS We discussed an approach to architecting powerproportional computation systems The approach was tested by designing and implementing the Intel 8051 microcontroller capable of operating in the range from 0.2V to 1.5V of supply voltages Our future research is to push the boundaries of powerproportionality even further by using the self-timed SRAM capable of operating at low voltages as well as by implementing the key control structures such as program counter using high-reliability logic with low fan-in gates A CKNOWLEDGEMENT This work was supported by EPSRC grant EP/I038357/1 eFuturesXD project PowerProp R EFERENCES  A Baz D Shang F  Xia and A Y ak o vle v  Self-T imed SRAM for Ener gy Harvesting Systems In Journal of Low Power Electronics  2011  J Cortadella M Kishine vsk y  A K ondratye v  L La v agno and A Yakovlev Logic synthesis of asynchronous controllers and interfaces  Advanced Microelectronics Springer-Verlag 2002  Cisco Systems Inc Entering the Zettabyte Era V isual Netw orking Inde x Forecast and Methodology 2010-2015 2011  Da vid Mei sner  Christopher M Sadler  Luiz Andr\351 Barroso W olfDietrich Weber and Thomas F Wenisch Power management of online data-intensive services In Proceedings of the 38th annual International Symposium on Computer Architecture ISCA'2011  pages 319\226330 2011  A Mokho v  A Iliaso v  D Sok olo v  M Rykuno v  A Y ak o vle v  and A Romanovsky Synthesis of Processor Instruction Sets from High-Level ISA Speci\002cations IEEE Transactions on Computers  2013 in print  A Mokho v and A Y ak o vle v  Conditional P artial Order Graphs Model Synthesis and Application IEEE Transactions on Computers  59\(11 2010  Ste v en No wick Automatic Synthesis of Burst-Mode Asynchronous Controllers  PhD thesis Stanford University 1993  Ale x Y ak o vle v  Ener gy-modulated computing In Design Automation and Test in Europe DATE conference  pages 1340\2261345 2011 


applicable in a variety of domains, its accuracy and robustness and the analytical form of the model at the output An important algorithmic issue in data mining is how to find the optimal complexity of the model or the fitting function. Too much complexity in the model can result in model overfit, whereas not enough complexity can result in an underfit. The mathematical foundations of MineTool are based on considerations to balance the competing dangers of underfit and overfit to identify the level of model complexity that guarantees the best out-of-sample prediction performance without ad hoc modifications to the fitting algorithms themselves[19  2 0  Min e T o o l creates a  predictive model architecture that is linear in the parameters. The algorithm searches for a model M that best relates rows of the input variable values Xij to the appropriate target value yi \(yi = M\(Xij\here i = 1,…,N and j = 1,…,K. The model parameters are either linear combinations of the input \(Xi where prime indicates transpose of the vector, index i refers to the i th  observation   linear transformations of the input variables  Xi   or highly non-linear transformations of the input Xi  Equation \(1\ describes the general form of a MineTool model, and the specifics of the method in given in [14  11   Q P i pq y     iipiqq X X X  1. MineTool Model Equation In its simplest form, the model would be a linear combination of the input parameters \(i.e., a linear regression model\eTool goes beyond a simple linear model by introducing the linear \(such as level-1 and level-2 transformations producing cross-products, ratios, squares cubes etc.\d non-linear transformation of the input variables, if their addition increases the model accuracy The non-linear transforms are single hidden layer feed forward ANN-like transforms, just like the ANNs of the same architecture, with the difference that the non-linear transformed inputs are combined into a linear model Metafeature and Global Feature Detection To be able to process a \(time\ataset \(represented with multiple rows of data describing one instance or observation\ using MineTool, the data needs to be “flattened,” or made static This needs to be accomplished without losing the important information incorporated in sequential measurements varying with time. Historically, this has been done either by summarizing the data and writing only the mean of the different row values of one observation, or recording the difference between the pairs of rows and then treating them as single instance entries. These techniques work somewhat well on just a limited set of time series problems. For real life, complex scientific datasets, these approaches are most often too weak to incorporate the important time changes in the data. The MineTool-TS solution to this problem is to collect the important time-changing information that can occur in one of the time series variables. While a value varies with time, it most often increases, decreases or stagnates. There are other, more complex features one can record, that consist of the three basic changes, such as bipolar signature \(relevant in case of flux transfer events where a value goes up, then goes down crossing the axis and goes up again \(the sinusoid function demonstrates the bipolar behavior, for example\eatures, just like the metafeatures, are used to extract the information from all the rows representing one observation. Global features describe one instance of rows using one measurement, such as: the maximum value, minimum value, mean, mode or the number of zero crossings. The metafeatures and global featu res included in the MineTool-TS algorithm are listed here \(all used in the analys except for the bipolar feature  Increasing Metafeature An increasing metafeature is recorded for all the consecutive rising time-series measurements. For each increasing event, we record its start point, duration, gradient and average value, so that the increasing events can be used for analysis and comparison Decreasing Metafeature A decreasing metafeature is recorded for all the consecutive reducing time-series measurements. For each decreasing event, similar to the increasing events, we record starting point, duration gradient \(which is negative in this case\d average value Plateau Metafeature A plateau metafeature is recorded for all the consecutive non-changing time-series measurements. MineTool-TS allows for a small amount of noise to be ignored, so that the true plateaus are captured Bipolar Signature Metafeature A bipolar signature metafeature is recorded for all the consecutive time-series measurements that increase, decrease and cross the zero and increase again Global Minimum For each single variable, the global minimum feature extracts the minimum value of all of the time observations belonging to one time series instance for the variable, and records it as the global minimum feature for that input channel Global Maximum The maximum value of all of the time observations belonging to one time series instance for the variable is recorded as the global maximum feature for that variable Mean The average value of all of the time observations belonging to one time series instance for the variable is recorded as the global mean feature for that specific variable Mode The mode value of all of the time observations belonging to one time series instance for the variable is recorded as the global mode feature for that specific input variable Number of Zero Crossings Lastly, the number of zero crossings occurring during the time observation recorded measurements is written down as the number of zero crossings global feature Next, once all the requested features are collected, the MineTool-TS algorithm performs the feature space segmentation to group similar features and makes them have a higher predictive value for data mining. More details on the algorithm can be found in  D  Modelling Results We provided a three-level predictive analysis of power usage: static analysis \(where all the sensor information 661 


entries are thought of as independent instances\ and time series \(where the entries are considered a part of a time series, i.e. time dependent measurements\or all three datasets \(CogSci, PAC Hall and SDSC Building data\he three buildings have different usage profiles, different schedules, occupancy and energy needs In the static case, we compared the results of static MineTool to a multilayer perceptron method \(an artificial neural network \(ANN\method   In th e cas e of  time series analysis, we used the MineTool-TS method described above To prepare the data for time series analysis, the dataset was treated as a set of one-hour long streams of data Below, we demonstrate the results of the static vs. time series analysis and show the accuracy achieved in predicting the energy usage for each of the different buildings Dataset 1 – CogSci Building Data  Table 1 demonstrates the results of the static vs. time series analysis for the first dataset, the CogSci building data.  We used the 10-fold cross validation evaluation and the table lists the standard numerical prediction evaluation measures: CC mean absolute error\d RMSE \(root mean squared error From the multivariate time series classification analysis point of view, out of all the standard metafeatures \(such as increment, decrement and plateau features\ and the global features \(such as mean, number of zero crossings\he best performing feature chosen by the algorithm was the mean as well as the plateau feature with a relaxed noise level This is due to the fact that the fluctuations within an hour are minimal, and there are no distinctive increases and decreases within the one hour long streams that have a high predictive capability Figure 3 demonstrates a part of the time series prediction model represented in a tree form. We can see that workday, hour of the day, month of the year and the temperature all seem to have a high predictive ability in forecasting the usage, and that the workday seems to be the highest determining factor of the levels of power usage Table 1 demonstrates that treating the data as time series indeed leads to stronger model accuracy. MineTooTS achieved the correlation coefficient of 0.9, as tested using 10-cross validation, whereas the static analysis only yielding accuracy at the random guessing level of below 0.49 and 0.44 TABLE 1. COMPARATIVE ANALYSIS OF POWER USAGE MODELS FOR THE COGSCI DATASET  Analysis Method CC MAE RMSE Static MineTool 0.4939 3.4812 7.3371 Static ANN 0.4436 6.8216 8.5553 Time Series MineTool 0.9013 2.6971 3.8941   Figure 3. Part of the CogSci Power Usage Prediction Model  We believe that adding more measurements, such as the occupancy, class schedule and similar information to the current data would increase the accuracy of the models Furthermore, campus events, cloud coverage and similar variables would, we believe would increase the predictive accuracy even further  Dataset 2 – PAC Hall Data  We provided a slightly different two-level analysis of the PAC Hall dataset. To be able to compare the time series analysis to the standard data mining, we preprocessed the second building data by averaging the values within each hour stream of the data Table 2 illustrates the results of the static averaged data vs time series analysis for this dataset.  We also used the 10fold cross validation evaluation and the table lists the standard numerical prediction evaluation measures: CC mean absolute error\d RMSE \(root mean squared error Table 2 above confirms that treating the data as time series leads to stronger model accuracy.  MineTool-TS achieved the accuracy of 0.95 correlation coefficient, tested using 10-cross validation, whereas the static analysis performed on the hour averages data yielded 0.64 CC accuracy Figure 4 demonstrates a part of the time series prediction model represented in a tree form. We can see that in the case of PAC Hall data, the power usage has changed from year 2011 to 2012, and year 2013 as well, as the predictive analysis output shows a distinct submodel for 2011. In the case of this building, particular day of the week seems to have a higher predictive ability as well, not just a workday as in the case of the previous dataset.  Hour of the day, month of the year and the temperature all still seem to have a high predictive ability in assessing the usage as in the CogSci dataset case TABLE 2. COMPARATIVE ANALYSIS OF POWER USAGE MODELS FOR THE PAC HALL DATASET  Analysis Method CC MAE RMSE Static ANN 0.6354 3.8081 5.1177 Time series MineTool 0.9508 1.3368 2.0348  662 


TABLE 3. COMPARATIVE ANALYSIS OF POWER USAGE MODELS FOR THE SDSC DATASET Analysis Method CC MAE RMSE Static ANN 0.7589 63.927 95.2282 Time series MineTool 0.9736 8.989 9.2737 Dataset 3 – SDSC Building Data  Using the same twolevel analysis as in the case of previous building, we obtained the SDSC power usage data analysis results shown in Table 3 Table 3 above reiterates that multi variate time series analysis is indeed over-performing static analysis of the data. The time series model achieved 0.97 correlation coefficient accuracy, while the static models achieved 0.76 CC accuracy Figure 5 demonstrates a part of the time series prediction model represented in a tree form. Power usage patterns changed from year 2011 to 2012 and 2013 in this building as well, and the day of the week, not just workday as well as the hour are important predictors of power usage In summary, predictive analytics of energy big historical usage data obtained excellent predictive results of power usage in kW, in three building of the UCSD micro grid system.  The multivariate time series classification produced excellent models \(accuracy yielding 0.9-0.97 CC as compared to the standard data mining analysis where the data is treated as static, independent rows, and instances are processed as independent examples, or averaged over an hour of streaming data.  The models demonstrate that the time of the day, workday, day of the week, month, hour and outside temperature are all excellent predictors of the power usage per building on the micro grid.  Put in the global perspective, this initial analysis of big data will enable us to be closer to predicting power usage and lower the cost of energy of the entire campus microgrid F IGURE 4  P ART OF THE PAC  H ALL P OWER U SAGE M ODEL   Figure 5. Part of the SDSC Power Usage Model V  C ONCLUSION  UCSD is committed to acquiring alternative forms of energy generation, with the goal of self-sufficiency and main grid independence by 2016. We are utilizing smart grid data with advanced forecasting and a “big data analytics platform leveraging available large scale data and HP system in order realize tangible improvements in energy efficiency and cost.  We are further exploring clustering methods, real time and active learning models employed for predicting peaks and potential outages.   As data is rapidly growing, the stream mining and predictions become futher challenging due to the vast volume and speed of data.   Future work will involve scalling of the ongoing efforts to efficiently model the campus in its entirety and evalutae the system in real-time.  As model complexity and data granularity increase the need for large memory nodes and HDFS rise.  Investigating the parallelization of the worksflows and Minetool on a number of different platforms is be the focus of the future work.  This initial real-life case study has shown clear value through the big data analytics in both efficiencies and cost savings.    Applying such approaches to the available large scale sensor data in the microgrid environment enables informed, real-time decisions and enhancements resulting in a truly intelligent and optimized microgrid 663 


A CKNOWLEDGEMENT  The authors would like to personally thank Byron Washom, director of Strategic Energy Initiatives; John Dilliott, manager of Energy and Utilities; and Robert Austin, administrator of Energy Management Systems at UCSD; Bob Caldwell, President of Centaurus Prime and a UCSD microgrid consultant who were instrumental in providing access to campus energy data; as well as our funding agencies, i.e., the National Science Foundation and the California Energy Commission R EFERENCES  1  U.S. Department of Energy Smart Grid / Department of Energy  2  http://www.smartgrid.gov/the_smart_grid#smart_grid 3  Washom, B., Dilliott, J., Weil, D., Kleissl, J., Balac, N Torre, W., Richter, C., "Ivory Tower of Power: Microgrid Implementation at the University of California, San Diego Power and Energy Magazine, IEEE vol.11, no.4, pp. 28,32 July 2013. doi: 10.1109/MPE.2013.2258278 4  Strande, S.M., Cicotti, P., Sinkovits, R.S., William, S Young, W.S., Wagner, R., Tatineni, M., Hocks, E., Snavely A., Norman, M. 2012. Gordon: Design, performance, and experiences deploying and supporting a data intensive supercomputer. In Proceedings of the 1st Conference of the Extreme Science and Engineering Discovery Environment Bridging from the eXtreme to the Campus and Beyond  XSEDE '12\. ACM, New York, NY, USA, Article 3, 8 pages. DOI=10.1145/2335755.2335789 5  Dean, J., Ghemawat, S. 2008. MapReduce: simplified data processing on large clusters Commun. ACM 51, 1 \(January 2008\, 107-113. doi: 10.1145/1327452.1327492 http://doi.acm.org/10.1145/1327452.1327492 6  http://www.r-project.org  7  http://mahout.apache.org  8  http://hive.apache.org  9  IBM What is big data? — Bringing big data to the enterprise 01.ibm.com 10  Solomon, D., Winter, R.L., Boulanger, A.G., Anderson R.N., Wu, L.L., \(2011 Forecasting Energy Demand in Large Commercial Buildings Using Support Vector Machine Regression Technical Report Columbia University Computer Science Technical reports; CUCS-040-11; New York; http://hdl.handle.net/10022/AC:P:12171  11  Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann P., Witten, I.A. \(2009\; The WEKA Data Mining Software An Update; SIGKDD Explorations, Volume 11, Issue 1 12  Breiman, L., Friedman, J., Stone, C. J., Olshen, R. A. \(1984 Classification and regression trees Chapman & Hall/CRC 13  Kalogirou S.A. 2000. Applications of artificial neuralnetworks for energy systems Applied Energy Volume 67 Issues 1–2 September 2000, Pages 17–35 14  Karimabadi, H., Sipes, T. B., White, H., Marinucci, M Dmitriev, A., Chao, L.K., Driscoll, J., Balac, N 2007\.  Data Mining in Space Physics: 1. The MineTool Algorithm J. Geophys. Res 112, A11215 doi:10.1029/2006JA012136 15  Myers, C. S. and Rabiner, L. R.  A comparative study of several dynamic time-warping algorithms for connected word recognition. The Bell System Technical Journal 60\(7\:1389-1409, September 1981 16  Rabiner, L. R. and Juang, B. H.  An introduction to hidden markov models.  IEEE Magazine on Acoustics, Speech and Signal Processing, 3\(1\4-16, 1986 17  Schmidhuber, J., Graves, A., Gomez, F. and Hochreiter, S Recurrent Neural Networks, Cambridge University Press 2012 18  Kadous, M. W Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series PhD thesis, School of Computer Science & Engineering University of New South Wales, 2002 19  Pérez-Amaral, T., Gallo, G. M. and White, H.,  A Comparison of Complementary Automatic Modeling Methods: RETINA and  PcGets Econometric Theory 2005 20  White, H., Personnel Readiness: Neural Network Modeling of Performance-Based Estimates Final Report to the Office of Naval Research, Contract #: N00014-95-C-1078 1999 21  technique: Automated detection of flux transfer events using Cluster data J. Geophys. Res  Vol 114, A06216 doi:10.1029/2009JA014202, 2009  22  White, H., Personnel Readiness: Neural Network Modeling of Performance-Based Estimates Final Report to the Office of Naval Research, Contract #: N00014-95-C-1078 1999  664 


 10  Acknowledgments  The authors are deeply indebted to Isabelle Guyon and her team for designing the problematics of the Causality Challenge, and for providing the Lucas0 test data  11  References  1 A g ra wa l, R. Srik a n t, H   Fa st a l g o rithm s f o r m i ning  association rules in large databases Research Report RJ 9839 IBM Almaden Research Center, San Jose, California June 1994  2 Ba stide Y Data mining : algorithmes par niveau techniques d'implantation et applications Doctoral dissertation, Université Blaise Pascal, Clermont-Ferrand 2000  3 Botta M., Bo ulic a u t J.-F  Ma sson C., Me o R A Comparison between Query Languages for the Extraction of Association Rules DaWaK 2002 1-10  4 C a do t, M   2 0 06   Extraire et valider les relations complexes en sciences humaines : statistiques, motifs et règles d'association Doctoral dissertation, University of Franche-Comté, France. Available online at http://www.loria.fr/~cadot/cadot_these_2006.pdf  5 C a dot M., Ma j  J  B  Zi a d  T   2005 A s s o c i a tion  Rules and Statistics, in J. Wang \(Ed Encyclopedia of Data Warehousing and Mining pp. 74-77\. Hershey, US, Idea Group Publishing  6 Ch e n Q Mining Exceptions and Quantitative Association Rules in Olap Data Cube Master Thesis, Simon Fraser University, 1999   F a b r is C C  A  A  F r eitas Disco v er y o f su rp risin g  patterns by detecting occurrences of Simpson's paradox Research and d eveloppement in intelligent systems XVI Proc ES99. The 19th SGES Int. Conf. on Knowledge-based systems and applied artificial intelligence\. 148-160 Springer-Verlag, 1999  8 F u Y  Discovery of multiple-Level Rules from large Databases Master Thesis, Simon Fraser University, 1996  9 G u ig ue s J  L  e t D uque nne  V   19 86 Fa m ille s  m i ni m a le s  d'implications informatives résultant d'un tableau de données binaires Math. Sci. Hum n°95, 5-18  10 G u ille t F., Ha m ilton H., \(200 7 Quality Measures in data mining Springer  11 G u y on I., Ca usa lity Cha lle ng e 1: Ca usa tio n a n d  Prediction, 2008 http://www.causality.inf.ethz.ch/challenge.php  12 H o c J.-M L'analyse planifiée des données en psychologie PUF, Paris, 1983  13 Ho we l D  C    Statistical Methods for Psychology  Duxbury, A Division of International Thomson Publishing Inc., 1997  1 Jaku l i n A    Attribute Interactions in Machine Learning  Master's thesis University of Ljubljana, Slovenija, 2003  15 L u x e nbur g e r M I m plic a tions  pa r tie lle s da ns u n  contexte Mathématiques, Informatique et Sciences humaines n°113,  35-55, 1991  1 P earl  J   Causality models, reasoning, and inference  Cambridge University Press, 2000, 267 - 279  17 Sim p son  E. H., T h e inte r p re ta tion of inte ra c tion in  contingency tables Journal of the Royal Statistical Society  Series B, 13, 238-241, 1951  18 Suz u k i E., K odr a t of f Y   D i s c ov e r y of Sur p r i s i ng  Exception Rules Based on Intensity of Implication Second European Symposium on Principles of Data Mining and Knowledge Discovery Springer-Verlag, London, UK. Source Lecture Notes In Computer Science. 1998, p. 10-18  19 T o iv o n e n H Kle m e n ttine n  M., R onk a i ne n  P   Ha t ne n  K., Mannila H., Pruning and Grouping Discovered Association Rules ECML'95   20 W i ne r B  J B r ow n D  R   Mic h e l s K  M Statistical principles in experimental design third edition\w York McGraw–Hill, 1991  21 Za k i M., Mini ng N onR e dun da nt A s s o c i a tion R u le s   Data Mining and Knowledge Discovery 9, 223-248, 2004 Kluwer Academic Publishers, Netherlands  22 Zh u  H  On-Line Analytical Mining of Association Rules Master Thesis, Simon Fraser University, 1998  
98 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





