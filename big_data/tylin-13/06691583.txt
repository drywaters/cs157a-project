Direct QR factorizations for tall-and-skinny matrices in MapReduce architectures Austin R Benson Institute for Computational and Mathematical Engineering Stanford University Stanford CA arbenson@stanford.edu David F Gleich Department of Computer Science Purdue University West Lafayette IN dgleich@purdue.edu James Demmel Computer Sciences Division and Department of Mathematics University of California Berkeley Berkeley CA demmel@cs.berkeley.edu Abstract The QR factorization and the SVD are two fundamental matrix decompositions with applications throughout scientiìc computing and data analysis For matrices with many more rows than columns socalled tall-and-skinny matrices there is a numerically stable ecient communication-avoiding algorithm for computing the QR factorization It has been used in traditional high performance computing and grid computing environments For MapReduce environments existing methods to compute the QR decomposition use a numerically unstable approach that relies on indirectly computing the Q factor In the best case these methods require only two passes over the data In this paper we describe how to compute a stable tall-andskinny QR factorization on a MapReduce architecture in only slightly more than 2 passes over the data We can compute the SVD with only a small change and no dierence in performance We present a performance comparison between our new direct TSQR method indirect TSQR methods that use the communicationavoiding TSQR algorithm and a standard unstable implementation for MapReduce Cholesky QR We nd that our new stable method is competitive with unstable methods for matrices with a modest number of columns This holds both in a theoretical performance modelaswellasinanactualimplementation Keywords matrix factorization QR SVD TSQR MapReduce Hadoop I Introduction The QR factorization of an m  n real-valued matrix A is A  QR where Q is an m  n orthogonal matrix and R is an n  n upper triangular matrix It is a fundamental subroutine in many advanced data analysis procedures including principal components analysis linear regression and general linear models We call a matrix tall-and-skinny if it has many more rows than columns  m  n  This case is common in big data applications with billions of data points with only a few hundred descriptors In practice this means that it is cheap to distribute O  n 2  data to all processors and cheap to perform O  n 3  oating point operations in serial In this paper we study algorithms to compute a QR factorization of a tall-and-skinny matrix for nearlyterabyte sized matrices on MapReduce architectures Previous work by one of the authors gave a fast MapReduce method to compute only R  The details of these are described further in Sec II In order to compute the matrix Q  an indirect formulation is used Q  AR  1  For R to be invertible A must be full-rank and we assume A is full-rank throughout this paper The indirect formulation is known to be numerically unstable Numerically stable algorithms are important because they ensure that the algorithm behaves predictably regardless of the properties of the input This feature is vital for implementing algorithmic libraries that handle the diversity of data input in real-world applications We refer readers to the text by Higham for more on the numerical properties of various algorithms  The simple process of repeating the algorithm which is called iterative reìnement can sometimes be used to produce a Q factor with acceptable accuracy H o w ev er i f a matrix is suciently ill-conditioned iterative reìnement will still result in a computed matrix Q that is not orthogonal and hence is not nearly the QR factorization from any matrix We shall describe a numerically stable method Sec III that computes Q and R directly in approximately the same time as performing the repetition of the indirect computation for some matrices In Sec IV-A we present a performance model for our algorithms on a MapReduce cluster which allows us to compute lower bounds on running times Real world performance is almost always within a factor of two of the lower bounds Sec IV-B A MapReduce motivation The data in a MapReduce computation is deìned by a collection of key-value pairs When we use MapReduce to analyze tall-and-skinny matrix data a key represents the identity of a row and a value represents the elements in that row Thus the matrix is a collection of key-value pairs We assume that each row has a distinct key for 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 264 


simplicity although we note that our methods also handle cases where each key represents a set of rows There are a growing number of MapReduce frameworks that implement the same computational engine rst map applies a function to each key-value pair which outputs a transformed key-value pair second shue rearranges the data to ensure that all values with the same key are together nally reduce applies a function to all values with the same key The most popular MapReduce implementation  Hadoop   s tores a ll data and i n t ermediate computations on disk Thus we do not expect numerical linear algebra algorithms for MapReduce to be faster than state-of-the-art in-memory MPI implementations running on clusters with high-performance interconnects However the MapReduce model oers several advantages that make the platform attractive for large-scale large-data computations see also for i nformation on tradeos First many large datasets are already warehoused in MapReduce clusters With the availability of algorithms such as QR on a MapReduce cluster these data do not need to be transferred to another cluster for analysis In fact a simple corollary of our analysis is the performance of the algorithms is largely bounded by simply reading and writing the data in the MapReduce cluster indicating that even using an MPI cluster for the computation would not greatly reduce running time Second MapReduce systems like Hadoop provide transparent fault-tolerance which is a major beneìt over standard MPI systems Other MapReduce implementations such as Phoenix LEMOMR and M RMPI 15 o ften store d ata i n memory and may be a great deal faster although they usually lack the automatic fault tolerance Third the Hadoop computation engine handles all details of the distributed input-output routines which greatly simpliìes the resulting programs For the majority of our implementations we use Hadoop streaming and the Python-based Dumbo MapReduce interface  T hese programs a re concise straigh t forw ard and easy-to-adapt to new applications We have also investigated C and Java implementations but these programs oered only mild speedups around 2-fold if any The Python implementation uses about 70 lines of code while the C implementation uses about 600 lines of code B Success metrics Our two success metrics are speed and stability The dierences in speed are examined in Sec IV-B To analyze the performance we construct a performance model for the MapReduce cluster After tting two parameters to the performance of the cluster it predicts the runtime to within a factor of two We study stability in an expanded online version of this manuscript 1 These results show that only our new direct TSQR method produces a matrix Q that is numerically orthogonal 1 Available from http://arxiv.org/abs/1301.1071 II Indirect QR factorizations in MapReduce One of the rst papers to explicitly discuss the QR factorization on MapReduce architectures was written by Constantine and Gleich ho w e v e r m an y h ad studied methods for linear regression and principal components analysis in MapReduce  T hese metho d s a ll b e ar a c lose resemblance to the Cholesky QR algorithm we describe next A Cholesky QR The Cholesky factorization of an n  n symmetric positive deìnite real-valued matrix A is A  LL T where L is an n  n lower triangular matrix Note that for any A that is full rank A T A is symmetric positive deìnite The Cholesky factor L for the matrix A T A is exactly the matrix R T in the QR factorization as the following derivation shows Let A  QR Then A T A  QR  T QR  R T Q T QR  R T R Since R is upper triangular and L is unique R T R  LL T  The method of computing R via the Cholesky decomposition of A T A matrix is called Cholesky QR  Thus the problem of nding R becomes the problem of computing A T A  This task is straightforward in MapReduce In the map stage each task collects rows  recall that these are key-values pairs  to form a local matrix A i and then computes A T i A i  These matrices are small n  n andareoutputbyrow.Infact A T i A i is symmetric and there are ways to reduce the computation by utilizing this symmetry We do not exploit them because disk access time dominates the computation a more detailed performance discussion is in Sec IV In the reduce stage each individual reduce function takes in multiple instances of each row of A T A from the mappers These rows are summedtoproducearowof A T A Formally,thismethod computes A T A  map tasks  i 1 A T i A i where A i is the input to each map-task Extending the A T A computation to Cholesky QR simply consists of gathering all rows of A T A on one processor and serially computing the Cholesky factorization A T A  LL T  The serial Cholesky factorization is fast since A T A is small n  n  The Cholesky QR MapReduce algorithm is illustrated in Fig 1 It is important to note the architecture limitation due to the number of columns n  The number of keys emitted byeachmaptaskisexactly n 0,1 n  1 one for each row of A T i A i  and the total number of unique keys passed to the reduction stage is n  Thus the row sum reduction stage can use at most n tasks Alternatively the reduce function can emit a key-value pair where the key represents the row and column index 265 


A  A 1  A 1 T A 1  map emit A 2  A 2 T A 2  map emit A 3  A 3 T A 3  map emit A 4  A 4 T A 4  map emit shuffle A 1 T A 1  1 A 2 T A 2  1 A 3 T A 3  1 A 4 T A 4  1 A T A 1 A 1 T A 1  2 A 2 T A 2  2 A 3 T A 3  2 A 4 T A 4  2 A T A 2 A 1 T A 1  3 A 2 T A 2  3 A 3 T A 3  3 A 4 T A 4  3 A T A 3 A 1 T A 1  4 A 2 T A 2  4 A 3 T A 3  4 A 4 T A 4  4 A T A 4 emit emit emit emit A T A R T R emit reduce reduce reduce reduce Local A T A Row Sum Cholesky Fig 1 MapReduce Cholesky QR computation for a matrix A with 4 columns of a given entry of A T i A i  and the value is the given entry This increases the number of unique keys to n 2 or by taking symmetry into account n  n  1 It is also valid to use more general reduction trees where partial row sums are computed on all the processors and a reduction to n processors accumulates the partial row sums The cost of this more general tree is the startup time for another map and reduce iteration Typically the extra startup time outweighs the beneìt of additional parallelism Each of these variations of Cholesky QR can be described by our performance model in Sec IV-A For experiments,weuseasmallcluster\(whereatmost40reduce tasks are available and these design choices have little eect on the running times We use the implementation where the reduce function takes in rows of A T A as it is the simplest B Indirect TSQR One of the problems with Cholesky QR is that the matrix A T A has the square of the condition number of the matrix A  This suggests that nite precision computations with A T A will not always produce an accurate R matrix For this reason Constantine and Gleich studied a succinct MapReduce implementation  of the c omm unicationavoiding TSQR algorithm by Demmel et al where m ap and reduce tasks both compute local QR computations This method is known to be numerically stable a nd was recently shown to have superior stability to many standard algorithms  C onstan tine and G leic hês i nitial implementation is only designed to compute R  We will refer to this method as Indirect TSQR because Q may be computed indirectly with Q  AR  1 InSec.III,we extend this method to also compute Q in a stable manner We will now brieîy review the Indirect TSQR algorithm and its implementation to facilitate the explanation of the more intricate direct version Let A beamatrixthatÖfor simplicity of explanation  has 8 n rows and n columns which is partitioned across four map tasks as A      A 1 A 2 A 3 A 4      Each map task computes a local QR factorization A      Q 1 Q 2 Q 3 Q 4      012  8 n  4 n     R 1 R 2 R 3 R 4      012  4 n  n  The matrix of stacked upper triangular matrices on the right is then passed to a reduce task and factored into  Q  R Atthispoint,wehavetheQRfactorizationof A in product form A   Q   012     Q 1 Q 2 Q 3 Q 4      012  8 n  4 n  Q 012 4 n  n  R   012  R 012 n  n  The above construction generalizes to the case that A is not partitioned evenly If A is m  n  in the rst step a QR decomposition is computed on the block of A that is streamed to a map task The Indirect TSQR method ignores the intermediate Q factors and simply outputs the n  n factors R i in the intermediate stage and  R in the nal stage Fig 2 illustrates each map and reduce output We do not need to gather all R factors onto a single task to compute  R  Any reduction tree computes  R correctly Constantine and Gleich found that using an additional MapReduce iteration to form a more parallel reduction tree could greatly accelerate the method This nding diers from the Cholesky QR method where additional iterations rarely helped In the Sec III we show how to save the Q factors to reconstruct Q directly C Computing AR  1 Given the matrix R  the simplest method for computing Q is computing the inverse of R and multiplying by A that is computing AR  1 Since R is n  n and upper-triangular we can compute its inverse quickly Fig 3 illustrates how the matrix multiplication and iterative reìnement step cleanly translate to MapReduce This indirect method of the inverse computation is not backwards stable for example see Th us a s tep o f i terativ e reìnemen t ma y be used to get Q within desired accuracy However the indirect methods may still have large errors after iterative reìnement if A is ill-conditioned enough This further motivates the use of a direct method 266 


S 1  A  A 1  A 2  A 3  A 3  R 1  map A 2  emit R 2  map A 3  emit R 3  map A 4  emit R 4  map shuffle S 1  A 2  reduce S 2  R 2,2  reduce R 2,1  emit emit emit shuffle A 2  S 3  R 2,3  reduce emit Local TSQR identity map A 2  S 2  R reduce emit Local TSQR Local TSQR Fig 2 MapReduce TSQR computation with a 2-stage reduction tree S 1 is the matrix consisting of the rows of the R i factors stacked on top of each other i 1  2  3  4 Similarly S 2 is the matrix consisting of the rows of the R 2 j factors stacked on top of each other j 1  2  3 A A 1  R 1 map R Q 1  A 2  R 1 map Q 2  A 3  R 1 map Q 3  A 4  R 1 map Q 4  A A A A A emit emit emit emit distribute TSQR Q Q 1  R 1 1 map R 1 Q 1  Q 2  R 1 1 map Q 2  Q 3  R 1 1 map Q 3  Q 4  R 1 1 map Q 4  Q Q Q Q Q emit emit emit emit distribute TSQ Q R dis tr ibu t e Local MatMul Local MatMul Iterative Refinement step Fig 3 Indirect MapReduce computation of Q with iterative reìnement D Pseudo-Iterative Reìnement A variety of fast randomized algorithms have recently been developed for least squares problems 12  16 A key idea from this work is that the R factor from the QR factorization of a small random subset of the rows of A isaìrst-orderapproximationtothe R factor of the entire matrix For tall-and-skinny matrices the number of rows sampled grows as approximately 100 n log n  The standard iterative reìnement procedure will compute the R factor of two m  n matrices the original matrix A and the approximate Q factor AR  1  We take a sampling approach to compute the R factor of only one m  n matrix and one smaller matrix To approximate random sampling we read a single block of the matrix from Hadoop Distributed File System HDFS Given a sample of rows A s  we compute its R factor R s Wethen compute the approximate Q factor via Q 1  AR  1 s Next the R factor of Q 1  R 1  is computed Finally the reìned Q factor is given by Q 1 R  1 1  In the implementation the computations of Q 1 and R 1 are performed simultaneously to avoid writing the disposable factor Q 1 to disk The reìned Q factor is computed by  AR  1 s  R  1 1 andthe R factor is given by R 1 R s Wecallthismethod PseudoIterative Reìnement  The standard iterative reìnement procedure is then the special case of Pseudo-Iterative Reìnement where A s  A  III Direct QR Factorizations in MapReduce One of the textbook algorithms to compute a stable QR factorization is the Householder QR method  T his method always produces a matrix Q where  Q T Q  I  2 is on the order of machine error We have implemented the algorithm in MapReduce and discuss it in the online version see previous footnote However the Householder method involves changing the entire matrix once foreachcolumn.Writingtheupdatedmatrixtodiskis prohibitively expensive in MapReduce and our performancedatashowedthatHouseholderQRisanorderof magnitude slower Thus we begin our discussion with our new stable algorithm Direct TSQR A Direct TSQR We nally arrive at our proposed method Here we directly compute the QR decomposition of A in three steps using two map functions and one reduce function as illustrated in Fig 4 This avoids the iterative nature of the Householder methods but maintains the stability properties 10  13 F or an example consider again a matrix A with 8 n rows and n columns which is partitioned across four map tasks for the rst step A      A 1 A 2 A 3 A 4      Theìrststepusesonlymaptasks.Eachtaskcollectsdata as a local matrix computes a single QR decomposition and emits Q and R to separate les The factorization of A then looks as follows with Q i 1 R i the computed factorization on the i th task A      Q 1  1 Q 2  1 Q 3  1 Q 4  1      012  8 n  4 n     R 1 R 2 R 3 R 4      012  4 n  n  The second step is a single reduce task The input is the set of R factors from the rst step The R factors are collected as a matrix and a single QR decomposition is performed The sections of Q corresponding to each R factor are emitted as values In the following gure  R is the nal upper triangular factor in our QR decomposition 267 


of A      R 1 R 2 R 3 R 4      012  4 n  n      Q 1  2 Q 2  2 Q 3  2 Q 4  2      012  4 n  n  R 012 n  n  The third step also uses only map tasks The input is the set of Q factors from the rst step The Q factors from the second step are small enough that we distribute the data in a le to all map tasks The corresponding Q factors are multiplied together to emit the nal Q  Q 012 8 n  n      Q 1  1 Q 2  1 Q 3  1 Q 4  1      012  8 n  4 n     Q 1  2 Q 2  2 Q 3  2 Q 4  2      012  4 n  n      Q 1  1 Q 1  2 Q 2  1 Q 2  2 Q 3  1 Q 3  2 Q 4  1 Q 4  2      012  8 n  n A  Q  R One implementation challenge is matching the Q and R factors to the tasks on which they are computed In the rst step the key-value pairs emitted use a unique map task identiìer e.g via the uuid package in Python as the key and the Q or R factor as the value The reduce task in the second step maintains an ordered list of the keys read The k th key in the list corresponds to rows  k  1 n 1 to kn of the locally computed Q factor The map tasks in the third step parse a data le containing the Q factors from the second step and this redundant parsing allows us to skip the shue and reduce Another implementation challenge is that the map tasks in the rst step and the reduce task in the second step must emit the Q and R factors to separate les For this functionality we use the feathers extensionofDumbo The thin singular value decomposition SVD of an m  n real-valued matrix is A is A  U  V T where U is an m  n orthogonal matrix  is a diagonal matrix with decreasing non-negative entries on the diagonal and V is an n  n orthogonal matrix To compute the SVD of A  we modify the second step and add a fourth step In the second step we also compute R  U  V T  Then A  QU  V T is the SVD of A Since R is n  n  computing its SVD is cheap The fourth step computes QU If Q is not needed i.e only the singular vectors are desired then we can pass U to the third step and compute QU directly without writing Q to disk In this case the SVD uses the same number of passes over the data as the QR factorization If only the singular values are needed then only the rst two steps of the algorithm are needed along with the SVD of R  However in this case it would be favorabletousetheTSQRimplementationfromSec.II-B to compute R only B Extending Direct TSQR to a recursive algorithm A central limitation to the Direct TSQR method is the necessity of gathering all R factors from the rst step onto one reduce task in the second step As the matrix becomes fatter this serial bottleneck becomes limiting We can cope by recursively extending the method and repeating the computation on the output R from the rst step The algorithm is outlined in Alg 1 and the performance beneìts of the algorithms are empirically analyzed in Sec IV-C  Algorithm 1 Recursive extension of Direct TSQR function DirectTSQR matrix A Q1 R1  FirstStep\(A if R1 is too big then Assign keys to rows of R1 Q2  DirectTSQR\(R1 else Q2  SecondStep\(R1 end if Q  ThirdStep\(Q1 Q2 return Q end function IV Performance Experiments We evaluate performance in two ways First we build a performance model for our methods based on how much data is read and written by the MapReduce cluster Second we evaluate the implementations on a 10-node 40-core MapReduce cluster at Stanfordês Institute for Computational and Mathematical Engineering ICME Each node has 6 2-TB disks 24 GB of RAM and a single Intel Core i7-960 3.2 GHz processor They are connected via Gigabit ethernet After tting only two parameters  the read and write bandwidth  the performance model predicts the actual runtime within a factor of two Although the cluster is small we emphasize that the algorithms scale with the number of map tasks launched not the number of nodes Therefore these algorithms scale to larger clusters This is covered in detail in our performance model and the numbers of map tasks used by the algorithms are listed in Table III All matrices used in the experiments are synthetic The matrix dimensions are chosen to reîect problems in model reduction a nd fast robust linear r egression 4 We do not perform standard parallel scaling studies due to how the Hadoop framework integrates the computational engine with the distributed lesystem This combination makes these measurements dicult without rebuilding the cluster for each experiment A Performance model Since the QR decomposition algorithms have more output data  Q and R factors than input data the matrix A  we choose a performance model that emphasizes read 268 


A  A 1  R 1  map emit Q 11  emit A 2  R 2  map emit Q 21  emit A 3  R 3  map emit Q 31  emit A 4  R 4  map emit Q 41  emit First step R 1  R 2  R 3  R 4  Q 12  Q 22  Q 32  Q 42  R reduce emit emit emit emit emit Second step Q 11  Q 12  emit Q 1  map shuffle Q 21  Q 22  emit Q 2  map Q 31  Q 32  emit Q 3  map Q 41  Q 42  emit Q 4  map Q 12  Q 22  Q 32  Q 42  2 distribute Third step Fig 4 Direct MapReduce computation of Q and R  and write volume Our performance model targets Hadoop so that we can more accurately evaluate the performance of the algorithms for a popular MapReduce framework Let M j and R j be the number of map and reduce tasks for step j  respectively Let M max and R max be the maximum number of map and reduce tasks that can run concurrently on the cluster Both M max and R max are xed in the Hadoop conìguration and M max  R max is usually at least the total number of cores Let k j be the number of distinct input keys passed to the reduce tasks for step j  We deìne the map parallelism for step j as p m j min M max  M j  and the reduce parallelism for step j as p r j min R max  R j k j  Let R m j  W m j be the amount of data read and written in the j th map step by all map tasks respectively We have analogous deìnitions for R r j and W r j for the j th reduce step Finally let  r and  w be the inverse read and write bandwidth respectively After computing  r and  w  we can provide a lower bound for the algorithm by counting disk reads and writes The lower bound for a job with N iterations is T lb  N  j 1 R m j  r  W m j  w p m j  R r j  r  W r j  w p r j  We use streaming benchmarks to estimate  r and  w for the 40-core ICME cluster and the results are in Table I On this cluster M max  R max  40 Table II provides the number of reads and writes for our algorithms and Tables III and IV provide the information for computing p m j and p r j  The keys for the matrix row identiìers are 32-byte strings The computed lower bounds for our algorithms are in Table V In Sec IV-B we examine how close the implementations are to the lower bounds TABLE III Values of M j and R j needed to compute p m j and p r j  M 1  M 3  and M 5 are dependent on the matrix size Other listed data are not The values M j and R j for Cholesky and Indirect TSQR are the same Matrix Cholesky PIR IR Direct Dimensions Indir TSQR 4  0 B  4 M 1 1200 3 1200 2000 2  5 B  10 1680 7 1680 2640 600 M  25 1200 3 1200 1600 500 M  50 1920 3 1920 2560 150 M  100 880 44 880 880 M 2 M max 1 M max M max 4  0 B  4 M 3 1200 1200 1200 2000 2  5 B  10 1680 1680 1680 2640 600 M  25 1200 1200 1200 1600 500 M  50 1920 1920 1920 2560 150 M  100 880 880 880 880 M 4  M max M max M max 4  0 B  4 M 5  1200 1200  2  5 B  10  1680 1680  600 M  25  1200 1200  500 M  50  1920 1920  150 M  100  880 880  R 1 R max R max R max R max R 2 1 111 R 3  R max R max  R 4  11 B Algorithmic comparison Using one step of iterative reìnement yields numerical errors that are acceptable in a vast majority of cases In these cases performance is our motivator for algorithm choice Tables VI and VII show performance results of 269 


TABLE I Streaming time to read from and write to disk Performance is in inverse bandwidth so larger  r and  w means slower streaming The streaming benchmarks are performed with M max map tasks  Rows Cols HDFS Size read+write read  r  M max  w  M max GB secs secs s/GB s/GB 4,000,000,000 4 134.6 713 305 2.2660 3.0312 2,500,000,000 10 193.1 909 309 1.6002 3.1072 600,000,000 25 112.0 526 169 1.5089 3.1875 500,000,000 50 183.6 848 253 1.3780 3.2407 150,000,000 100 109.4 529 151 1.3803 3.4552 TABLE I I Number of reads and writes at each step in bytes We assume a double is 8 bytes and K is the number of bytes for a row key  K 32 in our experiments The amount of key data is separated from the amount of value data For example 8 mn  Km is Km bytesinkeydataand 8 mn bytes in value data For iterative refinement p samp is the probability of sampling a row p samp 1 for standard iterative refinement Cholesky Cholesky Indirect Indirect Direct  I.R TSQR TSQR  I.R TSQR R m 1 8 mn  Km p samp 8 mn  Km  mn  Km p samp 8 mn  Km  mn  Km W m 1 8 M 1 n 2 8 M 1 n 8 M 1 n 2 8 M 1 n 8 M 1 n 2 8 M 1 n 8 M 1 n 2 8 M 1 n 8 mn 8 M 1 n 2  Km 64 M 1 R r 1 8 M 1 n 2 8 M 1 n 8 M 1 n 2 8 M 1 n 8 M 1 n 2 8 M 1 n 8 M 1 n 2 8 M 1 n 0 W r 1 8 n 2 8 n 8 n 2 8 n 8 R 1 n 2 8 R 1 n 8 R 1 n 2 8 R 1 n 0 R m 2 8 n 2 8 n 8 n 2 8 n 8 R 1 n 2 8 R 1 n 8 R 1 n 2 8 R 1 n 8 M 1 n 2  K M 1 W m 2 8 n 2 8 n 8 n 2 8 n 8 R 1 n 2 8 R 1 n 8 R 1 n 2 8 R 1 n 8 M 1 n 2  K M 1 R r 2 8 n 2 8 n 8 n 2 8 n 8 R 1 n 2 8 R 1 n 8 R 1 n 2 8 R 1 n 8 M 1 n 2  K M 1 W r 2 8 n 2 8 n 8 n 2 8 n 8 n 2 8 n 8 n 2 8 n 8 M 1 n 2 32 M 1 8 n 2 8 n R m 3 8 mn  Km 8 mn  Km 8 mn  Km 8 mn  Km 8 mn  Km  M 3 8 n 2 8 n  M 3 8 n 2 8 n  M 3 8 n 2 8 n  M 3 8 n 2 8 n  M 3 8 M 1 n 2 64 M 1  W m 3 8 mn  Km 8 M 3 n 2 8 M 3 n 8 mn  Km 8 M 3 n 2 8 M 3 n 8 mn  Km R r 3 08 M 3 n 2 8 M 3 n 08 M 3 n 2 8 M 3 n 0 W r 3 08 n 2 8 n 08 R 3 n 2 8 R 3 n 0 R m 4 8 n 2 8 n 8 R 3 n 2 8 R 3 n  W m 4 8 n 2 8 n 8 R 3 n 2 8 R 3 n  R r 4 8 n 2 8 n 8 R 3 n 2 8 R 3 n  W r 4 8 n 2 8 n 8 n 2 8 n  R m 5 8 mn  Km 8 mn  Km  2 M 5 8 n 2 8 n  M 5 8 n 2 8 n  W m 5 8 mn  Km 8 mn  Km  R r 5 0Ö0  W r 5 0Ö0  TABLE V Computed lower bounds for each algorithm p samp is the sampling probability for Pseudo-iterative refinement Rows Cols p samp Cholesky Indirect Cholesky Indirect Cholesky Indirect Direct TSQR PIR TSQR+PIR IR TSQR+IR TSQR T lb secs 4,000,000,000 4 0.0025 1803 1803 1821 1821 2343 2343 2528 2,500,000,000 10 0.0042 1645 1645 1655 1655 2062 2062 2464 600,000,000 25 0.0025 804 804 812 812 1000 1000 1237 500,000,000 50 0.0016 1240 1240 1250 1250 1517 1517 2103 150,000,000 100 0.0500 723 723 735 735 884 884 1217 TABLE VI Times to compute QR on a variety of matrices with seven MapReduce algorithms only the DirectTSQR method is guaranteed to be numerically stable Rows Cols HDFS Size Cholesky Indirect Cholesky Indirect Cholesky Indirect Direct GB TSQR PIR TSQR+PIR IR TSQR+IR TSQR job time secs 4,000,000,000 4 134.6 2931 3460 3276 3620 4365 4741 6128 2,500,000,000 10 193.1 2508 2509 2887 3354 3778 4034 4035 600,000,000 25 112.0 1098 1104 1275 1476 1645 2006 1910 500,000,000 50 183.6 1563 1618 1772 1960 2216 2655 3090 150,000,000 100 109.6 1023 1127 1146 1304 1400 1652 2076 270 


TABLE IV Values of k j needed to compute p m j and p r j   Chol Chol Indir TSQR Indir TSQR Direct PIR,+IR PIR,+IR k 1 nn M 1 n M 1 n M 1 k 2 nn M 2 n M 2 n M 1 k 3 0 n 0 M 3 n 0 k 4  n  M 4 n  k 5 0  0  TABLE VI I I Fraction of time spent in each step of the Direct TSQR algorithm fractions may not sum to 1 due to rounding Rows Cols Step 1 Step 2 Step 3 4,000,000,000 4 0.72 0.02 0.26 2,500,000,000 10 0.61 0.04 0.34 600,000,000 25 0.56 0.06 0.38 500,000,000 50 0.55 0.07 0.39 150,000,000 100 0.47 0.15 0.38 Cholesky QR and the the Indirect and Direct TSQR methods for a variety of matrices In our experiments we see that Indirect TSQR and Cholesky QR provide the fastest ways of computing the Q and R factors albeit  Q T Q  I  2 may be large For all matrices with greater than four columns these two methods have similar running times For such matrices the majority of the running time is the AR  1 step and this step is identical between the two methods This is precisely because the write bandwidth is less than the read bandwidth For the matrices with 10 25 and 50 columns Direct TSQR is competitive with the indirect methods with iterative reìnement albeit slightly slower The performance is the most similar for smaller number of columns e.g with ten columns However when the matrix becomes too skinny e.g with four columns Cholesky QR with iterative reìnement is a better choice When the matrix becomes too fat e.g with 100 columns the local gather in Step 2 becomes expensive Table VIII shows the amount of time spent in each step of the Direct TSQR computation Indeed Step 2 consumes a larger fraction of the running time as the number of columns increases Table IX shows how each algorithm performs compared to its lower bound from Table V We see that Direct TSQR diverges from this bound when the number of columns is too small To explain this dierence we note that Direct TSQR must gather all the keys and values in the rst step before performing any computation When the number of key-value pairs is large e.g with the 4,000,000,000  4 matrix then this step becomes limiting and this is not accounted for by our performance model Thus the model predicts the runtime of Cholesky QR and Indirect TSQR with iterative reìnement more accurately than Direct TSQR Although their lower bounds are greater the empirical performance makes these algorithms more attractive as the number of columns increases If guaran0 50 100 150 200 0 2000 4000 6000  number of columns running time \(s 150M rows   0 50 100 150 200 250 0 2000 4000 6000 8000  number of columns running time \(s 100M rows   0 50 100 150 200 250 300 0 5000 10000 15000  number of columns running time \(s 50M rows   no recursion  recursion no recursion  recursion no recursion  recursion Fig 5 Running time of Direct TSQR with and without recursion The recursive version takes only one recursive step teed stability is required Direct TSQR is the best method and the performance cost of stability is quite small for a modest number of columns C Recursive Direct TSQR In the preceding performance analysis we used Direct TSQR without the recursive extension described in Sec III-B Fig 5 shows the performance beneìts for the recursive extension as the number of columns increases for matrices with 50 100 and 150 million rows In these experiments a single recursive step is taken For these matrices the recursive version of the algorithm is faster once the number of columns is approximately 150 V Conclusion If numerical stability is required the Direct TSQR method discussed in this paper is the best choice of algorithm It is guaranteed to produce a numerically orthogonal matrix It usually takes no more than twice the time of the fastest but unstable method and it is often competitive with conceptually simpler methods Our code for this paper is openly available see https://github.com/arbenson/mrtsqr This software runs on any system with Hadoop streaming InthefutureweplantoinvestigatemixedMPIand Hadoopcode.Theideaisthatonceallthelocalmappers 271 


TABLE VI I Floating point operations per second on a variety of matrices with four MapReduce algorithms  Rows Cols 2  rows  cols 2 Cholesky Indirect Cholesky Indirect Cholesky Indirect Direct TSQR PIR TSQR+PIR IR TSQR+IR TSQR 2  rows  cols 2  sec 4,000,000,000 4 1.280e+11 4.367e+07 3.140e+07 3.907e+07 3.536e+07 2.932e+07 2.700e+07 2.089e+07 2,500,000,000 10 5.000e+11 1.994e+08 1.993e+08 1.732e+08 1.491e+08 1.323e+08 1.239e+08 1.239e+08 600,000,000 25 7.500e+11 6.831e+08 6.793e+08 5.882e+08 5.081e+08 4.559e+08 3.739e+08 3.927e+08 500,000,000 50 2.500e+12 1.599e+09 1.545e+09 1.411e+09 1.276e+09 1.128e+09 9.416e+08 8.091e+08 150,000,000 100 3.000e+12 2.933e+09 2.662e+09 2.643e+09 2.338e+09 2.143e+09 1.836e+09 1.393e+09 TABLE IX Performance of algorithms as a multiple of the lower bounds from Table V Rows Cols Cholesky Indirect Cholesky Indirect Cholesky Indirect Direct TSQR PIR TSQR+PIR IR TSQR+IR TSQR multiple of T lb 4,000,000,000 4 1.626 2.261 1.799 1.988 1.863 2.023 2.424 2,500,000,000 10 1.525 1.525 1.744 2.027 1.832 1.956 1.638 600,000,000 25 1.366 1.373 1.570 1.818 1.645 2.006 1.544 500,000,000 50 1.260 1.305 1.418 1.568 1.461 1.750 1.469 150,000,000 100 1.415 1.559 1.544 1.746 1.584 1.848 1.770 have run in the rst step of the Direct TSQR method the resulting R i matrices constitute a much smaller input If we run a standard in-memory MPI implementation to compute the QR factorization of this smaller matrix then we could remove two iterations from the direct TSQR method.Also,wewouldremovemuchofthediskIO associated with saving the Q i matrices These changes couldreduceruntimebyatmostafactorof4 Acknowledgment AustinR.BensonissupportedbyanOceofTechnology Licensing Stanford Graduate Fellowship David F Gleich is supported by a DOE CSAR grant We acknowledge funding from Microsoft award 024263 and Intel award 024894 and matching funding by UC Discovery award DIG07-10227 with additional support from ParLab aliates National Instruments Nokia NVIDIA Oracle and Samsung and support from MathWorks We also acknowledge the support of the US DOE grants DE-SC0003959 DE-SC0004938 DESC0005136 DE-SC0008700 DE-AC02-05CH11231 and DARPA award HR0011-12-2-0016 We are grateful to ICME for letting us use their MapReduce cluster for these computations We are grateful to Paul Constantine for working on the initial TSQR method and for continual discussions about using these routines in simulation data analysis problems References  H  A vron P  M a y mounk o v  a nd S T o ledo Blendenpik Supercharging LAPACKês least-squares solver SIAM J Sci Comput  32\(3 Apr 2010  K  B osteels Dum b o h t tp://klb ostee.gith ub.io/dum b o  2012 3 C T C h u S K K i m Y A L i n Y Y u  G  R  B r a d s k i A Y  Ng and K Olukotun Map-Reduce for machine learning on multicore In B Sch olkopf J C Platt and T Homan editors Advances in Neural Information Processing Systems 19 pages 281Ö288 MIT Press 2006  K  L  C larkson P  Drineas M  M agdon-Ismail M W Mahoney  X Meng and D P Woodru The fast Cauchy transform and faster robust linear regression In SODA  pages 466Ö477 2013  P  C onstan tine and D  F  G leic h T a ll and s kinn y Q R f actorizations in MapReduce architectures In MAPREDUCE2012  page 43.50 2011  P  G  C onstan tine D  F  G leic h Y Hou and J  T empleton Model Reduction with MapReduce-enabled Tall and Skinny Singular Value Decomposition arXiv  math.NA:1306.4690 June 2013  J  D ean and S  G hema w at MapReduce Simpliìed d ata processing on large clusters In Proceedings of the 6th Symposium on Operating Systems Design and Implementation OSDI2004  pages 137Ö150 2004  J  D emmel L Grigori M Ho emmen and J  L angou Communication-optimal parallel and sequential QR and LU factorizations EECS-2008-89  Aug 2008  Z  F adika E Dede M  G o v indara ju a nd L Ramakrishnan Benchmarking MapReduce implementations for application usage scenarios GRID 11 pages 90Ö97 2011  G H Golub a nd C F v a n L oan Matrix Computations The Johns Hopkins University Press third edition October 1996  N J Higham Accuracy and Stability of Numerical Algorithms  SIAM 2002  M W Mahoney  Randomized algorithms f or matrices and data arXiv  cs.DS 2011  D Mori Y Y a mamoto and S L Zhang B ac kw ard e rror analysis of the AllReduce algorithm for Householder QR decomposition Jpn J Ind Appl Math  29\(1 Feb 2012  B N P a rlett The Symmetric Eigenvalue Problem SIAM Philadelphia PA USA 1998  S J Plimpton and K  D  D evine MapReduce i n M PI for l argescale graph algorithms Parallel Comput  37\(9 2011  V Rokhlin and M  T ygert A f ast r andomized algorithm for overdetermined linear least-squares regression Proceedings of the National Academy of Sciences  105\(36 2008  A Stathop oulos a nd K W u  A blo c k o rthogonalization pro cedure with constant synchronization requirements SIAM J Sci Comput  23:2165Ö2182 June 2001  J T a lb ot R M Y o o and C  K ozyrakis Pho e nix mo dular mapreduce for shared-memory systems MAPREDUCE 2011 pages 9Ö16 New York NY USA 2011 ACM  V a rious H ado o p v ersion 0 21 h ttp://hado o p.apac he.org  2012  J Zhao a nd J Pjesiv ac-Grb o v ic M apReduce The p rogramming model and practice http://research.google.com/archive papers/mapreduce-sigmetrics09-tutorial.pdf 2009 Tutorial 272 


  10  te m p e r a tu r e  s e n s o r  pol ynom i a l  i s  0 9977  w hi l e  t he  R 2   fo r  t h e  ba s e pl a t e  t e m pe r a t ur e  s e ns or  pol ynom i a l  i s  0 9984   Ta b l e  8  pr e s e nt s  t he  pol ynom i a l  c oe f f i c i e nt s     Fi g u r e  14  q ue nc y  H z    Te m p e r a t u r e  d  Ce l s i u s   f o r  A u x i l i a r y  O s c i l l a t o r  a n d  Ba s e p l a t e   Ta b l e  8  Co e f f i c i e n t s  fo r  F r e q u e n c y  v s  T e m p e r a t u r e  Po l y n o m i a l s    Al t h o u g h  t h e  h i g h  R 2  va l ue s  s e e m  t o pr ovi de  r e a s on f or  co n f i d en ce i n  t h es e t em p er at u r e p r ed i ct i o n s   t h e E D L  th e r m a l e n v ir o n m e n t d if f e r s  n o ta b ly  f r o m  th a t in  c r u is e  St e a d y  t e m p e r a t u r e s  a n d  t h e r m a l  e q u i l i b r i u m  c h a r a c t e r i z e  th e  c r u is e  th e r m a l e n v ir o n m e n t  T h e  E D L  th e r m a l i ro n m e n t  i s  c h a ra c t e ri z e d  b y  ra p i d  h e a t i n g    A s  a  re s u l t   th e  S D S T  a n d  its  in te r n a l c o m p o n e n ts  w e r e  n o t in  th e r m a l eq u i l i b r i u m  d u r i n g  E D L   In  t h e rm a l  e q ui l i br i um   a s  s how n  Fi g u r e  15  be l ow   t he  A U X  O S C  i s  nor m a l l y s om e w ha t  wa r m e r  t h a n  t h e  b a s e p l a t e    Ho we v e r   a s  Fi g u r e  15   sh o w s  d u r i n g  E D L   t h e  b a se p l a t e  t e m p e r a t u r e  r o se  re l a t i v e l y  q u i c k l y  a n d  ra p i d l y  o v e rs h o t  t h e  A U X  O S C  te m p e r a tu r e   A lth o u g h  A U X  O S C  te m p e r a tu r e  r o s e  s lo w ly  to   c a tc h  u p   w ith  b a s e p la te  te m p e r a tu r e  th e  S D S T  w a s  n o t in  th e r m a l e q u ilib r iu m  d u r in g  E D L   T h is  im p a c ts  th e  accu r acy  o f  an y  f r eq u en cy  p r ed i ct i o n s  b as ed  o n  b as ep l at e te m p e r a tu r e     15  MS L  E D L  S D S T  T e m p e r a t u r e s  S 34 S 45 34 m y  a  As  a  b a c k up t o t he  pr i m e  70 m a n t e n n a   D S S 43   t he  si g n a l s f r o m  a b eam  w av eg u i d e  34 m a n t e n n a s   D S S 34    a h i g h  ef f i ci en cy  3 4 m a n t e n n a   S 45  w e r e  co m b i n ed  an d  r eco r d ed    k up  da t a  w a s  not  ne e de d in r e a l  d  wa s  a n a l y z e d  i n  p o s t ng  Di r e c t to h   re c e i v e d  b y  t h e  D S S S y dur i ng M S L  E D L  i s  s how n i n  Fi g u r e  16   Av e r a g e  c er to noi s e  pow e r   P c N o  p l o t t e d  i n  l i g h t  b l u e  m e a s u re d  by th e  E D A  us i ng t he  D S S S 45 a r r a y dur i ng E D L  w a s  dB     Fi g u r e  16  MS L  E D L  P c N o P e N o a n d R e s id u a l Fr e q u e n c y  w i t h  D SS S y  An t e n n a  se n si t i v i t y  i s m e a su r e d  b y  T  w h e r e  G  is   an t en n a g ai n   an d  T  is  th e  s y s te m   te m p e r a tu r e   G   fu n c t i o n  o f w a v e l e n g t h     phys i c a l  a pe r t ur e  a r e a   A p   ap er t u r e ef f i ci en cy     as  s h o w n  b el o w                    6    Ba s e d  s o l e l y  o n   ap er t u r e  ea  A p    T   a  34 m a n t e n n a  is  a b o u t 1 7  0  1 7  0  3 5  0  3 5    6   T   0   Ho we v e r   t h e  N 70 m a n t e n n a  al s o  h as  b et t er  ap er t u r e ef f i ci en cy     a n d  a  lo w e r  s y s te m  noi s e  t e m pe r a t ur e   T  th a n  th e  D S N  3 4 e    th e s e  f a c to r s  c o n s id e r e d  th e  T  of  a  D S N  34 m a n t e n n a  i s  ab o u t  1 8   t h e T  of  a  D S N  70 m a n t e n n a  8    pr e di c t e d di f f e r e nc e  i n a r r a y ga i n be t w e e n t he  70  an d  t w o  ar r ay ed  3 4 m a n t e n n a s  i s  10 g 10   1 0   18  18    4 44  dB   a s s um i ng no c om bi ni ng l os s   P c N o  is  p r o p o r tio n a l  T   Th e  m e a s u r e d  d i f f e r e n c e  i n  P c N o   S S S n  17   Th e  m e a n  m e a s u r e d  d i f f e r e n c e  f r o m  E 645 s e c onds  t o E 2 9 9  s e c o n d s  w a s  4  2 6  d B   Th e r e f o r e  m e a n  c o m b i n i n g  lo s s d u r in g th is tim e w a s  ab o u t  0  1 8                      0 1 23 45''67626 84  3 6 9 2 7 7    6593 9\(:6 562\(6\(2'47   0        0 1 5'695\(95 87 7 3 5   4 7 5 2  97599 562\(6\(&765 


  11   Fi g u r e  17  e re n c e  i n  D S S 43 a nd D S S S P c N o   Af t e r  E DL   E v e n t  R e c o r d s   E VR s   t h a t  l o g g e d  e a c h  t o n e  is s u e d  d u r in g  E D L  w e r e  obt a i ne d f r om  M S L   Th e s e  l o g s  we r e  c o m p a r e d  wi t h  t h e  r e a l tim e  r e s u lts  p r o v id e d  b y  th e  ED A  t o  d e t e r m i n e  p e r f o r m a n c e    Th e  D TE c o m m u n i c a t i o n s  sy st e m  r e c e i v e d  a n d  c o r r e c t l y  i d e n t i f i e d  1 0 0   o f  ra d i a t e d   i n r e a l tim e  d u r in g  M S L  E D L   Th e s e  r e s u l t s  a r e  co n s i s t en t  w i t h  t h e t h eo r et i cal  pr oba bi l i t i e s  of  c a r r i e r  acq u i s i t i o n  t r ack i n g  an d  d at a t o n e d et ect i o n  co m p u t ed  i n  Se c t i o n  3   5   C ON   Th e  D i r e c t to Ea r t h  X ba nd c om m uni c a t i ons  s ys t e m  ut i l i z e d dur i ng M S L  E D L  s uc c e s s f ul l y de t e c t e d a l l  ra d i a t e d   de s pi t e  c ha l l e ngi ng s i gna l  dyna m i cs  w i t h  l ar g e u n k n o w n  ch an g es  i n  D o p p l er  f r eq u en cy   r at e  an d  accel er at i o n   Fu t u r e  m i s s i o n s  w i t h  p e r i o d s  o f  r a p i d  a n d  u n k n o w n  s i g n a l  dyna m i c s  s uc h a s  Ma r s  o r  i c y  m o o n  la n d e r s  can  l ev er ag e fr o m  t h e  M S L  de s i gn f or  D T E  c om m uni c a t i ons    6   A CK NO W L E DG E M E NT S   T he  a ut hor s  w oul d l i ke  t o a c know l e dge  t he  c ont r i but i on s   Ja n  T a r sa l a   te s tin g  o f  th e  E D A  p r io r  to  M S L  E D L  us i ng a  P R S R  a nd M S L  t e s t be d    Th e  a u t h o r s  w o u l d   th a n k  J e r e m y  S r  fo r  p r o v i d i n g  6 D O F  s i m u l a t i o n  d a t a  th a t w a s  v a lu a b le  in  c o n f ig u r in g  th e  E D A   Th e  a u t h o r s  wo u l d  a l s o  l i k e  t o  t h a n k  t h e  C DS C C  s t a t i o n  p e r s o n n e l  f o r  th e ir  e x c e lle n t s u p p o r t a n d  ope r a t i ons  of  t he  D S N  eq u i p m en t  an d  t h e F u l l  S p ect r u m  P r o ces s o r  A r r ay  i n  u   Th i s  r e s e a r c h  w a s  c a r r i e d  o u t  a t  t h e  J e t  P r o p u l s i o n  La b o r a t o r y   C a l i f o r n i a  I n s t i t u t e  o f  Te c h n o l o g y    Co p y r i g h t  2012 C a l i f or ni a  I ns t i t ut e  of  T e c hnol ogy  Go v e r n m e n t  sp o n so r sh i p  a c k n o w l e d g e d     


  12  R EF ER EN C ES   1  E  S a t o r i u s   P   Es t a b r o o k   J   W i l s o n   D   F o rt    D i re c t to  Ea r t h  c o m m u n i c a t i o n s  a n d  s i g n a l  p r o c e s s i n g  f o r  M a r s  ex p l o r at i o n  r o v er  en t r y   d es cen t  an d  l an d i n g   T h e In t e rp l a n e t a ry  N e t w o rk  P ro g re s s  R e p o rt   IP N  P ro g re s s  Re p o r t  4 2 2003  2 A n d re  J o n g e l i n g  an d  S u s an  F i n l ey     M ar s  S ci en ce La b o r a t o r y  Te l e c o m  S y s t e m  En g i n e e r i n g  P r e  Re v i e w   E D L  D a t a  A n a l y s i s  S i m u l a t i o n s  Re s u l t s     A p r i l  24  2007   3 W   J   H u rd   P   E s t a b ro o k   C   S   R a c h o   a n d  E   S a t o ri u s   C r i t i cal  sp acecr af t to ear t h  co m m u n i cat i o n s   ex p l o r at i o n  r o v er   M E R   en t r y   d es cen t  an d  l an d i n g   Pr o c   I E E E  A e r o s p a c e  C o n f e r e n c e   v o l  3   p p   1 2 8 3  MT   Ma r c h  2 0 0 2    4 M  S o r i a n o   S   F i n l e y   A   J o n g e l i n g   D   F o r t   C   G o o d h a r t   D  R o g s t a d   R   Na v a r r o    Sp a c e c r a f t to Ea r t h  Co m m u n i c a t i o n s  fo r J u n o  a n d  M a rs  S c i e n c e  L a b o ra t o ry  Cr i t i c a l  E v e n t s   P r o c  I E E E  A e r o sp a c e  C o n f e r e n c e   M T   2   5 A   M a k o v s k y   P   Il l o t t   J   T a y l o r    M a rs  S c i e n c e  La b o r a t o r y  Te l e c o m m u n i c a t i o n s  S y s t e m  D e s i g n    D e e p  Sp a c e  C o m m u n i c a t i o n s  a n d  N a v i g a t i o n  Sy s t e m s  C e  of  E xc e l l e nc e  D e s i gn a nd P e r f or m a nc e  S um m a r y S e r i e s   No v e m b e r  2 0 0 9   6 M   S o ri a n o  a n d  P   E s t a b ro o k    M S L  E D L  S i m u l a t i o n s   i n t e rn a l  d o c u m e n t   J e t  P ro p u l s i o n  L a b o ra t o ry   P a s a d e n a   CA   M a y  7   2 0 1 2   7  Sa t o r i u s  R e v i s e d  T h r e s h o l d s  f o r  E D L    i n t e r n  doc um e nt    J e t  P r opul s i on L a bor a t or y  P a s a de na   C A   Ja n u a r y  1 4   2 0 0 3   8 A   K w o k     M o d u l e  2 0 6  Te l e m e t r y  G e n e r a l  In fo rm a t i o n    i n DS N  T e l e c o mmu n i c a t i o n s  L i n k  De s i g n  k B   D S N  N o  8 1 0 005   P a s a de na  Ca l i f o r n i a   J P L   Oc t o b e r  3 1   2 0 0 9  ht t p   e i s  j pl  na s a g o v d e e p s p a c e d s n d o c s 8 1 0 005     


  13  M el i s s a  S o r i a n o  ff f tw a r e  e n g in e e r  in  th e  T r a c k in g  Sy s t e m s  and A ppl i c at i ons  Se c t i on at  t he  J e t  P r opul s i on L abor at or y    She  has  de v e l ope d r e al  so f t w a re  f o r t To  co m m u n i ca t i o n s  w i t h  M a r s  Sc i e nc e  L abor at or y  dur i ng E nt r y   De s c e n t   a n d  L a n d i n g   th e  L o n g  W a v e le n g th  Ar r a y   N AS A s  Br e a d b o a r d  Ar r a y   a n d  t h e  W i d e b a n d  VL BI  S c i e n c e  Re c e i v e r  u s e d  i n  t h e  D e e p  S p a c e  N e t w o r k   Me l i s s a  i s  a l s o  cu r r en t l y t h e s o f t w a r e co g n i z a n t  en g i n eer  f o r  t h e D S C C  Do w n l i n k  A r r a y   She  has  a B  S   fr o m  C a lte c h  d o u b le  m a jo r  in  E le c tr ic a l a n d  C o m p ut e r  E ngi ne e r i ng and B us i ne s s  Ec o n o m i c s  a n d  M a n a g e m e n t    S h e  a l s o  h a s  a n  M  S    Co m p u t e r  S c i e n c e  fr o m G e o r g e M a so n  U n i v e rsi t y   Sus a n F i nl e y  is  a  k e y  s ta ff me mb e r  i n  t h e  P r o c e s s o r  S y s t e ms  De v e l o p me n t  Gr o u p  a t  J P L     is  th e  s u b s y s te m  e n g in e e r  fo r  th e  Fu ll S p e c tr u m  P r o c e s s o r  su b sy st e m  d e p l o y e d  i n  N A S A  s De e p  S p a c e  N e t w o r k     exp er i en ce i n cl u d es  t h e o p er a t i o n  of  t he  E D A  f or  bot h of  t he  M E R  l andi ngs  on M ar s  as  w e l l  as  th e  o p e r a tio n  o f th e  R a d io  S c ie n c e  R e c e iv e r  fo r  th e  la n d in g  o f th e  H u y g e n s  P r o b e  o n  T it an and f or  t he  P hoe ni x  l andi ng on s    Da v i d  t  re c e i v e d  a  B  A  S c  i n  En g i n e e r i n g  Ph y s i c s  a n d  M  S c  i n  As t r o n o m y  f r o m  t h e  U n i v e r s i t y  o f  To r o n t o  a n d  a n  M S c   a n d  P h  D   i n  Ra d i o  As t r o n o m y  f r o m  t h e  U n i v e r s i t y  of  M anc he s t e r    H e  j oi ne d N R C  C a n a d a  i n  1 9 7 2  a n d  w o r k e d  o n  a l l  as pe c t s  of  V L B I  unt i l  1987   H e  su b se q u e n t l y  j o i n e d  J P L  i n  se c t i o n  3 3 5  a n d  w o rk e d  o n  a  num be r  of  har dw ar e  and s of t w ar e  pr oj e c t s  f or  t he   be c am e  s upe r v i s or  of  t he  P r oc e s s or  Sy s t e m s  de v e l opm e nt  Gr o u p  f o r  t h e  t w o  y e a r s  p r i o r  t o  r e t u r n i n g  t o  N R C  i n  2 0 0 2   Un t i l  h i s  r e t i r e me n t  i n  2 0 1 0  h e  w o r k e d  o n   Co r r e l a t o r  P r o j e c t    No w a   G u e s t  W o r k e r    h e  h e l p s  o u t  wi t h  t h e  E V L A  a s  i t  b e c o m e s f u l l y  o p e ra t i o n a l  a n d  w i t h  oc c as i onal  que s t i ons  f r om  J P L    Br i a n  S c h r a t z  is  th e  le a d  e n g in e e r  fo r  th e  E D L  te le c o m m u n ic a tio n s  o n  th e  Ma r s  S c i e n c e  L a b o r a t o r y  m i s s i o n  a n d  a m e m be r  of  J P L  s  C om m uni c at i ons  Sy s t e m s  and O pe r at i ons  gr oup      jo in e d  J P L  th r e e  y e a r s  a g o   B S  E E  a n d  M  S  E E   Pe nns y l v ani a St at e  U ni v e r s i t y    Pe t e r  I l o t t  is  th e  te le c o m m u n ic a tio n s  sy st e m  l e a d  f o r t h e  M S L  m i ssi o n   H e  has  w or k e d on s pac e c r af t  te le c o m m u n ic a tio n s  s y s te m  d e s ig n  fo r  2 5  y e a r s  1 1  y e a r s  o n  c o m m e r c ia l sp a c e c ra f t   a n d  si n c e  2 0 0 0  a t  J P L    wo r k e d  o n  M E R   P h o e a te le c o m m u n ic a tio n s  s y s te m  e n g in e e r  Pe t e r  w o r k e d  o n  a l l  t h e  M a r s  ED L   e n t r y  and la n d in g   e ffo r ts  s in c e  M E R  a n d  in  b e tw e e n  M a r s  m is s io n s  he l pe d out  on t he  D e e p I m pac t  and C l oudat  mi s s i o n s  a t  J P L   He  c u r r e n t l y  s u p p o r t s  t h e  M S L  s u r f a c e  mi s s i o n  p h a s e   a n d  i s  th e  te le c o m m u n ic a tio n s  le a d  fo r  th e  E u r o p a  m is s io n  cu r r en t l y u n d er  s t u d y  I l o t t  h o l d s  B S c  M S c  a n d  P h D  de gr e e s  i n phy s i c s  and e l e c t r i c al  en g i n eer i n g  f r o m  M cG i l l  i st y  o f  M o n t re a l    i  re c e i v e d  t h e  B  S  E  E   and t he  M  S E  E   i n 1997 and t he  Ph  D   i n  El e c t r i c a l  En g i n e e r i n g  i n  2003  al l  f r om  U C L A    He  h a s  b e e n  em p l o yed  a t  t h e Jet  P r o p u l s i o n  La b o r a t o r y  a s  a  Te l e c o m m u n i c  en g i n eer  s i n ce 1 9 9 9  a n d  h a s  s er ved  on t he  M ar s  E x pl or at i on R ov e r   DA W N   C a s s i n i   J u n o   a n d  M a r s  Sc i e nc e  L abor at or y  pr oj e c t s     Po l l y  E s t a b r o o k  is  th e  d e p u ty  ma n a g e r  o f  t h e  C o mmu n i c a t i o n  Ar c h i t e c t u r e s  a n d  Re s e a r c h  S e c t i o n  at  J P L     She  i s  a m e m be r  o f N A S A  s  Spac e  C om m uni c at i on and Na v i g a t i o n  P r o g r a m  s u p p o r t i n g  t h e  de f i ni t i on of  t he  N A SA  s  f ut ur e  In t e g r a t e d  C o m m u n i c a t i o n  a n d  Na v i g a t i o n  Ne t wo r k  a n d  i s  a  m e m b e r  o f  t h e  I n t e g r a t e d  Sy s t e m  E ngi ne e r i ng t e am  f or  t he  M ar s  Sc i e nc e  L abor at or y  r   Fr o m  2 0 0 5  t o 2010  s he  l e d s e v e r al  c om m uni c at i on sy st e m  d e si g n  t e a m s w i t h  t h e  g o a l  o f  d e f i n i n g  t h e  mo d i f i c a t i o n s  t o  N A S A  s  S p a c e  C o mmu n i c a t i o n  a n d  Na v i g a t i o n  i n f r a s t r u c t u r e  n e e d e d  t o  s u p p o r t  t h e  p l a n n e d  hum an m i s s i ons  t o t he  M oon and M ar s   F r om  2000 t o 2004 sh e  w a s t he  l e ad t e l e c om  s y s t e m  e ngi ne e r  f or  t he  M ar s  Ex p l o r a t i o n  Pr o j e c t   r e s p o n s i b l e  f o r  t h e  p e r f o r m a n c e  o f  t h e  en t r y d es cen t  a n d  l a n d i n g  t el eco m m u n i ca t i o n s  s ys t em  a n d  fo r  th e  o v e r a ll d e s ig n  a n d  p e r fo r m a n c e  o f th e  D ir e c t to  Ea r t h  a n d  r e l a y  c o m m u n i c a t i o n s  s y s t e m s   In  2 0 0 4   D r   Es t a b r o o k  r e c e i v e d  t h e  N AS A Ex c e p t i o n a l  Ac h i e v e m e n t  Me d a l  f o r  h e r  w o r k  o n  t h e  Ma r s  E x p l o r a t i o n  R o v e r  T e l e c o m  Sy s t e m   She  has  w r i t t e n ov e r  35 t e c hni c al  pape r s  and ch a i r ed  n u m er o u s  I E E E  a n d  A I A A  co n f er en ce S es s i o n s    Po l l y  Es t a b r o o k  r e c e i v e d  h e r B  A   i n  e n g i n e e ri n g  p h y si c s fr o m  th e  U n iv e r s ity  o f C a lifo r n ia  B e r k e le y  a n d  M S  a n d  Ph  D   d e g r e e s  i n  e l e c t r i c a l  e n g i n e e r i n g  f r o m  S t a n f o r d  Un i v e r s i t y   S t a n f o r d   C A      


  14  Ka m a l  O u d r h i r i  is  a  s e n io r  r  in  th e  R a d io  S c ie n c e  Sy s t e m s  G r oup at  NA S A  s  J e t  Pr o p u l s i o n  L a b o r a t o r y   As  a co n t r a ct  t ech n i ca l  m a n a g er   Ou d r h i r i  lti di s c i pl i nar y  te a m s  th r o u g h  th e  de s i gn  im p le m e n ta tio n  a n d  d e liv e r y  of  flig h t h a r d w ar e  t o t he  r adi o sc i e n c e  c o m m u n i t y   Ov e r  t h e  l a s t  d e c a d e   Ou d r h i r i  se rv e d  i n  key r o l es  o n  m u l t i p l e N A S A  mi s s i o n s   T h e  M a r s  E x p l o r a t i o n  s  M E R   t h e  In t e r n a t i o n a l  C a s s i n i  m i s s i o n  t o  Sat ur n T he  GR A I L  l u n a r  mi s s i o n  a n d  T h e  M a r s  S c i e n c e  La b o r a t o r y     Da n i e l  K a h a n  is  a s e ni or  m e m be r  of  S ci en ce S ys t em s  G r o u p  at  NA S A  s  J e t  Pr o p u l s i o n  La b o r a t o r y   Ov e r  t h e  l a s t  ei g h t  yea r s   h e h a s  pr ov i de d e ngi ne e r i ng s uppor t  f or  t he  i o s c i e nc e  c om m uni t y   NA S A  m i s s i o n s   i n c l u d i n g  M a r s  G l o b a l  Sur v e y or   M ar s  R e c onnai s s anc e  Or b i t e r   th e  G R A I L  lu n a r  m is s io n  th e  In t e r n a t i o n a l  C a s s i n i  mi s s i o n  t o  S a t u r n   a n d  Ma r s  S c i e n c e  La b o r a t o r y   Ed g a r  H   S a t o r i u s  is  a  p r in c ip a l me mb e r  o f  t h e  t e c h n i c a l  s t a f f  i n  th e  F lig h t C o m m u n ic a tio n s  Sy s t e m s  Se c t i on of  t he  J e t  Pr o p u l s i o n  L a b   H e  p e r f o r m s  sy st e m s a n a l y si s i n   de v e l opm e nt  of  di gi t al  s i gnal  e ssi n g  a n d  c o m m u n i c a t i o n s sy st e m s w i t h  sp e c i f i c  a p p l i c a t i o n s t o  b l i n d  d e m o d u l a t i o n   di gi t al  di r e c t i on f i ndi ng and di gi t al  r e c e i v e r s   H e  has  publ i s he d ov e r  90 ar t i c l e s  and hol ds  t w o pat e nt s  i n t he  f i e l d of  di gi t al  s i gnal  pr oc e s s i ng and i t s  appl i c at i ons   I n a ddi t i on  he  i s  an A dj unc t  A s s oc i at e  P r of e s s or  at  t he  U ni v e r s i t y  of  Sout he r n C al i f or ni a w he r e  he  t e ac he s  di gi t al  s i gnal  pr oc e s s i ng c our s e s   H e  r e c e i v e d hi s  B  Sc   i n e ngi ne e r i ng fr o m  th e  U n iv e r s ity  o f C a lifo r n ia  L o s  A n g e le s  a n d  th e  M S  and P h D   de gr e e s  i n  el ect r i ca l  en g i n eer i n g  f r o m  t h e Ca l i f o r n i a  I n s t i t u t e  o f  T e c h n o l o g y   P a s a d e n a   Ca l i f o r n i a   


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





