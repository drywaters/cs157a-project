Fast Scalable Selection Algorithms for Large Scale Data Lee Parnell Thompson Department of Computer Science University of Texas at Austin Austin TX Email parnell@cs.utexas.edu Weijia Xu Texas Advanced Computing Center University of Texas at Austin Austin TX Email xwj@tacc.utexas.edu Daniel P Miranker Department of Computer Science University of Texas at Austin Austin TX Email miranker@cs.utexas.edu Abstract Selection nding and its most common form median nding are used as a measure of central tendency for problems in biology databases and graphics These problems often require selection nding as a subcomponent where it can be called many times and as such speed is important The Map/Reduce framework has been shown to be an important tool for creating scalable applications There are a number of valid implementations of the selection algorithms inside of a Map/Reduce framework certain of which are compared in this paper However as the volume of data increases subtle theoretical algorithmic implementation differences can lead to signiìcant differences in practical application Therefore an efìcient and scalable selection nding method has the potential to provide general beneìt to a number of applications This paper compares algorithms that have been redesigned or created for the Map/Reduce framework for the purpose of selection nding or nding the k-th ranked element in an unordered set This paper takes the concepts used from two existing selection algorithms and translates them into a novel method using the Map/Reduce framework with two variations Each approach uses a different methodology to reduce the total amount of workload needed for a selection All the algorithms are compared together for scalability and efìciency in a computing cluster environment with up to 256 processing cores The results show that the methods proposed in this paper outperform several common alternatives in identifying medians with Hadoop including using sorting Pig and BinMedian methods Our implementations are also available upon request Keywords Hadoop Map Reduce Selection Algorithms Median Finding I I NTRODUCTION Finding the k th smallest element of an orderable set is a problem that underlies many applications in computer science and math and is called selection nding The most common use of selection nding is the computation of a median or the element of rank  N  2  that divides a set into equal numbers Selection nding is often used as a subproblem to more advanced algorithms For example the median point is used when dividing points equally inside a space and this action is performed recursively when building a balanced tree for indexes For large scale datasets and particularly algorithms that contain the selection as a subcomponent an increase in speed can be very important Within the big-data community the Map/Reduce MR framework is commonly used for speed and scalability while running on large scale MR is a frame w ork that splits algorithmic tasks into two phases a mapping and a reduction where each phase is done on a set of computing nodes The mapping phase takes a key/value input pair and produces a set of intermediate key/value pairs These intermediate key/value pairs are then distributed to nodes based on the intermediate where the reduce phase merges together the values to the nal desired result Hadoop is an open source java implementation of the MR framework that is widely used In this paper our proposed methods are implemented and compared using Hadoop framework though they should generalize to all MR frameworks Despite the popularity of Hadoop little research or analysis has been done on selection nding within this framework The relative simplicity of the selection algorithm can misguide users into using a nave approach a total sort of the entire dataset For example a search of publicly available code revealed that the naive approach a total sort of the entire dataset was the most common implementation Even the median nding algorithm inside of the OêReilly book MapReduce Design Patterns Building Effective Algorithms and Analytics for Hadoop and Other Systems used a total sort to obtain the median The few statistical libraries found including the most popular Hadoop library Pig also require the data to be completely Among speciìc implementations many were incorrect storing all elements within a single main memory datastructure or inefìcient requiring all data to be analyzed at some point by a single processor The computational cost of sorting the entire dataset is unnecessary for selection algorithms and can be reduced to a much smaller cost by only sorting the small range of data that are potential k th candidates The total sort on large datasets can be time consuming even for large systems for example in the 32-node cluster an optimized Terasort algorithm running on 1Terabyte of data took over 40 minutes Reducing the total sort to a partial sort can provide substantial reductions in time when only a k th element is needed Selection nding algorithms range in time from O  n 2  to O  n   the theoretical minimum bound Most trivial implementations of the selection problem simply sort the data 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 412 


Figure 1 Eliminating points by only operating on the bins that might contain the k-th element yielding a total ordering on the data giving access to all k th elements and a typical running time of O  n log n   Since a total ordering is unnecessary when only the k th element is needed quicker algorithms for determining the selection can be created The most well-known is the algorithm QuickSelect developed by C.A.R Hoares which has a best case run time O  n  but O  n 2  worst case time This worst case time can be improved to a guaranteed linear time O  n   by using the Median of Median\(MoM algorithm developed by to smartly select pi v o t points Even with these algorithmic improvements to the lower bound quick implementations are still necessary and desired especially when using selection as a subcomponent of algorithms working on large data sets or when taking advantage of parallel environments There are multiple selection algorithms that can be transferred to the Hadoop framework But an efìcient redesign of these selection algorithms for Hadoop requires understanding the handling of data so that work is not duplicated Notably many of the selection algorithms considered in this paper have a sorting step but this step may be skipped if implemented correctly in Hadoop Also even with knowledge of the framework the best implementation of any algorithm is not obvious In this paper we describe and compare several different parallel algorithms for performing the k th selection and the process of moving these parallel selection algorithms and onto a MR framework These algorithms are analyzed from the standpoint of wallclock CPU and network utilization to determine the best approach for the distributed systems Two new variations are also introduced MrS and MrT which use the same selection algorithm but with different implementations These two algorithms were designed to explore the inherent properties underlying MR to achieve a fast selection that is scalable for large datasets Both operate on the same premise of using a multi-phase MR that to perform the sorting on a small range of data The algorithms recursively perform partial sorts of the data into interval ranges nd the interval range that contains the k-th element and then end with a total sort of all elements within an interval range once the number of points contained are less than some threshold All of the algorithms described in this paper that avoid a total sort of the data run signiìcantly faster up to 5x times than the optimized TeraSort algorithm While this paper focuses on the Hadoop framework for implementation and results the concepts should be generalizable to other MapReduce frameworks II R ELATED W ORK The work in this paper is related to the selection nding problem as well as the MapReduce framework Selection nding is an algorithmic problem with a variety of solutions moving from O  n 2  solutions to the absolute theoretical lower bound of O  n   The naive approach is a simple sort of the data with a running time usually of O  n log n   The sorting approach is often acceptable for smaller datasets but faster techniques are needed when speed is required or data size is large C A R Hoare devised a fast selection technique called Quickselect  in 1961 which w a s modeled after his Quicksort  Quickselect runs in e xpected O  n  time and works by utilizing the idea that Quicksort can be used to solve the selection problem The difference between Quickselect and Quicksort is that at each step during the Quicksort only half of the data needs to be analyzed for Quickselect The half that needs to be sorted is based on which subset the k th element would be found in By performing a sort on only half of the data at every step the Quickselect algorithm can run in expected O  n  time Unfortunately Quickselect suffers the same drawback as Quicksort when bad pivots are chosen the worst case running time degenerates to a O  n 2  search In 1973 Manuel Blum authored the paper Time Bounds for Selection in which he pro vides a guaranteed w a y o f nding a good pivot in O  n  time This lowers the worstcase run time of Quickselect to the theoretical optimum of O  n   The technique Blum proposed is called the Median of Medians MoM and works by choosing a pivot that will split the data into at most a 30 and 70 split The MoM divides any set into 5 subsets where the algorithm is recursively applied on each subset until it contains less than 10 elements At this point the middle value is returned from the 10 elements and thereafter from each of the 5 children until the nal middle value is obtained which becomes the pivot While this is theoretically optimal in practice the additional time spent obtaining the theoretically sound pivot causes the algorithm to run longer in most expected cases than it would if simply using Quickselect with a random pivot 413 


A Map/Reduce and Hadoop MapReduce\(MR was developed by Google in a 2004 paper and since that time has been implemented in many languages and is used in numerous applications The most widely used implementation of the MR framework outside of Google is Hadoop Hadoop is a java implementation that is currently being used in many companies such as Facebook Adobe Ebay and numerous others Where the MR environment can be customized for a host of applications there are no advanced selection techniques that provide optimal speedup on the framework The Hadoop API is very expansive but a few particular classes are important for this paper Mappers Reducers Partitioners and Combiners The Mappers and Reducers are classes for the Map and Reduce logic respectively The Partitioners and Combiners are optimizations that can be used for increased performance Combiners are considered to be mini-reducers which run on map nodes and can merge together results from mappers before the data is sent to the Reducers The combiners are not guaranteed to be called but allow the amount of data trafìc across the network to be reduced in certain cases The other important class is the Partitioners Partitioners provide a way of mapping particular keys emitted from a Mapper to a particular reducer This can be very useful for load balancing or as in the case of selection algorithms you would like distinct nearby keys to be placed into the same reducer Apache Pig is a high le v e l language that w orks on top of Hadoop The language allows concise programs to be written for data analysis over a distributed framework Pig compiles scripts written in Pig Latin into a sequence of Map/Reduce programs Pig is often used to speed development of Map/Reduce programs that can be difìcult to reason or adapt directly to a parallel environments or for analysis due to the large number of methods for this purpose It is also extensible and allows programmers to create their own libraries or User Deìned Functions One of these libraries developed by company LinkedIn is called DataFu which has a set of functions for median deviation and other B Modern Work in Selection Finding Modern work has focused on improving the speed of the selection impro ving the multiple selection and utilizing parallelization[10 11 12  6 14 From these algorithms the Binmedian[8 was chosen as a base algorithm and is explained below This algorithm was chosen both because of itês speed and more importantly because it uses an easy binning criteria which is easily converted to a MR framework This binning approach can also be seen in Baderês work Ultraf Many of the other parallel selection algorithms require a master/slave approach which are more difìcult to convert into a framework that relies on node independence Parallelization of the selection has been done for shared and distributed systems Work by M Cafaro in 2009 sped up distributed memory systems and proposed tw o algorithms Both algorithms use similar methods to nd the selection one using a single weighted 3-median and the second using two weighted 3-medians Calculating the weighted 3-median nds three intervals where the data can be separated based on their most compact clustering The selection nding itself is found through recursive iterations where each iteration discards at least one-third of the data until the median is converged upon Tibshiraniês 2008 paper Fast computation of the median by successive binning introduced binmedian  Binmedian uses a binning approach as a means to recursively reduce the set of possible median points The algorithm works iteratively in four steps Given a dataset X  1 Compute the mean  and standard deviation  2 Form bins based on         3 Determine the bin containing the median 4 Recurse on the bin with the median III A LGORITHMS AND I MPLEMENTATION The algorithms described here for the selection problem all perform only a partial sort of the data The central concept is to recursively bin the data eliminate unnecessary bins and then sort the bin that can contain the k th item The two new algorithms presented in this paper are MrS and MrT Both use multi phase mappers/reducers to solve the selection nding problem Both algorithms also use the internal structures within Hadoop to save computational costs when possible The BinMedian MrS and MrT algorithms all run in O  n  total time see the proof in This proof holds for MrS and MrT as the methodology for choosing the partitions is similar and in some cases smaller A MrT MrT is set up in a two-stage process trying to hold closely to the MR idea of using both a Map and Reduce phase In the rst Map/Reduce the bins and bincounts are discovered while the second Map/Reduce performs the actual binning The algorithm starts by taking a random sampling of   1 data where  is the number of processors and  is a natural constant These sampled points are passed into a partitioner from which P number of bins are created where P is how many reducers are available The rst map phase takes data in as a key value pair and and uses the partitioner to determine which bin the pair will fall into The default mapper is used as it calls the partitioner which will emit the key value to the correct bin location Speciìcally the partitioner assigns every element x akey i based on P  i   x  P  i 1  The reducer which represents a single bin takes all points and emits a bin count 414 


Listing 1 MrT PseudoCode 1 class MrT  2 def run  3 while   S   threshold  4 Partitioner p   findPartitions S 5   Find bins and bin counts 6 DefaultMap  run  7 Reduce1  run  8 9  Distribute Points 10 int bin   getBin  11 p   findPartitions bin 12 DefaultMap  run  13 S   Reduce2  run  14 S   totalSort S 15 16 class Partitioner  17 findPartition key  value  18 int index   findCorrectReducer key 19 value  clear 20 return index 21 22 class Reduce1  23 int reducerNumber   Set to appropriate value 24 def reduce key  values    25 context  counters  inc rement reducerNumb er  values  l ength  26 27 class Reduce2  28 def reduce key  value    29 for value in values  30 emit key  value  The second stage of MrT takes the interval range of the bin that will contain the k-th element and creates a new set of P bins between this interval The map phase then iterates over every element x and assigns it to a reducer if it belongs in the new bins or emit nothing if it falls outside of the range of new bins The algorithm terminates when the amount of remaining data is less than some constant C  Once the number of points is less than this threshold a total sort of the data is performed from which the selection is trivial The total running time broken into parts is O  N  for counting the number of elements inside of the bins which is then iterated O log N  times The nal sorting step is O  N log N  but on a fraction of the original points for a total runtime of O  N   See appendix A in the BinMedian for a complete proof of runtime for this style of B MrS MrS uses a similar approach to MrT but differs by using strictly Mappers in two passes over the same data The idea is to minimize the amount of data needing to be transferred between nodes when calculating the correct bin which happens between the map and reduce phase Once the correct bin has been determined the second pass over the data distributes the points across the network as usual MrS is set up as a two-stage map with no reducers and is performed in a recursive manner as follows The same precursor step of choosing the random sampling of   1 data is performed and the Partitioner creates P bins The rst map phase though instead of passing the data to a partitioner performs the bin calculation itself Hadoop will only use partitioners if there are reducers and in this case there are none The mapper increments the tally of bin counts but emits nothing The second stage of MrS takes the interval range of the bin that will contain the k-th element and creates a new set of P bins between this interval The second map phase then iterates over every element x and will either emit the key/value if it belongs in the new bins or emit nothing if it falls outside of the range of new bins Listing 2 MrS PseudoCode 1 class MrS  2defrun\(S 3 setNumberOfReducers 0 4 while   S   threshold  5   Find bin counts 6 Map1 setPartitions  findPartitions S 7Map1.run 8 9  Distribute points 10 int bin   getBin  11 Map2 setPartitions  findPartitions b ge tPoints  12 S   Map2  r u n   13 S   totalSort S 14 15 class Map1  16 Partitions  partitions  17 def map\(key  value  18 partition   findPartition  pa rtitions  key 19 context  counters  inc rement  partition 1 20 21 class Map2  22 Partitions  partitions  23 def map\(key  value  24  findPartition will return null if out of range 25 partition   findPartition  pa rtitions  key 26 if  partition not null  27 emit key  value  C BinMedian The BinMedian algorithm is also an iterative algorithm that stops when the number of points remaining is less than a The original BinMedian uses the e xact mean and median as a starting point to calculate the number of bins within the range          but to avoid an additional Map/Reduce phase this was skipped in favor of a random sampling of the distribution From this random sampling a Partitioner was created with the P bins on the range          The mapper in the BinMedian algorithm simply emits the key/value pair passed in and lets the Partitioner choose which Reducer will be used To prevent the default sorting by keys that occurs inside of Hadoop the normal key was changed to the number of the reducer so that every reducer receives only one key The Reducers simply emit the original key/value to form a series of P bins with unsorted data The appropriate bin is then chosen and the algorithm continues Listing 3 Binmedian PseudoCode 1 class BinmedianAdapted  415 


2defrun\(S 3 while   S   threshold  4   Find bins 5  findPartitions will calculate the mean and 6  deviation for a subset of points 7 Partitioner p   findPartitions S 8Map1.run 9 Reduce1  run  10 11  get the correct bin 12 int bin   getBin  13 S   getFile bin 14 S   totalSort S 15 16 class Map1  17 def map\(key  value  18 partition   findPartition key 19 emit partition  append\(key  value  20 21 class Reduce1  22 int reducerNumber 23 def reduce key  values    24 for value in values  25 k  v   split  value  26 emit k  v D Algorithmic Analysis The partial-sort stages of MrS and MrT have the following expected runtime assuming  processors The map step rst divides N points to  processors where each processor performs a linear amount of work   for an expected time of N    During the shufîe sort segment of the map step an additional  log  work is done for each of the  processors leading to N      log   and assuming N leads to O  N    The nal total-sort stage of MrS and MrT is simply a sort of the data remaining O  N  log N    In practice the amount of data in this stage is small and theoretically can be shown that the entire algorithm including this sort is still O  N   E Test Setup All test sets were created using the TeraGen program TeraGen is a program included in the Hadoop distribution which creates n data points of 100 bytes each The generated points are a 10 byte key 2 bytes of space and an 88 byte random value for a total of 100 bytes per point Algorithm speed comparisons were run on one terabyte of data The speed factors considered were total time measured from the start of an algorithm until the result was written to HDFS The time it takes to place the unsorted data into HDFS is not included This means that generating the key/value pairs using TeraGen and the corresponding write to disk are not part of the total time Other metrics such as Map time and Reduce time are also included when appropriate While these times are accurate it must be noted that in most cases Hadoop starts the Reduce phase while the Map phase is still running so there is overlap in the recorded times Apache Pig is a commonly used framework for writing easy queries on top of The most commonly used Figure 2 Algorithm CPU Comparison Shows the average processor utilization at each time step for 1TB of data Figure 3 Algorithm Network Comparison Shows the average of the bytes sent for all processors at each time step for 1TB of data statistical package for Pig is named DataFu which contains a method for Median selection which was used in this paper The Pig version used in this paper was Version 0.11.1 Scalability results were run on 100 gigabytes of data with the cluster size ranging from 16 cores to 256 cores as each node is 8 cores this means 4 through 32 nodes were used All tests were performed on TACC Longhorn Relevant machine specs each node is a SunBlade x6420 with two AMD Opteron Quad-Core 64-bit processors running at 2.3 GHz Each node has 48 GB of RAM running at 667 MHz DDR2 Nodes are connected with Sun InìniBand Full machine specs can be found at the TACC Software versions used for this paper Java 7u27 with Hadoop version 0.23.7 IV R ESULTS D ISCUSSION The performance of the algorithms is based around the time required to complete the map/reduce phases the number of iterative steps and the data transfer For a parallel selection algorithm the processing steps are as follows rst discovery of the number and choice of bins so that points can be distributed amongst the different processors second comparison of the points and insertion into the appropriate 416 


bins and nally either a sort of one or more bins or a re-run of the algorithm on one of these bins Many of these processing steps can be parallelized at the cost of transferring the data Pig and Terasort are signiìcantly slower than the selection algorithms as they perform a total sort on all the data Most of the Pig results are not shown on the charts as they are over 2x slower than Terasort As can be seen in Figure 4 Terasorts demarcation between the map and reduce phase is very clear In the map phase each point is assigned a bin and distributed to a reduce node During the map phase the average CPU usage is very high but starts decreasing as nodes start nishing their point distribution and become idle Once the Reduce phase starts then the CPUs are again mostly utilized until they nish their sort of the data As can be seen the data distribution during the Map phase is very low until nodes start nishing their distribution where they need to transfer their points to the Reducers The Reducer phase shows a continuous high data transfer as the results are written back to the HDFS The main problems with Terasort are during the map phase node CPU is under utilized as each node begins transferring data or nishes processing and during the reduce phase the time spent sorting the data accounts for about half of the time of the total algorithm 4 So we can expect that by simply not performing a total sort we should be able to create selection algorithms that are twice as efìcient The algorithms that only do a partial sort of the nal data are between 2x and 4x times faster than TeraSort on the 1TB dataset As can be seen in Figures 5 2 the BinMedian CPU curve during the map phase is similar to Terasort but less CPU is used and the phase is much quicker The reduced CPU comes from emitting already sorted keys to the reducers skipping the additional overhead of the default Hadoop sort This leads to both less CPU used and a quicker map phase The CPU proìles in Figure 6 and Figure 7 show the two stage approach of nding the bin rst then distributing the points First it is important to notice that both approaches show signiìcant speedups over Terasort despite performing redundant work in the form of repeating the bin calculations for each point twice More importantly despite having nearly the same algorithm for selecting the bins and distributing the points they have very different CPU and network proìles Especially noticeable is the CPU utilization and network utilization between MrS and MrT MrS is the only algorithm to have consistently high CPU utilization that does not decrease as the mapping phase completes The reason for this is that it needs no data transfer in between its stages The network overhead of transferring data inside of Hadoop is a bottleneck for CPU use that is eliminated So it appears that performance of the algorithms is tied to the data transfer As the Hadoop framework transfers data Figure 8 Scalability Results showing the decrease in the total amount of time needed for each algorithm as a function of the number of processors used for 100G of data Number of processors include the Namenode\(which doesnêt perform work directly from one node to another the CPU utilization is reduced and the time increases This effect is exacerbated on networks that utilize a replication factor for redundancy Since Hadoop places the data a priori on the nodes it is more efìcient to perform calculations on the same data if possible The MrS algorithm performs multiple passes on the same data in successive Map only phases The rst sweep through the data merely counts how many items will be in each bin and performs no other calculations so no data is transferred In the second sweep data that belongs to the appropriate bin are written out As each phase is discrete no reducers will be started until all maps are completed and all network trafìc is cabined to the end of each Map phase This approach despite more passes on the same data outperforms the others because the data transfer is minimized The speedup of the algorithms for the 100G dataset does not show a linear improvement see Figure 8 This shows a general trait of Hadoop that it is built for very large scale datasets As shown in the other results the data transfer in Hadoop can be very costly With this small size of dataset as the nodes increased the ratio of the network overhead versus the CPU time becomes a bottleneck for the algorithms In this case algorithms that minimized the amount of trafìc back and forth showed the best time improvement Runs with larger data sets would be expected to demonstrate the linear speedups provided by these algorithms because the transfer time would comprise a smaller portion of total run time V C ONCLUSION Many algorithms in the elds of indexing biology and statistics have a subcomponent that utilizes a k th point especially a median and that is often called recursively As selection nding is often a step where a total order is not needed having fast selection algorithms can greatly improve the overall running time of these algorithms The MapReduce model allows for the use of these algorithms to scale linearly on large scale datasets The open source java 417 


Figure 4 TeraSort Proìle showing the CPU usage CPU Bytes sent across the network send Bytes written to disk writ and the percentage complete d of the map/reduce phase Figure 5 BinMedian Proìle showing the CPU usage CPU Bytes sent across the network send Bytes written to disk writ and the percentage completed of the map/reduce phase implementation of MapReduce Hadoop was used as for the comparison but the underlying principles of the network and CPU utilization should generalize to other MapReduce frameworks Many selection algorithms can easily be made using the Hadoop framework but it is a non-obvious problem as to which approach will best take advantage of the Hadoop system This paper shows the implementation of several algorithms but more importantly shows the underlying reasons which explain why each algorithm performs the way it does Exposing the underlying CPU I/O access and data transfer for each of the algorithms gives concrete evidence about not only these algorithms but also ideas on how to write future Hadoop programs Several approaches to the selection problem utilizing a variety of algorithms and implementations were analyzed here in an effort to understand what is the best way to use Hadoop to solve the selection problem As with many high performance parallel systems reducing the amount of data passed between nodes and ensuring a relative load balance among processors is important Some of the approaches that can be used include naively sorting changing the binning criteria from Mappers to Partitioners moving work from the Mapper to the Reducer stage or visa versa or 418 


Figure 6 MrT Proìle showing the CPU usage CPU Bytes sent across the network send Bytes written to disk writ and the percentage completed of the map/reduce phase Figure 7 MrS Proìle showing the CPU usage CPU Bytes sent across the network send Bytes written to disk writ and the percentage completed of the map/reduce phase even eliminating one of the Map/Reduce phases A correct selection algorithm can be achieved through any of the approaches but given the multitude of options guidance is needed to direct programmers toward more optimal solutions In particular it is clear that avoiding data transfer is desirable in Hadoop even if it means performing multiple stage Map/Reduce algorithms From the results shown here it can be seen that the preferable approach is utilizing an algorithm that performs multiple Map phases over the same data to eliminate any sorting steps or data transfer until the last possible moment Using this approach selection of a point can be sped up by several times over the optimized TeraSort or other selection algorithms An efìcient and scalable selection nding method has the potential to provide general beneìt to a number of applications The results show that the methods proposed in this paper out perform several common alternatives in identifying medians with Hadoop including using sorting Pig and BinMedian methods If incorporated into some of the popular statistical packages for Hadoop all users will gain quicker access to the results they need 419 


A CKNOWLEDGEMENTS The authors acknowledge the Texas Advanced Computing Center TACC at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported within this paper URL:http://www.tacc utexas.edu This research was partially supported by the National Institutes of Health NIH Grants R01GM08533703 R EFERENCES  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters Commun ACM  vol 51 no 1 pp 107Ö113 Jan 2008  C Olston B Reed U Sri v asta v a  R  K umar  and A T omkins Pig latin a not-so-foreign language for data processing in Proceedings of the 2008 ACM SIGMOD international conference on Management of data  ser SIGMOD 08 New York NY USA ACM 2008 pp 1099Ö1110  M Blum R W  Flo yd V  Pratt R L Ri v est and R E Tarjan Time bounds for selection J Comput Syst Sci  vol 7 no 4 pp 448Ö461 Aug 1973  C A R Hoare  Algorithm 65 nd  Commun ACM  vol 4 no 7 pp 321Ö322 Jul 1961  C A R Hoare Quicksort  The Computer Journal  vol 5 no 1 pp 10Ö16 1962  M Caf aro V  De Bene and G Aloisio Deterministic par allel selection algorithms on coarse-grained multicomputers Concurr Comput  Pract Exper  vol 21 no 18 pp 2336 2354 Dec 2009  S Stadtm  uller S Speiser A Harth and R Studer Datafu a language and an interpreter for interaction with read/write linked data in Proceedings of the 22nd international conference on World Wide Web  ser WWW 13 Republic and Canton of Geneva Switzerland International World Wide Web Conferences Steering Committee 2013 pp 1225Ö1236  R J T ibshirani F ast computation of the median by successive binning Computing Research Repository CoRR  vol abs/0806.3301 2008  H Prodinger   Multiple quickselect&mdash;hoare s nd algorithm for several elements Inf Process Lett  vol 56 no 3 pp 123Ö129 Nov 1995  S G Akl  A n optimal algorithm for parallel selection  Inf Process Lett  vol 19 no 1 pp 47Ö50 Sep 1984  P  Gupta and G P  Bhattacharjee  A parallel selection algorithm BIT  vol 24 no 3 pp 274Ö287 1984  D A Bader   An impro v ed randomized algorithm for parallel selection with an experimental study J Parallel Distrib Comput  vol 64 no 9 pp 1051Ö1059 Sep 2004  S Rajasekaran Randomized parallel selection  i n Proceedings of the tenth conference on Foundations of software technology and theoretical computer science  ser FST and TC 10 New York NY USA Springer-Verlag New York Inc 1990 pp 215Ö224  L Monroe J W endelber ger  and S Michalak Randomized selection on the gpu in Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics  ser HPG 11 New York NY USA ACM 2011 pp 89Ö98  T e xas Adv anced Computing Center   T e xas adv anced computing center http://www.tacc.utexas.edu 2006 On A v ailable  http://www.tacc.utexas.edu/user-services user-guides/stampede-user-guide  420 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





