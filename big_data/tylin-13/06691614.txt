DL-MPI Enabling Data Locality Computation for MPI-based Data-Intensive Applications Jiangling Yin Andrew Foran and Jun Wang Department of Electrical Engi neering  Computer Science University of Central Florida Orlando Florida 32826 jyin jwang@eecs.ucf.edu Abstract Currently most scienti c applications based on MPI adopt a compute-centric architecture Needed data is accessed by MPI processes running on different nodes through ashared le system Unfortunately the explosive growth of scienti c data undermines the high performance of MPI-based applications especially in the execution environment of commodity clusters In this paper we present a novel approach to enable data locality computation for MPI-based data-intensive applications and refer to it as DL-MPI DL-MPI allows MPIbased programs to obtain data distribution information for compute nodes through a novel data locality API In addition the problem of allocating data processing tasks to parallel processes is formulated as an integer optimization problem with the objectives of achieving data locality computation and optimal parallel execution time For heterogeneous runtime environments we propose a scheduling algorithm based on probability to dynamically schedule tasks to processes by evaluating the unprocessed local data and the computing ability of each compute node We demonstrate the functionality of our methods through the implementation of scienti cdata processing programs as well as the incorporation of DL-MPI with existing HPC applications Keywords MPI Hadoop le system HPC application I I NTRODUCTION Scienti c analytic programs are usually developed with MPI and run parallel processes to perform analysis on different portions of a large data set For example bioinformatics 1 a nd s cienti c visualization applicationsconstitute an emerging category of scienti c applications that perform sophisticated calculation routines on a given dataset Usually anetwork le system is used to store the dataset which will transfer data to the compute nodes for processing Unfortunately the rapidly growing datasets 2 pos e a great challenge for these applications and can limit performance due to long network wait time Accordingly this leads to a reduction in scalability and an overall inability to handle large data sets well Distributed le systems constructed from machines with locally attached disks can scale with the problem size and number of nodes as needed For instance the Hadoop Distributed File System HDFS is designed for MapReduce applications and allows programs to realize local data access and avoid data movement in the network The idea behind HDFSisthatitisfasterandmoreef cient to send the compute executables to the stored data and process data locally rather than to pull the data from storage and send it via a network to the compute node Compared to the MPI programming model however MapReduce does not allow for the exibility and ef ciency of complexity expression in the implementation of scienti c applications Therefore to address the issues associated with the growth of data we present a novel approach in support of Data Locality computation for MPI-b ased data-intensive applications DL-MPI We propose a data locality API to allow MPI-based programs to retrieve data distribution information from the underlying distributed le system The proposed API is designed to be easily integrated into existing MPIbased applications or new MPI programs In addition to balance data locality computation and the parallel execution time in heterogeneous environments we propose a novel probability scheduler algorithm which schedules data processing tasks to MPI processes through evaluating the unprocessed local data and the computing ability of each compute node Finally we demonstrate the functionality of our method through the implementation of data analytic applications as well as the incorporation of DL-MPI with mpiBLAST an open-source parallel BLAST tool II DL-MPI D ESIGN AND M ETHODOLOGIES In this section we will present the design and methodologies of DL-MPI After giving our design goals and system architecture we describe an API for MPI-based programs to retrieve the data distribution information among nodes from a distributed le system We also discuss data resource allocation which is involved in the assignment of data processing tasks to compute processes A Design Goals and System Architecture We aim to provide a generic approach for enabling MPIbased data-intensive applications to achieve data locality computation using a distributed le system There are two main issues we need to address 1 The MPI programming model doesnít have an interface that allows MPI programs to retrieve data distribution information from underlying storage 2 An effective and ef cient scheduler to assign 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 506 


                                                         Figure 1 The DL-MPI software architecture data processing tasks to parallel processes in a heterogeneous running environment is needed To address these dif culties we propose an API which is convenient for MPI-based programs to retrieve data distribution information from an underlying distributed le system In addition a scheduler algorithm based on probability is proposed to determine data to process scheduling The scheduler aims to balance parallel execution time and data locality computation through evaluating the unprocessed data and data processing speed of each computing node The DL-MPI system consists of two important components a Data Locality Interface and a Data Resource Scheduler as illustrated in Figure 1 The data to be processed is stored along with several copies in a distributed le system which supports scalable data access Through our DL-MPI system the MPI-based application on top could do complexity computation and achieve data locality computation with HDFS as the underlying storage system B Data Locality Interface for MPI-based Programs In this section we describe the API for retrieving data distribution information from an underlying distributed le system deployed in a disk-attached cluster As we discussed with traditional MPI-based programming architectures data lo cality is not considered and it is not necessary to have a data location querying interface However with the co-located st orage and analytic processes data locality should be considered to gain high I/O performance especially for massively parallel applications with big data as input Our data locality interface is proposed to enable MPI-based program s to take advantage of locality computation in data-centric architectures By allowing MPIbased programs to query locality information the programs can ef ciently map compute processes to data We show a typical example of partitioning data across involved MPI processes as follows where each MPI process statically calculates its accessing offset based on the rank  MPI Comm rank rank Table I D ESCRIPTIONS OF D ATA L OCALITY API IN DL-MPI int DLI map process chunks\(char char void Retrieves the chunk list of a dataset co-located with a given process The arguments are dataset le name process name and return list buffer int DLI map chunks process\(char char void Builds the map of chunks local to a group of compute processes given a dataset The arguments are dataset le name processes name list and return map buffer int DLI map Dprocess Dchunks\(char char void Builds the maps of chunks local to a group of processes given a directory The arguments are directory name pro cesses name list and return map buffer int DLI get total chunks\(char char int Retrieves the total number of chunks of a dataset local to a given process The arguments are dataset le name process name and return number buffer int DLI get data percentage\(char char double Retrieves the percentage of a dataset local to a given process The arguments are dataset le name process name and return buffer int DLI check data\(char size t bool Check whether a given offset of a dataset is local to a given process The arguments are dataset le name offset and return buffer  offset  rank  Avg access  MPI File open fh  MPI File read at\(fh offset where rank is the process id  Avg access is access range per process and fh is the le pointer When such applications are compute-intensive the performance will not be affected by the data partitioning and assignment However when the applications become data-intensive the data locality is more relevant because more data potentially needs to be transferred to compute nodes Thus a better I/O performance could be achieved by knowing the location of data fragments and assigning MPI processes the fragments that are local to them We chose to build our API on the Hadoop Distributed File System HDFS for this study which is actually designed for MapReduce programs The API set for data locality computation is summarized in Table I To retrieve the local chunks mapping to a process on a speci c dataset a retrieval function  DLI map process chunks  is required which computes the data range of a given dataset that is local to a given process Chunk is the storage unit 64 MB by default in HDFS Another basic function is DLI map chunks process  which builds the map of all processes to their local chunks on a given dataset In order to get general distribution of a dataset these functions include DLI get total chunks DLI get data percentage  which help obtain the number of chunks local to a process and the percentage of a dataset co-located with a given process respectively Moreover the function of DLI data check is used to check whether a given offset of a dataset is local to a given process and the function of DLI map Dprocess Dchunks is to return the location information of all the les under a directory These functions give client applications the ability to make scheduling decisions based on data locality which can reduce data movement over the network To show how the API is executing for a call from a client application we take the DLI map process chunks function 507 


as an example and demonstrate how it works For a given le name the steps involved are 1 retrieve the le id based on the le name 2 get the chunk size and calculate the range of each HDFS chunk 3 for each chunk retrieve a list of datanodes with that chunk on its local hard disk 4 store the chunk index for each chunk found in step 3 that is local to the given process 5 write the chunk-map-process information to the buffer Currently we take the data as atles and more functions are needed for dealing with highlevel data format like NetCDF and HDF5 C Data Resource Allocation To achieve data locality computation for data-intensive applications not only the information of data distribution among nodes is required but also an effective strategy for mapping data to processes is needed 1 Problem Formalization To allow a process to achieve data locality computation a simple way is always to assign task to the process which has access to a local chunk However with such a greedy strategy there exist several heterogeneity issues that could potentially result in low execution performance First the HDFS random chunk placement algorithm may pre-distribute the target data unevenly within the cluster leaving some nodes with more local data than others Second the execution time of a speci c computation task could vary a lot among different nodes due to the heterogeneous run-time environment e.g multiple users and multiple applications These issues could make the processes in some compute nodes take remote computation and thus have a long I/O wait time We formally discuss the assignment issue next Our goal is to assign data processing tasks to parallel MPI processes running on a commodity cluster such that we can achieve a high degree of data locality computation and minimize the parallel execution time Let the total number of chunks of a dataset be n  and the total number of nodes be m  To easily discuss our problem we suppose the process i run on node i  We denote D   j d j where d j is the j th chunk j  1 n  and P   i p i where p i is the i th process i  1 m   Through the Data Locality Interface we can identify all the local chunks for the process i Let L i  l ik   where l ik   1 if d k is local to p i 0 otherwise F i  f ix   where f ix   1 if d x is processed by p i 0 otherwise Suppose the optimal parallel execution time for D being OP T D and the execution time of process p i on assigned data F i being T  F i   The goal of the assignment algorithm is to assign chunks for each process i that maximizes the degree of data locality and the degree of execution time optimization subject to a constraint that the union of chunks processed by all processes covers the entire D  More formally we need to solve for F i in the following optimization problem maximize   P  D    P  T     1  i  m  L T i F i   D     OP T D max 1  i  m T  F i  1 subject to  1  i  m F i  1  1    1 2 Where  and  are the parameters representing weights of the two objectives Suppose the process speed of each cluster node could be estimated according to historical execution performance and the time to nish processing the chunks is determined when a F i is given The OP T D is a constant reference value and given a xed  and   e.g 0.5 the problem can be converted into an integer programming problem and be solved using the CPLEX solver 3 However in a heterogeneous environment the process speed is not xed but changes with time and we always have a inconsistent evaluated T  F i  from time to time To get an effective assignment we should re-evaluate the running situation every time we assign a task to a process Thus a fast and dynamic algorithm is a must 2 Probability based Data Scheduler In this section we introduce the probability based data scheduler algorithm that balances data locality computation and the parallel execution time among MPI processes The probability scheduler algorithm takes the distribution information of unprocessed data chunks and the dataprocessing speed of each node as input The output of the algorithm is the assignment of a data processing task for a process The pseudo code of the probability scheduler algorithm is listed in Algorithm 1 Whenever a process i requests a task we initially try to launch a task with its requested chunks stored at the node We calculate for each candidate chunk d x the probability to be assigned based on an estimated execution time of other processes Due to the replication mechanism of HDFS for each candidate chunk d x wemay have other processes j that could take local computation on d x  We estimate the remaining local execution time of these processes j  as the number of all unprocessed chunks on process j divided by the process speed s k  We choose the minimum execution time over all these processes j to decide the probability for assigning d x to process i Thisis because the process with less remaining local execution time will have a larger probability to take remote computation in future That is T d x min 1  k  n,d x  D k k  i   D k  s k  3 508 


The probability of assigning d x to the requesting process is calculated as the following equation P  d x  T d x  X  D i T X 4 Algorithm 1 Probability based Data Scheduler Algorithm 1 Let C   c 1 c 2   c n  be the set of participating nodes 2 Let S   s 1 s 2   s n  be the data-process speed set of n nodes 3 Let D   d 1 d 2   d m  be the set of unprocessed data chunks 4 Let D i be the set of unprocessed data chunks located on node i  Steps 5 while  D   0 do 6 if an mpi process on node i requests a new task then 7 Update s i according to the historical execution performance 8 if  D i   0 then 9 for d x  D i do 10 T d x min 1  k  n,d x  D k k  i   D k  s k  11 end for 12 for d x  D i do 13 P  d x  T d x  X  D i T X 14 end for 15 Assign d x to the process on node i with probability P  d x  16 else 17 for d x  D do 18 T d x min 1  k  n,d x  D k k  i   D k  s k  19 end for 20 for d x  D do 21 P  d x  T d x  X  D i T X 22 end for 23 Assign d x to the process on node i with probability P  d x  24 end if 25 Remove d x from D 26 for all D k s.t d x  D k do 27 Remove d x from D k 28 end for 29 end if 30 end while The probability assignment algorithm could be implemented in applications with dynamic scheduling algorithms such as mpiBLAST in which scheduling is determined by what nodes are idle at any given time This kind of scheduling adopts a master-slave architecture and the assignment algorithm could be incorporated into master process The probability assignment algorithm could also be implemented in applications with static scheduling such as ParaView which uses static data partitioning so the work allocation can be determined beforehand For this kind of scheduling we can assume a round-robin request order for assignment in Step 6 of Algorithm 1 III E XPERIMENTS AND A NALYSIS A Experimental Setup We conducted comprehensive testing on our proposed DLMPI at both Marmot and CASS clusters Marmot is a cluster of PRObE on-site project and housed at CMU in Pittsburgh The system has 128 nodes  256 cores and each node in the cluster has dual 1.6GHz AMD Opteron processors 16GB of memory Gigabit Ethern et and a 2 TB Western Digital SATA disk drive CASS consists of 46 nodes on two racks one rack including 15 compute nodes and one head node and the other rack containing 30 compute nodes Each node is equipped with dual 2.33GHz Xeon Dual Core processors 4GB of memory Gigabit Ethernet and a 500GB SATA hard drive In both clusters MPICH 1  4  1 is installed as parallel programming framework on CENTOS55-64 with kernel 2.6 We installed PVFS2 version 2  8  2 with default con guration on the cluster nodes We chose Hadoop 0  20  203 as the distributed le system which is con gured as follows one node for the NameNode/JobTracker one node for the secondary NameNode and other nodes as the DataNode/TaskTracker B Evaluating DL-MPI 1 I/O performance of DL-MPI In this section we measure the performance of our DL-MPI using a benchmark application developed with MPI and the proposed API The benchmark application uses a master-slave implementation of the described scheduling algorithm Speci cally a single MPI process is developed to dynamically assign data processing tasks to slave processes which execute the assigned tasks In our benchmark program we analyze genomic datasets of varying sizes Since we are more concerned with the I/O performance of DL-MPI our test programs read all gene data into memory for sequence length analysis and exchange messages between MPI processes We rstly compare the process time of our benchmark program using DL-MPI probability based scheduling referred to as With DL-MPI and non locality scheduling referred to as Without DL-MPI on CASS for variable le sizes We use 32 nodes for these experiments and show the performance comparison in Figure 2 From the gure we nd that the benchmark program using DL-MPI consistently obtains much lower process times than without using DLMPI With increased size of input data the running time increases quickly for the benchmark program without using DL-MPI We also compare the bandwidth performance through varying the number of nodes in the Marmot cluster We 509 


     Figure 2 Process time Comparison of our benchmark program with and without DL-MPI on CASS using variable le sizes             Figure 3 Bandwidth comparison of our benchmark program on PVFS and HDFS using DL-MPI on Marmot using variable number of nodes measure the bandwidth for processing a 1 TB le and show the bandwidth comparison in Figure 3 We nd a massive improvement for the benchmark program using DL-MPI as the number of nodes increases resulting in approximately a four fold increase when the number of nodes reaches around 100 We also see the bandwidth of the benchmark program using PVFS begin to level off at around 60 nodes while the test with DL-MPI scales much better 2 mpiBLAST with DL-MPI BLAST algorithms are widely used in the study of biological and biomedical research They compare a query sequence with database sequences by a two-phased heuristic-based alignment algorithm mpiBLAST 1 i s a p a rallel im p lem en tatio n o f N CBI BLAST It organizes all parallel processes into one master process and many worker processes Before performing the actual search the raw sequence database is formatted into many fragments and stored in a shared network le system It implements a dynamic load b alancing scheduler which schedules search tasks on available idle nodes The worker process gets the requested data from the network parallel le system In our experiments we use mpiBLAST as an example of existing HPC applications to make use of DL-MPI and achieve data locality computatio n DL-mpiBLAST in brief We incorporate the probability scheduler algorithm into the master scheduler of mpiBLAST Whenever the master process is going to assign a new task to an idle process the         Figure 4 The execution performance comparison of DL-mpiBLAST and default mpiBLAST master process will call the Probability scheduler For the data processing speed we initialize it as 1 for all processes and update it by tracking the number of tasks nished by the process The nt nucleotide sequence database is selected as our experimental database At the time when we did the experiments the total raw size of the nt database is about 53 GB The input queries to search against the nt database are randomly chosen from nt and revised which guarantees that we nd some close matches in the database The total query size is around 60KB and the output is about 9.7 MB We track the parallel execution time for DL-mpiBLAST on HDFS and mpiBLAST on PVFS and show the performance comparison in Figure 4 From the gure we can nd that the parallel execution time of DL-mpiBLAST is smaller than that of mpiBLAST Given an increasing cluster size DL-mpiBLAST reduces overall execution time as well However mpiBLAST using PVFS does not scale well as the increasing number of nodes This indicates the data movement overhead in the baseline does become a performance barrier to the system scalability IV R ELATED W ORK With the explosive growth of data research works 5 6  7  8  a re pres e nted to s o lv e t he data mo v e ment and data management Specially SciMATE 9 is a framework that is developed to improve the I/O performance by allowing scienti c data in different formats to be processed with a MapReduce like API Kshitij et al 10 d e v el oped a new plugin for HDF5 using PLFS to convert the singlele layout into a data layout that is optimized for the underlying le system Jun et al 11 d emons trated ho w p atterns of I/O within scienti c applications can signi cantly impact the effectiveness of the underlying storage systems and utilized the identifying patterns to mitigate the I/O bottleneck These methods improve the I/O performance though using data access regularity While our DL-MPI improves the I/O performance by capitalizing o n data locality computation The data locality provided by a data-intensive distributed le system is a desirable feature to improve I/O perfor510 


mance This is especially important when dealing with the ever-increasing amount of data in parallel computing Mesos 12 i s a platform for s h aring c ommodity clus ters between multiple diverse cluster computing frameworks Mesos shares resources in a ne-grained manner allowing frameworks to achieve data lo cality by taking turns reading data stored on each machine CloudBLAST 13 a dopts a MapReduce paradigm to parallelize gnome index and search tools and manage their executions in the cloud VisIO 14 obtains a linear scalability of I/O bandwidth for ultrascale visualization by exploiting data locality of HDFS The aforementioned data movement solutions work in different contexts from DL-MPI V C ONCLUSION In this paper we propose a data locality interface and a dynamic scheduler to support exible mapping of compute processes for MPI-based data-intensive programs allowing them to take advantage of the l ocality information provided by HDFS The interface is designed to be easily adopted by existing MPI programs so that traditional applications can take advantage of it without massive code rewriting By testing our DL-MPI prototype system with real-world data we saw signi cant performance improvement over traditional MPI architectures These improvement were enhanced even further by increasing the data and cluster size We also saw performance and scalability improvement in mpiBLAST by using our DL-MPI probability scheduler algorithm By allowing for the easy integration of data locality awareness in the MPI programming model in the form of our DLMPI we believe that MPI-bas ed data-intensive applications can ef ciently run on commodity cluster VI A CKNOWLEDGMENTS This work is supported in part by the US National Science Foundation Grant CCF-0811413 CNS-1115665 and National Science Foundation Early Career Award 0953946 R EFERENCES 1 A  D ar l i ng L  C a r e y  and W  c F e ng T he desi gn i m plementation and evaluation of mpiblast Proceedings of ClusterWorld  vol 2003 2003 2  S uper c omput i n g 2008 e x ascal e w or kshop  http://www.lbl.gov/CS/html/SC08ExascalePowerWorkshop index.html   C p l e x l i n ear programmi ng  http://www.aimms.com/aimms/solvers/cplex  J  C  B ennet t  H Abbasi  P  T  Bremer  R  G rout  A  G yul assy  T Jin S Klasky H Kolla M Parashar V Pascucci P Pebay D Thompson H Yu F Zhang and J Chen Combining insitu and in-transit processing to enable extreme-scale scientific analysis in Proceedings of the International Conference on High Performance Computing Networking Storage and Analysis  ser SC 12 Los Alamitos CA USA IEEE Computer Society Press 2012 pp 49:1ñ49:9 5 X  W u K V i j a yakumar  F  M uel l e r  X Ma a nd P  C  Roth Probabilistic communication and i/o tracing with deterministic replay at scale in Proceedings of the 2011 International Conference on Parallel Processing ser.ICPP 11 Washington DC USA IEEE Computer Society 2011 pp 196ñ205  S  L akshmi narasi mhan J  J enki ns I  A rkat kar  Z  Gong H.Kolla,S.-H.Ku,S.Ethier,J.Chen,C.S.Chang,S.Klasky R Latham R Ross and N Samatova Isabela-qa Querydriven analytics with isabela-compressed extreme-scale scienti cdata,îin High Performance Computing Networking Storage and Analysis SC 2011 International Conference for  2011 pp 1ñ11  H A vron and A  G upt a Managi n g d at a-mo v e ment for effective shared-memory parallelization of out-of-core sparse solvers in Proceedings of the International Conference on High Performance Computing Networking Storage and Analysis  ser SC 12 Los Alamitos CA USA IEEE Computer Society Press 2012 pp 102:1ñ102:11 8 Z  Z hang D S  Kat z  J  M  W ozni ak A  E spi nosa and I Foster Design and analysis of data management in scalable parallel scripting in High Performance Computing Networking Storage and Analysis SC 2012 International Conference for  2012 pp 1ñ11  Y  W a ng W  Ji ang and G  A gra w a l  S ci mat e  A no v e l mapreduce-like framework for multiple scienti cdataformats in Proceedings of the 2012 12th IEEE/ACM International Symposium on Cluster Cloud and Grid Computing ccgrid 2012  ser CCGRID 12 Washington DC USA IEEE Computer Society 2012 pp 443ñ450  K Meht a J B e nt  A  T or r e s G Gr i d er  a nd E  Gabr i e l  A plugin for hdf5 using plfs for improved i/o performance and semantic analysis in Proceedings of the 2012 SC Companion High Performance Computing Networking Storage and Analysis  ser SCC 12 Washington DC USA IEEE Computer Society 2012 pp 746ñ752 11 J He J  B en t A T o rres G Grid er  G  G ib so n  C M a ltzah n  and X.-H Sun I/o acceleration with pattern detection in Proceedings of the 22nd international symposium on Highperformance parallel and distributed computing ser.HPDC 13 New York NY USA ACM 2013 pp 25ñ36  B  Hi ndman A  K onwi n ski  M Z a har i a A Ghodsi  A D Joseph R Katz S Shenker and I Stoica Mesos a platform for ne-grained resource sharing in the data center in Proceedings of the 8th USENIX conference on Networked systems design and implementation  ser NSDIí11 Berkeley CA USA USENIX Association 2011 pp 22ñ22  A Mat s unaga M  T suga w a  a nd J For t es  C l oudbl ast  C o mbining mapreduce and virtualization on distributed resources for bioinformatics applications in eScience 2008 eScience 08 IEEE Fourth International Conference on  Dec pp 222 229  C  Mi t c hel l  J Ahr e ns a nd J W a ng V i s i o  E nabl i n g i nt er active visualization of ultra-scale time series data via highbandwidth distributed i/o systems in Parallel Distributed Processing Symposium IPDPS 2011 IEEE International  May pp 68ñ79 511 


978-1-4673-1813-6/13/$31.00 \2512013 IEEE  7 From an edification position, the project participants gain 223real-world\224 understanding of responding to the needs of future employers. From a programmatic position, the lack of payload utilization introduces lack of mission relevance R EFERENCES   1  J  C h o n  H  K i m   C  S  L i n  S e a m l i n e  determination for image mosaicing: A technique minimizing the maximum local mismatch and the global cost. ISPRS Journal of Photogrammetry and Remote Sensing, Volume 65, Issue 1, January 2010, Pages 86-92  2 G  E r us N i c o l a s Lo m\351 ni e  H o w t o i nvo l ve  structural modeling for cartographic object recognition tasks in high-resolution satellite images? Pattern Recognition Letters, Volume 31 Issue 10, 15 July 2010, Pages 1109-1119  3  Y  L i n g  M  E h l e r s  E  L  U s e r y  M  M a d d e n  F F T enhanced IHS transform method for fusing highresolution satellite images. ISPRS Journal of Photogrammetry and Remote Sensing, Volume 61 Issue 6, February 2007, Pages 381-392  4 A  Ad d a i m   A K h e r r a s  E  B  Za n t o u  D S P  implementation of integrated store-and-forward APRS payload and OBDH subsystems for low-cost small satellite. Aerospace Science and Technology Volume 12, Issue 4, June 2008, Pages 308-317 5  Matth e w B r o w n a n d Da v i d  G. L o w e Au to m at i c Panoramic Image Stitching using Invariant Features.  International Journal of Computer Vision. Volume 74, Number 1 \(2007\59-73    D. T u rn er A Lu cieer C  W a ts on 20 12 A n  Automated Technique for Generating Georectified Mosaics from Ultra-High Resolution Unmanned Aerial Vehicle \(UAV\magery, Based on Structure from Motion \(SfM\oint Clouds Remote Sensing  4 5\2-1410  7 L Q i n g yua n L W e n y a n g Z Le i  W  J i nl i ng a n d  L. You. "A New Approach To Fast Mosaic UAV Images International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences Vol. XXXCIII-1/C22     A   Z o m e t    S  Pel e g  1998, O c t ober A ppl y i ng  super-resolution to panoramic mosaics. In Applications of Computer Vision, 1998. WACV'98 Proceedings., Fourth IEEE Workshop on pp. 286287    J. S t ra u b  223 M odel Ba s e d Da t a  T ran sm i s s i o n   Analysis of Link Budget Requirement Reduction\224 Communications and Network 2012   S  C  Par k M.K  Par k   M. G  K a ng 2003 Super-resolution image reconstruction: a technical overview Signal processing Magazine, IEEE, 20 3 21-36 1  C a m b ri d g ei n c ol o u r  co m   N  d  Digital Image Interpolation Onlin A v a ila ble http://www.cambridgeincolour.com/tutorials/imageinterpolation.htm   M. Br y s o n  A R e id F Ra m o s    S. S u l l arie h  2010\Airborne vision-based mapping and classification of large farmland environments Journal Of Field Robotics 27\(5\655     O rbi t a l   223S ci ence, E n vi ronm e n t a l a n d T e c h n o l ogy  Satellites \226 Reliable Spacecraft Enabling Affordable Missions\224 \(2012 Corporate brochure  Biographies Abhilasha Bhatia  is a graduate student in the Department of Computer Science at the University of North Dakota. She is a committee member of Student Association of India and also actively involved with Women in Science group at UND. She holds a Bachelor of Technology degree in Computer Science from Gautam Buddh Technical University, India   Kyle Goehner  is an undergraduate student pursuing a BS in Computer Science with a minor in Mathematics at the University of North Dakota  While at UND, Kyle has received numerous honors including the competitive Odegard Scholarship \(2011\, the Microsoft Scholarship 2011\, the Fetter Scholarship \(2010\ and the UND Community of Learners Scholarship \(2009-2013\.  Kyle\222s team placed in the top 10 at the Midwest Instruction and Computing Symposium Programming Competition.  He is active in the Association for Computing Machinery \(ACM serving as the chapter president from 2011 to 2013   John Sand  is an undergraduate student in the Department of Computer Science at the University of North Dakota   Jeremy Straub is a PhD student in the Department of Computer Science at the University of North Dakota.  He currently serves as the Student Program Director for the North Dakota Space Robotics Program.  His research is presently focused on artificial intelligence for space applications.  Before returning to pursue doctoral studies Mr. Straub held progressively responsible positions in industry.  He has over 10 years professional experience in developing and managing the development of cutting-edge commercial systems, including North America\222s first commercial traffic-adaptive nav igation system.  Jeremy holds a BS in Information Technology, a BS in Business, a graduate certificate in Space Studi es, an MBA and an MS in Computer Systems and Software Design.  He is a member of the AIAA, IEEE, SPIE and SSPI 


978-1-4673-1813-6/13/$31.00 \2512013 IEEE  8 Atif Farid Mohammad is an MIT certified Systems Architect and currently a PhD student in the Computer Science department at the University of North Dakota working on Cloud Computing Security. Atif has more than sixteen years of experience in software engineering professional business systems analysis, design, application development and staff management for diversified business and educational organizations. He is a member of IEEE ACM and AST Christoffer Korvald is a MS student in the Department of Computer Science at the University of North Dakota Christoffer is the President of the local chapter of the Upsilon Pi Epsilon Honor Society. He is currently the president of the Association of Norwegian Students Abroad USA, which has over 1500 members. Christoffer holds a BA in Computer Science with a Software Engineering specialization from the University of North Dakota. He is also the Associate Director of Software for the Open Orbiter Satellite Initiative  Anders Kose Nervold is an undergraduate student at the University of North Dakota pursuing a BA in Entrepreneurship with a minor in Space Studies. Anders is currently serving as the Associate Director for Communications, Outreach and Policy for the Open Orbiter Satellite Initiative. While at UND, Anders worked as a console operator for the International Space Station Agricultural Camera \(ISSAC\.  In 2012, Anders and his business partner claimed the 1 st prize in the GIANTS international business plan competition for their planned service venture in the aerospace industry.  Anders is currently serving as the treasurer for the nationwide student organization, the Association of Norwegian Students Abroad \226 USA, which has over 1300 members.  He has also served as the local president for the Association of Norwegian Students Abroad and as the treasurer for the Interfraternity Council.  He was awarded the competitive Mueller Scholarship in 2012 and the Lillian Elsinga Outstanding Student Leader Award in 2011.  Anders is also a member of the Dakota Space Society and mentors other international students at UND    


Jorda Polo, David Carrera, Yolanda Becerra, Malgorzata Steinder  and Ian Whalley. Performance-driven task co-scheduling for  mapreduce environments. In Network Operations and Management  Symposium \(NOMS\2010 IEEE, pages 373 Ö380, 19-23 2010 12 K. Kc and K. Anyanwu, çScheduling hadoop jobs to meet deadlines  in 2nd IEEE International Conference on Cloud Computing  Technology and Science \(CloudCom\, 2010, pp. 388 Ö392 13 Xicheng Dong, Ying Wang, Huaming Liao çScheduling Mixed Real time and Non-real-time Applications in MapReduce Environment  In the proceeding of 17th International Conference on Parallel and  Distributed Systems. 2011, pp. 9 Ö 16 14 Xuan Lin, Ying Lu, J. Deogun, and S. Goddard. Real-time divisible  load scheduling for cluster computing. In Real Time and Embedded  Technology and Applications Symposium, 2007. RTAS ê07. 13th  IEEE pages 303 Ö314, 3-6 2007 15 HDFS  http://hadoop.apache.org/common/docs/current/hdfsdesign.html  16 Chen He, Ying Lu, David Swanson. çMatchmaking : A New  MapReduce Scheduling Techniqueé. In the proceeding of 2011  CloudCom, Athens, Greece, 2011, pp. 40 Ö 47 17 Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma and Khaled  Elmeleegy, Scott Shenker, and Ion Stoica, çDelay scheduling: a  simple technique for achieving locality and fairness in cluster  schedulingé. In the proceedings of the 5th European conference on  Computer systems, 2010.  pp 265-278 18 Zhuo Tang, Junqing Zhou, Kenli Li, and Ruixuan Li "A MapReduce  task scheduling algorithm for deadline constraints.", Cluster  Computing, Vol. 15,  2012 19 Eunji Hwang, and Kyong Hoon Kim. "Minimizing Cost of Virtual  Machines for Deadline-Constrained MapReduce Applications in the  Cloud." Grid Computing \(GRID\, 2012 ACM/IEEE 13th  International Conference on. IEEE, 2012 20 Micheal Mattess, Rodrigo N. Calheiros, and Rajkumar Buyya  Scaling MapReduce Applications across Hybrid Clouds to Meet Soft  Deadlines." Technical Report CLOUDS-TR-2012-5, Cloud  Computing and Distributed Systems Laboratory, the University of  Melbourne, August 15, 2012 21 
 
11 
                
Chen He, Ying Lu, David Swanson. çReal-Time Application Scheduling in Heterogeneous MapReduce Environments Technical Report TR-UNL-CSE2012-0004, University  of Nebraska-Lincoln, 2012 Available: http://cse apps.unl.edu/facdb/publications/TR-UNL-CSE20120004.pdf 22 T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, K  Elmleegy, and R. Sears. çMapreduce Onlineé. In NSDI 2010 23 A. D. Ferguson, P. BodÌk, S. Kandula, E. Boutin, and R  Fonseca. çJockey: Guaranteed Job Latency in Data Parallel Clusters. In EuroSys, 2012 24 G. Wang, A. R. Butt, P. Pandey, and K. Gupta. çA Simulation Approach to Evaluating Design Decisions in MapReduce Setupsé. In MASCOTS 2009 25 H. Herodotou and S. Babu. Profiling, çWhat-if Analysis and Cost-based Optimization of MapReduce Programs In VLDB 2011 26 H. Herodotou, F. Dong, and S. Babu. çNo One \(Cluster Size Fits All: Automatic Cluster Sizing for Dataintensive Analyticsé. In SoCC 2011  
1544 
1544 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


