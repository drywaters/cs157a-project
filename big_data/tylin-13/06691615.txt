Sparse Poisson Coding for High Dimensional Document Clustering Chenxia Wu   Haiqin Yang  Jianke Zhu   Jiemi Zhang   Irwin King   and Michael R Lyu   College of Computer Science Zhejiang University Hangzhou China Corresponding author email:jkzhu@zju.edu.cn  Shenzhen Research Institute The Chinese University of Hong Kong Shenzhen China  Department of Computer Science and Engineering The Chinese University of Hong Kong Hong Kong Corresponding author email hqyang@cse.cuhk.edu.hk Abstract Document clustering plays an important role in large scale textual data analysis which generally faces with great challenge of the high dimensional textual data One remedy is to learn the high-level sparse representation by the sparse coding techniques In contrast to traditional Gaussian noisebased sparse coding methods in this paper we employ a Poisson distribution model to represent the word-count frequency feature of a text for sparse coding Moreover a novel sparse-constrained Poisson regression algorithm is proposed to solve the induced optimization problem Different from previous Poisson regression with the family of   regularization to enhance the sparse solution we introduce a sparsity ratio measure which make use of both   norm and   norm on the learned weight An important advantage of the sparsity ratio is that it bounded in the range of 0 and 1 This makes it easy to set for practical applications To further make the algorithm trackable for the high dimensional textual data a projected gradient descent algorithm is proposed to solve the regression problem Extensive experiments have been conducted to show that our proposed approach can achieve effective representation for document clustering compared with state-of-the-art regression methods Index Terms document clustering sparse coding Poisson regression I I NTRODUCTION During past decade an explosive growth of text contents on Internet makes Web documents become a kind of typical Big Data and brings both opportunities and challenges for knowledge discovery text mining and information retrieval Among these techniques document clustering plays very important role in automatic document organization topic extraction and fast information retrieval or ltering 3  T ypically  t e x t data is represented as a high dimensional binary or count bag-of-words vector that brings a great challenge to document clustering Sparse coding is able to provide a solution by using the unlabeled data to learn a highlevel sparse representation of the raw inputs for document clustering 20 Lots of pre vious studies ha v e addressed the ef“cacy of such method in image classi“cation 25 For a typical sparse coding problem the feature values are often assumed to be real which can be described by a Gaussian noise model Moreover Gaussian model is mainly designed for continuous data that could take fractional or negative values Such assumption is apparently inappropriate for the word-counts data 6 especially  the te xtual data To address this problem in this paper we employ a Poisson distribution model on the sparse coding for the frequency data More speci“cally we consider the problem of learning the low frequency count data in the high-dimensional setting The main challenge of sparse Poisson coding for text clustering is how to effectively model the nonnegative data while selecting the salient features for the succinct model interpretation Although this problem has been explored in the literature there still exist some limitations For web applications Poisson re gression is parallelized and implemented under the Hadoop MapReduce framework to provide a scalable and ef“cient solution for behavioral targeting The feature selection scheme is quite heuristic where the important features are selected based on the frequency counted in cookie and the most frequent entities are selected by a prede“ned threshold This method may ignore some combination features that occur rarely In neuroscience   regularized Poisson regression is proposed to learn a sparse representation on the neural activity data In medical imaging applications 11 sparsitybased penalties following the idea of compressed sensing are proposed to seek a sparse Poisson regression model to reconstruct the true function of the photon intensity These methods have to specify a regularization parameter to control the sparse level of the solution in selecting the important features However the range of the parameter is relatively large and the relationship between the sparsity level of the solution and the parameter is not directly evident The insuf“ciency of previously proposed work motivates our further exploration on the sparse Poisson regression models in this work To overcome the above issues we propose a novel Poisson regression model namely sparsity-constrained Poisson regression SCPR to build a linear model for the frequency data while providing the sparsity solution for the salient feature selection We highlight the contributions of our work in the following  Firstly we employ a Poisson distribution model to well describe the word-count feature of a text in sparse coding which can provide a high-level feature for document clustering  Secondly we induce a sparse prior to Poisson coding by adopting a sparsity ratio which is different from previously proposed sparsity constraints 24 The 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 512 


sparsity ratio is borrowed from the sparseness constraints for non-negative matrix factorization NNMF 13 and utilizes both the   norm and   norm on the weight More importantly the ratio is bounded in the range of 0 and 1 which clearly indicates the sparsity level of the solution It is more intuitive and easier to be set in realworld applications Also we investigate how to utilize the sparsity ratio and elaborate different settings for it We therefore propose the SCPR with an equal constraint to maintain the learned weight at a desired sparsity level  Finally in order to make SCPR applicable for high dimensional text data input we design a projected gradient descent algorithm for SCPR The algorithm is very ef“cient and scales with the average number of non-zero elements which is much less burden than the original Poisson regression model counting all features II P OISSON R EGRESSION FOR S PARSE C ODING In this section we will introduce how to employ Poisson regression to solve a sparse coding problem 9 17 22 25 We rst review the typical sparse coding problem Let     be the raw input feature vector Sparse coding aims to nd a set of basis vectors             and the sparse representations/coef“cients     with respect to the basis for   Moreover it is typically based on a Gaussian noise model                         where each feature is assumed independent and identically distributed     is the  th element of  and   is xed A sparse prior                  is assumed to penalize the nonzero representations Given an unlabeled sample  the basis vectors and the sparse representations are obtained by the MAP optimization problem 012                        015             1 The above problem can be ef“ciently solved by the alternative minimization over  and  variables Also the basis vectors can be ef“ciently selected as the cluster centers by running 012 means clustering on all the data samples or simply selected by randomly sampling from the data Given a xed    can be obtained by solving a regression problem by minimizing the objective for both training and the new input data samples The probabilistic model for the above problem assumes that the input data features are real-valued which is typically described by a Gaussian noise model Obviously it is inappropriate for the textual data with word-counts frequency             which may be poorly modeled by a continuous Gaussian distribution W e try to address this problem by introducing the Poisson distribution model in sparse coding on the frequency data Given a text document 0.1 0.5 0.9 Fig 1 Illustration of different sparsity levels 0.1 0.5 and 0.9 At low level of sparsity left the weight is dense At high level right most of elements are zeros and only a few take large values   we use the following Poisson distribution model            Poisson                     015          015           2 where      is the  th element of   and             is simply denoted as 015     As each feature is assumed to be independent and identically distributed            Poisson       015      Note that we assume both the basis vector and the new representation vector  to be non-negative which imposes a positive effect onto one-unit change in the word count for the expected target word count Gi v e n a n unlabeled sample  and the xed basis vectors   the sparse representations  can be obtained by the MLE optimization problem               015                  3 The above log-likelihood is a concave function on  and therefore guarantees the global optimal solution for Poisson regression Various algorithms e.g gradient descent can be adopted to seek its optimal solution The derivative of the loglikelihood with respect to     is                 015                 4 The multiplicative rule is used to update the coef cient vector     012                                5 III S PARSE P OISSON C ODING In this section we consider how to utilize the sparsity prior in sparse coding problem Then we propose an effective sparsity-constrained Poisson regression SCPR approach with a given sparsity level To solve this problem we design an ef“cient algorithm based on the projected gradient descent and sketch its average computational cost A Sparse Poisson Regression A sparse prior     is assumed to penalize the nonzero representations Then the sparse representations  are obtained 513 


by the MAP optimization problem 012   015  Pen     6 where  is the log-likelihood Pen    is based on the sparse prior which can be   regularization      or the h ybrid Huber penalty 11 B Sparsity Ratio Numerous measures can be used to evaluate for the sparsity level of the coef“cients A good indicator function can map a vector from    to    and quantify how much energy maintain on the component of the coef“cient An ideal one is to have only one non-zero element for the sparse vector and with all elements non-zero for the least sparse case Hence we adopt the sparsity ratio de“ned in for NNMF in this paper spr                        7 In Eq 7 the de“ned sparsity ratio is different from the previously proposed sparse Poisson regression mainly utilizing the   regularization Based on the relationship between the   norm and the   norm Moreover this ratio contains a good property bounding in the range of 0 and 1 We summarize this property of the sparsity ratio in the following proposition Proposition III.1         015    we have   spr       8 The above proposition can be proved by the following relationships                 9 The rst inequality follows the Cauchy-Schwarz inequality The second inequality is obvious when putting square on both sides Although Proposition III.1 is valid for negative  we still keep the non-negative condition This is mainly due to that we only consider non-negative coef“cient vectors In Eq 9 the lower bound is reached when a vector with equal non-zero elements On the other hand the upper bound is obtained when a vector with all but one vanishing elements Hence this ratio is intuitive and easy to set the sparsity level of a given vector   see more illustrated examples in Fig 1 C Proposed Model We consider how to employ the above de“ned sparsity ratio to obtain the sparse solution An intuitive setting is to bound the sparsity ratio as follows spr      10 An advantage of this setting is to maintain the convexity of the domain This can be shown that the domain of the learned coef“cient in Eq 10 is equivalent to              where         015  11 This is exactly a second-order cone or Lorentz cone Combining the restriction of       i.e     we can de“ne the domain of learned coef“cient in a convex set Fig 2 shows the different examples a  012  b  012  c  012 012 Fig 2 Illustration of the set of the learned weight bounded on different levels 0.1 0.5 and 0.9 The larger the bounded sparsity ratio the larger the domain of learned weight is It seems that we can de“ne the learned coef“cient by bounding the sparsity ratio as in Eq 10 and yield a convex optimization problem As illustrated in Fig 2 a large bound i.e spr     the de“ned domain of the learned coef“cient will recover the whole domain of the learned coef“cient which is exactly the original domain of Poisson regression Moreover de“ning  on the conic set in Eq 10 also cannot yield sparse solution Hence we borrow the idea of the sparsity constraints proposed for NNMF in and propose the sparsity-constrained Poisson regression SCPR as follows         s.t spr     12 Note that our proposed SCPR requires the learned coef“cient at a certain sparsity level via the pre-de“ned parameter  For the different sparsity levels we can refer to the charts shown in Fig 3 It can be found that the larger spr     the smaller valid set is Extremely the set consists of several isolated points on the corresponding axis when the sparsity ratio is set to one a spr    012  b spr    012  c spr    012 012 Fig 3 Illustration of the sparsity-constraint set in a 3D wireframe mesh for different sparsity levels 0.1 0.5 and 0.9 The set consists of the mesh except the green region The set becomes small as the sparsity ratio increases D Algorithms In order to make our proposed SCPR applicable for the high-dimensional textual data we design an ef“cient projected gradient descent algorithm for the minimization problem in SCPR The whole algorithm consists of two main steps 1 the rst step mainly follows the multiplicative update rule as in Eq 5 to update the coef“cient vector 2 the second step is to 514 


 Algorithm 1 Projected gradient descent for SCPR Input  The raw feature  and the xed basis vectors  the sparsity ratio   Output  The coef“cient vector of SCPR   1 Compute   dim     2 Compute   by Eq 11 3 Initialize  to random positive vector 4 Set   to the square of   norm on   i.e         5 Set   to the   norm value corresponding to the desired level of sparsity i.e          6 Call Projection          to update   7 repeat 8 Calculate  by Eq 5 9 Set   to the square of   norm on   i.e         10 Set   to the   norm value corresponding to the desired level of sparsity i.e          11 Call Projection          to update   12 until converge Algorithm 2 Projection          Objective  Find the closet non-negative vector to a vector  with a given   norm    and a given square of the   norm    1 Compute   dim     2 Compute              3 Compute           015            size  015  if     if    4 loop 5 Compute             size    if     if    6 Update    015     012   where  is the non-negative root of the quadratic equation   015     012         7 if all elements of  are non-negative then 8 return   9 end if 10 Update              11 Update            12 Compute                size     13 Update                  14 end loop project the coef“cient onto the constraint space to achieve the desired level of sparsity This procedure is summarized into Algorithm 1 In Algorithm 1 the function Projection          has to be called several times which is to project  with the corresponding   norm and the square of   norm to achieve the desired sparsity The procedure is de“ned in Algorithm 2 For Algorithm 2 we rst remove those elements with the value being zero at line 2 At line 3 a point on a hyperplane          is initialized Then we move from the center of the sphere towards the initialized point to satisfy the   constraint where the center is de“ned by the point with all elements being equal in the updated index If the updated point is non-negative for all the elements and then the algorithm is terminated Otherwise we reset those elements with negative values to zero and project the point back onto the hyperplane with           Time complexity analysis Comparing to the original Poisson regression model the proposed SCPR requires invoking the function Projection at each iteration which incurs some computation efforts In the worst case Algorithm 2 may take as many iterations as the number of coef“cients dimension i.e dim     to converge to an optimal solution However the algorithm converges much faster in practice It just needs about four iterations for the worse case and one or two iterations for the optimal solution at average Hence the number of iterations in the function Projection can be considered as a constant Moreover the computation cost for the function Projection is proportional to the number of non-zero elements NNZs in the learned coef“cient Additionally the number of outer iterations required by Algorithm 1 is much smaller than the original Poisson regression model due to the sparse solution In summary Poisson regression has to update the coef“cient for all elements due to non-sparsity The number of outer iterations is proportional to the dimension of the raw features and the number of coef“cient dimension We abstract it as      and obtain the time cost of PR as           SCPR requires      outer iterations which is nearly a constant and several times to invoking the function Projection  At for each iteration SCPR only needs to update those non-zero elements Hence the average run time cost for SCPR is in the order of        Avg  NNZs   which is much smaller than the original Poisson regression model IV E XPERIMENTS To study the ef“cacy and the merits of our proposed SCPR method in different perspectives we evaluate the document clustering performance using the different regression approaches in sparse coding for learning the coef“cients All the experiments are conducted on a notebook computer with Inter i3-3110M CPU@2.40GHz and 4GB memory A Sparse Coding for Document Clustering In this section we evaluate the performance of sparse coding for document clustering To show the ef“cacy of our algorithm we study the Poisson regression approaches with different sparse prior and regularization 1 Dataset We conduct the performance evaluations on the TDT2 document corpora which consists of the data collected during the rst half of    and taken from six sources including two newswires APW and NYT two radio programs VOA and PRI and two television programs CNN and ABC It is composed of    on-topic documents which are classi“ed into  semantic categories In this experiment those documents appearing in two or more categories were removed and only the largest  categories were kept 515 


thus leaving us with    documents in total 1  In the dataset the stop words are removed and each document is represented as a    dimensional TF-IDF vector 2 Evaluation Measure In our experiments the basis for sparse coding is rstly selected by randomly sampling from the data The ra w feature of each document is fed into the different regularized regression approaches in order to obtain the sparse representations which are further employ to cluster the documents The clustering result is evaluated by comparing the estimated label for each document using 012 means clustering algorithm Two typical metrics are used to measure the performance The rst metric is the accuracy AC Gi v e n a document   let    and   be the estimated cluster label and the label provided by the corpus respectively The AC is de“ned as follows                   13 where  is the total number of documents       is the delta function and      is the permutation mapping function that maps each cluster label    to the equivalent label from the data corpus The best mapping can be found by using the Kuhn-Munkres algorithm Another metric is the normalized mutual information NMI metric Let  denote as the set of ground truth clusters and   as the cluster set obtained from clustering algorithm The mutual information metric       is de“ned as follows                                                 14 where            are the probabilities that a document arbitrarily selected from the corpus belongs to the clusters   and     respectively         is the joint probability that the arbitrarily selected document belongs to the cluster   as well as    at the same time We further employ the normalized mutual information NMI                        15 where          are the entropies of  and    respectively From the de“nition we know       ranges from  to   If two sets of clusters are identical  is equal to one If they are independent  is set to zero 3 Performance Evaluation We compare our proposed SCPR algorithm with several algorithms Gaussian regression   norm regularizer GR   the original Poisson regression without sparse prior PR Poisson regression with   norm regularizer PR   Poisson regression with Recursive Dyadic Partitions RDPs regularizer PR-RDP Poisson regression with translationally-invariant cycle-spun RDPs PRTI Poisson regression with Total Variation semi-norm regularizer PR-TV To facilitate the fair comparisons we directly adopt the implementation of these reference methods with the 1 http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html recommended settings In our e xperiments the dictionary size is set to    and     respectively In each case the sparsity level  is set to    and     respectively according to the cross-validation Fig 4 plots the clustering results Table I reports the coding time when     It can be easily found that our presented SCPR approach obtains better clustering performance compared with other methods This is because our algorithm not only reduces the reconstruction error but also captures the same basis for the similar documents using a tted sparsity level Though RDPs and RDP-TI constraints can greatly improve the regression speed their performances decrease at the same time Except from these two fast regression methods other approaches with the sparsity prior show better performance than the raw regression and the Poisson regressionbased sparse coding outperforms the Gaussian regressionbased sparse coding This demonstrates that the sparsity prior would capture the salient high-level features of the text to improve the representation ability and Poisson distribution can better describe the word-count feature of the text data Among these effective sparse Poisson coding methods SCPR method performs best with the least computation time V C ONCLUSION In this paper we introduced the Poisson distribution model to represent the word-count textual feature in sparse coding A novel sparse-constrained Poisson regression algorithm was proposed to solve the induced optimization problem We have de“ned a sparsity ratio which employed both   norm and   norm on the learned weight To further make the algorithm applicable for the high dimensional textual data we designed a projected gradient descent algorithm to solve the regression problem We have conducted the regression experiments on the synthetic data to show the improvement of our presented SCPR algorithm compared with the original Poisson regression The clustering experiment on the high-dimensional textual data indicated that our proposed approach is able to learn the better sparse representation with faster speed for document clustering compared with the state-of-the-art regression methods A CKNOWLEDGMENT The work was supported by the Basic Research Program of Shenzhen Project No JCYJ20120619152419087 and JC201104220300A the Research Grants Council of the Hong Kong Special Administrative Region China Project No CUHK 413212 and CUHK 415212 and National Natural Science Foundation of China under Grants 61103105 and 91120302 R EFERENCES  S Bo yd and L V andenber ghe Convex Optimization  Cambridge University Press 2004  D Cai X He and J Han Document clustering using locality preserving indexing IEEE TKDE  17\(12 2005  D Cai X He and J Han Locally consistent concept f actorization for document clustering IEEE TKDE  23\(6 2011  Y  Chen D P a vlo v  and J F  Cann y  Beha vioral tar geting The art of scaling up simple algorithms ACM TKDD  4\(4 2010 516 


   5 10 15 20 25 30 0.4 0.5 0.6 0.7 0.8 0.9 1 Number of classes Accuracy   PR GRŠl1 PRŠl1 PRŠRDP PRŠRDPŠTI PRŠTV SCPR   5 10 15 20 25 30 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Number of classes NMI   PR GRŠl1 PRŠl1 PRŠRDP PRŠRDPŠTI PRŠTV SCPR a dictionary size      5 10 15 20 25 30 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Number of classes Accuracy   PR GRŠl1 PRŠl1 PRŠRDP PRŠRDPŠTI PRŠTV SCPR   5 10 15 20 25 30 0.3 0.4 0.5 0.6 0.7 0.8 Number of classes NMI   PR GRŠl1 PRŠl1 PRŠRDP PRŠRDPŠTI PRŠTV SCPR b dictionary size    Fig 4 Illustration of the clustering results TABLE I A VERAGE T IME PER D OCUMENT OF S PARSE C ODING WITH    approaches PR GR  PR  PR-RDPs PR-RDP-TI PR-TV SCPR time/doc.\(sec 17.28 0.41 0.53 0.10 0.15 0.88 0.49  C Cheng H Y ang  M  R  L yu and I King Where you lik e t o g o next Successive point-of-interest recommendation In IJCAI  Beijing China 2013  C Cheng H Y ang I King and M R L yu Fused matrix f actorization with geographical and social in”uence in location-based social networks In AAAI  Toronto Canada 2012  P  C Cosman R M Gray  and M V etterli V ector quantization of image subbands a survey IEEE TIP  5\(2 1996  D L Donoho Compressed sensing IEEE Transactions on Information Theory  52\(4 2006  J C Gemert J.-M Geusebroek C J V eenman and A W  Smeulders Kernel codebooks for scene categorization In ECCV  2008  Z Harman y  R Marcia and R W illett This is spiral-tap Sparse poisson intensity reconstruction algorithms theory and practice IEEE TIP  21\(3 2012  Z T  Harman y  R F  Marcia and R W illett Sparsity-re gularized photonlimited imaging In ISBI  pages 772…775 2010  M Heiler and C Schn  orr Learning sparse representations by nonnegative matrix factorization and sequential cone programming Journal of Machine Learning Research  7:1385…1407 2006  P  O Ho yer  Non-ne gati v e matrix f actorization with sparseness constraints Journal of Machine Learning Research  5:1457…1469 2004  R C K elly  M  A  Smith R E Kass and T  S Lee Accounting for network effects in neuronal responses using l1 regularized point process models In NIPS  pages 1099…1107 2010  S K umar  M  Mohri and A T a l w alkar  O n s ampling-based approximate spectral decomposition In ICML  2009  D D Lee and H S Seung Algorithms for non-ne gati v e matrix factorization In NIPS  pages 556…562 2000  H Lee A Battle R Raina and A Y  Ng Ef cient sparse coding algorithms In NIPS  pages 801…808 2007  H Lee R Raina A T eichman and A Y  Ng Exponential f amily sparse coding with applications to self-taught learning In IJCAI  pages 1113…1119 2009  L Lo v  asz and M Plummer Matching Theory Akad  emiai Kiad  o Budapest 1986  R Raina A Battle H Lee B P ack er  and A Y  Ng Self-taught learning transfer learning from unlabeled data In ICML  pages 759 766 2007  D W ang S C Hoi Y  He and J Zhu Retrie v al-based f ace annotation by weak label regularized local coordinate coding In ACM international conference on Multimedia  pages 353…362 2011  J W ang J Y ang K Y u  F  L v  T  S Huang and Y  Gong Localityconstrained linear coding for image classi“cation In CVPR  pages 3360 3367 2010  H Y ang M R L yu and I King Ef cient online learning for multi-task feature selection ACM TKDD  7\(2 2013  H Y ang Z Xu J Y e  I  King and M R L yu Ef cient sparse generalized multiple kernel learning IEEE TNN  22\(3 March 2011  J Zhang C W u  D  Cai and J Zhu Bile v e l visual w ords coding for image classi“cation In IJCAI  2013  T  Zhang Y  Y  T ang B F ang and Y  Xiang Document clustering in correlation similarity measure space IEEE TKDE  24\(6 2012 517 


000&\000R\000Q\000I\000H\000U\000H\000Q\000F\000H\000\003 000&\000U\000H\000D\000W\000L\000R\000Q 0008\000V\000H\000U 000,\000Q\000Y\000L\000W\000D\000W\000L\000R\000Q 0001\000R\000W\000L\000I\000L\000F\000D\000W\000L\000R\000Q  000&\000R\000Q\000I\000H\000U\000H\000Q\000F\000H 000\(\000Q\000W\000H\000U\000L\000Q\000J 0002\000Q\000O\000L\000Q\000H 0007\000L\000P\000H\000\003 0007\000U\000L\000J\000J\000H\000U 0009\000R\000L\000F\000H 0001\000R\000W\000L\000I\000L\000F\000D\000W\000L\000R 000Q 0006\0000\0006 0001\000R\000W\000L\000I\000L\000F\000D\000W\000L\000R 000Q 0008\000V\000H\000U 0009\000H\000U\000L\000I\000L\000F\000D\000W\000L\000R 000Q 000<\000H\000V 0001\000R 
A Functionality B Performance 
The notification sub-process of multimedia conference system mentioned above composes RESTful Web services with SOAP Web services using the proposed approach. The notification sub-process in BPEL consists of four web services The process is deployed in Ser viceMix [11  an ent e rp ri se  service bus, with Apache ODE as the process execution engine SoapUI which is the world leading Open Source Functional Testing Tool for Web Service Testing is used to simulate client to send SOAP message to start the process. Tomcat 6.0 is selected as the web container of web services. Through analyzing the runtime log information and the response message of process, we can verify that RESTful Web services are actually invoked and executed in the process In another word, the proposed approach really works Since the approach makes it possible to invoke RESTful Web services in BPEL processes, we could compare processes that use RESTful Web services with the ones that use SOAP-based Web services only to find out the efficiency of the approach Firstly, several different processes need to be created. These processes should complete same task with same control logic and same functional services except that the services are REST style or SOAP style. In our experiments, five processes are provided and each of them has different number of RESTful Web services. Process 1 only uses SOAP-based Web services while process 2 replaces one SOAP-based Web service in process 1 with a RESTful Web service that has the same function. Process 3 replaces two SOAP-based services in process 2 with two RESTful We b services, each replacement pair has the same function, and the rest can be deduced by analogy. Through using different number of RESTful Web services in one BPEL process, we could conclude the improvement of performance it brings After the five processes are deployed in the same Service Execution Environment \(SEE\ with ServiceMix and ODE soapUI is used to simulate multi users. In our experiment, the response time of each process wh en concurrently visited by 10 users, 50 users and 100 users are all recorded. Figure 9. shows the statistics data. The horizontal axis represents process 1 to process 5, and the vertical axis represents response time. Three lines with different color represent different amount of concurrent users Figure 9 Response Time of Visiting Processes Integrating Different Number of RESTful serivces With the increasing of concurrent users, response time is also increased because of the limitation of resources. But there is a common point among three lines. Generally, the more RESTful Web services used in one process, the less the response time. The main reason is that traditional SOAP-based Web services are designed to se nd SOAP message which is an XML-based message based on transfer protocol, such as HTTP while RESTful Web services are visited by directly HTTP requests. SOAP message parsing is time-consuming. For RESTful Web services, less transport information results to a higher performance. So RESTful Web services are recommended to be used in BPEL processes if exist VI C ONCLUSION In this paper, a new approach for BPEL to compose RESTful Web services with SOAP-based Web services is proposed. The approach wrap a WSDL document for RESTful 
Figure 8 Business Process for Multimedia Conference In the notification sub-process, the system uses different ways to notice users based on whether they are online or not. If yes, a voice service provided in SOAP style would be invoked and if no, a SMS service provided by China Telecom in REST style would be invoked. This scenario obviously illustrated that the integration of RESTful Web services and SOAP-based Web services is necessary in a real business process. And developers use the Service Generation System \(SGS\ which is a rich development platform introduced in section IV to assist their work In general, RESTful Web services are wrapped to provide WSDL documents and then they could be used to compose business processes in Service Development Platform \(SDP Finally, BPEL processes will be deployed and published to customers in Service Execution Environment \(SEE  
0 200 400 600 800 1000 1200 12345 10 users 50 users 100 users 
Time \(ms 
533 


Composition, IEEE Trans. Knowledge and Data Engineering Vol. 17, No. 5, 2005, pp 686-697 2 C o mpo si n g  REST fu l se rv ice s w i t h J O p e ra Ce sare Pa u t a sso  i n So ftw a re  Composition Lecture Notes in Computer Science Volume 5634, 2009 pp 142-159 3 O A S I S   W e b S e rv ice s Bus i ne s s P r o c e s s E x e c utio n L a ng uag e  W S B P E L   2.0. \(2006 https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=wsbp el 4 C e sare Pau t asso REST fu l W e b ser v i ce co mpo s it io n w i t h BPEL fo r  REST Data & Knowledge Engineering Vol. 68, Issue 9, September 2009, pp. 851 866 5 D e scrib e R EST W e b se rv ice s w ith W S D L 2  0  L a w r e n ce M a n d e l  http://www.ibm.com/developerworks/webservices/library/ws-restwsdl 29 May 2008 6 Y uY e n P e ng S h angP i n Ma J o nat h a n L e e  RE S T 2S O A P   A  framework to integrate SOAP services and RESTful services, in Proceedings of IEEE International Conference on Service-Oriented Computing and Applications, SOCA 2009  pp. 1-4 7 W e b Se rv ice s D e scrip tio n L a n g u a g e  W SD L  1  1  W 3 C N o te  1 5  M a rc h  2001 Erik Christensen, Microsoft Francisco Curbera, IBM ResearchGreg Meredith, MicrosoftSanjiva Weerawarana, IBM Research 8 h tt p  od e ap ac h e  o r g  w s d l 1 1 ext e n s ion s f o r r es t  h t m l   9 h tt p  www  e c l i p s e  o rg b p e l  10 L i l i F a ng Bu da n W u J u nl ia ng C h e n S e r v ice Manag e m e nt Mo de l Bas e d  on Ontology Computer and Information Science \(ICIS\, 2012 IEEE/ACIS 11th International Conference on June 2012, pp. 438- 443  h t t p  s e rvi c em i x  a p a ch e org 12 F Be l q asmi C. F u  R. G l ith o  RESTful Web Services for Service Provisioning in Next Generation Networks: A Survey, IEEE Communications Magazine, December 2011, Vol. 49, No12, pp. 66-73 13 C. Pa u t asso O  Z i mme r man n  a n d F   L e y man n   REST fu l W e b S e rv ice s  Web Services: Making the Right Architectural Decision, in Proceedings of 17th International World Wide Web Conf., Beijing China, Apr. 2008  pp. 805 14 14 Bu da n W u J unl i a ng C h e n  A S e m a n t ic E x te n d e d  M u l ti L a y e r Mo de l f o r  BPEL Process Generation, Proceedings of IEEE 9th International Conference on Services Computing, 2012, Hawaii, U.S., pp. 696-698 15 W S O 2 M a s h u p Se rv e r  h t t p    w so 2 co m/p r o d u c t s mas h u p se rv e r    Kej i n g He I n t e gra t i o n and Orch es tra t i on of Het e rogen e ou s Servi c es   Joint Conferences on Pervasive Computing JCPC, 2009, Taipei, pp 467-470 
 
 
Web services in order to allow BPEL to invoke RESTful Web services as same as SOAP-based Web services. RESTful services are invoked by sending HTTP requests instead of SOAP message. The WSDL wrapping of RESTful service is offered by our service generation system \(SGS\, as well as the orchestration of the SOAP services and RESTful services. The generated BPEL code can be executed in any environment that supports standard BPEL  Different with JOpera work [2 an d  W SO 2 M ash u p  To o l  15 in in du st ry th e p r op os ed app r o a ch u s es s tan da rd bu sin e ss process language to compose SOAP and RESTful service, and the business process language is standard without any extension or modification. Only RESTful service verbs are wrapped as a WSDL description to enable the integration with SOAP Web services in BPEL. This makes the approach easy for developers to accept. The GET and POST verb wrapping of a RESTful service in standard WSDL 1.1 specification enables the simple and efficient RESTful service invocation by HTTP requests As we know, a large proportion of the RESTful services only use GET and POST verbs. For PUT and DELETE verbs, we follow ODE WSDL 1.1 extension to specify and invoke the RESTful service. This is an improved approach different from ODE WSDL extension which uses WSDL 1.1 extension to specify all the four verbs of a RESTful service. We do not have to rely on certain versions of ODE for the RESTful service invocation, for RESTful services with GET and POST verbs only, we integrate it with SOAP services in BPEL in a standard way which can be executed in standard BPEL execution environment The greatest advantage of the proposed approach is that for a large proportion of RESTful services with only GET and POST verbs, it can be simply used in current BPEL execution environment without any modification of BPEL specification The real multimedia conference business process study verifies the high usability and operability of the proposed approach The overhead of the approach is the translation from SOAP message to HTTP message for invoking a RESTful service and the wrapping of the RESTful service response HTTP message to SOAP message for replying the BPEL invocation  In the future, more RESTful services should be invoked in real application s BPEL process to test and enhance the stability of the execution of th e BPEL process that integrate both SOAP services and RESTful services. From the testing experiences, we will conclude the proper kinds of services that are suitable for description in RESTful style and integration with SOAP services in BPEL  A CKNOWLEDGMENT This work is supported by the National Natural Science Foundation of China \(Grant No. 61003067\ National 973 Programs \(Grant No. 2013CB329102, 2011CB302704\ and Key Project of National Natural Science Foundation of China Grant No. 61132001  R EFERENCES 1 Z Ma amar S  Mo ste f ao u i  a n d H   Y ahy ao ui T o w ar d an A g e n t Base d  and Context-Oriented Approach for Web Services 
 vs. Big  
534 


Jorda Polo, David Carrera, Yolanda Becerra, Malgorzata Steinder  and Ian Whalley. Performance-driven task co-scheduling for  mapreduce environments. In Network Operations and Management  Symposium \(NOMS\2010 IEEE, pages 373 …380, 19-23 2010 12 K. Kc and K. Anyanwu, Scheduling hadoop jobs to meet deadlines  in 2nd IEEE International Conference on Cloud Computing  Technology and Science \(CloudCom\, 2010, pp. 388 …392 13 Xicheng Dong, Ying Wang, Huaming Liao Scheduling Mixed Real time and Non-real-time Applications in MapReduce Environment  In the proceeding of 17th International Conference on Parallel and  Distributed Systems. 2011, pp. 9 … 16 14 Xuan Lin, Ying Lu, J. Deogun, and S. Goddard. Real-time divisible  load scheduling for cluster computing. In Real Time and Embedded  Technology and Applications Symposium, 2007. RTAS 07. 13th  IEEE pages 303 …314, 3-6 2007 15 HDFS  http://hadoop.apache.org/common/docs/current/hdfsdesign.html  16 Chen He, Ying Lu, David Swanson. Matchmaking : A New  MapReduce Scheduling TechniqueŽ. In the proceeding of 2011  CloudCom, Athens, Greece, 2011, pp. 40 … 47 17 Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma and Khaled  Elmeleegy, Scott Shenker, and Ion Stoica, Delay scheduling: a  simple technique for achieving locality and fairness in cluster  schedulingŽ. In the proceedings of the 5th European conference on  Computer systems, 2010.  pp 265-278 18 Zhuo Tang, Junqing Zhou, Kenli Li, and Ruixuan Li "A MapReduce  task scheduling algorithm for deadline constraints.", Cluster  Computing, Vol. 15,  2012 19 Eunji Hwang, and Kyong Hoon Kim. "Minimizing Cost of Virtual  Machines for Deadline-Constrained MapReduce Applications in the  Cloud." Grid Computing \(GRID\, 2012 ACM/IEEE 13th  International Conference on. IEEE, 2012 20 Micheal Mattess, Rodrigo N. Calheiros, and Rajkumar Buyya  Scaling MapReduce Applications across Hybrid Clouds to Meet Soft  Deadlines." Technical Report CLOUDS-TR-2012-5, Cloud  Computing and Distributed Systems Laboratory, the University of  Melbourne, August 15, 2012 21 
 
11 
                
Chen He, Ying Lu, David Swanson. Real-Time Application Scheduling in Heterogeneous MapReduce Environments Technical Report TR-UNL-CSE2012-0004, University  of Nebraska-Lincoln, 2012 Available: http://cse apps.unl.edu/facdb/publications/TR-UNL-CSE20120004.pdf 22 T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, K  Elmleegy, and R. Sears. Mapreduce OnlineŽ. In NSDI 2010 23 A. D. Ferguson, P. Bodík, S. Kandula, E. Boutin, and R  Fonseca. Jockey: Guaranteed Job Latency in Data Parallel Clusters. In EuroSys, 2012 24 G. Wang, A. R. Butt, P. Pandey, and K. Gupta. A Simulation Approach to Evaluating Design Decisions in MapReduce SetupsŽ. In MASCOTS 2009 25 H. Herodotou and S. Babu. Profiling, What-if Analysis and Cost-based Optimization of MapReduce Programs In VLDB 2011 26 H. Herodotou, F. Dong, and S. Babu. No One \(Cluster Size Fits All: Automatic Cluster Sizing for Dataintensive AnalyticsŽ. In SoCC 2011  
1544 
1544 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


