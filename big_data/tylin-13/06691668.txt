Digging into Human Rights Violations Data modelling and collective memory Ben Miller   Ayush Shrestha   Jason Derby   Jennifer Olive   Karthikeyan Umapathy   Fuxin Li   Yanjun Zhao   Georgia State University miller,ashrestha2,jderby1,jolive1,yzhao9@gsu.edu  University of North Florida k.umapathy@unf.edu  Georgia Institute of Technology i@cc.gatech.edu Abstract Archives of human rights violations reports by virtue of their poor metadata basis in natural language and scale obscure ne grain analyses of violation event patterns Cross-document coreference of victim or perpetrator occurrences from across a corpus is challenging particularly when those mentions relate to different events These challenges are emblematic of the transition from small scale to big data analysis in the humanities This paper discusses these issues and proposes a framework to address these challenges so as to explore narrative construction and the formation of collective memory Though our framework is based on processing human rights violation reports it can be readily extended to support other big data problems in the humanities Keywords Digital Humanities Big Data I I NTRODUCTION Records pertaining to human rights violations are principally consulted so as to better understand the history of an event or potential responses to ongoing events These records have heterogenous origins capturing material produced by civilians governments and NGOs and by victims observers and perpetrators They are produced both during an event frequently as governments bureaucracies document their own behavior and after an event as witnesses emerge to speak out against violators or to participate in truth and reconciliation proceedings They contain heterogeneous information types ranging from aggregate lists of atomized acts to geospatial information regarding sites signiìcant to the prosecution of violations such as mass graves to interview or interrogation transcripts to observation reports by professional observers The desired analytic outcomes are equally broad encompassing 1 attempts to quantify the scope or frequency of violations so as to make determinations of the character of a violation pattern 2 determine emerging patterns of violations and assess possible interventions 3 attempts to study the generalizability of a given records collection in relation to a violation context 4 attempts to gather correlated evidence for truth and reconciliation or prosecutorial efforts or 5 attempts to tell the history of an event for the assuaging of public memory for the scholarly record or for the prosecution of suspected violators This heterogeneity of form a heterogeneity not uncommon for big data problems in the humanities makes analysis challenging As one example consider that most corpus processing methods only function for narrowly de ned data models e.g a key words in context script in R relies on the predictable extraction of well-transcribed text Although human rights corpora are smaller than the datasets addressed as big data in the sciences or social scientiìc projects examining the social web various features of human rights data and its analysis make this a big data problem These features include the high dimensionality of this information the heterogeneity and number of reports in a given corpus the population level coverage of the corpora the requirement for real-time analysis and the analysis requirement for veridicality Methods that address these requirements are drawn from the core elements of big data analysis statistics machine learning data visualization data architectures corpus linguistics high performance com puting and HCI Survivors and witnesses to human rights violations are placed in the complicated and problematic place of describ ing and relating information about a traumatic event In each persons individual recall incidents and details regarding the event may not always line up with one another perfectly This is problematic as it creates multiple accounts of a singular event however it is also interesting in that it allows for other considerations to be made to a narrative history that make up a collective memory and shape cultural identity regarding the traumatic event Problems however abound when considering the scope of human rights violations as many narratives become part of the larger narrative and not always in the same form How does one create meaning from these records collections when their scale and fragmentary nature resist access and interpretation How does one reveal meaning in ways that preserves the speciìc language and character of the witness observations How can NLP assist humanistic researchers in the performance of these tasks From photos to memos to transcripts to eld reports portions of traumatic narratives live in a variety of documents that are not only multiplied by the number of atrocities but also by the ways in which they are recounted To this end big data analysis can serve as a tool for constructing a narrative 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


out of many Our framework is designed to process large numbers of narratives parse elements describing time location person and semantic context and allow for larger narratives to emerge This cross-document narrative can then be used to understand how collective memories are formed and how history is made This same process can also identify perpetrators and victims on the basis of their linguistic and narratological structures and to contrast how groups self identify on linguistic levels This transversal reading of large text archives of human rights abuses can also facilitate the discovery of the stories of victims and perpetrators hidden by the scale of these corpora II R ELATED W ORK A Human Rights Data Analysis Narratives pertaining to the violation of human rights have occupied the humanities since the development of linear historiography with the Homeric retelling of the annihilation of Troy and documentation at scale has existed since the Federal Work Progress Administration sent writers throughout the American South to interview 2,300 former slaves between the years of 1936 and 1938 What differentiates the contemporary context is twofold First the scale of the records collections that document atrocities has grown astronomically and second the records being analyzed are in some instances produced contemporaneously with the violation Three examples indicate different aspects of the scale of contemporary data analysis in human rights violations research 1 the Guatemalan National Police Historic Archive is estimated to contain approximately 80,000,000 text records 2 two organizations the Shoah Foundation and the Fortunoff Archive for Holocaust Video Testimony have combined to produce over 100,000 hours of Holocaust survivor testimony and 3 in trying to estimate the number of casualties in the current Syrian crisis the Human Rights Data Analysis Group HRDAG had to work across the ledgers of eight separate organizations each of which was conducting idiosyncratic casualty counts over varying periods of the conîict Near real-time reporting of violations both enables human rights workers to respond to events in progress such as in the work of Best et al on the use of mobile devices in election monitoring and the w ork of Norheim-Hagtun and Meier on real-time crisis mapping in the aftermath of the 2009 Haiti earthquake In order for scholars from the humanities to engage with emerging big data analytical tools important changes are required to the weakly structured nature of traditionally collected data These changes both enable and proscribe the analyses that scholars can perform upon their data sets Migrating data for humanities scholars relied traditionally on methods of sampling such as qualitative coding string matching or transposition These classiìcations and operations limit data to normative values Data storage technologies such as structured databases also limited humanistic data and required careful processing of data sources and schema design to ensure informational integrity One way to redress the necessity of sampling operations could be derived from expanding data storage techniques associated with big data such as non relational database systems These possibilities present tempting options for humanistic inquiry as they can reduce the necessity to preprocess data via sampling methods This project is predicated on the premise that given a large collections of records describing a population gures will recur in multiple documents The idea that records over-represent victims in explicit and implicit ways emerges from research by Silva and Klingner et al in and Patrick Balls 2003 report on the number of killed or missing Peruvians from 1980-2000 That research e xamines collections of human rights documentation such as the 49,000 documents recovered from the Chadian Documentation and Security Directorate and the 24,000 reports of missing Peruvians submitted to the Comisin de la Verdad y Reconciliacin CVR By applying statistical methods for population counts such as Multiple-Systems Estimation  ha v e screened collections of reports about victims of abuse for recurring individuals and provided estimates of how many people enumerations missed That relatively simple statistical models of population analysis can determine quantitatively that victims of violence show up in multiple documents within a collection implies that other methods can be used to determine who those victims are Their methods by necessity focused not on identifying those individuals but on using statistical modelling of archives to determine the number of victims This over-reporting of individual victims within human rights corpora is endemic and presents an opportunity for a system that can read across a corpus Our work builds on their quantitative approach to human rights records by offering a qualitative method for assembling the fragmented stories of those victims In our method data provides the framework within which human beings communicate meaningful testimony that testimony is embedded in distributed through and obfuscated by the archives of human rights violation reports The difìcult work this project takes on is identifying the recurrence of those individuals and extracting the text that embodies that recurrence Our rst argument that fragmentary descriptions of perpetrators victims and abuses recur throughout a collection is important in human rights and truth and reconciliation contexts Revealing these implicit stories adds evidence to truth and reconciliation efforts and supplements historians access to the stories of an event This requires identifying related fragments of text within documents from across an archival collection and stitching them together in ways that preserve context to present a coherent accurate gure This mode of reading resembles traditional documentary 38 


biography and is already challenging with limited corpora such as a victims diary or a collection of reports from one police precinct When facing archives exceeding millions of words reading is impracticable and marginal gures effectively invisible B Big Data Models The choice of data model is complicated as the big data turn implicates data collection methods both enables and constrains analyses and inîuences the constitution of knowledge Replicating data models more comfortable with high-dimension data such as free text may expand the analytical possibilities available to humanities scholars For example big data analytical tools such as those developed by the Culturomics project cultural analytics project by L Manvoich require that researchers rst render te xts into accessible forms Both projects narrow the analytic scope to features susceptible to computational methods such as saturation or simple strings In order to leverage the power of big data tools researchers need to develop models that balance scalability exibility and mobility C Information Modeling and Extraction Most commonly approaches to information extraction in free text blend low context attempts at statistical analysis of material restricting scope to an internal frame of reference and high context approaches that seek to link extracted information to external data stores such as Wikipedia A low context approach for a corpus study of human rights violations reports might see the problem as one of document clustering and apply a method such as Term Frequency Inverse Document Frequency so as to build identifying keyword sets for each document Those keyword sets when compared imply document families A more complex but ideologically similar approach might use a technique from latent semantic analysis to infer topics for each document and produce clusters based not on explicit keywords but on implicit topics These techniques look to model a corpus based on typicallity at the level of the document and thereby facilitate a human researcherês engagement with a collection through categorization at a particular level of granularity A high context approach would check extracted information against an external reference so as to attempt more complex tasks such as disambiguation or geocoding of place names so as to map the various locations in a report Low context approaches seem best at the type of general engagement with a corpus described as distant reading while high conte xt approaches attempt to synthesize information beyond the limit of a given record Other information extraction and emplotment projects such as ChartEx and T rading Consequences demonstrate tw o approaches to the problem of disambiguation of entities and place names The rst relies on a diagrammatic approach to locations and rigid genealogies of entities The second correlates extracted location names to mapped places but then has to do so at a very general country level at which disambiguations do not occur Research into the problem of Information Extraction IE from unstructured and fragmentary documents has yielded many techniques but few are usable by the non-computer science researcher and few work for the particularities of large collections of violations reports Often the report formats are idiosyncratic metadata is sparse English is frequently a second language and narrative is the dominant conceptual mode These reports begin as free text that quickly gets sublimated in a reporting framework so that the violation information can be separated from the stories Soderland et al describe man y computational techniques in the area of IE that have been applied to various domains such as game scores from the National Football League and the Intelligence Community The difìculty of using these techniques and their inherent limits in the face of unstructured text has left witness reports medical narratives and many other domains unaddressed Humanistic work to elicit narrative connections across sets of documents pertaining to traumatic events has been limited to visualizations of the metadata contextualizing the free text and statistical understanding of populations and casualties These elisions were dictated by the purpose of the projects and the difìculty presented by unstructured text For example visual explorations of the Afghanistan War Logs by McCormick et al for The Guardian UK focused on geographic abstract understanding of the Logs metadata stories behind the data were necessarily elided Work in the broader eld of Narrative Intelligence of pattern detection and representation of a series of linked acts was pursued by an interdisciplinary reading group at MITs Media Lab in the early 1990s Frame w orks to facilitate the development of textual data analytics tools include UIMA GATE Seeker and SemTag Cerno and Armadillo Like the more familiar SemTag Cerno and Armadillo are focused on Semantic Web applications and so are most useful for automated tagging and retagging of documents in a corpus Each produces XML annotations of the source text and deìnitions for the annotations Of the three Armadillo is the most exible in regards to document regularity and the most automated UIMA and GATE are extensive in terms of the text processing modules and are open source allowing for further customization and extension Each allows for analysis of document corpora through the tokenization and parsing of the text as symbolic and syntactic objects Additional functionality is available in both frameworks for pipelining Java programs that introduce AI text-mining methods such as LSA or for structured regular expression searches based on user deìnitions Methods that rely on user-deìned patterns are both time consuming for the end user and fundamentally miss the central problem presented by implicit gures in large text archives the 39 


patterns are not visible to unaided readers because the scale of the archive resists conventional modes of reading Armadillo relieves some of the burden on the end user but retains a focus on highly local identiìcations i.e to what category does a particular phrase belong Despite the wealth of computational techniques for text analytics a gap exists between a humanistic researcherês conceptualization of the tasks required to make sense of records and the computational models currently used by IE tools D Anaphora Resolution Anaphora resolution has been a topic of interest in NLP since the work of o n pronominal anaphora resolution using domain and linguistic approaches and Hawkins  on associati v e anaphora The period from 1990 until today has focused on shallow low context syntactic approaches  Anaphora most often recognized in pronouns refer to any substitution of an ambiguous phrase for a more speciìc phrase Similarly exophora occur when the speciìc subject is not present in the current document The current state of the art for computational resolution of anaphora is best described in the work of R Mitkov S Lappin B W M Denber  and M Dimitro v[23 Anaphora resolution systems have been developed and examined in the work of Mitkov Webber Lappin and others and are classiìed as knowledge-poor or knowledge-rich An example of a knowledge-poor system i.e one that does not take into account complex linguistic semantic or grammatical rules is Dimitrov et als GATE-based implementation Dimitrovs system drew on earlier work by Lappin and Leass 25 pronominal resolution that used indicators such as deìniteness heading collocation referential distance and term preference Mitkovs system achieved pronominal resolution rates of 89.7 within a corpus of technical Dimitro vs system read a corpus of approximately 180,000 words drawn from broadcast and print news sources These genre speciìcations and corpora limits are typical of anaphora resolution systems Our work began with the core of Dimitrovs knowledge-poor system III M ETHOD Our framework describes a low context approach to the problem of transversal reading We propose a two tiered framework with Data and Presentation layers The data layer is responsible for data extraction parsing and running NLP modules to get the entities and events The presentation layer handles the visualization of the data and the feedback loop which modiìes the data based on the user feedbacks A Data Layer This layer deals with how the data is extracted processed and stored in the backend supporting the presentation layer This layer consists of the extraction module and the NLP module The extraction module is responsible for digitizing and parsing the documents A large portion of human rights violation documents that we obtained were stored in hard copy format and in many cases handwritten We used commercial OCR tools like OmniPage to digitize some of the documents In cases where the materials are barely readable due to poor archiving manual transcription is required After digitizing the document is parsed and the output is routed to the NLP module The goal of the NLP module is to facilitate crossdocument coreference of entities in collections of witness statements and interviews within the domain of rights The moduleês approach to resolving the task described as whether or not two mentions of entities refer to the same person begins by considering exophora and relies on placing pronominal entities within a high-order Event Storygram of location time name and semantic context Because temporal information is so often referential and ambiguous and therefore difìcult to extract and correlate our approach uses a phrase-based establishment of semantic context to support identifying the temporal context and to reinforce the automatic matching of elements within the Storygram Our model for processing linguistic uncertainty extends the work on veridicality in 29 30 incorporates semisupervised machine learning methods with a taxonomy This model describes the validity of automatically generated correspondences amongst the relations of person to time to location to semantic context This nounand verb-phrase extraction collocation detection and semi-automated matching feeds a 2D-planar visualization similar to network graph models Uncertainties in the document and information retrieval processes are visualized to allow researchers to conìrm whether entity occurrences should be conîated Because much human rights documentation contains sensitive information that cannot be made public this project is prototyping with both publicly available and restricted data Publicly available data sets used in this work include interviews with rst responders to the World Trade Center attacks and documentation exproduced by or related to the Extraordinary Chambers in the Courts of Cambodia ECCC as well as redacted version of reports describing contemporary violations committed by the Lordês Resistance Army in central Africa 1 Event summarization based on matching phrases Our main stratagem is to situate entities in the series of events that deìne their appearances Phrases useful for this process accord to a journalist template of Who What When Where and Why and are situated in the events reporting schema developed by Patrick Ball for human rights violations reporting 6 The goal is a system that can automatically extract these important entities as phrases and based on these extracts allow for the recognition of duplicate entities across documents A perceptual diagram of the system is shown in Figure 1 After the noun and verb phrases are extracted and passed 40 


Figure 1 Model used in the NLP module for cross-referencing the entitites on from the extraction module a phrase classiìer is used to determine which phrases fall into important entity categories such as Person Names/Geographic Locations/Date/Time or depiction of an event After classifying these phrases into categories a module called Collocation Detector detects which of the entities are described in the same context within a passage in the corpus This collocation of phrases is different from the collocation of words usually used in NLP in that it captures instances of the collocations instead of a global probability A collocation is only true when multiple elements correlate After a set of collocated phrases have been detected they are placed into the event template and fed into a visualization engine Human observers then decide which cross-document entities are identical The engine computes automatic scores to make suggestions to the observers on which events and entities should be merged 2 Phrase extraction and classiìcation The rst step of phrase extraction is done by running a full parser on each document and then extracting all the retrieved noun phrases and verb phrases from the parse tree We decided against using a shallow parser chunker because it has lower recall may not capture all the desired phrases than a full parser The parser we are using is the Stanford parser From the extracted phrases we formulate a classiìcation task for labeling important phrases for event extraction The important phrase classiìcation in our research is different from the traditional named entity recognition NER problem in NLP in that we are seeking to connect names to unnamed entities We have 8 categories for important phrases Organization Person Title Location Date Time Event Miscellaneous and the background category of Unimportant Not all of these categories are visualized Of these categories some are traditional NER or TimeML categories Event and Miscellaneous labels are new and determine some important phrases that might not be readily interpreted as named entities Phrases such as the pedestrian bridge the ferry or the second tower which are not identiìable as a particular named entity but might be crucial in depicting the event are classiìed as Miscellaneous To maximally utilize human knowledge in the phrase labeling phase an unsupervised selection mechanism selects the phrases to be labeled In this mechanism phrases are ranked by a score that is similar to a frequency or N-gram model but it discounts the probability of a phrase if it is very common in a background corpus S c  phrase  logP  phrase   max  logP bg  phrase   logP  phrase   0 1 where logP  phrase  is computed by an N-gram language model trained on the current corpus and logP bg  phrase  is based on a N-gram language model trained on a background corpus that is supposed to contain documents of all kinds Under this model the probability of a phrase is only discounted if P bg  phrase  P  phrase   This application of Term Frequency Inverse Document Frequenc helps us to nd frequent phrases in the corpus which are not popular in the background corpus The phrases with top scores are manually labeled Through this approach we can obtain the labels for the most frequent and unique phrases in the corpus which are likely to be more important in isolating an event Our N-gram training uses the modiìed Kneser-Ney smoothing from the MitLMpackage 34 The background language model is obtained from Microsoft Web N-gram Services Given a set of human-labeled phrases we then train two levels of classiìers on these phrases At the rst level a binary Important versus Unimportant phrase classiìer is trained At the second level a one-againstall multi-class classiìer is trained for each of the phrase categories described above except Miscellaneous which serves as the background category for important phrases The features used for the classiìers are common NER features  36 plus standard bag-of-w ords features F o r the Date and Time phrases we make use of the SUTime library which matches date and time expressions using an extensive set of rules deìned by regular expressions The classiìcation of these phrases do not depend on the labels For the collocation we use a simple metric a Gaussian kernel on the distance between mentions of different phrases Formally the collocation probability of one occurrence of a phrase given a set of other phrases is deìned as 41 


P  p 1  p 2   p k  exp     i  S  p 1   S  p 2  2  2 where S  p i  is the sentence number where pi occurred Given the deìned conditionals one can compute the joint probability P  p 1 p 2 p 3   pk  and use a threshold to determine which phrase set goes to an event template IV P RESENTATION LAYER The presentation layer consists of the visualization module and the feedback module The visualization module consists of Storygraphs and Storygrams Storygraph our earlier work is a 2D visualization technique for presenting time and location on the same chart It consists of two parallel vertical axes which are used for latitude and longitude and an orthogonal horizontal axis which is used for time An event E  lat lng time  is mapped in into the Storygraph by rst drawing a line segment connecting the corresponding latitude and the longitude in two vertical axes A marker is then placed on the line above the corresponding time of the event Hence a pre-requisite for using Storygraph is that the data needs to be structured and precise i.e it needs to have a precise geo-coordinates and timestamp Any additional attributes like the type of event can be shown by changing the size shape and color of the marker One of the visualization layer goals is to present certain as well as uncertain data Depending upon the context uncertainty refers to semantic uncertainty ranged values or missing data In our frame w ork uncertainty is introduced starting from the extraction phase as described above These uncertainties include the temporal By this time it had to be 11:00 oêclock at night and at that time I noticed locative I guess that would be North End Avenue and entity At this point I had my ve guys  W e use Storygrams to present uncertainty  A Storygram is a 2D-planar diagram consisting of events as its building blocks An event in this case is a 3-tuple consisting of time location and entity Storygrams are represented visually as triangles using the entities as vertices connected with weighted edges The weights represent the conìdence value in the relationship obtained from the NLP module These conìdence values show the certainty of the connection between two elements Figure 2 shows a trigram with conìdence values and elements To reduce visual clutter due to the excess of edges and vertices we employ details on demand and ltering capabilities on the dataset The users can also drag and drop the events over other events and manually enter the conìdence values to increase the association between two or more events Since the users load one corpus at a time this feature enables users to visually associate events in different documents The uncertainty in data also affects the output of semantic context and the verdicality modules To address this issue Figure 2 A model of Storygram showing events and uncertainty within the entities of the event we intend to make Storygrams interactive in that users can merge two vertices if they think they are the same Merging of two vertices also marks the underlying data to be changed The change is witheld until another expert veriìes it When the vertices are merged the edges are also merged in many cases In these cases the conìdence values are also merged This module is still under construction and we leave it as a future work V D ISCUSSION Applying our framework to various datasets describing genocide in Cambodia from 1974-1979 a contemporary militant movement in central Africa the attacks on the World Trade Center WTC in 2001 and U.S military logs from Afghanistan has yielded visualization showing the movements of individuals and groups across the time and space of event contexts Not per se indicative of violations two of our corpora are included in this research because they are publicly available and share syntactic and semantic features to primary rights violations data Those corpora are the WTC Task Force Interviews conducted with rst responders to the attacks of September 11 2001 and the material published by The Guardian UK as the Afghanistan War Logs At 511 interviews and 1.6m words the rst dataset does not qualify as big but the type of analysis our system affords is not replicable by non-computational methods as it relies on the cross-referencing of hundreds of thousands of granular phrases and the calculation of correlating uncertainties Our analysis of the WTC material revealed that one individual Father Mychal Judge appeared 86 times in 33 interviews These appearances describe his time on scene from when he arrived at the site to his death during the collapse of the north tower as bodies fell on the roof of 6 World Trade Center to his laying in state at St Peterês Extractions of these observations by survivors allow for the stitching together of a narrative describing the last day of Father Judgeês life We are still working on this corpus to extract narratives of other victims In the public reports from central Africa describing violations perpetrated by the Lordês Resistance Army there are noticeable differences in reports regarding important details such as number and descriptions of victims and perpetrators locations of incidences and variations in time 42 


and date Similarly detailed information may be given in ECCC interviews however key information such as what methods of interrogation and torture were permitted may vary depending on the individual and his or her position in the case For example discrepancies appear in the interview material in approved torture and interrogation techniques between Kaing Guek-Eav and Prak 44 An important feature of our model is the conîation of multiple accounts describing a singular event These multiple accounts provide a sense of depth and intricacy to the emerging broader narrative Mining these details allows scholars to examine the multiple perspectives embedded in collective memory and cultural identity Problems however abound when considering the scope of human rights violations as many narratives become part of the larger narrative and not always in the same form From photos to memos to transcripts to eld reports portions of traumatic narratives live in a variety of documents that are not only multiplied by the number of atrocities but also by the ways in which they are recounted One such example can be found in the ECCC documentation and the euphemism used by the Khmer Rouge In one of his interviews Kaing Guek-Eav details the meanings of the key terms smash execution resolve execution purgeé\(arrest and sweep cleanly away Furthermore his descriptions re g arding which administrators used which terms expand the ability to reconcile other narratives to others Through recognizing these terms and their variety of applications in regards to entities and times one can better reconcile these voices with other narratives in history and help to shape identities within a given perspective As discussed in our Storygraph visualization as seen in Figure 3 shows each individual report as a dot in a coordinate plane of latitude longitude and time What our visualization revealed are both patterns in the corpus indicative of documentation practices and patterns in the event the reports describe One such pattern seen in the void spaces numbered 1 2 and 3 corresponds to lulls in violence Vertical banding as seen at A B and C indicate events happening simultaneously across the geography described by the corpus In this case they indicated elections In Figure 4 also discussed at length in indi vidual units within the corpus can be seen traversing the geography and the temporality of the corpus Each colored line indicates one unit and their path corresponds to the mentions within documents An implication of these visualizations is that they atten the corpus so as to enable a holistic perspective on the data and may thereby lead researchers to focus on contextual analyses rather than granular analysis of individual victims and perpetrators However a holistic view of human rights data sets such as we are undertaking with the conìdential LRA material when modelled on the approach seen in the Storygraph gures facilitates drilling into the data Seeing the pathway taken by an individual through a corpus when connected to the document fragments that were mined to produce that pathway deeply connects the documentary evidence to the nal analytic visualization VI C ONCLUSION This paper explored the challenges faced by researchers working on human rights violations and proposed a framework for addressing some of those challenges That framework included elements for processing the textual data visualizing that data and feeding judgments made by users of the framework back into the processing layer Principally our methods for information extraction data visualization and user feedback work to generate narratives that traverse document boundaries These techniques for transversal reading of large historical corpora allow for the investigation of the relationship between big data and collective memories of traumatic events Memory as deìned by Maurice Halbwachs in 1948 in the seminal work on collective memory is by deìnition collective and only exists in the conversation undertaken by groups individuals have no capacity for memory only fantasy This founding theory on collective memory corresponds to our projects argument related to cross-document coreference information is only valid when it can be validated In the future we plan to reìne two areas of the framework and explore the potential applicability of this framework to corpora describing other categories of events Areas of the framework that we plan to improve are our methods for determining the semantic context of Storygrams and our modeling of linguistic uncertainty and concomitant veridicality Other categories of events that may be susceptible to this approach are environmental catastrophes popular uprisings and political movements A CKNOWLEDGMENT This material is based upon work supported by the National Science Foundation under Grant No 1209172 Any opinions ndings and conclusions or recommendations expressed in this material are those of the author\(s and do not necessarily reîect the views of the National Science Foundation R EFERENCES  V  Flusser  Into the universe of technical images  University of Minnesota Press 2011 vol 32  M L Best W  J Long J Etherton and T  Smyth Rich digital media as a tool in post-conîict truth and reconciliation Media War  Conîict  vol 4 no 3 pp 231Ö249 2011  I Norheim-Hagtun and P  Meier   Cro wdsourcing for crisis mapping in haiti innovations  vol 5 no 4 pp 81Ö89 2010  R Silv a J Klingner  and S W eikart State coordinated violence in chad under hissne habr A report by benetechês human rights data analysis group to human rights watch and the chadian association of victims of political repression and crimes Benetech Tech Rep 2010 43 


Figure 3 Figure adapted from our previous work sho wing the m o v ement of dif ferent units in the Afghanistan w a r  Figure 4 Figure adapted from our previous work sho wing patterns in the Afghanistan w a r data P atterns mark ed by A  C show lots of events happening at the same time 1  3 show lack of events and periodicity of pauses  P  Ball J Asher  D  Sulmont and D Manrique Ho w many peruvians have died American Association for the Advancement of Science Tech Rep 2003  P  Ball E T abeau and P  V erwimp The bosnian book of dead Assessment of the database full report Households in Conîict Network Tech Rep 2007  D Bo yd and K Cra wford Critical questions for big data Provocations for a cultural technological and scholarly phenomenon Information Communication  Society  vol 15 no 5 pp 662Ö679 2012  J.-B Michel Y  K Shen A P  Aiden A V eres M K Gray  J P Pickett D Hoiberg D Clancy P Norvig J Orwant et al  Quantitative analysis of culture using millions of digitized books science  vol 331 no 6014 pp 176Ö182 2011  L Mano vich Data stream database timeline the forms of social media 2012 On A v ailable http://lab softw arestudies.com/2012/10/datastream-database-timeline-new.html  Z S Syed T  Finin and A Joshi W ikipedia as an ontology for describing documents in ICWSM  2008  M L Jock ers Macroanalysis Digital Methods and Literary History  University of Illinois Press 2013  S Jones and H Petrie Charte x Disco v ering spatial descriptions and relationships in medieval charters 2013  C Gro v e r  T rading conferences  2012  S Soderland B Roof B Qin S Xu O Etzioni et al  Adapting open information extraction to domain-speciìc relations AI Magazine  vol 31 no 3 pp 93Ö102 2010  Guardian.co.uk  Afghanistan w a r logs  2011 Online Available http://www.theguardian.com/world/the-war-logs  M Mateas and A Stern F ac ade An experiment in building a fully-realized interactive drama in Game Developers Conference Game Design track  vol 2 2003 p 82  N Kiya vitskaya N Zeni J R Cordy  L  Mich and J Mylopoulos Cerno Light-weight tool support for semantic annotation of textual documents Data  Knowledge Engineering  vol 68 no 12 pp 1470Ö1492 2009  C L Sidner   T o w ards a computational theory of deìnite anaphora comprehension in english discourse DTIC Document Tech Rep 1979 44 


 J A Ha wkins Deìniteness and Indeìniteness  Humanities Press 1978  J Me yer and R Dale Mining a corpus to support associati v e anaphora resolution in Proceedings of the Fourth International Conference on Discourse Anaphora and Anaphor Resolution  2002  R Mitk o v  B  Bogurae v  and S Lappin Introduction to the special issue on computational anaphora resolution Computational Linguistics  vol 27 no 4 pp 473Ö477 2001  B W ebber  M  Egg and V  K ordoni Discourse structure and language technology Natural Language Engineering  vol 18 no 4 pp 437Ö490 2012  M Dimitro v  K Bontche v a  H  Cunningham and D Maynard A lightweight approach to coreference resolution for named entities in text Anaphora Processing Linguistic cognitive and computational modelling  vol 263 p 97 2005  S Lappin and H J Leass  A n algorithm for pronominal anaphora resolution Computational linguistics  vol 20 no 4 pp 535Ö561 1994  R Mitk o v   F actors in anaphora resolution the y are not the only things that matter a case study based on two different approaches in Proceedings of a Workshop on Operational Factors in Practical Robust Anaphora Resolution for Unrestricted Texts  Association for Computational Linguistics 1997 pp 14Ö21  K V a n Deemter and R Kibble On coreferring Corefer ence in muc and related annotation schemes Computational linguistics  vol 26 no 4 pp 629Ö637 2000  C Northw ood T ernip temporal e xpression recognition and normalisation in python Ph.D dissertation Masters thesis University of Shefìeld 2010  A Auger and J Ro y  Expression of uncertainty in linguistic data in 11th International Conference on Information Fusion 2008  IEEE 2008 pp 1Ö8  M J Druzdzel V erbal uncertainty e xpressions Literature review Pittsburgh PA Carnegie Mellon University Department of Engineering and Public Policy  1989  E Marshman Expressions of uncertainty in candidate knowledge-rich contexts A comparison in english and french specialized texts Terminology  vol 14 no 1 pp 124Ö151 2008  D Klein and C D Manning  Accurate unle xicalized parsing in Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1  Association for Computational Linguistics 2003 pp 423Ö430  W  W  Cohen and J Richman Learning to match and cluster large high-dimensional data sets for data integration in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining  ACM 2002 pp 475Ö480  S F  Chen and J Goodman  A n empirical study of smoothing techniques for language modeling in Proceedings of the 34th annual meeting on Association for Computational Linguistics  Association for Computational Linguistics 1996 pp 310 318  B.-J Hsu and J Glass Iterati v e language model estimation efìcient data structure  algorithms in Proceedings of Interspeech  vol 8 2008 pp 1Ö4  T  Zhang and D Johnson  A rob ust risk minimization based named entity recognition system in Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume 4  Association for Computational Linguistics 2003 pp 204Ö207  L Ratino v and D Roth Design challenges and misconceptions in named entity recognition in Proceedings of the Thirteenth Conference on Computational Natural Language Learning  Association for Computational Linguistics 2009 pp 147Ö155  A X Chang and C Manning Sutime A library for recognizing and normalizing time expressions in LREC  2012 pp 3735Ö3740  A Shrestha Y  Zhu B Miller  and Y  Zhao Storygraph Telling stories from spatio-temporal data in Lecture Notes in Computer Science  vol 8034 Springer 2013 pp 693 703  A T  P ang C M W ittenbrink and S K Lodha  Approaches to uncertainty visualization The Visual Computer  vol 13 no 8 pp 370Ö390 1997  W  T  C T  F orce W orld trade center task force intervie ws  2001  B Shneiderman The e yes ha v e it A task by data type taxonomy for information visualizations in Visual Languages 1996 Proceedings IEEE Symposium on  IEEE 1996 pp 336Ö343  C T rack er  Incidents e xport  I n visible Children and Resolve Tech Rep 2013  ECCC  Anne x 25 Written record of intervie w 09 june 1999 1999  ECCC Written record of intervie w o f Prak Khan  2007  ECCC Written record of intervie w o f Duch by CIJ o n 2101-2008 2008  M Halbw achs and L A Coser  On collective memory  University of Chicago Press 1992 45 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





