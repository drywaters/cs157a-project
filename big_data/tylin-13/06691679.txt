The Human Face of Crowdsourcing: A Citizen-led Crowdsourcing Case Study  Sheryl Grant  School of Information and Library Science \(SILS University of North Carolina Chapel Hill Chapel Hill, NC, USA sherylgrant@gmail.com Kristan E. Shawgo  HASTAC Duke University Durham, NC, USA kristan.shawgo@duke.edu  Richard Marciano  School of Information and Library Science \(SILS University of North Carolina Chapel Hill Chapel Hill, NC, USA richard_marciano@unc.edu   Jeff Heard  Renaissance Computing Institute \(RENCI University of North Carolina, Chapel Hill Chapel Hill, NC, USA jeff@renci.org   Priscilla Ndiaye  Southside Advisory Community Board Asheville, NC, USA priscillapower2@gmail.com     Abstract   The Cyber-Infrastructure fo r Billions of Electronic Records \(CI-BER\oject is a collaborative big data management project based on the integration of heterogeneous datasets and multi-source historical and digital collections, including a placebased citizen-led crowdsourcing case study of the Southside neighborhood in Asheville, North Carolina. The project is funded by the National Science Foundation and the National Archives and Records Administration \(NARA\ agencies A test-bed collection containing nearly 100 million files and 50TB of data was developed, with content representing electronic Federal Government records from 150 federal agencies.  The CI-BER project advances the state of the art in generalizable and extensible ultra-highly scalable data management architectures, potentially enabling robust technical preservation of, and access to, electronic records and digital data in the context of emerging national scale cyber-environments. A first-generation open source collaborative mapping environment prototype is currently being developed to support novel ìcitizen-led crowdsourcingî possibilities for archival material    Keywords: Crowdsourcing, citizen-sourcing, Southside Asheville, Housing Authority of the City of Asheville, Asheville NC, urban renewal, African-Americans I   I NTRODUCTION  The Cyber-Infrastructure for Billions of Electronic Records \(CI-BER\ project is a collaborative big data management project based on the integration of heterogeneous datasets and multi-source historical and digital collections, including a place-based citizen-led crowdsourcing case study of the Southside neighborhood in Asheville, North Carolina. The project is funded by the National Science Foundation and the National Archives and Records Administration \(NARA\ agencies A test-bed collection containing nearly 100 million files and 50TB of data was developed, with content representing electronic Federal Government records from 150 federal agencies.  The CI-BER project advances the state of the art in generalizable and extensible ultra-highly scalable data management architectures, potentially enabling robust technical preservation of, and access to, electronic records and digital data in the context of emerging national scale cyber-environments. A first-generation open source collaborative mapping environment prototype is currently being developed to support novel ìcitizen-led crowdsourcingî possibilities for archival material Concurrent with the development of this prototype was the co-creation of a citizen-led crowdsourcing protocol to solicit design input from community members, a process that represents a viable technique to deal with ìbig data processing and management.  The connection between these two prototypes lies in the ability to expose relevant CI-BER datasets \(1930 Census, 1937 redlining data, 1940 Census etc.\e ìcrowdsî and allow them to tag and augment this content with additional annotations and knowledge  2013 IEEE International Conference on Big Data 21 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


II  CITIZEN-LED  CROWDSOURCING Crowdsourcing is ìthe process of leveraging public participation in or contributions to projects and activities  d is carried ou t in diff ere n t w a y s depen d ing on t h e community involved, the content to be crowdsourced, and the technology available. What sets this project apart from a straightforward digitization or visualization project is its deployment of citizen-led crowdsourcing, in which the community assumes an essential role in the design and implementation process. The CI-BER study followed a fourphase crowdsourcing process designed with and for the Southside Asheville community: 1. crowdsourcing framework, 2. community history, 3. remapping process and 4. citizen-led crowdsourcing process   A  Crowdsourcing Framework Drawing on what citizens know and want  to know is instrumental to CI-BERís implementation of crowdsourcing The involvement of Ashevilleís Southside community in cocreating and designing the process was of paramount importance to its success. Dunn and Hedges c ribe three types of crowdsourcing approaches: contributory collaborative, and co-created. CI-BER proposed a process of co-creation, in which the community was actively involved in most or all steps of crowdsourcing, which framed their engagement as citizen-led crowdsourcing. Energetic and vital participation in an earlier North Carolina Humanities Council \(NCHC\ project, Twilight of a Neighborhood 2 indicated that the community would willingly participate in a citizen-led crowdsourcing process  CI-BERís employment of ìcitizen-led crowdsourcing is inspired by the Obama administrationís Open Government Initiativ w h i c h en cou r a g es pu blic participation and collaboration. It is a derivative of the term citizen sourcing, defined as the ìgovernment adoption of crowdsourcing techniques for the purposes of \(1\listing citizens in the design and execution of government services and \(2\g into the citizenryís collective intelligence 4 V i ve k K u nd r a  Ch i e f I nfo r m a t i o n O f fi c e r o f t h e U n i t e d  States from March 2009 to August 2011 under President Obama, described citizen sourcing as a way of ìtapping into the ingenuity of the American people to solve those problems that are too big for government to solve on its own itizen s o u r ci ng is  deriv e d f r o m th e ter m  crowdsourcing and emphasizes the type of civic engagement typically enabled through Web 2.0 participatory technologies, over a more impersonal crowd-based distributed problem-solving and production model  Hilgers and Ih g e st th a t  e x t ern a l collaboration  and innovation between citizens and public administrations can offer new ways of citizen integration and participation enhancing public value creation and even the political decision-making process.î  This type of collaboration extends to what the Archivist of the United States, David Ferriero, called ìcitizen archivists ref e re n ce to citizen  scientists, as a means to increase public engagement in the archives given the National Archives and Records Administrationís over-abundance of paper records and the need to digitize and transcribe them. A participatory archive is a n organization, site or collection in which people other than archives professionals contribute ìknowledge or resources, resulting in increased understanding about archival materials, usually in an online environmen  This definition relates to the notion of ìcitizen-led sourcingî implemented in the CI-BER project, putting civically engaged community members at the forefront so that the focus is on their collective engagement with the archive    Fig. 1 From crowdsourcing to citizen-led sourcing \(dates indicate when concepts were first introduced    B  Community History The University of North Carolina-Asheville \(UNC-A Ramsey Libraryís Special Collections & University Archives acquired the Housing Authority of the City of Asheville records in July of 2007. The nearly intact and complete collection of urban renewal documents includes approximately 130 linear feet and 129 cartons that document a number of ìsignificant redevelopment projects undertaken from the early 1960s to the mid-1980s.î CI-BER focuses on Ashevilleís Southside area, which at over 400 acres was the largest urban renewal area in the southeastern United States. In 1966, the Southside community 22 


represented about 4,000 people, 98% of whom were African American, living in nearly 1,300 households, and comprising 7% of Ashevilleís population. In 2008, the North Carolina Humanities Council \(NCHC\ sponsored a project that documented the communityís life prior to urban renewa Urban renewal as a federal government program was a 24-year \(1949-1973\itiative started under the Housing Act of 1949, and modified under the Housing Act of 1954 It used the 1930ís Home Ownersí Loan Corporation HOLC\ing terminology of ìblightî and ìslumsî to launch an ambitious redevelopment and eminent domain process that led to the bulldozing of some 2,500 neighborhoods in 993 American cities. It is estimated that one million people were dispossessed in the process  C  Remapping Process Maps are a natural visualization method for presenting the changes across neighborhoods. However, GIS applications focus primarily on structured data, analytics and thematic mapping, and require thorough grounding in geographic principles to be used  effectively.  A platform that supports community mapping must be able to handle structured data and unstructured data equally, and must allow for collaborative editing of content by non GISexperts.  These requirements are driven by the project itself and the nature of the data collection The goal of CI-BER was to recreate an entire collection in the very first iteration, to capture key elements \(owner renter, parcel number, street address\ per scanned documents \(initial appraisal sheet, and house photo\rom the ìHousing Authority of the City of Asheville Recordsî at UNC-A d rapidly re m a p th e en tire S ouths ide neighborhood. This information was combined with fragments of maps found in the collection to create a digital spatial canvas of the entire neighborhood in 1965 prior to urban renewal. Each digitized  parcel is clickable and linked to its key elements. Being able to reproduce this process online and identify and tag collection items is a key output of the citizen-led sourcing of the archive. Through iterative and incremental passes over the files, the CI-BER team gradually digitized strategic content, transcribed it through citizen-led crowdsourcing, visualized and mapped the content, enhanced the collection, and developed a working content model, and added functionality to the software user interface being built  The software supporting this process is Big Board, [1 part of RENCIís Geoanalytic e os patial cyberinfrastructure. Big Board is a web-based collaborative mapping interface that enables its users to juxtapose traditional GIS data with unstructured data via a set of drawing tools and an underlying content management system \(CMS\inally envisioned as a decision support tool for emergency managers [14  who need more flexibility and timeliness than traditional GIS affords, Big Board was repurposed to support the cooperative work of digital humanities scholars and Asheville community members  In th e Big Board, u s ers collaborate in a v i rtu a l data room that allows GIS maps to be layered onto web-maps and users are able to draw on and edit content, seeing changes in real time CI-BER collaborators began by pulling in an historical 1960s map of the Asheville area, georeferencing it in a GIS system, and adding it to a Big Board room.  Modern layers from the Asheville city GIS system such as street centerlines tax parcels, and addresses were then added to support and provide context to the historical content. From this basis collaborators drew individual polygons using the Big Boardís drawing tools that represent tax parcels from the historical map Big Board software associates an editable web page from the CMS with each polygon.  Web pages were edited in the Big Boardís WSIWYG editor adding the aforementioned key elements: owner, renter, parcel number street address and collaborators can append scanned pages photographs, audio clippings or additional descriptive text to each page.  In the Big Board map interface, the polygons are now clickable links on the map that take a user to the created content and allow editing if the user has sufficient permissions  This process allows incremental, ìschema-lessî editing of content by the CI-BER collaborative team whereas traditional GIS setups would enforce a schema to start with and force collaborators to email documents back and forth and work from a ìmaster copy,î changing the process and leaving it more error-prone D  Citizen-led Crowdsourcing Critical to citizen-led crowdsourcing is the collaboration of a key community member who contributed to the design of Big Board on behalf of the Southside residents. The key member of the CI-BER project team is the chair of the Southside Community Advisory Board in Asheville; as a resident of the Southside neighborhood during the time of urban renewal and a leader still residing in the community today, her feedback provided an invaluable connection to the community and its history Building and maintaining trust with the community cannot be understated in terms of its significance to the credibility and success of a project like CI-BER. Having a leader from the Southside Community Advisory Board represent the project at community events, discuss it locally via word of mouth, give interviews to local news d in trodu ce CIBER at a ìRemapping Communityî information session on August 3 rd 2013 ensures that the project is citizen-led 23 


Remapping Communityî was the CI-BER projectís initial introduction to the community and included information about the process of urban renewal in Asheville an introduction of the CI-BER team to the community and a demonstration of Big Boardís capabilities and possibilities for preserving and displaying the neighborhoodís African American history through varied data streams \(archival data text, image, audio, and video\he information session concluded with the Southside neighborhoodís civically engaged community members contributing to the design and input for execution of the next steps of data collection to be appended to Big Board, including the collection of photographs, written documents and oral histories.  Central to this process is the importance of understating what citizensí motivations are and how to sustain their interest We hope this project will also lead to insights in this area III  CONCLUSION   Nationwide, thousands of cities followed the same urban renewal development patterns that the Southside neighborhood in Asheville, North Carolina experienced, and this case study naturally leads to scaling at a very large national level. By focusing on the Southside neighborhood in Asheville, North Carolina, CI-BER demonstrates the potential for automation and integration of temporal and spatial datasets involving census, economic, historic planning, insurance, scientific, and financial content, with an eye on scalability and the goal of making a national impact on future research  A CKNOWLEDGMENT  This research was funded by the NSF/OCI Grant 0848296 called ìCyberinfrastructure for Billions of Electronic Recordsî, a cooperative agreement between NSF the National Archives and Records Administration \(NARA and UNC   R EFERENCES   1  S. Dunn and M. Hedges. \(2012\ìCrowd-sourcing scoping study: Engaging the crowd with humanities research.î Centre for e-Research, Department of Digital Humanities Kingís College London. Arts & Humanities Research Council. A project of the AHRC Connected Communities Theme Retrieved from http://humanitiescrowds.org/2012/12/21/finalreport  2  S. Judson \(2010\wilight of a neighborhood: Ashevilleís East End, 1970. Crossroads: A Publication of the North Carolina Humanities Council. Summer/Fall 2010. Retrieved from http://nchumanities.org/sites/default/files/documents/Crossroa ds%20Summer%202010%20for%20web.pdf  3  Open Government Initiative of The White House http://www.whitehouse.gov/open  4  T. Nam. \(2012\Suggesting frameworks for citizen-sourcing via Government 2.0.î Government Information Quarterly vol. 29\(1\, pp.12-20, doi:10.1016/j.giq.2011.07.005 5  V. Kundra. ìHow ëOpen Goví datasets affect parents and consumers.î White House Open Government Blog.  January 23, 2010. Retrieved from http://www.whitehouse.gov/blog/2010/01/23/why-open-govmatters-you  6  D. Hilgers and C. Ihl \(2010\ ìCitizensourcing: Applying the concept of open innovation to the public sector.î The International Journal of Public Participation, vol. 4\(1 p. 6788 Retrieved from http://search.ebscohost.com.proxy.lib.duke.edu/login.aspx?dir ect=true&db=poh&AN=48757187&site=ehostlive&scope=site  7  D. Ferriero \(2013\ ìVolunteers help NARA do its job support professional archivists.î Archival Outlook January/February 2013, p. 16. Society of American Archivists  8  K. Theimer, "Exploring the Participatory Archives", on ArchivesNext blog, August 30, 2011 http://www.archivesnext.com/?p=2319  9  City of Asheville Donates Urban Renewal Files to UNC Asheville. October 23, 2007 http://www2.unca.edu/news/releases/2007/renewal.html     M. Fullilove. Root Shock: How Tearing Up City Neighborhoods Hurts America, and What We Can Do About It. New York: One World/Ballantine Books, 2004    Housing Authority Records of the City of Asheville Acquired by University of North Carolina Asheville.  July 2007 http://toto.lib.unca.edu/findingaids/mss/housing_authority_cit y_asheville/default_haca.htm    J. Heard, S. Thakur, J. Losego, K. Galluppi. ìBig Board Teleconferencing Over Maps for Shared Situational Awareness.î Journal of Computer Supported Cooperative Work \(JCSCW\er, July 2013. doi:10.1007/s10606013-9191-9   J. Heard. ìGeoanalytics.î Tech Report TR-11-03 http://www.renci.org/publications/technical-reports    K. Galluppi, University of North Carolina, Chapel Hill, NC and J. Heard, J. L. Losego, B. E. Montz, C. F. Smith, and S S. Schotz ìWeather Service and Emergency Management Collaboration Through An Interactive Web-Based Map Conferencing Infrastructure.î AMS 2012   J. Heard, R. Marciano. ìThe Big Board: A Collaborative Environment for Geography.î Virtual School on Computational Science and Engineering \(VSCSE\, Data Intensive Summer School, workshop taught on July 8, 2013 http://www.vscse.org/summerschool/2013/bigdata.html    R. Neufield. ìProject Seeks to Recreate History of Ashevilleís Southside,î August 2, 2013 http://www.citizentimes.com/article/20130804/LIVING/308040026/Projectseeks-recreate-history-Asheville-s-Southside   24 


Positive seeds Negative seeds Word n\(Word Word n\(word Positive seeds Negative seeds Emoticon n\(Emoticon Emoticon n\(Emoticon Approaches Accuracy Learning type Feature number 
002\003 
002 
002 003\004 
TABLE I S EED S ENTIMENT W ORDS U SED IN joyful 113,991 poor 82,401 happy 83,250 wronged 36,677 happiness 54,095 contempt 27,627 wonderful 27,894 002 002 despair 25,027 002\003 beautiful 23,362 002 sad 14,089 002 002 handsome 20,450 002 002 disgusting 9,629 002 002 ne 18,128 003 002 catty 9,400 TABLE II S EED E MOTICONS USED IN 515,559 272,282 271,247 126,016 262,008 92,197 234,699 85,064 162,469 82,401 151,573 72,998 145,506 69,151 TABLE III T HE A CCURACY OF D IFFERENT A LGORITHMS PMI+W 68.5 unsupervised 14 words PMI+E 78.7 unsupervised 14 emoticons ECWM 81.5 supervised 80  vol 2 no 1-2 pp 1Ö135 Jan 2008  M Hu and B Liu Mining and summarizing customer re vie ws   August 2004  P  D T urne y  Thumbs up or thumbs do wn semantic orientation applied to unsupervised classiìcation of reviews in  ser ACL 02 Stroudsburg PA USA Association for Computational Linguistics 2002 pp 417Ö424  V  Hatzi v assiloglou and J M W iebe Ef fects of adjecti v e orientation and gradability on sentence subjectivity ser COLING 00 2000  P  D T urne y and M L Littman Measuring praise and criticism Inference of semantic orientation from association  vol 21 pp 315Ö346 October 2003  Hatzi v assilogou V asileios and K R McK eo wn Predicting the semantic orientation of adjectives  pp 174Ö181 1997  B P ang and L Lee  A sentimental education Sentiment analysis using subjectivity summarization based on minimum cuts in  Association for Computational Linguistics 2004 p 271  J Kamps M Marx R J Mokk en and M De Rijk e Using w ordnet to measure semantic orientations of adjectives 2004  S.-M Kim and E Ho vy  Determining the sentiment of opinions  in  ser COLING 04 2004  S A OKI and T  Uni v ersity   A method for automatically generating the emotional vectors of emoticons using weblog articles  2011  J Zhao L Dong J W u and K Xu Moodlens An emoticon-based sentiment analysis system for chinese tweets  August 2012  A Esuli and F  Sebastiani Determining the semantic orientation of terms through gloss classiìcation in  ser CIKM 05 New York NY USA ACM 2005 pp 617Ö624  I Jollif fe  Wiley Online Library 2005  C.-C Chang and C.-J Lin LIBSVM A library for support vector machines  vol 2 pp 27:1Ö27:27 2011 software available at http://www.csie.ntu.edu.tw cjlin/libsvm  L Xu H Lin and Y  P an The construction of the emotional v ocab ulary    vol 27\(2 pp 180Ö185 2008 
2 PMI+E PMI+E PMI+W PMI+W ECWM PMI+W PMI+E ECWM PMI+E 
002\003 
is short for PMI with emoticon seeds Itês similar with  But what we select emoticons as the seeds That means the orientation of a given word is calculated from the strength of its association with a set of positive emoticons minus the strength of its association with a set of negative emoticons The rules that we select the seed emoticons are the same as the approach of the  The seed emoticons are listing as Table II The overall result is showed as Table III For the result of  the feature number of original feature vector V sword is 640 After the reduction of the features the feature number of principal components of V sword is 80 which is much less than the original features What means that the original vector does have lots of redundant features Table III shows that emoticons are better features The accuracy of is just 68.5 compared to that of  which is 10.2 higher and is 2.8 higher than that of  The seeds play the key role In fact the selection of seeds is the process of human intervention and sentiment words are more widely used in terms of usage We have a large range to select the seed sentiment words but the range of emoticons are much more limited So the error that selection of emoticons brings is smaller V C ONCLUSION This work aims to classify the polarity of sentiment words The way we did it is using the emoticons First we proposed an efìcient approach to classify the polarity of emoticons Based on the polarity of the emoticons as well as the association between the sentiment word and emoticons we build the sentiment wordês model through the emoticons which is called ECWM Experimental results demonstrate that ECWM was a good model to represent the orientation of sentiment words For the future work more data would be crawled to capture the rich co-occurrence between emoticons and sentiment words Moreover the polarity of sentiment words would be used to classify the orientation of statuses in Weibo A CKNOWLEDGMENT This work was partially supported by the Fundamental Research Funds for the Central Universities the National Natural Science Foundation of China under Grant No 61271041 National Basic Research Program of China 973 Program under Grant No 2009CB320504 R EFERENCES  B P ang and L Lee Opinion mining and sentiment analysis  
002 003 002 
002 003 
PMI+W PMI+E Found Trends Inf Retr KDD Proceedings of the 40th Annual Meeting on Association for Computational Linguistics ACM Transactions on Information Systems Proceedings of ACL Proceedings of the 42nd annual meeting on Association for Computational Linguistics Proceedings of the 20th international conference on Computational Linguistics ACACOS KDD Proceedings of the 14th ACM international conference on Information and knowledge management Principal component analysis ACM Transactions on Intelligent Systems and Technology Journal of The China Society For Scientiìc and Technical Information 
002 003 
138 
138 
138 
138 
138 
138 


side sends a certain number of key-value pairs starting from the beginning of map output le instead of sending the entire map output le at once While receiving these key-value pairs from all map locations a ReduceTask now merges all these data to build up a Priority Queue It then keeps extracting the key-value pairs from the Priority Queue in sorted order and puts these data in a rst in rst out structure named as DataToReduceQueue As each map output le is already sorted in the mapper side the merger in the ReduceTask can only extract the data from Priority Queue until the point when the number of key-value pairs from a particular map decreases to zero At that point it needs to get next set of key-value pairs from that particular map task to resume extracting from Priority Queue For faster response in TaskTracker side we propose an efìcient caching mechanism for the intermediate data residing in map output les stored in local disk User can enable caching in TaskTracker side through a conìguration parameter  The following is a new component in the TaskTracker side for caching  MapOutputPrefetcher is a daemon threadpool which caches intermediate map output as soon as it gets available After nishing a map task one of the daemons starts to fetch the data from this map output and caches it in PrefetchCache The novel feature for this cache is that it can adjust caching based on data availability and necessity It can also prioritize which data to cache more frequently based on the demand from the ReduceTasks Depending on heap size availability it can limit the amount of data to be cached in PrefetchCache Even with caching cache misses may occur as ReduceTasks requests may arrive faster than caching In that case TaskTracker fetches data directly from disk itself without waiting for caching But after disk fetch it requests MapOutputPrefetcher to cache this particular map output data with more priority so that successive requests for this output le can be served from the cache Intermediate data pre-fetching and caching plays an important role for achieving better performance with respect to job execution time For large number of ReduceTasks more requests for map output les arrive to a single TaskTracker which can swamp out the I/O bandwidth in TaskTracker side So an efìcient cache implemented here can alleviate such bottleneck and provide better performance In Section IV-D we show how our caching mechanism provides signiìcant performance improvement Figure 3 shows our design which enhances performance by efìciently overlapping shufîe merge and reduce operations in ReduceTask In the default design the merge process starts immediately with shufîe However there exists an implicit barrier for the start of reduce operation the reduce operations can only be started after all the merge operations have completed In our design we start reduce operation as soon as the rst merge completes In this way we can achieve maximum beneìt by introducing pipelining between merge and reduce stages Here we distinguish our proposed RDMA-based design for MapReduce with that of Hadoop-A Some major differences are as follows 1  with RDMA-based communication in the shufîe stage we redesign the merge mechanism to get more overlapping of the reduce stage with the shufîe and merge stages In order to respond to the key-value pairs requested from the reducer as soon as possible we design the intermediate data pre-fetching and caching mechanism in Hadoop TaskTrackers On the other hand although Hadoop-A has proposed a DataEngine tool for intermediate data access on the disk of the mapper side DataEngine doesnêt provide data caching to decrease the disk access based on the paper In section IV we compare our design with Hadoop-A The better performance of our design is a result of the data pre-fetching and caching mechanism 2  HadoopA has implemented a separate merge algorithm and the DataEngine tool in native C and has provided a plug-in based approach to provide RDMA operations on both TaskTracker and ReduceTask side However in order to provide minimal code changes in the existing Hadoop codebase our implementation for merge algorithm still makes use of the existing Merge operation with minimal code changes And the data pre-fetching and caching mechanism is implemented as a daemon in TaskTracker side which does not require any separate tools 3  Between these two RDMA based implementations our design has more user level exibility in terms of conìguration and tuning We provide a number of conìguration parameters such as RDMA packet size enabling of caching number of key value pairs transmitted in each packet etc to give user the exibility and options of choosing the best conìguration for a particular workload Tuning of these parameters can also play a major role on achieving better performance in terms of job execution time Section IV-C shows one of these examples where better parameter tuning achieves better beneìts On the other hand we donêt nd the similar interfaces in Hadoop-A implementation 
3  MapOutputPrefetcher 4  C Differences with respect to Hadoop-A Data Pre-fetching and Caching JAVA Implementation vs C Implementation Conìguration and Tuning Interfaces 
Intermediate Data Pre-fetching and Caching Overlap of Shufîe Merge and Reduce 
mapred.local.caching.enabled 
1913 


017\012\037\007\034\022\020\027.\007\026$\012\016\034\010\012\027 017.*4\004\007\013\012\016\027.\007\026$\012\016\034\010\012\027 
We have used an Intel Westmere cluster for our evaluations This cluster consists of compute nodes with Intel Westmere series of processors using Xeon Dual quad-core processor nodes operating at 2.67 GHz with 12GB RAM and 160GB HDD Each node is equipped with MT26428 QDR ConnectX HCAs 32 Gbps data rate with PCIEx Gen2 interfaces The nodes are interconnected using a Mellanox QDR switch Each node runs Red Hat Enterprise Linux Server release 6.1 Santiago at kernel version 2.6.32131 with OpenFabrics version 1.5.3 This cluster also has dedicated storage nodes with the same conìguration but with 24GB of RAM each Additionally eight of the storage nodes are equipped with two 1TB HDD each Four of the storage nodes also have Chelsio T320 10GbE Dual Port Adapters with TCP Ofîoad capabilities In the gures presented in this section we have mentioned OSU-IB to indicate our RDMA-based design of MapReduce and Hadoop-A to indicate the design in 32 Gbps indicates InìniBand QDR card speed We have performed this experiment in 10 GigE IPoIB 32 Gbps with vanilla Hadoop and Hadoop-A and compared the results with our design For this experiment we have found that the optimal HDFS block-size for 10 GigE IPoIB 32 Gbps and our design is 256 MB whereas it is 128 MB for Hadoop-A We have used TeraGen to generate the input data for TeraSort Figure 4 shows the job execution times of the TeraSort benchmark in a four-DataNode cluster In the experiments for Figure 4\(a we show performance results with single and dual HDDs for each interconnect For single HDD 30 GB sort size our design reduces the job execution time by 9 over Hadoop-A 32 Gbps 35 over IPoIB 32 Gbps and 38 over 10 GigE Compared with IPoIB and 10 GigE our design uses native IB verbs communication for the data shufîe which is much better than the socket based communication on IPoIB and 10 GigE Although HadoopA also uses native IB verbs communication the data prefetching and caching mechanism of our design improves the performance On the other hand if two HDDs are used per node our design improves the execution time by 13 over HadoopA 32 Gbps 38 over IPoIB 32 Gbps and 43 over 10 GigE for the same sort size For 40 GB sort size our design achieves an improvement of 17 48 and 51 over Hadoop-A 32 Gbps IPoIB 32 Gbps and 10 GigE respectively When multiple HDDs are used per node the performance bottleneck of the local disk read and write bandwidth is alleviated Compared to Hadoop-A our design can utilize the improved bandwidth more efìciently to overlap the data shufîe merge and reduce further as illustrated in section III-B We have performed similar experiments with the TeraSort benchmark using eight DataNodes In this case we varied the sort size from 60 GB to 100 GB As shown in Figure 4\(b our design reduces the job execution time 21 over Hadoop-A 32 Gbps for 100 GB sort size with single 
Figure 3 Overlapping of different processes in MapReduce workîow IV P ERFORMANCE E VALUATION In this section we present the detailed performance evaluations of our RDMA-based design of Hadoop MapReduce and its impact on different Hadoop benchmarks We compare the performance of our design with socket based interconnects 10 GigE and IPoIB and Hadoop-A W e ha v e performed the experiments on different storage platforms single/multiple HDDs or SSD per node in order to illustrate the effect of I/O on our design In this study we perform the following set of experiments 1 Evaluation with the TeraSort benchmark and 2 Evaluation with the Sort benchmark These benchmarks are described in section II For each of the benchmarks we have identiìed the optimal values of HDFS block-size for different interconnects as well as for Hadoop-A and our design Additionally in our experimental setup we have also determined that four is the maximum number of map and reduce tasks that can be run simultaneously to achieve the optimal performance by a TaskTracker In all our experiments we have used Hadoop 0.20.2 Hadoop-A and JDK 1.7.0 
002\003\004 005\006\007\010\010\011\012 002\012\013\014\012 015\012\016\007\017\012 020\021\004\011\022\017\022\023\024\025\003\013\013\022\012\013 002\003\004 005\006\007\010\010\011\012 002\012\013\014\012 015\012\016\007\017\012 020\021\004\011\022\017\022\023\024\025\003\013\013\022\012\013 
A Experimental Setup B Evaluation with the TeraSort Benchmark 
1914 


      
0 500 1,000 1,500 2,000 2,500 40 30 20 Job Executiion Time \(sec Sort Size \(GB 10GigEä1disk 10GigEä2disks IPoIBä1disk \(32Gbps IPoIBä2disks \(32Gbps HadoopAäIBä1disk \(32Gbps HadoopAäIBä2disks \(32Gbps OSUäIBä1disk \(32Gbps OSUäIBä2disks \(32Gbps 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 100 80 60 Job Executiion Time \(sec Sort Size \(GB 1GigEä1disk 1GigEä2disks IPoIBä1disk \(32Gbps IPoIBä2disks \(32Gbps HadoopAäIBä1disk \(32Gbps HadoopAäIBä2disks \(32Gbps OSUäIBä1disk \(32Gbps OSUäIBä2disks \(32Gbps 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 200GBä24nodes 100GBä12nodes Job Executiion Time \(sec Sort Size 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 50 100 150 200 250 300 350 400 450 500 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 
C Evaluation with the Sort Benchmark 
a Total job execution times in 4 nodes cluster          b Total job execution times in 8 nodes cluster Figure 4 TeraSort benchmark evaluation HDD From Figure 4\(b we observe that with two HDDs per node our design achieves an improvement of 31 over Hadoop-A 32 Gbps          Figure 5 TeraSort benchmark evaluation with larger sort size We have also evaluated TeraSort with larger sized cluster In this case we have varied the sort size from 100 GB to 200 GB with 12 and 24 compute nodes in the cluster respectively Figure 5 shows these results For 100 GB sort size we achieve 41 beneìt over IPoIB 32Gbps and 7 beneìt over Hadoop-A 32Gbps For 200 GB sort size also we achieve similar beneìts From Figure 4 and Figure 5 we observe that Hadoop-A performs better with respect to IPoIB in a bigger cluster with the same sort size whereas our implementation achieves better performance in both cases compared to Hadoop-A As in our setup storage nodes have twice as much memory as compute nodes our implementation has more beneìts in storage nodes compared to those in compute nodes This clearly depicts the efìciency of the caching mechanism implemented in our design Figure 7 Sort benchmark evaluation with SSD Figure 6\(a shows the performance results of the Sort benchmark using four DataNodes For this we have used four compute nodes in our cluster Each DataNode has single HDD per node In this case our design reduces the job execution time by 26 over IPoIB 32 Gbps and 38 over Hadoop-A for 20 GB sort From Figure 6\(b we observe that with eight DataNodes our design can achieve an improvement of 27 over IPoIB 32 Gbps and 32 over Hadoop-A Compared with the TeraSort benchmark the difference in the Sort benchmark is the variable size of the key-value pairs In Sort the combined length of key-value pairs can be as large as 20,000 bytes From the results we can observe that Hadoop-A performs worse than IPoIB even after conìguring all the tunable parameters with optimum values as mentioned in Hadoop-A release It re v eals that only substituting the socket based communication with the native IB verbs some applications such as the Sort benchmark cannot get better performance due to the inefìciency in number of keyvalue pairs transferred each time that also affects proper overlapping between all the stages Our design with the efìcient caching mechanism can get better performance in 
We perform the regular Sort benchmark in 1 GigE IPoIB 32 Gbps with vanilla Hadoop and Hadoop-A and compare the results with our design For this experiment the optimal value of HDFS block-size is 64 MB We have used RandomWriter to generate the input data for the Sort benchmark            
1915 


        
0 200 400 600 800 1,000 1,200 1,400 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 200 400 600 800 1,000 1,200 1,400 40 35 30 25 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 100 200 300 400 500 600 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB IPoIB OSUäIB \(Without Caching Enabled OSUäIB \(With Caching Enabled 
D Beneìts of Caching 
a Total job execution times in 4 nodes cluster         b Total job execution times in 8 nodes cluster Figure 6 Sort benchmark evaluation both these cases as it considers the size of the key-value pair before the transfer We also evaluate Sort benchmark using SSD as HDFS data stores Figure 7 shows the comparison for this evaluation In this case our design achieves a beneìt of 22 over Hadoop-A 32Gbps and 46 over IPoIB 32Gbps in job execution time for 15 GB sort Figure 8 Effect of Caching Mechanism Figure 8 shows the performance comparison between caching enabled and caching disabled in our RDMA-based design for Sort benchmark We perform this experiment using SSD as HDFS data store In this case enabling caching with our design can enhance performance by 18.39 over caching disabled in the same design for 20 GB sort size It reveals that for big workload as in sort an efìcient caching mechanism can signiìcantly improve the performance for an RDMA-based design using a high performance interconnect V R ELATED W ORK Many studies have paid attention to improve the performance of MapReduce in recent years As mentioned before the most analogous one is Hadoop-A which pro vides a new merge method to Hadoop MapReduce framework by utilizing RDMA over InìniBand Our work has some major enhancements and differences with respect to their work along pre-fetching caching codebase modiìcation etc We have discussed these detail in section III-C In the authors ha v e proposed techniques of prefetching and pre-shufîing into MapReduce From their results the techniques can improve the overall performance of Hadoop We have focused on implementing an efìcient key-value pairs pre-fetching and caching mechanism inside TaskTracker which is responsible for fast data shufîe when the mappers are yet to be completed Such a design can help reduce the overhead in the reducer side when the shufîe and merge procedures run in an overlapped manner The research has demonstrated that there is an impressi v e space for performance improvement in Hadoop MapReduce compared with traditional HPC technologies such as MPI Our earlier work 7 re v ealed that SSD can reduce the I/O cost and make the overheads involved in datatransmission over the network prominent In this paper we have conducted experiments with multiple HDDs and SSDs per node to lessen the I/O bottleneck when studying the effect of communication over different interconnects VI C ONCLUSION In this paper we have presented an RDMA-based design of MapReduce over InìniBand We have also proposed efìcient pre-fetching and caching mechanisms for retrieving of the intermediate data Our performance evaluation shows that we achieve 21 beneìt in terms of execution time over Hadoop-A for 100 GB TeraSort For regular Sort benchmark our design outperforms Hadoop-A by 32 for 40 GB sort We also observe an improvement of 22 over Hadoop-A for the same sort size using SSD In future we plan to extend 
In our design we have implemented efìcient map output pre-fetching and caching mechanism We have also provided a conìguration parameter to enable/disable the caching In this experiment we have evaluated the performance improvement that we can get through caching enabled        
1916 


our design to handle faster recovery in case of task failures We will also evaluate our design on larger clusters with a range of applications R EFERENCES  Y  W ang X Que W  Y u D Goldenber g and D Sehgal Hadoop Acceleration through Network Levitated Merge in  ser SC 11 2011  Apache Hadoop http://hadoop.apache.or g  J Dean and S Ghema w at MapReduce Simpliìed Data Processing on Large Clusters in  2004  K Shv achk o H K uang S Radia and R Chansler  The Hadoop Distributed File System in  2010  J Appa v oo A W aterland D Da Silv a V  Uhlig B Rosenburg E Van Hensbergen J Stoess R Wisniewski and U Steinberg Providing A Cloud Network Infrastructure on A Supercomputer in  ser HPDC 10 New York NY USA ACM 2010 pp 385Ö394  Greenplum Analytics W orkbench http://www.greenplum.com/news/greenplum-analyticsworkbench  S Sur  H W ang J Huang X Ouyang and D K Panda Can High Performance Interconnects Beneìt Hadoop Distributed File System in  Atlanta GA 2010  J Jose H Subramoni M Luo M Zhang J Huang M W Rahman N S Islam X Ouyang H Wang S Sur and D K Panda Memcached Design on High Performance RDMA Capable Interconnects in  Sept 2011  J Huang X Ouyang J Jose M W  Rahman H W ang M Luo H Subramoni C Murthy and D K Panda High-Performance Design of HBase with RDMA over InìniBand in   N S Islam M W  Rahman J Jose R Rajachandrasekar H Wang H Subramoni C Murthy and D K Panda High Performance RDMA-based Design of HDFS over InìniBand in  November 2012  J Jose M Luo S Sur  and D K P a nda Unifying UPC and MPI Runtimes Experience with MVAPICH in  Oct 2010  Hadoop Map Reduce The Apache Hadoop Project  http://hadoop.apache.org/mapreduce  Sort http://wiki.apache.or g/hadoop/Sort  RandomWriter http://wiki.apache.or g/hadoop RandomWriter  Inìniband T rade Association http://www inìnibandta org  OpenF abrics Alliance http://www openf abrics.or g  P  Balaji H V  Shah and D K P anda Sockets vs RDMA Interface over 10-Gigabit Networks An In-depth analysis of the Memory Trafìc Bottleneck in  2004  RDMA Consortium  Architectural Speciìcations for RDMA over TCP/IP http://www.rdmaconsortium.org  B Fitzpatrick Distrib uted Caching with Memcached   vol 2004 pp 5 August 2004  A v ailable http://portal.acm.or g/citation.cfm id=1012889.1012894  Apache HBase The Apache Hadoop Project  http://hbase.apache.org  MV APICH2 MPI o v er InìniBand 10GigE/iW ARP and RoCE http://mvapich.cse.ohio-state.edu  Mellanox T echnologies Unstructured Data Accelerator http://www.mellanox.com/page/products dyn product family=144  S Seo I Jang K W oo I Kim J.-S Kim and S Maeng HPMR Prefetching and Pre-shufîing in Shared MapReduce Computation Environment in  Sep 2009 pp 1Ö8  X Lu B W ang L Zha and Z Xu Can MPI Beneìt Hadoop and MapReduce Applications in  2011 
Proceedings of 2011 International Conference for High Performance Computing Networking Storage and Analysis Operating Systems Design and Implementation OSDI IEEE 26th Symposium on Mass Storage Systems and Technologies MSST Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing Workshop on Micro Architectural Support for Virtualization Data Center Computing and Clouds in Conjunction with MICRO 2010 International Conference on Parallel Processing ICPP IEEE International Parallel and Distributed Processing Symposium IPDPSê12 The International Conference for High Performance Computing Networking Storage and Analysis SC Fourth Conference on Partitioned Global Address Space Programming Model PGAS Workshop on Remote Direct Memory Access RDMA Applications Implementations and Technologies RAIT in conjunction with IEEE Cluster Linux Journal Cluster Computing and Workshops 2009 CLUSTER 09 IEEE International Conference on IEEE 40th International Conference on Parallel Processing Workshops ICPPW 
1917 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





