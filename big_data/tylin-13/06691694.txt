Optimizing a MapReduce Module of Preprocessing High-Throughput DNA Sequencing Data   Wei-Chun Chung 1,3 Yu-Jung Chang 2 Chien-Chih Chen 2 Der-Tsai Lee 2,3,4 Jan-Ming Ho 1,2  1 Research Center for Information Technology Innovation 2 Institute of Information Science Academia Sinica Taipei, Taiwan, ROC E-mail: {wcchung, yjchang, rocky, dtlee, hoho}@iis.sinica.edu.tw   Abstract The MapReduce framework has become the de facto  choice for big data analysis in a variety of applications. In MapReduce programming model, computation is distributed to a cluster of computing nodes that runs in parallel. The performance of a MapReduce application is thus affected by system and middleware, characteristics of data, and design and implementation of the algorithms. In this study, we focus on performance optimization of a MapReduce application, i.e CloudRS, which tackles on the problem of detecting and removing errors in the next-generation sequencing de novo  genomic data. We present three strategies, i.e., contentexchange, content-grouping, and index-only strategies, of communication between the Map\(\nd Reduce\(\ctions The three strategies differ in the way messages are exchanged between the two functions. We also present experimental results to compare performance of the three strategies Keywords-error correction; genome assembly; mapreduce next-generation sequencing; optimization I  I NTRODUCTION  MapReduce [1  is  a p r om in en t dist ri bu te d  c o m p u t at ion a l  framework that possesses various key features for dealing with large-scale data processing on the cloud [2-4 in c l u d in g  fault-tolerance, scheduling, data replication, load balance and parallelization. By virtue of the scalability and simplicity on development, MapReduce and its implementations [5-7  have been widely-used in different applications, e.g., Web and social networks analysis, scientific emulation, financial and business data processing, and bioinformatics [8-1   However, the performance and efficiency of MapReduce are affected by different factors, and thus, become challenging for optimization Optimizing MapReduce is essential as processing data in a timely and cost-efficient manner becomes critical [13-18   Fortunately, various techniques have been introduced to improve the performance of MapReduce [19-25   in clu d i n g  hardware, software, and framework level optimization. One of the optimization techniques is tuning parameters for system, middleware, and MapReduce execution by utilizing expert systems [20-22  or th e r u leo f t h u m b po lic ies   2 6 2 7    Another type of optimization focuses on the design of algorithm or the characteristics of data of the application [28 29   In this study, we focus on CloudRS [9  a M a pR e d u c e  application for correcting errors in the next-generation sequencing \(NGS\ data. As the cost of DNA sequencing rapidly reduces [1 h e a cco m p a n y i ng gr o w t h o f ge no m e  data results in unpredictable execution time, even if the data is processed by MapReduce. Thus, to optimize the performance of CloudRS, we evaluate three kinds of message generation and transmission approaches to reduce the communication cost of MapReduce: content-exchange content-grouping, and index-only strategies. We also present the experimental results, and discuss the observation and limitation of our proposed strategies II  B ACKGROUND  A  The MapReduce programming model The MapReduce programming model is composed of two primitive functions Map and Reduce The input data of a MapReduce program is a list of key  value pairs, and thus, the Map function is applied to each pair and generate a set of intermediate pairs, e.g key  list\(value Then the Reduce function is applied to each intermediate pair process values of the list, and produce aggregated final results. Moreover, there are additional functions in the MapReduce execution model, e.g shuffle and sort to handle intermediate data. The shuffle function is applied on the Map  side, and performs data exchange by key after Map Thus data with the same key will be transmitted to a single Reduce function. The sort function is launched on the Reduce side after data exchange. It sorts data by the key field to group all the pairs with the same key for further processing B  The CloudRS algorithm The CloudRS algorithm [9  im p l em en ted w ith  m u ltip le  MapReduce rounds. It aims at conservatively correcting sequence errors to avoid yielding false decisions, and thus improves the quality of de novo assembly. To correct a possible mismatch, CloudRS emulates read alignment and majority voting for each set of reads, denoted as a read stack    These authors contributed equally to this work \(co-First authors 3 Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan, ROC 4 Department of Computer Science and Information Engineering, National Chung Hsing University, Taichung, Taiwan, ROC  Corresponding author 2013 IEEE International Conference on Big Data 1 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


with the same k-mer subsequence. Note that a k-mer  subsequence refers to a genomic subsequence of k base pairs of either guanine-cytosine or adenine-thymine. Once the reads are aligned and the read stack has been built, majority voting can be applied on each position of the read stack to summarize the quality value of each base. Then, a decision is made on each position to correct error if necessary III  M ETHODS  The basic idea of error detection and correction is to align reads having the same specific subsequence of length k and sort them according to the relative position of the subsequence in the read. A voting algorithm is then used to examine the symbols and quality values at each position of the stack of reads to detect and correct sequencing errors if the reads and their quality values show high level of consistency at each position. Interested readers may refer to 9 n d [30 f o r de t a i l s I n  t h i s sec t i o n, w e ar e g o i ng t o  present three strategies, i.e., content-exchange strategy content-grouping strategy, and index-only strategy, to collect reads with the same specific subsequence of length k in the error correction algorithm based on the MapReduce framework. Note that each strategy consists of a pair of Map and Reduce functions. The Map function scans through a read for the k-mer subsequences it contains and emits the k-mer  read pairs. The shuffle stage of MapReduce then aggregates reads with the same k-mer  subsequence for further processing by a Reduce function later. The Reduce function thus performs the align/sort/voting algorithm to identify and recommend sequencing errors containing in the reads it receives. Details of these strategies are presented as follows A  Content-exchange strategy For each read r of length l the Map function of the content-exchange strategy generates a message of the form  k-mer i  identifier, sequence, quality value at each position i of r where 1  i   l k 1 sequence and quality value are vectors of length l representing the DNA sequence and quality value of r given by the sequencer, and k-mer i is the subsequence of r of length k starting at position i  B  Content-grouping strategy In this subsection, we present a content-grouping strategy in which the Map function groups messages destined for the same Reduce function and thus reduces the total size of messages transmitted during the shuffle stage. That is, the key-value pair is defined as group key  list\(identity key identifier, sequence, quality value In other words, we divide the original key into two parts, the group key and the identity key Messages with the same group key are sent to the same instance of Reduce function which first sorts the reads it receives according to their identity keys then performs align/sort/voting algorithm to detect and recommend sequencing errors in the subset of reads with the same identity key  C  Index-only strategy In the third strategy, we aim at reducing communication overhead further by using the distributed cache mechanism of Hadoop. The input data file containing sequence data and quality value of each read is replicated to each computing node of the cluster before executing the Map function. So that the Map function does not have to duplicate read data including sequence data and quality value for each message generated with each k-mer subsequence. Instead, it generates messages in key-value pairs containing only the k-mer  subsequence and the read identifier which will be used later by the Reduce function to retrieve read data from its local file cache. Each message generated by the Map\(\tion is formatted as group key  list\(identity key\, identifier  Thus, the communication cost in terms of total message size is reduced at the cost of I/O overhead of retrieving read data from the local file cache D  Qualitative comparison of the three strategies To evaluate the effectiveness of proposed methods, we estimate the intermediate data size and its reducing rate by calculation. The input dataset consists of a set of reads; each read is composed of read id DNA sequence, and a qualityvalue character for each DNA. For a dataset with r reads and each readís sequence composed of l characters, the quality values of a read is also length l Let the size of a sliding window on readsí sequences be k there will be r*\(l-k+1  kmer substrings, abbreviated as k-mers in the following, to be processed \(denoted as t as the input of mappers We also define the grouping rate of the k-mers denoted as  n aluate the performance of grouping mechanism Hence, we can approximately estimate the amount of key groups and the number of k-mers in each group. For convenience, we give related notations as that \(a\he length of read id is a fixed size i b\he length of group key is p  and identity key is k-p where 1  p   k see content-grouping strategy in Methods for definitions\, \(c\the grouping rate is a normal distribution, thus  0   n  1, and there are n*t  k-mer  groups and 1/n  k-mers in each group The estimated sizes of intermediate data for the three strategies are as follows. For the content-exchanging strategy, the intermediate data size is at least t*\(k+i+2l  bytes, since a key-value pair of a k-mer passing to reducers has to carry itself and the id, sequence and quality values of its read with size k+i+2l The content-grouping strategy produce nt\*\(p+1/n*\(k-p\+i+2l bytes intermediate data because a message contains a grouping key, a list of identity keys, the id, sequence and quality values of its read. The size of identity key list is 1/n*\(k-p since there are 1  n  k-mers in each group and the length of each identity key is k-p The index-only strategy generates nt\*\(p+1/n*\(k-p\+i bytes since the key-value pair contains only the id. Table I summarizes the intermediate data size and the complexity of storage space of our proposed strategies 2 


IV  E XPERIMENTAL R ESULTS  A  Environment setup and datasets Our experiments are evaluated in a Hadoop cluster with 10 dedicated computing nodes and an isolated internal network. Each node has 2 quad-core Intel Xeon E5410 CPUs, 16 GB memory, 1 TB local storage, and 1 Gb network connection. We use Ubuntu Linux 8.04 and Hadoop version 0.20.203 for our experimental environment. We also set up at most 7 map tasks and 7 reduce tasks execute concurrently for each node. Thus, there are at most 70 map tasks and 70 reduce tasks in a MapReduce wave. The detail configuration of job parameters lists in Appendix Table A1 In addition, to separate the control flow and computation flow of the Hadoop framework, we add an additional control node into our cluster. We also define roles for the 11 nodes one control node roles as Name Node and Job Tracker, while 10 computing nodes act as Data Nodes and Task Trackers We use three real datasets to evaluate the performance of CloudRS. Information of datasets is listed in Table II. The dataset D1 is a set of short read data from an Escherichia coli   E. coli library \(SRX000429\ which consists of 20.8M 36bp reads. The dataset D2 is released by Illumina, which includes 12M paired-end 150-bp reads. This dataset contains sequences from a well-characterized E. coli strain K-12 MG1655 library sequenced on the Illumina MiSeq platform The dataset D3 is Illumina reads from an African Male  NA18507 Note that we set up the size of k-mer as 24 characters in our experiments. Parameter settings of evaluations are bundled within the physical computation limitations, i.e., 8 cores and 16 GB memory of each computing node B  Evaluation Results We use dataset D1 to demonstrate the effect of parameters affect to a MapReduce program by evaluating the content-exchange strategy. As shown in Table III, the execution time is reduced near 23%, comparing to the first and the third row. We observed that the execution time in the second row is longer than the first row. We also observed that multiple mapper/reducer waves also increases total execution time, as shown in the last 3 rows. The parameter settings of 70 mappers, 70 reducers and 950 MB memory achieves the shortest execution time in our experiment. Thus we use the setting and the parameters list in Appendix Table A1 for the rest of the evaluations To demonstrate the efficiency of the content-grouping strategy, we evaluate the strategy with dataset D2 and various partitions of keys. As shown in Table IV, the intermediate data size and execution time decrease with the grouping mechanism. We also evaluate the performance comparing to the content-exchange strategy by setting up the key partition as 24:0. However, we encounter an error during execution since we set the key partition is 6:18 and below For index-only strategy, we use dataset D2 and D3, and use 12:12 as the key partition. The result lists in Table V. In dataset D2, the execution time has a reduction about 37 with index-only strategy, comparing to the content-grouping one. However, we encounter an unexpected longer execution with dataset D3 V  D ISCUSSIONS  A  A brief summary on the three strategies The three versions of error correction algorithms basically consider a read as an object and consider each kmer subsequence of the read as a feature of the read. In the content-exchange strategy, the Map functions generate a message, for each feature of each object, containing the feature as well as the object. The shuffle stage then collects objects with the same feature for further processing by an instance of Reduce function. The content-grouping strategy defines features with the same prefix as belonging to the same feature group. The Map functions generates a message for each feature group of each object. The shuffle  stage thus collects objects belonging to the same feature group to an instance of Reduce function for further processing. Note that total size of messages generated by the content-grouping strategy is smaller than that generated by the content-exchange strategy. However, the Reduce  function of the content-grouping strategy may suffer an exception of JAVA due to insufficient amount of physical memory, and thus, terminate the execution. The index-only strategy incorporates the grouping mechanism, and thus, to reduce the message size. The index-only strategy is thus the least time-consuming among the three. Unfortunately though the strategy works well with small dataset, it failed when the input data is large B  Overhead of index-only strategy The index-only strategy utilizes the grouping mechanism and distributed cache to successfully reduce the size of data transmitted by the Map function, and thus, also reduce communication cost in the shuffle stage. However, since the data, i.e., sequence and quality value, is read from the local cache, performance bottleneck shifts to disk I/O among the Reduce function. The overhead increases rapidly as the size of input data becomes large. This is mainly due to the fact that reads with the same key, the k-mer usually scatters in the local cache. Furthermore, there are multiple tasks running concurrently on a single computing node. The lack of cache hit results in a high page-fault rate, especially when physical memory is exhausted by the running tasks. This phenomenon is known as thrashing. When it occurs, the execution time of the application may run indefinitely TABLE I  A PPROXIMATED INTERMEDIATE DATA SIZE PRODUCED BY PROPOSED STRATEGIES  Proposed strategies Approximated intermediate data size \(bytes Complexity of storage spaces Content-exchange t*\(k+i+2l  O\(rl 2  Content-grouping nt\*\(p+1/n*\(k-p\+i+2l  O\(nrl 2   Index-only nt\*\(p+1/n*\(k-p\+i\ O\(krl 3 


VI  C ONCLUSION  In this era of big data, it is critical to process a large amount of data timely and efficiently. MapReduce is one of the prominent solutions to this end. It provides scalability and fault-tolerance for big data applications. However, the share-nothing nature of MapReduce also elicits researches that study applications with high degree of data dependency An error detection and correction algorithm based on processing reads with the same k-mer subsequence is an application with high degree of data dependency, especially when it is applied to a large genome In this paper, we present three strategies to handle data communication between the Map and Reduce functions of the MapReduce framework in a bioinformatics application that detects and corrects sequencing errors in the NGS data Note that the NGS data consists of fixed-length reads, each being associated with sequence and quality value. The first strategy replicates the read data of each k-mer subsequence and transmits the entire set of data from Map to Reduce  The second strategy groups the k-mer subsequences of a read by their prefix, and thus, transmits fewer amounts of data through the network. The third strategy, i.e., index-only strategy, pre-caches the read data directly on each node, and transmits only the indices of reads as messages. The indexonly strategy has been shown to be most efficient for small genomes. However, for large genomes, our current implementation may suffer the thrashing problem Our future research will focus on improving the performance of the index-only strategy. We will also look into other problems with similar nature, e.g de novo  assembly, and develop applications based on the MapReduce framework A CKNOWLEDGMENT  The authors wish to thank anonymous reviewers for their helpful suggestions, and thank Dr. Wen-Liang Hsu and Dr Chung-Yen Lin for their valuable discussions and comments They also wish to thank Chunghwa Telecom Co. and National Communication Project of Taiwan for providing the cloud computing resources. The research is partially supported by Digital Culture Center, Academia Sinica, and National Science Council under grant NSC 102-2221-E-001013-MY3  TABLE II  L IST OF DATASETS OF EVALUATING OUR PORPOSED STRATEGIES  Dataset SRA accession number Reference genome NCBI reference sequence accession number Genome length \(MB Read length Number of reads \(M Genome coverage Data size GB D1 SRX000429 E. coli NC_000913 ~4.64 36 bp ~20.8 161x ~1.59 D2 E. coli NC_000913 ~4.64 150 bp ~12.1 388x ~3.50 D3 SRA000271 African Male NA18507 3000 36 bp ~218.0 2.6x ~17.3  TABLE III  R ESULTS OF DIFFERENT PARAMETER SETTINGS WITH CONTENT EXCHANGE STRATEGY O N DATASET D1 Parameters \(mapred Run time \(s map.tasks reduce.tasks child.java.opts 40 40 -Xmx4000m 2,520 40 40 -Xmx950m 2,558 70 70 -Xmx950m 1,946 140 140 -Xmx950m 1,967 70 140 -Xmx950m 1,968 140 70 -Xmx950m 1,991  TABLE IV  R ESULTS OF CONTENT GROUPING STRATEGY ON DATASET D2 Partition of keys group:identity Intermediate data size \(bytes b  Run time \(s 24:0 a 393,610,577,668 22,738 20:4 393,596,531,558 22,454 12:12 393,439,082,022 21,949 8:16 391,616,362,031 21,396 6:18 379,682,068,879 GC overhead exceeded 3:21 160,805,782,820 GC overhead exceeded a Content-grouping method with key partition of 24:0 is same as content-exchange method b The input data size is 3,062,609,572 bytes  TABLE V  R ESULTS OF I NDEX ONLY M ETHOD O N DATASET D2 AND D3 Dataset Method a  Run time \(s D2 Content-grouping 21,728 Index-only 13,691 D3 Content-grouping 16,345 Index-only 8 hr a Both methods use 12:12 as the partition of keys \(group:identity 4 


R EFERENCES  1  J. Dean and S. Ghemawat, "MapReduce: simplified data processing on large clusters Commun. ACM vol. 51, pp. 107-113, 2008 2  Amazon Amazon Elastic Compute Cloud \(Amazon EC2 Available http://aws.amazon.com/ec2  3  Amazon Amazon Simple Storage Service \(Amazon S3 Available http://aws.amazon.com/s3  4  Amazon Amazon Elastic MapReduce \(Amazon EMR Available http://aws.amazon.com/elasticmapreduce  5  Apache Welcome to Apache  Hadoop Available http://hadoop.apache.org  6  Nokia Disco MapReduce Available http://discoproject.org  7  M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, "Dryad distributed data-parallel programs from sequential building blocks presented at the Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, Lisbon, Portugal 2007 8  Y.-J. Chang, C.-C. Chen, C.-L. Chen, and J.-M. Ho, "A de novo next generation genomic sequence assembler based on string graph and MapReduce cloud computing framework BMC Genomics vol. 13 pp. 1-17, 2012 9  C.-C. Chen, Y.-J. Chang, W.-C. Chung, D.-T. Lee, and J.-M. Ho CloudRS: An Error Correction Algorithm of High-Throughput Sequencing Data based on Scalable Framework," in IEEE International Conference on Big Data In press   B. Langmead, K. D. Hansen, and J. T. Leek, "Cloud-scale RNAsequencing differential expression analysis with Myrna Genome Biol vol. 11, p. R83, 2010   M. C. Schatz, "CloudBurst: highly sensitive read mapping with MapReduce Bioinformatics vol. 25, pp. 1363-9, Jun 1 2009   L. D. Stein, "The case for cloud computing in genome informatics Genome Biol vol. 11, p. 207, 2010   E. Anderson and J. Tucek, "Efficiency matters SIGOPS Oper. Syst Rev vol. 44, pp. 40-45, 2010   J. Cohen, B. Dolan, M. Dunlap, J. M. Hellerstein, and C. Welton MAD skills: new analysis practices for big data Proc. VLDB Endow vol. 2, pp. 1481-1492, 2009   D. J. DeWitt and M. Stonebraker. \(2008 MapReduce: A major step backwards Available http://databasecolumn.vertica.com/databaseinnovation/mapreduce-a-major-step-backwards    B. Irving Big data and the power of hadoop    A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt, S. Madden  et al A comparison of approaches to large-scale data analysis presented at the Proceedings of the 2009 ACM SIGMOD International Conference on Management of data, Providence, Rhode Island, USA, 2009   M. Schroepfer Inside large-scale analytics at facebook    J. Ekanayake, H. Li, B. Zhang, T. Gunarathne, S.-H. Bae, J. Qiu et al Twister: a runtime for iterative MapReduce," presented at the Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing, Chicago, Illinois, 2010   W. Guanying, A. R. Butt, P. Pandey, and K. Gupta, "A simulation approach to evaluating design decisions in MapReduce setups," in Modeling, Analysis & Simulation of Computer and Telecommunication Systems, 2009. MASCOTS '09. IEEE International Symposium on 2009, pp. 1-11   H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, F. B. Cetin et al Starfish: A Self-tuning System for Big Data Analytics In Proceedings of the 5th Conference on Innovative Data Systems Research 2011   E. Jahani, M. J. Cafarella, C. R, and #233, "Automatic optimization for MapReduce programs Proc. VLDB Endow vol. 4, pp. 385-396 2011   S. Lattanzi, B. Moseley, S. Suri, and S. Vassilvitskii, "Filtering: a method for solving graph problems in MapReduce," presented at the Proceedings of the 23rd ACM symposium on Parallelism in algorithms and architectures, San Jose, California, USA, 2011   K.-H. Lee, Y.-J. Lee, H. Choi, Y. D. Chung, and B. Moon, "Parallel data processing with MapReduce: a survey SIGMOD Rec vol. 40 pp. 11-20, 2012   C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C Kozyrakis, "Evaluating MapReduce for Multi-core and Multiprocessor Systems," presented at the Proceedings of the 2007 IEEE 13th International Symposium on High Performance Computer Architecture, 2007   Cloudera Optimizing MapReduce Job Performance Available http://www.slideshare.net/cloudera/mr-perf    T. White Hadoop: The Definitive Guide O'Reilly Media, 2009   J. Lin and C. Dyer Data-Intensive Text Processing with MapReduce  Morgan and Claypool Publishers, 2010   J. D. Ullman, "Designing good MapReduce algorithms XRDS vol 19, pp. 30-34, 2012   S. Gnerre, I. MacCallum, D. Przybylski, F. J. Ribeiro, J. N. Burton B. J. Walker et al High-quality draft assemblies of mammalian genomes from massively parallel sequence data Proceedings of the National Academy of Sciences 2010  A PPENDIX  A  Rules-of-thumb policy for configurations Table A1 lists our Hadoop configuration parameters depends on the rules-of-thumb policy that aims at ensuring the values are not exceed the physical limitation of each computing node. Assume that we have 10 computing nodes and each node has 8 CPU cores, 16 GB memory, and acts as Data Node and Task Tracker. We demonstrate the calculation of the first three parameter values of Hadoop framework in Table A1. To achieve the best-effort of CPU utilization, there would be assign 2 processes to utilize for each CPU core, in general. Thus, there are 16 processes execute simultaneously in a node. However, to obtain the functionality of underlying operating system, we prepare one CPU core for system routine process and I/O operations Thus, there are at most 7 CPU cores for Hadoop framework and we decide to set up at most 7 map tasks and 7 reduce tasks concurrently. Since in-memory processing is faster than performing operation with swap space or content switching we optimism the memory usage of each node is within its physical boundary. Furthermore, we preserved around 500 MB for processes of operating system, 1 GB for operations of Data Node, and 1 GB for Task Tracker. To utilize the rest 13 GB memory with at most 14 tasks concurrently, we can assign 950 MB memory for each task 5 


 TABLE  A1  A SUBSET OF JOB CONFIGURATION PARAMETERS OF H ADOOP THAT AFFECT JOB PERFORMANCE SIGNIFICANTLY  Parameter Name in Hadoop Description and Use Default Values Our Settings mapred.child.java.opts Java options for the task tracker child processes Xmx200m Xmx950m mapred.tasktracker.map.tasks.maximum Maximum number of map tasks run simultaneously by a task tracker 2 7 mapred.tasktracker.reduce.tasks.maximum Maximum number of map tasks run simultaneously by a task tracker 2 7 mapred.reduce.slowstart.completed.maps Fraction of the completed map tasks to start reduce tasks in a job 0.05 0.4 mapred.reduce.parallel.copies Number of parallel transfers 5 15 io.sort.mb Map-side buffer size \(in MegaBytes\ for buffering and sorting key-value pairs 100 384 io.sort.record.percent Fraction of io.sort.mb to store metadata of key-value pairs 0.05 0.15 io.sort.factor Number of sorted streams to merge at once when sorting files 10 38 mapred.map.tasks Default value of map tasks per job 2 10 mapred.reduce.tasks Default value of reduce tasks per job 1 10  6 


of the clusters revealed that PSCAN identiìes two users of a cluster although there is no direct follower/following relation exists but sharing some common interests Such capability of PSCAN helped us to nd users from a city an organization or a country It is not feasible to discuss all the clusters here one of the interesting clusters is a cluster representing twitter pages of BBC weather channel and weather alerts The cluster of 20 members all representing BBC weather related pages is found There are many such clusters that represents a group of users who share some common interests There is no base line of communities in Twitter to measure accuracy of the clustering but our manual observations found the accuracy is signiìcant Moreover the experiment is designed to prove the feasibility of SCAN in MapReduce framework because SCAN is proved to be accurate enough for clustering The experiment on Twitter data proved the accuracy and scalability of PSCAN V C ONCLUSIONS AND F UTURE W ORK We present a parallel structural clustering algorithm PSCAN for big networks in MapReduce in this paper PSCAN identiìes clusters as well as vertices playing critical roles such as outliers and hubs in big networks with billions of edges in three steps namely calculating structural similarity of edges cutting off edges with low structural similarity and nding connected components All the steps can be executed in parallel in MapReduce The time complexity of PSCAN is linear with the number of edges in the graph Our empirical evaluation demonstrated an accurate clustering result and an excellent running time in terms of scaleup sizeup and speedup Moreover we applied PSCAN for analysis of a Twitter social network with over 40 million users and 1.4 billions of follower/following relationships The result shows that PSCAN can nd interesting communities of people sharing common interests or other features In the future we plan to further investigate the performance of PSCAN by applying it for the analysis of some really big networks in real world A CKNOWLEDGMENT This project was funded by Acxiom Corporation The authors are grateful for invaluable collaboration with Kevin Liles and Derek Leonard throughout the project This work was supported in part by the National Science Foundation under Grant CRI CNS-0855248 Grant EPS-0701890 Grant EPS-0918970 Grant MRI CNS-0619069 and OISE0729792 Weizhong Zhao would like to thank the support of the National Natural Science Foundation of China No 61105052 R EFERENCES  A Lancichinetti S F ortunato and F  Radicchi 
 Physical Review E 78 046110 2008  S Y ook H Jeong and A Barabasi  In PNAS Proceedings of the National Academy of Science pages 13382-13386 October 2002  L Hubert and P  Arabie  Journal of Classiìcation 193C 218 1985  A Strehl J Ghosh R Moone y   Proceedings of the workshop on artiìcial intelligence for web search pp 58-64 2000  X.Xu N.Y uruk Z Feng T  Schweiger  Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining pp 824-833 2007  Bin W u Y aHong Du  2010 International Conference on Artiìcial Intelligence and Computational Intelligence AICI vol.3 no pp.122-126 23-24 Oct 2010  Usha Nandini Ragha v an Rka Albert and Soundar K umara  Phys Rev E 76 036106 2007  M E J Ne wman and M Girv an  Phys Rev E 69 026113 2004  Jimmy Lin and Chris Dyer   Morgan and Claypool Publishers 2010 pp 94-101  Santo F ortunato  Physics Reports Volume 486 Issues 35 February 2010 Pages 75-174 ISSN 0370-1573 10.1016/j.physrep.2009.11.002  Jia wei Han Micheline Kamber  and Jian Pei  3rd edition Morgan Kaufmann 2011  B.W  K ernighan S Lin  Bell Syst Tech J 49 1970 291307  J Shi J Malik  IEEE Trans Pattern Anal Mach Intell 22 8 2000 888905  U Brandes D Delling M Gaertler  R G  orke M Hoefer Z Nikolski D Wagner  URL http://digbib ubka.unikarlsruhe.de/volltexte/documents/3255  A Clauset M.E.J Ne wman C Moore  Phys Rev E 70 6 2004 066111  G P alla I Der  enyi I Farkas T Vicsek  Nature 435 2005 814818  Martin Ester  Hans-Peter Krie gel J  org Sander Xiaowei Xu  Proceedings of the Second International Conference on Knowledge Discovery and Data Mining KDD-96 AAAI Press pp 226231 ISBN 1-57735004-9 
Benchmark graphs for testing community detection algorithms Modeling the internetês large scale topology Comparing partitions Impact of similarity measures on web-page clustering SCAN a structural clustering algorithm for networks Cloud-based Connected Component Algorithm Near linear time algorithm to detect community structures in large-scale networks Finding and evaluating community structure in networks Data-Intensive Text Processing with MapReduce Community detection in graphs Data Mining Concepts and Techniques An efìcient heuristic procedure for partitioning graphs Normalized cuts and image segmentation On modularity npcompleteness and beyond Finding community structure in very large networks Uncovering the overlapping community structure of complex networks in nature and society A density-based algorithm for discovering clusters in large spatial databases with noise 
868 


 Akshay U Bhat  http://www.akshaybhat.com/LPMR 2008  Hae w oon Kw ak Changhyun Lee Hosung P ark and Sue Moon  WWW 2010 April 2630 2010 
Scalable Community Detection using Label Propagation  Map-Reduce What is Twitter a Social Network or a News Media 
869 


state of innovation stakeholder  node PQ It  s a balanced node Based on this, we could calculate the  node PQ Calculation process is: set different inn ovation stakeholders state i U  and j U Value of ij 000T can be get from  innovation time difference. Innovation stakeholdersí social effect and industrial effect can be obtained upon ij B  and ij G set according to relation between innovation stakeholders  Model 4.1 points out  that the value of Gij  directly affects social benefits and sector benefits. Large Gij  can lead to increasing benefits of the entire industry and the entire social growth Bij reflects big organizationís impact on businesses. Only strengthening the inter agent association within big organization and enhancing the str ategic partnership between enterprises can jointly promote the development of the entire industry, and bring more social benefits, so that each agent can be improved   5 Summary This paper puts forward the concept of the big organization based on the CSM t heory. It introduces the basic implication of the big organization and theoretical framework of the big organization including: the big organization's perspective  overall perspective, dynamic perspective, and new resource perspective; the big organizat ionís sense  the purpose of the organizational structure is innovation, organizational activities around the flow of information, breaking the traditional organizational structure, encouraging self run structure, and blurring organizational boundaries; the big organizationís platform  the platform ecosystem of the big organization ; the big organizationís operation mode  borderless learning mode, and cluster effect; the big organizationís theory  active management theory  leading consumers, and culture  entropy reduction theory  negative culture entropy and humanistic ecology theory  inspiring humanity, and circuit theory  a virtuous circle, and collaborative innovation theory  collaborative innovation stakeholder. This paper also discusses culture entropy reduction theory of the big organization  negative culture entropy, and coordinated innovation theory  innovation stakeholders collaboration. Culture entropy change model and collaborative in novation model are constructed   The research has just begun for the big organization. It also needs further improvement but remains the trend of the times   Reference  1  Gordon Pellegrinetti, Joseph Bentsman. Nonlinear Control Oriented Boiler Modeling A Benchmark Problem for Controller De sign [J  I E E E tr a n s a c tio n s o n c o n tr o l s y s te m s te c h n o lo g y 2 0 1 0  4 1\57 65  2  Klaus Kruger, Rudiger Franke, Manfred Rode Optimization of boiler start up using a nonlinear 457 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





