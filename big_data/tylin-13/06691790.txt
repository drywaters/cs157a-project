Risk Adjustment of Patient Expenditures A Big Data Analytics Approach Lin Li Saeed Bagheri Helena Goote Asif Hasan Gregg Hazard Philips Research North America Briarcliff Manor US Lin-Li@philips.com Saeed.Bagheri@philips.com Helena.Goote@philips.com Asif.Hasan@philips.com Gregg.Hazard@philips.com Abstract For healthcare applications voluminous patient data contain rich and meaningful insights that can be revealed using advanced machine learning algorithms However the volume and velocity of such high dimensional data requires new big data analytics framework where traditional machine learning tools cannot be applied directly In this paper we introduce our proof-of-concept big data analytics framework for developing risk adjustment model of patient expenditures which uses the divide and conquer strategy to exploit the big-yet-rich data to improve the model accuracy We leverage the distributed computing platform e.g MapReduce to implement advanced machine learning algorithms on our data set In speci“c random forest regression algorithm which is suitable for high dimensional healthcare data is applied to improve the accuracy of our predictive model Our proof-ofconcept framework demonstrates the effectiveness of predictive analytics using random forest algorithm as well as the ef“ciency of the distributed computing platform Keywords Healthcare Big Data Risk Adjustment Distributed Computing Random Forest Patient Expenditure I I NTRODUCTION Risk adjustment model 1 performs a s a n act uari al t ool to predict health costs based on the relative actuarial risks of a patient which is becoming increasingly important due to their value towards a Identi“cation of high-risk future high cost or high utilization individuals for program management b\ormalization of population to evaluate the provider effectiveness and ef“ciency in terms of managing resources among different types of patient c Pricing health plan or predicting future claims cost trends etc There are many risk-adjustment models that have been published 2  w h e r e th e C MS h i er ar ch ical co n d itio n cate g o r ies HCC model has been adopted by CMS for Medicare risk-adjustment because of its transparency simplicity of modeling and clinical coherence 2  C MS  H C C ri s k adjustment model represents the patient diagnosis information with hierarchical condition categories HCC that effectively reduce the number of the diagnostic categories by imposing the hierarchies among diseases and use linear regression algorithm to train the predictive model mainly because of its computational ef“ciency and the robustness to limited data However the nonlinear functional relationship between the patient risk factors and the health costs cannot be suf“ciently captured by linear regression model Health care industry is experiencing the impact of Big Data which brings the opportunity to improve the accuracy of the risk adjustment model The high volume of data can overcome the data noise and bias On the other hand Big data also enable the effectiveness of the advanced machine learning algorithms I n general advanced machine learning algorithms have more parameters to be optimized than simple methods Therefore a limited supply of data may introduce the over-“tting problem and thus reduce its effectiveness which is not an i ssue for big data applications Therefore we aim to leverag e advanced machine learning algorithms and healthcare big data to improve the prediction performance of risk adjustment model For patient big data the new processing environment is required to store transfer and analyze voluminous data which is beyond the ability of conventional data processing tools for example Matlab Therefore a distributed computing platform with the Apache Hadoop infrastructure 3 i s e mpl o yed i n our s t udy t o support our data-intensive application Hadoop derived from MapReduce 4 pro v i d es a d i s t r i b ut ed s cal abl e  a nd port a bl e le system to hold vast amounts of data on a cluster of machines with high aggregate bandwidth across the clusters and uses divide and conquer strategy to run an application in parallel across the cluster against parts of the stored data In addition the large amount of risk factors of the patient expenditure including demographics diagnoses comorbidities etc induces the data high-dimensionality which also leads to the dif“culty of modeling Random Forests  as one type of ensemble machine learning algorithms are particularly suitable to modeling the complex functional relationship in high-dimensional data which can achieve increased performance by generating multiple prediction models each with a different feature subset Therefore we proposes a random-forest-based risk adjustment model running on the MapReduce platform to ght the curse of dimensionality This paper demonstrates the promising results of our prototype framework with respect to both prediction accuracy and c omputation ef“ciency 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 12 


II P AT I E N T D ATA This paper utilized inpatient databases collected from New York State to test the proposed framework of the risk adjustment model for Healthcare big data It contains a core set of clinical and nonclinical information on all patients,which allows estimation of inpatient expenditure based on various patient risk factors including demographics principal and secondary diagnosis and comorbidities A Patient Feature Extraction In this paper we use CMS hierarchical condition categories HCC to extract patient features which is adopted by CMS for Medicare risk adjustment because of its transparency ease of modi“cation and good clinical coherence To be speci“c demographic risk factors included in the model are 24 mutually exclusive age/sex cells For diagnosis information CMS/HCC model rst maps the raw diagnosis data coded with ICD9 code 6 t o C o n d itio n C ate g o r ies CCs and then map CCs to 70 hierarchical condition categories HCC by imposing hierarchies as s h o w n i n Figure 1 In addition multiple co morbidities such as alcohol abuse congestive heart failure depression and diabetes are also incorporated to enrich the p atient representation Overall the risk factors for patient expenditure are represented by 125 dimension features Figure 1 Diagnostic feature extraction using CMS-HCC model III P REDICTIVE M ODEL IN M AP R EDUCE P ARADIGM For predictive model linear regression model is widely used as a standard one for risk adjustment 2  p artly becaus e of its robustness to limited data with high-dimensionality The availability of enormous patient data enables the effective application of the advanced regression algorithms which are better quali“ed to identify the nonlinear complex functional relationship in patient data In this paper Leo Breimans random forests algorithm as one type of advanced regression model is chosen to build the predictive model  because of its suitability for high-dimensional data Given the high-dimensionality of the patient risk factors The random forest methodology use divide and conquer strategy to achieve enhanced predictio n performance by training an ensemble of decision trees each with a different feature subset and using the vote scheme to combine the results Random forest also holds some other desirable advantages for this application such as robustness to noise and outliers suitability for parallelization and lack of dependence upon tuning parameters 5 8  The r andom fores t re gres s i on model can be generally described by three steps  Draw n bootstrap sample from the original patient data  For each of the bootstrap samples grow a regression tree At each node randomly sample m risk factors and choose the best split among those variables  Predict new sample by aggregating the prediction of the n trees where the optimal value of free parameters n and m are determined by the best results of cross-validation dataset after scanning these parameters Apache Hadoop is used to build a distributed computing cluster which contains one master server and three slave servers for the initial prototype framework We use Apache Mahouts random forest implementation 9   1 0  t o t r a in th e r isk a d j u s tm en t model on the cluster which builds enormous number of decision trees in the model in parallel IV R ESULTS In this paper we would like to evaluate the effectiveness of the random forest-based risk adjustment model and the ef“ciency of the distributed computing framework The linear regression model used in Medicare risk adjustment is adopted as the baseline Five-fold cross-validation is implemented on one hospital data 100,000 samples to compare the performance between the linear regression and random forest regression model where R 2 is calculated to measure the prediction accuracy The results show that the random forest  R 2 0  38  0  008 mean standard derivation signi“cantly outperforms linear regression model  R 2  0  31  0  01 mean/standard derivation which indicates the effectiveness of the random forest to identify the complex patterns in high dimensional patient data and thus illustrates its capability of enhancing the risk adjustment model performance The comparison of 2D histogram between linear regression and random forest for test data is shown in Figure 2 where x axis and y axis present the predicted inpatient expenditure  log 10 and the actual/target inpatient expenditure  log 10  respectively The color is labeled by the number of test samples located at  x y   If the inpatient expenditure of a sample is able to be correctly predicted then the sample will lie along the diagonal It is observed that there are more samples located close to diagonal for random forest model than linear regression model In addition we evaluate the enhancement by distributed computing on multiple servers to address the computation 13 


 Target log 10 expenditure Predicted log 10 expenditure    Random Forest  3 4 5 6 3 3.5 4 4.5 5 5.5 6 0 20 40 60 80 100 Target log 10 expenditure Predicted log 10 expenditure    Linear Regression  3 4 5 6 3 3.5 4 4.5 5 5.5 6 Figure 2 Comparison of 2D histograms between linear regression and random forest for test data burden induced by both model complexity and high volume of data All New York inpatient data  3  400  000 samples are used to build the random forest model 200 trees and 10 features per tree Compared to running on a single server we achieve a 2.32 times speedup by using Hadoop infrastructure to parallel the computation on 3 slave servers where computation time on a single server is 299 sec and computation time on 3 slave servers is 121 sec V D ISCUSSION This paper describes our ongoing research that uses divide and conquer strategy to leverage health care big data and advanced machine learning technology to improve the risk adjustment model Our results show that the randomforest-based model signi“cantly outperform the linear regression model employed by CMS which illustrates the effectiveness of the random forest algorithm to identify the complex relationship in high-dimensional patient big data It also indicates its potentials to incorporate more patient risk factors such as disease history insurance income level secondary diagnoses to enrich the risk adjustment model In addition it is evident that our distributed-computing platform successfully supports the data-intensive and computationintensive application The distributed-computing cluster can be expanded to enable learning the model from nationalwide data set billions of samples and thus to exploit the power of big data in risk adjustment and other health care service areas R EFERENCES 1 I  D uncan Healthcare Risk Adjustment and Predictive Modeling  Actex Publications 2011 2 G P o p e  J K a u t t e r R E l l i s  A A s h J A y a n i a n  L  I e z z o n i  M Ingber J Levy and J Robst Risk adjustment of medicare capitation payments using the cms-hcc model Health Care Financing Review  vol 25 pp 119  141 2004 3  O n l i ne  A v ai l a bl e ht t p    h adoop apache o r g  4 J  D ean a nd S  G h ema w at   Mapr educe si mpl i  ed dat a processing on large clusters Commun ACM  vol 51 no 1 pp 107…113 Jan 2008 5 L  B r e i m an  R a ndom f o r e st s  Machine Learning  vol 45 no 1 pp 5…32 2001 6 ICD-9-CM Of“cial Guidelines for Coding and Reporting  7 G P o p e R E l l i s  A A s h  J A y a n i a n  D B a t e s H B u r s t i n  L Iezzoni E Marcantonio and B Wu Diagnostic cost group hierarchical condition category models for medicare risk adjustment Health Economics Research Inc Waltham MA  2000 8 A  L i a w a nd M W i ener   C l assi  cat i o n a nd r e gr essi on by randomforest RNews  vol 2 no 3 pp 18…22 2002 9  O n l i ne  A v ai l a bl e mahout  a pache or g  S  O w e n R  A n i l  T  D unni ng and E  F r i edman Mahout in action  Manning 2011 14 


   Figure 2 Example of a simple product code The blue parity blocks are generated using a horizontal 5,3 MDS code whereas the red block is a simple parity check of the column or a 3,2 code each x i P we obtain p g,i  x i  G g  x i  p g,i   where p g,i   x i   The vector with all the cross-object parity blocks p g  p g 1  p g 5   contains p g  o 11  o 21 o 12  o 22 o 13  o 23 p 11  p 21 p 12  p 22   In Fig 2 we depict this two-phase encoding process Note that p g can be viewed as the Reed Solomon encoding of the respective parities of the systematic symbols We refer to a code with a generator matrix G that takes a composed data object o  o 1  o 2  and encodes it to a codeword c  o  G  c 1  c 2  p g   as the product code of G g and G o It is easy to see how this example product code repairs any single missing block by using the outer erasure code G g  e.g we can repair o 1  1 using o 1  1  o 2  1  p g 1  In addition in case of more than one failure per column the code still has the opportunity to repair up to two failures per codeword c i  and up to two failures within the extra parity vector p g  De“nition of COREs Product Code Let G c and G o respectively be the generator matrices of an  n c k c  and an  n o k o  code Then the product code of G c and G o isa n c n o k c k o  linear code with generator matrix G  G c  G o  where the operator  represents the Kronecker product In the case of the product code used in CORE we will consider that the single parity check SPC with generator matrix G c  I t  1 t   i.e a  t 1 t  MDS erasure code over F 2 q is the vertical code For an input o  o 1 o t   o i  F q  this code generates a systematic codeword c  c 1 c t 1   o 1 o t c t 1   where c t 1   t i 1 o i  Since F 2 q is a binary extension eld the last symbol in the codeword corresponds to the exclusive-or XOR of the t original symbols It can repair any single erasure in the codeword by xoring the remaining t symbols The inner  horizontal  code G o used in CORE is a MDS  n k  erasure code For the sake of simplicity we will consider that it is a  n k  Reed-Solomon code with generator matrix G o  I k H   where H is a k  m Vandermonde matrix recall that m  n  k  H      0 1   m  1 1           0 k   m  1 k     for any  i  F 2 q  Then the COREs product code is a linear code that cross-encodes t different data objects using a generator matrix G  G c  G o  We will refer to such a code as a  n k t  CORE product code Lastly it is worth noting that by varying the value of t  CORE allows for tuning the trade-off between good repairability small t  and good storage overhead large t  In our analytical study as well as in our e xperiments we have chosen t  k 2  in order to strike a balance between the two criteria V CORE S A LGORITHMIC A SPECTS One of the new aspects of CORE is its higher level of granularity i.e instead of working with individual independent objects it works with a matrix of t objects This higher granularity provides new opportunities e.g in a CORE scheme of  n k t  it is possible to repair an object that has more than n  k failed blocks and also poses new challenges e.g given a pattern of failures is it possible to recover  i.e repair all the failed blocks  the CORE matrix or what is the best schedule for repairing a set of failures In this section we look at these algorithmic problems and provide solutions for them We adopt a divide-and-conquer approach to tackle these issues Speci“cally given a matrix representing the available and failed nodes subsequently called the CORE matrix we rst split the failures into independent clusters de“ned below Other algorithms i.e recoverability-checking and repair scheduling can be performed within each cluster We discuss these next A Identifying Independent Clusters We de“ne disjoint subsets of failed nodes that can be handled without interference as independent clusters 2 Essentially two different clusters must not share any common row or column containing failed nodes Two important bene“ts of such clusters are i they allow parallel repairs ii they may allow partial recovery when the full CORE matrix in not recoverable A naive way to create the clusters is as follows Initially each single failure is considered a cluster Two clusters are then merged if there exists at least one common row or column on which both clusters have a failure The process is continued until there are no mergeable clusters left The number of clusters in a CORE matrix is between 0 and t number of rows To investigate the distribution of the number of clusters based on the number of failures we ran our clustering algorithm on 10M randomly-generated CORE matrices for code parameters 14,12,5 and varied the number of random failures from 1 to 20 The results depicted in Figure 3 show that after an initial increase the 2 In other parts of this paper we also use the term computer/node cluster in the common sense of the word which should not be confused with the failure clusters in the CORE matrix 249 


0 1 2 3 1234567891011121314151617181920 Number of Failures Avg Number of Clusters Figure 3 The average number of clusters versus the number of failures for COREs code parameters 14,12,5 number of clusters begins to drop for failure numbers greater than 6 B Recoverability-Checking Algorithm For coding schemes that work at the level of single objects given the number of failures one can directly infer whether an object is recoverable or not In the case of CORE however this is more subtle For instance objects may still be recoverable even if there are more than n  k failed blocks within a single CORE row We rst identify two bounds and then introduce an algorithm to determine an objects recoverability The Ir\verability Bounds  For a  n k t  code  the lower bound of irrecoverability  L  is 2   n  k 1 It occurs if two 3 rows are minimally irrecoverable each has n  k 1 failures and the column indexes of their failures are identical i.e no vertical repair possible  the upper bound of recoverability  U  is t   n  k 2 k  n   1 This occurs when all rows are maximally recoverable each has n  k failures and have identical failure column indexes i.e the remaining k   n  k  k  n columns can each tolerate a single failure These two bounds de“ne an interval For any failure number outside of this interval the ir/recoverability can be immediately decided More precisely if the number of failures is smaller than L then the pattern is recoverable  although as we will see later this is a very pessimistic bound  likewise if the number is greater than U  then it is certainly not recoverable For all the values within the above interval inclusive the outcome depends on the distribution of the failures We propose a recursive algorithm which is able to decide whether a given CORE matrix with a speci“c failure pattern is recoverable or not At each step of the algorithm all the repaired and repairable rows/columns are removed and the algorithm restarts with the reduced matrix as the new 3 Any single-row failure pattern is always recoverable 99.996 99.969 99.877 99.634 99.102 98.021 96.066 92.699 87.253 78.900 66.928 51.237 33.061 15.784 4.125 0 20 40 60 80 100 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of Failures Likelihood  Figure 4 The recoverability likelihood of the scheme 14,12,5 based on the number of failures input If it results in an empty matrix then the patterns is recoverable otherwise it is not We implemented this algorithm and used it to carry out an analysis on the recoverability likelihood of different patterns Figure 4 obtained from 10M random runs shows the recoverability likelihood of the CORE matrix of size 14,12,5 for failure numbers between the lower bound of irrecoverability  L 6  and the upper bound of recoverability  U 20  It clearly illustrates the fact that COREs lower bound of irrecoverability is too strict For a detailed and more rigorous study of fault-tolerance and recoverability we refer the reader to our technical report C Repair Scheduling Algorithms Many different repair schedules may exist for a given fault pattern Here we rst investigate two straw man approaches namely column-“rst and row-“rst  then propose an algorithm called Recursively Generated Schedule RGS Analytical and experimental studies show that RGS outperforms the baseline approaches The column-“rst algorithm always gives higher priority to vertical repairs and applies horizontal repair when no further vertical repairs are possible The row-“rst analogously prefers horizontal repairs In both algorithms while doing horizontal repairs always the best candidate the one with maximum number of failures but still repairable is prioritized over the other ones Recursively Generated Schedule RGS algorithm  This algorithm rst identi“es the critical set of failures failures that decrease the minimum number of required vertical or horizontal repairs and repairs them rst along the call chain of a recursive cost function c  All other repairs non-critical ones are then scheduled using c   a simple non-recursive cost function In order to identify the critical failures we de“ne two variables v and h  as follows v  t  i 1 minV  Row i  h  k  j 1 minH  Col j  in which minV  Row i  returns the minimum number of vertical repairs required by row Row i  and minH  Col j  250 


returns the minimum number of horizontal repairs required by column Col j  more precisely minV  Row i   0 if  X   n  k   X   n  k  otherwise minH  Col j   0 if  X  1  X  1 otherwise The most important element of RGS is the recursive cost function c  h v  de“ned as c  h v   012 012  012 012  c  h dec  v   t if v 0 c  dec  h  v  k if v 0 or dec  v  is not applicable in which dec  v  and dec  h  re”ect the decreases in the values of v and h after a single repair is performed The cost function c decreases the values of rst v and then h by at least one unit at each recursion step until we reach c 0  0  which is the base case 4  The notable property of the base case is that any remaining repair can be done either vertically or horizontally In other words there is at most one failure per column and at most n  k failures per row Therefore all remaining repair decisions can be safely made using the static cost function c  de“ned below c   r  015 k if repaired horizontally r  t if repaired vertically in which r denotes the number of remaining repairs for a given row To demonstrate the differences between the repair schedules generated by the above three algorithms we use two failure pattern examples in the CORE matrix of size 14,12,5 a 3-failure step shaped pattern and a 5-failure plus shaped one These examples are shown in Table I         00   00   X 0   XX   00                 000   0 X 0   XXX   0 X 0   000         Table I The step-shaped and the plus-shaped failure pattern examples representing two classes of failure patterns It should be noted that since swapping any two rows or any two columns in the CORE matrix results in an equivalent failure matrix each of these patterns represents a class of failure patterns and not singular instances Table II presents the schedules generated by each algorithm for each failure pattern along with its calculated cost in terms of repair traf“c The corresponding experimental results are reported in Section VII Finally we generalized our analytical study of the above three algorithms to include failure patterns of size 1 to 20 The results for 10,000 randomly-generated recoverable failure patterns are depicted in Figure 5 Four conclusions 4 If the failure pattern is recoverable the base case will always be reached  Row-First Column-First RGS Step Schedule R 3 R 2 C 1 R 2 C 0 c 1  0 R 3  c 0  0  C 1 Cost 2 k 24 2 t  k 22 k  t 17 Plus Schedule R 1 R 3 C 0 R 2 C 0 C 2 R 1 R 2 C 1 c 2  1 C 0  c 2  0 R 2  c 1  0 R 1  c 0  0  C 1 Cost 3 k  t 41 3 t 2 k 39 2 t 2 k 34 Table II The analytical cost number of blocks read  of repairing the Step and Plus failure patterns using Row-First Column-First and RGS algorithms where k 12 and t 5  0 20 40 60 80 100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 RGS Column First Row First Repair Cost blks Number of Failures Figure 5 Comparing the Column-First Row-First and RGS algorithms w.r.t number of blocks required to carry out the repair on the scheme 14,12,5 can be drawn from this gure i RGS and column-“rst perform better than row-“rst and this is especially noticeable when the number of failures is very small which is in essence the MDS code vs CORE comparison ii as the number of failures and consequently the number of choices to make increases the bene“ts of RGS over column-“rst become more pronounced iii for the large failure numbers distinct schedule possibilities are limited and all the algorithms perform similarly and nally iv a more general conclusion is that if one wishes to avoid the relatively complex scheduling algorithms then the naive column-“rst approach nevertheless delivers signi“cant bene“ts w.r.to the row-“rst which is roughly like for MDS codes highlighting the immediate bene“ts of COREs product code VI I MPLEMENTATION To implement the CORE primitive we used HDFSRAID an open-source module inspired by DiskReduce and de v eloped at F acebook It wraps around Apache Hadoops distributed le system HDFS and provides HDFS with basic erasure coding capabilities encoding and decoding Below we rst introduce HDFS-RAID then explain two optimizations that we did on HDFS-RAID to improve its performance and nally give an overview of our implementation of CORE A HDFS-RAID HDFS-RAID embeds the Apache HDFS inside an erasure code-supporting wrapper le system named Distributed Raid File System DRFS DRFS supports both Reed-Solomon coding as well as simple XOR parity les These two coding 251 


alternatives are orthogonal and used separately based on user preference Furthermore both provide two basic features encoding a.k.a RAIDing data blocks and repairing the corrupt/missing blocks The two main components of HDFS-RAID are RaidNode and BlockFixer RaidNode is a daemon responsible for the creation only once following the initial le write and maintenance re-creating periodically or on demand the corrupt/missing parities and purging orphan ones of parity les for all data les Since the default block policy of HDFS is not aware of the dependency relation between the data and parity blocks of a given le HDFS-RAID manages the placement of parity blocks to avoid co-location of data blocks and parity blocks The BlockFixer component reconstructs missing or corrupt blocks by retrieving the necessary blocks encoding/decoding them and sending the reconstructed blocks to new hosts B HDFS-RAID Optimizations In our experiments with HDFS-RAID we noticed two common performance inef“ciencies and optimized them Opt1 The HDFS-RAID implementation uses the generator polynomial and not the more well-known generator matrix  representation of Reed-Solomon codes In this representation typically and as is in the HDFS-RAID implementation always all the remaining blocks of a given row which can be more than k  are fetched and used to repair the missing ones Generally this use of extra blocks results in faster decoding since there will be fewer equations to solve However for cases in which network is a bottleneck this trade-off fetching extra blocks versus faster decoding does not pay off Our optimized version retrieves exactly k blocks and pretends that all other n  k blocks are missing As con“rmed by our experimental results the bandwidth-scarce clusters can greatly bene“t from this optimization Opt2 The HDFS-RAID implementation implicitly assumes that there is only a single failure per row stripe In case there are more failures they are discovered only when the read access attempts fail These newly-detected failed blocks are then added to the list of failed blocks and the repair process starts again Our optimized implementation checks for multiple failures beforehand and repairs them simultaneously amortizing the repair costs C CORE Implementation The CORE storage primitive has been organically integrated with HDFS-RAID by extending its two main functionalities as described below RAIDing The CORE implementation allows vertical coding across les in a given directory The cross-object size parameter  t  can be con“gured similar to the row stripe size parameter of HDFS-RAID The vertical encoding is reused in the full matrix RAIDing rst row-by-row then column-by-column for both data and parity blocks Repair An additional vertical repair option is introduced The 2-dimensional repair feature implements all the algorithms discussed in Section V i failure detection and failure matrix population ii failure clustering iii recoverability-checking and iv repair scheduling The correctness of our implementation was veri“ed through multiple test cases in which the MD5 hash values of the repaired les were compared against those of the original les Moreover since all changes have been made within the RAID subdirectory of the HDFSs code replacing the corresponding Java library is suf“cient to upgrade HDFSRAID to CORE The source codes binary distribution and documentations of our implementation are available at http://sands.sce.ntu.edu.sg/StorageCORE  VII E XPERIMENTS We benchmarked the implementation with experiments run on two different HDFS clusters of 20 nodes each  Network-Critical cluster A university cluster which has one powerful PC 4  3.2GHz Xeon Processors with 4GB of RAM hosting the NameNode/RaidNode and 19 HP t5745 ThinClients acting as DataNodes The average bandwidth of this cluster is 12MB/s  Computation-Critical cluster An Amazon EC2 cluster of 20 homogeneous nodes of type m1.small approximately 1.2 GHz 2007 Xeon Processor with 1.8GB of RAM In this cluster one node is hosting the NameNode/RaidNode and the rest are used as DataNodes The maximum bandwidth between EC2 m1.small instances is 250MB/s The block size  q  used was 64MB Files were added to HDFS and encoded horizontally rst and then the vertical parity was computed We ran two sets of experiments one set to compare the performance of CORE with that of HDFS-RAID and another set to study the repair scheduling algorithms In both sets we primarily use the completion time of the repair process as the main comparison measure However we also measured the amount of transferred data in each experiment as repair traf“c The data transfer numbers serve two purposes i to verify the correctness of our implementation they must match the analytical numbers and ii to use as a reference point in analyzing the completion time numbers  since the amount of transferred data is independent of the type of cluster used Finally in all experiments the reported numbers are the average of 10 runs Since the variations were small up to few percents they are omitted from the graphs A CORE vs HDFS-RAID In these experiments we compared three methods namely HDFS-RAID HDFS-RAID-Optimized and CORE 252 


0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Bytes Read GB a Transferred data 0 20 40 60 80 100 120 140 160 180 200 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Repair Time seconds b Time network-critical cluster 0 20 40 60 80 100 120 140 160 180 200 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Repair Time seconds c Time computation-critical cluster Figure 6 Comparing the repair performance of HDFS-RAID HDFS-RAID-Optimized and CORE using two different sets of coding parameters 9,6,3 and 14,12,5 inspired respectively by the code length and storage overheads of Googles GFS and Microsoft Azure In these schemes the overhead of COREs extra parities are 1  3=33 and 1  5  20 accordingly In each case two different failure patterns were enforced a one-failure pattern represented by X and a two-failures pattern represented by XX  For the two-failures pattern both are set to happen in the same object i.e on the row The reason for this setting is two-fold i it favors the HDFSRAID since at almost the same cost it can repair two failures instead of one ii if two failures happen on different rows the experiment will be in effect a variation of the onefailure pattern From the results shown in Figure 6 we can draw several conclusions  For single failure the overhead of CORE is less than 50 of HDFS-RAID This is quite signi“cant since in real-world clusters e.g in the Facebook cluster single f ailures per stripe are by far the most common type of failures This improvement results from two inherent advantages of CORE i single failure can be repaired vertically using far fewer blocks and ii it uses a much cheaper XOR operation instead of expensive decoding/re-encoding this is particularly signi“cant in the computation-critical cluster  The impact of our rst HDFS-RAID optimization Opt1 in Section VI-B can be seen in the results the difference between the 2nd and the 3rd chart bars As explained before this optimization is targeted speci“cally for the clusters in which the network is a scarce resource part b in Figure 6 The improvements are particularly pronounced in cases where the number of avoided block retrievals are higher e.g one failure in the scheme 9,6,3  The gains from our second HDFS-RAID optimization Op2 in Section VI-B are also noticeable the 5th and the 6th chart bars in all setups  Growth in the CORE matrix size from 9,6,3 to 14,12,5 results in even higher gains especially in clusters where computation power is scarce B Repair Scheduling Algorithms In this set of experiments the three repair scheduling algorithms of Section V-C were compared using the Step and Plus failure patterns HDFS-RAID has neither a notion of repair scheduling  it treats objects independently  nor can it fully recover from the Plus failure pattern so it was not considered in the following experiments These experiments were run for CORE matrix of size 14,12,5 The results are shown in Figure 7 and as expected the data part of this gure part a  mirrors the analytical results presented in Table II Moreover the completion time numbers parts b and c  are also to large extent in-line with the data results The only two discrepancies are explained below  Completion time of the Column-First algorithm on the Plus pattern in the network-critical cluster part b  is longer than expected This is caused by the last repair which uses two other freshly-repaired blocks Accessing those blocks is delayed until NameNodes heartbeat-driven mapping tables are updated  Completion time of the RGS algorithm in the computationcritical cluster part c  is only slightly better than that of Column-First despite applying one vertical repair less see Table II for the schedules This is due to the fact that for these patterns the RGS and Column-First apply the same number of horizontal repairs and these are the main driving factor of the cost in the computation-critical cluster VIII C ONCLUSIONS AND F UTURE W ORK In this paper we demonstrated that some simple and standard techniques and thus easy to implement and organically 253 


0 0.5 1 1.5 2 2.5 3 Step Plus Row First Column First RGS B ytes R ea d GB a Transferred data 0 50 100 150 200 250 300 Step Plus Row First Column First RGS Repair Time seconds b Time network-critical cluster 0 50 100 150 200 250 300 Ste p Plus Row First Column First RGS Repair Time seconds c Time computation-critical cluster Figure 7 Performances of the repair scheduling algorithms on two different failure patterns integrate can provide signi“cant data repair and access boost in erasure coded distributed storage systems Specifically we studied our approach of introducing cross-object coding on top of normal erasure coding The ideas were implemented and integrated with HDFS-RAID available at  and benchmark ed o v er a proprietary cluster and EC2 Experiments with the implementation as well as accompanying analytical studies comparing the approach with not only MDS codes but also with the very recently proposed Local Reconstruction Codes used in Azure demonstrate the superior performance of CORE over state-of-the-art techniques for data reads and repairs While naive solutions can be readily used in future we will like to explore the CORE code properties to achieve better performance also during data insertion/updates The current evaluations are static based on snapshots of the system state We speculate that COREs better repair properties will yield a system in a better state over time We will thus carry out trace driven experiments to study the systems dynamics better R EFERENCES  P  Elias Error Free Coding  Transactions on Information Theory  vol 4 no 14 1954  HDFS-RAID http://wiki.apache.or g/hadoop/HDFSRAID  C Huang et al Erasure Coding in W indo ws Azure Storage  in USENIX ATC  2012  A Datta et al Redundantly Grouped Cross-object Coding for Repairable Storage in Proc APSys  2012  P  Gopalan et al On the locality of code w ord symbols  Information Theory IEEE Transactions on  vol 58 no 11 pp 6925…6934 2012  K S Esmaili et al The CORE Storage Primiti v e  CrossObject Redundancy for Ef“cient Data Repair and Access in Erasure Coded Storage CoRR  vol abs/1302.5192 2013  CORE http://sands.sce.ntu.edu.sg/StorageCORE  H W eatherspoon et al Erasure Coding vs Replication A Auantitative Comparison in Proc IPTPS  2002  J K ubiato wicz et al OceanStore An Architecture for Global-Scale Persistent Storage in Proc ASPLOS  2000  R Bhagw an et al T otal Recall System Support for Automated Availability Management in NSDI  2004  S Plank The RAID-6 Liber8T ion Code  Intl Journal of High Performance Computing Applications  vol 23 no 3 2009  A P atterson et al A Case for Redundant Arrays of Ine xpensive Disks RAID SIGMOD Records  vol 17 no 3 1988  O Khan et al Rethinking Erasure Codes for Cloud File Systems Minimizing I/O for Recovery and Degraded Reads in USENIX FAST  2012  B F a n e t al DiskReduce Replication as a Prelude to Erasure Coding in Data-Intensive Scalable Computing CMU Tech Rep CMU-PDL-11-112 2011  B Calder et al W indo ws Azure Storage A Highly A v ailable Cloud Storage Service with Strong Consistency in ACM SOSP  2011  A Thusoo et al Data W arehousing and Analytics Infrastructure at Facebook in ACM SIGMOD  2010  C Huang et al Pyramid Codes Fle xible Schemes to T rade Space for Access Ef“ciency in Reliable Data Storage Systems in IEEE NCA  2007  F  Oggier et al Coding T echniques for Repairability in Networked Distributed Storage Systems FnT in Communications and Information Theory  vol 9 no 4 2013  A Dimakis et al A Surv e y on Netw ork Codes for Distributed Storage The Proc of IEEE  vol 99 2011  A Duminuco et al Hierarchical Codes Ho w t o Mak e Erasure Codes Attractive for Peer-to-Peer Storage Systems in Proc P2P  2008  M Li et al GRID Codes Strip-Based Erasure Codes with High Fault Tolerance for Storage Systems ACM Trans on Storage  vol 4 2009  A K ermarrec et al Repairing Multiple F ailures with Coor dinated and Adaptive Regenerating Codes in Proc NetCod  2011  K W  Shum Cooperati v e Re generating Codes for Distributed Storage Systems in Proc ICC  2011  F  Oggier et al Self-Repairing Homomorphic Codes for Distributed Storage Systems in Proc INFOCOM  2011  F  Oggier et al Self-Repairing Codes for Distrib uted Storage A Projective Geometric Construction in Proc ITW  2011 26 D Papailiopoulos et al Locally Repairable Codes in Proc ISIT  2012  M S et al Xoring elephants No v e l erasure codes for big data Proceedings of the VLDB13 To appear  2013  L P amies-Juarez et al Data Insertion  Archi ving in Erasure-coding Based Large-scale Storage Systems in Proc ICDCIT  2013  L P amies-Juarez et al RapidRAID Pipelined Erasure Codes for Fast Data Archival in Distributed Storage Systems in Proc INFOCOM  2013  Y  Hu NCFS On the Practicality and Extensibility of a Network-Coding-Based Distributed File System in Proc NetCod  2011  R Li et al CORE Augmenting Re generating-coding-based Recovery for Single and Concurrent Failures in Distributed Storage Systems in Proceedings of IEEE MSST13  2013  J I Hall Notes on coding theory  Citeseer 2003  K V  Rashmi and others A Solution to the Netw ork Challenges of Data Recovery in Erasure-coded Distributed Storage Systems A Study on the Facebook Warehouse Cluster in Proceedings of USENIX HotStorage13  2013 254 


 L Kaufman and P  Rousseeuw  Clustering by means of medoids Technische Hogeschool Delft Netherlands Department of Mathematics and Informatics Tech Rep 1987  S Deerwester  S Dumais G Furnas T  Landauer  and R Harshman Indexing by latent semantic analysis  vol 41 no 6 pp 391…407 1990  C Boutsidis J Sun and N Anerousis Clustered subset selection and its applications on it service metrics in  2008 pp 599…608  C Boutsidis M W  Mahone y  and P  Drineas  An impro v ed approximation algorithm for the column subset selection problem in  2009 pp 968…977  C Boutsidis P  Drineas and M Magdon-Ismail Near optimal column-based matrix reconstruction in  2011 pp 305 314  J Dean and S Ghema w at MapReduce Simpli“ed data processing on large clusters  vol 51 no 1 pp 107…113 2008  T  White  1st ed OReilly Media Inc 2009  A Frieze R Kannan and S V empala F ast Monte-Carlo algorithms for nding low-rank approximations in  1998 pp 370 378  P  Drineas A Frieze R Kannan S V empala and V  V inay  Clustering large graphs via the singular value decomposition  vol 56 no 1-3 pp 9…33 2004  P  Drineas R Kannan and M Mahone y  F ast Monte Carlo algorithms for matrices II Computing a low-rank approximation to a matrix  vol 36 no 1 pp 158…183 2007  P  Drineas M Mahone y  and S Muthukrishnan Subspace sampling and relative-error matrix approximation Column-based methods in  Springer Berlin  Heidelberg 2006 pp 316…326  A Deshpande L Rademacher  S V empala and G W ang Matrix approximation and projective clustering via volume sampling  vol 2 no 1 pp 225…247 2006  A C  i vril and M Magdon-Ismail Column subset selection via sparse approximation of SVD  vol 421 no 0 pp 1  14 2012  A K F arahat A Ghodsi and M S Kamel  An ef cient greedy method for unsupervised feature selection in  2011 pp 161 170   Ef cient greedy feature selection for unsupervised learning  vol 35 no 2 pp 285…310 2013  T  Elsayed J Lin and D W  Oard P airwise document similarity in large collections with MapReduce in  2008 pp 265…268  A Ene S Im and B Mosele y  F ast clustering using MapReduce in  2011 pp 681…689  H Karlof f S Suri and S V assilvitskii A model of computation for MapReduce in  2010 pp 938…948  S Dasgupta and A Gupta An elementary proof of a theorem of Johnson and Lindenstrauss  vol 22 no 1 pp 60…65 2003  D Achlioptas Database-friendly random projections Johnson-Lindenstrauss with binary coins  vol 66 no 4 pp 671…687 2003  P  Li T  J Hastie and K W  Church V ery sparse random projections in  2006 pp 287…296  G Golub and C V an Loan  3rd ed Johns Hopkins Univ Pr 1996  A Deshpande and L Rademacher  Ef cient v olume sampling for row/column subset selection in  2010 pp 329 338  V  Gurusw ami and A K Sinop Optimal column-based lo wrank matrix reconstruction in  2012 pp 1207…1214  D D Le wis Y  Y ang T  G Rose and F  Li Rcv1 A ne w benchmark collection for text categorization research  vol 5 pp 361…397 2004  W Y  Chen Y  Song H Bai C.-J Lin and E Chang Parallel spectral clustering in distributed systems  vol 33 no 3 pp 568 586 2011  A T orralba R Fer gus and W  Freeman 80 million tin y images A large data set for nonparametric object and scene recognition  vol 30 no 11 pp 1958…1970 2008  N Halk o P G Martinsson Y  Shk olnisk y  and M T ygert An algorithm for the principal component analysis of large data sets  vol 33 no 5 pp 2580…2594 2011 
Journal of the American Society for Information Science and Technology Proceedings of the Seventeenth ACM Conference on Information and Knowledge Management CIKM08 Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms SODA09 Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science FOCS11 Communications of the ACM Hadoop The De“nitive Guide Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science FOCS98 Machine Learning SIAM Journal on Computing Approximation Randomization and Combinatorial Optimization Algorithms and Techniques Theory of Computing Theoretical Computer Science Proceedings of the Eleventh IEEE International Conference on Data Mining ICDM11 Knowledge and Information Systems Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies Short Papers HLT08 Proceedings of the Seventeenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD11 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODA10 Random Structures and Algorithms Journal of computer and System Sciences Proceedings of the Twelfth ACM SIGKDD international conference on Knowledge Discovery and Data Mining KDD06 Matrix Computations Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science FOCS10 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODA12 The Journal of Machine Learning Research Pattern Analysis and Machine Intelligence IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE Transactions on SIAM Journal on Scienti“c Computing 
180 


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even goodŽ partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity … the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the clouds elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPs synchronous barrier between supersteps offers a window for dynamic scaleout and …in at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an oracleŽ approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workers time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440…442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


