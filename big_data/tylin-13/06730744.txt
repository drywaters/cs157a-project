Towards Machine Learning-Based Auto-tuning of MapReduce Nezih Yigitbasi Theodore L Willke and Guangdeng Liao Intel Labs Hillsboro OR 
nezih.yigitbasi,theodore.l.willke,guangdeng.liao intel.com Dick Epema Delft University of Technology the Netherlands D.H.J.Epema@tudelft.nl 
  
Abstract 
MapReduce which is the de facto programming model for large-scale distributed data processing and its most popular implementation Hadoop have enjoyed widespread adoption in industry during the past few years Unfortunately from a performance point of view getting the most out of Hadoop is 
still a big challenge due to the large number of conìguration parameters Currently these parameters are tuned manually by trial and error which is ineffective due to the large parameter space and the complex interactions among the parameters Even worse the parameters have to be re-tuned for different MapReduce applications and clusters To make the parameter tuning process more effective in this paper we explore machine learning-based performance models that we use to auto-tune the conìguration parameters To this end we rst evaluate several machine learning models with diverse MapReduce applications and cluster conìgurations and we show that support vector regression model SVR has good accuracy and is also computationally efìcient We further assess our auto-tuning approach 
which uses the SVR performance model against the Starìsh autotuner which uses a cost-based performance model Our ndings reveal that our auto-tuning approach can provide comparable or in some cases better performance improvements than Starìsh with a smaller number of parameters Finally we propose and discuss a complete and practical end-to-end auto-tuning ow that combines our machine learning-based performance models with smart search algorithms for the effective training of the models and the effective exploration of the parameter space 
I I NTRODUCTION The low cost of data acquisition and storage has enabled industry to store massive amounts of data with the hope of driving their innovation MapReduce the programming model 
of the data center and Hadoop the most popular open source MapReduce implementation have taken a huge step towards solving the problem of processing these 
in a scalable and fault-tolerant way and made it possible to process tens of petabytes of data daily  Ho we v er  performancewise getting the most out of Hadoop is non-trivial as of Hadoop 0.20.2 there are around 190 conìguration parameters and 10 of these parameters have impact on the application performance On the one hand the large parameter space enables a wide range of opportunities for signiìcant performance improvement by careful parameter tuning Section III but on the other hand the large parameter space and the complex 
big data 
interactions among the parameters make manual tuning time consuming and difìcult Therefore the research community has recently started exploring 
Hadoop conìguration parameters Auto-tuning in v olv es tw o phases model building and parameter optimization Section VI In the rst phase the auto-tuner establishes a performance model to predict the performance of an application given a parameter conìguration Then using this performance model in the parameter optimization phase the auto-tuner searches for the optimal parameter values Therefore the performance model is of crucial importance to the auto-tuner and the quality of the auto-tuner depends strictly on the quality of the performance model In this paper we explore the feasibility of machine 
auto-tuning 
learning-based performance models as we move towards a new framework for the auto-tuning of Hadoop MapReduce Cost-based analytical performance modeling is a wellknown approach that has proven itself useful in diverse computer systems but it suffers from several drawbacks when modeling MapReduce applications First cost-based modeling is a white box approach where a deep knowledge about a systemês internals is required and since the system comprising the software stack operating system the Java R 
virtual machine Hadoop and the workloads and the hardware stack are very complex it becomes very difìcult to capture this complexity with cost-based models Second Hadoop has several extension 
002 
points where users can plug in their own policies such as scheduling and block placement policies making it necessary for the performance model to support these different policies Finally although cost-based models may be effective due to the high coupling of the model to the system internals a change in the system such as the hardware technologies or the Hadoop framework itself triggers a change in the model We are motivated by these drawbacks to explore machine learningbased performance models which are black box models since building cost-based models for a complex system such as Hadoop with good accuracy while being exible and robust is challenging Compared with the white box approach black box models 
have two main advantages First black box models and the recommendations of an auto-tuner using these models are based on observations of the actual system performance for a particular workload and cluster Second they are usually simpler to build than white box models as there is no need for detailed information about a systemês internals One of the interesting research questions that motivated our work is Can we build an effective auto-tuner even if we treat the underlying system as a black box As we demonstrate in Section V the answer is yes our machine learning-based approach results in comparable and in some scenarios even better performance than a cost-based approach 
2013 IEEE 21st International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems 1526-7539/13 $26.00 © 2013 IEEE DOI 10.1109/MASCOTS.2013.9 11 


   
io.sort.mb mapred.tasktracker.map.tasks.maximum mapred.tasktracker.reduce.tasks.maximum mapred.reduce.tasks 
Parameter Description Default Rules of Thumb 
the performance model which makes our approach more robust and exible The training data have to be collected separately for every application and cluster and as expected the data collection process may take more time than proìling the workload once which is a common method used by cost-based approaches Ho we v er  in a real deployment we expect the training to be done once because in practice applications rarely change instead their input datasets change and our performance models can easily capture the changes to the input dataset as the input size is one of our model parameters Moreover training data collection time can be further reduced by using the logs of the completed jobs as Hadoop daemons already generate a large amount of logging information Our contributions are threefold We develop several machine learning-based performance models of two Hadoop benchmarks wordcount and sort using data collected from two different cluster conìgurations and we assess these models in terms of accuracy computational performance and their sensitivity to training data size Section IV We perform an extensive evaluation of the support vector regression model SVR which has both good accuracy and computational performance by comparing it against the cost-based model of the Starìsh auto-tuner the rules of thumb settings which are industry recommended parameter settings and the default Hadoop parameter settings Our results reveal that the SVR model is able to achieve comparable and in some cases even better performance improvements than the cost-based model Section V We propose a machine learning-based auto-tuning approach that uses smart search algorithms for both training the performance models and exploring the parameter space Section VI II M AP R EDUCE AND H ADOOP MapReduce is a programming model used for processing large amounts of data on commodity clusters The user speciìes a function that processes a key-value pair to produce a list of intermediate key-value pairs and a function to aggregate the output of the map function Hadoop is a framework that implements the MapReduce programming model and simpliìes cluster programming by taking care of automatic parallelization load balancing and fault-tolerance A typical Hadoop cluster runs over the Hadoop Distributed File System HDFS and has a single job tracker master that is responsible for managing the task trackers slaves that run on each of the nodes in the cluster When a user submits a consisting of map and reduce functions Hadoop rst splits the input data that resides in the HDFS into  Then Hadoop divides the job into several depending on the size of the input data For every split Hadoop runs a separate map task which produces a list of key-value pairs Hadoop then partitions the map output based on the keys and runs a reduce task for each key writing the nal output to the HDFS Hadoop gives users enough exibility to change its behavior by exposing a large number of conìguration parameters tuning knobs While some of these parameters such as the number of map and reduce slots have impact at the machine level some parameters such as the number of reduce tasks have impact at the cluster level Similarly parameters may impact different resources in the system while the size of the map output sort buffer impacts mainly the memory and disk I/O performance the number of reduce tasks parameter impacts mainly the network and disk I/O performance In this work to model the performance of MapReduce applications we have used the ve parameters shown in Table I The rst four parameters are Hadoop parameters and the last one is the size of the job input dataset For the Hadoop parameters the default column shows the default settings of Hadoop and the rules of thumb column shows the values that the industry recommends 7 note the dif ferent rule of thumb settings for our SNB and ZT clusters Section IV-B Our motivation for using these parameters is threefold First these parameters have impact on different resources and the impact of this parameter set covers all the available resources in a cluster Second these parameters have impact at different levels machine-level or cluster-level Finally based on our domain expertise we expect these parameters to have signiìcant impact on the performance However selecting the parameters to use in a machine learning-based performance model is a non-trivial research problem in its own which is known as the feature selection problem Therefore we have used our domain expertise to select these parameters and we have left the feature selection problem as an important future work III T HE N EED FOR A UTO TUNING In this section we make the case for why auto-tuning is the right approach to tuning Hadoop parameters First through experiments in our 8-node SandyBridge SNB cluster see Section IV-B we show that there is signiìcant performance improvement opportunity even by tuning a small number of parameters Second we explain why manually tuning a complex system such as Hadoop is not feasible 
SNB Cluster ZT Cluster map output sort buffer The amount of memory used while sorting les during the map phase 100 200 400 map slots The maximum number of map tasks to run simultaneously by a task tracker 2 7 31 reduce slots The maximum number of reduce tasks to run simultaneously by a task tracker 2 2 2 reduce tasks The number of reduce tasks to run for a job 1 15 15 Job input size The size of the input dataset N/A N/A N/A TABLE I F IVE PARAMETERS THAT WE HAVE USED IN OUR PERFORMANCE MODELS T HE FIRST FOUR PARAMETERS ARE H ADOOP CONFIGURATION PARAMETERS SO THEY HAVE DEFAULT VALUES AND RULE OF THUMB SETTINGS FOR THE TWO CLUSTERS WE HAVE USED IN OUR EXPERIMENTS  THE SNB AND ZT CLUSTERS S ECTION IV-B AND THE LAST PARAMETER IS THE SIZE OF THE JOB INPUT DATASET  Unlike the cost-based approach our approach relies on training data to 
learn map reduce job splits tasks 
12 


20 10 20       
y 002 x 002 x  002 x 002 x x e 002 x x 
i i i k ik k p,r p 004 r pr ip ir i pr ip ir 
Simple Multiple Linear Regression MLR Multiple Linear Regression with Parameter Interactions MLR-I Multiple Linear Regression with Quadratic Effects MLR-Q 
  for parameters the size of the search space will be  and it is simply impossible to manually explore this search space Even if we explore a small subset of the search space which may yield sub-optimal performance in the end manual tuning will still be inefìcient since the user has to run a large number of trials to carefully search for the performance sweet spots For example when tuning the number of reduce slots the user has to address the trade-off between resource utilization and resource contention while large values can cause resource contention low values may leave resources underutilized IV M ODELING THE P ERFORMANCE OF M AP R EDUCE A PPLICATIONS In this section we study several machine learning algorithms for modeling the performance of MapReduce applications as performance modeling is the key component in an auto-tuning system We evaluate our models in terms of their accuracy computational performance and their sensitivity to various variables such as the training data size the application characteristics and the cluster conìguration We have explored various machine learning models to use in a practical auto-tuner such as the one we propose in Section VI Our models have ve inputs Table I and one output which is the job completion time Although multiple linear regression is also considered as a machine learning model when presenting the results we refer to it as a traditional statistical model to discriminate it from the other more complex machine learning models we have explored We brieîy describe our models in turn We have used the simplest form of multiple linear regression for modeling the job completion time This model does not have any parameter interactions and higher order terms With this model the completion time is modeled as where terms represent the parameter interactions As we donêt know which parameters have interactions among them we have performed an exhaustive search over all possible pairwise interactions to identify the interactions that yield the best performing model With this model the completion time is modeled as 
Fig 1 Performance of the sort benchmark with 80GB input on our 8-node SNB cluster see Section IV-B with different parameter settings for the size of the map output sort buffer left the number of reduce tasks middle and the number of map and reduce slots right  Fig 2 Performance obtained by tuning several Hadoop parameters at once for the sort benchmark with 120GB input on our 8-node SNB cluster see Section IV-B for two conìgurations map output sort buffer map slots reduce slots reduce tasks Figure 1 shows the performance of different parameter conìgurations comprising different values for the map output sort buffer parameter left the reduce tasks parameter middle and the map and reduce slots parameters right for the sort benchmark with 80GB input on our 8-node SNB cluster see Section IV-B The performance difference between the best and the worst conìguration is 16 for the map output sort buffer parameter 35 for the reduce tasks parameter and 42 for the map and reduce slots parameters Our results show that even by only tuning a single parameter in isolation it is possible to obtain up to around 40 improvement in performance Figure 2 shows the result when we tune several Hadoop parameters at once for the sort benchmark with 120GB input on our 8-node SNB cluster For Conìguration 1 we set the map output sort buffer size to 100MB the number of map slots to 4 the number of reduce slots to 1 and the number of reduce tasks to 32 For Conìguration 2 we set the map output sort buffer size to 100MB the number of map slots to 5 the number of reduce slots to 2 and the number of reduce tasks to 24 Our results show that by tuning only a small number of parameters together we can achieve roughly a factor of two performance improvement Although there is a vast performance improvement opportunity with careful parameter tuning as we have shown above manual tuning is not feasible for two reasons First it is very difìcult to understand the complex interactions among the parameters and reason about their performance impact as the impact of a parameter depends on several factors such as the workload characteristics the input dataset the cluster conìguration and even on the values of other parameters Second the parameter search space is huge even if we assume that the parameters take values from different sets of the same size say 
002 
0  100  200  300  400  500  600  700  800  100  150  200  250  300  350  400          0  100  200  300  400  500  600  700  800  16  32  64  128  Number of Reduce Tasks     1  2  3  4  5  6  7  8  9  1  2  3  4 600 800 1000  Number of Map Slots Number of Reduce Slots 0 200 400 600 800 1000 
10 1 1 2 2 1 003  
A Machine Learning-based Performance Models 
0  200  400  600  800  1000  1200  1400  1600  1800  2000  Configuration 1 100MB,4,1,32  Configuration 2 100MB,5,2,24   
13 


        
1 1 2 2 1 2 1 2 2 2 
B Experimental Setup 
 
002 002 002 
Multiple Linear Regression with Parameter Interactions and Quadratic Effects MLR-IQ Artiìcial Neural Networks ANN Model Trees M5Tree Support Vector Regression SVR Workloads Clusters 
where the terms represent the quadratic effects of each input parameter Finally we have also explored a regression model that captures both parameter interactions and quadratic effects ANNs are one of the powerful learning models for general nonlinear regression between multiple input and output variables ANNs have several desirable properties for performance modeling that motivated us to assess ANNs for auto-tuning purposes First ANNs are learning models that adjust their internal state to the data without any assumptions about the data Second ANNs can represent any function to arbitrary precision making them universal function approximators Third ANNs can capture complex relationships between the input and output variables such as nonlinearity and parameter interactions making them suitable for practical performance modeling Finally ANNs have been shown to be useful for performance modeling in several previous studies Section VII Model trees are an important class of machine learning algorithms for constructing tree models from data In particular we have used the M5 model tree algorithm provided by the Weka toolkit Support Vector Machines SVM are a set of machine learning algorithms used for classiìcation and regression In the context of regression SVMs are called Support Vector Regression SVR Two unique features of SVR have motivated us to explore it for performance modeling First SVR is considered as a powerful learning algorithm that has good generalization capabilities and has less risk of overìtting than the other approaches Second unlike ANNs SVR is faster to train see Section IV-E and there is no local minima problem during the training phase as SVR transforms the regression problem into a convex optimization problem where all local minima are guaranteed to be global minimums We have used the wordcount and the sort benchmarks from the HiBench benchmark suite Our motivation for using these benchmarks is threefold First these workloads are simple and hence easy to reason about Second these benchmarks are representative of real MapReduce applications as the computation performed by these benchmarks are common use cases of MapReduce namely extracting a small amount of data from a large dataset and transforming data from one representation to another respectively Second these two benchmarks have different characteristics in terms of resource requirements wordcount is a CPU bound benchmark while the sort benchmark is mostly I/O bound as shown in Figure 3 We have assessed our models using data collected from two different clusters SandyBridge SNB cluster and ZT cluster The SNB cluster comprises eight machines connected through a 1 Gbps Ethernet switch and each machine has a four-core Intel R Core TM i7-2600 processor running at 3.4 GHz has 16GB main memory and three rotational drives that serve the HDFS data The ZT cluster has more resources than the SNB cluster it comprises eight machines connected through a 10 Gbps Ethernet switch and each machine has a sixteen-core Intel R Xeon R E5-2670 processor running at 2.6 GHz has 64GB main memory and three rotational drives that serve the HDFS data In our experiments we have used Hadoop 0.20.2 and the master node running the job tracker and the name node were also responsible for running application tasks Finally to give an idea about the workload characteristics and our cluster conìgurations we present the resource utilizations when running the sort and wordcount benchmarks with 80GB input in Figures 3 and 4 While the sort benchmark is mostly network I/O bound on the SNB cluster it achieves a speedup of around 35 on the ZT cluster where the network is no longer a bottleneck Similarly on the SNB cluster the wordcount benchmark is CPU bound but on the ZT cluster 
y 002 x 002 x  002 x 002 x  002 x e x 
i i i k ik k i k i k i i k 
a Sort CPU/Disk Util  b Sort Network Util  c Wordcount CPU/Disk Util  d Wordcount Network Util Fig 3 CPU disk and network utilization when running the sort a and b and the wordcount c and d benchmarks on the SNB cluster  a Sort CPU/Disk Util  b Sort Network Util  c Wordcount CPU/Disk Util  d Wordcount Network Util Fig 4 CPU disk and network utilization when running the sort a and b and the wordcount c and d benchmarks on the ZT cluster 
0  10  20  30  40  50  60  70  80   90  100  0  200   400   600   800  1000  Time Since Experiment Start CPU Util Disk Util 0  10  20  30  40  50  60  70  80  90  100  0  200  400  600  800  1000  Time Since Experiment Start 0  10  20  30  40  50  60  70  80  90  100   0   100  200  300  400  Time Since Experiment Start CPU Util Disk Util 0  10  20  30  40  50  60  70  80  90  100  0  100  200  300  400  Time Since Experiment Start 0  10  20  30  40  50  60  70  80   90  100  0  100  200   300   400   500   600  700  Time Since Experiment Start CPU Util Disk Util 0  10  20  30  40  50  60  70  80  90  100  0  200  400  600  800  Time Since Experiment Start 0  10  20  30  40  50  60  70  80   90  100  0  100   200   300  400  Time Since Experiment Start CPU Util Disk Util 0  10  20  30  40  50  60  70  80  90  100  0  100  200  300  400  Time Since Experiment Start 
14 


i P i O i R 2 SS SS SS SS P O SS SS O O O 
P O O i i SS SS err tot err err i i i tot tot i i P O N 
    2 
i i i err tot i i i 
To train our models we have collected data from real sort and wordcount executions on the SNB and ZT clusters During training an important question to address is which parameter values should we explore while collecting the training data Since our parameter space is huge we have used a sparse sampling approach In particular we have collected data both with exhaustive search over different small subsets of the parameter space exploring a speciìc range of parameter values and with random explorations where we have picked parameter values uniformly random from different range of values The former approach is guided by our domain knowledge when choosing the speciìc parameter ranges while the latter approach provides us unbiased observations from the real performance surface of the application We expect that these two approaches together provide more insights into the real performance surface than the individual approaches During data collection the clusters were dedicated to our workloads For each benchmark and each cluster we have explored the same parameter values to collect the training data and to capture the performance variability we have executed the benchmarks three times for every parameter conìguration resulting in around 400 training points for each benchmark After we have collected the raw data we have split the dataset randomly into an 80 training set and a 20 test set We have trained the models with the training set and we have evaluated their accuracy with the test set We have performed this train-test cycle ve times with different random splits and then we report the average performance metrics To build our models we have used both the Weka toolkit and the R statistical softw are en vironment 11 Our models have various parameters such as the number of hidden layers in the ANN and the maximum tree depth of the M5Tree As these parameters depend on the nature of the data there is no simple way to determine the best parameter values Therefore we have used ve-fold crossvalidation and exhaustive search to nd the best parameter values for our models In particular we have used a subset of one of the training sets as the validation set and we have performed exhaustive search using cross-validation to identify the model parameters that yield the minimum error We have assessed both the computational performance and the accuracy of our models as both are important when using the models for auto-tuning To characterize the computational performance we have evaluated the time it takes to build the model and the time it takes to make a single prediction using the model To characterize the accuracy we have used the absolute percentage error the R 2 statistic and the root mean squared error RMSE statistic The absolute percentage error for the th prediction is deìned as where is the th predicted value and is the th observation in the test set We also present the mean absolute percentage error MAPE which is the mean of the absolute percentage error values computed over all the data points in the test set The smaller the absolute percentage error the better in particular a zero absolute percentage error denotes a perfect prediction The R 2 is deìned as where is the residual sum of squares and is the total sum of squares is deìned as  and is deìned as  where is the mean of the observed values in the test set R 2 simply shows the predictive power of a model Models with R 2 values close to 1 are considered as better models in terms of predictive power Finally the RMSE statistic is deìned as the square root of the mean squared error which is deìned as 002  where N is the number of data points in the test set A smaller RMSE value denotes a more accurate model In this section we explore the accuracy of our models using the data collected from two benchmarks on two different clusters Table II shows the basic statistics for the absolute percentage error for all the models for the sort benchmark and Figure 5 a and b present the corresponding box plots that show the distributions of the absolute percentage error In the box plots presented in this section the outliers are deìned as points with values either less than Q1-1.5IQR or greater than Q3+1.5IQR where Q1 and Q3 are the rst and third quartiles and IQR is the interquartile range Q3-Q1 As the multiple linear regression MLR model gets more complex from MLR to MLR-IQ the model gets more accurate the mean model error decreases from 17 to 14 for the SNB cluster and from 23 to 18 for the ZT cluster Similarly the median and the max errors also decrease as the 
C Methodology D Model Accuracy 1 Sort Benchmark 
002 003\002\004\002\005\002\006\002\007\002\002 002 003\002\004\002\005\002\006\002\007\002\002 002 003\002\004\002\005\002\006\002\007\002\002 002 003\002\004\002\005\002\006\002\007\002\002 002 003\002\004\002\005\002\006\002\007\002\002 002 003\002\004\002\005\002\006\002\007\002\002 002 003\002\004\002\005\002\006\002\007\002\002 010\011\012\013\014\015\016\017\020\021\017\022\023\017\024\016\025\026\017\020\027\022\022\013\022\020\030 031 032 033\034\035 033\034\035 036 037 033\034\035 036  033\034\035 036 037 010 033#$\022\017\017 035 002 003\002\004\002\005\002\006\002\007\002\002 017\025\024 017*+\025\024 013\015\016\014+\017\022 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 010\011\012\013\014\015\016\017\020\021\017\022\023\017\024\016\025\026\017\020\027\022\022\013\022\020\030 031 032 033\034\035 033\034\035 036 037 033\034\035 036  033\034\035 036 037 010 033#$\022\017\017 035 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 017\025\024 017*+\025\024 013\015\016\014+\017\022 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 010\011\012\013\014\015\016\017\020\021\017\022\023\017\024\016\025\026\017\020\027\022\022\013\022\020\030 031 032 033\034\035 033\034\035 036 037 033\034\035 036  033\034\035 036 037 010 033#$\022\017\017 035 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 017\025\024 017*+\025\024 013\015\016\014+\017\022 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 010\011\012\013\014\015\016\017\020\021\017\022\023\017\024\016\025\026\017\020\027\022\022\013\022\020\030 031 032 033\034\035 033\034\035 036 037 033\034\035 036  033\034\035 036 037 010 033#$\022\017\017 035 002 003\002\004\002\005\002\006\002\007\002\002\007\003\002 017\025\024 017*+\025\024 013\015\016\014+\017\022 
100 1  002   2  002    2  
Data Collection Training and Testing Model Tuning Metrics 
 002    
a Sort benchmark on SNB cluster b Sort benchmark on ZT cluster c Wordcount benchmark on SNB cluster d Wordcount benchmark on ZT cluster Fig 5 The distributions of the absolute percentage error for the sort and wordcount benchmarks on the SNB a,c and ZT clusters b,d the CPU is not a bottleneck anymore We also observe that the resources on the ZT cluster are utilized less than the SNB cluster as the ZT cluster has more resources 
15 


Considering the results in Table II and Table III we conclude that overall machine learning models perform better than the traditional statistical models and in particular SVR has the best performance among the models we have explored for the sort benchmark 
Table IV presents the basic statistics for the absolute percentage error for all the models for the wordcount benchmark and Figure 5 c and d present the corresponding absolute percentage error distributions Similarly to the results for the sort benchmark as the complexity of the MLR model increases the mean/median error decreases from 15%/13 to 12%/9 for the SNB cluster and from 26%/20 to 12%/9 for the ZT cluster The decreasing error trends conìrms that complex MLR models can capture the nonlinearity and second order effects in the performance surface also for the wordcount benchmark In general SVR and ANN have better performance than the other models with a median error of 1 and 6 for the SNB cluster and 5 and 8 for the ZT cluster respectively A particularly interesting result is that the M5Tree model performs worse than the other machine learning models with a similar performance as the MLR-IQ model This result suggests that the workload characteristics deìnitely have an impact on the performance of different modeling techniques since for the sort benchmark M5Tree model has a similar performance as the other machine learning models Therefore it is important to assess the models with data collected from workloads with different characteristics When we look at the error IQRs traditional statistical models tend to have a higher variability IQR than the machine 
MLR 0 7 15 17 23 68 16 0 8 18 23 33 117 25 MLR-I 0 6 14 16 22 69 16 0 8 17 22 31 129 23 MLR-Q 0 6 11 15 22 66 16 0 7 16 19 27 92 20 MLR-IQ 0 6 11 14 20 67 14 0 7 15 18 25 87 18 ANN 0 4 10 12 17 61 13 0 4 10 12 17 61 13 M5Tree 0 5 10 12 17 65 12 0 5 10 14 19 71 14 SVR 0 2 4 8 10 73 8 0 3 6 10 13 64 10 TABLE II B ASIC STATISTICS FOR THE ABSOLUTE PERCENTAGE ERROR  FOR THE SORT BENCHMARK ON THE SNB AND THE ZT CLUSTERS T HE BEST PERFORMING MODEL IS DEPICTED WITH LIGHT GRAY Q1,Q3 AND IQR DENOTE THE FIRST QUARTILE  THE THIRD QUARTILE  AND THE INTERQUARTILE RANGE  RESPECTIVELY  MLR 0.79 0.8 179.23 160.69 0.76 0.73 199.75 213.65 MLR-I 0.81 0.81 170.8 155.9 0.79 0.8 184.38 190.91 MLR-Q 0.82 0.84 165.38 154.64 0.82 0.82 173.66 176.45 MLR-IQ 0.84 0.85 157.08 145.67 0.85 0.87 156.03 160.66 ANN 0.87 0.89 135.71 134.71 0.93 0.94 108.78 106.84 M5Tree 0.83 0.81 159.81 164.2 0.92 0.93 114.62 104.74 SVR 0.89 0.92 125.47 122.2 0.95 0.96 87.51 88.87 TABLE III T HE R 2 AND RMSE STATISTICS FOR THE SORT BENCHMARK ON THE SNB AND THE ZT CLUSTERS T HE BEST PERFORMING MODEL IS DEPICTED WITH LIGHT GRAY  complexity of the MLR model increases This result conìrms the presence of both nonlinearity and second order effects in the actual performance surface and further conìrms that these effects are captured by the complex MLR models In general all machine learning models ANN M5Tree and SVR have better accuracy and lower variability IQR than the traditional statistical models MLR models ANN and M5Tree models have similar accuracy for both SNB and ZT clusters with a MAPE of around 12 while SVR has a MAPE of 8 for the SNB cluster and 10 for the ZT cluster Moreover SVR has a signiìcantly better median error than the other models with a median error of 4 and 6 for the SNB and the ZT cluster respectively For the SNB cluster the minimum and the maximum errors of different models are similar while this is not the case for the ZT cluster the same modeling techniques can behave differently when trained with data collected from different clusters Therefore it is important to assess machine learning models with data from signiìcantly different clusters In particular the maximum error of the traditional statistical models increase signiìcantly with the data collected from the ZT cluster whereas the maximum errors obtained with the machine learning models increase only slightly which shows their robustness Similarly for the ZT cluster the MLR models have higher variability with an IQR of up to 25 than the machine learning models with an IQR of 10 to 13 but in contrast this difference is less pronounced for the SNB cluster An important error statistic is the third quartile Q3 of the error distribution which constitutes the bulk 75 of the test data Our results reveal that at the third quartile machine learning models have smaller errors than the traditional statistical models on both clusters Table II In particular for the SNB cluster 75 of the test data have an error of 20%-23 for the MLR models and an error of 10%-17 for the machine learning models Similarly for the ZT cluster 75 of the test data have an error of 25%-33 for the MLR models while for the machine learning models the error is 13%-17 Finally Table III presents the R 2 and RMSE statistics for the sort benchmark on the two clusters Similarly to the error statistics presented in Table II machine learning models have higher R 2 values and lower RMSE values conìrming their better predictive capabilities than the MLR models 
2 Wordcount Benchmark 
SNB Cluster ZT Cluster Model min Q1 median mean Q3 max IQR min Q1 median mean Q3 max IQR SNB Cluster ZT Cluster Model R 2 RMSE R 2 RMSE mean median mean median mean median mean median 
16 


002 
SNB Cluster ZT Cluster Model min Q1 median mean Q3 max IQR min Q1 median mean Q3 max IQR SNB Cluster ZT Cluster Model R 2 RMSE R 2 RMSE mean median mean median mean median mean median 
In an auto-tuner the computational performance of the models also matters To this end in this section we assess the time to train our models and the time it takes to make a single prediction with each model We perform the measurements on a machine that has a dual-core Intel R Core TM i5-2540M processor running at 2.60 GHz and 4GB main memory and we report the average of ten measurements We present the computational performance of the models only for the SNB cluster using the sort benchmark as the results are similar for the other datasets Figure 6 top shows the time to train the models which is measured using the whole dataset the nal models that will be used in the auto-tuner will also be trained using the whole dataset As the multiple linear regression models get more complex from MLR to MLR-IQ the training time increases by 2.08x from 2.5ms to 5.2ms Machine learning models have signiìcantly longer training times than multiple linear regression models with ANN having the longest training time 03729s SVR having a training time of 0371s and M5Tree having the shortest training time 037100ms Overall we nd that all traditional statistical models are relatively lightweight to train Among the machine learning models ANN takes the longest time to train as we train the ANN for 100,000 epochs and for each epoch the network weights are updated using gradient descent which is a computationally intensive operation Finally as shown with the vertical lines in Figure 6 all models have little variability in their training times Figure 6 bottom shows the time to make a single prediction using the models Similarly to the results for the training time as the multiple linear regression models get more complex the time to make a single prediction increases by 2x from 2.5ms to 5ms On the other hand although machine learning models take signiìcantly longer time to train they 
E Computational Performance of the Models 
When we consider the results for sort and wordcount together overall machine learning models have better predictive capabilities than the traditional models In particular SVR is the best performing model except for the dataset collected from the ZT cluster using the wordcount benchmark for which ANN performs slightly better 
MLR 0 7 13 15 21 94 14 0 9 20 26 32 181 23 MLR-I 0 6 12 14 18 187 12 0 9 17 22 27 208 18 MLR-Q 0 5 10 13 18 94 13 0 7 14 19 27 169 20 MLR-IQ 0 4 9 12 16 133 12 0 4 9 12 16 133 12 ANN 0 2 6 8 12 92 10 0 4 8 10 13 71 9 M5Tree 0 3 9 12 17 124 14 0 4 9 11 16 72 15 SVR 0 0 1 4 2 91 2 0 1 5 9 12 70 11 TABLE IV B ASIC STATISTICS FOR THE ABSOLUTE PERCENTAGE ERROR  FOR THE WORDCOUNT BENCHMARK ON THE SNB AND THE ZT CLUSTERS T HE BEST PERFORMING MODEL IS DEPICTED WITH LIGHT GRAY Q1,Q3 AND IQR DENOTE THE FIRST QUARTILE  THE THIRD QUARTILE  AND THE INTERQUARTILE RANGE  RESPECTIVELY  MLR 0.45 0.51 241.98 193.85 0.45 0.47 234.24 246.8 MLR-I 0.45 0.48 242.13 194.29 0.53 0.54 215.02 230.04 MLR-Q 0.6 0.72 206.78 142.16 0.67 0.75 174.57 193.76 MLR-IQ 0.59 0.74 209.38 134.38 0.69 0.76 169.93 193.85 ANN 0.73 0.88 162.83 80.36 0.87 0.95 96.77 76.71 M5Tree 0.48 0.5 233.9 192.1 0.83 0.91 115.23 110.67 SVR 0.76 0.95 147.8 56.35 0.81 0.83 131.66 137.92 TABLE V T HE R 2 AND RMSE STATISTICS FOR THE WORDCOUNT BENCHMARK ON THE SNB AND THE ZT CLUSTERS T HE BEST PERFORMING MODELS ARE DEPICTED WITH LIGHT GRAY  learning models For the SNB cluster the best performing machine learning model SVR has an IQR of 2 but in contrast the best performing MLR model MLR-IQ has an IQR of 12 Likewise for the ZT cluster the best performing machine learning model ANN has an IQR of 9 while the best performing MLR model MLR-IQ has an IQR of 12 Since 50 of the predictions have an error in the range Q1 to Q3 the IQR a smaller IQR for the machine learning models conìrms their better robustness than the traditional models Similarly to the results of the sort benchmark machine learning models also tend to have smaller errors at the third quartiles for the wordcount benchmark Table IV For the traditional statistical models 75 of the predictions have an error in the range 16%-21 for the SNB cluster and 16%32 for the ZT cluster On the other hand for the machine learning models 75 of the predictions have an error in the range 2%-12 for the SNB cluster and 12%-16 for the ZT cluster We conclude that machine learning models have better accuracy than the traditional statistical models for the 75 of the predictions which constitute the bulk of the test data Finally Table V shows the R 2 and RMSE statistics for wordcount Similarly to the error statistics presented in Table IV and similarly to the results for sort Table III machine learning models have higher R 2 values and lower RMSE values conìrming their better predictive capabilities than the traditional models In particular SVR performs the best for the SNB cluster while ANN is the best modeling approach for the ZT cluster This result is particularly interesting as it demonstrates the impact of the workload and cluster characteristics on the predictive power of the models 
17 


0  10  20  30  40  50   60   70   80   90  100  25  50  75   100  Fraction of data used MLR-IQ ANN M5Tree SVR  0  10  20  30  40  50   60   70   80   90  100  25  50  75   100  Fraction of data used MLR-IQ ANN M5Tree SVR 
 
In this section we explore the sensitivity of the models to the size of the training data To this end we have trained the models using only a fraction of the data that is 25 50 75 and 100 of the data For each fraction we have applied the same modeling method described in Section IV-C Figure 7 presents the results for the sort and wordcount benchmarks on the SNB cluster The results are similar for the datasets collected from the ZT cluster Among the multiple linear regression models we only present the sensitivity of the MLRIQ model as it performs the best For sort as the training set size increases MAPE decreases for all the models by 3 for MLR-IQ 2 for ANN 1 for M5Tree and 7 for SVR Similarly for wordcount as the size of the training set increases MAPE decreases by 26 for MLR-IQ 14 for ANN 16 for M5Tree and 6 for SVR Based on these results we conclude that our models can beneìt from additional data if we further increase our training set size we expect the accuracy of our models to increase V M ACHINE L EARNING B ASED VS C OST B ASED M ODELS In this section we compare our SVR model against the cost-based model of the Starìsh auto-tuner to assess ho w much performance improvement we can get with each approach We have performed the experiments in our SNB cluster using two different datasets 80GB and 240GB Therefore for each dataset we have a different SVR performance model as the job input size is one of the model parameters To nd the completion time with our approach rst we have identiìed the best conìguration by performing exhaustive search over the performance surface generated by our model then we have measured the completion time with this conìguration We have compared the completion time of our approach against the completion time achieved with the conìguration recommended by Starìsh  
F Model Sensitivity to Training Data Size 
10 0  10 1  10 2  10 3  10 4  10 5  MLR  MLR-I  MLR-Q  MLR-IQ  ANN  M5Tree  SVR   0  1  2  3  4  5  6  7  8  9  10  MLR  MLR-I  MLR-Q  MLR-IQ  ANN  M5Tree  SVR  
Fig 6 Time to train the models top and to make a single prediction bottom for all models Please note that the vertical axis has a logarithmic scale for the top graph are relatively lightweight when making predictions with a prediction time of less than a millisecond for all the machine learning models Fig 7 Accuracy of the models for various training data sizes for the sort top and wordcount bottom benchmarks on the SNB cluster An important difference between Starìshês cost-model and our model is that the cost-based model uses fourteen Hadoop parameters while our model uses ve parameters which gives the cost-based model more exibility when exploring the available headroom for performance improvement as it has more knobs to tune Consequently the absolute completion time numbers are not directly comparable and for a fair comparison we have normalized the completion time with respect to the completion time obtained with the rules of thumb settings In addition we have also compared our approach against the default parameter settings we refer to Table I for our model parameters and their default and rules of thumb values Figure 8 shows the results of our comparison For the sort benchmark our SVR model has improved the performance more than Starìsh for the 80GB input While Starìsh has improved the performance by 13 over rules of thumb our SVR model has provided a 39 improvement For the 240GB input both models perform similarly and improve the performance around 40 Similarly for the wordcount benchmark both models have relatively similar performance improvements with Starìsh being slightly better 0375 Since our model has a smaller number of inputs than the costbased model we expect that increasing the number of model inputs will provide more exibility for parameter tuning and help further improve the performance As expected for both benchmarks the default settings perform signiìcantly worse than both approaches as default settings do not involve any tuning at all Our results reveal that compared with the costbased model our SVR model achieves comparable or in some cases even better performance improvements One limitation of our approach may be the time it takes to collect the training data However in a real deployment we expect the training to be done once per application on a particular clusterÑin practice applications rarely change but rather their inputs change and this is easily captured by our models as job input size is one of our model parameters Moreover the training time can be further reduced by using the logs of the completed jobs in a Hadoop cluster 
18 


Benchmarks Use smart sampling to collect training data Determine model parameters Train the machine learning model Evaluate accuracy Generate parameter space Use smart search to explore the parameter space for the optimal parameter values Run the job with optimal parameter values Model Building Parameter Optimization Performance Model Not Accurate Enough 
 
Machine learning has been successfully used to model the performance of diverse applications ANNs and multiple linear regression have been shown to be effective for modeling the performance of parallel applications Similarly  ANNs and M5 model trees have been used to model the performance of virtualized systems and utility computing systems 16 respectively ANNs support vector machines and decision trees have also been used for optimizing the performance of data center applications Our w ork contrib utes to this body of work by applying machine learning to model the performance of an interesting class of applications namely MapReduce applications with the goal of performance autotuning 
0.5  1  1.5  2  Default  Rules of Thumb  SVR  Starfish Normalized Job Completion Time 11 0.5  1  1.5  2  Default  Rules of Thumb  SVR  Starfish Normalized Job Completion Time 9 0.5  1  1.5  2  Default  Rules of Thumb  SVR  Starfish Normalized Job Completion Time 0.5  1  1.5  2  Default  Rules of Thumb  SVR  Starfish Normalized Job Completion Time 
Machine Learning-Based Performance Modeling 
a Sort with 80GB input  b Sort with 240GB input  c Wordcount with 80GB input  d Wordcount with 240GB input Fig 8 The normalized job completion times with respect to the rules of thumb settings for the sort a and b and wordcount benchmarks c and d For gur es a and b the numbers on the Default bar show the normalized completion time with the default settings Fig 9 A practical end-to-end machine learning-based auto-tuning ow VI T OWARDS AP RACTICAL M ACHINE L EARNING B ASED A UTO TUNER A practical auto-tuner has to implement the model building and parameter optimization phases shown in Figure 9 So in addition to the performance model several additional components are required to implement this end-to-end auto-tuning ow In this section we propose a practical auto-tuning approach that realizes this ow using machine learning-based performance models and smart search techniques which are used both in the model building phase for training the models and in the parameter optimization phase for exploring the parameter space Our machine learning-based auto-tuning approach learns the underlying performance model from the training data collected from benchmark executions and thus is exible and robust when dealing with different applications and clusters One of the major challenges with our approach is to train a relatively accurate performance model quickly Random sampling which is a simple approach to collect unbiased observations from the performance surface of an application has been used to collect training datasets in previous studies 13 Ho we v er  based on our e xperience random sampling may require a large number of samples to build an accurate model and since the overall tuning time is an important consideration in an auto-tuner a long training time can reduce its effectiveness To address the above challenge we propose using smart sampling techniques during the model building phase to reduce the model training time With smart sampling we employ a direct search algorithm rather than random sampling The direct search method focuses on regions of interest e.g a region with good performance and searches towards such regions for nding the parameter conìguration that yields the best performance Unlike random sampling this method can reduce the number of samples required to accurately represent the features of the performance surface in those regions As our results have already shown that the performance surfaces of MapReduce applications are usually nonlinear and multimodal we apply a global search algorithm such as a genetic algorithm or recursive random sampling to avoid being trapped in a local minimum The smart search algorithm collects training samples and trains the performance model using these samples as it searches through the actual performance surface of the application Figure 9 Then the auto-tuner evaluates the accuracy of the performance model and stops the sampling process if the model is accurate enough e.g 90 Otherwise sampling continues until either the search algorithm converges reaches the search budget or the model is accurate enough After building a reasonably accurate performance model the auto-tuner proceeds to the parameter optimization phase In this phase the auto-tuner generates a parameter search space and explores it using smart search techniques and it uses the performance model to nd the parameter conìguration that has the best predicted performance When the search algorithm nds the best parameter conìguration it terminates and nally the auto-tuner starts the job with the best conìguration VII R ELATED W ORK Closest to our work are the previous research efforts on machine learning-based performance modeling and performance auto-tuning which we describe in turn 
19 


Performance Auto-tuning 
Previous studies have used different approaches to performance auto-tuning Cost-based approaches have been successfully used in query optimization in traditional database systems W ith this approach the cost model is the most important component of the optimizer as the accuracy of a cost model directly impacts the resulting performance Empirical auto-tuning which is based on an empirical search of the best parameter conìguration has been used in systems such as Atlas and PHiP A C 20 However one major drawback of this approach is the long search time for large search spaces which is usually the case in practice To auto-tune the performance of MapReduce applications Babu proposes a competition-based approach where two copies of the same task are started with different parameter conìgurations and the best conìguration is identiìed empirically Similarly Kambatla et al propose a history-based approach where the system keeps a history of job executions and similar jobs are executed with the same optimal conìguration Finally machine learning-based approaches have also been used successfully for auto-tuning the performance of parallel applications and data center applications Closest to our work is the Starìsh auto-tuner which relies on a cost-based performance model However building a cost model that is accurate robust and exible at the same time is challenging for a complex system such as Hadoop Although cost-based models provide deep insights into a systemês internals one of their main limitations is their exibility a change in the scheduling policies the Hadoop framework internals or the cluster requires the cost models to be rebuilt However our approach does not have this limitation because addressing such a change with our approach only requires the models to be retrained and model retraining is easier than building a new cost-model Moreover Starìsh only models the performance impact of job-level parameters mostly memory-related and it does not model various important cluster-level parameters such as the number of map and reduce slots Finally our ndings show that a machine learning-based approach yields comparable and in some cases better performance improvements than Starìshês cost-based approach VIII C ONCLUSION AND F UTURE W ORK In this paper we have explored various machine learningbased performance models for auto-tuning Hadoop MapReduce We have shown that support vector regression model SVR has good accuracy and computational performance across diverse workloads and clusters We have then compared our auto-tuning approach which uses the SVR performance model against the Starìsh auto-tuner which uses a cost-based model Our ndings reveal that the SVR model is able to achieve comparable and in some cases even better performance than Starìsh by considering less parameters Unlike the costbased approach our approach is more robust and exible as it is easier to adapt to changes our models learn the performance surface of the applications rather then being hardwired as is the case for the cost-based models Our results demonstrate that it is possible to build an effective auto-tuner with a black box approach that is by only using observations from the system and without getting exposed to its internals Finally we have also proposed a practical end-to-end auto-tuning ow by combining our models with smart search algorithms R EFERENCES  Apache Hadoop Project http://hadoop.apache.or g  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters 
Communications of the ACM CIDR Proc of the 2nd ACM Symposium on Cloud Computing SOCC Data Mining Practical Machine Learning Tools and Techniques Neural Netw Intl Conference on Data Engineering Workshops R A Language and Environment for Statistical Computing Proc of the GCC Developers Summit SIGPLAN Not Proc of the 11th Intl Euro-Par Conference on Parallel Processing Proc of the High Performance Computer Architecture HPCA Proc of the Intl Conference on Autonomic Computing Proc of the Conference on High Performance Computing Networking Storage and Analysis SC Proc of the 17th ACM symposium on Principles of database systems PODS Proc of the ACM/IEEE Conference on Supercomputing SC Proc of the 11th Intl Conference on Supercomputing ICS Proc of the 1st ACM symposium on Cloud computing Socc Proc of the Conference on Hot topics in cloud computing HotCloud 
 vol 51 no 1 pp 107 113 Jan 2008  F acebook hadoop and hi v e  2009 http://www dbms2.com/2009/05 11/facebook-hadoop-and-hive  H Herodotou H Lim G Luo N Boriso v  L Dong F  B Cetin and S Babu Starìsh A self-tuning system for big data analytics in  2011 pp 261Ö272  P  Bhatotia A W ieder  R Rodrigues U A Acar  and R P asquin Incoop Mapreduce for incremental computations in  2011 pp 1Ö14  7 tips for impro ving mapreduce perfor mance 2012 http://www.cloudera.com/blog/2009/12 7-tips-for-improving-mapreduce-performance  Optimizing hadoop deplo yments  2010 http://softw are.intel.com/ìle 31124  I H W itten and E Frank  2nd ed Morgan Kaufmann 2005  K Hornik  Approximation capabilities of multilayer feedforw ard networks  vol 4 no 2 pp 251Ö257 Mar 1991  S Huang J Huang J Dai T  Xie and B Huang The hibench benchmark suite Characterization of the mapreduce-based data analysis in  march 2010 pp 41 51  R De v elopment Core T eam  R Foundation for Statistical Computing Vienna Austria 2008 ISBN 3-900051-07-0 A v ailable http://www  R-project.org  G Fursin C Miranda O T emam M Namolaru E Y om-T o v  A Zaks B Mendelson E Bonilla J Thomson H Leather C Williams M OêBoyle P Barnard E Ashton E Courtois and F Bodin MILEPOST GCC machine learning based research compiler in  2008 A v ailable http://hal.inria.fr/inria-00294704/fr  J Ca v azos and M F  P  OêBo yle Method-speciìc dynamic compilation using logistic regression  vol 41 no 10 pp 229Ö240 Oct 2006  E Ipek B R de Supinski M Schulz and S A McK ee  An approach to performance prediction for parallel applications in  2005 pp 196Ö205  S K undu R Rangasw ami K Dutta and M Zhao  Application performance modeling in a virtualized environment in  2010 pp 1Ö10  J W ildstrom P  Stone and E W itchel Carv e A cogniti v e agent for resource value estimation in  ser ICAC 2008 pp 182Ö191  S.-w  Liao T H Hung D Nguyen C Chou C T u and H Zhou Machine learning-based prefetch optimization for data center applications in  2009 pp 56:1Ö56:10  S Chaudhuri  An o v ervie w of query optimization in relational systems in  1998 pp 34Ö43  R C Whale y and J J Dongarra  Automatically tuned linear algebra software in  1998 pp 1Ö27  J Bilmes K Asano vic C W  Chin and J Demmel Optimizing matrix multiply using PHiPAC a portable high-performance ANSI C coding methodology in  1997 pp 340Ö347  S Bab u T o w ards automatic optimization of mapreduce programs  in  2010 pp 137Ö142  K Kambatla A P athak and H Pucha T o w ards optimizing hadoop provisioning in the cloud in  2009 
20 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





