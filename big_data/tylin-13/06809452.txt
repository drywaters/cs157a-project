A Measurement Study of Data-intensive Network Traf“c Patterns in a Private Cloud Daniele Venzano Pietro Michiardi EURECOM  Sophia-Antipolis France Email rst name.last name@eurecom.fr 
Abstract 
In this work we investigate the impact of virtualization on the raw network performance attainable by dataintensive applications deployed in a private cloud To this end we developed a new software tool called OSMeF to take repeatable measurements on our OpenStack-based platform We also discuss the implications of our measurement results toward informed deployments of distributed applications such as Hadoop 
I I NTRODUCTION Nowadays cloud computing represents a key enabler for the rapid provisioning of resources from public or private datacenters to deploy a wide range of applications To provide exible and cost-effective resource sharing among users most cloud service providers rely on machine virtualization that support multiple instances of virtual machines VMs on the same physical server VM instances share physical processors and I/O interfaces with other instances as such it is important to understand the impact of virtualization on computation and communication performance of cloud services In this work we study the performance  from the networking perspective  that private cloud deployments expose to a speci“c class of applications namely data-intensive scalable 
computing frameworks such as Hadoop In this conte xt vir tualization brings many bene“ts including ease of operation high availability elasticity and multi-tenancy Ho we v er  only few studies attempted to measure rigorously the raw performance achievable by I/O intensive applications running on virtual compute clusters section II Thus our goal is to cast light on the 
attainable by a range of traf“c patterns derived from a typical Hadoop operation By using our OpenStack-based pri v ate cloud deplo yment detailed in section III and by leveraging a software tool we built the OpenStack Measurement Framework or OSMeF described in section IV we studied the behaviour of virtual and physical networks along with the loopback 
bulk transfer capacity 
mechanism that is heavily used by distributed applications like Hadoop Our measurement results can be used to inform the proper con“guration and tuning of OpenStack and Hadoop deployments including 
the selection of appropriate VM avors to run Hadoop components the importance of VM placement across available physical servers which largely determines the attainable network throughput of data transfers and the need for novel mechanisms to support fast crosstenant communications II R ELATED WORK Understanding the impact of virtualization on network performance has attracted several studies 5 in the past 
i ii iii 
albeit oriented toward public cloud providers These works use a black-box approach in their study as it is dif“cult if not impossible in some cases to determine the measurement conditions e.g interference from other users in their experiments not to mention knowledge of the physical servers and networks the cloud operator uses for the service Instead in our approach the platform used for the experiments is completely under our control and hardware and software con“gurations are known and measurable Other works focus especially on studying network performance from the hypervisor point of view for example studies  and 7 e xamine TCP and HTTP throughput respecti v ely  in Xen virtual machines In our work instead we use KVM 
the most commonly used hypervisor in conjunction with OpenStack Moreover we also study end-to-end throughput across the whole virtual network setup by OpenStack From the application perspective some recent works analyze the behavior of Hadoop when deployed in virtual machines In Han et al present a model of Hadoop to examine how performance could be affected by VM characteristics and placement in a cluster However that work focuses on application-level performance metrics only neglecting network-level ones Similarly the work in studies the performance of Hadoop deployed in an Amazon EC2 cluster albeit the focus is on the design of a more ef“cient scheduling mechanism to optimize application-level metrics Finally recent works 11 recognize the challenge 
of designing ef“cient virtual networks and propose software approaches to increase virtual network performance III E XPERIMENTAL SET UP We conduct our measurement study on a cluster of dedicated machines con“gured as a 
 using the OpenStack cloud operating system In this section we brie”y outline the design of OpenStack concentrating on its networking component  1 and provide details on our platform both from the hardware and software point of view OpenStack provides a range of management functionalities needed by a cloud provider from user and tenant 
OpenStack 
private cloud 
quantum 
 to interfacing with hypervisors   virtual networking   block   and object storage   All these software components are implemented in a distributed and fault-tolerant way with each service storing a minimal amount of state and communicating through a message queueing service 1 This component is called 
neutron 
keystone nova-compute quantum cinder swift 
management  in the current development branch 
2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing 978-0-7695-5152-4/13 $26.00 © 2013 IEEE DOI 10.1109/UCC.2013.93 476 
2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing CFP13UCC-USB/13 $26.00 © 2013 IEEE DOI 10.1109/UCC.2013.93 476 
2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing CFP13UCC-USB/13 $26.00 © 2013 IEEE DOI 10.1109/UCC.2013.93 476 


Con“guring and tuning OpenStack is complex the large number of parameters that govern system behavior the variety of hypervisor technologies the different avors of storage systems various lesystem and logical volume management combinations the number of alternatives to implement network switching GRE tunnels dynamic VLANs all play a crucial role in determining the overall system performance Our cluster uses a heterogeneous set of physical machines we have two nodes running on a dual quad-core Xeon L5320 server clocked at 1.86GHz with 16GB of RAM two 1TB hardware RAID5 volumes and two 1Gbps network interfaces nodes execute on six dual exa-core Xeon E5-2650L with hyperthreading enabled servers clocked at 1.8GHz with 128GB of RAM ten 1TB disks con“gured as JBOD and four 1Gb/s network cards In this work we use a single network interface per host as splitting traf“c or using bonding would have added complexity to the system that instead we strive to keep as simple as possible since in this work we are interested only in baseline performance The hardware and network con“guration closely resembles the one suggested by commercial private cloud providers such as Rackspace In particular storage is distrib uted on the master and compute hosts and is not concentrated on a separate storage network Each machine in the cluster runs the same Linux distribution a Ubuntu 12.04.2 LTS updated with the most recent patches All energy saving settings in the BIOS are disabled since they cause severe performance penalties We use the KVM hypervisor with and acceleration modules enabled Virtualization support in the CPUs is enabled VMX and KVM uses it automatically The hypervisor is con“gured by Nova to use LVM for VM storage VMs use the unmodi“ed Ubuntu 13.10 image from the Ubuntu Cloud archives We use the release of OpenStack which is installed via the Ubuntu cloud repository One of the master nodes runs the OpenStack management services the webbased dashboard console    and including the server layer 2/3 services and DHCP agents Worker nodes are con“gured as compute-only nodes and they host all the VMs created by our tenants and users Currently we con“gure to use GRE tunnels over a physical network that interconnects all nodes of our cluster We implemented the most common setup where is con“gured to use the OpenVSwitch O VS plugin to provide connectivity between VMs OVS is a software switch implementation that materializes as a virtual switch spanning across multiple physical hosts In our con“guration creates a single OVS switch for all VMs using VLAN tagging to separate traf“c from different tenants To provide connectivity between tenants and the external network our virtual network is con“gured according to the use-case described in the OpenStack documentation Thus each tenant has its own IP subnet and exchange traf“c between each other and the Internet using a single virtual router connected to the subnets of each tenant from one side and to the external network on the other side The virtual router is implemented as network namespace on the master node where a number of NAT and routing rules provide interconnection external access and oating IPs allocated to the VMs These settings are the result of a tedious trial and error process that lasted several months The OpenStack installation manuals only cover the basics to setup a system that is mostly operational but far from being optimized for performance IV M EASUREMENT METHODOLOGY We now describe and motivate the methodology used for our measurements the tools we used and the performance metrics we considered for our results We consider data-intensive scalable computing applications and in particular we focus on the Hadoop framework Hence we are interested in studying the network performance of traf“c patterns that mimic 2 those generated by a typical data analysis task In addition we consider several avors of Hadoop deployments that exploit the characteristics of virtualization in the context of multi-tenant shared clouds Next we brie”y describe the basic principles of Hadoop MapReduce MapReduce popularized by Google and by Hadoop is both a programming model and an e x ecution framework In MapReduce a data analysis job consists of three phases and accepts as input a dataset appropriately partitioned and stored in a  In the rst phase called M AP  a user-de“ned function is applied in parallel to input partitions to produce intermediate data stored on the local le system of each machine of the cluster intermediate data is sorted and partitioned when written to disk Then a R EDUCE phase begins It comprises a S HUFFLE phase where intermediate data is pulled by the  data from multiple mappers is sorted and aggregated to produce output data When a job is submitted to an Hadoop cluster the Hadoop scheduler assigns a number of M AP tasks equal to the number of partitions of the input data The scheduler tries to assign M AP tasks to slots available on machines in which the underlying storage layer holds the input intended to be processed an important concept called  Essentially when reading and writing data local communications are to be preferred over remote ones since the network is generally assumed to be the bottleneck R EDUCE tasks are scheduled once intermediate data output from mappers is available Overall the performance of an individual job is determined by the slowest task asymmetries due to an uneven amount of data to process or to network bottlenecks may create stragglers that should be carefully handled by the application As a concluding remark it is important to notice that Hadoop is made of a number of components that run as daemons nodes execute daemons e.g the scheduler that orchestrate several workers each in charge of and operations For the principle of data locality outlined above data and compute components are collocated on the 2 Note that in this work we do not execute Hadoop in our cluster hence we do not measure communication performance at the application level Rather we generate traf“c with system-level tools according to the communication patterns of Hadoop 
Our Platform 
master worker Grizzly provider router with private networks A Motivation and background distributed le system reducers single data locality master data compute 
virtio vhost_net cinder glance keystone quantum quantum quantum quantum quantum 
477 
477 
477 


same cluster machine All components communicate through a simple REST-ful API in particular co-located components exchange messages through the loopback interface whereas components on different hosts use network interfaces The nature of the communications between Hadoop components discussed above leads us to focus on the performance achieved by both regular network and loopback interfaces the former being used mainly during the S HUFFLE phase of MapReduce and the latter used mainly for data I/O operations As such in section V we measure the performance of the  both at the hypervisor and VM level  and of the  between hypervisors and VMs Recall the data-intensive nature of the applications we consider therefore we study the behavior of in all our experiments we consider 6 minutes long data transfers that are typical in Hadoop For example in the S HUFFLE phase it is common to transfer several GBytes of data among cluster machines Moreover from a measurement point of view long-lived connections have the advantage of reducing the variance caused by load spik es in netw ork and CPU usage in the cluster Finally we consider the effects of a number of taking place concurrently this is another typical traf“c pattern of Hadoop whereby each component could establish a large number of connections to exchange data 3 Note that it is also important to study whether bandwidth allocation among competing parallel connections is fair as uneven performance may contribute to the creation of stragglers and impact the overall application-level performance of a data analysis job Deploying data-intensive applications in a cluster of virtual machines involves a wide range of possible architectural choices As we alluded above in a typical bare-metal Hadoop architecture and components are co-located in light of the data locality principle Similarly with respect to the traf“c patterns we consider a virtual Hadoop cluster in which individual VMs host both components In addition we also study more elaborate setups that stem from recent efforts to de“ne reference Hadoop architectures for multi-tenant shared clusters of virtualized resources Essentially we consider traf“c patterns that arise when and components live in different VMs and possibly in different tenants In our experiments we therefore study network performance under a variety of VMto-VM communication patterns in doing so we revisit the notion of data locality and de“ne a new distance metric that accounts for choices For example we consider communications to be local even when they involve distinct VMs as long as they are instantiated on the same physical host Finally we also study the effects on network performance of a variety of VM avors namely we focus on the number 3 For example during the S HUFFLE phase a T ASK T RACKER serves up to 40 connections and establishes up to 5 connections to pull intermediate data of virtual CPUs available to a VM Indeed the CPU plays a crucial role in determining the performance of some software network components we use in our platform including the loopback interface and the virtual switches As such we use VMs with 1 up to 16 virtual CPUs The OpenStack Measurement Framework OSMeF provides a way to perform a large number of measurements in an automatic and reproducible way It is implemented in Python and uses a JSON output format We released it as an opensource project and we plan to e xtend it to co v er more scenarios and measurements OSMeF implements the OpenStack and Quantum APIs which are used to instantiate and delete VMs and virtual interfaces as required by the speci“c measurement scenario being performed Essentially OSMeF reads a measurement con“guration le performs VM placement according to the speci“cation including a proper selection of VM avor instruments all VMs with additional software components required to perform a speci“c measurement and nally runs the necessary tools parsing and aggregating their outputs to create a compact JSON summary for each experiment In addition before any experiment is executed OSMeF gathers a number of statistics from each end point operating system which are required to verify that measurements are performed under known system conditions These statistics include a time-stamp the current load on each machine both physical and virtual network utilization kernel versions and many others Such experiment meta-data is appended to the JSON output of each measurement campaign Currently OSMeF uses  to perform the actual network measurements 4 is a well-known tool originated from  that produces a number of useful statistics in addition to raw throughput gures gathered during a TCP transfer of a speci“ed length For each experiment OSMeF con“gures according to the measurement con“guration le including the number of parallel connections and their duration The main performance metric we measure in our experiments is related to network throughput indeed the very nature of the data-intensive applications we consider in this work is geared toward moving and ingesting large volumes of data rather than guaranteeing low-latency access to small records In addition to throughput we also consider a metric related to the fairness of bandwidth allocation across competing parallel connections Next we describe our metrics in detail BTC is de“ned as the rate that a transmitting entity implementing a standard congestion control mechanism can attain over a given network path  Throughout this paper  we measure BTC as the a v erage throughput generated during 6-minute TCP transfers between two physical or virtual network interfaces 4 We also used 
B Traf“c patterns localhost cluster network long-lived connections parallel communications compute data compute data VM placement C OSMeF nuttcp Nuttcp ttcp nuttcp D Performance metrics 
Application-level characteristics Virtualization effects Bulk Transfer Capacity BTC 
as an alternative to  and preferred the latter for its verbosity 
IPerf nuttcp 
478 
478 
478 


   
 
002\003\004\005\006\007\010\011\004\012\006\005\013\004   014\015\003\005\016\017\013\020 021\013\022\007\022\005\011\023 021\013\022\007\022\005\011\024 002\025\026 002\025\027 002\025\030 002\025\031   032\033\034\026 032\033\034\027 032\033\034\030 
Fig 1 The distance between two VMs is equal to the number of edges data must traverse in this tree Distance between VM1 and VM2 is 2 and distance between VM2 and VM3 is 4 even if they reside on the same physical host Processes on the same VM not shown communicate with distance 0 Jains index is a well-kno wn fairness metric used to establish how equally a network resource is being shared When J is equal to or approximately 1 then all connections are treated equally in that they receive roughly the same amount of bandwidth Instead if J is less than 1 some connections are mistreated with respect to others which creates an asymmetry that may severely impact application-level performance see section IV-B These metrics are enriched by two important elements We borrow the de“nition of distance between communicating entities from that used in Hadoop to tak e informed decisions on data and compute tasks placement Logically the network is represented as a tree and the distance between two nodes is the sum of the distances to their closest common ancestor As can be seen in gure 1 this de“nition takes into account OpenStacks virtual topology Our de“nition of CPU load refers to the percentage of time spent running by a receiving or sending UNIX process Indeed CPU speed and memory bus bandwidth are important factors that limit the throughput achieved by a local i.e distance equals zero communication This metric is measured by and reported by OSMeF on a per-connection basis V R ESULTS We now present our results in terms of the metrics de“ned in section IV-D that we obtained with more than 250 OSMeF runs using various combinations of the traf“c patterns discussed in section IV-B We consider the following scenarios performance of an individual host or VM with avors in the range from 1 to 16 virtual CPUs performance which provides a physical baseline for the network performance performance 5 and the following VM placements across physical servers the same convention is used in the remainder of the paper 1 Same host and same tenant 5 In this work we present results for a 1 virtual CPU VM since results for different VM avors are qualitatively similar  Fig 2 Loopback BTC comparison between a physical host with 16 cores 32 with hyperthreading and VMs with 1 to 16 virtual cores 2 Different host and same tenant 3 Same host and different tenant 4 Different host and different tenant In addition to the scenarios presented above we also focus on speci“c characteristics of our cluster con“guration and on the OpenStack/KVM implementation of virtual networking our goal is to characterize in a ne-grained way the path taken by a packet from its source to the destination and pinpoint potential bottlenecks in the system Precisely we perform the following additional set of measurements and  these traf“c patterns allow to measure the capacity available through the virtualization layer that is between a network interface inside the VM and the corresponding TAP interface managed by the hypervisor through a GRE tunnel GRE is a IP-over-IP tunneling technique used by OVS to instantiate a single switch spanning multiple physical hosts All results are produced by averaging three to ve OSMeF runs of the same scenario varying the number of parallel connections between end-points from 1 to 50 with each connection having a xed duration of 6 minutes Note that in our experiments OSMeF is the only active user in the cluster we thus eliminate interference due to multiple applications and background traf“c in the system a necessary condition in obtaining baseline results Armed with the motivations discussed in section IV-B we now focus on the behavior of the loopback interface We use OSMeF to establish a variable number of parallel concurrent connections with both end processes client and server running in the same host or VM using the loopback interface to exchange data Figure 2 illustrates the BTC we measure for both the physical and virtual interfaces computed as the sum of the individual BTC each connection achieves in our measurements In particular for each individual connection we compute the average across 5 distinct measurement runs Figure 2 reports 
Jains fairness index J Distance CPU load Physical  Virtual loopback Host-to-Host VM-to-VM VM-to-Host Host-to-VM Host-to-Host aggregate 
nuttcp A Loopback Performance 
0  10  20  30  40  50 Loopback concurrent connections  0  5000  10000  15000  20000  25000 BTC in MB/s Loopback BTC 16 CPUs 16 VCPUs 8 VCPUs 4 VCPUs 1 VCPU 
479 
479 
479 


y 
virtual loopback per-connection 
        
the aggregate BTC as a function of the number of parallel connections and each line in the gure corresponds to a different VM avor We observe that both the physical and virtual loopback interfaces share similar characteristics the aggregate BTC increases as more connections are established up to a plateau at which BTC saturates Clearly the loopback behavior is dominated by the complex interplay between several components the physical or virtual CPU and the number of cores the speed at which data can be copied in RAM and the operating system scheduler While loopback performance is quite obviously dominated by the CPU load it is important to note that BTC has a point of maximum where the number of connections maximises the amount of resources available After this point available bandwidth attens or even starts to decrease Another important observation is about the performance the number of parallel connections that saturate BTC is proportional to the number of virtual cores with the exception of the 1 VCPU slope which does not cope well with parallel connections In particular BTC doubles by doubling the number of VCPUs available but comparing the 16 virtual CPUs throughput with the 16 physical CPUs one we see a 40 penalty due to virtualization With additional measurements we observe that despite the negligible VCPU load under 1 the hypervisor CPU is saturated with VM processes occupying more than 90 of CPU time For each case discussed above we computed the Jains fairness index as a function of the number of parallel connections established by OSMeF for different VM avors Our data indicates that for long-lived communications each connection receives a fair share of the available bandwidth to transfer data with values very close to 1 independently of the number of connections and across different VM avors In summary the analysis of the loopback behavior allows to draw a number of conclusions It is important to properly chose a VM avor that supports and scales well with the number of parallel connections required by the application Con“guring and tuning data-intensive applications such as Hadoop requires an in-depth knowledge of virtualization impact to avoid saturation by limiting the number of parallel tasks that establish connections through the loopback interface and to ensure that available capacity is not left unused For speci“c traf“c patterns such as the S HUFFLE phase of Hadoop it is important to understand and quantify asymmetries due to the use of the loopback interface  whose behavior is heavily in”uenced by CPU load  with respect to other network interfaces We now study the behavior of VM-to-VM traf“c patterns for this OSMeF establishes a variable number of connections between different VMs varying their placement each hosting one end client or server of the connection In addition to VM measurements we use OSMeF to perform a series of  Fig 3 BTC behaviour of VM to VM communication with increasing distances and number of parallel connections p For distance 4 we chose to plot scenario 3 same host different tenant Please note the log scale on the axis measurements that involve the physical host to upper-bound the BTC for VM-to-VM communications This is a summary of our results refer to the beginning of this section for placement descriptions VM-to-VM 1 in this case the BTC is upper-bounded by the minimum of the available capacity between the VM and the underlying hypervisor corresponding to VM-to-Host and Host-to-VM traf“c patterns VM-to-VM 2 the capacity of the physical network is the bottleneck in this case with BTC upper-bounded by the Host-to-Host traf“c patterns VM-to-VM 3 and 4 given the platform con“guration we use suggested in the BTC for both traf c patterns we study is upper bounded by the physical network even when VMs run in the same physical host Complete results with CPU usage statistics and standard deviations for all scenarios are available in a technical report  Next we study the behavior of VM-to-VM communications as a function of the distance we de“ne in section IV-D Figure 3 illustrates the BTC computed as the average BTC each connections achieves in 5 consecutive runs of our measurements where each line is representative of a measurement campaign with a different number of parallel connections In summary we observe that Distance 0 the physical network is not involved in this case since all communications are established within the same physical host As anticipated above the bottleneck that determines the overall performance of this traf“c pattern is related to VM-to-Host and Hostto-VM communications which suffer from overheads due to network virtualization Distance 2 in this case the physical network is the bottleneck Virtualization overheads are low as the loss in BTC for this traf“c pattern is roughly 4 with 
B VM-to-VM performance 
0  2  4 Distance  10 0  10 1  10 2  10 3  10 4 BTC in MB/s VM to VM BTC p=1 p=10 p=30 p=50 
480 
480 
480 


Proc of INFOCOM10 Usenix Login et al Proc of VTDC 06 et al Proc of CLOUD 2010 Proc of ICOIN 13 et al Proc of OSDI 08 Proc of ACM CoNEXT 12 Proc of OSDI 04 SIGCOMM Comput Commun Rev et al IEEE Network Proc of 51st Internet Engineering Task Fo rc e CoRR Hadoop The De“nitive Guide Proc of CloudNet13 
 IEEE Press pp 1163…1171  E W alk er  Benchmarking amazon ec2 for high-performance scienti“c computing  vol 33 no 5 pp 18…23 2008  P  Apparao  Characterization of network processing overheads in xen in  Washington DC USA 2006  Y  Mei  Performance measurements and analysis of network i/o applications in virtualized cloud in  IEEE Press 2010 pp 59…66  J Han H Makino and M Ishii Design and performance e v aluation for hadoop clusters on virtualized environment in  IEEE 2013 pp 244…249  M Zaharia  Improving mapreduce performance in heterogeneous environments in  Berkeley CA USA 2008  L Rizzo and G Lettieri V ale a switched ethernet for virtual machines in  2012 pp 61…72  D Crisan R Birk e G Cressier  C Mink enber g and M Gusat Got loss get zovn IBM Research Tech Rep RZ 3840 March 2013  Rackspace Rackspace pri v ate cloud installation manual   A v ailable http://www rackspace.com/kno wledge center article/rackspace-private-cloud-installation-prerequisites-and-concepts  Open vswitch  Online A v ailable http://open vswitch.or g  OpenStack F oundation Openstack netw orking administration guide  grizzly 2013.1 A v ailable http://docs.openstack.or g/grizzly openstack-network/admin/content/use cases single router.html  J Dean and S Ghema w at MapReduce Simpli“ed data processing on large clusters in  2004 pp 107…113  M Jain and C Do vrolis End-to-end a v ailable bandwidth measurement methodology dynamics and relation with TCP throughput  no 4 2002  D V enzano Osmef release  Online A v ailable https://github com bigfootproject/OSMEF  Nuttcp  Online A v ailable http://www nuttcp.net  R Prosad  Bandwidth estimation metrics measurement techniques and tools  no 6 2003  M Mathis and M Allman  A frame w ork for de“ning empirical b ulk transfer capacity metrics in  IETF 2001  R Jain D.-M Chiu and W  Ha we  A quantitati v e measure of f airness and discrimination for resource allocation in shared computer systems  vol cs.NI/9809099 1998  T  White  2nd ed OReilly and Yahoo Press October 2010  D V enzano and P  Michiardi Netw ork performance measurements of data-intensive applications in a private cloud Eurecom Tech Rep RR-13-287  G Urv o y-K eller  D M L P acheco and H S Ha Netw orking in a virtualized environment the TCP case in  IEEE 
 
VM placement host-locality 
respect to the physical upper-bound Additionally we remark that VM-to-VM communications at distance 2 achieve a BTC that is one order of magnitude lower than what can be obtained at distance 0 Distance 4 also in this case the physical network constitutes the bottleneck for the attainable BTC even for connections between VMs in the same physical host We note a further drop in performance as compared to distance 2 due to routing overheads to move data between different tenants Overall the results we show in Figure 3 indicate that the total network capacity is shared consistently among competing connections a single connections uses virtually all of the available network capacity whereas on average for example 10 connections receive roughly 1/10-th of the total capacity We computed the Jains fairness index for VM-to-VM traf“c patterns as a function of the number of connections Our results indicate that each long-lived connection receives a fair share of the available capacity Interestingly we notice that this is not the case for Host-to-Host communications in this case the hypervisor operating system does not distribute evenly the network capacity among competing ows a result that is corroborated also by some recent works In addition our results indicate a 2 performance loss of Hostto-Host communication through a GRE tunnel which imposes a 15 overhead in CPU utilization In summary our results cast light on the impact of network and system virtualization for the applications we consider in our work plays a crucial role in determining application-level performance For S HUFFLE like traf“c patterns sub-optimal VM placement might contribute to the creation of stragglers due to the inherent asymmetry of the BTC attainable between VMs depending on their distance Our results can thus be used to inform VM placement strategies to cope with application requirements Moreover we observe that the architecture suggested in whereby data-intensive applications are deployed by separating across different tenants and layers may suffer from a severe performance degradation The cluster con“guration we used in our experiments  which follows OpenStack guidelines  is inappropriate for inter-tenant communications that occur on the same physical host It is thus necessary to devise new mechanisms that exploit to improve network performance and avoid unnecessary traf“c routing VI C ONCLUSIONS AND FUTURE WORK Understanding the consequences of machine and more generally cluster virtualization on communication performance of cloud applications is fundamental for their correct operation This is particularly true for data-intensive computing where I/O performance plays a crucial role in determining the overall rate at which data analysis tasks can proceed In this work we described our measurement results and their implications in light of an appropriate approach to deploy and tune data-intensive applications Essentially this work helps to inform the design of mechanisms to perform VM placement on physical servers and to appropriately tune communication parameters of Hadoop-like applications Currently we are extending the traf“c patterns supported by our measurement tool for example to account for nway communications and we plan to run a range of new measurement campaigns to understand and quantify the impact of interference caused by background traf“c and multiple coexisting applications running in our platform In addition we will complement our study by considering the impact of virtualization on disk I/O performance R EFERENCES  Apache Hadoop  Online A v ailable http://hadoop.apache.or g  VMW are Hadoop virtualization e xtensions on VMw are vSphere 5 A v ailable http://www vmw are.com/“les/pdf Hadoop-Virtualization-Extensions-on-VMware-vSphere-5.pdf  OpenStack F oundation OpenStack  Online A v ailable http openstack.org  G W ang and T  S E Ng The impact of virtualization on netw ork performance of amazon ec2 data center in 
data compute 
481 
481 
481 


  7 28 GHz or 19-30 GHz can also work as long as the front end is mixing down to those IF frequencies and the channel spacing is uniform 20 25 30 15 50 40 30 20 10 60 0 freq, GHz dB\(S\(2,1   Figure 10 Individual response of 9-channel HyMAS IF filters  Insertion loss ranged from 2.2 dB in channel 1 at the IF low end frequency of 18.65 GHz \(channel 1\o 5.4 dB at 28.9 GHz \(channel 9, the high end\. Bandwidths ranged from 550-650 MHz, with return loss in the respective pass bands always greater than 10 dB 16 17 18 19 20 21 22 23 24 25 26 27 28 29 15 30 25 20 15 10 5 0 5 30 10 freq, GHz dB\(S\(1,1   Figure 11: HyMAS 9-channel Prototype Return Loss Channels 1-9 \(>10 dB over each respective passband  Filter resonator unloaded Q \(Q u as estimated from the relationship between filter insertion loss \(IL\nator unloaded Q, fractional bandwidth d the low-pass filter parameters \(g k 222s  003  n k k u g Q IL 1 343.4  Measured Q u 250 was less than simulated \(Q u 600\t still significantly better than microstrip or stripline coupled line filters \(typical Q u 100-150\inally, inter-channel isolation was > 35 dB between channels 3 and 5, 5 and 7 and 7 and 9 as measured between adjacent filters. Isolation improvement is expected with the vertical filter design. The 52-channel HyMAS IFP will be prototyped and tested in August 2013  6  S UMMARY  Accommodation  for the  HyMAS IF frequency of  18-29 GHz has been planned throughout the HyMAS system Careful frequency planning selection of components and component placement has improved the chances that the LTCC implementation of hyperspectral filtering can function in a tight package. The CoSMIR and CoSSIR teams at Goddard have had great success with existing designs but the instruments are undergoing a data system upgrade, and see the potential of the miniaturized filter bank. The preliminary results of the 9-channel prototype filterbank developed by MIT Lincoln Labs is very promising and the implementation of more compact vertical filter structure will likely improve that performance on the flight build expected in August 2013. Further integration into the HyMAS scanhead , instrument, and NASA ER-2 is being planned for summer of 2014  7  A CKNOWLEDGEMENT  The hyperspectral microwave receiver system will be integrated into a new scanhead compatible with the NASA GSFC Conical Scanning Microwave Imaging Radiometer/Compact Submillimeter-wave Imaging Radiometer \(CoSMIR/CoSSIR\ airborne instrument system to facilitate demonstration and performance characterization under funding from the NASA ESTO Advanced Component Technology program We would also like to acknowledge the Global Precipitation Measurement \(GPM\ Mission which is investing in the adaptation of IRC to the aircraft instruments required for ground calibration and validation. The G-Band LNA work is underway at Goddard Space Flight Center thanks to internal Research and Development funding and previous investment in Small Business Innovative Research The LTCC filter development work was originally sponsored by the National Oceanographic and Atmospheric Administration under Air Force Contract FA8721-05-C0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government 


  8 R EFERENCES  1  Su tto n  M L  H ill iar d P  Ra cette, N S h u r E. S h e y b a n i    G. Javidi, A. Eslami,  J.Luttamaguzi, D. Djavadi 223Computer Aided Designs of Hyperspectral Microwave Atmospheric Sounder \(HyMAS\Scanhead\224, Summer Internship Poster, July 2012, Goddard Space Flight Center, Greenbelt MD 2  P i ep m e ier  J   Rac e tte, P  W a n g J   Cr ite s A   Do ir o n T   Engler, C.; Lecha, J.; Powers, M.; Simon, E.; Triesky, M Krebs, Carolyn , \223An Airborne Conical Scanning Millimeter-wave Imaging Radiometer \(CoSMIR\ \223 NASA Technical Reports Server NASA Center: Goddard Space Flight Center, Publication Year: 2001,Document ID: 20010091686  e ll, W.J Galbra it h, C Ha n c oc k T L e s l ie R  Osaretin, I. ,  Shields, M. ,  Racette, P. ,  Hilliard, L 223Design and Analysis of a Hyperspectral Microwave receiver subsystem\224, IEEE-IGARSS, 22-27 July. 2012 Munich; Germany  Bl ack w e l l H y p e rs pec t ral  M i cr o w a v e At m o s p h e ri c Sounding, IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 49, NO. 1, JANUARY 2011  Hos l er, T r o y J an d C  F. Hos t et ter ,  Ins t r u m e n t  R e m o te Control Application Framework, SpaceOps Conference 19-23 Jun. 2006; Rome; Italy 6  W illia m s J   L  Hi lliar d J  B e ck J  Ho s l er G Cr u m E Sheybani,  G. Javidi, A. Eslami,  J.Luttamaguzi, D Djavadi ,\224 Hyperspectral Microwave Atmospheric Sounder Emulator\224, Summer Internship Poster, July 2012 Goddard Space Flight Center, Greenbelt MD B IOGRAPHIES  Lawrence M. Hilliard received a B.S. in Electrical Engineering from Michigan Technological University in 1983. He has been with NASA GSFC for more than 30 years. He is currently development lead for VLBI2010 in the Space Geodesy Project \(SGP\ at Goddard. Prior to SGP, he led an AESMIR aircraft instrument team winning a Robert Goddard Exceptional Achievement Award. He has also managed efforts for COBE-DMR, SWAS, and Swift-BAT instrument and payload development and has been involved in the formulation and development of numerous aircraft and Earth-orbiting spacecraft and payloads. His career started with parts engineering on the GOES program for NASA Goddard   William J. Blackwell received the B.E.E degree in electrical engineering from the Georgia Institute of Technology, A tlanta, GA, in 1994 and the S.M. and Sc.D. degrees in electrical engineering and computer science from the Massachusetts Institute of Technology \(MIT\, Cambridge, MA, in 1995 and 2002 Since 2002, he has worked at MIT Lincoln Laboratory where he is currently an Assistant Leader of the Sensor Technology and System Applications Group. His primary research interests are in the area of atmospheric remote sensing, including the d evelopment and calibration of airborne and spaceborne microwave and hyperspectral infrared sensors, the retrieval of geophysical products from remote radiance measurements, and the application of electromagnetic, signal processing and estimation theory. Dr. Blackwell held a National Science Foundation Graduate Research Fellowship from 1994 to 1997 and is a member of Tau Beta Pi, Eta Kappa Nu, Phi Kappa Phi Sigma Xi, the American Meteorological Society, the American Geophysical Union, and Commission F of the International Union of Radio Science.\302  He is currently an Associate Editor of the IEEE Transactions on Geoscience and Remote Sensing and the IEEE GRSS Newsletter. He is Chair of the IEEE GRSS Frequency Allocations for Remote Sensing \(FARS\ technical committee, the IEEE GRSS Remo te Sensing Instruments and Technologies for Small Sat ellites worki ng group, and the Boston Section of the IEEE GRSS and serves on the 


  9 NASA AIRS and Suomi NPP science teams and the NPOESS Sounding Operational Algorithm Team. He is the Principal Investigator on the MicroMAS \(Micro-sized Microwave Atmospheric Satellite\ program, comprising a high-performance passive microwave spectrometer hosted on a 3U cubesat planned for launch in 2013.\302  He was previously the Integrated Program Office Sensor Scientist for the Advanced Technology Microwave Sounder on the Suomi National Polar Partnership that launched in 2011 and the Atmospheric Algorithm Development Team Leader for the NPOESS Microwave Imager/Sounder. Dr. Blackwell received the 2009 NOAA David Johnson Award for his work in neural network retrievals and microwave calibration and is co-author of Neural Networks in Atmospheric Remote Sensing published by Artech House in July, 2009.\302  He received a poster award at the 12th Specialist Meeting on Microwave Radiometry and Remote Sensing of the Environment in March 2012 for ``Design and Analysis of a Hyperspectral Microwave Receiver Subsystem'' and was selected as a 2012 recipient of the IEEE Region 1 Managerial Excellence in an Engineering Organization Award ``for outstanding leadership of the multidisciplinary technical team developing innovative future microwave remote sensing systems  Paul E. Racette has been the principal engineer responsible for the overall instrument concept development and deployment of highly-innovative remote sensing instruments. Each of these instruments has produced unique, scientifically rich data. Paul has participated in more than fifteen major field experiments around the world pioneering techniques to observe the Earth. As a member of the senior technical staff at Goddard, he has initiated technology developments research projects, and international collaborations that have advanced the state of th e art in microwave remote sensing and instrument calibration. For these efforts and accomplishments Paul recei ved the NASA Medal for Exceptional Service and was the first recipient of Goddard\222s Engineering Achievement Award established to publicly recognize Goddard\222s highest achieving engineers. In 2005 he completed the requirements for his Doctor of Science in elect rical engineering from The George Washington Universi ty. Recognizing the critical needs in education and a desire to seek new adventures Paul applied and was accepted into the NASA Administrator\222s Fellowship Program. As a NAFP fellow he returned to his home state to serve as a guest faculty at the Haskell Indian Nations University during the 2005 \226 2006 academic year Paul recently completed the s econd year of his fellowship working at NASA Headquarters as Special Assistant to the Deputy Assistant Administrator in the Office of Education  Paul is highly commited to serving the public through professional activities. Paul has served the IEEE in many capacities including secretary of the University of Kansas\222 IEEE student chapter, the Geoscience and Remote Sensing Society\222s New Technology Directions Committee Representative, Chair of the Instrumentation and Future Technologies Committee, and Professional Activities Committee for Engineers Representative. He now serves as Editor-In-Chief for Earthzine  Christopher J. Galbraith is a member of the Technical Staff at MIT Lincoln Laboratory in the RF and Quantum System s group where he develops microwave circuits for communications, radar, and radiometric systems, small form-factor packaging and antennas, and superconducting electronics  He  received the B.S.E.E., M.S.E.E. and Ph.D degrees from the University of Michigan, Ann Arbor. During the summers of 2001 and 2002, he was an intern with TRW Space and Electronics, Redondo Beach, CA, where he worked on satellite communications syst ems and microwave circuit design. He is active in the IEEE Microwave Theory and Techniques society \(MTT-S\ where he currently serves as the chair of the Boston chapter   Erik Thompson Assistant Staff at MIT Lincoln Laboratory. He r eceived a B.E. in Electrical Engineering from Stevens Ins titute of Technology. As a Stevens student T hompson was selected as the Cooperative Education and Internship Student of the Year award by the New Jersey Cooperative Education and Internship Association \(NJCEIA As an undergraduate at Stevens, Thompson took part in five Co-op internships. The first two assignments were with Datascope Patient Monitors, where he worked with the electrical engineering staff to test hospital products and implement fixes. Next, he worked as a computer engineer at the Armament Research, Development and 


  10 Engineering Center \(ARDEC\ at Picatinny Arsensal Finally, Thompson spent two semesters at Safe Flight Instrument Corporation. There, he served as project lead for the development of co ckpit sensors that prevent airplanes from stalling. He was primarily responsible for overseeing the design and testing of software and electronics systems    


  11  


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440…442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


