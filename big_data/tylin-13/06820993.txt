Abstract 
002\003 003 004 003 002 003 004 
Migration Cost Aware Mitigating Hot Nodes in the Cloud 
Li Deng  Hai Jin  Huacai Chen and Song Wu 
Cloud enables dynamic resource sharing among different applications Live virtual machine VM migration 
School of Computer Science and Technology Wuhan University of Science and Technology Wuhan 430065 China Email dengli@wust.edu.cn School of Computer Science and Technology Huazhong University of Science and Technology Wuhan 430074 China Jiangsu Lemote Technology Corporation Limited Changshu 215500 China 
is an important way to reconìgure workloads in the Cloud However reconìguration cost using live VM migration is always ignored In fact migration cost may vary signiìcantly for different workloads due to diverse characteristics In this paper we present a quick and efìcient method to make migration decisions to mitigate hot nodes The process of live VM migration is theoretically analysed A quantitative model is gured out to estimate migration costs for applications We implement system prototype based on Xen 3.2.0 The experimental results show that our approach can fast mitigate hot nodes in the Cloud and effectively reduce migration cost 
I I NTRODUCTION Virtual machine VM technology has recently emerged as an essential building-block for data centers and cluster systems mainly due to its capabilities of isolating consolidating  and migrating w orkload 3 Cloud computing 
 enables dynamic resource sharing and pro vides e xibility easy and simple management Virtualization technology can dynamically allocate resource among services Resource can be allocated based on services real dynamic demands rather than their peak values When resource demands of all the VMs resided on node 002 003 004 005 
exceed the amount of resource that the node provides the node is regarded as heavy-load node or hot node We formulate it to be the following formulae 
002 003 
002 003 004 005 
002 
i i 
002 003\002 003\004\005 
006 003 004 005 006 
002 003 004\005 004\005 002 003 002 003 004\005 
002 006 007\003 007 002\002\002 007\010 006 007 002\002\002 007\011 002 003 004 005 006 007\003 007 002\002\002 007\010 006 
002 
002 003\002 003\004\005 
002 006 002 006 
004\005 002 003 002 003 002 003 002 
007 002\002\002 007\011 002 003 003 002 003 003 005 006 006 005 
In the above formulae the deìnitions of variables are described as the following  the amount of processor that node provides  the amount of memory that node provides  the amount of processor that VM requests 
006 
006 012 003 006 
 the amount of memory that VM requests if VM does NOT reside on node if VM resides on node When some nodes become hot due to dynamic resource demands live migration of VM is an important method to reallocate resource If not to be explained especially VM migration means live migration by default in this paper Precopy is employed to implement virtual machine migration in some mainstream virtualization software such as Xen 
003 002 003\002 003\004 006 005 
003 
j i j i 
 VMw are 6 It rst transfers all memory pages and then iteratively copies pages just modiìed during the last round VM service downtime is expected to be minimal by iterative copy operations However live migration of VMs would bring additional resource consume such as processor memory and network bandwidth As shown in Fig 1 our tests on Xen show that the performance degradation would happen during VM migration in both modes of Xen adaptive mode and non-adaptive mode Apache server is deployed in a VM conìgured with 4 VCPUs and 1GB RAM When VM migrates between two nodes command 
ab 
is used to access a 512KB static web page The number of concurrent access is set to 100 and the total number of requests is 50 million The results in Fig 1 tell us that VMs with low cost should be choosed to be 
migrated Fig 2 gives the relationship between migration time and memory size of VMs 
RUBiS tbench mem et al 
 an open-source auction site benchmark is run on LAMP server  is an open source benchmark testing disk I/O througput just like NetBench is memory-intensive Apache application There is a linear ascending relationship between migration time and memory size Diverse applications with the same memory size have different migration time Using Web 2.0 applications Voorsluys   analyze the inîuences of migration process on application performance and give some advices how to deploy applications 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2829-3/13 $26.00 © 2013 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.72 197 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2830-9/14 $31.00 © 2014 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.72 197 


0 10 20 30 40 50 256 1024 2048 3072 Total Migration Time \(s Memory Size of Migrated VMs \(MB  tbench    mem  RUBiS    
002\007 004\010 011\012\013 002\014\015\002\016 012\014\015\002\016 012\017\020 021\022\023 
et al et al et al pre-copy A Migration Process Based on Pre-copy Approach pre-migration reservation pre-copy stop-and-copy submission activation 
Fig 1 The inîuence of pre-copy approach on web service during VM migration in Xen Fig 2 VM migration time changes with the increase of memory size on cloud computing Entropy resource manager b uilts a VM migration cost model based on the amount of memory allocated to the migrated VMs It is observed that the duration of migration process mostly depends on the amount of memory used by the migrated VM However memory size is not the only factor which affects migration cost Based on pre-copy approach of migration sub-system in Xen Akoush  present a VM migration model to predict total migration time and downtime of VM migration Tarighi  design a migration scheduling algorithm  Technique for order preference by similarity to ideal solution TOPSIS based on fuzzy theory which makes effective choices under some uncertain conditions Liu propose a performance and energy modeling for live migration of virtual machines according to virtual machine conìgurations and workload characteristics The y quantitati v ely predict migration performance and energy cost based on previous migration behavior characteristics In this paper we propose a quick and efìcient method to make migration decisions to mitigate hot nodes in the Cloud The process of live VM migration is theoretically analysed A quantitative model is gured out to estimate migration costs for a kind of VM applications Based on Xen 3.2.0 we present the modiìed version of Xen to implement our system prototype The prototype can track page dirtying inside virtual machines on real-time The contributions of the paper are summarized as follows 1 we theoretically analyse the process of live VM migration and give a quantitative model of migration costs for a kind of VM applications 2 we design and implement a prototype system to track page dirtying rate inside virtual machines based on Xen 3.2.0 The rest of the paper is organized as follows Section II presents VM migration cost analysis in detail We design and implement a dirty page tracing approach in section III In section IV we describe our evaluation methodology and present the experimental results Finally we summarize our work in section V II VM M IGRATION C OST A NALYSIS B ASED ON P RE COPY A PPROACH When some VMs on hot nodes are migrated to other nodes the rst problem to face is how to select migrated VMs on hot nodes Different VMs would bring different migration overhead Migrated VMs should have less migration overhead which has less inîuence on the performance of VM service In fact the key point of live VM migration is to dynamically synchronize memory image data on source node and target node by iteratively copying dirty data Total migration time and downtime is two important metrics to measure VM migration performance and cost In this section we rst analyze the migration process based on pre-copy approach and the factors that affect the VM migration performance Then we quantify migration cost in stage  Finally we propose VM migration cost prediction model The key point of VM migration based on pre-copy is to iteratively copy dirty pages from source node to target node to synchronize the maximum amount of memory data Thus VM migration would have the shortest downtime VM migration based on pre-copy includes the following six stages 1  in the stage it is to select a target node which has enough free resource in advance The time used by the stage is denoted as  2  some resources on target node are reserved for VM to be newly migrated in the near future The time that the stage spends is denoted as  3  the stage is mainly to repeatedly copy dirty pages of migrated VM from source node to target node to synchronize the maximum amount of data The duration that the stage consumes is denoted as  4  VM on the source node is suspended and the last synchronized data are transferred to the target node The time that the stage consumes is denoted as  5  memory data of VM on target node is completed synchronized with that on source node The time that the stage spends is denoted as  6  VM is resumed on target node The time that the stage consumes is denoted as  There are two metrics to evaluate VM migration performance total migration time and downtime Total migration 
0 200 400 600 800 1000 1200 0 10 20 30 40 50 60 70 80 90 100 Throughput \(Mbit/s Elapsed Time \(second adaptive mode non-adaptive mode 
013 013 013 013 013 013 
198 
198 


004 007 007 007 007 007 004 007 007 004 007 004 007 004 007 007 007 004 007 007 007 007 007 010\011 012 007 
time   is the sum of the above six stages from on source node to on target node Downtime   is the sum of the last three stages from to  We have the following equations Stage and are incorporated into stage  which is denoted as  Apparently  Stage includes stage and  The duration is denoted as   Then we have the following equations In migration sub-system of Xen stage terminates if one of the following three conditions is met 1 the number of dirty pages is less than 50 2 the number of iterations in stage reaches 29 3 to synchronize memory pages on source node and target node the amount of transferred data has been larger than 3 times of memory size The rst condition is set for short downtime when applications have converging writable work-sets The last two conditions are set mainly for short total migration time when there are many dirty pages in each round of stage for applications with diverging writable work-sets They prohibit too many redundant data to be sent Memory data are transferred once at least when VM migrates According to termination condition 3 of stage  the maximum amount of transferred data is less than 4 times of memory size The amount of transferred data in stage is not larger than memory size So we can get the following formulae means memory size of transferred VM and denotes to available network bandwidth for VM migration According to the analysis of VM migration process described in part II-A the main cost of VM migration is to implement the synchronization of memory pages on source node and destination node The performance of VM migration relies on synchronization operations It is to synchronize the maximum amount of memory data by repeatedly copying dirty data from source node to target node which thus induces the shortest downtime So the factors that affect the performance of VM migration include memory size overhead used for initialization and resume page dirtying rate and available network bandwidth for VM migration We respectively discuss them in the following 1 memory size In the rst round of stage  all the memory pages are sent to target node 2 overhead used for initialization and resume Initialization and resume operations include initializing virtual container block device free resource connecting VM and drivers and announing new IP address on destination node These operations consume relatively constant time The length of duration is only related to hardware platform In diverse network environments the ratio of the time that the above operations consume to total migration time or downtime would change greatly In slow network environment stage takes long time to synchronize memory image data and the ratio is very low even negligible But in high-speed network environments total migration time is shorten largely while the time spent on initialization and resume keeps unchanged The ratio would increase sharply even high to about 77 3 dirty page rate Dirty page rate directly determines the amount of transferred data in each round of stage  The bigger dirty page rate the larger the amount of transferred data in each round of stage  the longer total migration time The larger dirty page rate the bigger the amount of transferred data in stage  the longer downtime 4 network bandwidth Available network bandwidth is used for copying memory image data from source node to target node It directly affects the performance of VM migration and is one of the main factors that affect migration performance The larger available network bandwidth the shorter the time to transfer memory data the shorter total migration time and downtime Because applications and VM migration share network bandwidth the more network resource that VM migration consumes the less resources applications can use It is important to balance the relationship between application and VM migration To mitigate hot nodes in virtualized environments some VM workloads on hot nodes should be migrated to other nodes Because VMs are resided on a same physical node they have the same overhead for initialization and resume The main difference of their migration cost is the overhead in stage and  which depends on the amount of transferred data for memory image synchronization This part quantitatively analyzes the amount of transferred data for memory synchronization between source node and 
003 004 
013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 013 014 015\005 004\013 004\013 013 014 015\005 013 004\013 014 015\005 013 014 005 
premigration activation stop-and-copy activation pre-migration reservation initialization resume submission activation pre-copy pre-copy pre-copy pre-copy stop-and-copy B Factors Affecting the Performance of VM migration pre-copy pre-copy pre-copy pre-copy stop-and-copy C Quantitative Analysis of Stage Pre-copy pre-copy stop-and-resume 
006\004\010 024\025 004\006\026 006\004\010 002\007 004\010 011\012\013 002\014\015\002\016 012\014\015\002\016 012\017\020 021\022\023 024\025 004\006\026 012\014\015\002\016 012\017\020 021\022\023 004\027\004 004\027\004 002\007 004\010 011\012\013 011\026\012 011\026\012 012\017\020 021\022\023 006\004\010 004\027\004 002\014\015\002\016 012\014\015\002\016 011\026\012 024\025 004\006\026 012\014\015\002\016 011\026\012 004\027\004 011\026\012 023\006\012 023\011\021\027 006\004\010 004\027\004 011\026\012 023\006\012 023\011\021\027 011\026\012 024\025 004\006\026 023\006\012 023\011\021\027 011\026\012 023\006\012 023\011\021\027 
199 
199 


pre-copy pre-copy pre-copy pre-copy pre-copy precopy D The Prediction Model of Transferred Data during VM Migration pre-copy pre-copy pre-copy 
Table I Symbol and Deìnition of VM migration the time and the amount of transferred data in each round are respectively expressed as Vector and Vector  Then we can get the following formulae Let  we can get the following equations So the amount of transferred data during VM migration total transferred data,TDT is According to the value of  there are two cases to discuss 1  In this case dirty page rate is less than available network bandwidth Writable work-sets would converge to smaller number of pages with the change of time When the amount of dirty data in certain round of stage is less than or equals to the threshold  or when the number of iterations reaches to the threshold _  stage terminates The number of iterations is _ If writable work-sets of applications converge to smaller pages rapidly the number of dirty pages would be less than or equals to the threshold when the iterations is less than _  When stage terminates the number of iterations is  If converging rate of dirty pages is slow the number of dirty pages is still larger than the threshold when the number of iterations reaches the maximum threshold _  Then the number of iterations is _  If the number of iterations in stage is  the amount of transferred data is 1 2  In this case dirty page rate is larger than or equals to available network bandwidth for migration Writable work-sets of this kind of application VMs would not converge when the time changes Once the available network bandwidth reaches the maximum value stage would terminate which ensures that total migration time or downtime is not prolonged The formula 1 is not suitable to the applications with  This kind of applications ordinarily have long downtime  the duration during which service is completely unavailable They are not the best candidates to be migrated For VMs resided on a same physical node the difference of migration cost is the amount of transferred data during migration The larger the amount of transferred data the bigger migration cost According to quantitative analysis of migration process in Section II-C we can use formula 1 to predict the amount of transferred data during migration when the ratio of dirty page rate to network bandwidth is less than 1 But when the ratio is larger than or equals to 1 formula 1 is not suitable We design a VM migration prediction model to predict the amount of transferred data as shown in Algorithm 1 Algorithm 1 predicts migration cost by simulating stage during VM migration Before VM migration occurs we monitor memory behaviour characteristics of applications and collect samples of dirty page rate available network bandwidth for migration Combining the collected information with conìguration information migration cost prediction model simulates migration process and computes the amount of transferred data The algorithm has three parameters available network bandwidth for migration dirty page rate and memory size The output of the algorithm is the amount of transferred data between source node and target node In the rst round of stage  all the memory data are copied to target node The duration of the rst round is the time spent on transferring the whole memory data on the y Dirty pages in the rst round are computed using memory characteristics functions of applications The second round transfers dirty page data in the rst round The duration and the number of dirty pages in each round are computed in turn until stage terminates So the total amount of transferred data during the whole migration is gured out In Algorithm 1 variable 
memory size of migrated VM the threshold of the amount of dirty data when stage terminates the average dirtying page rate during stage the average rate to transfer pages during stage  available network bandwidth for VM migration destination node First some symbols are deìned in the following Table I In stage 
002 002 003 003 
013 004\016 007\016 007 007\016 007\016 017 014 004\020 007\020 007 007\020 007\020 017 020 014 007\016 014 005 020 005 016 007\016 005 016 005 020 005 016 007\016 005 016 005 021 016 014 021 005 007\022 007 007 007 020 014 021 007\022 007 007 007 014 014 021 021 021\004 014 011\012\023 024\013\025\005 022 022 007\011\012\023 024\013\025\005 014 011\012\023 024\013\025\005 014 011\012\023 024\013\025\005 011\012\023 024\013\025\005 022 014 014 021 021 021 021 014 026\026\027\030\022\016\003\031\032 
Symbol Deìnition 
pre-copy pre-copy pre-copy 
002\003\004 002\005\006 007\010\011\012 002\013\010\014 
027 002 027 027 002 027 023\006\012 023\006\012 023\011\021\027 002\021\010\026 002\021\010\026 023\011\021\027 027 002\021\010\026 027 002 027 002\021\010\026 027 002 023\011\021\027 030 002\003\004\005 030 006\007\003\010 027 023\006\012 027 023\011\021\027 027 023\006\012 027 025\031\025 027 002 004 023\006\012 004 023\032\024 033 006\011\012 034\033 006\013\014 035 023\032\024 033 006\011\012 034\033 006\013\014 035 023\032\024 025\031\025 023\006\012 027 025\031\025 
006 005 005 006 005 005 006 006 005 006 005 006 005 005 004\006 007\005 
002\002\002 002\002\002 002\002\002\002\002\002 002\002\002 002\002\002 005 006 005 006 007 007 010 010 
004 004 004 004 004 004 004 004 004 004 004\006 005 013 004 004\006 005 013 004 005 004 014\015\016\010 017\020\021 012 017\020\021 004 010\005 012 005 005 005 
200 
200 


010 012 004\022\006 004\013\023 004\024 004\006 004 004\006 004\006 004\004 006 002 003\004\010 005 025\012 004 007 005 025 004\004 005 004 010 012 002 003\004\010 011 025\012 004 007 011 025 004 010 012 002 003\004\010 011 025\012 004 007 011 025 004 007\005 004 004\005 004 010 012 004 007 011 025 004 011 025 
014 005 005 016 014 026\026\027\030\022\016\003\031\032 014 011\012\023 024\013\025\005 011\012\023 033\012\034\013\035\005 003\016\032\036\037\016\003\027\022 014 026\026\027\030\022\016\003\031\032 037 003\016\032\036\037\016\003\027\022 004 011 012\023 024\013\025\005 037 003\016\032\036\037\016\003\027\022 016 003\016\032\036\037\016\003\027\022 014 015\005 014 014 014 003\016\032\036\037\016\003\027\022 003\036\016 037"\032 016 005 016 016 003\016\032\036\037\016\003\027\022 003\036\016 037"\032 015\005 014 014 003\036\016 037"\032 003\036\016 037"\032 016 005 016 016 003\016\032\036\037\016\003\027\022 003\036\016 037"\032 015\005 014 014 003\036\016 037"\032 003\016\032\036\037\016\003\027\022 003\016\032\036\037\016\003\027\022 014 017\011\012\023 033\012\034\013\035\005 014 003\036\016 037"\032 004 014 037 003\036\016 037"\032 016 005 016 014 014 003\036\016 037"\032 026\026\027\030\022\016\003\031\032 003\036\016 037"\032 014 026\026\027\030\022\016\003\031\032 
Algorithm 1 Require Ensure for and not do if then else if then else end if if or then end if end for return 
004 004 004 
003 003 003 003 002 003 003 003 003 002 003 003 003 003 003 002 003 003 003 003 
002 002 002 
The VM Migration Prediction Model    _ _ _ MB MB _ _ kB _ kB _ _ kB _ kB _ _ _ _ kB _ kB  determines the length of time that stage stop-and-copy takes It also determines the length of downtime For some realtime applications such as game web-site video server which are very sensitive to downtime VMs with the shortest downtime are the best choice to be migrated But for other applications VMs with shortest total migration time are the best choice which descreases performance degradation of applications induced by VM migration III S YSTEM I MPLEMENTATION First a new status of running VM  pseudo-migration is introduced to capture memory behaviour characteristics Before VM is into real migration status it is rst into status pseudo-migration to determine if it is suitable to be migrated to other nodes and if its migration would bring too much cost In this new status virtual machine monitor collecting resource usage Control Thread Migration Execution Thread VM set to be migrated out executing VM migration choosing target nodes for each migrated VMs terminating the thread starting a new thread status pseudoämigration making hot nodes into predicting migration cost selecting VMs to be migrated identifying hot nodes Fig 3 The workîow of system controller observes memory behaviour especially dirtying pages and then predicts migration cost A running VM is ordinarily in one of three kind of status The possible kinds of status are described as the follows  The status is set for tracing memory behaviour characteristics When VM is in this status all the operations related to dirtying pages are recorded in bitmap and dirty page rates are periodically computed The mapping of dirty page rate and time is used for predicting VM migration cost  The status shows that VM is migrating from source node to destination node  This status means that active VM is not in pseudo-migration or migration  To demonstrate the utility of our approach we design and implement a prototype based on Xen 3.2.0 System controller lists the workîow of the whole system The core of system is the work in status pseudo-migration Its main task is to detect memory behaviour characteristics of applications for migration overhead prediction Based on the work in status pseudo-migration  a VM set to be migrated is generated according to the prediction value of migration cost A System Controller Fig 3 describes the workîow of system controller The controller mainly includes two threads control thread and migration execution thread Control thread is responsible for identifying hot nodes and selecting VMs to be migrated out Based on migrated VM list generated by control thread migration execution thread then chooses a target node for each migrated VM and executes VM migration According to resource usage status of each node that sensors collect control thread determines if there is resource contention on the nodes Then control thread makes hot 
002 003 002 002 003 002 002 003 004 pseudo-migration 004 migration 004 non-migration 
002 005\003 004\006 002 005\003 004 002 010\003 002 005\003 004 002 010\003 
023\006\024 023\011\021\027 002\021\010\026 025\031\025 023\032\024 025\031\025 023\006\024 023\011\021\027 025\031\025 025\031\025 023\006\024 023 004\023\026\011\021\023\004\015\027 023 002\021\010\026 023\011\021\027 025\031\025 025\031\025 023 004\023\026\011\021\023\004\015\027 023 023 004\023\026\011\021\023\004\015\027 002\021\010\026 023\011\021\027 025\031\025 025\031\025 025\031\025 023\006\024 023\032\024 023 004\023\026\011\021\023\004\015\027 023 023 004\023\026\011\021\023\004\015\027 002\021\010\026 025\031\025 025\031\025 025\031\025 
201 
201 


014 026\026\027\030\022\016\003\031\032 014 026\026\027\030\022\016\003\031\032 
respectively in paravirtualizd VM and hardware virtual machine HVM nodes into status  Virtual machine monitor observes memory pages of each VM resided on hot nodes They then predict migration cost for VMs Based on these prediction information control thread determines VM set to be migrated out for each hot node Then VM migration execution thread chooses target nodes for these migrated VMs and executes VM migration The migration cost aware selection approach to generate VM set to be migrated out includes the following steps 1 migration cost prediction model predicts migration overhead    of each VM resided on hot node 2 VMs on hot node are sorted to an ordered queue by or increasingly 3 VMs on head of queues are chosed to be migrated out in turn until resource contention disappears In Xen 3.2.0 parameters of hypercall are modiìed to express three kind of running status  and  Two new Macros are deìned to start and terminate status  deìne Xen_DOMCTL_pseudoMigration_start 60 deìne Xen_DOMCTL_pseudoMigration_end 61 The work of status respectively in paravirtualization and full-virtualization is depicted in Fig 4 Using bitmap Xen enables hypervisor in the layer of virtual machine monitor VMM to trace page modiìcation in memory For para-virtualized VMs all pages are transparently marked as read only Thus they are protected to trap any modiìcation to their page tables The updates are then recorded in bitmap But for hardware virtual machine HVM because guest OS is unmodiìed hypervisor tracks updates and propagates them to shadow page tables Shadow page tables record the mapping of logical address and real machine address They are implemented to emulate virtual page tables of virtual machine Register 3 points to shadow page tables rather than page tables of guest OS Furthermore we design a timer to periodically compute dirty page rate and to reset bitmap marking modiìed pages In the modiìed version of Xen 3.2.0 we provide two new instructions and to respectively make VMs into and out of status  IV P ERFORMANCE E VALUATION In this part we use several typical applications to conduct experiments on our prototype system We test the correctness of migration cost prediction model and the effectiveness of migration cost aware selection approach The experimental results show that the prediction model can correctly and quickly sort VMs on a same node by migration cost Compared with other algorithms our VM selection approach can effectively reduce migration cost We conduct our experiments on several identical serverclass machines each with 2-way quad-core Xeon E5405 2GHz CPUs and 8GB DDR RAM The machines have Intel Corporation 80003ES2LAN gigabit network interface card NIC and are connected via switched gigabit Ethernet VMs access their image les through NFS We use Redhat Enterprose Linux 5.0 as the guest OS and the privileged domain OS domain 0 The host kernel is the modiìed version of Xen 3.2.0 The guest OS is conìgured to use 4 VCPUs and 1024MB of RAM except where noted otherwise Each experiment is repeated ten times and every reported test result comes from the arithmatic average of ten values The experiments used the following workloads representative of typical server applications 1 idle an idle Linux OS for daily use 2 Dynamic Web Workload a Tomcat 5-5.5.23 web server acts as the workload of migrated virtual machine Several client machines are used to generate the load for the server and each machine simulates a collection of users concurrently accessing the web site using  3 MUMmer MUMmer 3.21 is a s cientiìc application to rapidly align genomes that has a very intensive memory usage 4 dbench dbench 4.0 is an open source benchmark producing the lesystem load The benchmark can 
privileged privileged 
pseudo-migration pseudo-migration B Tracing Dirty Pages domctl pseudo-migration migration non-migration pseudo-migration pseudo-migration CR enable_logdirty disable_logdirty pseudomigration A Experimental Setup httperf 
unmodified OS page table b HVM Fig 4 Status 
mode preämigration App page rate dirtying Dom0 VM VMM page table modified OS bitmap Timer reset a para-virtualized VM mode preämigration App shadow page table bitmap Timer reset page rate dirtying Dom0 VM VMM 
025\031\025 025\031\025 
202 
202 


B The Correctness of Migration Cost Prediction Model VM VM VM VM VM VM VM VM pre-copy C The Effectiveness of Migration Cost Aware Selection Algorithm idle mixApps 
Table II The Conìguration of VMs Table III The Prediction Value of Transferred Data during VM Migration The conìguration details of ve VMs are listed in Table II VMs are into status pseudo-migration Samples are taken The prediction values of transferred data are depicted in Table III VM 1 2 3 and 5 are in converging queue The number of dirty pages would become smaller with the change of time VM 4 belongs to nonconverging queue Its dirty pages would not get smaller during VM migration due to rapid dirty page rate At the same time ve VMs are respectively migrated to other nodes Their migration time is described in Fig 5 VM 4 has the longest migration time due to its rapid dirty page rate There are a great deal of redundant data to be transferred VM 2 has the shortest migration time There are two reasons one is that 2 has small memory size  small amount of data to be transferred in the rst round of stage  the other is relatively low dirty pages From Table III and Fig 5 we can observe that their results are the same which proves that migration cost prediction Fig 5 Total migration time of different VMs Fig 6 The comparison of three algorithms to remove hot nodes model can correctly predict migration cost of different VMs on a same physical node We compare migration cost of three VM selection algorithms using diverse applications one is migration cost aware selection algorithm MOA one is random selection algorithm RA and the third one is memory-size-based algorithm MIA Three VMs run on a same node They are conìgured with different memory size providing the same service such as Tomcat MUMmer and dbench means that there is not any application running in VM In the last group of tests means the hybird of diverse applications conìgured with different memory size Three algorithms are respectively used in the same application scenes and total migration time is tested All the measurement values are standardized to the value using memory-size-based algorithm The experimental results are listed in Fig 6 From Fig 6 we can conclude that migration cost using random selection algorithm is 0.264 times larger than that using memory-size-based algorithm That means memory size of migrated VM is one of the factors affecting migration cost At the same time the cost using migration cost aware selection algorithm is 0.226 times less than that using memory-size-based algorithm which shows that it is not enough to estimate migration cost only considering memory size of migrated VM There are still other factors to affect migration cost Compared with random selection algorithm migration cost aware selection algorithm reduces the cost by 38.8 on average V C ONCLUSION In this paper a quick and efìcient method is presented to make migration decisions to mitigate hot nodes We theoretically analyse the process of live VM migration and give a quantitative model of migration costs for a kind of VM applications We implement system prototype based on Xen 3.2.0 The experimental results show that our approach can fast mitigate hot nodes in the Cloud and effectively reduce migration cost 
1 512MB RAM 2 256MB RAM 3 384MB RAM 4 512MB RAM program with rapid dirtying pages rate 5 512MB RAM 1 512.0 2 287.0 3 711.8 4 2117.3 5 664.5 simulate a variety of real le servers by executing create/write/read/delete operations on a large number of directories and les with different sizes 
VM idle VM MUMmer VM Tomcat VM C VM dbench VM VM VM VM VM 
VM Conìguration VM The Prediction Value MB 
0 4 8 12 16 20 VM1 VM2 VM3 VM4 VM5 Total Migration Time \(s Virtual Machine 0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 idle Tomcat MUMmer dbench mixApps Normalized Value of Migration Time VM Applications MOA MIA RA 
203 
203 


VI A CKNOWLEDGMENT This work is supported by the National Natural Science Foundation of China under Grant 61073024 and 61232008 It is also supported by the Outstanding Youth Foundation of Hubei Province under Grant 2011CDA086 National 863 Hi-Tech Research and Development Program under Grant 2013AA01A213 and EU FP7 MONICA Project under Grant 295222 R EFERENCES  R Singh P  Sheno y  K K Ramakrishnan R K elkar  and H V in eTransform Transforming enterprise data centers by automated consolidation in 
Proceedings of the 32nd International Conference on Distributed Computing Systems ICDCSê12 Proceedings of the 7th ACM SIGOPS/EuroSys European Conference on Computer Systems EuroSysê12 Proceedings of the 19th ACM symposium on Operating Systems Principles SOSPê03 Future Generation Computer Systems Proceedings of the Second Symposium on Networked Systems Design and Implementation NSDIê05 Proceedings of the USENIX Annual Technical Conference USENIXê05 Proceedings of the 5th IEEE Annual Workshop on Workload Characterization Proceedings of the First International Conference on Cloud Computing Proceedings of the ACM/Usenix International Conference on Virtual Execution Environments VEEê09 Proceedings of the 18th Annual IEEE/ACM International Symposium on Modeling Analysis and Simulation of Computer and Telecommunication Systems MASCOTSê10 Journal of Telecommunications Proceedings of the 20th International Symposium on High Performance Distributed Computing HPDCê11 
 2012 pp 1Ö11  N Bila E de Lara K Joshi H A Lagar Ca villa M Hiltunen and M Satyanarayanan Jettison Efìcient idle desktop consolidation with partial vm migration in  2012 pp 211Ö224  P  Barham B Drago vic K Fraser  S Hand T  Harris A Ho R Neugebauer I Pratt and A Warìeld Xen and the art of virtualization in  2003 pp 164Ö177  R Buyya C S Y eo S V enugopal J Brober g and I Brandic Cloud computing and emerging IT platforms Vision hype and reality for delivering computing as the 5th utility  vol 25 no 6 pp 599Ö616 2009  C Clark K Fraser  S Hand J G Hansen E Jul C Limpach I Pratt and A Warìeld Live migration of virtual machines in  2005 pp 273Ö286  M Nelson B Lim and G Hutchines F ast transparent migration for virtual machines in  2005 pp 391Ö394  C Amza A Chanda A L Cox S Elnik ety  R Gil K Rajamani W Zwaenepoel E Cecchet and J Marguerite Speciìcation and implementation of dynamic web site benchmarks in  2002 pp 3Ö13  2013 Open source benchmark producing the lesystem load On A v ailable http://samba.or g/ftp/tridge/dbench  W  V oorsluys J Brober g S V enugopal and R Buyya Cost of virtual machine live migration in clouds A performance evaluation in  2009 pp 254Ö265  F  Hermenier  X Lorca J.-M Menaud G Muller  and J La w all Entropy a consolidation manager for clusters in  2009 pp 41Ö50  S Ak oush R Sohan A Rice A W  Moore and A Hopper  Predicting the performance of virtual machine migration in  2010 pp 37Ö46  M T arighi S A Motamedi and S Shariìan  A ne w model for virtual machine migration in virtualized cluster server based on fuzzy decision making  vol 1 pp 40Ö51 2010  H Liu C.-Z Xu H Jin J Gong and X Liao Performance and energy modeling for live migration of virtual machines in  2011 pp 171Ö182  2013 httperf homepage Online A v ailable http://www.hpl.hp.com/research/linux/httperf  2012 A s ystem for aligning entire genomes Online A v ailable http://mummer.sourceforge.net 
204 
204 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 1455Ö1456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593Ö605  Y  Loukas  Artiìcial neural netw orks in liquid chromatography Efìcient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119Ö129 2000  N Serbedzija Simulating artiìcial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 56Ö63 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 90Ö97  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 24Ö34 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 1726Ö1730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179Ö187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281Ö288 2007  U Seif fert  Artiìcial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135Ö150 2004  D Calv ert and J Guan Distrib uted artiìcial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 2Ö10  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534Ö537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 2Ö16  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810Ö818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Efìcient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285Ö296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 2Ö16 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


