Mining of EL GCIs Daniel Borchmann and Felix Distel Faculty of Computer Science TU Dresden Dresden Germany f borch,felix g tcs.inf.tu-dresden.de Abstract We consider an existing approach for mining general inclusion axioms written in a lightweight Description Logic In comparison to classical association rule mining this approach allows more complex patterns to be obtained Ours is the 002rst implementation of these algorithms for learning Description Logic axioms We use our implementation for a case study on two real world datasets We discuss the outcome and examine what further research will be needed for this approach to be 
applied in a practical setting Index Terms Description Logics General Inclusion Axioms I I NTRODUCTION Data mining has traditionally focused on the extraction of relatively simple rigid patterns from datasets For example in association rule mining the patterns are simple pairs of itemsets Their semantics is that transactions that contain the 002rst itemset also contain the second In this paper we implement and test an approach for mining a more complex and 003exible type of associations namely general concept inclusions GCIs that are written in the Description Logic DL EL  Our hope is that this combination of data mining and DL will be mutually bene\002cial for both 002elds 
Classical association rule mining requires a simple type of data where individuals or transactions have attributes or items associated with them In man y data sets there are also binary relations between individuals e.g there might be a relation linking two transactions if they have been performed by the same customer In another scenario the data may have been obtained from a social network and the binary relations are the various degrees of friendship among individuals Classical association rules cannot take these relations into account at least not natively Data mining and in particular social graph mining might bene\002t from the use of a language e.g a language from the DL family that is inherently capable of talking about binary relations Conversely DL might bene\002t from a data mining technique 
that can assist in the design of ontologies Ontology design can be a tedious and error-prone task It is usually done by domain experts who are not experts in logics By mining GCIs from existing data one could obtain a starting point for building an ontology Ideally the results of the mining process should be veri\002ed by a domain expert The work in this paper is based on an algorithm presented in  4 Ours is the 002rst impleme ntation of this algorithm The main purpose of this work is to serve as a prove of concept that mining GCIs can yield useful results Unlike algorithms for mining association rules which usually try to compute all possible association rules with suf\002cient support and con\002dence this algorithm only computes a compact representation of the GCIs a so-called 
base  This is necessary since even for a 002nite number of relations and attributes there are in\002nitely many possible EL GCIs We have chosen two datasets with different characteristics for testing our implementation The 002rst is an excerpt from DBpedia namely those indi viduals who either ha v e children or are someone's child As this dataset has been semiautomatically extracted from Wikipedia it contains relatively many errors Thus it can teach us valuable lessons on how the algorithm behaves on noisy and incomplete data Second we have used the DrugBank which is part of the Link ed Open Data Cloud It contains data about v arious drugs and their protein targets In comparison to DBpedia it is relatively free 
of errors We test the algorithm on both datasets and discuss the outcome The results reveal potential but they also give indications in what areas more research will be needed cf Section V The structure of the paper is divided as follows We start with a short introduction to EL and the mining algorithm This is followed by a description of our implementation and the two datasets We present some of our observations and 002nally discuss what lessons can be learned from them II T HEORETICAL F OUNDATIONS A The Description Logic EL EL is a lightweight Description Logic Although being 
relatively inexpressive compared to other DLs it has gained widespread acceptance e.g in biomedical ontologies 9 and within the latest OWL standard that contains the pro\002le OWL 2 EL EL s success is largely due to its favorable algorithmic properties We give an overview over the syntax of EL as well as an intuition for its semantics to the extent which is needed to understand this work For a more formal introduction to DL consider Lik e all Description Logics EL can be used to describe individuals and classes of individuals using concept descriptions  The main building blocks for 
concept descriptions are a set N C of concept names and a set N R of role names  Concept names e.g Parent  Child or FictionalCharacter  are themselves concept descriptions Role names describe relationships between individuals such as hasChild  createdBy  etc New concept descriptions can be 
2011 11th IEEE International Conference on Data Mining Workshops 978-0-7695-4409-0/11 $26.00 © 2011 IEEE DOI 10.1109/ICDMW.2011.119 1083 


Homer Bart Person  Parent FictionalCharacter Person FictionalCharacter hasChild Fig 1 A Simple Model constructed using the constructors top    conjunction  u  and existential restrictions  9  If C and D are concept descriptions and r is a role name then C u D and 9 r:C are also concept descriptions For example the concept description Parent u FictionalCharacter 1 describes individuals that are both parents and 002ctional characters The description 9 hasChild  Parent 2 describes individuals with children who are themselves parents More formally the semantics of EL is based on models I  001 I  001 I   A model consists of a set 001 I called domain and a function 001 I mapping each concept name A to a subset A I 022 001 I and each role name r to a relation r I 022 001 I 002 001 I  The function 001 I can be extended recursively to map every concept description C to its interpretation C I by de\002ning  I  001 I   C u D  I  C I  D I and  9 r:C  I  f x 2 001 I j 9 y 2 C I   x y  2 r I g  Each model can be represented by a directed graph the so-called description graph of I  whose edges are labeled with a role name and whose nodes are labeled with sets of concept names as in Figure 1 In Figure 1 the description 1 is interpreted as f Homer g and as there are no grandparents in this model the description 2 is interpreted as the empty set Relationships between concepts can be expressed in the form of general concept inclusions GCIs These are statements of the form C v D where C and D are concept descriptions Consider for example Person u 9 hasChild   v Parent  3 which expresses that every person who has a child is a parent A GCI holds in a model if the interpretation of its left hand side is contained in the interpretation of its right hand side e.g the GCI from 3 holds in the model from Figure 1 because both sides have the interpretation f Homer g  A concept description C is more speci\002c than a concept description D if the GCI C v D holds in all possible models If B is a set of GCIs and C v D is another GCI then we say that C v D follows semantically from B if and only if in every model I in which all GCIs from B hold the GCI C v D holds as well Observe that ordinary association rules can be expressed as a simple form of GCIs where only conjunction and no existential restrictions are used An association rule stating that people who buy cereals and honey also buy milk could be expressed as BuyerOfCereal u BuyerOfHoney v BuyerOfMilk  EL GCIs are more expressive whenever relations exist between individuals In such a situation an in\002nite number of concepts can be described using EL Usually only a fraction of them will be interesting in practice Therefore we have tested our approach only on datasets where relations between individuals exist The two datasets used in this work are presented in Section III B The Basic Mining Algorithm We provide an overview of the theory that is involved It is described in detail in 4 In con v entional association rule mining there is only a 002nite number of items and therefore only a 002nite number of rules When using EL GCIs to describe associations this is no longer the case as it allows for nesting of existential restrictions causing a blowup Even worse the number of possible concept descriptions grows faster than exponential with the number of nestings the so-called role depth  Hence even with a restricted role depth we cannot expect to enumerate all associations that hold in the data in reasonable time Instead we enumerate a base of the EL GCIs holding in the data i.e a set B of GCIs such that 017 all GCIs from B hold in the model and 017 all GCIs that hold in the model follow semantically from B  In 4 a 002rst algorithm for computing a 002nite base from a given dataset is presented Its main purpose is knowledge base completion and as a knowledge base completion algorithm it competes with other methods e.g 12 Here we emplo y it in a slightly different setting where no knowledge base but a large dataset is initially present The algorithm proceeds in two steps First since it is impossible to consider all possible EL concept descriptions a 002nite set of relevant concept descriptions is obtained from the data using a technique called model-based most speci\002c concepts For a given model I and a set X 022 001 I the concept description C is called the model-based most speci\002c concept of X if it is the most speci\002c concept description that still satis\002es X 022 C I  The model-based most speci\002c concept of X is denoted by X I  The set of relevant concept descriptions is then obtained as M I  N C  f9 r:X I j X 022 001 I g  In it is sho wn that the GCIs where both sides are conjunctions of these relevant concept descriptions already form a base B  This base usually contains a large number of redundancies Lemma 1 The set of GCIs B  f C v  C I  I j C 2 R I g where R I  f d U j U 022 M I g is a base for the model I  Whenever only conjunctions occur in a GCI its semantics become simpler This allows the use of established theories to further reduce the size of B  In the second step of the algorithm this is done using a mathematical theory called Formal Concept Analysis FCA FCA uses a data structure 
1084 


called formal contexts  A formal context K is de\002ned to be a triple K   G M I  where G is a set of objects M is a set of attributes and I 022 G 002 M is a binary relation Pseudointents are subsets of M with certain special properties and that can be used to characterize the implicational theory of a formal context cf for a formal de\002nition Using an algorithm from FCA called Next-Closure  pseudo-intents can be computed A smaller base B 0 can be obtained using the following result from Lemma 2 Let I be a model and K I  001 I  M I  I  be the context where I is de\002ned to contain the pair  x C  iff x 2 C I  The set of GCIs B 0  010 l P v  l P  I  I j P pseudo-intent of K I 011 is a base for the model I  Given K I as input Next-Closure successively returns new GCIs or more precisely their left hand sides however the delay between two GCIs can be exponentially large It has been a longstanding problem in FCA whether improvements to Next-Closure can be made yet no breakthrough has been achieved so far It can be shown that B 0 has minimal cardinality among all bases of the EL GCIs holding in a given dataset Notice that the algorithm will only compute GCIs that hold in a given dataset In data mining terminology one could say it computes associations with con\002dence 1 The reasons for this and its implications will be discussed in Section IV-C C Modi\002cations to the Basic Algorithm The basic algorithm described above consists of two parts 002rst the relevant concept descriptions are computed and then Next-Closure is used to obtain the basis During the 002rst step a long delay can occur during which no output is produced To avoid this a modi\002cation to the basic algorithm is suggested in where rele v ant concept descriptions are computed on the 003y i.e during the execution of Next-Closure Our implementation uses this modi\002ed approach Models that occur in practice can be cyclic i.e the graph that represents them is not a tree or forest In this case the existing strategy for obtaining the relevant concept descriptions does not work when EL is used This makes it necessary to use an extension of EL namely the DL EL gfp that allows for cyclic concept descriptions Using EL gfp the cycles in the model can be mirrored within the concept descriptions In this work one of the models DBpedia contains cycles while the other does not III D ESCRIPTION OF D ATA U SED We have implemented the EL exploration algorithm prototypically as part of a larger program called conexp-clj  which has been developed for Formal Concept Analysis The implementation language is Clojure a dialect of Lisp running on the Java Virtual Machine The implementation itself provides an abstract handling of Description Logics and interpretations and allows for the examination of properties of the algorithm Not necessarily is it designed for performance Nevertheless some optimizations ideas have been implemented and evaluated The descriptions of these requires more technical insight into the algorithm and is therefore out of the scope of this paper For details see We have tested the algorithm on two different data sets of different quality and size In the next two subsections we describe these two data sets and how they have been constructed from data from the Linked Open Data Cloud A Child-Relation in DBpedia The 002rst data set has been extracted from data from the DBpedia project The aim of this project is to automatically extract relational data from Wikipedia infoboxes The data snapshot from March 2011 which has been used for our experiments contains over 10 million RDF triples To handle them all at once is out of the scope of our capabilities yet Instead we have chosen a small subset of the data to construct an EL model for our experiments To this purpose we have used two data sets of RDF triples from the DBpedia one containing relations between individuals and the other containing instances of certain classes We then chose the role http://dbpedia.org/ontoloy/child and collected all RDF triples from the relational data set labeled with this relation The individuals which appeared in these collected triples formed the base set 001 DB for our model Then the concept names were chosen from the data set containing the instances Here all those classes have been chosen which had at least one instance in 001 DB  In sum this procedure resulted in an EL model I DB containing 5626 individuals one role relation and 60 concept names As we shall see later this model has some odd properties which are due to numerous errors in the DBpedia data sets First of all the child relation we have chosen proves to be cyclic in this interpretation Furthermore the relation does not exist between persons alone but also between authors and their work or even between persons and places Those errors are mostly owed to the nature of Wikipedia's infoboxes which are not standardized in any way Thus the data collected by the DBpedia project has to be normalized in certain ways and often enough errors occur We shall see later what consequences this has for our algorithm and the computed results B DrugBank As another data set from the Linked Open Data Cloud we have chosen the DrugBank and Diseasome 16 data sets They provide information about drugs their possible disease targets and information about genes that are in\003uenced by given drugs Three different roles occur in the datasets namely isSubtypeOf between diseases describing an is-a relation treatedBy from diseases to drugs and targets from drugs to protein sequences being affected by the given drug In contrast to the DBpedia model we construct the concept names for this model by looking at certain RDF triples More precisely we collect the RDF triples named with 
1085 


017 http://www4.wiwiss.fu-berlin.de/drugbank/resource drugbank/drugCategory 017 http://www4.wiwiss.fu-berlin.de/diseasome/resource diseasome/class 017 http://www4.wiwiss.fu-berlin.de/drugbank/resource drugbank/goClassi\002cationProcess Every individual appearing on the left side of one such RDF triple is associated with the right side of the triple as its concept name The 002rst of the three relations associates drugs with their category in the DrugBank data set such as being an antianemic agent an anticoagulant an antiviral agent and so on In a similar fashion classes like metabolic endocrine immunological are associated with diseases by the second relation The third relation associates protein sequences with their processes they are involved in These processes have a verbatim description in the corresponding RDF triples and are therefore quite many resulting in a lot of different concept names if taken literally We therefore did a very simple clustering by searching for words like metabolism signal transduction or homeostasis in those process descriptions and classi\002ed the genes according to the 002rst match found Undoubtedly this is not a very sophisticated method of clustering the processes but it is an easy method to keep the example small and controllable The construction described so far results in an EL model I drugs with 13335 individuals three role relations and 899 concept names In contrast to the model constructed from the DBpedia data set the model I drugs is supposed to be of much higher quality Furthermore the roles have been chosen such that the resulting model I drugs is acyclic IV R ESULTS A Results for DBpedia We have tested our implementation on the model derived from the child relation from DBpedia as described in Section III-A In total 1252 GCIs were found Among these only the 002rst 339 i.e less than one third are of a form that could have been obtained without using a Description Logic More precisely they were either of the form l C 2 A C v l C 2 B C 4 or l C 2 A C v All  5 for some sets of concept names A and B  All stands for the concept description that combines all possible attributes that an individual can have Intuitively 5 states that an individual that belongs to all concepts from A must belong to any EL gfp concept In practice this usually occurs when there is no individual belonging to all concepts from A in the model Examples for obtained GCIs are the following Chancellor v Politician Book v Work Person u Place v All Astronaut u Model v All 6 The second and third GCI in 6 refer to concepts such as Book or Place that one would not expect to apply to persons Since the model that we use is restricted to individuals occurring in the child relation one would expect all individuals to be persons Book  Work  and Place still occur because of errors in DBpedia These errors result from the process in which DBpedia is created We discuss ways to address 003awed data in Section IV-C1 The algorithm has a tendency to enumerate GCIs in an order of increasing role depth of the left hand sides More details on why this is the case can be found in This is a desired property since in our tests GCIs with very large role depth were more likely to be too speci\002c due to over\002tting and therefore less interesting For the DBpedia model the GCIs of role depth 1 were mostly of the form A u 9 child   v All  One example is Model u 9 child   v All  which states that individuals belonging to Model that have a child successor must belong to all possible concepts More bluntly speaking no offspring of a fashion model has ever gained enough relevance to merit a DBpedia entry The GCIs appearing later are more complex For example the relatively general GCI 9 child  Person v Person 7 is found which applies to 1359 individuals but also very speci\002c GCIs with larger role depths appear such as 9 child  Artist u 9 child  Politician u Person v v 9 child  Actor u 9 child  O\016ceHolder u 9 child  Congressman u O\016ceHolder  8 which is only applicable to the individual Robert F  Kennedy  Notice that while 7 is most likely a desired outcome 8 is an artefact of incomplete data This problem shall be discussed Section IV-C1 Towards the end of the exploration the GCIs become even more speci\002c and much more convoluted resulting in expressions which are hardly understandable B Results for DrugBank Owing to the mining algorithm's tendency to produce GCIs of lower role depth earlier the 002rst GCIs are of relatively simple nature In our DrugBank model there are concepts that only apply to drugs such as Nootropics for drugs that belong to the class of nootropics and concepts that only apply to protein targets such as RNA processing for proteins that are involved in RNA processing No individual can be both a drug and a protein target thus conjunctions of such 
1086 


concepts must be unsatis\002able This results in a large number of GCIs of the form d C 2 A C v All  Furthermore all drugs in the DrugBank have a target yielding GCIs of the form d C 2 A C v 9 target    In total about 1700 GCIs of these two forms were found Among the more interesting GCIs relatively few were of role depth 0 one example is tRNA processing v RNA processing u Metabolism u Physiological process  Quite a large number of GCIs of role depth 1 are found such as MuscarinicAntagonists v 9 targets 000 Signal transduction u Cell communication u Cellular process 001 or more complex ones like Anti in\015ammatory LocallyApplied v Anti allergicAgents u AntiulcerAgent topical u 9 targets  000 Response to biotic stimulus u Defense response u Immune response u Response to stimulus 001 u 9 targets  000 Signal transduction u G protein coupled receptor protein signaling pathway 001  Notice that in the second GCI the two existential restrictions really refer to two distinct targets one typically Interleukin 3  is responsible for an Immune response and another e.g Cysteinyl leukotriene receptor1  is responsible for Signal transduction  Unlike the DBpedia which is the result of a semiautomated process the DrugBank has been created by human experts Hence can be assumed to contain relatively few errors Therefore one could imagine that the results of the mining process form a good starting point for the construction of an ontology Ideally each of them should nevertheless be veri\002ed by an expert C Observations 1 Insuf\002cient or Noisy Data Two types of shortcomings in the data can affect the results of the mining First there may be incomplete data In such a situation there might be GCIs that one would not expect to hold but for which there is no counterexample present in the data Second noise or factual errors can cause the opposite problem There may be GCIs that one would expect to hold but the data contains a faulty counterexample Both of these problems occur in the DBpedia model while the DrugBank is relatively free of errors in comparison We present some examples from DBpedia to illustrate these issues Insuf\002cient Data One example for this issue is the GCI 8 This GCI holds in the model since no other person with an artist and a politician as children has a DBpedia entry Yet in reality such persons are nevertheless likely to exist Errors in the Data Among the output of the DBpediaExploration is the following fairly incomprehensible GCI Person u 9 child  020 Person u 9 child  000 Person u 9 child   Person u 9 child  9 child  Person u 9 child    001 021 v 9 child  020 Person u 9 child  000 Person u 9 child   Person u 9 child   Person u 9 child  9 child  Person  001 021 It roughly states that a fourth great-grandparent's second great-grandchild must be a person It is clearly true but unnecessarily complicated It occurs only because the much simpler GCI Person u 9 child   v 9 child  Person  9 from which it could be concluded is not among the output There is a whole range of false counterexamples that seemingly disprove 9 The individual named Bertolt Brecht is one such example Bertolt Brecht has two child successors Frank Banholzer and Stefan Brecht  Both are not instances of Person  It is surprising to see that this is not an exceptional counterexample as 1188 individuals in our DBpedia-model contradict 9 Some of these reveal even more oddities in the DBpedia largely due to incorrect use of the child relation For example the individual John Perkins the author has April 1982 as one of his children most likely because his real daughter was born in April 1982 Other examples include places as children and literary works as children of their authors Combinations of Errors and Insuf\002cient Data Among the output is the GCI Person u 9 child  020 Person u 9 child  000 Person u 9 child   Person u 9 child  Artist  001 021 u 9 child  020 Person u 9 child  000 Person u 9 child  9 child   Person u 9 child    001 021 v 000 C f C 021 Writer u 9 child C g 001 Just like 8 it has a complicated left hand side that is only satis\002ed by one individual namely Carol Ann Du\013y  the UK's poet laureate Hence it owes its existence to incomplete data What makes it even more odd is its right hand side The right hand side is an EL gfp concept description describing individuals whose child is a Writer whose child is a Writer  and so on in\002nitely This peculiar consequence is due to an error where both Carol Ann Du\013y and her husband are each labeled as their respective spouse's child  As we can see the combination of effects from incomplete and false data can lead to very undesirable artifacts Approaches for Dealing with Noise and Incompleteness A supervised approach for dealing with incomplete data has been examined in 4 Each of the ne wly found GCIs is presented to an expert who can accept it or reject it by providing a counterexample This method is quite sensitive 
1087 


with respect to errors in the data It is suitable when the data can be assumed to be largely correct Since expert interaction is usually costly it should in most situations be applied on a limited scale or with restricted role depths The DrugBank appears to be a good application for this approach while DBpedia does not An approach that we would like to investigate in the future is by adapting the notions of support and con\002dence to GCIs For a more detailed discussion of this approach see Section V 2 The In\003uence of Roles One of the distinguishing features of the presented algorithm is the added expressivity provided by a DL language that can talk about roles in addition to concepts It also causes some new problems among the problem of cyclic roles To test the impact of cyclic roles on the overall behavior of the exploration we have extended both our DBpedia model and the DrugBank model with cyclic roles More precisely we have constructed a model I cycDB from the given DBpedia data sets in the same way as described in III-A but instead of only considering the child relation we also added the relation of having someone as mother or father The resulting model proved to be very hard to be explored Instead of the 4.5 hrs needed to explore the original model I DB  after three days of runtime only 1700 GCIs had been collected and the computation of the next GCI took over half an hour Furthermore due to the highly cyclic nature of I cycDB most of the resulting GCIs were very hard to understand and thus of very little value However things got even worse when we considered the relation interactsWith given in the DrugBank data set modeling the information which two drugs interact with each other Obviously this relation is highly cyclic but not symmetric due to some technical reasons If we add this relation to our model I drugs exploration gets unfeasible with our implementation The reason for this is that two drugs Bumetanide and Furosemide show up as two individuals for which a description graph has to be computed which represents the model based most speci\002c concept of both of them However both drugs are very similar and have a large number of other drugs they interact with resulting in a description graph of over 5 million vertexes Our prototypical implementation was not built to handle such large description graphs and therefore we could not conduct the exploration However before this large description graph appeared 6 GCIs had been collected one of which contained a non-trivial cyclic concept description as it's conclusion But this description was too large to be understandable showing that even when the exploration yields GCIs in this case the result might be of little value for the domain expert 3 Time and Space Behavior The time and space behavior may give insights into properties of the algorithm itself and we want to discuss our corresponding observations in this section For the exploration of the DBpedia-model they are depicted graphically in Figure 2 The time behavior of the exploration shows some kind of exponentially growing delay between the computation of two successive GCIs Since the FCA part of the implementation 0 0  2 0  4 0  6 0  8 1 1  2 1  4 001 10 4 0 500 1  000 Time in seconds GCIs found 0 0  2 0  4 0  6 0  8 1 1  2 1  4 001 10 4 40 60 80 100 Time in seconds Space in  of the available space Fig 2 Time and space behavior when exploring the DBpediamodel relies on the computation of pseudo intents in lexicographic order using Next-Closure this bottleneck can most likely not be avoided Unfortunately  our e xperiments indicate that the worst case might be very close to the average case in our application For the space behavior of our implementation concrete interpretations are much harder to make This is because of the Java Virtual Machine and its memory model on which the implementation runs on which is based on automatic garbage collection However from the description of the algorithm we can expect a very moderate memory consumption of the overall run of the exploration which should mostly be due to storing the generated GCIs All other operations involved might need some extra space but only for a short time This expectation can be seen from the graphical representation because of the appearance of stairs in the diagram showing the periods where the next GCI is searched for alternating with the periods of steep increase where other computations are performed The steep declines of the curve correspond to calls of the garbage collector and they show that in almost every case that 50 of the main memory could have been reclaimed further supporting our hypothesis that only a moderate number of memory is used Finally it is interesting to see that the calls to the garbage collector get less and less the longer the exploration runs This corresponds to the fact that it takes more and more time in average to compute the next GCI the longer the exploration runs 
1088 


V F UTURE W ORK In the future two main issues should be addressed First we would like to investigate approaches to improve the quality of the output in particular in the case of incomplete or noisy data Second we want to improve on memory and space requirements A Improving the Quality of the Output One should not forget that the algorithms that are used here are mathematically proven to yield a sound and complete base of minimal cardinality for the dataset provided That means provided that the data is complete and error-free there is no room for improvement in terms of quality In practice we are unlikely to ever encounter perfect data Hence the question is how can the effects of poor data be mitigated In Section IV-C1 we have observed the two different types of in\003uence of incomplete data and faulty data In association rule mining these two problems are usually dealt with by introducing the measures of support and con\002dence  A nai v e translation of the support and con\002dence to the terminology of EL GCIs could look like this supp A v B   j  C u D  I j 001 I conf C v D   j  C u D  I j j C I j 10 The idea is that by searching only for GCIs with a certain minimal support one can mitigate the effects of incomplete data At the same time imposing a minimum on the con\002dence can limit the effects of noise in the data Several obstacles need to be overcome to make this approach work First we have argued that the large number of possible GCIs requires a compact representation e.g a base Now if A v B and B v C both have a con\002dence greater than 0.9 the GCI A v C  even though it follows from the 002rst two may have a con\002dence as low as 0.81 Hence imposing a lower limit on the con\002dence results in a set of GCIs that is no longer closed with respect to semantic deductions In such a situation computing a base is meaningless An obvious way to mend this is to consider both GCIs with suf\002cient con\002dence and support as well as all GCIs that follow semantically from them to be trustworthy This set of trustworthy GCIs has the necessary closure property but has not been researched theoretically An alternative approach is to look for other compact representations of the GCIs with suf\002cient support One might think of an analogous notion to frequent closed itemsets e.g frequent closed EL concept descriptions and Iceberg lattices We also need to address whether the naive translation of the classical notions of support and con\002dence really does what one intuitively expects it to As a toy example assume that a dataset about art is very incomplete containing only one Museum  the Louvre  but thousands of pieces of art that belong to the Louvre cf Figure 3 Clearly the GCI Museum v 9 hasLocation  FrenchCity  Mona Lisa Venus de Milo Louvre Paris Museum FrenchCity hasLocation ownedBy ownedBy    Fig 3 Classical Support is Counterintuitive which states that all museums are in a French city is no more trustworthy than the GCI 9 ownedBy  Museum v 9 ownedBy   9 hasLocation  FrenchCity   which states that something that is owned by a museum is owned by a museum in a French city While the 002rst GCI has very small support there is only one museum in the dataset the latter has very large support in the dataset there are thousands of pieces of art belonging to a museum Whether other notions of support are less counterintuitive needs to be investigated B Speeding Up the Mining Process In its current state the runtime and the memory requirements of our implementation leave plenty of room for improvement While it is unlikely that the bottleneck with respect to runtime that is presented by Next-Closure can be avoided completely there are some improvements that can be made There is hope that the introduction of a concept constructor  describing the empty concept will signi\002cantly improve memory requirements by making the All description redundant All is a very large concept description and since it occurs quite frequently a large amount of memory is quickly 002lled with various instances of All  Another rather technical improvement would be to avoid computing large intermediate results This is mostly due to computing large description graphs which then turn out to be highly redundant It would be more desirable to directly compute a smaller part of this description graph which is suf\002cient for our purposes Improvements in this direction would not only decrease the memory requirements for the implementation but also the overall runtime because extra computation for reducing description graphs could be avoided As a rather drastic measure one could impose an upper bound on the role depth of the EL descriptions This would both remove the need for cyclic concept descriptions and eliminate concept descriptions that consume large amounts of memory Our experiments provide anecdotal evidence that GCIs with larger role depths are usually among the least interesting C Quality Assessment The computed GCIs are sound and complete for the given data set However that neither means that they are correct 
1089 


in the domain nor that they are complete therein Therefore the results computed should merely be regarded as a starting point for constructing of a knowledge base Whether the quality of the GCIs that are produced is suf\002cient in practical applications should be examined in a case study in cooperation with domain experts VI C ONCLUSION We have prototypically implemented an algorithm that is able to mine a small set of EL GCIs from a dataset We have tested this algorithm on two real world datasets The results at least for the relatively error-free dataset of the DrugBank appear promising However for a 002nal verdict a quality assessment with domain experts will be needed We have also observed and discussed the implications of incomplete and faulty data as they appeared in the DBpedia dataset To the purpose of improving the quality of the GCIs when mining from imperfect data we have suggested to look into measures similar to support and con\002dence which are known from association rule mining R EFERENCES   F Baader D Calvanese D McGuinness D Nardi and P F PatelSchneider Eds The Description Logic Handbook Theory Implementation and Applications  Cambridge University Press 2003   R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases in Proceedings of the ACM SIGMOD International Conference on Management of Data  May 1993 pp 207–216   F Distel Learning description logic knowledge bases from data using methods from formal concept analysis Ph.D dissertation TU Dresden Dresden Germany June 2011   F Baader and F Distel Exploring 002nite models in the Description Logic EL gfp  in Proc of the 7th Int Conf on Formal Concept Analysis ICFCA 2009  S Ferr  e and S Rudolph Eds Springer 2009   S Auer C Bizer G Kobilarov J Lehmann and Z Ives Dbpedia A nucleus for a web of open data in In 6th Intl Semantic Web Conference Busan Korea  Springer 2007 pp 11–15   D2R server publishing the drugbank database FU Berlin Available http://www4.wiwiss.fu-berlin.de/drugbank   the data hub Comprehensive Knowledge Archive Network Available http://ckan.net   K Spackman K Campbell and R Cote SNOMED RT A reference terminology for health care pp 640–644 1997 fall Symposium Supplement   M Ashburner C A Ball J A Blake D Botstein H Butler J M Cherry A P Davis K Dolinski S S Dwight J T Eppig M A Harris D P Hill L Issel-Tarver A Kasarskis S Lewis J C Matese J E Richardson M Ringwald G M Rubin and G Sherlock Gene Ontology Tool for the uni\002cation of biology Nature Genetics  vol 25 no 1 pp 25–29 2000   B Motik B C Grau I Horrocks Z Wu A Fokoue and C Lutz Owl 2 web ontology language Pro\002les W3C Recommendation October 2009   F Baader B Ganter U Sattler and B Sertkaya Completing Description Logic knowledge bases using Formal Concept Analysis in IJCAI07  2007   S Rudolph Relational exploration  combining Description Logics and formal concept analysis for knowledge speci\002cation Ph.D dissertation Technische Universit  at Dresden 2006   B Ganter and R Wille Formal Concept Analysis Mathematical Foundations  New York Springer 1997   F Distel Hardness of enumerating pseudo-intents in the lectic order in Proc of the 8th Int Conf on Formal Concept Analysis ICFCA 2010  ser Lecture Notes in Arti\002cial Intelligence B Sertkaya and L Kwuida Eds vol 5986 Springer 2010 pp 124–137   D Borchmann Implementing the exploration algorithm for ELgfp TU Dresden Tech Rep 2011   D2R server publishing the diseasome dataset FU Berlin Available http://www4.wiwiss.fu-berlin.de/diseasome   G Stumme R Taouil Y Bastide N Pasquier and L Lakhal Computing iceberg concept lattices with TITANIC Data Knowl Eng  vol 42 no 2 pp 189–222 2002 
1090 


incremental option for a data mining algorithm is of course preferable in an inductive database system, since it allows the exploitation of all the available information in the system in order to speed up the response time The better performance of incremental algorithm depicted in the result section  worked on problems with item and context dependent constraint present a solution for item extraction from frequently updated database. This is done by running first the  Proceedings of the International Conference on Communication and Computational Intelligence 2010  523  mining algorithm of our choice \(on the problem defined by the query but without the context dependent constraints then applying the incremental algorithm on top of it \(with the addition of context dependent constraints whenever the mining constraints select a very small part of the original dataset, proposed incremental update strategy is likely to be very fast and efficient REFERENCES 1]  G. Grahne and J. Zhu, Mining Frequent Itemsets from Secondary Memory, IEEE Intl Conf. Data Mining \(ICDM 04 2]   J. Han, J. Pei, and Y. Yin, Mining Frequent Patterns without Candidate Generation, ACM SIGMOD, 2000 3]  Y.-L. Cheung, Mining Frequent Itemsets without Support Threshold With and without Item Constraints, IEEE Trans. Knowledge and Data Eng., vol. 16, no. 9, pp. 1052-1069, Sept. 2004 4]   G. Cong and B. Liu, Speed-Up Iterative Frequent Itemset Mining with Constraint Changes, IEEE Intl Conf. Data Mining \(ICDM 02 107-114, 2002 5] C.K.-S. Leung, L.V.S. Lakshmanan, and R.T. Ng, Exploiting Succinct Constraints Using FP-Trees, SIGKDD Explorations Newsletter, vol. 4 no. 1, pp. 40-49, 2002 6]  T. Uno, M. Kiyomi, and H. Arimura, LCM ver. 2: Efficient Mining Algorithms for Frequent/Closed/Maximal Itemsets, IEEE ICDM Workshop Frequent Itemset Mining Implementations \(FIMI 7]   J. Pei, J. Han, and L.V.S. Lakshmanan, Pushing Convertible Constraints in Frequent Itemset Mining, Data Mining and Knowledge Discovery, vol. 8, no. 3, pp. 227-252, 2004 8]   M. Botta, J.-F. Boulicaut, C. Masson, and R. Meo, A Comparison 


between Query Languages for the Extraction of Association Rules 9]  E. Baralis, T. Cerquitelli, and S. Chiusano, Index Support for  Frequent Itemset Mining in a Relational DBMS, 21st Intl Conf. Data Eng ICDE 10]   G. Liu, H. Lu, W. Lou, and J.X. Yu, On Computing, Storing and Querying Frequent Patterns, Ninth ACM SIGKDD Intl Conf Knowledge Discovery and Data Mining \(SIGKDD 


frequent, and there is no equal-support pruning in its subsets. So the Bitwise And Operation on binary strings of 0001100011 and 0001100011 is employed. As the result is 0001100011, the support of the itemset {G C, E} is 4, which is equal to {G, C}. E is put into the equal-support set of {G, C}, and does not need to add a new son node. When itemset {C, A, E} is checked, all of its subsets are frequent, and none of them is in the equal-support set. The corresponding two bit strings 0001101111 and 0011101011, are operated by &. The resultant string is 0001101011. The number of 1 in string 0001101011 is 5. So the support of itemset {C, A E} is 5, which is not equal to the support of {C, A}. So a new son node is added to {C, A}. The Trie after generation\(3  5 8 8 G C A E A 4 C 4 E 6 A 6 E 6 E 7 5 E 0001101011 E  Fig. 2. Trie after generation\(3 In generation\(4 therefore, the height of the Trie does not increase. All frequent itemsets are in Fig. 2. In the last step, all frequent itemsets are written out according to the Trie  5. EXPERIMENTAL RESULTS  


The proposed algorithm is tested on all the five datasets prepared by Roberto Bayardo, from UCI and PUMSB datasets. The datasets are available at http://fimi.cs.helsinki.fi. The characteristics of the datasets are shown in Table 6. The first column contains the names of the datasets. The second column shows the number of items contained in each dataset. The third column shows the average length of each transaction and the last column indicates the total number of transactions in each dataset TABLE 6 DATABASE CHARACTERISTICS Datasets Items Avg. length Records mushroom 119 23 8,124 chess 75 37 3,196 pusmsb* 2,088 50.4 49,046 connect 129 43 65,557 pusmsb 2,113 74.0 49,046 In order to illustrate the performance of the proposed algorithm, BitApriori is compared to the fast Apriori implemented in Ferenc [20], and another similar recently published, algorithm Index-BitTableFI proposed by Song [16]. In order to show the efficiency of the pruning technology employed in BitApriori another algorithm BitAprioriNE, which is the same as BitApriori, except that it does not use the special equal-support pruning, is designed. All of the above four algorithms are implemented in C++ and compiled with Microsoft Visual C++ 6.0. The experiments are performed on a Windows XP PC equipped with a Pentium 2.0 GHz CPU and 1.5 GB of RAM memory  For each dataset, a mass of different support thresholds are tested, and the five most important of them are chosen for reporting in this paper. The experimental results are shown in Fig. 3-7. In the figures, the y-coordinate denotes the execution time \(in seconds while x-coordinate denotes the support threshold 0 100 200 300 400 


500 600 0.05 0.06 0.07 0.08 0.09 0.1 0.11 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.3 Execution time comparison on mushroom  0 200 400 600 800 1000 0.45 0.5 0.55 0.6 0.65 0.7 0.75 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.4 Execution time comparison on chess  0 500 1000 1500 2000 2500 3000 3500 4000 0.25 0.3 0.35 0.4 0.45 0.5 0.55 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.5 Execution time comparison on pusmsb 0 1000 


2000 3000 4000 5000 6000 0.65 0.7 0.75 0.8 0.85 0.9 0.95 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.6 Execution time comparison on connect  0 500 1000 1500 2000 2500 0.675 0.725 0.775 0.825 Apriori BitApriori BitAprioriNE Index-BitTableFI  Fig.7 Execution time comparison on pusmsb  As shown in Fig. 3, BitApriori outperforms all other algorithms, and the dominance is apparent. In this dataset, the special equal-support pruning works efficiently. In Fig. 4, BitAprioriNE beats Apriori and Index-BitTableFI. In Fig. 5, the effectiveness of the proposed algorithm is verified, especially when the minimum support is low. In Fig. 6, BitAprioriWE exhausts the memory when the support threshold is lower than 0.8, but BitApriori does not. That means the special equal-support pruning contributes to save the memory. In Fig. 7, when the threshold is larger than 0.725, Apriori beats the BitApriori. But BitApriori outperforms the Apriori, when the threshold is lower than 0.725  


Apriori does not use the binary string and the special equal-support pruning. The BitTable is employed in Index-BitTableFI, but there is no special equal-support pruning, except for the frequent 2-itemsets BitAprioriNE outperforms Apriori in Fig. 3-5, but not in Fig. 6-7, because of the limitation of memory BitAprioriNE does better than Index-BitTableFI in Fig 4 and Fig. 5. In the mushroom, there is a vast number of equal-support itemsets for frequent 2-itemset. So Index-BitTableFI outperforms BitAprioriNE  On one hand, the special equal-support pruning is a useful technique for improving efficiency. The performance is improved significantly, especially when the databases contain many equal-support itemsets, such as the mushroom. It also reduces memory requirement On the other hand, the technique of binary string in BitApriori improves the efficiency of Apriori. These two techniques combine perfectly in BitApriori. So BitApriori has very good performance. It is the best for all of the five datasets  6. CONCLUSIONS  In this paper, two effective techniques are employed to improve the performance of Apriori, by reducing the cost of candidate generation, and by support counting The two effective techniques are integrated perfectly in BitApriori, and improve the computational efficiency significantly. Experimental results have shown that BitApriori outperforms the fast Apriori and Index-BitTableFI, especially when the minimum support threshold is low  When the database is large, the BitApriori may suffer the problem of memory scarcity. So how to solve the memory problem will be the question addressed in one of our future works. And another work is to improve Bitwise And Operation on the binary string, or replace it by some more effective techniques  REFERENCES 


 1] Agrawal R., T. Imielinski, A. Swami, Mining association rules between sets of items in large databases, in Proceedings of the ACM SIGMOD Conference on Management of Data. pp. 207-216 1993 2] Agrawal R., R. Srikant, Fast algorithms for mining association rules, The International Conference on Very Large Databases, pp. 487-499, 1994 3] Zaki M.J., S. Parthasarathy, M. Ogihara, W. Li New algorithms for fast discovery of association rules, in Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining, pp. 283-296, 1997 4] Han J., J. Pei, Y. Yin, Mining frequent patterns without candidate generation," in Proceedings of the 2000 ACM SIGMOD international conference on Management of data, ACM Press, pp. 1-12, 2000 5] Pork J.S., M.S. Chen, P.S. Yu, An effective hash based algorithm for mining association rules, ACM SIGMOD, pp. 175-186, 1995 6] Brin S., R. Motwani, J.D. Ullman, S. Tsur Dynamic itemset counting and implication rules for market basket data, in Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 255264, 1997 7] Brin S., R. Motwani, C. Silverstein, Beyond market baskets: generalizing association rules to correlations, in Proceedings of the ACM SIGMOD International Conference on Management of Data Tuscon, Arizona, pp. 265-276, 1997 8] Toivonen H., Sampling large databases for association rules, in Proceedings of 22nd VLDB Conference, Mumbai, India, pp. 134-145, 1996 9] Savasere A., E. Omiecinski, S.B. Navathe, An efficient algorithm for mining association rules in large databases, in Proceedings of 21th International Conference on Very Large Data Bases VLDB'95 10] Tsay Y.J., J.Y. Chiang, CBAR: an efficient method for mining association rules, Knowledge Based Systems, 18 \(2-3 


11] Liu G., H. Lu, W. Lou, Y. Xu, J.X. Yu, Efficient mining of frequent patterns using ascending frequency ordered prefix-tree, Data Mining Knowledge Discovery, 9 \(3 12] Grahne G., J. Zhu, Fast algorithms for frequent itemset mining using FP-Trees, IEEE Transaction on Knowledge and Data Engineering, 17 \(10 1347-1362, 2005 13] Zaki M.J., Scalable algorithms for association mining, IEEE Transactions on Knowledge and Data Engineering, 12 \(3 14] Zaki M.J., K. Gouda, Fast Vertical Mining Using Diffsets, in Proceedings of the ACM SIGMOD International Conference on Knowledge Discovery and Data Mining, pp. 326-335, 2003 15] Dong J., M. Han, BitTableFI: an efficient mining frequent itemsets algorithm, Knowledge Based Systems, 20 \(4 16] Song W., B.R. Yang, Z.Y. Xu, Index-BitTableFI An improved algorithm for mining frequent itemsets, Knowledge Based Systems, 21 \(6 507-513, 2008 17] Ferenc B., Surprising results of trie-based FIM algorithms, IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'04 CEUR Workshop Proceedings, vol. 90, G. Bart, J.Z Mohammed, and B. Roberto, Eds, Brighton, UK 2004 18] Ferenc B., A Survey on Frequent Itemset Mining Technical Report, Budapest University of Technology and Economic, 2006 19] Bart G., Survey on Frequent Pattern Mining Manuskript, 2002 20] Ferenc B., A fast APRIORI implementation IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'03 USA, 2003  


15] R. Agrawal and R. Srikant, "Fast algorithms for mining association rules in large Databases," presented at the Proceedings of the 20th International Conference on Very Large Data Bases, 1994 16] NKUDIC. \(June, 2006, National Kidney and Urologic Diseases Information Clearinghouse:Prostate Enlargement. Available http://kidney.niddk.nih.gov/kudiseases/pubs/prostateenlargement accessed on 10/06/2010 17] M. J. ZAKI, "Mining Non-Redundant Association Rules," Data Mining and Knowledge Discovery, 2004 18] Y. Xu and Y. Li, "Mining for Useful Association Rules Using the ATMS," presented at the International Conference on Computational Intelligence for Modelling, Control and Automation, and International Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC05 544 2010 10th International Conference on Intelligent Systems Design and Applications 


8]  Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine. Proceedings of the seventh international conference on World Wide Web 7: pp. 107-117, 1998 9]  J. Pei, J. Han, B. Mortazavi-Asl and H.Zhu, Mining Access Patterns Efficiently from Web Logs, Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 396-407 2000 10]  C. H. Cai, A. W. C. Fu, C.H. Cheng and W. W. Kwong, Mining Association Rules with Weighted Items, In Database Engineering and Applications Symposium, Proceedings IDEAS'98, pp. 68  77, 1998 11]  F. Tao, F. Murtagh and M. Farid, Weighted Association Rule Mining using Weighted Support and Significance Framework, In Proceedings of the 9th SIGKDD conference, 2003 12]  Show-Jane Yen, An Efficient Approach for Analyzing User Behaviors in a Web-Based Training Environment, International Journal of Distance Education Technologies, Vol. 1, No. 4, pp.55-71, 2003 13]  Show-Jane Yen, Yue-Shi Lee and Chung-Wen Cho, Efficient Approach for the Maintenance of Path Traversal Patterns, In Proceedings of IEEE International Conference on e-Technology, eCommerce and e-Service \(EEE 14]  M. Spiliopoulou and L. C. Faulstich, Wum: A web utilization miner EDBT Workshop WebDB98, Springer Verlag, 1996 15]  M. S. Chen, J. S. Park and P. S. Yu, Efficient data mining for path traversal patterns,  IEEE Transactions on Knowledge and Data Engineering, pp. 209-221, 1998 16]  H. Yao,H. J. Hamilton, and C. J. Butz, A Foundational Approach to Mining Itemset Utilities from Databases, Proceedings of the 4th SIAM International Conference on Data Mining, Florida, USA, 2004 17]  Z. Chen, R. H. Fowler and A. Wai-Chee Fu, Linear Time Algorithm for Finding Maximal Forward References, Proceedings of International Conference on Information Technology. Computers and Communications  \(ITCC'2003 18]  T. Jing, Wan-Li Zou and Bang-Zuo Zhang, An Efficient Web Traversal Pattern Mining algorithm Based On Suffix Array, Proceedings of the 3rd International Conference on Machine Learning and Cybernetics , pp 1535-1539, 2004 19]  Show-Jane Yen, Yue-Shi Lee and Min-Chi Hsieh, An efficient incremental algorithm for mining Web traversal patterns, Proceedings of the 2005 IEEE International Conference on e-Business Engineering ICEBE05 20]  L. Zhou, Y. Liu, J. Wang and Y. Shi, Utility-based Web Path  Traversal Pattern Mining, Seventh  IEEE International Conference on Data 


Mining Workshops, pp. 373-378, 2007 21]  C. F. Ahmed, S. K. Tanbeer, Byeong-Soo Jeong and Young-Koo Lee Efficient mining of utility-based web path traversal patterns, 11th International Conference on Advanced Communication Technology ICACT09 22]   http://en.wikipedia.org/wiki/PageRank 23] en.wikipedia.org/wiki/Association_rule_mining  Attributes? FPW Algorithm FTPW Algorithm Recognition of User behavior Visiting Frequency Page Rank Time Spent on Web page Page Size Accessibility of required information in less time Improving Web navigation and system design of Web applications  Enhancing server performance 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 200 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


