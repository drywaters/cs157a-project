Spatial Association Rules Mining for Land Use Based on Fuzzy Concept Lattice  Jiqiang Niu 1 Yang Zhang 2 Wenjuan Feng 1 Lele Ren 1 1 School of Urban and Enviro nmental Science, Xinyang Normal University, Xinyang, China 2 School of Resources and Environment Science, Wuhan University, Wuhan, China Corresponding author chant-cereal@163.com   Abstract Spatial data mining can extract the spatial patterns and characteristics, general relations of spatial and non spatial data and other data features in common that hidden in the spatial database. Formal concept analysis theory is very suitable for data mining research. In th is paper, a fuzzy conce pt lattice is proposed to mine the spatial association of knowledge. The incremental algorithm and drawing algorithm of Hasse are established, and the index tree is applied in this algorithm to solve the problem of complex spatialsystem. This paper selects the land use database and other data of Yicheng, Hubei province, China, which are integrated by existing mathematical models. Th ese data compose a new data set that can be mining by propose d algorithm. The fuzzy concept lattice is applied to acquire the spatial association rules of land use Keywords-land use; spatial data mining; spatial association rules fuzzy concept lattice I I NTRODUCTION  In recent years, due to the speedy development of earth observation technology, data base technology and network technology within the field of space information technology and the construction of a large number of spatial databases, the acquisition and management of spatial data becomes more conve  The com plexity and am ount of data overste p the analytical capacity of the human brain. How to mine the useful features and knowledge from th e land use database becomes a bottleneck of spatial database us e. From the spatial database spatial data mining can ex tract the spatial patterns and characteristics, general relations of sp atial and non spatial data and other data features in common that hidden in the spatial   As a part of data mining, spatial data mining has been a hot issue in the study in both China and abroad Spatial association rule is an im portant aspect of spatial data m ining, and the most current study of the data mining theory and technology are conduct ed for the association rules. With the extensity, mass, heterogeneity and the diversity of data types complexity of the structure and  other characteristics of data in land database, association rule mining research comes to new issues. The complexity of land use database require to map all the data and the majority of temporal and spatial data set showing time and space factors such as autocorrelation on the spatialtemporal framework, so spatial data mining research encounters a series of challenging problems Based on the land use data, the concept lattice with characteristics of a complete model is extended and introduced to spa tial association rule mining, and the use of rapid, quantitative mode l for the processing in analysis helps continuously find and supply knowledge, revealing the spatial association rules of phenomenon of land use, so as to obtain instructive knowledge on land use data II S PATIAL A SSOCIATION R ULE M INING M ODEL  As computers and earth observa tion technology advances more and more data is stored in the temporal and spatial database of land use, and the rapidly in creasing amount of information leads that data is difficult to be used with more and more fuzzy information. In 1982, based on th e concept and the application of mathematics expression of the concept level, the theory of formal concept analysis was first proposed by Rodolf Wille, a German professor. Fuzzy set theory was first proposed and applied to fuzzy control by Americans Zadeh, then many scholars continue to develop this theory, and now, the theory is an important method to solve the ambiguity Fuzzy set theory is used in this article to improve the traditional concept lattice, being useful for the mining association rules of th e spatial database of land A Representation of Fuzzy Con cept Lattice of Land Use Concept lattice which is the classical framework of the space associ ated knowledge di scovery, not only can improve the measurement of the mining algorithm but also is widely proved to be the only framework mining no lack of association In existing studies, the concept latt ice is constructed based on the standard single-value multi-backgrou nd, and the theory using the basic form of cross-tables \(cross table\of the data background \(formal context structure concepts associated with these processes, concept lattice discoveries understandable, meani ngful spatial and non-spatial knowledge The purpose of land use associati on rule mining is analyzing t he concepts of different levels and their relationships from these polygons and the fo rmal background mad e up with their Supported by the National Natural Science Foundation of China \(No 41001219\Natural Science Foundation of Education Department, Henan Province, China \(No. 2010A170002 978-1-61284-848-8/11/$26.00 ©2011 IEEE  


properties and Hasse diagram can be used to achieve the formal expression of plex giant system, in each region of the land use system, the number of its point features, linear features and polygon surface features polygons r the obtainment of the association rules, the properties of polygon are not simple, and sometimes involves dozens of attr ibutes, one of which is not a simple binary data, but con tinuous data. The ones of land database are belonging to such a property, and many properties have a certain space of ambiguity, therefore, the traditional approach to handling multi-value d attribute brings both a lot of data processing and some possibl e errors in mining association  the existing concept lattice, in favor of dealing with the spatial information in the land use  e a fuzzy concept lattice and its construction algorithm are pr esented, spatial association rules for the land use will be mined and some problems of complex space applications can be solved Research on fuzzy information an d its representation has existed, and there have been some achievements abroad, such as Karl Erich Wolffs fuzzy information representation, Buruscos L_ fuzzy set method and Girards fuzzy quantifier set method. In 1992 Karl Erich Wolff publis hed a book of "Conceptual interprettation of fuzzy theory" in wh ich a fuzzy information representation was bought forwar  With this method, property was represented by the value of fu zzy linguistic variables and the scale grid of values of linguistic variables was constructed, which was used to classify the object u nder the form background. In the form background of L_ fuzzy set s Burusco \(2000\using the fix point theory gave the calculati on method of lattice structure, and Girard \(2005\studied th e concept lattice construction problem of the data described by the fuzzy quantifier, dealing with fuzzy information by inserting fuzzy quantifiers to the data type definition sets. National study on the fuzzy concept lattices is later [8  Shanghai University had published a great deal papers, in which fuzzy concept lattice was studied; many scholars have carried out some correlative applied research to solve part of the problems in Web search [10 11], knowledge discovery and software evolution anal  hom e and abroad are mainly in the theoretical aspects of the concept lattice studies, which havent achieved a better fruit and dont take the charac-teristics of spatial databases into account, and there is still not studies about fuzzy concept lattice faced spatial data and its algorithm. Spatial data is fuzzy and the values of accurate information can be viewed as a special case of fuzzy information  h erefore the fuzzy formal context can be re gar ded as the membership table between property and object. The fuzzy set is a collection of   special definiti on, which reflects the degree of membership how its elements belo ng to the fu zzy set. Based on the examples of relevant definition, an example illustrates the processing of fuzzy concept lattice For example, there are five spots in the class diagram with four properties, and deal with th e original objects and properties with the membership function va lue instead of binary assignment generating the fuzzy formal context in Tab Following, we study the concept lattice structure of the standardized forms background TABLE I F UZZY F ORMAL C ONTEXT  Shape Index Water Distance Closed Road Slope Object Set  d1 d2 d3 d4 d5 d6 d7 P1 0 0.7 0.6 0 0 0.8 0.5 P2 0.7 0 0.6 0 0.9 0 0.1 P3 0.9 0 0.6 0 0.9 0 0 P4 0 0.8 0.6 0 0 0.8 0.6 P5 0.6 0 0 0.9 0.8 0 0.2 Threshold 0.5 0.5 0.5 0.5 0.5 0.5 0.5 B Fuzzy Concept Lattice There is a fuzzy formal context K U,A  U is the set of object o U  A is the graphical label set of fuzzy attributes d A   is a mapping to meet the style U×A 0 or   o,d\=m  Membership function can be a general function In the fuzzy formal context K  O U  D A  dj  0 dj 1 For every attribute d in D two mapping f and g can be defined in between O and D The formula can be describe as      j d OUfO doONod       1      j d OAgD odDNod       2 The function of f and g are called Galois connecting between power sets U and A  If the tuple C O U,D A meets O=g\(D and D=f\(O it is called a fuzzy concept of the fuzzy background K  where O and D are the extension and intension of the fuzzy concept. All the fuzzy concepts of fuzzy formal context K are denoted CS\(K The structure of CS\(K is produced by generalization and cases, which is defined as: If the fuzzy set O 1 and O 2  of C 1 O 1 D 1  and C 2 O 2 D 2  meet O 1 O 2  O 1 D 1 O 2 D 2   exists.  Ordered set of CS\(K obtained through this relation is called the fuzzy con cept lattice of the fuzzy background K Fuzzy parameter E a collection of objects O and attributes collection D=f\(O  d D parameter E  1   d oO ENod O   012  O the set of O 3 1 d dD EE the set of 4 D D D   012 When need to specify O,D they are written as E d O,D and E\(O,D  


Fuzzy parameter  to the objects set of O and the attributes set of D=f\(O  d D the parameter  can be defined as follows   2   d oO d Nod E O     012  5  1 d dD D    012 6 When need to specify O,D they are written as  d O,D and O,D  For each fuzzy concept C=\(O,D\\(O U,D A we can use the fuzzy background information to calculate the value of fuzzy parameter E and  and when getting association rules from the fuzzy concept lattice adopting E and  can avoid generating redundant nodes so as to avoi d creating ineffective rules E is the average m embership of all objects in C   is the average value of all the  d C which is the deviation degree of the membership of all objects in C in relative with the average value of the property d and  reflects the deviation degree of the membership of all objects  in C in relative with the average membership In the fuzzy concept lattice constru ction algorithm, in order to be able to use the gradual method to calculate parameters E and   the intermediate variables k d and h d are the introduced   d oO kNo   012  d 7 2   d oO hNod   012  8 Based on the in termediate variables k d and h d  E and  can be calculated as follows 11 d d dD dD k EE DD   012\012 O 9 2 1 2   2 11 1  dddd oO oO d dD dD dD Nod E h k E E DD O D O     015   012 012\012 012  2 1 2  d  012 10 In summary, each fuzzy concept lattice node is a formal concept, being made up with both extension and intension. In the process of rule mining, with the relationship between sets of connotation to describe the rules, it reflects the inclusion \(or similar inclusive\between sets of corresponding extension. As the concept lattice nodes reflect unifi cation of connotation and extension of the concept, and th e relationship between nodes reflects generalization and cases of conce pts, it is very suitable as the basis data structure for rule extraction C Fu zzy Concept Lattice Cons truction Algorithm Incremental construction of fuzzy co ncept lattice is to solve the fuzzy concept L  corresponding to the formal context of K=\(X x  A concept lattice the original fuzzy formal context given under the condition of the original concept lattice L  and the new object x  given by the origina l fuzzy formal context of K=\(U, A During the solving process of the fuzzy concept lattice generated incrementally, three main problems are to solve first, the formation of all th e new node; second, avoiding duplication of the nodes having been generated; third , side updates. In order to address these three issues effectively, each node in the original fuzzy concept lattice according to their content description of the relati onship between them and the new object, which you can define as different types: the same node updating the node, Lattice node generat ed and the ne Godin realizes that finding all the Lattice nodes generated is among the four nodes is the key to generate all the new nodes in the Incremental algorithm. The corresponded new node is different with the different la ttice node generated each other during the construction of the fuzzy concept lattice, and therefore we must deal with this issue in the algorithm. The basic idea of the construction of the fuzzy concept lattice is explained below According to the objects and attributes of t he fuzzy formal context, it initializes a node with only the top and bottom nodes of the fuzzy concept lattice. The fuzzy formal context removed is defined as a new adding node, if it is not a subset of existing nodes, the node will join the conn otation and extension in fuzzy concept lattice, computing fuzzy parameter E and intermediate calculation results k d and h d If the connotation of the intersection of the grid nodes and new nodes is not equal with the connotation of any node in th e grid range , the nodes in such extension will take the largest one, called the Lattice node generated, each lattice node generated with the new node as a new node is added together to the fuzzy concept lattice; if there are updating nodes in the new no des in the concept lattice, then these nodes will be updated, or add new nodes generated to the fuzzy concept lattice, and connect the new node to its child nodes and parent node, updating the intermediate results k d and h d of E  and  until all of the objects join in the concept lattice. R or each node which has completed construction of the fuzzy concept lattice, we compute and fuzzy parameters E and   When using the progressive method to concept lattice in fuzzy formal context, we can initialize an empty fuzzy concept lattice After all the objects being joined in the concept lattice, the t op node and the end nodes \(al so known as root\ed using the following way to search all the nodes without child node in fuzzy concept lattice If only one, such node will be defined as the end node; If more than one, such node generates the end node and adds it to the fuzzy concept lattice, adding the edges from these points to the end nodes. All the nodes of no parent node are Searched in the fuzzy concept lattice, if only one defining it as the top node; if many ones, generating them as the top nodes and adding them to the fuzzy concept lattice, and adding the edges from the top nodes to these points 


III C ASE S TUDY  A St udy Area The city of Yicheng, located in north-central of Hubei Provi nce, the Han River. It involves th e east to Suizhou, Zaoyang,the south to Zhongxiang Jingmen, the west to Nanzhang north to Xiangyang it is across the longitude of 111 ° 57'-112 57 ',the latitude of 31 ° 26'-31 ° 54 ';its total land area is of 2,107.86 square kilometers, accounting for 10.69% of the total area of Xiangfan City; it rules over 8 towns, 2 street offices; The total population is 57.26 milli on,of which urban population is 24.38 million. Yicheng city belongs to easily accessible city known as Wuyi arteries, eight provinces thoroughfares, for Jiao zuo\\(zhou National Road, the Han River waterway being from north to sout h, Suinan Road acrossing from east to west, high-speed through Xiang Jing Habitat, Xiangfan Airport 40 km away from Yicheng, has formed a threedimensional surface and air transportation network, with a more obvious location advantage. The city's urban system is being completed, infrastructure beginning to take shape, which can meet the needs of productions and life of the residents. There are the Transportation and communi cation infrastructure, and the region's highway mileage of 2,668.6 km. Telecommunication facilities are available, the mobile communication network covering the whole region to achieve digital transmission switching program control B La nd Use Data Preparation According to the 2005 survey data of land use and change st atus of the statistical analysis dat abase, by the end of 2005, land area of Yicheng city is of 210,785.87 hectares of which agricultural land area includes 148,548.28 hectares, accounting for 70.48% of total land area, construction land area including17636.70 hectares, accounting for land 8.36% of total area, other land area of 44,600.89 hectares included, accounting for 21.16% of total land area \(T ABLE  m urban land use of the city \(Fig. 2\be seen is on the regional differenttiation of land use obviously and relatively larg e polygon area used for land use zoning  Figure 1 Location map of Yicheng TABLE II U RBAN L AND U SE S TRUCTURE S HOULD F ORM  Land Use Type Spot Max Area Min Area Area \(ha Ratio Total land area 45311 1846.93 0.01 210785.87 100.0 agricultural land 36865 1846.93 0.06 148548.28 70.48 Arable land 16320 568.01 0.09 72647.55 34.47 Corner 663 153.24 0.08 4232.01 2.01 Woodland 4454 1846.93 0.15 54083.07 25.66 Other agricultural land 15428 66.85 0.06 17585.65 8.34 Construction land 6792 1048.49 0.01 17636.70 8.36 Urban land 6210 1048.49 0.01 11829.62 5.61 Urban industrial land 268 1048 49 0.04 1625.59 0.77 Rural land 5942 120.92 0.01 10204.03 4.84 Other construction land 582 703.86 0.04 5807.08 2.75 Other land 1654 674.72 0.15 44600.89 21.16   Figure 2 Land use map of Yicheng \(2005 C Data Preprocessing In order to obtain better results of spatial association rules, to reduce t he workload of the kernel of data mining, to improve the mining accuracy of the data, we must do pre-cleaning of the date which is a key technology to solve the existing vacancies in the original data, outliers, noisy data and other data quality issues 1 Processi ng unit polygons During the obtaining of ur ban land use spatial association rul es in Yicheng city, you can get the main map dates containing the 2005 land use database, Yicheng citys zoning map of the city administration, city mast er plans, town plans, functional diagram and basic farmland protec tion zones of thematic maps etc. However, these data alone is not enough, in order to achieve efficient spatial association rule mining, data pre-processing needed to generate the basic unit map, and access to relevant fields of information. In this paper, spatial overlay of vector data and the methods are used for processing 


2 Pre-processing spatial data structure The 1:1 million map of land in the database is broken, and has t he large number of patches, if status of polygons used as the basic unit, the computational efficiency will be very low. when building a database of status of library land, the polygon is not a ground type for the external bord ers, in the same way of a large class of continuous, because of ownership, ridge, small channels and the existence of some other natural lines, they are looked as several polygons tone during pain ting in the wild . For these polygons, they must be carried out a certain degree of consolidation before treatment, taking into account the circumstances of the experiment, through the spatial distance and polygon topology we have the same properties of polygons to merge, specifically through the topology to determine Rules: IF LandUT1= LandUT2 an d S1>20 and S2>20 THEN Land UT LandUT1 This rule can be understood as if the status of two adjacent pol ygons is equal to class, and area were greater than 20 hectares then merging the two polygons 3 Preprocessing of attribute data Data from different sources in differe nt departments because the departments have different requirements, there may be the case of repeated sampling, after the integration of the database after data integration pron e to the phenomenon of data redundancy, which not only can not guarantee that the data can be better than the thin results, but will add unnecessary overhead during the computation causing waste of resources. Some algorithms can only handle discrete values, if the input data is continuous, satisfactory results can not be excavated, it is necessary to discretize continuous data. Two data reduction techniques of Removed redundant pr operties and discretization are mainly used in this paper D C onstruction of Fuzzy Concept Lattice and Mapping Hasse Spatial association rule mining model of land can be used for m ining spatial association rules specifically including some steps 1 Data initialization Many attribute dates are continuous, therefore need to prep rocess the data to improve mining efficiency, and can express more knowledge with less ru les. Although the data in the database has been preprocessed, it provides the user with selecting the properties option, which shows the prototype interface, the user can select different attributes for polygon spatial association rule mining 2 Par ameters setting Property for future mining parameters can be set, and save the generated rules to the appropriate folder, because this rule of t his paper is to be managed throug h the knowledge base, so the resulting file will be automatically saved to the knowledge base Due to the vast amounts of data land use, this is automatically achieved all the records in the database file for processing 3 Gene rate Hasse diagram The experimental system also provides a mapping function Hasse, leading to realize the visualization of knowledge mining spatial association. Based on fuzzy concept of lattice-based data of mining algorithms, in the mining process, the data generated reflect the relationship between the conceptual levels of Hasse diagram. The Hasse diagram is shown in Fig. 3 by the above experiment  Figure 3 Rresult of spatial association rule mining by Hasse diagram 4 Rules mining As the complexity of land use system, it is difficult to get th rough all the rules once, you can select the appropriate number of experiments and analysis of excavated objects, in order to facilitate mining rules more useful, and be used to solve practical problems. The Hasse diagram is based on the former proposed model in the paper can do rule mining, and access to 83 spatial association rules initially by calculating. The rules are stored into the knowledge base in the form of a relational database, but these are difficult to understand for peopl e, and that you need to open things Knowledge Base every time is a very cumbersome thing In this study, rules are saved as th e texts to read and understand easily and form part of the d ecoded initial association rules are below TABLE III T ABLE OF S PATIAL A SSOCIATION R ULES  No Spatial association rules Sup Con 1 IF FW=1 SD 1 FT=2 AD=1 CO=1 TD=0.6 AI=3 CS=AL SI=2 THENLT=JN 0.4113 0.8261 2 IF FW=1 SD 1 FT=1 AD=1 CO=1 TD=1 AI=1 CS=TC SI=1 THENLT=JN 0.3682 0.8265 3 IF FW=1 SD 3 FT=3 AD=2 CO=2 TD=0.6 AI=1 CS=FL SI=1 THENLT=AL 0.3217 0.9529 4 IF SD={1 2 3 FR 1 CO=2 TD=2 AI=4 CS=RC SI=4 THENLT=GL 0.3571 0.8316 5 IF SD=4 CO=3 CS=TC SI=3, THENLT=FL 0.3838 0.8567   IV C ONCLUSION  Formal concept analysis theory also known as concept lattice theory, is a powerful tool to analysis the generating process of the concept from the data in ma thematically formal methods 


which is the same with the pr ocess that a data mining is from large amounts of data generated. Therefore, theory of it is very suitable for data mining research. In this paper, because the concept lattices can not express the problems of space, the method of constructing of co ncept lattices has been researched under the context of a multi-val ue and extended it. The fuzzy concept lattices for knowledge mining spatial association will study the evolutionary fuzzy c oncept lattice type construction algorithm. In view of the characteristics of spatial data for mass dictionary order based on the in troduction of the index tree algorithm and Hasse diagram drawing method have been introduced, proposing the me thods of extracting spatial association rules, establishing methods of a knowledge representation and storage. We select the land use dates of Yi city in Hubei Province, analyzing and obtaining land use spatial association rules, which can offer decision supports in land suitability evaluation, classifica tion and grading and land use planning The concept lattice improved and applied to space data m ining has broad application prospects, the current concept lattice construction algorithm of the efficiency is still relatively low, and the parallel construction operator concept lattices method has great advantages and poten tial. With the increasing in the forms and backgrounds of the handle, concept lattice parallel algorithm can solve massive, multi-dimensional, and complex spatial dates, which is the focus of future research R EFERENCES   J. Han and M. Kam ber, DataMining Concepts and Techniques San Mateo, CA : Morgan Kaufma nn Publishers, 2000 pp.1-35  H. Mille r and J. Han, Geographic data mining and knowledge diseovery Londonand Newyork: TAYLOR  FRANCIS, 2001, pp. 196-209  G. Bancer ek, Complete lattices,Ž Journal of Formalized Mathematics, vol 4\(2\, 2003, pp.1-7  H. Lai and D. Zhang, Concept lattices of fuzzy contexts: Formal concept analysis vs. rough set theory,Ž International Journal of Approximate Reasoning, vol. 50 \(4  J Medina, M. Aciego, and J. Calvino, Formal concept analysis via multiadjoint concept lattices,Ž Fuzzy Sets and Systems, vol. 16 \(1 pp.130…144  Y Lei and M. Luo, Rough concept lattices and domains,Ž Annals of Pure and Applied Logic, vol. 159\(1 pp. 333…340  L Nourine, A fast algorithm for building lattices,Ž Information Processing letters, vol. 27\(1 p. 199-204  O. K won and J. Kim, Concept lattices for visualizing and generating user profilesfor context- aware service recommendations,Ž Expert Systems with Applications, vol. 36 \(5 pp. 1893-1902  P Ghosh, K. Kundu, and D, Sar kar, Fuzzy graph representation of a fuzzy concept lattice,Ž Fuzzy Sets and Systems, vol. 16\(6  S Wang and D. Li, A perspective of spatial data mining. Geospatial Information, data mining and application,Ž Wuhan: Wuhan University Press. 2005, pp. l10  W ille R, Concept Lattices and conceptual knowledge systems Computers and Mathematics with Application vol. 23\(11 522  


pnewitemarray pnewitemarray=ptempitem>productid ptempitem=ptempitem->next cofnewitemarray=cofnewitemarray+1  else  newdbitemset=new dbitemset item item  customer_id customer_id customer_id  item item    item item  item item  item item   item item   itemsets for each y? { Tk-1 Tk-1} do  // Tk-1 is all frequent k-1-itemsets in t. Tk-1  Tk-1 is a natural join of Tk-1 and Tk-1 on the first k-2 items 


if? z| z=k-1 subset of y ? ?Hk-1.hassupport\(z then Hk.add\(y itemsets=itemsets? y end Dk= Dk? t   //such that t contains itemsets only in the set itemsets End Hk.prune\(min_sup IV. TEST CONTRAST The test data are sale data from a real company. There are 5581 affairs and 1559 different fields, in Fig.1 the data structure of improved PHP algorithm, candidates k-itemset Hash are in database Dk Fig.1 the structure of database Dk of improved PHP algorithm  The structure of the node customer_id is struct tid  short customerid struct tid *next struct dbitemset* pdbitemset  The structure of item struct dbitemset  short *pitem struct dbitemset *next  In the structure of dbitemset, pitem is point the field or item of fixed size array. For put out the itemset from array, should know the length k of the itemset , and take out k items combinations from array to gain a itemset. The size of tha array should be proper to fill the itemset, not too much nodes there, to save the memory size. The main program of candidate k-itemset ot database Dk is as follow newdbitemset=new dbitemset pnewitemarray=new short[SIZEOFPITEMARRAY ptempitem=pkitemset; // pkitemset point to the list of candidate k-itemset pnewitemarray=ptempitem->productid;     //product ID is the item of candidate k-itemset newdbitemset->pitem=pnewitemarray 


newdbitemset->next=NULL thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next while\(ptempitem!=NULL  if\(cofnewitemarray<SIZEOFP ITEMARRAY  pnewitemarray pnewitemarray=ptempitem>productid ptempitem=ptempitem->next cofnewitemarray=cofnewitemarray+1  else  newdbitemset=new dbitemset item item  customer_id customer_id customer_id  item item    item item  item item  item item   item item 


  pnewitemarray=new short[SIZEOFPITEMARRAY pnewitemarray=ptempitem>productid newdbitemset->pitem=pnewitemarray newdbitemset->next=NULL thisnewdbitemset->next=newdbitemset thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next   The test result is as Tab TABLE 1 ITEMSETS IN DIFFERENT MINIMUM SUPPORT COUNT The minimu m support count F1 | F| 2 | F| 3 F  4  F 5 Frequent set  7 1559 406 0 0 0 1965 6 1559 1960 0 0 0 3519 5 1559 8605 8 0 0 10172 4 1559 34172 129 1 4 2 35876 It can get the same frequent activities that PHP algorithm and improved PHP algorithm, in Tab.2 we can find that there is inverse ratio between frequent activity and minimum support count, when the minimum support counts decrease 


frequent activities increase, just as the real world. The cost of CPU time compared as Fig.2 of these two algorithms 213 124 89 73 57 53 52 65 0 40 80 120 160 200 240 4 5 6 7 Minimum Support Count T i m e  s e c o n d s  PHP Improved Fig.2 the compare CPU time cost of PHP and improved PHP From Fig.2, as support count decreasing, the CPU running time of these two algorithms gets increasing, but the PHP is much speeder than the other, especially when count in 6. so wen know the more frequent itemsets and their items, the more efficient of the algorithm V. THE APPLICATION After we got the frequent itemset by the improved PHP algorithm, we can generate the strong associate rule that satisfied minimum support and minimum confidence. The 


confidence can be computed by Confidence\(A ? B A | B A  B support_count\(A Support_count\(A  B support_count\(A And then we can use confidence threshold strengthen the association, we use improved PHP algorithm dig X company 2002 year sale data, minimum support is 5, 3-itemset, they are 83,549,915}, {485,558,1290}, {454,1097,1546 83,549,982}, {631,980,1490}, {454,1103,1546 810,1026,1469}, {360,830,1036}, and their none null subset as for frequent 3-item {83,549,915}, we can get 83 ? 549? 915,      confidence = 5/9 = 56 83 ? 915? 549,      confidence = 5/5 = 100 915 ? 549? 83,      confidence = 5/5 = 100 83? 915 ? 549,      confidence = 5/50 = 10 549? 83 ? 915,      confidence = 5/78 = 6 915? 83 ? 549,      confidence = 5/73= 7 If the minimum confidence threshold is 80%, only if 83 ? 915 ? 549and 915 ? 549 ? 83 can generate strong associate rules, after these rules, we can not only arrange the shelf of related goods in pairs or groups, but also combine those goods that have most consumers, promote the consumer the pretermission goods. For example, the stronger associate rule 83 ? 915? 549, from the database field product, we know product_id corresponding product_name, this stronger rule is pillow ? pillow clothe ? bedsheet , if there is someone bought pillow and pillow clothe, then promote good bedsheet is an efficient way of sale VI. CONCLUSION We put Hash candidates k-itemset into affair of Dk, that is itemset include in affair, so CPU running time decreased improved PHP algorithm is more efficiency, we can use this improved PHP algorithm in commercial database digging and other wide usages REFERENCES 1] YANG Xuejun. The application of CRM[J].computer application,2002\(5 2] CHEN Shuangqiu, LIU Dinghong, LI Hongxing, the costumer analysis and design based on data warehouse[J].computer engineering and application,2001\(4 3] Jawei Han and Micheline Kamber,Data Mining:Comcepts and 


Techniques \(2001 4] J.D.Holt,and S.M.Chung, Efficient Mining of Association Rules in Text Databases CIKM99,  Kansas City, USA, pp.234-242, \(Nov.1999 5] J.S.Park, M.S.Chen and P.S.Yu, Using a Hash-Based Method with Transaction Trimming for Mining Association Rules, IEEE Transactions on Knowledge and Data Engineering, Vol.9, No.5 Sept/Oct, 1997 6] S. A. zel and H. A. Gvenir, An Algorithm for Mining Association Rules Using Perfect Hashing and Database Pruning, in: Proceedings of  pnewitemarray=new short[SIZEOFPITEMARRAY pnewitemarray=ptempitem>productid newdbitemset->pitem=pnewitemarray newdbitemset->next=NULL thisnewdbitemset->next=newdbitemset thisnewdbitemset=newdbitemset cofnewitemarray=1 ptempitem=ptempitem->next   The test result is as Tab TABLE 1 ITEMSETS IN DIFFERENT MINIMUM SUPPORT COUNT The minimu m support count F1 | F| 2 | F| 3 F  4  F 5 Frequent set  7 1559 406 0 0 0 1965 6 1559 1960 0 0 0 3519 


5 1559 8605 8 0 0 10172 4 1559 34172 129 1 4 2 35876 It can get the same frequent activities that PHP algorithm and improved PHP algorithm, in Tab.2 we can find that there is inverse ratio between frequent activity and minimum support count, when the minimum support counts decrease frequent activities increase, just as the real world. The cost of CPU time compared as Fig.2 of these two algorithms 213 124 89 73 57 53 52 65 0 40 80 120 160 200 240 4 5 6 7 Minimum Support Count T i m e  s e c o n d s  PHP Improved Fig.2 the compare CPU time cost of PHP and improved PHP From Fig.2, as support count decreasing, the CPU running 


time of these two algorithms gets increasing, but the PHP is much speeder than the other, especially when count in 6. so wen know the more frequent itemsets and their items, the more efficient of the algorithm V. THE APPLICATION After we got the frequent itemset by the improved PHP algorithm, we can generate the strong associate rule that satisfied minimum support and minimum confidence. The confidence can be computed by Confidence\(A ? B A | B A  B support_count\(A Support_count\(A  B support_count\(A And then we can use confidence threshold strengthen the association, we use improved PHP algorithm dig X company 2002 year sale data, minimum support is 5, 3-itemset, they are 83,549,915}, {485,558,1290}, {454,1097,1546 83,549,982}, {631,980,1490}, {454,1103,1546 810,1026,1469}, {360,830,1036}, and their none null subset as for frequent 3-item {83,549,915}, we can get 83 ? 549? 915,      confidence = 5/9 = 56 83 ? 915? 549,      confidence = 5/5 = 100 915 ? 549? 83,      confidence = 5/5 = 100 83? 915 ? 549,      confidence = 5/50 = 10 549? 83 ? 915,      confidence = 5/78 = 6 915? 83 ? 549,      confidence = 5/73= 7 If the minimum confidence threshold is 80%, only if 83 ? 915 ? 549and 915 ? 549 ? 83 can generate strong associate rules, after these rules, we can not only arrange the shelf of related goods in pairs or groups, but also combine those goods that have most consumers, promote the consumer the pretermission goods. For example, the stronger associate rule 83 ? 915? 549, from the database field product, we know product_id corresponding product_name, this stronger rule is pillow ? pillow clothe ? bedsheet , if there is someone bought pillow and pillow clothe, then promote good bedsheet is an efficient way of sale VI. CONCLUSION We put Hash candidates k-itemset into affair of Dk, that is itemset include in affair, so CPU running time decreased improved PHP algorithm is more efficiency, we can use this improved PHP algorithm in commercial database digging and 


other wide usages REFERENCES 1] YANG Xuejun. The application of CRM[J].computer application,2002\(5 2] CHEN Shuangqiu, LIU Dinghong, LI Hongxing, the costumer analysis and design based on data warehouse[J].computer engineering and application,2001\(4 3] Jawei Han and Micheline Kamber,Data Mining:Comcepts and Techniques \(2001 4] J.D.Holt,and S.M.Chung, Efficient Mining of Association Rules in Text Databases CIKM99,  Kansas City, USA, pp.234-242, \(Nov.1999 5] J.S.Park, M.S.Chen and P.S.Yu, Using a Hash-Based Method with Transaction Trimming for Mining Association Rules, IEEE Transactions on Knowledge and Data Engineering, Vol.9, No.5 Sept/Oct, 1997 6] S. A. zel and H. A. Gvenir, An Algorithm for Mining Association Rules Using Perfect Hashing and Database Pruning, in: Proceedings of  the Tenth Turkish Symposium on Artificial Intelligence and Neural Networks \(TAINN'2001 Eds Gazimagusa, T.R.N.C. \(June 2001 7] SUN Zhengxi, The theory and technology of Intelligent control M].Beijing Qinghua press, 1997 8] Mitani, Koji, CRM pursues economies of depth [J]. Japanese Journal of Diamond Harvard Business, 1999, \(617  the Tenth Turkish Symposium on Artificial Intelligence and Neural Networks \(TAINN'2001 Eds Gazimagusa, T.R.N.C. \(June 2001 7] SUN Zhengxi, The theory and technology of Intelligent control M].Beijing Qinghua press, 1997 8] Mitani, Koji, CRM pursues economies of depth [J]. Japanese Journal of Diamond Harvard Business, 1999, \(617  


algorithm could not extract the correct hierarchy with 30 assigning five labels incorrectly to the root label. None of the HE algorithms could extract the correct hierarchy in the absence of 40% multi-labels. With 40% and Voting, the number of labels falsely assigned to the root was 13, while with GT it was only three. For BoosTexter, Voting assigned two labels wrongly to the root label in the experiment with TABLE I 20 NEWSGROUPS ALL, -20%, -30% AND -40% RESULTS Measure all 20% 30% 40%ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT A 0.635 0.638 0.429 0.549 0.613 0.633 0.383 0.456 0.596 0.619 0.322 0.412 0.563 0.591 0.255 0.387 F1 0.694 0.696 0.565 0.677 0.675 0.688 0.528 0.604 0.662 0.677 0.469 0.566 0.640 0.657 0.392 0.542 F 0.691 0.692 0.480 0.605 0.671 0.688 0.429 0.507 0.658 0.676 0.364 0.465 0.630 0.652 0.296 0.441 OE 0.221 0.220 0.336 0.222 0.259 0.236 0.387 0.275 0.273 0.259 0.415 0.301 0.301 0.291 0.434 0.316 RL 0.100 0.098 0.124 0.073 0.108 0.110 0.128 0.077 0.098 0.103 0.132 0.079 0.101 0.106 0.135 0.082 C 4.188 4.168 6.080 4.164 4.397 4.446 6.184 4.286 4.246 4.340 6.326 4.328 4.334 4.463 6.397 4.379 AP 0.789 0.790 0.677 0.778 0.774 0.782 0.657 0.758 0.769 0.774 0.645 0.747 0.759 0.764 0.638 0.740 AUPRC 0.775 0.772 0.618 0.749 0.743 0.727 0.581 0.691 0.733 0.722 0.555 0.671 0.715 0.708 0.535 0.660 H-loss 0.103 0.123 0.121 0.094 0.102 0.098 0.124 0.108 0.106 0.103 0.132 0.117 0.115 0.111 0.145 0.122 Wins 1 5 0 3 1 6 0 2 2 6 0 1 2 6 0 1 LCAPD 0 0 0 0 0 0 0 0 0 0 0.12 0 0.05 0 0 0 0.51 0.26 0.17 0 CTED 0 0 0 0 0 0 0 0 0 0 0.14 0 0.07 0 0 0 0.50 0.21 0.21 0 TO* 0 0 0 0 0 0 0 0 0 0 0.11 0 0.05 0 0 0 0.39 0.18 0.15 0 30% removed labels and and six labels in the experiment with 40% removed labels. GT resulted in zero distances in the both cases. Assigning more labels to the root creates more shallow and wider hierarchies \(trivial case as stated before The good hierarchy extraction with ART networks demonstrates the system robustness  even with strongly damaged data the system can rebuild the original hierarchy C. RCV1-v2 Dataset The next experiment was based on the tokenized version of 


the RCV1-v2 dataset introduced in [21]. Only the topics label set consisting of 103 labels arranged in a hierarchy of depth four is examined here. Documents of the original training set of 23,149 were converted to TF-IDF weights and normalized Afterwards the set was splitted in 15,000 randomly selected documents as training and the remaining as test samples In this case, the Voting variant of HE applied to the TTML resulted in the LCAPD, CTED and TO* values 0.12, 0.15 and 0.13, respectively. The corresponding values of the GT variant are 0.05, 0.07 and 0.05. The poor performance of the Voting method is due to the fact that for the TTML only very high threshold values succeed in removing enough noise The Voting results are thus dominated by bad hierarchies extracted for all but the highest thresholds The classification and HE results for this dataset are shown in Table II. ML-ARAM has better performance results on this data set in all points than ML-FAM except for RL being the best of all classifiers in terms of the multi-label performance measures. BoosTexter is the best in terms of all ranking measures For both HE algorithms the distances of BoosTexter are the best, those of ML-FAM second, followed ML-ARAM and ML-kNN. All three distance measures correlate. Interesting is also that for ML-kNN the distance values obtained by both HE methods are almost the same The hierarchy extracted by GT from the TTML has much lower distances values as compared with the hierarchies extracted by both methods from predicted multi-labels. This reflects a specific problem of HE, since only a small fraction of the incorrectly classified multi-labels can prevent building of a proper hierarchy. For example, 16.5% of misassigned labels in the extracted hierarchy are responsible for about 80% of LCAPD calculated from the predictions of MLARAM. This large part of the HE error is caused by only 4% of the test data. Under these circumstances the other distances behave analogically. Most labels were not assigned making them trivial edges, but six labels were assigned to a false branch. This can happen when labels have a strong correlation and in the step Hierarchy Construction of the basic algorithm the parent is not unique in the confidence matrix. BoosTexters results suffer less from this problem because it generally sets more labels for each test sample 


Both HE algorithms behaved similarly on the predictions of the ART networks. They constructed a deeper hierarchy than the original one and wrongly assigned the same 11 labels to the root node. The higher distances come from Voting assigning more labels to the wrong branch. For MLkNN both algorithms again create very similar hierarchy trees, both misassigned 28 labels to the root label. For BoosTexter it was seven with Voting and eight with GT Voting produced a deeper hierarchy here D. WIPO-alpha Dataset The WIPO-alpha dataset1 comprises patent documents collected by the World Intellectual Property Organization WIPO ments. Preprocessing was performed as follows: From each document, the title, abstract and claims texts were extracted stop words were removed using the list from [20] and words were stemmed using the Snowball stemmer [22]. All but the 1%-most-frequent stems were removed, the remaining stems were converted to TF-IDF weights and these were normalized to the range of [0, 1]. Again, TF-IDF conversion and normalization were done independently for the training and the test set. The original hierarchy consists, from top to bottom, of 8 sections, 120 classes, 630 subclasses and about 69,000 groups. In our experiment, only records from the sections A \(5802 training and 5169 test samples H \(5703 training and 5926 test samples 1http://www.wipo.int/classifications/ipc/en/ITsupport/Categorization dataset/wipo-alpha-readme.html August 2009 TABLE II RCV1-V2 RESULTS Measure ARAM FAM kNN BoosT A 0.748 0.731 0.651 0.695 F1 0.795 0.777 0.735 0.769 F 0.805 0.787 0.719 0.771 OE 0.077 0.089 0.104 0.063 RL 0.087 0.086 0.026 0.015 C 11.598 11.692 8.563 5.977 AP 0.868 0.860 0.839 0.873 AUPRC 0.830 0.794 0.807 0.838 H-loss 0.068 0.077 0.097 0.081 Wins 4 0 0 5 LCAPD 0.29 0.22 0.25 0.20 0.34 0.34 0.21 0.18 


CTED 0.32 0.23 0.28 0.22 0.38 0.37 0.24 0.20 TO* 0.27 0.18 0.22 0.17 0.31 0.30 0.21 0.17 document in the collection has one so-called main code and any number of secondary codes, where each code describes a group the document belongs to. Both main and secondary codes were used in the experiment, although codes pointing to groups outside of sections A and H were ignored. We also removed groups that did not contain at least 30 training and 30 test records \(and any documents that only belonged to such small groups 7,364 test records with 924 attributes each and a label set of size 131 In this case, the Voting variant of the HE algorithm applied to the TTML resulted in the LCAPD, CTED and TO* values of 0.13, 0.12 and 0, respectively. GT showed the same values. Remarkable are the TO* distances, which are equal to 0. This is due to the fact that the WIPO-alpha hierarchy contains 16 single-child labels that are not partitioned by the true multi-labels: whenever a single-child label j is contained in a multi-label, so is its child, and vice versa. It is therefore theoretically impossible to deduce from the multilabels which of them is the parent of the other. As a result the HE algorithms often choose the wrong parent, resulting in higher LCAPD and CTED values. TO*, as described above is invariant under such choices The results obtained on the WIPO-alpha dataset are shown in Table III. The classification performance of the ART-based networks on this dataset is slightly worse than that of BoosTexter. Mostly in the terms of OE, RL, C, AP, AUPRC, and H-loss measures BoosTexter is better because its rankings are better and it assigned more labels to each sample. But the ART networks have better HE results because their predicted labels are more consistent with the original hierarchy. MLkNN has the worst classification results and distance values again. The reason for the high relative difference between LCAPD as well as CTED and TO* obtained for the ART networks or BoosTexter as compared to the results of the other datasets is because most of the labels were assigned in the right branch but not exactly where they belong Both HE algorithms extracted the same hierarchy from the predictions of ML-ARAM and a very similar hierarchy for ML-FAM. About 5% labels were assigned wrongly to the 


root label in the hierarchies of the ART networks. For MLTABLE III WIPO-ALPHA\(AH Measure ARAM FAM kNN BoosT A 0.588 0.590 0.478 0.564 F1 0.694 0.691 0.614 0.693 F 0.682 0.682 0.593 0.679 OE 0.052 0.057 0.110 0.042 RL 0.135 0.136 0.056 0.025 C 25.135 25.269 22.380 11.742 AP 0.790 0.785 0.724 0.802 AUPRC 0.720 0.684 0.688 0.762 H-loss 0.090 0.093 0.149 0.079 Wins 1 2 0 6 LCAPD 0.16 0.16 0.17 0.17 0.32 0.38 0.21 0.21 CTED 0.18 0.18 0.19 0.19 0.38 0.53 0.27 0.27 TO* 0.05 0.05 0.07 0.07 0.24 0.32 0.08 0.08 kNN both HE methods wrongly assigned about the half of the labels and about 20% of total labels were assgined to the root label. Here, GT extracted a much worse hierarchy as shown by CTED being 0.15 higher for GT than for Voting For BoosTexter both HE methods built the same hierarchy and no label was wrongly assigned to the root. All extracted hierarchies were one level deeper than the original one Although Voting produced worse hierarchies than GT on two previous datasets, this time its distance values were comparable or even better. In comparison to Voting, GT has higher values for all distances on the multi-labels of MLkNN. Voting has the advantage of being a much simpler method and of being more dataset independent. Still the tree distances have the same ranking order for all classifiers for both HE methods VI. CONCLUSION In this paper we studied Hierarchical Multi-label Classification \(HMC tive was to derive hierarchical relationships between output classes from predicted multi-labels automatically. We have developed a data-mining-system based on two recently proposed multi-label extensions of the FAM and ARAM neural networks: ML-FAM and ML-ARAM as well as on a Hierarchy Extraction \(HE algorithm builds association rules from label co-occurrences 


and has two modifications. The presented approach is general enough to be used with any other multi-label classifier or HE algorithm. We have also developed a new tree distance measure for quantitative comparison of hierarchies In extensive experiments made on three text-mining realworld datasets, ML-FAM and ML-ARAM were compared against two state-of-the-art multi-label classifiers: ML-kNN and BoosTexter. The experimental results confirm that the proposed approach is suitable for extracting middle and large-scale class hierarchies from predicted multi-labels. In future work we intend to examine approaches for measuring the quality of hierarchical multi-label classifications REFERENCES 1] M. Ruiz and P. Srinivasan, Hierarchical text categorization using neural networks, Information Retrieval, vol. 5, no. 1, pp. 87118 2002 2] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni, Incremental algorithms for hierarchical classification, The Journal of Machine Learning Research, vol. 7, pp. 3154, 2006 3] , Hierarchical classification: combining Bayes with SVM, in Proceedings of the 23rd international conference on Machine learning ACM New York, NY, USA, 2006, pp. 177184 4] F. Wu, J. Zhang, and V. Honavar, Learning classifiers using hierarchically structured class taxonomies, in Proceedings of the 6th International Symposium on Abstraction, Reformulation And Approximation Springer, 2005, p. 313 5] L. Cai and T. Hofmann, Hierarchical document categorization with support vector machines, in Proceedings of the thirteenth ACM international conference on Information and knowledge management ACM New York, NY, USA, 2004, pp. 7887 6] C. Vens, J. Struyf, L. Schietgat, S. Dz?eroski, and H. Blockeel Decision trees for hierarchical multi-label classification, Machine Learning, vol. 73, no. 2, pp. 185214, 2008 7] E. P. Sapozhnikova, Art-based neural networks for multi-label classification, in IDA, ser. Lecture Notes in Computer Science, N. M Adams, C. Robardet, A. Siebes, and J.-F. Boulicaut, Eds., vol. 5772 Springer, 2009, pp. 167177 8] M. Zhang and Z. Zhou, ML-kNN: A lazy learning approach to multilabel learning, Pattern Recognition, vol. 40, no. 7, pp. 20382048 2007 9] R. Schapire and Y. Singer, BoosTexter: A boosting-based system for text categorization, Machine learning, vol. 39, no. 2, pp. 135168 


2000 10] K. Zhang, A constrained edit distance between unordered labeled trees, Algorithmica, vol. 15, no. 3, pp. 205222, 1996 11] A. Maedche and S. Staab, Measuring similarity between ontologies Lecture notes in computer science, pp. 251263, 2002 12] G. Carpenter, S. Martens, and O. Ogas, Self-organizing information fusion and hierarchical knowledge discovery: a new framework using ARTMAP neural networks, Neural Networks, vol. 18, no. 3, pp. 287 295, 2005 13] A.-H. Tan and H. Pan, Predictive neural networks for gene expression data analysis, Neural Networks, vol. 18, pp. 297306, April 2005 14] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps, IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 698713, 1992 15] Y. Freund and R. Schapire, A decision-theoretic generalization of online learning and an application to boosting, Journal of computer and system sciences, vol. 55, no. 1, pp. 119139, 1997 16] K. Zhang and T. Jiang, Some MAX SNP-hard results concerning unordered labeled trees, Information Processing Letters, vol. 49 no. 5, pp. 249254, 1994 17] G. Tsoumakas and I. Vlahavas, Random k-labelsets: An ensemble method for multilabel classification, Lecture Notes in Computer Science, vol. 4701, p. 406, 2007 18] K. Punera, S. Rajan, and J. Ghosh, Automatic construction of nary tree based taxonomies, in Proceedings of IEEE International Conference on Data Mining-Workshops. IEEE Computer Society 2006, pp. 7579 19] T. Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization, in Proceedings of the Fourteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1997, pp. 143151 20] A. McCallum, Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering, 1996 http://www.cs.cmu.edu/ mccallum/bow 21] D. Lewis, Y. Yang, T. Rose, and F. Li, RCV1: A new benchmark collection for text categorization research, The Journal of Machine Learning Research, vol. 5, pp. 361397, 2004 22] M. Porter, Snowball: A language for stemming algorithms, 2001 http://snowball.tartarus.org/texts/introduction.html 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


