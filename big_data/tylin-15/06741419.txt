Distributed Compilation System for High Speed  Software Build Processes   Geunsik Lim 1  Minho Lee 2  R.J.W.E. Lahaye 3 and Young Ik Eom 4  CICE 124  Department of Physics 3   S oftware Cente r 1  Sungkyunkwan University 1 234   Samsung Electronics 1  Suwon 440 746, Republic of Korea 1234  leemgs 1  zeteman2ya 2  lahaye 3 yieom 4 skku.edu   geunsik.lim 
1 samsung.com   Abstract  227  The i dle time  of personal computers ha s  increased steadily due to the generalization of computer usage and cloud computing. Clustering research aims at utilizing idle computer resources for processing a variable work load on a large number of computers. The workload  is processed co ntinually despite the volatile status of the individual computer resources. This paper proposes a distributed compilation system for improving the processing speed of CPU intensive software compilation s   This significantly reduces 
the compilation time of mass sources by using the idle resources. We expect gains of up to 65 compared to  non distributed compilation systems    Keyword s  227  Distributed Computing, Grid Computing, High Performance Computing Hi g h Throughput Computing Super Computing Distributed Compilation  I   I NTRODUCTION  Grid computing 1 3  can provide services that effectively distribute  tasks to suitable resources connected in a network For example, we can improve the processing speed of intensive scientific calculations by installing 
workload  management software  that  utilizes the idle Personal Computers  PCs  of  public libraries or administration systems 4 However studies  into high performance computing 5  6  still lack  research of  public computer facilit ies which have  a high percentage  of idle  time     Figure 1  Used time \(dark grey\ and idle time \(light grey\ of 300 dual core PCs on a typical week  day in a university library  T he idle time  of generic personal computers ha 
s  ultimately increased due to the generalization of computer usage and cloud computing environments   in educational institutions national administrative agencies, and public institutions. Figure 1 shows an example of the used and idle time s  of 300 dual core PCs in a university lib rary during a typical week  day The total u nused idle time  is  28% \(2,6 72 hours\ of the  time  that  the PCs are powered on  9,550 hours    The remainder of this paper is organized as follows. Section II describes the compilation cost due to the evolution of software. Section III addresses the design and implementation 
of the proposed techniques   Section IV shows the experimental results   and  r elated work is described in Section V   Finally Section VI concludes the paper  II  B UILD C OST A NALYSIS OF M OBILE P LATFORM  A  Software Evolution  The size of recent software is growing due to  its  increased complexity and enhanced functionality Figure 2 shows the  source size growth  of popular software from 2009 to  2013 Especially   the source size of 
the Android 8  mobile platform has increased a 7 fold in only  4 years    Figure 2  Current state of source size growth of software from January 2009 to  July 2013  B  Compilation Cost s  The tremendous growth of software has cause d  a  sudden increase  of source code  lines which induces  high software  compilation costs. Figure 3 illustrates  the time cost needed to build a large mobile platform such as Android 4.2.2 Compilation cost 
s  account  for 67   34 minutes\of the total cost of execution \(51 minutes   Figure 3  Use time per work for building mobile platform  This research was supported by the Basic Science Research Program through the National Research Foundation of Korea \(NRF   funded by the Ministry of Education, Science and Technology \(2010 0022570  116 


It is therefore important that we study distributed compilation technology to accelerate  the compilation of source codes In this study, we have used previous studies  1  4  6    that describe how to effectively utilize various hardware devices in a distributed network  III  D ESIGN AND I MPLEMENTATION OF D IST C OM  The Distributed Compilation System \(DistCom\ is a technique designed for implementing a distributed compilation platform in order to accelerate  the comp ilation of large software by using the resources of idle PCs  A  System A rchitecture of DistCom  Figure 4 shows a  diagram of the entire DistCom system The proposed DistCom system consists of three components   Server and client model   to control distributed PC resources connected via the network   DistCom manager   for scheduling distributed PC resources   Cross c ompiler infrastructure   to support heterogeneous architecture when compiling source codes   Figure 4  System a rchitecture of the DistCom  s ystem  B  Distributed Server and Client Model  Figure  5 shows the operation diagram of the server and client model   10  for executing a source code as a distributed compilation. When users execute the compilation command via the 1.2\ DistCom  Client the 2\ DistCom Manager  acquires the information of the distributed PC resources such as the workload  status and the user\222s access status. The  2\ DistCom Manager  then distributes the compilation commands to the distributed PCs to r un the distributed compilation of the software code  T he  2\ DistCom Manager  uses a checkpoint/restart  mechanism 1  to minimize speed degradation, where the object files are the atomic level for carrying out check pointin s   T he checkpoint/restart  is the ability to save the state of a compiling source code so that compilation  can later resume on the same or a different distributed PC from the moment at which it s checkpoint was carried out   The approach is practical and useful because  t he proposed system  supports the retry  mechanism that execute s  the r e compilation based on the object file unit s   when ever  compilation failure of a distributed PC  occurs  during the distributed compilation  The 2\ DistCom Manager  distributes workload  to the 1.1\ DistCom Service Daemons   depending on the workload  status of the PCs in the distributed network. Whenever the 1.1\ DistCom Service Daemon  of the distributed PC finishes  the compilation tasks in its queu e, the 2\ DistCom Manager  distributes the  new tasks to the 1.1\ DistCom Service Daemon    Figure 5  Server and c lient f lowchart of DistCom  for the distributed network  C  CPU Scheduling of Distributed PC Resources  Our proposed system finds a suitable idle PC resource through the HTCondor Pool Manager  12  13 When DistCom uses the distributed PC resources, the 2\ DistCom Manager  uses the two following methods to control the CPU resources in order to optimize the compilation  performance  of running tasks o n the distributed computer    Dedicated resource scheduling  operates when the  CPU resource s are always  idle  It is  a useful and practical method  in the case  where a user does not  use all the CPU power  of a distributed PC resource   The existing Distcc    allocates one workload  per PC sequentially   regardless of the number of CPUs    Shared resource scheduling   shares CPU usage of a distributed PC when  its  CPU is runnable This method is useful in the case where there are a few  idle PCs and the C PU usage of the PCs is  not high   The performance  of o ur dedicated resource scheduling  is similar  to that described in an  existing study    However our proposed technique considerably  improves the performance by utilizing the checkpoint/restart  mechanism and by maximizing the CPU usage in a multi core system W hen S ource C ompilation  of  Figure 5 is executed, the minimum number of object files is equal to the number of available  CPUs    Figure 6  User aware CPU resource scheduling  for improv ing  the performance of the compila tion  on idle PCs  Figure 6 shows the CPU sharing method when the 1.1 DistCom Service Daemon  runs with the shared resource scheduling  method. To avoid degrading  the processing speed 117 


during the user\222s work  period, the 1.1  DistCom Service Daemon  runs the compilation  as a task of real time priority  to monopolize the  CPU resource in  the case where  the user do es  not use the PC resource  O n the other hand  the task of using the available CPU usage in t his  run compilation is of lowest  priority   in the case where the  user accesses  the PC resource Therefore  the distributed computer system  does not need  any  modification  because 1.1\ DistCom Service Daemon  dynamically controls  its own  scheduling priority in the distributed computers   The s cheduling p riority  of 1.1 DistCom Service Daemon  is dependent on  the user 222 s input device access  such as keyboard, mouse, and touch screen  The 1.1\ DistCom Service Daemon  decides whether or not it must allocate the next ta sk continually after finish ing  the allocated compilation task s At this time the  c riteria of task allocation include  the user\222s input device access, the workload  of the remote PC resource, and the scheduling cost of the task  Figure 7 shows the three state transition diagram of the task to accelerate  the processing speed by effectively controlling the distributed compilation First R eject  is used to deny the allocation of the task. Second S top  is used to b reak the allocation of the task to the PC resource because of the user\222s access. Finally F inish  is used to complete the running tasks normally 2 DistCom Manager  manages all jobs with two task queues to separate either the dedicated resources or shared resources   Figure 7  Three  state transition flowchart of the task man aged by  DistCom m anager  for the high performance distributed compilation  D  Cross Compiler Infrastructure to Support Distributed  CPU Architectures  The b inary files from a  compilation must be independent of the  CPU architecture of the remote PC resource. In other words the compiled binary files   must  be executed at specified target devices. Figure 8  shows the cross compiler infrastructure   for  generat ing  exe cutable binary file s  for a system  other than that  on which the compiler is running  The existing system  does not handle the different operating systems  between the distributed build environment and target environment  10  17    Therefore  t he Heterogeneous CPU Mapper  of the proposed cross compiler infrastructure connects a source code  to the target machine code after probing the operating system structure of the distributed PCs    Figure 8  Cross c ompiler d iagram  for handling  the distributed compilation  environment  and the target environment  IV  E XPERIMENTAL R ESULT  We used distributed idle PC resources to evaluate the effectiveness of  DistCom in  a real environment  W hen we executed a distributed compilation, we set the  maximum available computers  to 9 machines  CPU: Intel Core2Duo  MEM: DDR2 1G  Network  Interface Card In tel 100  Mbps Ethernet Controller   and  the  dedicated resource scheduling  policy as the  default CPU scheduling policy. Figure 9 shows  the  time cost s  of the distributed compilation. From our experiment the time cost to build this mobile platform source is reduced by 65   33 minutes   Figure 9  Compilation time by a single computer  top time line\ and of a distributed dedicated system with 9 computer s \(the nine lines bel ow\. The compilation time is reduced by 33 minutes from 51 to 18 minutes   Figure 10 shows a  breakdown of the compilation cost. We verified that  25% is consumed by the Network Speed 30% by the Computing Power of PCs and 45% by the CPU Scheduling Method    Figure 10  B reakdown of distributed compilation cost  of mobile platform source code  In addition, we set up the experimental environment  in different ways  with 5, 10, 15, 20, 25 and 30 machines CPU Intel Core2Duo  MEM: DDR2 1G  Network  Interface Card  In tel 100  Mbps Ethernet Controller   to evaluate the performance improvement difference depending on the number of available PC resources  The Intel  Ethernet Controller support s  a theoretical maximum bandwidth of 100 Mbps  Figure 11 shows h ow PC resources connected by a network for building a mobile platform source affect the processing speed of the distributed compilation From our experiment  the  dedicated resource scheduling  method was faster than the shared resource scheduling  method by 40% on average From our analysis, we found that compilation processing performance of the shared resource scheduling  method largely depends on the CPU usage of the PC resource compared  with the dedicated resource scheduling  method Moreover, in the 118 


case where the available PC resources is  more than 10 distributed PCs the compilation speed of the dedicated resource scheduling  method was improved compared to  a high performance computing server \(8 C ore Intel Xeon E5 P rocessor 12 GB memory    Theoretically the performance of 8 machines would be similar to the 8 C ore PC. From our analysis we found that the performance loss of 2 PCs  results from  the  network speed and low  computing power  of the distributed PCs    Figure 11  Comparison of the compilation processing performance between the distributed PCs and the high performance computer server  Figure 11 also shows a comparison of the compilation processing performance  between Distcc 14  and DistCom \(our system The existing Distcc [14 al lo ca tes  o n e  workload  per PC without allocating the one workload  per CPU  and does not support shared resource scheduling  such as DistCom However   our proposed system executes a multi core aware distributed compilation that allocates one workload  per CPU after calculating the number of CPUs   From our experiments, we determined  that the multi core aware dedicated resource scheduling  technique was  more effective than the existing Distcc   Figure 12  Performance comparison  between  a  cloud server  \(Guest  OSes on KVM virtual machine\ and one high performance server \(native operating system  Figure 12 shows the experimental result when we executed our proposed system in  a cloud computing environment. The cloud computing environment consisted of up to 40 Windows XP VMs \(Virtual Machines\ as Guest OSes after installing the KVM virtual machine   on a high performance cloud server 40 C ore Intel Xeon E7 P rocessor, 32 GB mem ory   The results show that the dedicated  resource scheduling  method with 40 VMs consumed 20 minutes and the high performance cloud server on its  native operating system consumed 17 minutes The 3 minutes difference in performance is due to the emulation operation [20 o f t h e KVM v i r tu al m ac h in e  T h is  demonstrates that in the case of many idle VMs, our proposed system is as effective as one high performance computer   Figure 13 shows the result s  when our proposed  system is executed with Ccache  2 1    to store  redundant  files to random access memory at compilation time Ccache  is a program that caches the output of the compilation in order to accelerate  the second time compilation which  can significantly  accelerate  the overall recompiling time   When  we ran the compilation of a mobile platform source together with Ccache  we reduced the compilation time by about  10 F urther  analysis showed  that the effect of the Ccache  23  is correlated with  the memory shortage of the d istributed PC resources and with the physical me mory capacity for caching at the compilation  time     Figure 13  Performance comparison between with and without Ccache  V  R ELATED W ORK  In this section, we discuss the existing  distributed compilation schemes, as well as the existing high performance computing, which served as a prelude to the proposed DistCom system  in this study   A  Existing Works on High Performance Computing  Ccache  21  improves the acceleration  of the second time compilation by storing the output of the C/C++ compilation This technique only focuses on the improvement of the recompiling time with the available memory. In other words Ccache  only handles random access memory without consideration  of the available CPU resource s  Distcc  14  accelerate s  compilation by distributing compilation tasks across a network of  participating hosts. This technique does not handle a shared resource scheduling  method such as  our proposed system and  is not aware of multi core CPU environments   B  Existing Works on High Throughput Computing  Berkeley Open Infrastructure for Network Computing  BOINC  24  supports a high performance distributed computing platform for the massive  processing power of personal computers around the world BOINC  uses the unused Graphics Processing Unit   GPU cycles as well as the unused CPU cycles on a computer for scientific computations 119 


However BOINC  does not handle distributed compiling tasks HTCondor  25    supports a high throughput computing framework for coarse grained distributed parallelization of computationally intensive tasks. However HTCondor  does not handle distributed compilation tasks and is not aware of multi core CPU environments   O ur proposed system handles high performance computing as well as high throughput computing such as a hybrid computing concept. Moreover, our techniques support multi core aware scheduling without any modifications to the distributed system in order to mana ge shared resources as well as dedicated resources   VI  C ONCLUSION AND F UTURE W ORK  Idle computer resources connected by a network are more ubiquitous than ever before and it is therefore important to determine a technique of distributed systems  that   utilizes unused computer devices. In this paper, we proposed a distributed compilation system, which consists of a distributed server and a client model, a resource manager for scheduling distributed computers, and a cross compiler infrastructure to support heterogeneous architectures. We verified that our proposed DistCom system can significantly improve compilation speeds using existing idle PC resources by proposing a distributed compiler system of compatible heterogeneous CPU architectures. M oreover, the proposed DistCom system minimizes the performance degradation of the distributed compilation by executing resource scheduling of remote compu ters based on object file units   In the future, we plan to study a network aware task scheduling techn ique considering the physical network speed   to distribute tasks and a task migration algorithm 11  to migrate distributed task s  to another idle PC resource   A CKNOWLEDGMENT  We gratefully acknowledge  Seong Tae Jhang  and Hyosuk  Kim for their feedback and comments, which helped improve the content  and presentation of this paper   R EFERENCES    D. Kondo , A. A. Chien , and H. Casanova, \223Resource management for rapid application turnaround on enterprise desktop grids,\224 in Proc ACM/IE EE Conf. on Supercomputing, 2004    T. Kosar and M. Livny, \223Stork: Making Data Placement a First Class Citizen in the Grid,\224 in Proc.  24th IEEE Int ernational  Conference on Distributed Computing Systems \(ICDCS\,  Mar. 2004    I. Sfiligoi, G. Quinn, C. Green, G Thain, \223Pilot job accounting and auditing in open science Grid,\224 in Proc. 9th IEEE/ACM International Conference on Grid Computing, Japan, 2008    M. Silberstein, D. Geiger, A. Schuster, and M. Livny, \223Scheduling mixed workloads in multi Grids: The Grid exe cution hierarchy,\224 in Proc 15th IEEE Symposium on High Performance Distributed Computing 2006    G. Bosilca, A. Bouteiller, A. Danalis, T. Herault, P. Lemarinier, and J Dongarra, \223DAGuE: A generic distributed DAG engine for high performance computing,\224 in  Proc. Parallel Computing 38, no. 1, 2012    G. Bell and J. Gray 223 What's next in high performance computing 224  Communications  of the ACM, vol. 45, Feb. 2002      D. Kondo, B. Javadi, P. Malecot, F. Cappello, and D. P. Anderson 223Cost benefit analysis of cloud computing versus desktop grids,\224 in Proc IEEE International Symposium on Parallel & Distributed Processing May 2009    E. Burnette,  \223Hello, Android: introducing Google's mobile development platform,\224 Pragmatic Bookshelf, 2009, ISBN:1934356492 97819343564 94    Douglas Thain and Miron Livny, \223Building reliable clients and servers,\224 The Grid: Blueprint for a New Computing Infrastructure, Morgan Kaufmann, 2003, 2nd edition. ISBN: 1 55860 933 4    G. Lim, T. H. Kim, R.J.W.E. Lahaye, and Y. I. Eom, \223HTCondor based distributed compiler network,\224 in Proc. the KIISE Fall Conference, Jeju Korea, Nov. 2013    M. Litzkow, T. Tannenbaum, J. Basney, and M. Livny, \223Checkpoint and migration of UNIX processes in the Condor distributed processing system,\224 University of Wisconsin Madison Computer Sciences Technical Report #1346, Apr. 1997    D. Thain, T. Tannenbaum, and M. Livny, \223Distributed Computing in practice: The Condor experience,\224 Concurrency and Computation Practice and Experience, vol. 17, pp. 323 356, Feb. 2005    R. Sobie A. Agarwal, I. Gable, C. Leavett Brown, M. Paterson, R Taylor, A. Charbonneau, R. Impey, and W. Podiama, \223HTC scientific computing in a distributed cloud environment,\224 in Proc. 4th ACM workshop on Scientific cloud computing, pp. 45 52, 2013    Google, \223Di stcc: Distributed compilation for faster C/C++,\224 https://distcc.googlecode.com, Nov. 2008    P. Bui, \223A compiler toolchain for distributed data intensive scientific workflows,\224 University of Notre Dame, 2012    O. Beaumont, A. Legrand, and Y. Robert, \223Scheduli ng divisible workloads on heterogeneous platform,\224 in Proc. Parallel Computing 2003    Kyongjin, J. O., Seon W. K., and Jong Kook K. I. M., \223DiSCo Distributed scalable compilation tool for heavy compilation workload,\224 IEICE Transactions on Information and Systems 96, no. 3, 2013    J. Gattermayer and P. Tvrdik, \223Different Approaches to Distributed Compilation,\224 in Proc. 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum \(IPDPSW\, May 2012    A. Kivity, Y. Kamay, D. Laor, U. L ublin, and A. Liguori. \223 KVM the Linux virtual machine monit or,\224 i n Proc. the Linux Symposium, vol. 1 pp. 225 230. 2007    J. Jang, E. Seo, H. Jo, and J. Kim. \223A low overhead networking mechanism for virtualized high performance computing systems,\224 The Jour nal of Supercomputing 59, 2012    J. Rosdahl, \223Ccache: A fast C/C++ compiler cache,\224 http://ccache.samba.org, Feb. 2010    K. Kim, S. K. Cha, and K. Kwon, \223Optimizing multidimensional index trees for main memory access,\224 in Proc. ACM SIGMOD international confe rence on Management of data ,  2001    M. Abdalkader, I. Burnette, T. Douglas, and D. G. Wonnacott 223Distributed Shared Memory and Compiler Induced Scalable Locality for Scalable Cluster Performance,\224 in Proc. 12th IEEE/ACM International Symposium on Cluster Cloud and Grid Computing, IEEE Computer Society, Washington, 2012    D. P. Anderson et al., \223BOINC: a system for public resource computing and storage,\224 in Proc. Grid Computing, 2004    C. B. Ries and V. Grout, \223Code generation approaches for an automatic transformation of the unified modeling language to the Berkeley open infrastructure for network computing framework,\224 in Proc. International Conference on Soft Computing and Software Engineering \(SCSE\, 2013    S. K. Singh, A. Madaan, A. Aggarwal, and A. Dew an, \223Design and implementation of a high performance computing system using distributed compilation,\224 in Proc. International Conference on Advances in Computing, Communications and Informatics \(ICACCI\,  2013    J. Basney and M. Livny, \223Managing network reso urces in Condor,\224 in Proc. Ninth IEEE Symposium on High Performanc e Distributed Computing \(HPDC  Aug. 2000    120 


Age Amount Gender Amount 15-25 15 Male 11 25-45 6 Female 16 45 6 Table 4 Demographic data Google Health data The usefulness of the Internet-based service emerged as the second significant issue within among the Google among participants. Despite some positive reactions, most of the interviewees failed to see direct value for themselves as result of using Google Health Most noted not holding their own health information thus mitigating the need to in the future, consistent with low compatibility with current practices Moreover participants noted their own relative good health as a reason to not use such a PHR system With respect to barriers to using Google Health, one participant noted 
ho can see all of this information, who has the right in certain circumstances to be allowed access, who does not  In 2012, some 27 interviews were conducted with potential users of Google Health Table 4 reports demographics for this sub-sample The information needs describ e which information the user likes to receive from the system and should align with the information the service delivers and captures Our interview explicitly covers the source and completeness of the information, while implicitly reviewing other information quality measures such as accuracy and timeliness. Our literature study demonstrated several connections between information quality and factors directly influencing the adoption of a service, leading to the following hypothesis More than half of the interviewees offered negative feedback specific to the hypothesis as \(1\e system was only available in English, \(2\e systems used too many medical terms, \(3\e quality of data input by patients, and the \(4\ that current information along was enough For each success factor in Table 1 we again identified specific results, which we summarize in Table 5 Privacy concerns again emerged as the greatest threshold for users with 23 out of 27 noting the issue as a significant concern. Out of all of the interviews emerges the view that users consider health information as very personal with a commercial company like Google untrustworthy when it comes to such information have any information to put onto Google Health, I really would want privacy guarantees before putting my information into the system to prevent my This sentiment illustrates the general opinion emerging from the collective interview data We highlight the main problems, or objection points, in Table 5. Users do not see the relevance of Google Health, expressing a predominantly negative attitude toward the product 
4.2. Google Health 
Success factor Microsoft Health Vault 
         
questioned, çw  in addition to the fact that I donêt information getting public on the internet 
Service quality  Information quality  System quality  Trust  Perceived usability  Perceived risks HIGH Perceived usefulness  Social and personal influence  Perceived compatibility  Table 3 Overview results Mic rosoft HealthVault  data One an essential aspect, namely social and personal influence, merged as important to the future success of the PHR Our assessment of all of the interviews concluded that a bad result can also be seen as positive We posit a relationship between the implementation of the application and outside influences. A positive external signal may see more people develop a more favorable impression of the system and thus potentially develop a more positive attitude about the product. In addition, negative signals to the greater marketplace often yield negative impacts 
The better the fit b etween information needs and information provided by the e-health service, the higher the user adop tion of the service 
2823 


as a çnew ol or later periods in oneês life 
 Google Health 
5. Analysis 6. Conclusions 
Table 5: Interview results Google Health The relevance of new information systems is seen as one of the most important success factors. Our results see perceived usefuln  an indicator of relevance negative in both the Google Health and Microsoft HealthVault cases. Despite po sitive perceptions of the information quality of Mic rosoft HealthVault, low relevance makes it potentially difficult for the product to ultimately succeed In both cases, Google Health and Microsoft HealthVault, interviewees viewed both system quality and service quality positively Consistent with Delone and McLean tog eth er w i t h t h e positi v e  information quality, favorable views of system and service bode well for the ultimate success of Microsoft HealthVault. That said, we contend that our study, not grounded in a tethered system context, shows less likely chances for success. Based upon the factors identified within our literature study, we question whether the IS success model should consider trust as fourth causal determinant The trust and perceived risk indicators are closely related in both the Google and Microsoft cases with trust negative and risk high. Turban [50  dem onstrat ed  that Internet-based information systems depend on trust to ultimately achieve success. Accordingly, as a driver of adoption, high risk and low trust serve as significant hindrances for both Google Health and HealthVault to achieve widespread adoption As with many studies, social and personal influence as an influence factor within our study  Ou r qu alitati v e  analysis found social and pe rsonal influence negative for both Google Health and Microsoft HealthVault Neither system is well known or widely used in The Netherlands, where the current study was conducted Therefore, few peers or healthcare professionals likely promote either system. Moreover, conducting this work outside of a tethered PHR system context; i.e not recruiting participants through a healthcare system hospital, or other healthcare related environment; likely impacts the healthcare pr ovider social and personal influence commonly experienced by participants. Here again, our context mirrors the general population of potential standalone PHR users As we already know, Google Health did not succeed Our analyses based upon the literature study and interview data supports the contention that Microsoft HealthVault will similarly fail to reach a critical mass of users, at least as a standalone product offering in a market such as The Netherlands The primary reason we see for the lack of success lies in the relevance of both PHR applications. Relevance perceived usefulness, performance expectancy, relative advantage, and net benefits all focus on the value of a new information system. These factors have failed to draw the attention of vendor s developing standalone personal health records applications Low trust and high risk emerge as two additional significant reasons for the failure of personal health records, given the analysis of the Google and Microsoft products. Together these factors might be considered success factor and validated for standalone as well as tethered applications Further, healthy people are una ware of personal health records, and therefore, social and personal influences will likely play a negative role in the success of Microsoft HealthVault and serve as one potential reason for the failure of Google Health. While healthy individuals may not see a need for or interest in PHRs demonstrating benefits of long-term and accurate PHRs might frame such applications as a beneficial planning to In addition to continuing to examine the PHR phenomenon outside strict t ethered system contexts future studies of personal health records should focus on designing applications that address factors specifically relevant to patien t and caregiver end users within untethered contexts. Trust and risk should further be studied and validated as a significant determinant of success under the greater umbrella of reliability 
Service quality  Information quality  System quality  Trust  Perceived usability  Perceived risks High Perceived usefulness  Social and personal influence  Perceived compatibility  
2824 


7. References 
This study aims for a qualitative reasoning why PHR has not been successful so far. The amount of interviews \(51\ valid to support this reasoning but not enough to generalize to larger populations or dissimilar contexts Personal Health Records Empowering Consumer 76-86  2  Potential of electronic personal health records 330-333  Su ny aev  D C Evaluation Framework for Personal Health Records: Microsoft HealthVault vs pp. 1-10  le. \(2 011 Ju n e 24  Retrieved from GoogleOfficialBlog:http://googleblog.blogspot.nl/2011 06/update-on-google-health-and-google.html  19 89 P e rceiv e d u s efu l n e ss  perceived ease of use and user acceptance of information technology 319 340  Venk ate sh V., Mo rris M. G., Davi s G. B   Davis, F. D. \(2003\cceptance of Information Technology: Toward a Unified View 425-478  illiam s, M. D., Ra n a  N. P., Dwivedi, Y. K Lal, B. \(2011\ UTAUT Really Used or Just Cited for the Sake of It? A Systematic Review of Citations of riginating Article. In  paper 231  iv ed i, Y., Ra n a N., C h en, H  W illia m s, M  2011\ Meta-analysis of the Unified Theory of Acceptance and Use of Technology \(UTAUT 155-170  Ch erv a n y D. H. \(2 001 2002\ What Trust Means in E-Commerce Customer Relationships: An Interdisciplinary Conceptual Typology 35 59  re, C. L Krach e r, B  W ied enbeck S  2003\line trust: concepts, evolving themes, a model 737 758  e e, G G    L i n   H.F. \(2005 C u s t o m er  perceptions of e-service quality in online shopping 161-176  L e e, M C 2009  Factors inf l u e n c i n g the  adoption of internet banking: An integration of TAM and TPB with perceived risk and perceived benefit  130 141  Han L   Jin  Y  200 9  R e v i e w of Technology Acceptance Model in the E-commerce Environment pp 28 31\, China: IEEE  en, D., Karahan n a E Stra u b D. W 20 03  Trust and TAM in Online Shopping: An Integrated Model 51-90  Ch en L  d Gillenson  M L., & Sherrell, D. L 2002\ticing online customers: an extended technology acceptance perspective 705-719  DeL on e, W  2003 T h e DeL on e an d Mc L e a n  model of information systems success: a ten-year update 9 30   Measuring the impact of patient portal California Healthcare Foundation  T a n g P C A s h J S., B ates, D.W Ov erh a g e  M Sands, D.Z. \(2006\. Personal health records definitions, benefits, and strategies for overcoming barriers to adoption. Jour nal of the American medical informatics association 13\(2\pp. 121-126  W itry M.J Do u c ette, W  R., Dal y  J  M., L e v y  B.T., & Chrischilles, E.A. \(2010\ily physician perceptions of personal health records. Perspectives in health information management 7, 1d. PMCID PMC2805556  T e nf orde, M J ain, A Hick n e r, J  200 8 T h e value of personal health re cords for chronic disease management: what do we know? Family medicine 43\(5\, pp. 351-354  Do, N. V B a rn h ill, R., H e er m a n n D o K. A  Salzman, K. L., & Gimbel, R. W. 2011. The Military Health System's Personal Health Record Pilot with Microsoft HealthVault and Google Health 
1  Ball, E. F., Carla Smith, N. F., & and Richard S Ba 
Limitations Journal of Healthcare Information Management Vol. 21, No. 1 BMJ Volume 335 43rd Hawaii International Conference on System Sciences An update on Google Health and Google PowerMeter MIS Quarterly 13\(3 MIS Quarterly Vol. 27, No. 3 ECIS Proceedings Governance and Sustainability in Information Systems Managing the Transfer and Diffusion of IT International Journal of Electronic Commerc e / Winter , Vol. 6, No 2 Int. J. Human-Computer Studies 58 International Journal of Retail & Distribution Management, Vol. 33, No. 2 Electronic Commerc e Research and Applications 8 International Conference on Management of e-Commerce and e-Government MIS Quarterly, Vol. 27, No. 1 Information Management 39 Journal of Management Information Systems 19 Journal of 
 
kalar, M. \(2007 s Pagliari, D. D. \(2007  2010 Google Health  UTAUTês O      Emont, S. \(2011 s: What the literature tells us 
2825 


      2005 MIS Quarterly 
18\(2 118-124  Collin s, S. A Va w d re y   D. K., Kudaf k a  R  Kuperman, G. J. 2011. Policies for Patient Access to Clinical Data Via PHRs: Current State and Recommendations 18\(Suppl 1\-i7  Sch n i pper, J  L Gandh i T  K W ald  J  S Grant R. W., Poon, E. G., Volk, L. A., Businger, A Williams, D. H., Siteman, E., Buckel, L., & Middleton B. 2012. Effects of an Online Personal Health Record on Medication Accuracy and Safety: A Clusterrandomized Trial 19\(5\: 728-734  W a gn er, P  J., Dias, J   Ho w a rd, S., Kin tzi g e r, K W., Hudson, M. F., Seol, Y.-H., & Sodomka, P. 2012 Personal Health Records and Hypertension Control: A Randomized Trial 19\(4\: 626-634  L i u L S., S h i h P C  Ha y es, G.R 2011 Barriers to the adoption and use of personal health record systems. Proceedings of the 2011 iConference pp. 363-370. DOI: 10.1145/1940761.1940811  Spil T A Mich e l Ve rk erk e M. B 201 2 Dutch  Eburon, Delft  T A Schu r ing  R W  Mich e l Ver k e rk e  M. B. \(2004\. Electronic prescription system: do the professionals USE IT 32-55  R o g e rs E. M. \(198 3 New York: Free Press  Easti n M. S 2002 f u s i on of e co mm erce a n  analysis of the adoption of four e-commerce activities 251 267  an n a E.; Ag ar w al R   An gs t, C 2006  Reconceptualizing Compatibility Beliefs," MIS Quarterly, \(30: 4  l G a h ta n i, S. S. \(2011 Mo deli n g t h e electron ic  transactions acceptance using an extended technology acceptance model 47 77  C respo, A  H. \(2008  plain i n g B2C e commerce acceptance: An integrative model based on the framework by Gatignon and Robertson 212-224  B h attach e rj ee, A 20 00 c cep tance of E Commerce Services: The Case of Electronic Brokerages 411-420  C h en  L  D   T a n  J. \(200 4 T e ch n o log y  Adaptation in E-commerce: Key Determinants of Virtual Stores Acceptance 74-86  Sh ih H P 2004  e m pirical s t u d y on predicting user acceptance of e-shopping on the Web 351-368  Ven k ates h V. A 2000 A th eoretical ex te n s io n of the Technoly Acceptance Model: Four longitudal field studies  186-204  Wan g Y.-S. \(2008  A s s e s s i ng e co m m e rce  systems success: a respecification and vali on of the DeLone and McLean model of IS success 529 557  Pav l ou P. A  20 03  C o n s um er A c ceptan ce o f Electronic Commerce: Integrating Trust and Risk with the Technology Acceptance Model 69 103  Green   D. T    P e ars on J  M. \(2011 teg rating website usability with the electronic commerce acceptance model 181 199  h ou T  Z h ang  S 200 9 Ex a m in in g th e  Effect of E-commerce Website Quality on User Satisfaction pp. 418-421 Nanchang: IEEE  Eg g e r, F. N 200 1 Aff e cti v e Desi g n o f E commerce User Interfaces: How to Maximise Perceived Trustworthiness London: Asean Academic Press  Reichh eld F. F  Sa sser, W J  199 0 Z e ro  Defections: Quality Comes to Services 105-111  Z e ut h a m l V B e rr y L   P a rasu ra m a n A   1996\The behavior consequences of service quality Journal of Marketing 60, 31-46  Kim  C., Gallier s, R. D Sh i n N R y oo, J  H   Kim, J. \(2012\ influencing Internet shopping value and customer repurchase intention 374 387  L iette L a p o in te  S u zan n e  Ri v a rd A Multilevel Model of Resi stance to Information Technology Implementation 461-49 
the American Medical Informatics Association Journal of the American Medical Informatics Association Journal of the American Medical Informatics Association Journal of the American Medical Informatics Association De waarde van informatie in de gezondheidswereld Int. J. Healthcare Technology Management, Vol. 6, No. 1 Diffusion of Innovations Telematics and Informatics 197 Applied Computing and Informatics 9 Interacting with Computer 20 IEEE Transactions on Systemens, Man And Cybernetics - Part A Systems and Humans, Vol 30, No. 4 European Management Journal Vol.22, No. 1 Information % Management 41 Management Science, Vol. 46, No. 2 Info Systems 18 International Journal of Electronic Co mmerce, Vol. 7, No. 3 Beh aviour & Information Technology Vol. 30, No. 2 Second International Symposium on Electronic Commerce and Security International Conference on Affective Human Factors Design Harvard Business Review Electronic Commerce Research and Applications 11 
2826 


Information & Management 48 Case study research: Design and methods \(2nd ed Journal of Business Research 63 Behavior and Information Technology Vol. 20, No. 1 International Journal of Electronic Commerce Vol. 6 Qualitative data analysis: An expanded sourcebook Social Science Research:Principles, methods and practice 
Sia, C. L. \(2011\onsumerês decision    
PRIMA construct Success factors expected to be measured Examples of questions asked Process Perceived compatibility Which health records you regularly use?  Are you using a fixed sequence of actions Which alternatives you have to find information Relevance Perceived usefulness Perceived usability Which functions of a PHR are most important for you? Which parts of the system you experience as a bottleneck? Do you have suggestions for improvements Informati on needs Information quality Which information you need to get from the service? Do you get sufficient information from the system? Is the information quality sufficient Means and people Service quality System quality Perceived risks Do you get sufficient support? Is the system reliable Does the system offer enough privacy Attitude Trust Social and personal influence Do you think IT is necessary to improve health in formation? Do you feel social pressure of using the service? How much time do you want to spend for learning to use the service Appendix 1: Interview framework 
 L e e, M. K., Sh i, N., Ch eung, C. M., Lim, K. H to shop online The moderating role of positive informational social influence 185 191  R  K. \(199 4 Thousand Oaks, CA: Sage  Her n  ndez, B  J i m  n e z, J  Ma rtÌn M. J  2010\Customer behavior in electronic commerce The moderating effect of e-purchasing experience 964 971  Freu d en th al D. \(2001  A g e diff ere n ces  in th e  performance of 9-22   T u rban, M. K 20 1 1  A T r u s t Mo del f o r  Consumer Internet Shopping 75 91  Miles, M.B  and Huber m an A  M Sage, 1994  attach e rj ee, A 2012  Creative Commons e-book 
2827 


Figure 4  Payload Allocation Figure 5  Procurement vs Hosted Payloads Hosted payloads vs 100 procurement This case study focuses on the impact of including hosted payloads as part of the system Figure 5 presents an extended representation of the tradespace where hosted payload architectures are included The 002rst immediate conclusion that can be drawn is that based on the cost model from hosted payloads offer the possibility of signi\002cant cost reductions 050between 15 and 30%\051 As a result all architectures on the Pareto front rely on hosted payload architectures On the other hand concerns about using hosted payload assets for critical applications like astronaut support have been 003agged as a major caveat In this case an optimal solution would entail using a portfolio of hosted payloads and privately owned satellites and schedule the communications through ones or the others according to operational constraints Therefore building this portfolio can be done by analyzing what communication payloads are better suited for hosted payload con\002gurations Figure 6 presents the tradespace of architectures that mix hosted payload and procured assets Results indicate that including hosted payload assets in the system does not have a major impact on its performance but requires disaggregating the monolithic satellites identi\002ed as optimal in the previous case study That being the case the next questions arise Figure 6  Portfolios of hosted payload and procured assets Which payload obtains better savings when being hosted How many payloads do you want to 003y in a hosted payload approach The answer for the 002rst question can be obtained by analyzing the non-dominated architectures from 002gure 6 and estimating the cost savings when hosting the different types of antennas that are considered in this analysis Results indicate that hosting an optical payload can potentially save an 28 of the cost while a low data rate antenna 050S-band\051 only obtains a 16 cost reduction 050note that these numbers are based on the system life-cycle cost and are heavily in\003uenced by the hosted payload cost model\051 Therefore it seems to be more advisable to put high data rate payloads like optical terminals in commercial satellites and retain control of S-band communications 050which in turn ensures that contingency event will be addressed by privately owned assets not constrained by the operational limitations of host platforms\051 Finally 002gure 7 presents the tradespace of hosted payload architectures color coded by the number of antennas that are being hosted By looking at the Pareto front it can be seen that almost all architectures host one single antenna as opposed to two in order to minimize the burden on the host platform This burden is also contingent on the mass and power of the hosted communication payload This is the main reason why optical terminals become so attractive for hosted payload architectures as opposed to low frequency systems that require big parabolic antennas and power ampli\002ers to close the link budgets 6 C ONCLUSION Summary This paper has presented the 002rst set of results obtained through the developed architectural tool to evaluate space communication networks It has 002rst provided a high level overview of the tool with a description of four main constitutive elements An STK model an architecture generator an architecture evaluator and a resource manager Next it has described the newly developed scheduling algorithm based on a rule-based expert system that ef\002ciently allocates the network resources to satisfy near-Earth mission communication requirements Then a discussion on the conducted validation exercise has 11 


Figure 7  Portfolios of hosted payload and procured assets been presented in order to gain some level of con\002dence on the results that the tool outputs This discussion has focused on the validation of the scheduling algorithm by benchmarking it with operational schedules from the TDRSS Finally this paper has presented two case studies for future implementations of the TDRSS system Based on a demand forecast for the network a 002rst case study has considered the problem of selecting the frequency band to be supported and how to allocate them into the relay satellites It has been shown that maximum performance architectures require a mix of optical and RF payloads that support high throughput communications as well as reliable low data rate communications If the traditional procurement strategy is assumed 050NASA buys and operates the relay satellites\051 then monolithic architectures are preferable unless more than two high gain antennas render the resulting satellite con\002guration too complex In turn the second case study has extended this analysis by introducing architectures with hosted payloads It has been shown that according to the current available pricing model hosted payload architectures are clearly preferable than the traditional procurement strategy with cost savings between 15 to 30 for the same level of system performance It has then been discussed the advantages of having mixed procured and hosted payload architectures as a compromise to obtain networks with reduced lifecycle costs that can still address the requirements of highly sensitive and reliable applications Results have demonstrated that high data rate payloads 050speci\002cally optical payloads\051 are the best candidates to be hosted 050with savings up to 28%\051 thanks to their reduced mass and power requirement On the other hand low data rate communication payload should be allocated in privately owned satellites as they require bigger antennas and power ampli\002ers that increase the burden on the host platform Future Work The main streams of future work are as twofold On one hand additional features should be added to the model in order to better capture the complexity of the network con\002gurations 050e.g coupling between the costs of the communication payloads depending on the level of on-orbit processing the they perform\051 Additionally the size of the tradespace is currently limited to less than two thousand architectures due to computational limitations a stringent limitation given the possible combinations from the identi\002ed architectural decisions The solution currently under development is to include a genetic algorithm that alleviates this problem by iteratively generates new populations of architectures by combining the best previously evaluated networks On the other hand the other main stream of future work is related to exercising the tool in a variety of architectural decisions and mission scenarios In the presented case studies only geosynchronous constellations were considered although the tool allows comparing them with systems that place relays in MEO and LEO orbits Additionally the tool can also provide insight in valuing inter-satellite links and how the cost of putting in orbit their extra communication payloads can be leveraged by reducing the number of operating ground stations Moreover couplings between the payload allocation and fractionation strategy should also be further explored so as to understand if monolithic architectures still become preferable if relay satellites can be decomposed in clusters of independent antennas and payloads Finally supplementary what-if analysis can be conducted based on 0501\051 the demand forecast for the 2020-2030 time frame 0502\051 granted spectrum allocations to NASA and 0503\051 technology improvements that increase the spectral ef\002ciency of the communication payloads 12 


A PPENDIX Table 7  Acronyms Arch Architecture CER Cost Estimating Relationship DESDYNI Deformation Ecosystem Structure and Dynamics of Ice DSN Deep Space Network EIRP Equivalent Isotropically Radiated Power GEO Geosynchronous Orbit GRTG Guam Remote Ground Terminal GSFC Goddard Space Flight Center HD High De\002nition HP Hosted Payloads ISL Intersatellite Link ISS International Space Station LCC Life Cycle Cost LEO Low Earth Orbit MEO Medium Earth Orbit MIT Massachusetts Institute of Technology MOC Mission Operating Center MPCV Multi-Purpose Crew Vehicle NASA National Aeronautics and Space Administration NCCDS Network Control Center Data System NDA Non-Disclosure Agreement NEN Near Earth Network NISN NASA Integrated Services Network NOAA National Oceanic and Atmospheric Administration RF Radio Frequency SA Single Access SBRS Space based Relay Study SCaN Space Communication and Navigation SN Space Network STGT Second TDRSS Ground Terminal STK Systems ToolKit TDRSS Tracking and Data Relay Satellite System TT&C Telemetry Tracking and Command USGS United States Geological Survey WSGT White Sands Grount Terminal A CKNOWLEDGMENTS This project is funded by NASA under grant NNX11AR70G Special thanks for Gregory Heckler David Milliner and Catherine Barclay at NASA GSFC for their help getting the dataset and their feedback on our study R EFERENCES   National Aeronautics and Space Administration 223Space Communications and Navigation 050SCaN\051 Network Architecture De\002nition Document 050ADD\051 Volume 1  Executive Summary,\224 Tech Rep 2011   227\227 A v ailable http   www.nasa.gov/directorates/heo/scan   e a Sanchez Net Marc 223Exploring the architectural trade space of nasas space communication and navigation program,\224 in Aerospace Conference 2013 IEEE  2013   P Brown O  Eremenko 223Fractionated space architectures a vision for responsive space,\224 Tech Rep   S M V  D C E Teles J 223Overview of TDRSS,\224 Advances in Space Research  vol 16 pp 67\22676 1995   Analytical Graphics Inc A v ailable http   http://www.agi.com   D Selva Valero 223Rule-Based System Architecting of Earth Observation Satellite Systems by,\224 Ph.D dissertation Massachusetts Institute of Technology 2012   K D W  S P Davidson A 223Pricing a hosted payload,\224 in Aerospace Conference 2012 IEEE  2012   W J Larson and J R Wertz Space mission analysis and design  Microcosm Inc 1992   M Adinol\002 and A Cesta 223Contributed Paper Heuristic Scheduling of the DRS Communication System,\224 vol 8 1995   National Aeronautics and Space Administration Space Network Users Guide 050 SNUG 051  2007 no August 2007   e a Tran J J 223Evaluating cloud computing in the nasa desdyni ground data system,\224 in Proceedings of the 2nd International Workshop on Software Engineering for Cloud Computing  2011 B IOGRAPHY  Marc Sanchez Net is currently a second year M.S student in the department of Aeronautics and Astronautics at MIT His research interests include machine learning algorithms and rule-based expert systems and their suitability to the 002elds of system engineering and space communication networks Prior to his work at MIT Marc interned at Sener Ingenieria y Sistemas as a part of the team that develops and maintains FORAN a CAD/CAM/CAE commercial software for shipbuilding Marc received his degrees in both Industrial engineering and Telecommunications engineering in 2012 from Universitat Politecnica de Catalunya Barcelona Dr Daniel Selva received a PhD in Space Systems from MIT in 2012 and he is currently a post-doctoral associate in the department of Aeronautics and Astronautics at MIT and an adjunct Assistant Professor in the Sibley School of Mechanical and Aerospace Engineering at Cornell University His research interests focus on the application of multidisciplinary optimization and arti\002cial intelligence techniques to space systems engineering and architecture Prior to MIT Daniel worked for four years in Kourou 050French Guiana\051 as a member of the Ariane 5 Launch team Daniel has a dual background in electrical engineering 13 


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


