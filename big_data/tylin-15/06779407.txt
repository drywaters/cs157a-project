Abstract 
   
204 Hadoop is an open source cloud computing platform 
 Ravinder Kaur Assistant Professor, CSE Department Lovely Professional University Jalandhar, India ravinder.17686@lpu.co.in 
Hadoop: Addressing Challenges of Big Data  Kamalpreet Singh Assistant Professor, CSE Department Lovely Professional University Jalandhar, India kamalpreet.17706@lpu.co.in 
of the Apache Foundation that provides a software programming framework called MapReduce and distributed file system, HDFS It is a Linux based set of tools that uses commodity hardware which are relatively inexpensive, to handle, analyze and transform large quantity of data. Hadoop Distributed File System, HDFS, stores huge data set reliably and streams it to user application at high bandwidth and MapReduce is a framework that is used for processing massive data sets in a distributed fashion over a several machines. This paper gives a 
I 
 Big data \(also spelled Big Data\ [1 a g en e ra l t erm u s ed  to describe the voluminous amount of unstructured and semistructured data that a company generates, the data that would 
brief overview of Big Data, Hadoop MapReduce and Hadoop Distributed File System along with its architecture 
Keywords\204Big Data;Hadoop;MapReduce;HDFS; Namenode Datanode 
 I NTRODUCTION A ND R ELATED W ORK  
 
take too much time and cost too much money to load into a relational database for analysis. Big Data refers to large collection of data \(that may be structured, unstructured or semi structured\ that expands so quickly that it is difficult to manage with regular database or statistical tools. Hadoop [2  is  a  framework written in Java, distributed under Apache License developed by Doug Cutting. Hadoop is basically used for analyzing and processing Big Data. Hadoop supports the running of application on Big Data, and addresses three main challenges \(3V\ created by Big Data 
   
Volume - Hadoop provides framework to scale out horizontally to arbitrarily large data sets to address volume of data 
Velocity - Hadoop handles furious rate of incoming data from very large system 
200 200 200 
Variety -    Hadoop supports complex jobs to handle any variety of unstructured data  Hadoop is a whole ecosystem of projects that work together to provide a common set of services. It transforms commodity hardware into a coherent service that stores petabytes of data reliably and also processes that data efficiently through huge 
distribution  A key characteristic of Hadoop is the partitioning of both data and computation across number of hosts reliably, and then executing application computations in parallel close to their required data. The key attributes of Hadoop are that it is redundant and reliable, that is if you lose a machine due to some failure, it automatically replicates your data immediately without the operator having to do anything, it is extremely powerful in terms of data access and is preliminary batch processing centric and makes it easier to distributed 
applications using MapReduce software paradigm. Moreover it runs on commodity hardware which cuts off the cost of buying special expensive hardware and RAID systems    
 In traditional approach, as depicted in Figure 1, an enterprise uses a powerful computer to process the data available. But there is an upper limit to the amount of data processed because it is not scalable and Big Data grows with great velocity and variety, whereas Hadoop follows a very different approach as compared to the traditional enterprise approach. In this Big Data is first broken into smaller pieces 
Figure 1: Various Challenges of Big Data 
 
002 
 
686 978-1-4799-2572-8/14/$31.00 c 
so that large amount of data can be handled efficiently and effectively. Along with this data segregation, Hadoop breaks the computations into pieces as well that need to be performed on the data. Once all the child computations are finished, the results are combined together and sent back to the application 2014 IEEE 


r computations collection of intermediate key for that keys and merges the a gulf of clusters, there is a sin ord as <key> and its associated ling these computations to the It also takes the responsibility rk splits the input data into Task Tracker relatively inexpensive. Thus, with help of M intermediate key, that is, MapReduce wor s exclusively on key, value> pair. The two op MapReduce and filesystem, HDFS as sh well as an associated implementation for ters of relatively lification. The reduce function for a particular word <key the processing part of Hadoop and the framework, serve as input to the user red distributed systems can also easily make use a large distributed system machines, scheduling the task execution as rogrammers could focus on writing scale ollection of intermediate <key Projects here refer to set of tools that are ma File System GF d  MapR edu ce [4 Apache HDFS and Apache MapReduce resp required inter-machine communications airs of merging all the pairs associate amming model as aggregation 5 Reduce script, which MapReduce application that occurrences of each word in a 200 200  4 The pairs with same Hadoop MapReduce is a software prog ce function which ociated with same function ith managing the tput <key, value value> pairs having the same k across various independent chunks which amount of occurrence as <val MapReduce is the processing part of Had stores all the data redundantly required Figure 2: High Level Architecture of  acker acts as a coordinator for aged by Apache assistance in the under the umbrella of Hadoop to provide task related to Hadoop. Hadoop\220s origin c le master Job Tracker and one node. The master Job Tracker segments 2. These segments y explanation of above explained   WordCount example for sim pReduce software MapReduce library and pa responsibility of splitting the input da and II Following is the step wise MapReduce 1. MapReduce framew s the programs ly parallelized, the smaller components and sched reduces/merges all < key, value > pairs ass intermediate key. The MapReduce fra making programming easier. These scal executed on large clusters of commodity aggregates the values of the Below is the diagrammatic scenario taking WordCount task tracker fails. Thus, Job T A RCHITECTURE  At a very high level, Hadoop has two slave Task Tracker per cluster is responsible for accepting t and reduce come from func which pass functions as a s two parts wn in Figure 2 ree programs thus ed programs are achines which are e-executes the task if a re processed in a ified e users\220 jobs, dividing it into op and manages ient reliable, fault ue > pairs a across several Implementation MapReduce framework is which manages the jobs. It h pair rkflow of MapReduce 2014 IEEE International Advance Computing Conference IACC ey are grouped together by the completely parallelized manner by user spec that generates a set of intermediate < key, va  o w h with the same s also written by the user, takes I along with their set of values ted File System me from Google  h ich becom e  ectively  o MapReduce Having multiple machines wit framework, programmers with little knowle The map function emits each e>. Value is taken as \2171\220 in this written in MapReduce styles are automatical machines/clusters for computat 3 Map script, which is machine to process portion of set of intermediate <key, value script into a smaller set of v individual slave task trackers of monitoring the task tracke A the jobs. HDFS refers to Hadoop Distrib ain components Hadoop creates a cluster. For sets in a distributed fashion on large clu  map Working of MapReduce The MapReduce framework maps the key, value> pairs and produces a set of o tolerant manner. A MapReduce task divide ework takes the lues. Thus, the reduce function fo fo The output of these map functions, aft the input data set iven input data  according to the user specified Job Tracker aggregates all count emitted together rations in MapReduce, i.e.  map tional programming languages inexpensive commodity hardware in an effi ell as monitoring Hadoop Figure 3: W o 687 m o o u n o r p r s c s a l e u m t w w A p f m a d u p d k m o o u n o r o s c s a l e u m t w w A f m a d u d k e r o a  k s i m a c e e g w u p a h g h u r r e r o a  k s i m a c e e g  u p a g h u r r r r being sorted by guments to other functions description of execution of re then passed different ion written by user, runs on each data given to it and produces a xample. WordCount i s a outputs the total number of and re-executing the failed tasks along cessing large data ge of parallel and of the resources of input dataset into sed to Reduce function for 


6 an d MapReduce. The MapReduce server on a typical machine is called Task Tracker which is responsible for running the tasks assigned by the Job Tracker and reports the Job Tracker once the task assigned to it is completed. Job tracker then combines the results and sends the final result back to the application block report heartbeat DataNode Data Nodes are responsible for storing the blocks of file as determined by the Name Node. Data file to be stored is first split into one or more blocks internally. Data Nodes serve the read write requests from file system\220s client data These are also responsible for creating, deleting and replicating blocks of file after being instructed by the Name Node  one as main master and the other as backup master in case the main master fails III  The HDFS [6 is  dis tr ibu t e d f i l e s y s t em com pon en t of  Hadoop designed to store and support large amount of data sets on commodity hardware reliably and efficiently. Like other file system, PVFS [7 Luste r  Ha do op Dis trib ut e d Fi le Sy ste m a l so has master / slave architecture, with master as Name Node and slave as Data Node. HDFS stores all the filesystem metadata on single Name Node. However unlike Luster and PVFS HDFS uses replication of data stored on Data Node to provide reliability instead of using data protection mechanism such as RAID. The application data is stored as multiple copies on a number of slave Data Nodes, which are usually one per node in cluster 2014 IEEE International Advance Computing Conference IACC R EPLICA P LACEMENT  The placement of replica is critical to HDFS reliability and performance. HDFS acts as a self-healing system. As depicted in the figure suppose the second data node fails, we still have two other DataNodes having the required data\220s replicas. If a DataNode goes down, then the masters I NCREASED THROUGHPUT BY M IGRATING C OMPUTATIONS RATHER THAN D ATA  When the size of data is very large, it is always more efficient to perform the computations requested by an application closer to where the data it needs to operate on is stored.  In Hadoop, MapReduce framework and Hadoop Distributed File System run on the same set of nodes, that is the storage and compute nodes are the same. This configuration of Hadoop framework minimizes network congestion and increases bandwidth across the clusters as it schedules the computation closer to where data \(or its replica\ is present rather than migrating the large data sets to where application is running. This increases the overall throughput of the system IV 200 200 200     from DataNode to NameNode will cease and after ten minutes NameNode will consider that DataNode to be dead and all the blocks that were stored on that DataNode will be rereplicated and distributed evenly on other living DataNodes 688 messages received from Data Nodes Name Node also prevents Data Node from being over replicated and under replicated by evenly distributing the block of file and its replicas. Thus if a Data Node fails due to some reason, data can still be retrieved from other Data Nodes having its replica Name Node is the single point of availability failure. If a Name Node goes down, the Data Nodes would not be able to make any sense of the data blocks on them. So in order to avoid this single point of failure, Enterprise version of Hadoop keeps two B Fault Tolerance for Computation Hadoop is built keeping in mind the hardware and software failures. It is inbuilt for fault tolerance for task tracker service running on the slave computers. If any of the clusters fails or if just the task tracker service fails, Job Tracker through regular monitoring detects the failure and delegates the same job to other working task tracker. Fault tolerance is not only limited to the slave task trackers. In order to avoid single point of failure in case if master Job tracker fails, enterprise version of Hadoop keeps two master one as main master and the other as back up master in case the master dies Fault Tolerance for Data In Hadoop, failures are treated as norms rather than exception. The master Name Node makes sure that each block always has the required number of replicas through NameNode The master Name Node as a coordinator of HDFS manages the filesystem namespace by keeping index of data location and regulates access to files by clients. Files and directories are represented on Name Node and it executes operations like opening, closing and renaming files and directories. NameNode itself doesn\220t store any data neither does any data flows through the name node. It only determines and keeps track of mapping of file blocks to Data Node, thereby acting as a repository for all HDFS metadata Any application requiring data, first contacts the NameNode which provides locations of data blocks containing the file While storing/writing data to HDFS, NameNode chooses a group of \(by default three\ to store the block replicas. The client application then pipelines the data to the Data Node nominated by Name Node HDF S Figure 4: Master/Slave Architecture of Hadoop Heartbeat   200  


No rack should contain more than two replicas of the same block, provided there are sufficient numbers of racks on the cluster   6 V J. Dean, S. Ghemawat, \215MapReduce: Simplified Data Processing on Large Clusters,\216 In Proc. of the 6th Symposium on Operating Systems Design and Implementation, San Francisco CA, Dec. 2004 5 200 200 Zach Brown. \(2014, January 22\.  Big Data  \(2nd ed.\ [Online  Available http://technologyadvice.com/category/big-data  http://wiki.apache.org/hadoop/WordCount WordCount Example, June 10, 2011 [Online  No DataNode should contain more than one replica of any block of file  2  3    S. Ghemawat, H. Gobioff, S. Leung. \215The Google file system,\216 In Proc of ACM Symposium on Operating Systems Principles, Lake George NY, Oct 2003, pp 29\20543 4 Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler 215The Hadoop Distributed File System\216, Yahoo!, IEEE 2010 7 The default HDFS block placement policy provides a tradeoff between minimizing the write cost, and maximizing data reliability, availability and aggregate read bandwidth. When a new block is created, HDFS places the first replica on the node where the writer is located, the second and the third replicas on two different nodes in a different rack, and the rest are placed on random nodes with restrictions that no more than one replica is placed at one node and no more than two replicas are placed in the same rack when the number of replicas is less than twice the number of racks. The choice to place the second and third replicas on a different rack better distributes the block replicas for a single file across the cluster. If the first two replicas were placed on the same rack, for any file, two-thirds of its block replicas would be on the same rack C ONCLUSION  Hadoop was originally designed for processing batchoriented processing jobs, such as creating web page indices or analyzing log data. Hadoop is not used for OnLine Transaction Processing workloads and OnLine Analytical Processing or Decision Support System workloads where data are randomly or sequentially on structured data like a relational data to generate reports that provide business intelligence. However being reliable, \(both in terms of computation and data\ fault tolerant, scalable and powerful, Hadoop is now widely used by Yahoo!, Amazon, eBay, Facebook, IBM, Netflix, and Twitter to develop and execute large-scale analytics or applications for huge data sets. MapReduce framework of Hadoop also eases the job of programmers as they need not to worry about the location of data file, management of failures, and how to break computations into pieces as all the programs written are scaled automatically by Hadoop. Programmers only have to focus on writing scale free programs R EFERENCES    1 R ACK A WARENESS P OLICY  An HDFS file consists of blocks. Whenever a new block is required to store the data, the NameNode allocates a block with a unique block ID and determines a list of DataNodes to host replicas of the block. Data is then pipelined from the client to the DataNodes in following the sequence of minimum distance from the client. As shown in the figure, nodes are spread across multiple racks that share a switch connected by one or more core switches. The NameNode, acting as a central place maintains the metadata that helps in resolving the rack location of each DataNode. The main motive of rack aware replica policy is to improve reliability and availability of data along with network bandwidth utilization. The default HDFS rack aware replica policy is as follows T. White, Hadoop: The Definitive Guide. O\220Reilly Media, Yahoo Press,June 5, 2009         689 Figure 5: Replica Placement wi th replication value =3 Figure 6: Rack Awareness 2014 IEEE International Advance Computing Conference IACC P. H. Carns, W. B. Ligon III, R. B. Ross, and R. Thakur. \215PVFS: A parallel file system for Linux clusters,\216 in Proc. of 4th Annual Linux Showcase and Conference, 2000, pp. 317\205327  


Dividend: 9 Divisor: 4 Number: 144 Quotient: 2 Remainder: 1 SquareRoot: 12 Sum: *s divide plus plus sqrt IF ?r = 0 IF ?r = 0 T T T T T T T T T F 
  
B. Adaptive and Dynamic Operation 
A Machine An Interpreter Algorithm An Interpretation An Agent A Graph A Behavior 
runs Is a generating for Responsible for of Producing 
The first time through the CG execution, an Input nodes typically read data values, or access databases, before execution is completed. Minimally, if a single input node executes and is true, the nodes connected by CG arcs are activated A case in which zero input nodes execute as true The system control mechanism is notified that the CG Execu tion Cycle is complete. This is but the in the opposite case and the system control mechanism is notified. Hence, KEE has the ability to determine that no further nodes can be activated A CG can be viewed and analyzed in terms of graph theory utilizing the rules of CG formation. The constraints on CG formation lead to directed graphs that allow the optimization of computation  The execution of the graph is extremely efficient and straightforward. For cu stomization purpos es, several different techniques exist Ö each maintains efficiency. The exact circumstances of system integration efforts may vary from one job to the next. Hence, different execution optimization methods may be employed; and each method is within scope of the KEE The subsequent description of th e KEEês preferred embodiments provides several op timization possibilities In general, a graph G consists of a set of nodes N, and a set of arcs A with a truth value associated with each graph element The truth values permit a singl e graph to encompass multiple execution paths/outcomes that are rel ated \(shown in Figure 6 the arcs indicate a direction, as the graph in Figure 4 indicates the graph is called a As such, a CG is a directed graph. Furthermore, a CG has speci fic input and output nodes called in general graph theory. Sources and sinks are the starting point of outgoing arcs, an d the terminal point of incoming arcs, respectively A path P is a set of nodes N and directed arcs A starting at an input node, terminating at an output node, and contains a sequence of internal nodes and arcs \(Figure 5, highlighted in red\ber of paths exist in a given CG. Various techniques can be employed to compute the paths, and to store and utilize path information. Tec hniques used to store and access path information will vary from job to job, and are within the scope of the KEE The method in which the truth value is used to control graph execution allows the KEE to implement first order predicate calculus logic rules to cont rol the system \(a critical KEE point as illustrated in Figure 7 Consequently, the KEE enables useful extensions of techniques, common to expert systems. Standard systems integration theory and techniques are incorporated in the design and control system. Clearly, KEE is a powerful tool in system control, and is advanced far beyond standard systems integration theory and techniques The KEE unites numerous di sciplines in realization, and derives value from the union. The KEE provides a knowledgebased representation of the system to be integrated. Furthermore the entire toolkit of advanced system theory is employed in a trouble-free, straightforw ard manner, which contrasts significantly from current complicated methods Adaptive systems theory is an emerging science, central to automated, or çsmarté, systems. However, a major barrier, or problem, is associated with automated devices and systems Current systems are in sufficient in prov iding complex and robust control mechanisms The KEE provides a solution to the barrier in advancement and proliferation of adaptive, automated systems. The KEE has the ability to dynamically chang e the structure system control logic and which components, in cluding new ones, are currently being utilized. As such, KEE facilitates the implementation of adaptive systems The benefits obtained by using KEE are extended in several dimensions \(for example the ability to interactively and automatically control the system\ performance and 
Execution Cycle \(EC\xecution of the input nodes the Execution Cycle is complete the simplest case general concept is true When there are no more nodes that can be activated the Execution Cycle is complete Figure 6. Sample conceptual graph showing a path outlined in red\ontained within the graph Figure 7. Use of truth values to control CG execution 
directed graph sources and sinks 
A. First Order Predicate Calculus 
545 
61 
61 
61 


                 
flexibility is achieved by u tilizing several CGs at once to integrate and control a particularly complex system The ability to dynamically chang e the system control logic is central to the KEEês enhanced flexibility. Typically, a system upgrade in which the control mechanism is altered requires the system to be restarted regularly the equivalent of a çrebooté to PC users\he KEE eliminates the need to restart KEE has the capability to Execute multiple CGs Dynamically load a new CG simultaneous to system execution Pause a CG execution Re-start a CG execution once it has been paused Stop a CG execution with no re-start Enabling a powerful set of tools to manage the complexity of system integration solution analysis design, and implementation will have a dramatic impact on the industries where system integration is a key function  V. E XAMPLE   B UILDING A UTOMATION M ANAGEMENT  To illustrate CONDORês concept of operation, consider the challenge in constructing a building automation management system. Managing the high tech bui ldings in an energy efficient manner, as well as satisfying the occupan ts, is a difficult and costly task without intelligent control systems. An intelligent building maximizes efficiency and energy resources, and minimizes costs. Effectual intelligen ce depends on the intelligent design Minimally, the attri butes an intelligent building should possess include Instant knowledge of occurrences inside and out Assessment and resolution fo r the most convenient comfortable, and productive environment Immediate response to occupants' requests The attributes indicate a ne ed for various technology and management systems. Successful systems integration produces the intelligent bu ilding. Smart automation systems respond to external climate factors and co nditions. Simultaneo usly, sensing control, and monitoring of internal environment factors occur CONDOR provides Reduction of energy consumption and environmental pollution Security for people, possess ions, and environment Increased processing efficiency and decreased time Simple and clear processing features Operations-oriented maintenance management of technical installations to reduce system downtime and decrease living costs Historical and dynamic data processing, presentation, and analysis To realize the above benefits a powerful control and automation system, with va rying levels of information processing \(Figure 8\ust be installed. In order to implement such a system, several sub-systems must be employed \(shown in Figure 14 A perfect integrated buildin g automation system allows physical and functional access to all building systems Integration does not occur unl ess data communication among the various systems transpires in accordance with requirements For integration purposes, an analysis of information requirements is essential \(i.e. safe access to the right information in the right way, at the right place  Supervisory control systems are central to building energy management, and consist of hierarchi cally organized, functional control systems with separate intellig ent automation units. The following aspects are required Each level must operate independently Data interchange must be reduced to a minimum Operational readiness of machinery must not be impaired by communication interchange breakdowns A commanding supervisory syst em is based on distributed intelligence, in which outstations controllers\by a communication bus \(network The control system is configured into four hierarchical Information processing levels Analyzes the systems operating status. Physical and technical data relating to the building, and emanating from the lower control and automation levels, are accessed in a condensed form and processed. Processing is achieved via miniature single board 
                 
Level 1 Information management system Level 2 Information processing And supervisory system Level 3 Information processing Automation system Level 4 Process field 
Figure 8. Automation levels Information and Management Level 
546 
62 
62 
62 


computers \(SBCs\ploying web-browser interfaces that interface to all automation levels Controls, monitors, and logs the processes within the entire building. Configures automation units, measurements, and control units at a level three and sets the parameters. Further functions include: data processing, data recording, and maintenance management of the technical installationês energy management Contains the distri buted intelligence for mathematical and ph ysical-based operation functions, as outstation multi-controllers. The di stributed digital control DDC\onitors and controls the critical statuses and processes within the building The control system, which also provides programmable controller \(pLC functions, allows a logical link in the form of time or status el ements. It optimizes performance and security of installations. The auto mation level consists of numerous DDC controller outstations. Each panel is fully programmable and autonomous in operation. The panels coordinate communication up wards to the central computer horizontally to other outstations Contains the sensors and actuators which directly link to automation syste ms at level three. Most sensors and actuators are available solely as analogue units Communication with levels two and on e is limited to level three  VI  S UMMARY  An approach has been presented for in telligent application development and systems integration within the CONDOR architecture. The approach provides a low-cost, straightforward method for developing easy to use, yet complex consumer devices and systems. CONDOR employs advanced system representation frame works, flexible comm unications structures intelligent application services \(e.g. real-time control\plex synchronization relati onships and constraints, data time tagging checkpoint restart, dynamic syste m control, and persistence CONDOR also provides intelligent application developers and system integrators with a pa rallel and distributed computing capability that supports a wide variety of hardware hosting and integration VII  R EFERENCES   Wallace, Jef f rey and Sara Kam bouris, 2012 A utomated Intelligent Instructional Monitoring Systems,é Proceeding of the 2012 International Conference on Artificial In telligence, World Academy of Sciences, Las Vegas, NV, July 16-19  Wallace, Jef f rey  et al, 2009  Ontolog y based Software and Hardware System Integration and Intelligent Automation,é Proceedings of the World Multi-Conference on Systemics, Cybernetics and Informatics, International Inst itute of Informatics and Systemics Orlando, FL, July 10-13  Peirce, C  S Coll ect ed Pap e r s  of Charles S. Peirce, Hartshorn e Weiss, and Burks \(eds.\ University Press, Cambridge, MA 1932  Sowa, J.F., Knowledge R e presentation  Logical, Philosophical, and  Computational Foundations. 2000, Paci\002c Grove, CA: Brooks/Cole, pp xiv+594  Microsoft .N ET Framework http m sdn.m icrosoft.com enus/vstudio/aa496123  , Microsoft, Inc., 2014  Parallel Virtual Machine http://www.csm.ornl.gov/pvm/pvm_building.html , Oak Ridge National Laboratory, 2005  Environment http://www.cs.wustl.edu/~schmidt/ACE.html , Washington University 2011  POSIX Threads Programming http://www.llnl.gov/computing/tutorials/pthreads/ , Lawrence Livermore National Laboratory, June 30, 2013  Real Time C O RBA with TA O http://www.cs.wustl.edu/~schmidt/TAO.html , Washington University 2013  IBM Comp onent Conn ector  Overview http://www.redbooks.ibm.com/abstracts/SG242022.html?Op en IBM, Inc. June 19, 1998  ORBit2, http://www.gnome org/projects/OR Bit2/, GNOME Foundation, June 29, 2004  SAP Solutions, http www.sap.com/solution html, SAP AG 2014  Oracle Enterprise Resource Planning http://www.oracle.com/us/products/applications/enterprise-resourceplanning/overview/index.html, Oracle, Inc. January 2014  Oracle Ser v ice-Ori ented Architecture http://www.oracle.com/us/products/middleware/soa/overview/index.ht ml, January 2014  About W3C: Technolog y   http://www.w3.org/Consortium/technology, W3C Consortium, June 2012 
Supervisory Control Level Automation Level Field Level Figure 9. Building sub-systems 
Heating Air conditioning Computers Computer Accessories Sanitary Lighting Wastemanagement Electrical Acoustics Video/Security Safety systems Kitchen appliances Entertainment Systems Controls Remote Access Ventilation 
Home 
547 
63 
63 
63 


               


C C C 
Intuitively delta compression should slow down the data-restore performance of a data-reduction system since it needs to restore the resembling chunks by two reads one for the delta data and the other for the base-chunk and then delta decode them But in our evaluation of the restore operations for resembling chunks we nd that the speed of delta decode i.e Xdelta  8  tends to be v ery f ast about 1GB/s in the DARE system Another interesting observation is that for a restoration cache of a given size with a delta chunk and its based chunk  DARE actually caches more logical content of the two chunks and than a deduplication-only system and thus improves the datarestore performance by virtual of the enlarged restoration cache due to delta compression Figures 7\(a and 8\(a show that DARE on average doubles the data-restore speed of the deduplication-only system both running on the RAID Figures 7\(b and 8\(b clearly show that the reason lies in the fact that DARE reads half as many containers for restoration as the deduplicationonly system The superior data-restore performance of SF-2F and SF-4F to the deduplicationonly system is attributed to their data reduction efìciency see Tables 3 and 4  Since the restoreperformance for the other six datasets is similar to and consistent with that of the Less dataset they are omitted to save space The sudden increase in the data-restore performance of the deduplicationonly approach at the backup version 17 Figure 8\(a we observe is due to the fact that most of the backed-up sources targeted for restoration are from the current and recent backups and thus have fewer random reads for restoration 
Figure 5 Percentages of data reduced by DupAdj and the SF SF SF of the super-feature approach respectively in the streaminformed DARE SF-2F and SF-4F approaches a Throughput on RAID b Throughput on SSD Figure 6 Throughputs of four resemblance detection enhanced data reduction approaches on the two synthesized datasets a Restoration throughput b Containers read Figure 7 Data-restore performance versus backup version on the Linux dataset with an LRU cache of size 256MB a Restoration throughput b Containers read Figure 8 Data-restore performance versus backup version on the Less dataset with an LRU cache of size 512MB because it incurs the largest computation overhead for resemblance detection It is noteworthy that DAREês average data-reduction throughput on RAID at 50MB/s is much lower than DAREês average throughput of 85MB/s on SSD The root cause of RAIDês inferior data-reduction performance in Figure 6\(a mainly lies in the random reads of the base-chunks In general DARE achieves superior performance of both throughput and data reduction efìciency among all the resemblance detection approaches as indicated in Figure 6 and Table 4  
002 
4.5 Restoration Performance 
i,k i i k 
1 2 3 
st nd rd 
211 


In this paper we present DARE a deduplication-aware low-overhead resemblance detection and elimination scheme for delta compression on the top of deduplication on backup datasets DARE uses a novel resemblance detection approach DupAdj which exploits the duplicate-adjacency information for efìcient resemblance detection in existing deduplication systems and employs an improved super-feature approach to further detecting resemblance when the duplicate-adjacency information is lacking or limited Our preliminary results on the data-restore performance suggest that supplementing delta compression to deduplication can effectively enlarge the logical space of the restoration cache but the data fragmentation in data reduction systems remains a serious problem  19  W e plan to further study and improve the data-restore performance of storage systems based on deduplication and delta compression in our future work This work was supported in part by National Basic Research 973 Program of China under Grant No 2011CB302301 NSFC No 61025008 61173043 and 61232004 863 Project 2013AA013203 US NSF under Grants IIS-0916859 CCF-0937993 CNS-1116606 and CNS-1016609 This work was also supported by Key Laboratory of Information Storage System Ministry of Education China 
 G W allace F  Douglis H Qian P  Shilane S Smaldone M Chamness and W  Hsu Characteristics of backup workloads in production systems in  2012  P  Shilane M Huang G W allace and W  Hsu W AN optimized replication of backup datasets using streaminformed delta compression in  2012  A Muthitacharoen B Chen and D Mazieres A lo w-bandwidth netw ork le system  in  2001  C Constantinescu J Glider  and D Chambliss Mixing deduplication and compression on acti v e data sets  in  IEEE 2011 pp 393Ö402  B Zhu K Li and H P atterson A v oiding the disk bottleneck in the data domain deduplication le system  in  USENIX Association 2003  J Gailly and M Adler  The gzip compressor   http://www gzip.or g 1991  P  K ulkarni F  Douglis J LaV oie and J T race y  Redundanc y elimination within lar ge collections of les  in  USENIX Association 2004  J MacDonald File system support for delta compression  Masters thesis Department of Electrical Engineering and Computer Science University of California at Berkeley 2000  S Quinlan and S Dorw ard V enti a ne w approach to archi v al storage  in  2002  F  Douglis and A Iyengar   Application-speciìc delta-encoding via resemblance detection  in  USENIX Association 2003  L Arono vich R Asher  E Bachmat H Bitner  M Hirsch and S Klein The design of a similarity based deduplication system in  ACM 2009  M Rabin  Center for Research in Computing Techn Aiken Computation Laboratory Univ 1981  D Gupta S Lee M Vrable S Sa v age A C Snoeren G V ar ghese G M V oelk er  and A V ahdat Dif ference engine harnessing memory redundancy in virtual machines in  2008  Q Y ang and J Ren I-CASH Intelligently coupled array of ssd and hdd  in  2011  A Broder  Identifying and ltering near duplicate documents  in  2000   Some applications of Rabins ngerprinting method  in  1993   On the resemblance and containment of documents  in   V  T araso v  A Mudrankit W  Buik P  Shilane G K uenning and E Zadok Generating realistic datasets for deduplication analysis in  2012  M Lillibridge K Eshghi and D Bhagw at Impro ving restore speed for backup systems that use inline chunkbased deduplication in  2013  W  Xia H Jiang D Feng and Y  Hua SiLo A Similarity-Locality based Near Exact Deduplication Scheme with Low RAM Overhead and High Throughput in  2011 
Proc USENIX FAST Proc USENIX FAST Proc ACM SOSP Data Compression Conference DCC 2011 Proc USENIX FAST USENIX Annual Technical Conference Proc USENIX FAST Proc USENIX FAST Proceedings of SYSTOR 2009 The Israeli Experimental Systems Conference Fingerprinting by random polynomials Proc USENIX OSDI Proc IEEE HPCA Combinatorial Pattern Matching Sequences II Methods in Communications Security and Computer Science Compression and Complexity of Sequences 1997 USENIX Annual Technical Conference Proc USENIX FAST USENIX Annual Technical Conference 
5 Conclusion and Future Work Acknowledgments References 
212 


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


