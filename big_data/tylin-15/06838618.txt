   Abstract\227 POWER8\231 delivers a data-optimized design suited for analytics, cognitive workloads, and today\222s exploding data sizes.   The design point results in a 2.5x performance gain over its predecessor, POWER7+\231, for many workloads.    In addition, POWER8 delivers the efficiency demanded by cloud computing models and also represents a first step toward creating an open ecosystem for server innovation I  INTRODUCTION eeting the needs of evolving server workloads requires new system capabilities.   Data volume is growing exponentially, and \223Big Data\224 problems can require the use of exabytes of data within a single application.   Moreover this mass of ever-growing data is typically unstructured Scanning, organizing, and ultimately analyzing large unstructured data-sets requires large memory capacities with high bandwidth, high compute resources, and strong thread performance to address serial code segments   At the same time, delivering significant performance gains is increasingly difficult.   While technology continues to scale to smaller dimensions, extracting performance gains requires higher levels of innovation, and increasing complexity drives greater resource investments  In the face of these challenges, POWER8\231 delivers a dataoptimized design suited for analytics and cognitive workloads In addition, POWER8 provides the efficiency demanded by cloud computing models and also represents a first step toward creating an open ecosystem for server innovation II  D ATA OPTIMIZED DESIGN  The POWER8\231 processor [1 s i m pl e m e n t e d i n IBM\222s  22nm SOI technology h e 649m m 2 di e, sh o w n i n F i g u r e 1, includes 12 enhanced 8-way multithreaded cores with private l2 caches, a 96MB high-bandwidth eDRAM L3 cache an on-chip SMP fabric, cryptography and compression accelerators, memory controllers that connect to up to 8 memory buffers, seven SMP links, and 32 lanes of integrated PCI Gen 3 IO  Figure 1:   The POWER8 Processor  The design point results in a 2.5x performance gain over its predecessor, POWER7+\231, for many workloads.    All aspects of the design including the technology, core design, cache structure, and nest architecture are optimized for large datasets, analytics, and cognitive workloads A  Technology POWER8 leverages IBM\222s 22nm SOI technology with embedded DRAM and deep trench capacitance.   The technology density enables over 4.2 billion transistors and 31.5\265F of decoupling capacitance.  15,283 pads provide power, ground, and signal IO.   Three thin-oxide transistor Vt\222s are used for power/performance tuning, and thick-oxide devices enable high-voltage I/O and analog designs  The 15-layer hierarchical metal stack \(BEOL\ with multiple high performance signal wire planes is specifically architected for high-frequency and low-latency design.   The metal stack enables the creation of high-bandwidth super-highways within the chip that transport up to 3.6TB/s of data between cores caches, and processor I/O.   As shown in Figure 2, the BEOL contains 5-80nm, 2-144nm, 3-288nm, and 3-640nm pitch layers as well as 2-2400nm ultra thick metal \(UTM\tch layers for low-resistance distribution of power and clocks The Power8 TM Processor:   Designed for Big Data, Analytics, and Cloud Environments Joshua Friedrich 1 Hung Le 1 William Starke 1 Jeff Stuechli 1 Balaram Sinharoy 4 Eric J. Fluhr 1 Daniel Dreps 1  Victor Zyuban 2 Gregory Still 3 Christopher Gonzalez 2 David Hogenmiller 1 Frank Malgioglio 4 Ryan Nett 1  Ruchir Puri 2 Phillip Restle 2 David Shan 1 Zeynep Toprak Deniz 2 Dieter Wendel 5 Matt Ziegler 2 Dave Victor 1   1 IBM STG, Austin, TX   2 IBM T. J. Watson, Yorktown Heights, NY  3 IBM STG, Raleigh, NC 4 IBM STG, Poughkeepsie, NY      5 IBM STG, Boeblingen, Germany M 


   Figure 2:   POWER8\222s Metal Stack  3 specialized SRAM cells enable performance versus density tradeoffs in the cache designs:   \(1\0.160um 2 6T with splitwordline for banked read high-performance SRAMs, \(2 0.144um 2 6T for density-optimized SRAMs, \(3\2um 2 8T for two-port SRAMs.   Additionally, ground-rule clean CAM and register file cells enable efficient architecture support such as the 8 read, 6 write GPR cell.   Most importantly, the embedded DRAM, with a cell size of 0.026um2, provides an area and power efficient implementation of the 96MB L3 cache needed to store large datasets on-die B  Core While most multi-core processors focus on core count increase at roughly constant core performance, the POWER8   s h o w n i n  Figu re 3, h a s an enh a n ced m i c roarchitecture that doubles POWER7\222s per-core throughput Eight-way simultaneous multi-threading allows eight independent software threads to efficiently share the core resources.   Instruction level parallelism is also exploited with increased dispatch and execution bandwidth.   In a given cycle, the core can fetch, decode, and dispatch up to eight instructions and issue and execute up to ten instructions. There are 16 execution pipelines within the core: 2 fixed-point, 2 load-store, 2 load, 4 double-precision \(8 single-precision floating-point pipelines, 2 fully symmetric vector units for VMX and VSX instructions, 1 crypto, 1 branch, 1 condition register logical, and 1 decimal floating-point pipeline  To accommodate the large footprint of big data and analytics applications, the POWER8 core has twice as much L1 data cache per core, twice as many ports from the data cache for higher read/write throughput, and four times as much translation \(TLB\ootprint, compared to POWER7  Single-thread performance is improved by > 1.5x in POWER8 TM with deeper out-of-order execution, better branch prediction, reduced latencies to key resources such as caches and TLBs, and advanced prefetching with greater application software control. In single-thread mode, almost all the resources of the highly-parallel SMT8 core are used by the single thread   Figure 3:   POWER8 Core Floorplan C  Cache The POWER8 cache hierarchy is ts o f a n  L 1  instruction cache, a store-thru L1 data cache, a private, storein L2 cache, and a large, local L3 cache region built from eDRAM that is part of the chip\222s shared L3 cache  POWER8\222s cache hierarchy has been architected for big-data workloads by doubling POWER7\222s capacity and bandwidth as shown in Error! Reference source not found The L1 data cache increased from 32KB to 64KB, the L2 grew from 256KB to 512KB per core, and the L3 local region increased from 4MB to 8MB.  To enable the larger cache size, the 22nm eDRAM physical design implemented a 2-level bitline hierarchy with 66 cells per bitline to provide a 30% subarray area reduction at constant technology versus the prior 3-level 34-cell-per-bitline design  In spite of the increased size, the latencies to each level of cache remain roughly the same as in POWER7.  POWER8 also introduces up to 128MB of shared L4 memory cache per UTM 5   UTM 323-8x 


  processor chip in the attached memory buffer chips  The POWER8 cache hierarchy is plumbed end-to-end for data bandwidth.   Double-wide dataflows extend through the L2 load and store paths, L2 cache arrays, local L3 region read/write paths and cache arrays, and through the on-chip interconnect to the memory read/write interfaces TABLE I  P OWER 8 CACHE ATTRIBUTES   POWER 7 POWER 8 L1 data capacity / core 32kB 64kB L2 capacity / core 256kB 512kB L3 capacity / core 4MB 8 MB L4 capacity / chip N/A 128 MB L2 bandwidth / core @ 4GHz 256 GB/s 512 GB/s L3 bandwidth / core  @ 4GHz 128 GB/s 256 GB/s Total L3 Interconnect bandwidth 512 GB/s 1382 GB/s  D  Nest POWER8 maintains a balanced system and optimizes for large dataset workloads by scaling the nest architecture and IO bandwidths with the increased compute capacity.   The end to end data and coherence bandwidth of the system is more than 2x POWER7  Each of the 12 cores connects to the constant frequency fabric via a 192GByte/s asynchronous data interface.   Eight differential memory interfaces supply 230GByte/s sustained memory data bandwidth \(>2x POWER7\o the processor from a new memory buffer chip called Centaur \(shown in Figure 4\The processor to buffer interface speed increased from 6.4Gb/s to 9.4Gb/s and the overall memory latency was reduced by roughly 20%.   In addition, Centaur contains 16 MB of eDRAM cache along with an improved DRAM scheduler with support for prefetch and write optimizations The scheduling enhancements and L4 cache enable over 90 processor  to memory channel efficiency  Figure 4:   Centaur Memory Buffer  The on-node and off-node SMP buses enable 494 GByte/s of chip to chip communication that includes data, command control, error correction, and sparing.  32 lanes of integrated PCIe Gen-3 with 3 PCI Host Bridges \(PHBs\ provide 64GB/s of IO bandwidth and achieve a DMA read latency less than 1/3 of what POWER7 could achieve with a discrete IO hub chip  To support POWER8\222s 7.6Tbit/s of raw off chip bandwidth 160% POWER7\231\rcuit innovations were needed to reduce I/O power.   The memory interface is 5pJ/bit with a 20dB reach at nyquist.  It invokes all equalization in the receiver uses decision feedback equalization \(DFE-1\d leverages a low-power 4.8GHz CMOS resonant clock distribution.   As a result, it runs 1.5X faster than POWER7+\222s memory interface while consuming 50% less power.  All differential interfaces use T-coils for return loss management and deep trench \(DT capacitors for noise control.  Th e socket to socket interface is single-ended running 4.8Gbit/s \(1.5x faster than POWER7 and enables low-latency, synchronous data transfer III  H IGH E FFICIENCY D ESIGN FOR CLOUD ENVIRONMENTS   Servers increasingly operate in cloud environments and hyperscale data centers where efficiency is an essential metric.   As a result, POWER8 focuses on optimizing for cloud virtualization and energy efficiency  Cloud partitions often have a small number of threads and therefore, may not be able to fully utilize all eight threads on the POWER8 core.   To accommodate many small cloud partitions and efficiently use the core\222s high throughput, the POWER8 core can be put in a \223split core mode\224 where four partitions run on the core at the same time, with at most 2 threads per partition  To increase energy efficiency and maintain POWER7+\222s power dissipation in spite of its large increase in performance POWER8 invested significantly in power management  The core+L2 & L3 voltage are partitioned into independent power-gating regions using 5 header columns.   A charge pump creates a near zero-power idle state by raising the header gate voltage for >1000x leakage reduction over active mode  A new On-Chip Controller \(OCC\ built from an embedded PowerPC\231 core with 512KB of SRAM runs real-time control firmware to respond to workload variations by adjusting the per-core frequency and voltage based on activity, thermal voltage, and current sensors.  The on-die OCC achieves 100X speed up in response to workload changes over POWER7+ and enables reaction under the timescale of a typical OS timeslice.   It also allows for multi-socket, scalable systems to be supported as the number of OCCs scales with the number of processor sockets  POWER8 is also capable of r unning each core at a unique operating voltage using an internal voltage regulator 


  Optimizing both voltage and frequency for workload variation enables ~ 50% increased power savings at \275 frequency versus optimizing frequency only.  The distributed regulator includes a central voltage controller \(VREGC\which receives a sense line from the grid, a code representing the target voltage, and an external high precision reference voltage. To minimize the error between target and output voltages, VREGC transmits a 7-bit up/down correction code \(UPDN\o 64 charge-pump based microregulators \(UREGs\which are distributed throughout the 5 header column s and control the headers using a mix of fast-switching and RC filtered control lines.   Figure 5 shows the regulator structure and its power saving benefit   Figure 5:   Internal Voltage Regulator & Benefit  Other key power reduction techniques such as cache architecture, Vt and device size tuning, and PHY optimization achieved >18% chip power reduction.  Low and high frequency resonant clocking modes for the core mesh saved an additional 4% of chip power.   Each core clock sector includes programmable strength clock buffers, two identical inductors built from the 3um-thick UTM, two mode-switches, and a common large decoupling capacitor node.  Automatic changes between non-resonant, low-frequency resonant, and highfrequency resonant modes during functional operation are supported with no impact to performance through a gradual 15-step process to turn-on the switches connecting the LC tank to the mesh IV  OPEN ECOSYSTEM ENABLEMENT  Since server workloads will continue to evolve, the POWER8 team wanted to establish an open standard that enabled ongoing off-processor innovation and adaptation in POWER8 systems from a variety of industry sources.   In addition, many workloads can benefit from encapsulating specific algorithms in hardware to support the general purpose cores for a heterogeneous computing solution  To meet these needs, POWER8 introduces the Coherent Accelerator Processor Interface \(CAPI A P I pr ov ides  the capability for off-chip accelerators to participate in the system memory coherence protocol as a peer of other caches in the system and to use effective addresses to reference data structures just as an application running on the cores does These accelerators can be plugged into PCIe slots and implemented in field-programmable gate arrays \(FPGAs application-specific integrat ed circuit \(ASIC\ chips  A block diagram of the CAPI hardware is shown in Figure 6 The Coherent Accelerator Processor Proxy \(CAPP\ and PCIe Host Bridge \(PHB\ on the POWER8 processor chip act as memory coherence, data transfer, interrupt, and address translation agents on the SMP interconnect fabric on behalf of the POWER Service Layer \(PSL\nd Accelerator Function Units \(AFU\which reside in an FPGA or ASIC connected to the processor chip by up to 16 lanes of a PCIe Gen3 link Address translation is provided by a memory management unit MMU\ the PSL.   The PSL may also generate interrupts on behalf of AFUs to signal AFU completion, or to signal a system service when a translation fault occurs             Figure 6:   CAPI Block Diagram V  C ONCLUSION  POWER8 delivers a design optimized for big-data, analytics and cognitive workloads while supporting efficient cloud operating environments and introducing the CAPI framework for off-chip innovation and acceleration R EFERENCES  1  J. Stuechli, et. al, \223Next Generation POWER Microprocessor\224, Hot Chips 2013 2  E. Fluhr et al, \223POWER8 TM A 12-Core Server-Class Processor in 22nm SOI with 7.6Tb/s Off-Chip Bandwidth\224 , ISSCC 2014, 5.1 3  S. Narasimha, \22322nm High-Performance SOI Technology Featuring Dual-Embedded Stressors, Epi-Plate High-K Deep-Trench Embedded DRAM and Self-Aligned Via 15LM BEOL,\224 IEEE IEDM, 2012 4  B. Sinharoy et. al, \223IBM POWER TM Processor Core Micro-architecture\224 IBM Journal of Research Development  5  W. Starke et al, \223The cache and memory subsystems of the POWER8 processor,\224 IBM Journal of Research Development  6  J. Stuecheli and Bart Blaner, \223CAPI:   A Coherent Accelerator Processor Interface\224 IBM Journal of Research & Development  


                          


14 H. T. Kung, F. Luccio and F. P. Prep arata, On finding the maxima of a set of vectors, ACM Journal, 1975, vol. 22, no. 4, pp. 469-476 15 M. W. Lee and S. W. Hwang, Continuous Skyline on Volatile Moving Data, Proc. ICDE, 2009, pp. 1568-1575 16 X. M. Lin, Y. D. Yuan, W. Wang and H. J. Lu, Stabbing the Sky Efficient Skyline Computation over Sliding Windows, Proc. ICDE, 2005 pp. 502-513 17 E. Lo, K.Y. Yip, K. I. Lin, and D.W. Cheung, Progressive skyline over web-accessible databases. Data Know ledge Engineering 2006, vol. 57 no.2, pp.122-147 18 M. Morse, J. M. Patel and W. I. Grosky, Efficient Continuous Skyline Computation. Informa tion Science, 2007, vol. 177, no. 17, pp. 34113437 19 D. Papadias, Y. F. Tao, G. Fu and B. Seeger, An optimal and progressive algorithm for skyline queries, Proc. SIGMOD, 2003, pp 467-478 20 D. Papadias, Y. F. Tao, G. Fu and B. Seeger, Progressive skyline computation in database system. AC M Journal, 2005, vol. 30, no. 1, pp 41-82 21 N. Roussopoulos, S. Kelly and F. Vincent, Nearest Neighbor Queries Proc. SIGMOD, 1995, pp. 71-79 22 I. F. Su, Yu-Chi Chung, Chiang Lee and Yi-Ying Lin, Efficient skyline query processing in wireless sensor networks, Journal of Parallel and Distributed Computing, 2010, vol. 70, no. 6, pp. 680-698 23 K. L. Tan, P. K. Eng and B. C Ooi, Efficient Progressive Skyline Computation, Proc. VLDB, 2001, pp. 301-310 24 Y. F. Tao and D. Papadias, Maintia ning sliding window skylines on data streams, TKDE, 2006, vol. 18, no. 3, pp. 377-391 25 L. Tian, L. Wang, P. Zou, Y. Jia and A. Li, Continuous Monitoring of Skyline Query over Highly Dynamic Moving Objects, Proc. MobiDE\22207 2007, pp. 59-66 26 L. Tian, P. Zou, A. Li and Y. Jia, Grid Index Based Algorithm for Continuous Skyline Computation, Chinese Journal of Computers, 2008 vol. 6, no. 6, pp. 998-1012 27 A. Vlachou, C. Doulkeridis, Y. Kotidis, and M. Vazirgiannis SKYPEER: Efficient subspace skyline computation over distributed data Proc. ICDE, 2007, pp.416-425 28 S. Wang, Q.H. Vu, B.C. Ooi, A.K.H. Tung and L. Xu, Skyframe: A framework for skyline query processing in peer-to-peer systems,VLDB Journal, 2009, vol. 18, pp. 345-362  59 


important component of the discussed B DLM and must also be done in a secure and trustworthy way  V  B IG D ATA I NFRASTRUCTURE BDI  Figure 4 provide s a general view on the Big Data infrastructure that includes the general infrastructure for general data management, typically cloud based, and Big Data Analytics part that will require high performance computing clusters  which in their own turn will req uire high performance low latency network    General BDI services and components include  000x  Big Data Management tools  000x  Registries, indexing/search, semantics, namespaces  000x  Security infrastructure \(access control, policy enforcement confidentiality, trust, availa bility, privacy  000x  Collaborative environment \(groups management     Figure 4   General Big Data Infrastructure functional components   We define Federated Access and Delivery Infrastructure FADI as an important component of the general BDI that interconnects different components of the cloud/Intercloud based infrastructure combining dedicated network connectivity provisioning and federated access control [19, 37   A  B ig Data Analytics Infrastructure  Besides the general c loud base infrastructure services storage compute infrastructure/VM management  the following specific applications and services will be required to support Big Data and other  data centric applications  23, 24, 38  wh ich we will commonly refer to as Big Data Analytics Infrastructure \(BDAI  000x  Cluster services  000x  Hadoop related services and tools  000x  Specialist data analytics tools logs events data mining etc  000x  Databases/Servers SQL, NoSQL  000x  MPP \(Massively Parallel Processing  databases     Figure 5   Big Data Analytics Infrastructure components   Big Data analytics tools are currently offered by the major cloud services providers such as: Amazon Elastic MapReduce and Dynamo 39   Mi c r o s o f t  A zu r e H DI n s ig h t   4 0    IBM Big Data Analytics  41   Sc ala b l e  Had o o p  an d  d a ta  an aly tics  to o ls  service s are offered by few companies that position themselves as Big Data companies such as Cloudera, [42  an d  f ew  o th er s  43   VI  C LOUD B AS ED I NFRASTRUCTURE S ERVICES FOR B DI  Figure 6  illustrates the typical e Science or enterprise collaborative infrastructure that  is created on demand and includes enterprise proprietary and cloud based computing and storage resources, instruments, control and monitoring system visualization system, and users represented by user clients and typically residing in real or virtual cam puses   The main goal of the enterprise or scientific infrastructure is to support the enterprise or scientific workflow and operational procedures related to processes monitoring and data processing Cloud technologies simplify the building of such infras tructure and provision it on demand. Figure 6 illustrates how an example enterprise or scientific workflow can be mapped to cloud based services and later on deployed and operated as an instant inter cloud infrastructure It contains cloud infrastructure s egments IaaS VR3 VR5 and PaaS VR6 VR7 separate virtualised resources or services \(VR1, VR2\, two interacting campuses A and B, and interconnecting them network infrastructure that in many cases may need to use dedicated network links for guaranteed p erformance   Efficient operation of such infrastructure will require both overall infrastructure management and individual services and infrastructure segments to interact between themselves This task is typically out of scope of the existing cloud service  provider models but will be required to support perceived benefits of the future e SDI. These topics are a subject of another research we did on the InterCloud Architecture Framework [19 37    110 


  Figure 6 From scientific workflow to cloud based infrastructure  VII  R ELATED WORK  There are not many academic papers related to the definition of the Big Data Architecture or its components Due to the specifics of this paper that intends to explore a new emerging technology domain   we have widely researched  both currently existing publications related to the Big Data technology and research papers and best practices documents from other domains that could contribute to the definition of the proposed Big Data Architecture Framework. A  number of publications  standards, and industry best practices have been  mentioned and cited in this paper. Here we just mention these works that we consider as a foundation for our work The authors actively contribute to the NIST Big Data Working Group that provide s  a good forum for discussion but have plans to produce initial set of the draft document s  only by the end of September 2013 The following publications contribute to the research on the Big Data Architecture  NIST Cloud Computing Reference Architecture CC RA 18   B ig  Data  E co s y s te m  A r ch itect u r e definition by Microsoft [20   B ig  Data  tech n o lo g y  a n al y s is  b y  G.Mazzaferro [21     We also refer to other related architecture definitions Information as a Service  by Open Data Center Alliance [22   TMF Big Data A nalytics Architecture 23   IBM Business Analytics and Optimisation  Reference Architecture 24   LexisNexis HPCC Systems [25   VIII  F UTURE R ESEARCH AND D EVELOPMENT  The future research and development will include further Big Data definition initially presented in this paper  At this stage we attempt ed to summarise and re think some widely used definition s  related to Big Data, further research will require more formal approach and taxonomy of the general Big Data use cases in different Big Data origin and target domains also analyzing different stakeholder groups    The authors will extend their research into defining the Big Data Security Framework with the specific focus on data centric security that should allow secure data storage transfer and processing in di stributed data storage and processing infrastructure   The authors  are  also looking into defining data structures for high performance streaming applications and developing new types of disk based stream oriented data bases, continuing the work started fro m the authors work on CakeDB  database  4 4    The authors will continue contributing to the NIST Big Data WG targeting both goal s  to propose own approach and to validate it against the industry standardisation process   Another target research direction is d efining a Common Body of Knowledge \(CBK\ in Big Data to provide a basis for a consistent curriculum development. This work and related to the Big Data metadata procedures and protocols definition is planned to be contributed to the Research Data Alliance RDA   4 5     The authors believe that the presented paper will contribute  toward the definition of the Big Data Architecture Framework and provide a basis for wider discussion to define a new research and technology domain   en-GB A CKNOWLEDGEMENTS  This work is supported by the FP7 EU funded project s  GN3plus and EUBrazil Cloud Connect The authors also express acknowledgement to the members of the Big Data Interest Group at the University of Amsterdam for contribution to the discussion on Big Data Ar chitecture framework and provided valuable advices regarding use cases, suggested technology use and basic operational models  R EFERENCES  1  Global Research Data Infrastructures: Towards a 10 year vision for global research data infrastructures Final Roadm ap March 2012  O nline  Available  http://www.grdi2020.eu/Repository/FileScaricati/6bdc07fb b21d 4b90 81d4 d909fdb96b87.pdf  2  Riding the wave: How Europe can gain from the rising tide of scientific data Final report of the High Level Expert Group on Scientific Data October 2010   O nline   Available at http://cordis.europa.eu/fp7/ict/e infrastructure/docs/hlg sdi report.pdf  3  Y.Demchenko  P Membrey, P  Grosso, C   de Laat 000 Addressing Big Data Issues in Scientific Data Infrastructure  000  in First International Symposium on Big Data and Data Analytics in Collaboration \(BDDAC 2013 Part of The 2013 Int  Conf   on Collaboration Technologies and Systems CTS 2013 May 20 24, 2013, San Diego, California, USA  4  NIST Big Data Working Group NBD WG   O nline  Available  http://bigdatawg.nist.gov/home.php  5  Definting Big Data Architetcure Framework Outcome of the Brainstorming Session at the University of Amsterdam 17 July 2013 Present ed  at  NBD WG 24 July 2013  O nline  Available  http://bigdatawg.nist.gov/_uploadfiles/M0055_v1_7 606723276.pdf  6  Reflections on Big Data Data Science and Related Subjects Blog by Irving Wladawsky Berger  online  Available http://blog.irvingwb.com blog/2013/01/reflections on big data data science and related subjects.html  7  E Dumbill, What is big data? An introduction to the big data landscape  O nline  Available  http://strata.oreilly.com/2012/01/what is big data.html  8  The Big Data Long Tail  Blog post by Jason Bloomberg   Jan uary  17, 2013  O nline  Available  http://www.devx.com/blog/the big data lon g tail.html  111 


9  J  Gantz and David Reinsel   Extracting Value from Chaos  IDC IVIEW June 2011   O nline  Available  http://www.emc.com/collateral/analyst reports/idc extracting value from chaos ar.pdf  10  The Fourth Paradigm Data Intensive Scientific Discovery Edite d by Tony Hey Stewart Tansley and Kristin Tolle  Microsoft Corporation October 2009 ISBN 978 0 9825442 0 4  O nline  Available  http://research.microsoft.com/en us/collaboration/fourthparadigm  11  Big Data defintion Gartner Inc  O nline  Available  http://www.gartner.com/it glossary/big data  12  S.Sicular 000 Gartner's Big Data Definition Consists of Three Parts, Not to Be Confused with Three "V"s 000  Gartner, Inc. 27 March 2013   O nline  Available  http://www.forbes.com/sites/gartnergroup/2013/03/27 gartners big data definition consists of three parts not to be confused with three vs  13  J Layton  000 The Top of the Big Data Stack: Database Applications 000  July 27 2012  O nline  Available  http://www.enterprisestorageforum.com/storage management/the top of the big data stack database applications.html  14  Explore big data analytics and Hadoop  O nline  Available  http://www.ibm.com/developerworks/training/kp/os kp hadoop  15  A Bloom 7 Myths on Big Data Avoiding Bad Hadoop and Cloud Analytics Decisions April 22 2013  O nline  Available  htt p://blogs.vmware.com/vfabric/2013/04/myths about running hadoop in a virtualized environment.html  16  European Union. A Study on Authentication and Authorisation Platforms For Scientific Resources in Europe Brussels  European Commission 2012. Final Report Contributing author. Internal identification SMART Nr 2011/0056  O nline  Available  Available at http://cordis.europa.eu/fp7/ict/e infrastructure/docs/aaa study final report.pdf  17  Y Demchenko  P.Membrey C.Ngo C de Laat D.Gordijenko  Big Security for Big Data: A ddressing Security Challenges for the Big Data Infrastructure Proc  0006\000H\000F\000X\000U\000H\000\003\000'\000D\000W\000D\000\003\0000\000D\000Q\000D\000J\000H\000P\000H\000Q\000W\000\003\000\013\0006\000'\0000\000¶\000\024\000\026\000\014\000\003\000:\000R\000U\000N\000V\000K\000R\000S\000\021\000\003\0003\000D\000U\000W\000\003 of VLDB2013 conference, 26 30 August 213, Trento, Italy  18  NIST SP 500 292 Cloud Computing Reference Architecture v1.0  O nline  Available  http://colla borate.nist.gov/twiki cloud computing/pub/CloudComputing/ReferenceArchitectureTaxonomy/NIS T_SP_500 292_ _090611.pdf  19  Y Demchenko  M Makkes R.Strijkers C.Ngo C de Laat Intercloud Architecture Framework for Heterogeneous Multi Provider Cloud based Inf rastructure Services Provisioning, The International Journal of Next Generation Computing \(IJNGC\, Volume 4, Issue 2, July 2013  20  NIST Big Data Reference Architecture  NBD WG NIST   O nline  Available  http://bigdatawg.nist.gov/_uploadfiles/M0226_v10_1554566513.docx  21  NIST Big Data T echnology Roadmap  NBD WG  O nline  Available  http://bigdatawg.nist.gov/_uploadfiles/M0087_v8_1456721868.docx  22  Open Data Center Alliance Master Usage model Information as a Service Rev 1.0  O nline  Available  http://www.opendatacenteralliance.org/docs  Information_as_a_S ervice_Master_Usage_Model_Rev1.0.pdf  23  TR202 Big Data Analytics Reference Model  TMF Document Version 1.9, April 2013  24  IBM GBS Business Analytics and Optimisation 2011 IBM  2013   O nline  Available  https://www.ibm.com/developerworks  mydeveloperworks  files/basic/anonymous/api/library 48d92427 47d3 4e75 b54c b6acfbd608c0/document/aa78f77c 0d57 4f41 a923 50e5c6374b6d/media&ei=yrknUbjMNM_liwKQhoCQBQ&usg=AFQjC NF_Xu6aifcAhlF4266xXNhKfKaTLw&sig2=j8JiFV_md5DnzfQl0spVr g&bvm=bv.42768644,d.cGE  25  A.M Middleton  HPCC Systems Introduction to HPCC High Performance  Computer Cluster LexisNexis Risk Solutions LexiNexis  May 24, 2011  26  Bierauge M Keeping Up With Big Data American Library Association  2013   O nline  Available  http://www.ala.org/acrl/publications/keeping_up_with/big_data  27  Unstructured Data Mana gement Hitachi Data System  2013  online  http://www.hds.com/solutions/it strategies/unstructured data management.html  28  NIST Big Data WG discussion  O nline  Available  http://bigdatawg.nist.gov/home.php  29  D.Koopa, et al 000 A Provenance Based Infrastructure to Support the Life Cycle of  Executable Papers 000  in International Conference on Computational Science  ICCS 2011   O nline  Available  http://vgc.poly.edu/~juliana/pub/vistrails executable paper.pdf  30  Open Access: Opportunities and Challenges. European Commission for UNESCO  O nline  Available  http://ec.europa.eu/research/science society/document_library/pdf_06/open access handbook_en.pdf  31  OpenAIR 000  Open Access Infrastructure for Research in Europe  O nline  Available  http://www.openaire.eu  32  Open Researcher and Contributor ID online  h t t p    a b o u t  o r c i d  o rg  33  Roundup of Big Data Pundits' Predictions for 2013. Blog post by David Pittman January 18 2013   O nline  Available http://www.ibmbigdatahub.com/blog/roundup big data pundits predictions 2013  34  The Forrester Wave: Big Data Predictive Analytics Solutions, Q1 2013 Mike Gualtieri January 13 2013  O nline  Available  http://www.forrester.com/pimages/rws/reprints/document/85601/oid/1 LTEQDI  35  Big data: The next frontier for innovation, competition, and producti vity May 2011 McKinsey Global Institute  O nline  Available  http://www.mckinsey.com/insights/business_technology/big_data_the_n ext_frontier_for_innovation  36  Data Lifecycle Models and Concepts  O nline  Available  http://wgiss.ceos.org/dsig/whitepapers/Data%20Lifecycle%20Models 2 0and%20Concepts%20v8.docx  37  M Makkes  C  Ngo Y  Demchenko R  Strijkers R  Meijer C   de Laat 000 Defining Intercloud Federation Framework for Multi provider Cloud Services Integration 000  in The Fourth International Conference on Cloud Computing GRIDs and Virtua lization CLOUD COMPUTING 2013  May 27  June 1, 2013,Valencia, Spain  38  M Turk   A chart of the big data ecosystem take 2 online  http://mattturck.com/2012/10/15/a chart of the big data ecosystem take 2  39  Amazon Big Data  O nline Available   http://aws.amazon com/big data  40  Microsoft Azure Big Data Microsoft   2013  O nline  Available  http://www.windowsazure.com/en us/home/scenarios/big data  41  IBM Big Data Analytics  IBM 2 o 13   O nline  Available  http://www 01.ibm.com/software/data/infosphere/bigdata analytics.html  42  Cloudera Impala Big Data Platform   O nline  Available  http://www.cloudera.com/content/cloudera/en/home.html  43  10 hot big data startups to watch in 2013  10 January 2013  O nline  Available  http://beautifuldata.net/2013/01/10 hot big data startups to watch in 2013  44  P Membrey K  C.C. Chan, Y  Demchenko, A Disk B ased Stream Oriented Approach For Storing Big Data I n First International Symposium on Big Data and Data Analytics in Collaboration \(BDDAC 2013 Part of The 2013 International Conference on Collaboration Technologies and Systems \(CTS 2013\, May 20 24, 2013  San Diego, California, USA  45  Research Data Alliance  RDA  O nline  Available  http://rd alliance.org   112 


Size of Largeincrease The reasons are twofold Firstly when increases according to the node selection scheme to construct decrease There are two reasons Firstly when the memory size increases the stop condition for graph contraction is easier to be satis\336ed since more nodes can 336t in memory Secondly when the memory size increases the costs of the external sorts in both graph contraction and graph expansion phases decrease  Average Degree Number of Largeincreases the time and I/O consumptions for both increases the number of iterations in graph contraction increases This is because when number of edges increases the cost to sort and scan edges in each iteration increases thus more time and I/Os are consumed in each iteration                                     2 4 1 u E  V v E    u v  G  V E M V M V K G V V M KB Range 25M,50M,100M,150M,200M 2,3,4,5,6 200M,300M,400M,500M,600M 400K 8K 20,30,40,50,60 30,40,50,60,70 10K to in the operator in line 4 both in line 4 and augmented in all nodes in in line 5-7 VIII P ERFORMANCE S TUDIES In this section we conduct experimental studies by comparing four external algorithms for is the number of bytes to keep a node in memory We set the max time cost to be 24 hours If a test does not stop in the time limit we will denote it using until all nodes form an to Size of Small The results are shown in Fig 7\(a and Fig 7\(b for time and I/O costs respectively When the memory size increases the time and I/O costs for both 100M 400M 40 1 1 50 TABLE I R ANGE AND D EFAULT V ALUE FOR P ARAMETERS Parameter in all cases since more nodes/edges are removed in each iteration in Operator  operator speci\336es a unique total order among all nodes in the graph operator in line 9 when generating algorithm needs to hold and and and and and used in introduced in 26  w h i ch is cu r r e n tly th e m o s t I O ef 336cien t sem i e x t er n a l algorithm for and  Secondly when and out out out out out out out Fig.6\(a and Fig 6\(b show the time and I/O costs when varying the number of edges of WEBSPAM-UK2007 from 20 to 100 respectively v G v G v G v G v G v G v G v G v G v G v G v G v G v G 002 212 327 327  002 002 327 327 327 327 327       2 cannot stop in the time limit even if the graph contains only 20 of the edges When Size of 8 our e x t e rnal cont ract i on-e xpans i o n b as ed algorithm Algorithm 2 and our algorithm by applying the optimization techniques introduced in Section VII in new edges are added into In Algorithm 3 in order to make use of the new s The graphs contain nodes from 25M to 200M with average degree varying from 2 to 6 A synthetic graph is generated as follows We construct a graph computation namely the external contraction based 13 t he e x t e rnal DFS based by randomly selecting all nodes in SCC SCC SCC SCC SCC SCC  we apply the algorithm computation The  Finally additional random nodes and edges are added to the graph The parameters for synthetic datasets and their default values are shown in Table VIII outperforms 1PB 1PB since it cannot stop in all cases Memory Size need to be computed in Ext Ext Ext Ext EM Ext EM Ext Ext Ext Ext Ext Ext Ext Ext    3  002  For the semi-external algorithm  In our experiments we use a real large web graph and several synthetic datasets The real web graph is WEBSPAM-UK2007 4  which consists of 105,896,555 web4 barcelona.research.yahoo.net/webspam/datasets/uk2007/links   4H   8H   12H   16H   20H   24H   INF   20   40   60   80   100   Time\(hour Ext-SCC-Op   Ext-SCC                           DFS-SCC                 Largein in in in in in in 2 plus one disk block in the main memory that is Size of MassiveNumber of Massive\(a Time Vary Memory   1M   2M   3M   4M   5M   6M   7M   8M   INF   400M   600M   800M   1G   Number of I/Os Ext-SCC-Op   Ext-SCC                       DFS-SCC               iff one of the following three conditions holds 1 b I/Os Vary Memory Fig 7 WEBSPAM-UK2007 Varying Memory Size pages in 114,529 hosts in the UK domain The graph contains 105,895,908 nodes and 3,738,733,568 edges with the average degree 35 per node For synthetic data we generate 3 different kinds of datasets denoted Massive.The 4K,6K,8K,10K,12K 6K,8K,10K,12K,14K u w u G u G u G u G u G u G u G  For any  containing different sizes of and Small 327     V i i i i i d d d i i De\036nition 7.1 DFS Op Op Op Op Op Op 217 before adding  The default memory size is  In our experiments we do not show the results of  thus more iterations are needed according to the stop condition of graph contraction in 327 Semi add Datasets Exp-1 Performance on WEBSPAM-UK2007 in Algorithm 3 more nodes will be selected in id id 200K,300K,400K,500K,600K as follows DFS SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC  By considering  All the algorithms are implemented using Visual C 2005 and tested on a PC with Intel Core2 Quar 2.66GHz CPU and 3.5GB memory running Windows XP The disk block size is  according to Theorem 5.3 nodes with small degrees are removed when constructing 256 400   256 400 Default INF 327 deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg  4 w V V E V G V G   u  v V G G E E E E G where can be further reduced We rede\336ne the operator Number of SmallD M s s s i i i i i i i 1 1 1 1 1 1 u>v 4  Secondly using operator  and for each removed node  s 336rst Then we add edges among the nodes in an  We vary the memory size from                     a Time Vary Graph Size   1M   2M   3M   4M   5M   6M   7M   8M   INF   20   40   60   80   100   Number of I/Os Ext-SCC-Op   Ext-SCC                           DFS-SCC                 b I/Os Vary Graph Size Fig 6 WEBSPAM-UK2007 Varying Graph Size Percent   4H   8H   12H   16H   20H   24H   INF   400M   600M   800M   1G   Time\(hour Ext-SCC-Op   Ext-SCC                       DFS-SCC               


SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC increases the time and I/O costs for both dataset dataset The results for both Largedatasets are similar to those in the Massive When either the average   800 105 895 908 8  256  847 200 600 25 200 50 12 30 70 f I/Os SmallTo test the synthetic data we vary the memory size M   K  M M M M M M M M M M D D D D K s s s of nodes from 2 to 6 The time and I/O costs on Largedo not have signi\336cant impact on the ef\336ciency of our algorithms as long as by 20 on average for both time and I/O consumptions Fig 8\(c and Fig 8\(d show the results on Large 1 1 4  the costs for both d I/Os Vary Degree   c Time Vary Degree   25 327         Size and G M G M V V V V V V V K E G Wevary the node size decrease sharply The reason is that in order to process the graph using in all test cases to to is smaller the decrease rate is larger This is because when is smaller more iterations are needed for both to  and the time and I/O costs are shown in Fig 9\(a and Fig 9\(b respectively When to to respectively Fig 9\(g and Fig 9\(h show the time and I/O costs when varying the number of 4   s and and and and and and and and Wevary the average degree Size   Size   Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os 2H   3H   4H   5H   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 200K   400K   600K   800K   1M   1.2M   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 1H   2H   3H   4H   5H   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 200K   400K   600K   800K   1M   1.2M   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 1H   2H   3H   4H   5H   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 200K   400K   600K   800K   1M   1.2M   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 8H   12H   16H   20H   INF   25M   50M   100M   150M   200M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 1M   2M   3M   4M   5M   INF   25M   50M   100M   150M   200M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 2H   4H   6H   8H   10H   INF   2   3   4   5   6   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 400K   600K   800K   1M   1.2M   INF   2   3   4   5   6   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 2H   3H   4H   5H   INF   4K   6K   8K   10K   12K   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 400K   600K   800K   1M   INF   4K   6K   8K   10K   12K   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 2H   3H   4H   5H   INF   30   40   50   60   70   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 400K   600K   800K   1M   INF   30   40   50   60   70   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 e Time Small,andwhen increase This is because when n in Synthetic Data in Synthetic Data DFS DFS DFS Exp-5 Vary Op Op Op Op Op Op Op Op Op Op Op Op Ext  The time and I/O costs on Massiveare shown in Fig 9\(c and Fig 9\(d respectively When decrease When f I/Os Vary Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext  V  V Number in Synthetic Data c Time Large\(a Time Vary e Time Vary cannot stop in limited time in all cases Similar to the results on the real dataset in Fig 7 when dataset and Fig 8\(e and Fig 8\(f show the results on Smallconsume less than 1 hour increases the time and I/O consumptions for both consumes more than 20 hours while both are the number of nodes and the number of edges of the graph As a result the size of memory is needed thus when the memory size is dataset are shown in Fig 8\(a and Fig 8\(b respectively dataset and this is true for all the remaining test cases when varying other parameters in synthetic data In the following due to the lack of space we only show the test results on the Large and in the graph contraction phase the contraction rate decreases when the number of iterations increases since the graph becomes denser with larger number of iterations is larger and the cost on each iteration to scan and   Exp-4 Vary Average Degree in Synthetic Data from from increases the time and I/O consumptions for both and the number of outperforms outperforms cannot stop within the time limit when outperforms in all cases When the memory increases from increases the time and I/O costs for both When a Time Massive\(b I/Os Massiveis larger is larger the gap between is larger This is because when number of edges is larger more edges can be pruned by the edge reduction techniques used in increases the number of edges increases As a result more iterations are needed and larger cost is consumed in each iteration as analyzed in Exp-1 when varying the graph size size increases or the number of are not in\337uenced much As anal yzed in Section VII the key factors that in\337uence the cost of Num   Num Fig 9 Synthetic Data Largecan be directly applied on the original graph to output all                       Fig 8 Synthetic Data Vary Memory Size outperforms  and Smallincrease This is because the stop condition for graph contraction is harder to be satis\336ed when  sort nodes/edges is larger when   Fig 9\(e and Fig 9\(f show the time and I/O costs when varying the average no iteration is needed and size from SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC 1H   4H   200K   1H   200K   1H   200K   sfrom b I/Os Vary g Time Vary h I/Os Vary        218 d I/Os LargeSemi Semi Exp-2 Vary Memory Size Exp-3 Vary Node Size   


are 336xed This also explains why the results in the three datasets Massiveis a primitive operation in directed graph exploration which has been studied for both internal memory model and external memory model In the internal memory model strongly connected components of a directed graph can be computed in s for a directed graph with the assumption that the nodes of the graph cannot reside entirely in memory We overcome the de\336ciencies of the existing external    sort sort computation algorithms and propose a new two-phase algorithm with graph contraction followed by graph expansion We analyze the I/O cost of our approach and show that our algorithm can signi\336cantly reduce the number of random I/Os We propose techniques to further reduce the I/O cost of our algorithm and con\336rm the I/O ef\336ciency of our approaches using extensive experiments on both real and synthetic web scale graphs The work was supported by grant of the Research Grants Council of the Hong Kong SAR China No 418512 R EFERENCES  J  A bello A  L  Buchs baum  a nd J  W e s t brook A f unctional a pproach to external graph algorithms s of a graph Zhang et al 26 i mpro v e s uch a n a l gori t h m by constructing and maintaining a special in-memory spanning tree of the graph The semi-external algorithms 23 an d  2 6  are introduced in details in Section III Other than the problem of 336nding time based on DFS 12  A naive way to externalize the internal DFS algorithm requires s Such an algorithm may end up an in\336nite loop and cannot compute all  33\(2 2001  H  Y ildir im  V  C haoji and M  J  Z aki Grail Scalable reachability index for large graphs s repeatedly until the graph 336ts in memory then an internal memory algorithm is used to 336nd the 336nal sor DFS tree on external directed graphs several problems in the external memory model are studied in the literature Dementiev et al 14 p ro vi de an i m pl ement a t i o n o f a n e xt ernal m emory minimum spanning tree algorithm based on the ideas of 22 which performs extremely well in practice even though theoretically inferior to the algorithms of 1   1 0   A jw an i e t al 4 6  propos e i mpl e ment at i ons of e x t e rnal undi rect ed breadth-\336rst search algorithm with the idea from 18 Ul rich Meyer et al 20 21  19  des i gn and i mpl e ment pract i c al I/O-ef\336cient single source shortest paths algorithm on general undirected sparse graphs Surveys about designing I/O ef\336cient algorithms for massive graphs can be found at 24 5  X C ONCLUSIONS In this paper we study I/O ef\336cient algorithms to 336nd all  3\(1 2010  J  Hellings  G  H  F letcher  and H  H averkort Ef\336cient external-memory bisimulation on dags In  3\(1 2010  Z  Z h ang J  X Y u  L  Q in L  C hang a nd X L i n I/O e f 336 cient Computing sccs in massive graphs In scan by maintaining the list of nodes that should not be traversed using tournament trees 17 and b uf fered repos i t o ry t rees 8  respectively Despite their theoretical guarantees these algorithms are considered impractic al for general directed graphs that encountered in real applications Cosgaya-Lozano and Zeh 13 p res e nt a c ont ract i o n b as ed al gori t h m w hi ch cont ract s V M E B V B 2 are similar as stated in Exp-2 IX R ELATED W ORK Finding strongly connected components of a directed graph V G V G E G E V E E V E  14\(1 1985  T  H  Cor m en C  S tein R  L  R i v es t and C  E  L eis e r s on  2003  U Me yer a nd N Z e h I/O-ef 336c ient undirected shortest paths with unbounded edge lengths In s Both DFS based algorithm 8 and c ont ract i o n b as ed algorithm 13 a re i n t roduced i n det a i l s i n S e ct i o n III In addition to external algorithms there are semi-external algorithms for ACM Comput Surv Introduction to Algorithms IFIP TCS         Proc of ESA\22202 Proc of ESA\22206 SIAM J Comput Commun ACM Proc of SIGMOD\22213  32\(3 2002 2 A  A ggar w a l a nd J  S  V itter  T h e i nput/output com p le xity of s o r ting and related problems  31\(9 1988  A  V  A ho J  E  Hopcroft a nd J  D Ullm an I/Os Chiang et al 10 propos e a n a l gori t h m with I/O complexity  Addison-Wesley 1983 4 D  A jw ani R D e m e ntie v  and U  M e y er  A com putational s tudy of external-memory bfs algorithms In  2006  D  A jw ani a nd U Me yer   6\(1 2011  A  L  Buchs baum  M  H  G oldw a sser S Venkatasubramanian and J Westbrook On external memory graph traversal In  2002  J  S  V itter  E x ter n al m e m o r y algor ithm s and d ata s tr uctur e s   2007 7 E  A ngel R Cam p igotto a nd C L a f o r e s t  A nalys i s a nd com p ar is on of three algorithms for the vertex cover problem on large graphs with low memory capacities  1995  N  Chiba a nd T  N i s h izeki A r bor icity and s ubgr aph lis ting algor ithm s   2009  R Dem e ntie v  P  Sanders  D  S chultes  and J  F  S ibe y n E ngineering an external memory minimum spanning tree algorithm In  2012  V  K u m a r a nd E  J  Schw abe Im pro v e d a lgorithm s and d ata s tructures for solving graph problems in external memory In  2002  U  M e yer a nd V  O s ipo v  D es ign a nd im plem entation o f a pr actical i/o-ef\336cient shortest paths algorithm In  2009  U Me yer a nd N Z e h I/O-ef 336c ient undirected shortest paths In  2006  J  F  S i be yn E x ter n al connected com p onents  I n  2013 A CKNOWLEDGMENT  Algorithmic Operations Research          267        and computation which assume that all nodes of the graph can 336t in the main memory Sibeyn et al 23 propose a semi-external DFS  which can be used to 336nd all og  pages 457\320468 2012  Y  J  C hiang M T  Goodrich E  F  Gro v e  R  T am as s i a D E  V e ngrof f and J S Vitter External-memory graph algorithms In  Proc of ALENEX\22207 Proc of SIGMOD\22212 Proc of SODA\22295 Proc of SEA\22209 PVLDB Proc of ALENEX\22209 Proc of ESA\22203 PVLDB                    McGraw-Hill 2001  A  Cos g ayaL o zano a nd N  Z e h A h eur i s tic s t r o ng connecti vity algorithm for large graphs In Algorithmica LargeProc of SPAA\22202 G O O O O Algorithmics of Large and Complex Networks Data Structures and Algorithms SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC  Later Kumar and Schwabe 17 a nd B u chs b aum e t a l  8  i mprove the I/O complexity to  chapter 1 Design and Engineering of External Memory Traversal Algorithms for General Graphs Springer 2009  D  A jw ani U Me yer  and V  O s i po v  Im pro v e d e xternal m em ory b fs implementation In  2000  J  Cheng Y  K e  S  Chu and C Cheng E f 336 cient p roces s i ng of distance queries in large graphs a vertex cover approach In  2004  W  F a n J  L i  S  M a H W a ng and Y  W u Graph hom om orphis m revisited for graph matching  1996  K Mehlhorn a nd U Me yer  E xtern al-memory breadth-\336rst search with sublinear i/o In  2004  J  F  Sibe yn J  Abello a nd U Me ye r Heuristics for semi-external depth 336rst search on directed graphs In Proc of SWAT\22204  and SmallProc of SODA\22206 Proc of SODA\22200 219 Proc of SPDP\22296 Proc of SIGMOD\22212 


                  


             


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


