ONLINE DICTIONARY LEARNING OVER DISTRIBUTED MODELS Jianshu Chen Zaid J Towìc and Ali H Sayed Department of Electrical Engineering University of California Los Angeles ABSTRACT In this paper we consider learning dictionary models over a network of agents where each agent is only in charge of a portion of the dictionary elements This formulation is relevant in big data scenarios where multiple large dictionary models may be spread over different spatial locations and it is not feasible to aggregate all dictionaries in one location due to communication and privacy considerations We rst show that the dual function of the inference problem is an aggregation of individual cost functions associated with different agents which can then be minimized efìciently by means of diffusion strategies The collaborative inference step generates local error measures that are used by the agents to update their dictionaries without the need to share these dictionaries or even the coefìcient models for the training data This is a useful property that leads to an efìcient distributed procedure for learning dictionaries over large networks Index Terms  Dictionary learning distributed model diffusion strategies dual decomposition 1 INTRODUCTION AND RELATED WORK Dictionary learning is a useful procedure by which dependencies among input features can be represented in terms of suitable bases It has found applications in many machine learning and inference tasks including image denoising 1 dimensionality-reduction 3 4 biclustering feature-e xtraction and classiìcation 6 and no v e l document detection Dictionary learning usually alternates between two steps i an inference sparse coding step and ii a dictionary update step The rst step nds a sparse representation for the input data using the existing dictionary by solving an  1 regularized regression problem and the second step usually employs gradient descent to update the dictionary entries With the increasing complexity of various learning tasks it is natural that the size of the learning dictionaries is becoming increasingly demanding in terms of memory and computing requirements It is therefore important to study scenarios where the dictionary need not be available in a single location but is instead spread out over multiple locations This is particularly true in big data scenarios where multiple large dictionary models may be already available at separate locations and it is not feasible to aggregate all dictionaries in one location due to communication and privacy considerations This observation motivates us to examine how to learn a dictionary model that is stored over a network of agents where each agent is in charge of only a portion of the dictionary elements Compared with other works the problem we solve in this article is how to learn a distributed dictionary model which is for example different from the useful work in where it is assumed instead that each agent maintains the entire dictionary model  This work was supported in part by NSF grant CCF-1011918 Emails  cjs09 ztowìc sayed  ucla.edu In this paper we will rst formulate a modiìed version of the sparse coding problem where we add an additional  2 regularization term besides the  1 term also known as elastic net regularization This modiìed problem is not in a form that is directly amenable to a distributed implementation However we will show that the modiìed problem has a dual function that can be solved in a distributed manner using diffusion strategies Useful consensus strategies 14 can also be used Ho we v e r  since it has been noted that diffusion strategies have enhanced stability and learning abilities over consensus strategies we continue our presentation by focusing on diffusion strategies The inference algorithm that we develop is fully distributed in the sense that each agent only needs to apply a local gradient descent step followed by an information exchange step of the dual variable within its neighborhood We will show that this dual variable has a useful interpretation namely it corresponds to the representation error for the input data sample relative to all dictionary elements Therefore the agents do not need to share their private dictionary elements but only this representation error which is computed in a distributed manner through local interactions We test our algorithm on a typical image denoising task The dictionary is learned from a collection of patches arising from natural scenes and the learned dictionary is used to reconstruct a noisy image not included in the training set The denoised imageês peak-signal-to-noise-ratio PSNR is found to rival that of a centralized dictionary learning algorithm In other words our results show that the distributed solution does not limit performance On the contrary it can perform as well as a fully centralized solution This observation has useful ramiìcations for dealing with large dictionaries and large data sets 2 PROBLEM FORMULATION We seek to solve the following global dictionary learning problem over a network of N agents connected by a topology min W E  1 2  x t  W y o t  2 2    y o t  1   2  y o t  2 2  1 s  t   w k  2 2  1 k 1 N 2 where E x denotes the expectation operator x t is the M  1 input data vector at time t we use boldface letters to represent random quantities W is an M  N dictionary matrix w k is the k th column of W also known as the k th dictionary element or atom   and  are positive regularization factors for the  1 and  2 terms respectively and y o t is the solution to the following sparse coding problem for each input data sample x t at time t the regular font x t 2014 IEEE International Conference on Acoustic, Speech and Signal Processing \(ICASSP 978-1-4799-2893-4/14/$31.00 ©2014 IEEE 3874 


denotes a realization for x t  y o t  arg min y  1  2  x t  Wy  2 2    y  1   2  y  2 2      Q  W,y  x t  3 Note that dictionary learning consists of two steps the sparse coding step inference for the realization x t at each time t in 3 and the dictionary update step learning in 2 Let y k denote the k th entry of the N  1 vector y  Then the objective function of the inference step 3 can be written as Q  W y  x t   1 2     x t  N  k 1 w k y k     2 2  N  k 1 012   y k    2  y 2 k  4 The dictionary elements  w k  are linearly combined to represent each input data sample and the rst term in the cost function 4 requires the representation error to be small In this paper we focus on using quadratic costs to measure the representation error In we generalize the results to any differentiable strictly convex costs The second and third terms in 4 which correspond to the  1 and  2 regularizations in 3 are meant to ensure that the resulting combination coefìcients  y k  are sparse and small The  2 term makes the regularization strongly convex which will allow us to develop a fully decentralized strategy that enables the dictionary elements  w k  and the corresponding coefìcients  y k  to be stored and learned in a distributed manner over the network That is each agent k will infer its own y k and update its own dictionary element w k  by relying solely on limited interactions with its neighboring agents Furthermore as explained in such strongly con v e x re gularization terms help transform the non-differentiable primal cost 4 into a better-conditioned smooth optimization problem  see 16 further ahead Figure 1 shows the conìguration of the knowledge and data distribution over the network The dictionary elements  w k  can be interpreted as the wisdom that is distributed over the network and which we wish to combine in a distributed manner to form a greater intelligence for interpreting the data sample x t  By being distributed we would like the networked agents to nd the global solutions to both the inference problem 3 and the learning problem 2 with interactions that are limited to their neighborhoods Note that the problem we are solving in this paper is different from and the traditional distrib uted learning setting 9 10 12 18 where the entire set of model parameters the dictionary elements  w k  in this case are maintained at each agent in the network whereas the data samples are collected and processed over the network i.e these previous scenarios correspond to data distributed formulations What we are studying in this paper is to nd a distributed solution where each agent is only in charge of a portion of the model e.g w k for each agent k  This scenario corresponds to a model distributed formulation This case is important because each agent may be limited in its memory and computing power and may not be able to store large dictionaries By having many agents cooperate with each other a larger model that is beyond the ability of any single agent can be stored and analyzed in a distributed manner 3 LEARNING OVER DISTRIBUTED MODELS 3.1 Inference over distributed models Observe that solving the cost function 4 directly requires knowledge of all dictionary elements  w k  and coefìcients  y k  from the other agents due to the sum inside the  2 2 that runs from k 1 up   Fig 1  Each agent is in charge of one dictionary element w k  and the corresponding coefìcient y k  and the data sample x t at each time t is available to all agents in the network The results in this paper are generalized to the case where the data sample x t is only available to a subset of the agents and where each agent is responsible for a submatrix of W consisting of multiple columns and not only a single atom w k  see the extended work to N  Therefore this formulation is not directly amenable to a distributed solution However we can arrive at an efìcient distributed strategy by transforming the original optimization problem into a dual problem To begin with we rst transform the minimization of 4 into the following equivalent constrained optimization problem min  y k  z 1 2   x t  z   2 2  N  k 1 012   y k    2  y 2 k  5 s  t z  N  k 1 w k y k 6 Note that the above problem is convex over both  y k  and z since the objective is convex and the equality constraint is linear By strong duality 20 it follo ws that the optimal solution to 5 6 can be found by solving its corresponding dual problem and then recovering the optimal  y k  and z  To arrive at the dual problem we introduce the Lagrangian of 6 for each input realization x t as L   y k  z  x t   1 2   x t  z   2 2  N  k 1 012   y k    2  y 2 k    T 012 z  N  k 1 w k y k  7 where  y k  and z are the primal variables and  is the Lagrange multiplier also known as the dual variable The dual function g    x t  is deìned as the minimization of L   y k  z  x t  over the primal variables  y k  and z for each given   g    x t   min  y k  z L   y k  z  x t  8 Given that strong duality holds it is known that the optimal solution of 6 can be found by solving the following dual problem  o t  arg max  g    x t  9 and then recovering the optimal primal variables y o k,t and z o t via   y o k,t  z o t rgmin  y k  z L   y k  z o t  x t  10 3875 


Notice from 7 that the minimization in 10 over the variables  y k  and z for a given  is decoupled and the minimization over each y k is also decoupled for different k  Therefore the minimization over the primal variables can be done independently Computing the derivative of L   y k  z  x t  with respect to z and setting it to zero we obtain for each given   the optimal solution of z satisìes   x t  z   0  z  x t   11 Furthermore since L   y k  z  x t  is not differentiable in y k  the condition for minimizing L   y k  z  x t  with respect to y k is given by 21 0   y k L   y k  z  x t    y k     y k  y k   T w k 12 where  y k denotes the sub-differential the set of all subgradients with respect to y k  and the sub-differential for  y k  is  k  y k    sign y k  y k  0   1  1 y k 0 13 Applying an argument similar to the one used in to Eq 12 we can express the optimal y k as y k  T    015  T w k   14 where T     denotes the following soft-thresholding scalar-valued operator of x  R  T   x     x     sgn x  15 where  x   max  0 x   Observe that the solutions obtained in 11 and 14 are optimal for a given   Only when we have the optimal  o t to the dual problem 9 the corresponding z and y k acquired from 11 and 14 become the optimal solution to the original problem 6 the notation z o t and y o k,t will be used to represent the z and y k solutions corresponding to  o t  Substituting 11 and 14 into 7 we obtain the dual function as g    x t   1 2    2   T x t  N  k 1 S   015  T w k     N  k 1  1 2 N    2  1 N  T x t  S   015  T w k       J k    x t  16 where we introduced the following scalar-valued function of x  R  which is a differentiable convex function S    x     2 T 2    x        T    x        x T    x  17 The functions T   x  and S   x  are illustrated in Fig 2 Therefore the maximization of the dual problem 9 is equivalent to the following minimization problem min  N  k 1 J k    x t  18 Note that the new equivalent form 18 is an aggregation of individual costs associated with different agents each agent k is associated with cost J k    x t   which only requires knowledge of w k and x t       0  0 x Amplitude   T   x  S   x   Fig 2  Illustration of the functions T   x  and S   x   Therefore we can now directly apply the diffusion strategies developed in 11 to solv e the abo v e problem in a fully distrib uted manner over the network  k,i   k,i  1      J k   k,i  1  x t  19  k,i    N k a k   i 20 where  k,i denotes the estimate of the optimal  o t at each agent k at iteration i we will use i to denote the i th iteration of the inference and use t to denote the t th data sample  k,i is an intermediate variable   is the step-size parameter chosen to be a small positive number and a k is the combination coefìcient that agent k assigns to the information shared from agent  and it satisìes   N k a k 1 a k  0if  N k a k 0if  N k 21 Let A denote the matrix that collects a k as its   k  th entry Then it is shown in 11 that as long as the matrix A is primitive doubly-stochastic and the step-size is sufìciently small then the algorithm 20 converges to the optimal solution of 18 with a small bias on the order of O   2   in squared Euclidean norm Finally after  o t is estimated at each agent k  the optimal z and y k can be recovered from  by substituting  o t into 11 and 14 respectively z o t  x t   o t 22 y o k,t  T   015 w T k  o t   23 Note that 23 only requires local knowledge of w k  An important remark we have is a physical interpretation for the optimal dual variable  o t  Since z o t and y o k,t are the optimal solutions to problem 5 6 then z o t and y o k,t also need to satisfy constraint 6 so that z o t  N  k 1 w k y o k,t 24 Expressions 22 and 24 imply that  o t  x t  N  k 1 w k y o k,t 25 In other words  o t admits the interpretation of corresponding to the optimal prediction error of the input data sample x t using all the dictionary  w k   In this way the diffusion algorithm 20 is able to estimate the prediction error in a distributed manner for all agents 3876 


3.2 Distributed dictionary updates We now derive the strategy that updates the local dictionary element w k at each agent k  Speciìcally we need to solve the constrained stochastic optimization problem 2 which can be rewritten as min W E Q  W y o t  x t  26 s  t   w k  2  1 k 1 N 27 where y o t  col  y o 1 t  y o N,t  and Q  W y o t  x t  is deìned in 4 Our strategy is to apply stochastic gradient descent to the cost function 26 with respect to each w k followed by a projection onto the constraint set  w k   w k  1   The stochastic gradient of the cost function 26 with respect to w k is the gradient of Q  W y o t  x t  with respect to w k  Therefore the algorithm can be described as w k,t  B 012 w k,t  1   w  w k Q  W y o t  x t   28 where  B  x  is the projection operator onto  w k   w k  1   From 4 the stochastic gradient can be computed as  w k Q  W y o t  x t   015 x t  N  k 1 w k y o k,t  y o k,t 29 On the face of it expression 29 requires global knowledge of all dictionary elements  w k  across the network which would prevent the distributed implementation However recalling 25 the expression inside the parenthesis on the right-hand side of 29 is nothing but  o t  which is estimated locally by each agent by means of the distributed inference algorithm 20 Therefore the dictionary learning update 28 can be expressed as w k,t  B 012 w k,t  1   w   o t y o k,t  30 where each agent k replaces the above  o t by the estimate  k,i after a sufìcient number of inference iterations large enough i  The rightmost update term in 30 for dictionary element k is effectively the correlation between the global prediction error  o t  and the coefìcient y o k,t the activation 4 EXPERIMENT We consider learning a 100  196 dictionary W over a network of N  196 agents The network is generated according to a random graph where the probability that any agent is connected to another agent is 0  2  The network connectivity is checked by inspecting the algebraic connectivity of the graph Laplacian matrix and we will repeat this random graph generation until we nd a connected topology Each agent in the network is in charge of one dictionary element We extract a total of 1 million 10  10 patches from images 101 200 of the the non-calibrated natural image dataset Each image is originally 1536  1024 pixels in size but the border two pixels were discarded around each image and the top-left 1019  1019 pixels were then used for patch extraction With each data sample being a 10  10 patch from a certain image the dimension of the input data sample is M  100 vertically stacked columns In each experiment we randomly initialize each entry of the dictionary matrix W with a zero mean unit variance Gaussian random variable The columns are then scaled to guarantee that the sub-unit-norm constraint 2 is satisìed Furthermore in the combination step 20 of the distributed inference we use the Metropolis rule 9 24 Fig 3  Application of dictionary learning to image denoising a Original image b denoised image by using the centralized method from c dictionary obtained by the centralized method from 2 d image corrupted by additive white Gaussian noise e denoised image by our proposed distributed method at agent 1  f dictionary obtained by our proposed distributed method which is known to be doubly-stochastic The patch extraction preprocessing and image reconstruction code utilized excluding dictionary learning and patch inference steps is borrowed from For the dictionary learning we utilize  45   0  1  and   0  7  Computer code from the SPAMS toolbox was used to compare the algorithm from using its def ault parameters e xcept where otherwise stated A step-size of  w 5  10  5 was utilized for adapting the dictionary atoms The number of iterations for the diffusion algorithm to optimize 3 was chosen to be 300 iterations The data were presented in minibatches of size four samples/minibatch and the dictionary update gradients  o t y o k,t were averaged over the four samples at each step 1  In the far right of Fig 3 we show the dictionary learned over the 196 agents in the network bottom as well as the one learned by using the centralized method in as a benchmark top The learned dictionary can be used to denoise an image corrupted by noise as shown in the left four images of Fig 3 Observe that since the dictionaries were trained on patches arising from natural scenes these dictionaries are capable of denoising other natural scenes since they are expected to share the same statistics In denoising Fig 3 the step-size for our algorithmês inference was increased to be   1 to increase the quality of the inference result   o  The number of iterations of the inference step increased to 500 iterations to ensure convergence and  45 and  0  1 remained constant for all algorithms The corrupted imageês PSNR 2 is 14  056 dB while the PSNR for the recovered images using the centralized solution of and our proposed distributed solution were found to be 21  771 dB and 21  976 dB at agent 1  respectively Furthermore the average denoising PSNR performance across the distributed network was found to be 21  979 dB with a standard deviation of 0  00340 dB We observe that the performance is relatively uniform across the network  1 We perform the inference for four samples  x 1 x 4  at a time to obtain   o k 1  o k 4  all using the same dictionary W  Then we update W by averaging the gradient listed in 30 for those four samples 2 PSNR is the peak-signal-to-noise ratio deìned as PSNR  10 log 10  I 2 max  MSE  where I max is the maximum pixel intensity in the image and MSE is the mean-square-error over all image pixels 3877 


5 REFERENCES  M Elad and M Aharon Image denoising via sparse and redundant representations over learned dictionaries IEEE Trans Image Process  vol 15 no 12 pp 3736Ö3745 Dec 2006  J Mairal F  Bach J Ponce and G Sapiro Online learning for matrix factorization and sparse coding The Journal of Machine Learning Research  vol 11 pp 19Ö60 Mar 2010  H Zou T  Hastie and R T ibshirani Sparse principal component analysis Journal of Computational and Graphical Statistics  vol 15 no 2 pp 265Ö286 Jan 2006  H Shen and J Z Huang Sparse principal component analysis via regularized low rank matrix approximation Journal of Multivariate Analysis  vol 99 no 6 pp 1015Ö1034 Jul 2008  M Lee H Shen J Z Huang and J S Marron Biclustering via sparse singular value decomposition Biometrics  vol 66 no 4 pp 1087Ö1095 Dec 2010  J Mairal F  Bach J Ponce G Sapiro and A Zisserman Supervised dictionary learning in Proc NIPS  Lake Tahoe Nevada Dec 2008 pp 1033Ö1040  S P  Kasi visw anathan H W angy  A  Banerjee y  and P Melville Online  1 dictionary learning with application to novel document detection in Proc NIPS  Lake Tahoe Nevada Dec 2012 pp 2267Ö2275  P  Chainais and C Richard Learning a common dictionary over a sensor network in Proc IEEE CAMSAP  St Martin French West Indies Dec 2013  F  S Catti v elli and A H Sayed Dif fusion LMS strate gies for distributed estimation IEEE Trans Signal Process  vol 58 no 3 pp 1035Ö1048 Mar 2010  A H Sayed S.-Y  T u J Chen X Zhao and Z J T o wìc Diffusion strategies for adaptation and learning over networks IEEE Signal Process Mag  vol 30 no 3 pp 155Ö171 May 2013  J Chen and A H Sayed On the limiting beha vior of distributed optimization strategies in Proc Allerton Conf  Monticello IL Oct 2012 pp 1535Ö1542  J Chen and A H Sayed Distrib uted P areto optimization via diffusion adaptation IEEE J Sel Topics Signal Process  vol 7 no 2 pp 205Ö220 Apr 2013  J Chen and A H Sayed On the learning beha vior of adapti v e networks  Part I Transient analysis submitted for publication also available as arXi 2013  S Kar  J  M  F  Moura and K Ramanan Distrib uted parameter estimation in sensor networks Nonlinear observation models and imperfect communication IEEE Trans Inf Theory  vol 58 no 6 pp 3575Ö3605 Jun 2012  S Lee and A Nedic Distrib uted random projection algorithm for convex optimization IEEE Journal Sel Topics Signal Process  vol 7 no 2 pp 221Ö229 Apr 2013  S.-Y  T u and A H Sayed Dif fusion strate gies outperform consensus strategies for distributed estimation over adaptive networks IEEE Trans Signal Process  vol 60 no 12 pp 6217Ö6234 Dec 2012  J Chen Z J T o wìc and A H Sayed Dictionary learning over distributed models submitted for publication also available as arXiv Feb  2014  S Chouv ardas K Sla v akis and S Theodoridis  Adapti v e robust distributed learning in diffusion sensor networks IEEE Trans Signal Process  vol 59 no 10 pp 4692Ö4707 Oct 2011  P  Di Lorenzo S Barbarossa and A H Sayed Sparse distributed learning based on diffusion adaptation IEEE Trans Signal Process  vol 61 no 6 pp 1419Ö1433 Mar 2013  D P  Bertsekas Nonlinear Programming  Athena Scientiìc 2nd edition 1999  B Polyak Introduction to Optimization  Optimization Software NY 1987  A Beck and M T eboulle  A f ast iterati v e shrinkagethresholding algorithm for linear inverse problems SIAM Journal on Imaging Sciences  vol 2 no 1 pp 183Ö202 2009  J H v a n Hateren and A v a n der Schaaf Independent component lters of natural images compared with simple cells in primary visual cortex Proc Biological Sciences  vol 265 no 1394 pp 359Ö366 Mar 1998  A H Sayed Dif fusion adaptation o v e r netw orks  in Academic Press Library in Signal Processing vol 3 R Chellapa and S Theodoridis editors pp 323Ö454 Elsevier 2014 also available online as arXiv:1205.4220v2 May  X Zhao and A H Sayed Performance limits for distrib uted estimation over LMS adaptive networks IEEE Trans Signal Process  vol 60 no 10 pp 5107Ö5124 Oct 2012  G Pe yr  e The numerical tours of signal processing advanced computational signal and image processing IEEE Computing in Science and Engineering  vol 13 no 4 pp 94Ö97 Jul 2011  O Dek el R Gilad-Bachrach O Shamir  and L Xiao Optimal distributed online prediction using mini-batches The Journal of Machine Learning Research  vol 13 pp 165Ö202 Jan 2012 3878 


  6   4  F UTURE W ORK  While the health monitoring system has proven itself effective, the UASE team is continuing development of new features and improvements to this system  One major addition currently being created is a real time data analysis scheme to replace the current periodic analysis system.  This will allow for more constant data monitoring of individual systems which translates to the health monitoring system having a faster reaction time and even more efficiency than before.  This analysis will also keep a data ar chive of flight history to assist in identifying trends that could potentially cause problems in future flights  Another addition is of a battery level prediction analysis scheme  In an effort to improve the efficiency of the voltage monitoring portion o f the health monitoring system this analysis will predict how much longer a subsystem battery will last based on the rate of change of voltage over time.  This system will give the UAS operators ample time to anticipate how much longer a mission has to be  completed.  The data will also be used to tune the ABSAA payload system to conserve power if necessary to extend the life of a mission   In order to incorporate these updates, hardware upgrades are also being made  A new updated version of the Gumstix COM is currently being developed for implementation into the UAS for more computing capability for the continuing development of the health monitoring system  Other microcontroller options for the environmental sensor system are also being investigated to  further optimize the health monitoring system  5  C ONCLUSIONS   Unmanned aircraft can become compromised through a variety of failures within its subsystems  These failures include degradation of data degradation of system  performance or a hardware or s oftware issue  To address  this problem the UASE team at the University of North Dakota developed and tested a UAS health monitoring system to allow tracking and analysis of flight system data during flight operations and using this data to identify and handle an y problems that may arise  By monitoring the flight critical systems such as the GPS, autopilot, and ADS B as well as the payload environmental conditions and the integrity of the ABSAA system itself, the health monitoring system is able to acco mplish these tasks  The UASE team was able to demonstrate the ability of the health monitoring system in finding and handling a variety of problems that can affect the integrity of a UAS and the ABSAA system through rigorous testing using SWIL simulation t o replicate data degradation flight critical system failures, and deteriorating environmental conditions In each case, the health monitoring system was able to react properly and efficiently and find and execute the best course of action based on the pr oblem  A CKNOWLEDGEMENTS  This research was supported by the North Dakota Experimental Program to Stimulate Competitive Research ND EPSCoR/National Science Foundatio n grant  UND0018589 Rockwell Collins Charitable Corporation under grant  UND0017909, Depar tment of Defense contract number FA4861 06 C  Remote Sense and Avoid System and Advanced Payload  Laboratory contract number FA8650 09  Antenna for Wireless Communic ations Supporting Unmanned Aerial Vehicles in the Battlefield," and the North   authors would like to also acknowledge the contributions of the Unmanned Aircra ft Systems Laboratory team at UND   R EFERENCES  1  E  W    Fre w T  X  B r o w n   Net w o r k in g  I s s u e s  f o r  S m all Unmanned Aircraft Systems. Journal of Intelligent and Robotic Systems, Volume 54, Nos. 1 3, 21 37. Springer 2008   2  E    So f g e  Ho u s to n  co p s  te s t d o r n e  now in Iraq http://www.popularmechanics.com/science/air_space/4 234272.html \(2008   3  T    Z aj k o w s k i S Du n ag a n J  E iler s   Sm a ll U A S Communications mission. In Eleventh Biennial USDA Forest Service Remote Sensing Applications Conference, Salt Lake City, UT \(2006    4  C  E    C o r r ig an   et al   C ap tu r in g  v er tica l p r o f iles  o f  aerosols and black carbon over the Indian Ocean using autonomous unmanned aerial vehicles. Atmos. Chem Phys. Discus 7 11,428   11,463 \(2007    5   J  A    C u r r y J    Ma s la n ik   G   Ho llan d   J    P in to  Applications of aerosondes in the arctic. Bull. Am Meteorol. Soc. 85\(12\, 1855 1861 \(2004     6   NOAA: NOAA and Partners conduct first successful unmanned aircraft hurrican e observation by flying through Ophelia http://www.noaanews.noaa.gov  stories  2005/s 2508.htm \(2005    7  B    W ag n er   C iv ilia n  m ar k et f o r  u n m a n n ed  air cr af t struggles to take flight. In: National Defense Magazine 2007    


  7   8  Kyle Foerster, Brandon Wh itney , Alex Westhoff Naima Kaabouch , and William Semke, "A Health Monitoring System For UAS Utilizing a Miniature Airborne Sense and Avoid System," AIAA Infotech@Aerospace, August 2013   9   Michael Mullins, Michael Hollman, Kyle Foerster Naima Kaabo uch, and William Semke, "Dynamic Separation Thresholds for a Small Airborne Sense and Avoid System" AIAA Infotech@Aerospace, August 2013   10   Foerster, K., Mullins, M., Kaabouch, N., Semke W 2012\ Flight Testing of a Right of Way Compliant ADS B based Miniature Sense and Avoid System Proceedings of AIAA Infotech@Aerospace   11   Mullins, M., Foerster, K., Kaabouch, N., Semke, W 2012\. A Multiple Objective and Behavior Solution for Un manned Airborne Sense and Avoid Systems AUVSI's Unmanned Systems North America 2012   12  Kyle Foerster, Michael Mullins, Naima Kaabouch, and William Semke, "Incorporating Terrain Avoidance into a Small UAS Sense and Avoid System Infotech@Aerospace Proc eedings, AIAA, June 2012   13  Michael Mullins, Kyle Foerster, Naima Kaabouch, and William Semke, "Incorporating Terrain Avoidance into a Small UAS Sense and Avoid System" Accepted for publication and presentation at the Infotech@Aerospace Conference, AIA A, June 2012   14  Florent Martel, Michael Mullins, William Semke, and Naima Kaabouch, "Cooperative Miniature Collision Avoidance System Flight Testing For Small Unmanned Aircraft Systems" Proceedings of AUVSI conference August 2011   15  Florent Martel Michael Mullins, Naima Kaabouch William Semke, "Flight Testing of an ADS B based Miniature 4D Sense and Avoid System for Small UAS AIAA Proceedings, Infotech@Aerospace 2011 Conference AIAA, March 2011      16     Broadcast  ADS B\ Out Performance Requirements To Support  Air Traffic Control \(ATC Service   Docket No. FAA 2007 29305, May 28, 2010      B IOGRAPHY  Jason Hahn  is an un der graduate  student at the University of North Dakota currently pursuing a degree in Electrical Engineer ing with a focus on Computer Science  He has  May 2013  His research interests include unmanned a erial  systems development and avionics software  Naima Kaabouch  is currently an associate professor in the Electrical Engineering Department at the Universit y of North Dakota She got her B.S M.S and Ph.D degrees in electrical engineering from the University of Paris 11 and the University of  Paris 6  respectively  Her research interests include signal/image processing intelligent communications and sensing as well as their applications in aerospace and biomedical engineering fields  Kyle Foerster  is a graduate student in Electrical Engineering at the University of North Dakota from which he also received bachelor degrees in Electrical Engineering a nd Computer Science in December of 2010 He has three years of experience with unmanned aircraft system development His research interests include unmanned aircraft systems development and simulation and high performance computing  


6.1.3 The Other Benchmarks Setup For SPEC CPU2006  we run the of\002cial applications with the 002rst reference input and report the average results into two groups integer benchmarks  SPECINT  and 003oating point benchmarks  SPECFP  HPCC is a representative HPC benchmark suite and we run HPCC with version 1.4 We run all seven benchmarks including HPL  STREAM  PTRANS  RandomAccess  DGEMM  FFT  and COMM  PARSEC is a benchmark suite composed of multi-threaded programs and we deploy PARSEC 3.0 Beta Release We run all 12 benchmarks with native input data sets and use gcc with version 4.1.2 to compile them 6.2 The Implication of Data Volume for Architecture Research Intuitively data input should be highly relevant to big data workloads characterization Speci\002cally the size of data input should be relevant to micro-architectural characteristics In this subsection we pay attention to an important issue\227 what amount of data quali\002es for being called big data from a perspective of workload characterization  This issue is very important and interesting because simulation is the basic approach for architecture research but it is very time-consuming Bigger input data size would signi\002cantly increase the run time of a program especially on the simulation platform If there is no obvious difference between large and small data inputs in terms of micro-architectural events the simulation-based approaches using small data sets can still be valid for architecture research As different workloads have different input data types and sizes we set the minimum data scales in Table 6 as the baseline data inputs e.g 32 GB for Sort  1000000 pages for PageRank  and 100 requests per second for Nutch Server  On the baseline data input we scale up the data size by 4 8 16 and 32 times respectively There are hundreds of micro-architectural events in modern processors For better readability and data presentation we only report the numbers of MIPS and L3 cache MPKI Figure 3-1 demonstrates MIPS numbers of each workload with different data scales From Figure 3-1 we 002nd that for different workloads the instruction executing behaviors exhibit different trends as the data volume increases For example MIPS numbers of Grep and WordCount increase after the 16 times baseline while for some other workloads they tend to be stable after the data volume increases to certain thresholds The cache behavior metrics also exhibit a similar phenomenon as the MIPS metric does As important as L3 cache misses are\226a single one can cause hundreds of cycles of latency\226we track this value for different workloads under different con\002gurations In Figure 2 for a workload we call the data input on which the system achieves the best performance as the large input for a workload and the baseline as the small input  From Figure 2 we can see Figure 2 L3 cache MPKI of different con\002gurations in big data workloads that some workloads have lower number of L3 cache MPKI on the large con\002guration e.g Sort  while some have higher number of L3 cache MPKI on the large con\002guration e.g Grep  There are the other workloads showing no obvious difference under the two con\002gurations e.g Index  K-means has the largest difference under the two con\002gurations and the number of L3 cache MPKI is 0.8 and 2 for the small and large data inputs respectively which shows different data inputs can result in signi\002cantly different cache performance evaluation results To help understand the micro-architecture events we also stress test the cluster system with increasing data scale and report the user-perceivable performance number as mentioned in Section 6.1.2 Because workloads in Table 6 have different metrics we set the performance number from the experiments with the baseline data inputs as the baseline and then normalize the collected results for each workload with varying data inputs over the baseline number For example the performance number of Workload A for the baseline input is x and that for 4 002 baseline input is y  and then for Workload A we normalize the performance number for the baseline input and 4 002 baseline input as one and  y 004 x  respectively Figure 3-2 reports the normalized performance numbers of each BigDataBench workload with different data volumes Please note that the performance of Sort degrades with increased data size in Figure 32 because Sort is an I/O intensive workload when the memory cannot hold all its input data Besides the larger data sizes demand more I/O operations like shuf\003ing and disk accesses Worse still the network communication involved in data shuf\003ing causes congestion thus impairing performance  Lessons Learned  As the abo v e 002gures sho w  we 002nd that different big data workloads have different performance trends as the data scale increases This is the reason we believe that the workloads that only cover a speci\002c application scenario are not suf\002cient to evaluate big data systems and architecture Second architectural metrics are closely related to input data volumes and vary for different work 


Figure 3 Performance data vary with different data input sizes loads and data volume has non-negligible impact on workload characterization For example the MIPS number of Grep has a 2.9 times gap between the baseline and 32X data volume the L3 cache MPKI of K-means has a 2.5 times gap between the baseline and 32X data volume This result implies that using only simple applications with small data sets is not suf\002cient for big data systems and architecture research which may impose great challenges 6.3 Workload Characterization This section mainly focuses on characterizing operation intensity and cache behaviors of big data workloads Figure 4 Instruction Breakdown 6.3.1 Measuring operation intensity In order to characterize instruction behaviors 002rst we breakdown the execution instructions As shown in Figure 4 big data workloads have the distinct feature that the ratio of integer instructions to 003oating-point instructions is very high On Intel Xeon E5645 the average ratio is 75 The maximum is 179  Grep  and the minimum is 10  Bayes  For comparison these ratios for PARSEC  HPCC and SPECFP are very low on the average 1.4 1.0 and 0.67 respectively The ratio for SPECINT on the average 409 is the highest because it is intended to evaluate integer operations of processors From this perspective we can conclude that the big data workloads signi\002cantly differ from the traditional benchmarks like HPCC  PARSEC  and SPECCFP  Please note that the reported numbers may deviate across different processors  For example Intel processors uses different generations of SSE Streaming SIMD Extensions which introduces both scalar and packed 003oating point instructions Furthermore for each workload we calculate the ratio of computation to memory access to measure the operation intensity Floating point or integer operation intensity is de\002ned as the total number of 003oating point or integer instructions divided by the total number of memory accesses in terms of bytes in a run of the workload F or e xample in a run of program A  it has n 003oating point instructions and m bytes of memory accesses so the operation intensi 


ty of program A is  n 004 m  Since the memory hierarchy would impact the memory access performance signi\002cantly for comparison we report experiments on two state-ofpractice processors the Xeon E5310 and the Xeon E5460 respectively The Xeon E5310 is equipped with only two levels of caches while the Xeon E5645 is equipped with three levels of caches The con\002guration of the Xeon E5310 is shown in Table 7 Table 7 Con\002guration details of Xeon E5310 CPU Type Intel CPU Core Intel R 015 Xeon E5310 4 cores@1.60G L1 DCache L1 ICache L2 Cache L3 Cache 4 002 32 KB 4 002 32 KB 2 002 4MB None In Figure 5-1 we can see that big data workloads have very low 003oating point operation intensities and the average number of BigDataBench is 0.007 on the Xeon E5310 and 0.05 on the Xeon E5645 respectively However the number of PARSEC  HPCC  and SPCECFP is higher as 1.1 0.37 0.34 on the Xeon E5310 and 1.2 3.3 1.4 on the Xeon E5645 respectively SPECINT is an exception with the number closing to 0 On the average HPCC and PARSEC have high operation intensity because of their computingintensive kernels and SPECFP has relatively high operation intensity for it is oriented for 003oating point operations In summary the 003oating point operation intensity of BigDataBench is two orders of magnitude lower than in the traditional workloads on the Xeon E5310 and Xeon E5645 respectively The reason the 003oating point operation intensity of BigDataBench on E5645 is higher than on E5310 can be partly explained by the fact that L3 caches are effective in decreasing the memory access traf\002c which will be further analyzed in next subsection Though the average ratio of integer instructions to 003oating-point ones of big data workloads is about two orders of magnitude larger than in the other benchmarks the average integer operation intensity of big data workloads is in the same order of magnitude like those of the other benchmarks As shown in Figure 5-2 the average integer operation intensity of BigDataBench  PARSEC  HPCC  SPECFP and SPECINT is 0.5 1.5 0.38 0.23 0.46 on the Xeon E5310 and 1.8 1.4 1.1 0.2 2.4 on the Xeon E5645 respectively  Lessons Learned  In conclusion we can say that in comparison with the traditional benchmarks the big data workloads in BigDataBench have low ratios of computation to memory accesses The above phenomenon can be explained from two aspects First big data processing heavily relies upon memory accesses Second big data workloads must process large volume of data and hence most big data workloads adopt simple algorithms with low computing complexity In BigDataBench  they range from Figure 5 Operation Intensity on Intel Xeon E5310 and E5645 O\(n to O\(n*lgn  In comparison most HPCC or PARSEC workloads have higher computing complexity ranging from O\(n*lgn to O n 3   We can make the conclusion that big data workloads have higher demand for data movements than instruction executions The state-of-practice processor is not ef\002cient for big data workloads Rather we believe that for these workloads the 003oating-point unit is overprovisioned 6.3.2 Memory Hierarchy Analysis Finally we show the operation intensity of the big data workloads is low and we want to further investigate their cache behaviors In this subsection we report L3 cache MPKI L2 cache MPKI L1 instruction MPKI instruction TLB MPKI and data TLB MPKI respectively We leave out L1D cache MPKI because its miss penalty can be hidden by the out-of-order pipeline From Figure 6 we observe that the cache behaviors of BigDataBench have three signi\002cant differences from the traditional benchmarks as follows First the average L1I cache MPKI of BigDataBench is at least four times higher than in the traditional benchmarks The average L1I cache MPKI of BigDataBench is 23 while that of HPCC  PARSEC  SPECFP  and SPECINT is 0.3 2.9 3.1 and 5.4 respectively This observation corroborates the ones in CloudSuite and DCBench The possible main factors leading to the high L1I cache MPKI are the huge code size and deep software stack of the big data workloads Second the average L2 cache MPKI of Big 


Figure 6 Memory hierarchy behaviors among different workloads DataBench is higher than in the traditional workloads The average L2 cache MPKI of BigDataBench is 21 while that for HPCC  PARSEC  SPECFP  and SPECINT is 4.8 5.1 14 and 16 respectively Among BigDataBench  most of the online service workloads have the higher L2 cache MPKI on the average 40 except Nutch server 4.1 while most of the of\003ine and realtime analytics workloads have the lower L2 cache MPKI on the average 13 except BFS 56 Third the average L3 cache MPKI of BigDataBench is 1.5 while the average number of HPCC  PARSEC  SPECFP  and SPECINT is 2.4 2.3 1.4 and 1.9 respectively This observation shows that the LLC L3 caches of the processors Xeon E5645 on our testbed are ef\002cient for the big data workloads corroborating the observation in DCBench  The ef\002ciency of L3 caches also can explain why the 003oating point intensity of BigDataBench on the Xeon E5645 is higher than on the Xeon E5310 only two levels of caches The TLB behaviors are shown in Figure 6-2 First the average number of ITLB MPKI of BigDataBench is higher than in the traditional workloads The average number of ITLB MPKI of BigDataBench is 0.54 while that of HPCC  PARSEC  SPECFP  and SPECINT is 0.006 0.005 0.06 and 0.08 respectively The more ITLB MPKI of BigDataBench may be caused by the complex third party libraries and deep software stacks of the big data workloads Second the average number of DTLB MPKI of BigDataBench is also higher than in the traditional workloads The average number of DTLB MPKI of BigDataBench is 2.5 while that of HPCC  PARSEC  SPECFP  and SPECINT is 1.2 0.7 2 and 2.1 respectively And we can also 002nd that the numbers of DTLB MPKI of the big data workloads range from 0.2  Nutch server  to 14  BFS  The diversity of DTLB behaviors re\003ects the data access patterns are diverse in big data workloads which proves that diverse workloads should be included in big data benchmarks  Lessons Learned  On a typical state-of-practice processor Intel Xeon E5645 we 002nd that L3 caches of the processor are ef\002cient for the big data workloads which indicates multi-core CPU design should pay more attention to area and energy ef\002ciency of caches for the big data applications The high number of L1I cache MPKI implies that better L1I cache performance is demanded for the big data workloads We conjecture that the deep software stacks of the big data workloads are the root causes of high frond-end stalls We are planning further investigation into this phenomenon by changing the software stacks under test e.g replacing MapReduce with MPI 7 Conclusion In this paper we presented our joint research efforts with several industrial partners on big data benchmarking Our methodology is from real systems covering not only broad application scenarios but also diverse and representative real-world data sets We proposed an innovative data generation methodology and tool to generate scalable volumes of big data keeping the 4 V properties Last we chose and developed nineteen big data benchmarks from dimensions of application scenarios operations algorithms data types data sources software stacks and application types Also we reported the workloads characterization results of big data as follows 002rst in comparison with the traditional benchmarks the big data workloads have very low 


operation intensity Second the volume of data input has non-negligible impact on micro-architecture characteristics of big data workloads so architecture research using only simple applications and small data sets is not suf\002cient for big data scenarios Last but not least on a typical state-ofpractice processor Intel Xeon E5645 we 002nd that for the big data workloads the LLC of the processor is effective and better L1I cache performance is demanded as the big data workloads suffer high L1I cache MPKI 8 Acknowledgements We are very grateful to the anonymous reviewers and our shepherd Dr Michael D Powell This work is supported by Chinese 973 projects Grants No 2011CB302502 and 2014CB340402 NSFC projects Grants No 60933003 and 61202075 and the BNSF project Grant No.4133081 References  http://www-01.ibm.com/software/data/bigdata  http://hadoop.apache.org/mapreduce/docs/current/gridmix.html  Alexa website http://www.alexa.com/topsites/global  Amazon movie reviews http://snap.stanford.edu/data/webAmazon.html  Amp lab big data benchmark https://amplab.cs.berkeley.edu/benchmark  Bigdatabench http://prof.ict.ac.cn/BigDataBench  facebook graph http://snap.stanford.edu/data/egonetsFacebook.html  google web graph http://snap.stanford.edu/data/webGoogle.html  ibmbigdatahub http://www.ibmbigdatahub.com/infographic/fourvs-big-data  Sort benchmark home page http://sortbenchmark.org  wikipedia http://en.wikipedia.org  T G Armstrong V Ponnekanti D Borthakur and M Callaghan Linkbench a database benchmark based on the facebook social graph 2013  L A Barroso and U H 250 olzle The datacenter as a computer An introduction to the design of warehouse-scale machines Synthesis Lectures on Computer Architecture  4\(1 2009  M Beyer Gartner says solving big data challenge involves more than just managing volumes of data http://www.gartner.com/it/page.jsp?id=1731916  B F Cooper A Silberstein E Tam R Ramakrishnan and R Sears Benchmarking cloud serving systems with ycsb In Proceedings of the 1st ACM symposium on Cloud computing  SoCC 10 pages 143\226154 2010  C Engle A Lupher R Xin M Zaharia M J Franklin S Shenker and I Stoica Shark fast data analysis using coarse-grained distributed memory In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data  pages 689\226692 ACM 2012  M Ferdman A Adileh O Kocberber S Volos M Alisafaee D Jevdjic C Kaynak A D Popescu A Ailamaki and B Falsa\002 Clearing the clouds A study of emerging workloads on modern hardware Architectural Support for Programming Languages and Operating Systems  2012  W Gao Y Zhu Z Jia C Luo L Wang J Zhan Y He S Gong X Li S Zhang and B Qiu Bigdatabench a big data benchmark suite from web search engines The Third Workshop on Architectures and Systems for Big Data ASBD 2013 in conjunction with ISCA 2013   A Ghazal M Hu T Rabl F Raab M Poess A Crolotte and H.-A Jacobsen Bigbench Towards an industry standard benchmark for big data analytics In SIGMOD 2013  2013  S Huang J Huang J Dai T Xie and B Huang The hibench benchmark suite Characterization of the mapreducebased data analysis In Data Engineering Workshops ICDEW 2010 IEEE 26th International Conference on  pages 41\22651 IEEE 2010  Z Jia L Wang J Zhan L Zhang and C Luo Characterizing data analysis workloads in data centers In Workload Characterization IISWC 2013 IEEE International Symposium on  IEEE  C Luo J Zhan Z Jia L Wang G Lu L Zhang C Xu and N Sun Cloudrank-d benchmarking and ranking cloud computing systems for data processing applications Frontiers of Computer Science  6\(4 2012  Z Ming C Luo W Gao R Han Q Yang L Wang and J Zhan Bdgs A scalable big data generator suite in big data benchmarking In Arxiv  2014  A Pavlo E Paulson A Rasin D J Abadi D J DeWitt S Madden and M Stonebraker A comparison of approaches to large-scale data analysis In Proceedings of the 2009 ACM SIGMOD International Conference on Management of data  SIGMOD 09 pages 165\226178 2009  M Seltzer D Krinsky K Smith and X Zhang The case for application-speci\002c benchmarking In Hot Topics in Operating Systems 1999 Proceedings of the Seventh Workshop on  pages 102\226107 IEEE 1999  Y Tay Data generation for application-speci\002c benchmarking VLDB Challenges and Visions  2011  P Wang D Meng J Han J Zhan B Tu X Shi and L Wan Transformer a new paradigm for building dataparallel programming models Micro IEEE  30\(4 2010  S Williams A Waterman and D Patterson Roo\003ine an insightful visual performance model for multicore architectures Communications of the ACM  52\(4 2009  H Xi J Zhan Z Jia X Hong L Wang L Zhang N Sun and G Lu Characterization of real workloads of web search engines In Workload Characterization IISWC 2011 IEEE International Symposium on  volume 11 pages 15\22625 IEEE 2011  M Zaharia M Chowdhury T Das A Dave J Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation  pages 2\2262 USENIX Association 2012  J Zhan L Zhang N Sun L Wang Z Jia and C Luo High volume computing Identifying and characterizing throughput oriented workloads in data centers In Parallel and Distributed Processing Symposium Workshops  PhD Forum IPDPSW 2012 IEEE 26th International  pages 1712\226 1721 IEEE 2012 


Triggering Creativity through Semantic Cross-domain Web Crawling and Routing Francesco Taglino, Fabrizio Smith Institute for System Analysis and Co mputer Science, CNR, Rome, Italy  Integration of Emerging Learning Technologies in Secondary Schools: A Burkina Faso Case Study Romaric R. Zongo University of Minnesota ñ Twin Cities, Minnesota, USA  Visualization of Energy Consumption: Motivating fo r a Sustainable Behaviour Through Social Media Caroline So  e Olsen stfold University College, Halden, Norway   INDUSTRY POSTERS  Annotation and Deep Search of Se mi-Structured Technical Documents Chris Macks Progeny Systems Corporation, Virginia, USA   


  Paper 2581 Version 2  14   Flight i nformation and processing   Presentation and projection of flight profiles   Real time operational information   Managing both pilot operator and air traffic controller workloads   Procedural processes supported by enhanced automation capabilities   Control sector complexity and management   Separation a ssurance   Airspace d esign and p rocedures   Investment in resolving these constraints  will drive the rate of adoption in civil UAS operations in the NAS One p art of the investment will be driven by the FAA\222s \223F acilities and Equipment\224 F&E Budget over  the next ten years Another part will be driven by defense and federal investment  in R&D  The remaining part will be driven by private investment seeking to develop profitable supplie r s and manufacturing firms of UAS Unfortunately   private investment will not fully launch until there is a clear indication of the public investment into implementing the requisite air traffic management  and air traffic control  ATM/ATC changes into the NAS infrastructure and there i s a common  approach through avionic standards in UA V  equipage   It is expected that avionic standards would lower the end user costs The impact on NAS infrastructure is the outline in the following subsection  Today\222s Air Traffic Control System  Today\222s air traffic control system is built around four basic pillars. These are communications, navigation, surveillance and infrastructure. The mortar holding these pillars together is aviation policy, rules and procedures  The FAA Air Traffic Organizat ion \(ATO\ has identified over 25 FAA procedural orders and notices that will need to be revised to accommodate routine  UAS operations in the NAS. This does not include advisory and informational materials that need to be developed and briefed to air traffi c controllers, pilots and other aviation stakeholders Additionally NAS investments to accommodate UAS efforts and associated costs must extend  beyond training requirements and procedural developments to include modification of infrastructure to permit an d to optimize efficiency and safety of UAS operations in the NAS  The majority of UAS operating in the NAS today are predominantly operated by the DoD. They were not designed with NAS compatibility in mind but rather to meet military mission needs. It is e xpected future commercial UAS will be designed and operated much more along the lines of manned aircraft. For example, current DoD UAS do not have Flight Management Systems and associated NAS aeronautical database and cannot be program m ed to fly approved F AA instrument procedures such as an instrument landing system \(ILS\ approach. Future commercial UAS operating in the NAS will need have integrated avionics systems and NAS capabilities similar to those of manned aircraft. However, unique challenges associa ted with UAS will remain and drive functional changes across many NAS project and programs. Some of these anticipated drivers or changes are listed by FAA program area below  En Route Automation Modernization \(ERAM 227 ERAM will be impacted by the need to be able to accept and process and display lengthy and complex UAS flight plans that include automated flight profile contingencies to the air traffic controller. This includes the ability to display a 20 minute planned route of flight on a controller\222s displa y suite  ERAM needs to be able to display to review to modify and to approve a UAS flight plan request originating in an en route center airspace environment as a 24 hour file and fly replacement to the current constrained  COA  process now is in use  Nati onal Voice System \(NVS  227 Future air traffic control voice switching systems must be able to provide timely and direct pilot to controller voice and data e.g text messaging\ communications via Voice Over Internet Protocol \(V o IP  Terminal Automation Mode rnization and Replacement TAMR  227 TAMR needs to be able to display to review to modify and to approve a UAS flight plan request originating in a terminal airspace environment as a 24 hour file and fly replacement to the current constrained COA  process now is use. TAMR also needs to be able to surveil approved UAS flights within the terminal environment  Terminal Information Display System \(IDS\ and En Route Information Display System \(ERIDS  227 Information support systems need to be able to inges t to and  to rapidly display unique local procedures and associated graphics associated with UAS operations. These include pilot communications and UAV control lost ink procedures  System Wide Information Management \(SWIM  227 SWIM  must be able to rapidly mov e critical UAS information around the NAS to provide the right information to the right controller or pilot at the right time and in the desired  format  Flight Information Exchange Model \(FIXM  227 UAS flight information needs to move to and from other Air Navigation Service Providers \(ANSP\ throughout the world and new data terminology must be created that supports the collection, processing and dissemination of UAS movements through the airspace  Air Traffic Control Optimum Training Solutions \(ATCOTS  227 UAS provides new training challenges for FAA in creating new teaching curricula that contains new procedures and paradigms used to accommodate UAS safely and efficiently in the NAS. This is notable c hallenge given 


  Paper 2581 Version 2  15  the diversity and dynamic of evolving UAS technologies and missions  Future Facilities 227 FAA real estate assets may likely be affected as new UAS related systems are development and deployed. UAS NAS integration may drive additional FAA manpow er needs for positions of operations, coordination and management. This may require additional physical footprints to accommodate new positions of operations personnel and their supporting infrastructure  Next Generation Weather Processor \(NWP\ and Weathe r and Radar Processor \(WARP  227 The removal of a pilot in the cockpit also removes the ability to \223see\224 from a traditional pilot\222s perspective. Not only does this relate to the need for electronic systems for detect and avoid \(i.e., the FAR reference 91.113 for see and avoid\ but also affects many aspects of flight safety associated with detection and avoidance of hazardous weather from visual perspective of the manned cockpit. Determination of flight visibility is one such factor. It should also be noted tha t many of the UAS flying in the NAS today have a very low tolerance to hazardous weather such as turbulence and icing  Detect and Avoid \(DAA  227  Depending upon architecture and functionality, detect and avoid for UAS could have a profound impact on a variety of FAA systems and infrastructure components. DAA can be airborne based, and affect Automatic Dependent Surveillance Broadcast \(ADS B\ In and Out and an associated Cockpit Display of Traffic Information \(CDTI\nd Traffic Alert and Collision Avoida nce System \(TCAS\ functionality, including designs for the new Next Generation Airborne Collision Avoidance Systems \(ACAS X\. GBSAA that leverages ASR 11 Surveillance Radars and Standard Terminal Automation Replacement Systems \(STARS\ or TAMR also demands automation and surveillance system modification to provide this capability. Additional future work on portable primary three dimensional \(3D\ radars integrated in the NAS will also demand close analysis  The previous overview of FAA air traffic control sys tems gives insight in the investment required to be undertaken within the F AA\222s F&E  budget It also s erve s to illustrate the pervasiveness of changes that the UAS will eventually have on air traffic control and its supporting  ATM  infrastructure   It is impo rtant to recognize these ATM/ATC changes need to be planned for, funded and deployed. It must also be recognized that these UAS related investments do not stand alone in that these changes must be done while other planned ATC system improvements are taking  place I nvestments   such as   the Surveillance Broadcast System Program  National Voice Switch, Future Cyber Security Data Communications, and others are planned under the Next Generation Air Transport System  NextGen and others  Obviously   if future UA S forecasts are to be sustained it will entail a major shift in current FAA investment in addition to the  currently planned funding of NextGen  The forecasts outlined in this paper  will require an increase in funding levels of between $500 million and $1 billion dollars per year over and above current FAA requested F&E funding levels   Doing so will greatly facilitate realization of the $30 billion per year UAS market  9   S UMMARY  From an aviation perspective, UAS represents a new and disruptive technology  challenging the staid institutions policy, procedures and technologies that exist today and have served manned aircraft well for the last fifty years or more. This UAS technology supports an incredibly wide range of uses that not only allows old challeng es to be addressed in new ways but also creates new innovative world markets for hundreds, if not thousands, of new creative applications answering the call of \223better, faster and cheaper.\224  A growing number of early adopters and innovators of aviation tech nologies are beginning to realize the significance of a growing diversity and number of UAS  This growth  is  driven by a confluence of technological developments in airframes, powerplants, sensor s command and control sys tems and information management The  economic  value  of this industry projected to be $30 billion per year supporting 300,000 American jobs by 2035   As concluded b y the study team   the realization of UAS market forecasts will be constrained  unless key investment is performed in   The NAS ATC/A TM facilities needed to be updated to support new aircraft performance and controller capabilities   Training for ATC to be ready to adopt the new unmanned aircraft performance with acceptable workload    ATC to pilot communications that have the availability  integrity confidentiality and continuity as to not heavily impact either  pilot o r  controller workload   Development of Civil standards for the c ontrol and communications data link that w ill foster low cost access for a b road  range of UAS 226  this could in clude standards of f orm, fit and function as well as covering signal in space  networked channel  access and common control exchange applications    This key investment will develop a C 3  approach to support NAS integration.  It is agreed that other areas such as of DAA are important for investment, but there are already multiple economic incentive s  following this path  


  Paper 2581 Version 2  16  R EFERENCES  1 223 Unmanned Aircraft System \(UAS\ Service Demand 2015 2 035: Literature Review and Projections of Future Usage 224  Volpe National Transportation Systems Center Cambridge MA September 2013, Version 0.1   2 AINonline, June 8, 2012, Bill Carey   3  UAV Vision Pty Ltd  2/10 Uralla Street  P ort  M acquarie  NSW 2444  Australia   4  M1 D  Thermal FLIR PTZ Camera, Sierra Pacific Corporation, Las Vegas, NV   5   Experimental Advanced Research Lidar  USGS.gov  Retrieved 8 August 2007   6  Luke Wallace  Arko Lucieer  Christopher Watson and Darren Turner 223Devel opment of a UAV LiDAR System with Application to Forest Inventory\224, Remote Sensing Volume 4 issue 6, May 25 2012 http://www.mdpi.com/2072 4292/4/6/1519  7 M ag r et t a  J o a n  223 Wh y B u s i n e s s M o d e l s M at t er  224 H ar v a r d  Business Review, pp 3 8, May 2002   B IOGRAPH IES  Chris Wargo  is a program manager and director business development for Mosaic ATM, Inc 226  a firm specializing in air traffic management systems development unmanned systems and data management systems for the aviation sector. He also leads the Autonomous  Systems Group and  served as a Chair of the System Engineering Working Group of RTCA SC 203  He has also held positions as President of Computer Network Software, Inc., Vice President and General Manager for ARINC, Inc., C3I Program Manager RC A Automated Systems and GE, as well as Systems Engineer for GTE Sylvania, Electronic Systems Group, and the US Army. In his role as a leader in aviation  next generation systems engineering, he has participated in numerous ICAO, RTCA AEEC, IEEE and IATA c ommittees and standards working groups throughout his 30 year defense and aeronautical systems career. He has presented a number of papers and chaired many industry conference sessions related to the CNS and network system programs, project and technologie s of the general, business and air transport community.   He has a BSEE from the University of Wisconsin and an MS, Systems Engineering, from the University of Southern California, and has attended the Defense Systems Management College and the Advanced Ma nagement Program at the Harvard Business Sch ool    Jason Glaneuski  is a Program Manager and Operations Research Analyst in the Air Traffic Management Systems Division at the Volpe National Transportation Systems Center \(Volpe Center\ in Cambridge, MA His  Division applies information technology and operations research disciplines to enhance the capacity, safety and security of the National Airspace System. A key component of this work is developing concepts and designing automated decision support tools a nd capabilities  that provide solutions to existing and anticipated traffic flow issues. Mr. Glaneuski has experience both performing and managing technical work in the areas of traffic flow management \(TFM\, time based flow management \(TBFM and unmanned aircraft systems \(UAS\ sense and avoid SAA\, among others. Mr. Glaneuski is a graduate of Daniel Webster College in Nashua, NH, where he received his B.S in Aviation Management and Flight Operations. Prior to joining the Volpe Center in 2001, Mr. Glaneus ki worked for the FAA\222s Free Flight Program Office in Washington, D.C   Mark Strout  is an Operations Research Analyst and Project Manager in the Air Traffic Management Systems Division at the Volpe Center in Cambridge, MA. Mr. Strout provides project oversight and technical expertise attendant to several ATM related concepts and capabili ties, including traffic flow management \(TFM\, time based flow management \(TBFM\, National Airspace System Common Reference \(NCR unmanned aircraft systems and flow based trajectory management \(FBTM\. Mr. Strout is a graduate of the University of Virgini a, where he received his B.S. in Aerospace Engineering   Gary Churc h is the President of Aviation Management Associates, Inc. a Washington, DC based aviation firm established in 1984 to provide consulting services related to the Federal Aviation Administ ration and the National Aeronautics and Space Administration Mr. Church leads a team of two dozen aviation domain experts. He is a former air traffic controller and Manager of Air Traffic Control for the Air Transport Association \(now known as Airlines fo r America as well as an instrument rated pilot. Mr. Church has authored many air traffic control related articles and reports over the years as well participat ing  in numerous  key aviation related committees on behalf of FAA and others Mr. Church majored in Economics and Physics at the Un iversity of California Berkeley       
























































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


