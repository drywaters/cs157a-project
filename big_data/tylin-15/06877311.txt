Characterization and Optimization of Memory-Resident MapReduce on HPC Systems Yandong Wang 003 Robin Goldstone  Weikuan Yu 003 Teng Wang 003 Auburn University 003 Lawrence Livermore National Laboratory  f wangyd,wkyu,tzw0019 g auburn.edu f goldstone1 g llnl.gov Abstract MapReduce is a widely accepted framework for addressing big data challenges Recently it has also gained broad attention from scientists at the U.S leadership computing facilities as a promising solution to process gigantic simulation results However conventional high-end computing systems are constructed based on the compute-centric paradigm while big data analytics applications prefer a data-centric paradigm such as MapReduce This work characterizes the performance impact of key differences between computeand data-centric paradigms and then provides optimizations to enable a dual-purpose HPC system that can ef\002ciently support conventional HPC applications and new data analytics applications Using a state-of-the-art MapReduce implementation Spark and the Hyperion system at Lawrence Livermore National Laboratory we have examined the impact of storage architectures data locality and task scheduling to the memory-resident MapReduce jobs Based on our characterization and 002ndings of the performance behaviors we have introduced two optimization techniques namely Enhanced Load Balancer and Congestion-Aware Task Dispatching  to improve the performance of Spark applications I I NTRODUCTION One grand challenge faced by our society is a deluge of digital data so called Big Data  According to the 2011 IDC report a total of 1,220 e xabytes of data w as created and replicated on earth in 2010 IDC estimated that the volume of digital data will continue to grow at an annual rate of 50 i.e  the amount of data is expected to reach more than 8 zettabytes by 2015 To cope with the data deluge challenge the past few years have witnessed rapid development of big data analytics frameworks 3 4 5 6 Among them MapReduce 2 has achieved widespread success Many organizations have been embracing MapReduce and deploying its different implementations such as Hadoop Dryad and Spark 5 to meet their needs of massi v e computation and analysis of enormous datasets thereby mining critical knowledge for their business In this modern rush for gold from data different organizations are facing very different considerations when it comes to a decision on their data analytics systems With the prevalence of cloud platforms and commercial computing services many customers can leave that decision to their system providers But the system providers really have to juggle between two choices should they construct from scratch dedicated systems for data analytics or should they evolve their systems to meet the demands of data analytics applications while continuing to support existing applications and customers The latter is a particularly perplexing situation faced by the users and administrators at the leadership computing facilities who have been relying on traditional HPC High-Performance Computing systems for their scienti\002c applications Along with this dilemma is that there is a hidden paradigm shift along with the emergent focus on big data For the 002rst few decades of computer history computing power has been a scarce resource Thus conventional systems are constructed based on a compute-centric paradigm while the grand objective is to aggregate as much computing power as possible in terms of the number of 003oating-point operations per second The need to analyze big data has actually pushed the transition of computer systems into a data-centric paradigm for which the grand objective is to attain the fastest analytics power in terms of the number of bytes and records processed per second Fig 1 Data-centric and compute-centric paradigms Fig 1 shows a comparison between computeand datacentric paradigms There are two key distinctions between these paradigms First there is a key difference on the placement of compute and storage resources The conventional compute-centric paradigm has separated compute and storage resources in the form of a computer cluster and a parallel 002le system that are connected via high speed networks In contrast the data-centric paradigm provides co-located compute and storage resources on the same node Second there is a key difference in terms of the impact of task scheduling and data placement In the compute-centric paradigm tasks on compute nodes are in general equally distant from the backend storage system HPC systems are Typical manifestations of this paradigm In the data-centric paradigm tasks have strong af\002nity to the nodes containing their datasets Note that we are aware of some hierarchical compute-centric systems such as BlueGene series on which physical locations of compute nodes affect the speed of storage access Because of these distinctions on compute-centric paradigm applications sharing 
2014 IEEE 28th International Parallel & Distributed Processing Symposium 1530-2075/14 $31.00 © 2014 IEEE DOI 10.1109/IPDPS.2014.87 799 


the same data often involve repetitive data movement between the computing resource and the storage backend In contrast the data-centric paradigm provides co-located compute and storage resources on the same node to facilitate localityoriented task scheduling By scheduling computing tasks to where data resides data movement can be minimized for applications sharing the data These distinctions between computeand data-centric paradigms have signi\002cant performance implications to different types of application workloads For system providers who are eager to support more MapReduce-based analytics applications on HPC platforms it is imperative to characterize the performance of key architectural components in these two different paradigms Particularly how does the con\002guration of storage resources such as parallel 002le systems affect job scalability and throughput What is the impact of data placement and task scheduling And how to reconcile and converge the architectural differences between the two paradigms so that one system can be con\002gured and tuned for productive sharing by both conventional HPC applications and the emergent MapReduce-based analytics applications In this paper we undertake an effort with intensive experiments to characterize the performance identify the inef\002ciency of a MapReduce-based framework on the compute-centric paradigm and compare its performance with that on the data-centric paradigm Accordingly we also introduce several optimizations targeting at compute-centric HPC systems Among many MapReduce frameworks we have chosen Spark which is a memory-resident implementation sho wn to outperform Hadoop for many applications by orders of magnitude 7 W e le v erage the Hyperion 8 system at Lawrence Livermore National Laboratory with two distinct con\002gurations one under the compute-centric paradigm and the other under the data-centric paradigm In summary we conduct a comprehensive investigation to characterize the performance critical aspects of computeand data-centric paradigms and shed light on how to build a dualpurpose HPC system to enable fast data analytics We have made the following contributions in this research 017 We have studied the impact of storage architecture to the performance of different types of MapReduce jobs and revealed that their performance on HPC systems is highly dependent on their computation intensity 017 We have characterized the importance of intermediate data placement and the bene\002ts of hierarchical storage media to Spark applications Particularly we show that MapReduce applications need to be aware of the performance implications of storage consistency mechanisms on HPC systems and avoid the cascading effects of lock contention from HPC 002le systems such as Lustre 017 We have evaluated the impact of locality-oriented scheduling techniques for MapReduce jobs on computecentric HPC systems We show that maximizing data locality is not so critical and delay scheduling a popular strategy to delay tasks for data locality can even cause performance degradation 017 We have introduced two optimization techniques Enhanced Load Balancer and Congestion-Aware Task Dispatching The former takes into account performance variation and imbalanced data distribution when scheduling tasks resulting an improvement of 26 on job execution time The latter recognizes the existing oblivity of Spark to new storage devices such as SSD throttles the launch of Spark tasks and mitigates the congestion thereby achieving a performance gain up to 41.2 II C OMPARISON B ETWEEN C OMPUTE C ENTRIC AND D ATA C ENTRIC P ARADIGMS In this section we provide a direct comparison between the compute-centric and data-centric processing paradigms Fig 2 Detailed comparison between computeand datacentric paradigms A HPC Systems Representing the Compute-Centric Paradigm Fig 2\(a shows a diagram of typical compute-centric HPC systems The core of such systems consists of a large collection of compute nodes i.e processing elements PEs which offer the bulk of computing power Via a high-speed interconnect these PEs are connected to a parallel 002le system from the storage backend for data I/O Lustre is a typical 002le system used on HPC systems It is a POSIX-compliant object-based parallel 002le system offering parallel I/O services to the clients PEs through a MetaData Server MDS and many Object Storage Servers OSSes Lustre provides 002ne-grained parallel 002le services with its distributed lock management To guarantee 002le consistency it serializes data accesses to a 002le or 002le extents using a distributed lock management mechanism Because of the need for maintaining 002le consistency all processes 002rst have to acquire locks before they can update a shared 002le or an overlapped 002le block Thus when all processes are accessing the same 002le their I/O performance is dependent not only on the aggregated physical bandwidth from the storage devices but on the amount of lock contention among them as well B Spark  A Representative of Data-Centric Paradigm MapReduce frameworks distribute computation map and reduce tasks among a number of slave nodes Reduce tasks consolidate and transform the intermediate data generated by tasks from the previous map phase Spark is a recent highly popular MapReduce implementation It consists of two categories of components a scheduler and many executors The 
800 


scheduler is in charge of scheduling tasks monitoring their progress and fault handling through task re-execution The executors are responsible for executing the actual computing and data processing tasks As many MapReduce implementations Spark usually works together with distributed 002le systems that are designed to co-locate the storage resource  i.e DataNode with the compute resources  i.e tasks launched by Executors as shown in Fig 2\(b For example Spark relies on the HDFS to manage the 003o w of data HDFS is composed of a master NameNode and many slave DataNodes Google's MapReduce has a similar reliance on the Colossus the latest version of Google 002le system Such co-localization of DataNodes and Executors realizes a data-centric computing model to minimize data movement between computation tasks and the storage system C Memory-Resident Resilient Distributed Datasets in Spark Compared to other MapReduce implementations such as Hadoop Spark pro vides tw o k e y features First Spark leverages the distributed memory from all slave nodes to store most intermediate data during job execution and the 002nal execution results at job completion By doing so it avoids the 002le system retaining most data resident in distributed memory across phases in the same job and/or different jobs Such memory-resident feature bene\002ts many applications such as machine learning or iterative algorithms that require extensive reuse of results among multiple MapReduce jobs Second Spark introduces resilient distributed datasets RDDs to facilitate the programming of parallel applications Each RDD represents a collection of data partitions that spread across the cluster A rich set of operations are provided to manipulate RDDs  e.g  map  003atMap  groupBy  and reduce  etc  Overall those operations can be categorized into two types which are transformation and action  respectively Fig 3 MapReduce processing pipeline via using RDDs A transformation converts a source RDD to a destination RDD by applying User-De\002ned Functions UDF to each partition contained in the former Fig 3 illustrates an example of a Spark MapReduce job in which an HDFS 002le is transformed to the 002nal MappedRDD through four transformations 002lter  003atMap  groupByKey and map  When Spark is deployed on a cluster featuring compute-centric paradigm HadoopRDD can be replaced by system dependent RDD such as LustreRDD  to retrieve input from HPC parallel 002le system Spark's actions include reduce  count  collect  etc  An action triggers Spark to construct an execution plan represented internally as a directed acyclic graph DAG that consists of multiple stages Each stage includes many transformations that can be pipelined Stages are connected through the shuf\003e operations for intermediate data shuf\003ing An implicit stage is embedded into the DAG for every shuf\003e operation For example 002lter and 003atMap in Fig 3 are grouped into a same stage while the groupByKey is in an independent stage Sparks launches stages within the DAG in a serialized manner The shuf\003ing of intermediate data is a major performance bottleneck of MapReduce implementations including Spark However such shuf\003e operation widely exists in many critical operations such as join  reduceByKey  and groupBy  etc  To avoid substantial overhead and provide reliable job execution Spark materializes partitions onto the local 002le system When a shuf\003e operation is encountered Spark will undertake two phases for moving intermediate data storing and shuf\003ing In the storing phase Spark schedules a round of Shuf\003eMapTasks to 003ush in-memory output from the previous stage to the 002le system Then in the shuf\003ing phase a Shuf\003edRDD is introduced to transfer the intermediate data across the network III M ETHODOLOGY A Experimental Testbed TABLE I List of key Spark con\002guration parameters Parameter Name Value spark.reducer.maxMbInFlight 1GB spark.rdd.compress false spark.shuf\003e.compress true spark.buffer.size 8MB spark.default.parallelism application dependent Unless otherwise speci\002ed our experiments are carried out on the Hyperion cluster with 101 compute nodes at Lawrence Livermore National Laboratory One node serves as the master of the Spark and the NameNode of HDFS Each compute node is equipped with two 2.60 GHz Intel E52670 processors 16 cores per node and 64 GB of RAM We allocate 30 GB per node for Spark jobs and reserve 32 GB for RAMDisk On each node there is one SATA-based SSD of 128 GB storage space mounted via ext4 002le system Its peak sequential write and read bandwidths reach 387 MB/sec and 507 MB/sec respectively All compute nodes span across two racks and are fully connected through In\002niBand QDR which delivers up to 32 Gpbs link bandwidth A centralized Lustre 002le system providing 47 GB/sec aggregated bandwidth is mounted on all the compute nodes All compute nodes run Linux 2.6.32 kernels Spark 0.7.0 along with Scala 2.9.2 and Oracle Java 1.7.0 are used The HDFS block size is set as 128 MB We have also carefully tuned Spark on Hyperion Table I summarizes main parameters that have noticeable performance impact For all tests we report the median of 002ve test runs B Benchmarks We have selected three representative benchmarks including GroupBy  Grep  and Logistic Regression LR They are described as follows 
801 


Fig 4 Execution plans of three representative benchmarks GroupBy is a critical operation used by many applications including kMeans  wordcount  and calculating transitive closure of a graph  etc  It helps reveal the pattern of shuf\003e operations Fig 4\(a depicts the execution plan of GroupBy It consists of three stages In the 002rst computation stage each task generates  key value  pairs in memory In the second stage Spark schedules Shuf\003eMapTasks to partition the intermediate data and store them into the 002le systems In the last stage fetching tasks shuf\003e intermediate data over the network Across such data processing pipeline the intermediate data size is equal to the input size Grep searches a string that matches a regular expression from a set of documents It represents a wide range of data analytics applications such as logQuery and select  etc  Grep's execution plan as shown in Fig 4\(b bears some similarity to that of GroupBy However it generates much less intermediate data requiring very little shuf\003ing of data Its intermediate data size ranges from 1 MB to 200 MB in our test cases Logistic Regression LR is an iterative application that predicts the value of a vector according to a qualitative response model It can leverage the strength of Spark in caching job results in memory As shown in Fig 4\(c we run three iterations for LR Every iteration is translated into one Spark job that is executed in one stage Multiple stages are not pipelined in this benchmark IV T HE I MPACT OF S TORAGE A RCHITECTURE As discussed in the introduction the storage architecture is a key distinction between dataand compute-centric paradigms In the data-centric paradigm computation tasks are co-located with the storage resources while in compute-centric paradigm tasks need to access a separate storage subsystem via interconnect In this section we characterize the impact of storage architecture on MapReduce jobs To have a storage architecture for the data-centric paradigm we con\002gure an HDFS 002le system with 32 GB RAMDisk as the storage for each DataNode on Hyperion For the storage architecture of the compute-centric paradigm we directly use the Lustre 002le system of Hyperion A Location of Data Source Among the three benchmarks both Grep and LR work with a varying amount of input data But they differ signi\002cantly in terms of their analytics computation Grep generates a small amount of intermediate data for which shuf\003ing is required a Grep b Logistic Regression LR Fig 5 Performance of retrieving inputs from HDFS and Lustre and LR does mostly computation We run both benchmarks with their input coming from the compute-centric Lustre based con\002guration and the data-centric HDFS based con\002guration Fig 5 shows the comparison of the job execution time of Grep and LR benchmarks for both con\002gurations Overall we have observed that the extent of impact is highly dependent on the computational intensity of MapReduce tasks For Grep jobs with low computation such as simply scanning of the input the Lustre con\002guration results in severe performance penalty Fig 5\(a shows that with 32 MB split size the computecentric Lustre con\002guration performs up to 5.7 002 worse than HDFS on average For the Lustre con\002guration increasing the split size from 32 MB to 128 MB reduces the job execution time by 15.9 due to less scheduling overhead But there is still a signi\002cant performance loss when running Grep on the compute-centric Lustre con\002guration On the contrary for the computation-intensive jobs such as multidimensional vector multiplication in LR the cost of retrieving input from Lustre is not as signi\002cant as shown in Fig 5\(b Furthermore as shown in the 002gure the Lustre con\002guration outperforms HDFS by 12.7 on average for a 32 MB split size This improvement is consistent across different split sizes The performance difference is caused by delay scheduling policy adopted by Spark which will be further analyzed in Section V-A Taken together the impact of the storage architecture to MapReduce applications depends on the characteristic of the applications computation tasks For LR-type computationintensive jobs the impact is negligible But for Grep-based jobs with low computation requirements the compute-centric Lustre con\002guration negatively affects the performance B Location of Intermediate Data The location of intermediate data is another critical issue It directly determines the performance of intermediate data shuf\003ing To investigate this factor we use the GroupBy benchmark that allows 003exible tuning of the intermediate data size During the evaluation we run GroupBy and store the intermediate data to the two different storage con\002gurations Fig 7\(a illustrates the performance of GroupBy when intermediate data resides in different storage architectures Overall the data-centric HDFS con\002guration exhibits signi\002cant advantage over the compute-centric alternative It outperforms the optimal Lustre case Lustre-local by up to 6.5 002 on average and the improvement ratio increases linearly 
802 


Fig 6 Two approaches to use Lustre to conduct intermediate data shuf\003ing with the size of intermediate data However due to the limited storage spaces HDFS can only support a maximum of 1.2 TB intermediate data size However in many scenarios compute nodes in HPC clusters are not equipped with any local persistent storage systems for which placing intermediate data on the compute-centric Lustre-based storage is the only choice Lustre-local and Lustre-shared  as shown in Fig 6 illustrate two approaches to use Lustre for intermediate data shuf\003ing In the Lustre-local case fetching tasks that need to shuf\003e the intermediate data are unaware of the existence of the Lustre Thus they initiate FetchRequests to the remote servers which in turn retrieve the data from their local Lustre directories and send them back across the network However since data retrieval from Lustre requires a movement over the network Lustre-local can cause repetitive data movements wasting the network bandwidth We have examined the alternative Lustre-shared approach in which each fetching task directly retrieves intermediate data from Lustre Although this approach seemingly addresses the issue of repetitive data movement within Lustre-local it suffers from tremendous performance degradation due to 002le consistency ensured by Lustre Fig 7\(a illustrates that Lustre-shared performs worse than Lustre-local by up to 3.8 002 with GroupBy benchmark The detailed dissection in Fig 7\(b further reveals that although the two approaches perform comparably in the data storing phase the shuf\003ing phase of Lustre-shared is inferior to that of Lustre-local by up to one order of magnitude The main reason for the inferiority of Lustre-shared is that retrieving intermediate data written by remote servers incurs costly metadata operations at the OSSes due to the need to maintain the storage consistency In the Lustre-local approach the server that handles the FetchRequests simply retrieves the intermediate data written by the tasks on the same node Meanwhile due to the effect of large buffer cache in a compute node it is likely that those intermediate data and corresponding metadata such as write locks  still reside in the local memory Thus they can be quickly retrieved to serve the FetchRequests without involving expensive internal operations of Lustre for maintaining the data consistency On the contrary in the Lustre-shared case each fetching task accesses Lustre to retrieve the data written by remote nodes Such design requires the Distributed Lock Manager a Job Execution Time of GroupBy b Dissection of Lustre Cases Fig 7 Performance when intermediate data resides in Lustre of Lustre to revoke the write locks After lock revocation intermediate data cached remotely is forced to be 003ushed to the OSSes before they become available to fetching tasks This sequence of internal operations substantially delays the intermediate data movement Furthermore current Spark launches fetching tasks of a job simultaneously during the shuf\003ing phase forcing all the intermediate data to be 003ushed to the OSSes around the same time As a result such behavior can cause serious contention at Lustre signi\002cantly degrading the performance of the shuf\003ing phase In summary the data-centric HDFS con\002guration shows dramatic advantage over the compute-centric con\002guration when used for storing the intermediate data In the computecentric case with a shared 002le system such as Lustre fetching tasks can avoid costly metadata operation for better performance if they are oblivious to the features of the shared 002le system C Leveraging Solid State Disks for Intermediate Data Many HPC systems are embracing a hierarchical stack of different storage devices in order to support both data-centric and compute-centric paradigms so that they can support both traditional HPC applications and emerging data analytics programs A major effort to achieve such goal is the trend to integrate high-performance Solid State Drives SSD to the compute nodes An immediate impact to MapReduce is that they can ef\002ciently facilitate the processing of intermediate data To understand such performance implication we have conducted a set of experiments to study the performance impact of SSD on MapReduce jobs with similar data-centric HDFS con\002guration as that in Section IV-B The performance of using RAMDisk as the local persistent storage is employed for performance comparison We continue to use GroupBy as the benchmark for this study Fig 8\(a presents the job execution time of GroupBy when intermediate data is stored on RAMDisk and SDD respectively Overall using SSD for intermediate data achieves comparable performance as RAMDisk when the data size ranges from 100 GB to 600 GB due to the caching effects from the 002le system Once the data size exceeds 700 GB RAMDisk performs substantially better than SSD Note that SSD can support jobs with much larger intermediate data sizes than RAMDisk due to the capacity advantage of SSD Fig 8\(b further shows a detailed dissection of job execution time when SSD is employed Data shuf\003ing is shown as 
803 


a Job Execution Time b Detailed dissection c Performance variation among tasks that write SSDs d Execution time of all Shuf\003eMapTasks Fig 8 Performance when SSD is used for storing the intermediate data and detailed analysis of tasks that write to and shuf\003e data from SSDs the key bottleneck when data size 024 600 GB in which the throughput is bounded by the network bandwidth When the data size is between 700 GB and 900 GB the cache can no longer satisfy all the write operations during the storing phase As a result both storing and shuf\003ing of intermediate data contribute equally to the job execution When the data size increases further beyond 900 GB we observe sharp drops on the performance of storing and shuf\003ing phases due to the degraded performance of SSD write and read operations In addition the write performance falls more drastically than that of read When the storing phase of intermediate data becomes the major bottleneck of job execution the throughput of data shuf\003ing then becomes SSD-bound D Inef\002ciency in Utilizing SSD In our experiments with SSD there is a signi\002cant performance variation among Shuf\003eMapTasks writing intermediate data to SSDs as shown in Fig 8\(c The performance gap between the fastest and the slowest tasks can be as wide as 18 002 when the data size reaches 1.5 TB On the contrary the performance variation among shuf\003ing tasks is moderate not shown for brevity indicating a mild interference among SSD read operations The dramatic variations among Shuf\003eMapTasks is because Spark aggressively launches tasks as they arrive in order to reduce the latency This is oblivious to the congestion of underlying SSDs When multiple data-intensive tasks are running and issuing a large number of write requests such oblivity can result in substantial interference amony tasks To gain insight into this issue we have pro\002led the execution times of all Shuf\003eMapTasks in the 1.5 TB test case We plot the execution times of these tasks based on the order of their launch time in Fig.8 d As shown in the 002gure early tasks can take advantage of write buffer and clean blocks on SSDs They can quickly complete their work When the buffer gradually 002lls up and clean SSD blocks are depleted internal operations for delayed write and garbage collection are activated These operations start to interfere with the execution of Shuf\003eMapTasks Thus we observe a degraded performance for Tasks ranging from 3100 to 4500 However Spark is unaware of such interference and continues to insert tasks This behavior further exacerbates the contention on the SSDs and leads to severer interference among Tasks from 4800 to 6400 In summary our study reveals that the lack of awareness on the unique features of SSD can lead to inef\002cient utilization of resource when in the storage of intermediate data Fortunately the inef\002ciency of congestion-oblivious write has also been documented by many prior studies on SSD 12 13 In Section VI-B we will demonstrate that an optimization using a throttling mechanism can effectively mitigate the interference and improve the storing phase by 41.2 V T HE I MPACT OF D ATA L OCALITY AND T ASK S CHEDULING A Locality-Oriented Scheduling a Grep b Logistic Regression LR Fig 9 Performance degradation caused by delay scheduling Maximizing data locality has been a critical objective of MapReduce schedulers 14 15 Delay scheduling  adopted by Spark is a notable effort for obtaining high data locality for MapReduce frameworks in the environments where network bandwidth is a scarce resource Using the same computeand data-centric con\002gurations as described in Section IV we conduct an experiment to characterize the importance of locality-oriented scheduling Fig 9 shows the experiment results when we activate delay scheduling for the data-centric HDFS con\002guration When the split size is equal to 32 MB job execution time degrades by 42.7 and 9.9 on average for Grep and LR respectively Similar degradation occurs for other split sizes as well In contrast with the compute-centric Lustre con\002guration tasks can be immediately launched on available compute nodes since there is no locality constraint All the computation tasks are roughly at the same distance from storage resources Thus compared to the data-centric con\002guration that favors the use of delay scheduling for better data locality of tasks this setting can bene\002t the computation-intensive MapReduce jobs as shown in Fig 5\(b 
804 


Fig 10 Task execution time of three benchmarks In addition Spark pipelines computation with data input further diminishing any bene\002t of data locality Fig 10 demonstrates such argument It shows the comparison of average task execution times along with maximum and minimum values of three different benchmarks Task with local data denotes that the data input is obtained locally while Task with remote data indicates the data input from remote servers As shown in the 002gure enforcing tasks to achieve 100 locality provides little performance gain for all three benchmarks Taken together our evaluation and characterization of locality-oriented scheduling for the computeand data-centric con\002gurations suggest that 1 scheduling for good locality may not be effective in improving the performance of MapReduce jobs in HPC environments and 2 introducing delays for better task locality is even detrimental on computecentric systems because of the uniform reachability of storage resources to all computation tasks B Load Balance of MapReduce Tasks Although the compute nodes in a compute-centric environment are homogeneous there exist performance variations among compute nodes due to the skew of workloads over time As a result fast nodes tend to be assigned with more tasks by the scheduler When each of these tasks deposits a unit of intermediate data fast nodes end up with much more data to shuf\003e or move This leads to imbalanced distribution of intermediate data When a shuf\003e operation is needed such imbalanced distribution can cause straggler issue that prolongs the ensuing I/O-intensive data storing and shuf\003ing phases as depicted in Fig 11 Fig 11 Straggler issue caused by imbalanced intermediate data distribution during I/O intensive shuf\003e operation To investigate this issue we use GroupBy as the benchmark with a split size of 256 MB Three sets of experiments are conducted to run 2500 tasks on 50 nodes 5000 tasks on 100 nodes and 7500 tasks on 150 nodes respectively Fig 12 a and b illustrate the cumulative distribution functions CDF of task and intermediate data distributions a Task distribution b Intermediate data distribution Fig 12 Unbalanced task assignment leads to unbalanced intermediate data distribution As shown in Fig 12 a the workload among compute nodes varies substantially In the case of 100 nodes for the 002rst 3 nodes at the head of the distribution each machine only hosts 7 GB of intermediate data While for the last 10 nodes at the tail of the distribution each node accommodates more than 14 GB i.e 2 002 of workload difference Because the execution time of storing and shuf\003ing phases are directly determined by the slowest tasks those nodes with the most intermediate data can severely drag down the performance regardless of how fast other tasks have achieved In summary performance variations and workload skews on compute-centric systems can lead to imbalanced distribution of both MapReduce tasks and their intermediate data Without an appropriate solution such issue can hinder MapReduce systems from achieving the best performance on computecentric HPC systems We will demonstrate in Section VI-A that by taking into account of the intermediate data size the shuf\003e operation can be effectively accelerated VI O PTIMIZATIONS FOR S PARK ON C OMPUTE C ENTRIC HPC S YSTEMS Based on the characterization from Sections IV and V we have shown that there are two performance issues that need to be addressed for the memory-resident Spark framework in order for it to be effectively supported by the compute-centric HPC systems Firstly the scheduler should take into account of the need to balance the intermediate data among compute nodes and mitigate the variations of task execution thereby avoiding stragglers Secondly the MapReduce workers should be aware of the unique features of hierarchical storage devices such as SSDs to effectively utilize them Accordingly we introduce two optimizations namely Enhanced Load Balancer ELB and Congestion-Aware task Dispatching CAD to address these issues A Enhanced Load Balancer ELB We design ELB to address the issue of imbalanced distribution of intermediate data It considers the size of intermediate data generated by tasks before making further task assignment decision When a job starts ELB-enabled scheduler assigns tasks to the workers in a round-robin manner During the job execution ELB records the amount of intermediate data generated by each completed task and monitors the average data size among all nodes When the size on a node goes beyond the average by a threshold 25 currently ELB 
805 


a Storage is the bottleneck b Network is the bottleneck Fig 13 Dissection of GroupBy job execution time a Job execution time b Dissected of job execution Fig 14 Performance of Congestion-Aware task Dispatching noti\002es the scheduler to stop assigning more tasks to that worker node Instead it picks the nodes hosting the least amount of intermediate data to execute the pending tasks Once the average size goes up ELB resumes to assign more tasks to the original heavily loaded worker Although ELB can balance the size of intermediate data among compute nodes two issues arise under such design Firstly ELB may con\003ict with the data locality since the nodes hosting the least amount of intermediate data may not possess the input for the tasks However as shown in Section V-A enforcing data locality has negligible impacts on the task execution time in the HPC environment Thus it is desirable to trade off the locality of task scheduling for a balance of data distribution Secondly ELB can cause the idling of certain workers when they have completed their share of computation tasks and the entire computation phase can be consequently delayed due to the slowest task However we have observed that the cost of waiting for the slowest computation task is much less than the cost of waiting for the slowest I/O tasks In this context to demonstrate the performance improvement of ELB-enabled scheduler to communication and storage bottlenecks during the shuf\003e operations the GroupBy benchmark is used To create a scenario of storage bottlenecks SSD is used as the local storage device In Hyperion we are not allowed to use other networks other than In\002niBand So to create a scenario of network bottlenecks we reduce the data size set in FetchRequest from 1 GB to 128 KB Thus many more requests are needed to shuf\003e the same amount of data and the network bandwidth is consequently narrowed due to space constraint we only present the dissection of job execution and omit execution time of computation phases for clearness Storage Bottlenecks  Fig 13\(a shows that when the data size 024 900 GB Spark and ELB perform similarly However ELB outperforms Spark by 26 on average in terms of job execution time when the data size is between 1 TB and 1.5 TB Such improvement is mainly attributed to the accelerated staging phase introduced by the ELB When data sizes reach beyond 1 TB Spark performs worse than ELB by 2.2 002 on average in the staging phase On the contrary computation phases from both remain nearly the same Network Bottlenecks  Spark performs 14.8 worse than ELB on average in terms of job execution time Moreover when the network is the bottleneck unbalanced distribution has severe impact on small datasets showing up to 17.5 degradation when data size is 400 GB Such difference is strongly determined by the shuf\003ing phases as shown in Fig 13\(b On average Spark shuf\003es data slower than ELB by 29.1 when the input size ranges from 400 GB to 1.2 TB Taken together our ELB demonstrates that unbalanced distribution of intermediate data can prevent memory-resident Spark from achieving the optimal performance in the HPC environment B Congestion-Aware Dispatching CAD of Tasks We design CAD as a feedback control algorithm that aims to mitigate the task interference when SSD is used as the storage device for intermediate data CAD speculates the congestion status of SSD devices by monitoring the task execution time of completed Shuf\003eMapTasks When a signi\002cant jump of execution time is detected it throttles the dispatching of tasks by introducing a delay interval before each dispatching step In the current design we increase the interval by 50 ms whenever the average execution time increases by 2 002 these are empirically chosen during our tuning process Conversely we reduce the interval accordingly when the average task execution time drops by half Though simple we have observed that such mechanism is effective in optimizing the SSD writes This is because such delay interval allows more time for outstanding operations inside SSD to complete their work without worsening the congestion In addition it also provides more opportunity to group many small writes which are harmful to SSD thus further reducing the interference Fig 14 compares the performance of the original Spark with our CAD-enabled Spark by using the GroupBy benchmark with different input sizes Overall CAD effectively accelerates the intermediate data storing phase once the data size goes beyond 600 GB It achieves this without affecting another two phases as shown in Fig 14\(b On average CAD reduces the storing phase by up to 41.2 when the data size ranges from 700 GB to 1.5 TB Such acceleration is re\003ected in the job execution time as shown in Fig 14\(a The average improvement ratio reaches 19.8 VII D ISCUSSION In this section we summarize our major 002ndings and discuss their implications to the design of future systems The Impact of Storage Architecture  Computation intensity of MapReduce tasks determines how much impact the storage architecture of HPC systems will have on the 
806 


job execution For computation-intensive applications there is little impact between the storage architectures of dataand compute-centric paradigms In addition there is no locality to the storage for compute nodes on compute-centric systems tasks can be launched on any node with little loss of performance or even better performance compared to the data-centric environment However the data-centric paradigm still exhibits superior performance for applications with low computation intensity and high data intensity Therefore it is critical to consider the characteristics of MapReduce jobs before making data placement decisions This is important for system providers in planning the evolvement of their computecentric HPC systems for data-centric analytics applications In addition the storage architecture may use distributed locking mechanism for maintaining 002le consistency which can severely degrade the performance of intermediate data movement So we show that designing shuf\003ing mechanisms can avoid the cascading effects of locking contention and keep the ef\002ciency of intermediate data shuf\003ing Users need to avoid a pitfall to use traditional HPC parallel 002le system as a bridge for fast storage of intermediate data When SSDs are used as the storage device for intermediate data our analysis shows that Spark is currently incapable of utilizing them ef\002ciently Uncoordinated resource utilization can cause severe congestion on the device leading to significant task interference as also shown in Our 002ndings suggest that comprehensive examinations are needed to assure the performance of MapReduce applications while evolving the underlying storage of a system to SSDs Optimization strategies such as task throttling as shown by our study can be leveraged to improve the ef\002ciency of SSD device utilization The Effectiveness of Locality-Oriented MapReduce Schedulers in HPC Environment  Our characterization reveals that when a data-centric storage architecture is con\002gured for computer nodes of an HPC system MapReduce schedulers that strive for maximum data locality is not critical Moreover they may even hurt the performance by forcing a task delay for future opportunistic locality We have also revealed that while HPC systems generally have homogeneous computer nodes load imbalance can still arise The current scheduler is oblivious to the size of intermediate data generated by computation tasks leading to imbalanced data distribution that can cause many stragglers during shuf\003e operations Our study demonstrates that such imbalanced distribution can cause suboptimal performance to MapReduce jobs MapReduce applications on HPC systems shall not focus on locality-oriented task scheduling but other critical factors such as balancing distribution of intermediate data VIII R ELATED W ORK Spark is a critical cornerstone of Berkeley Data Analytics Stack BDAS that aims to compete with the opensource Hadoop It plays a pivotal role in many industry and academia projects 19 20 21 etc Shark is a query processing framework on top of Spark It compiles user-submitted SQL queries into Spark jobs and leverages optimization strategies commonly used in database systems to optimize the execution plan Also coupled with Spark BlinkDB is another approximate query engine that trades query accuracy for response time so that it can delivery near instant response for interactive queries over massive scale datasets Spark streaming e xploits the potential of Spark to process real-time streaming data It partitions streaming computations into small-sized deterministic batch jobs to 002t the computation model of Spark Sparkler optimizes the Spark to support large-scale matrix factorization more ef\002ciently It identi\002es a major inef\002ciency existing in current Spark's broadcast variable and introduces a Carousel Maps to spread large dataset via using distributed hash table Our work is orthogonal to those efforts In addition Zaharia et al have introduced LATE Ananthanarayanan et al have introduced Mantri and small job cloning 23 to mitig ate the impact of stragglers However none of them considers the imbalanced intermediate data distribution issue Many parties have tried to incorporate MapReduce frameworks with distributed 002le systems for compute-centric paradigm Ananthanarayanan et al  e v aluated MapReduce when it runs with HDFS and GPFS Maltzahn et al  studied the combination of Hadoop with Ceph 002le system Panasas is also deli v ering the support for Hadoop Our analysis in this work provides researchers with the 002rst hand data about absorbing MapReduce into compute-centric HPC paradigm that relies on above high-performance 002le systems Many efforts have been conducted to investigate the performance of HPC applications on data-centric cloud Evangelinos et al  analyzed a scienti\002c HPC application on Amazon EC2 and revealed that the performance of network in cloud is worse than that of HPC by one to two orders of magnitude Gupta et al  observ ed similar performance on different cloud platforms Though raw performance difference between compute-centric HPC and data-centric cloud is pronounced Marathe et al  pointed out that queue w ait time is another critical factor to consider when choosing which environment is the best for the applications Our work stands on the other side of the spectrum by investigating the datacentric analytics framework on compute-centric paradigm IX C ONCLUSIONS While many existing HPC facilities are evolving new capabilities to support ef\002cient analytics of big data this research addresses an important question on how to support the traditional compute-centric paradigm for HPC applications and the emerging data-centric paradigm for big data analytics applications on the same HPC systems We have examined the design and architecture of a state-of-the-art MapReduce framework  Spark  on HPC systems Our work sheds light on the performance issues and design inef\002ciency when running Spark jobs on HPC systems with distinct data-centric and compute-centric con\002gurations In particular we have investigated the impact of storage architecture locality-oriented scheduling and emerging storage devices to memory-resident MapReduce applications on HPC systems Based on the 
807 


experimental results our optimization techniques including the Enhanced Load Balancer and the Congestion-Aware Task Dispatching can ef\002ciently improve the performance of Spark applications on HPC systems Acknowledgments We are very thankful to the anonymous reviewers for their insightful comments This work is funded in part by an Intel grant an Alabama Innovation Award and by National Science Foundation awards 1059376 1320016 and 1340947 This work was also performed under the auspices of the US Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 LLNL-CONF647813 R EFERENCES   The 2011 Digital Universe Study Extracting Value from Chaos http://www.emc.com/collateral/demos/microsites/emc-digital-universe2011/index.htm   J Dean and S Ghemawat Mapreduce simpli\002ed data processing on large clusters Commun ACM  vol 51 pp 107–113 Jan 2008   Apache Hadoop Project http://hadoop.apache.org   Michael Isard and Mihai Budiu and Yuan Yu and Andrew Birrell and Dennis Fetterly Dryad distributed data-parallel programs from sequential building blocks in EuroSys P Ferreira T R Gross and L Veiga eds pp 59–72 ACM 2007   M Zaharia M Chowdhury T Das A Dave J Ma M McCauley M J Franklin S Shenker and I Stoica Resilient distributed datasets a fault-tolerant abstraction for in-memory cluster computing in Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation  NSDI'12 Berkeley CA USA pp 2–2 USENIX Association 2012   Y Wang X Que W Yu D Goldenberg and D Sehgal Hadoop acceleration through network levitated merge in Proceedings of 2011 International Conference for High Performance Computing Networking Storage and Analysis  SC 11 New York NY USA pp 57:1–57:10 ACM 2011   R S Xin J Rosen M Zaharia M J Franklin S Shenker and I Stoica Shark Sql and rich analytics at scale in Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data  SIGMOD 13 New York NY USA pp 13–24 ACM 2013   Hyperion Project https://hyperionproject.llnl.gov   M Zaharia D Borthakur J Sen Sarma K Elmeleegy S Shenker and I Stoica Delay scheduling a simple technique for achieving locality and fairness in cluster scheduling in Proceedings of the 5th European conference on Computer systems  EuroSys 10 New York NY USA pp 265–278 ACM 2010   K Shvachko H Kuang S Radia and R Chansler The hadoop distributed 002le system in Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies MSST  MSST 10 Washington DC USA pp 1–10 IEEE Computer Society 2010   Bcache http://bcache.evilpiepirate.org   F Chen D A Koufaty and X Zhang Understanding intrinsic characteristics and system implications of 003ash memory based solid state drives in Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems  SIGMETRICS 09 New York NY USA pp 181–192 ACM 2009   K Shen and S Park Flashfq A fair queueing i/o scheduler for 003ashbased ssds in Proceedings of the USENIX Annual Technical Conference  USENIX ATC'12 Berkeley CA USA USENIX Association 2013   W Wang K Zhu L Ying J Tan and L Zhang A throughput optimal algorithm for map task scheduling in mapreduce with data locality SIGMETRICS Performance Evaluation Review  vol 40 no 4 pp 33 42 2013   Y Wang J Tan W Yu X Meng and L Zhang Preemptive reducetask scheduling for fair and fast job completion in Proceedings of the 10th International Conference on Autonomic Computing  ICAC'13 June 2013   G Ananthanarayanan S Kandula A Greenberg I Stoica Y Lu B Saha and E Harris Reining in the outliers in map-reduce clusters using mantri in Proceedings of the 9th USENIX conference on Operating systems design and implementation  OSDI'10 Berkeley CA USA pp 1–16 USENIX Association 2010   X Li Y Wang Y Jiao C Xu and W Yu Coomr Cross-task coordination for ef\002cient data management in mapreduce programs in Proceedings of SC13 International Conference for High Performance Computing Networking Storage and Analysis  SC 13 New York NY USA pp 42:1–42:11 ACM 2013   Berkeley Data Analytics Stack https://amplab.cs.berkeley.edu/software   M Zaharia T Das H Li S Shenker and I Stoica Discretized streams an ef\002cient and fault-tolerant model for stream processing on large clusters in Proceedings of the 4th USENIX conference on Hot Topics in Cloud Ccomputing  HotCloud'12 Berkeley CA USA pp 10–10 USENIX Association 2012   S Agarwal B Mozafari A Panda H Milner S Madden and I Stoica Blinkdb queries with bounded errors and bounded response times on very large data in Proceedings of the 8th ACM European Conference on Computer Systems  EuroSys 13 New York NY USA pp 29–42 ACM 2013   B Li S Tata and Y Sismanis Sparkler supporting large-scale matrix factorization in Proceedings of the 16th International Conference on Extending Database Technology  EDBT 13 New York NY USA pp 625–636 ACM 2013   M Zaharia A Konwinski A D Joseph R Katz and I Stoica Improving mapreduce performance in heterogeneous environments in Proceedings of the 8th USENIX conference on Operating systems design and implementation  OSDI'08 Berkeley CA USA pp 29–42 USENIX Association 2008   G Ananthanarayanan A Ghodsi S Shenker and I Stoica Effective straggler mitigation attack of the clones in Proceedings of the 10th USENIX conference on Networked Systems Design and Implementation  nsdi'13 Berkeley CA USA pp 185–198 USENIX Association 2013   R Ananthanarayanan K Gupta P Pandey H Pucha P Sarkar M Shah and R Tewari Cloud analytics do we really need to reinvent the storage stack in Proceedings of the 2009 conference on Hot topics in cloud computing  HotCloud'09 Berkeley CA USA USENIX Association 2009   M Carlos M.-E Esteban K Amandeep J N Alex S A Brandt and W Sage Ceph as a scalable alternative to the hadoop distributed 002le system login 10 USENIX Association 2010   Accelerating and Simplifying Apache Hadoop with Panasas ActiveStor https://www.panasas.com/sites/default/\002les/uploads/docs/hadoop wp lr 1096.pdf   C Evangelinos and C N Hill Cloud computing for parallel scienti\002c hpc applications Feasibility of running coupled atmosphere-ocean climate models on amazons ec2 in In The 1st Workshop on Cloud Computing and its Applications CCA  2008   A Gupta and D Milojicic Evaluation of hpc applications on cloud Open Cirrus Summit  vol 0 pp 22–26 2011   A Marathe R Harris D K Lowenthal B R de Supinski B Rountree M Schulz and X Yuan A comparative study of high-performance computing on the cloud in Proceedings of the 22nd international symposium on High-performance parallel and distributed computing  HPDC 13 New York NY USA pp 239–250 ACM 2013 
808 


  11  Sl o w  Su r v e y  c o l l e c t i o n  w a s t h e  m o st  b a si c  d a t a  acq u i s i t i o n    Fa s t  Su r v e y  d a t a  we r e  r e c o r d e d  wh e n  t h e  p r o b e s  cr o s s ed  sp e c i f i e d  r e g i o n s o f  in te r e s t s u c h  a s  th e  lu n a r  w a k e  re g i o n  o r t h e  d e e p  m a g n e t o s p h e ri c  t a i l  o f  t h e  E a rt h    Fa s t  Su r v e y  c o l l e c t i o n s  w e r e  a l s o  s c h e d u l e d  f o r  t h e  E a r t h  an d  l u n ar  f l y b y s   Bu r s t  d a t a  c o l l e c t i o n  to  c a p tu r e  h ig h re s o l u t i o n  d a t a  f o r sh o r t  p e r i o d s o f  t i m e  d  wh e n  e i t h e r  sc i e n c e   a ba s e d on s e ns or  m e a s ur e m e nt s  w e r e  m e t   or  dur  pr e pr ogr a m m e d pe r i ods  of  t i m e  t ha t  co i n ci d e d   pr e di c t a bl e  c r os s i ngs  of  s c i e nt i f i c  r e gi ons  of  i nt e r e s t   si m u l t a n e o u s d a t a  c o l l e c t i o n  p e r i o d s o n  t h e  ot he r  pr obe s   Sc i e nc e  I ns t r um e nt  C onf i gur at i on  Sc i e n c e  i n s t r u m e n t s  w e r e  c o n f i g u r e d  f o r  d a t a  a cq u i s i t i o n  o u t  on boa r d s e que nc e  t a bl e s  t we r e  u p l o a d e d  o n  a  we e k l y  ba s i s  a nd n   fl i g h t  s o ft w a r e  sc r i p t s l o a d e d  i n t o  th e  I n s tr u m en t  D at a P r o ces s i n g  U n i t   I D P U     co n f i g u r at i o n s  an d  cal i b r at i o n  t ab l e u p d at es  w er e p er f o r m ed  ou nd c om m a ndi ng  Te l e m e t r y  R e q u i r e m e n t s  Wh e n  A R T E M I S  w a s  p r o p o s e d   t h e  m a x i m u m  a v a i l a b l e  te le m e tr y  d a ta  r a te  w a s  p r e d ic te d  to  b e  6 5 5 3 6  k b p s  W ith  a  dow nl i nk t i m e  of  3 5 h e ve r y ot he r  da y  t he  e xpe c t e d da t a  vol um e  w a 8 0 M bi t s  pe r  da y  Ac t u a l  r e s u l t s  f o r  t tw o y e a r s a r e s u m m a r iz e d in T a b le 3   Pa s s  S c h e d u l i n g  a n d  D a t a  Re c o v e r y  Co m m u n i c a t i o n s  p a s s e s  w e r e  s c h e d u l e d  a c r o s s  m u l t i p l e  ne t w or ks    me n t i o n e d  e a r l i e r   F o r  D S N  s c h e d u l i n g    pr ovi de d  st a n d a r d i z e d  so f t w a r e  t o o l s to  in te r f a c e  w ith  th e  DS N sc h e d u l i n g  sy st e m   Ex e c u t i o n  o f  m u l t i mi s s i o n  pa s s  s c he dul e s  a t  t he  M S O C  w a s  ba s e d on hi ghl y a ut om a t e d s   4   Ta b l e  3 S  Sc i e nc e  O pe r a t i o ns  M e t r i c s   Pa r a m e t e r  2012  2013  P1  I n s t r u m e n t  Ef f i c i e n c y  5  9  P1  D a t a  R e c o v e r y  Ef f i c i e n c y  0  7   Av e r a g e    Da t a  Vo l u me  s  s  P1  R e c o v e r e d  D a t a  V o l u m e  ab o v e R eq u i r em en t s  0  8  P2  I n s t r u m e n t  Ef f i c i e n c y  9  8  P2  D a t a  R e c o v e r y  Ef f i c i e n c y  8  8  P2    Da i l y  me  424 5 M bi t s  s  P2  R e c o v e r e d  D a t a  V o l u m e  ab o v e R eq u i r em en t s  7  1  AR T E M I S  a c h i e v e d  e x c e l l e n t  m e t r i c s  f o r  b o  in s tr u m e n t a n d  d a ta  r e c o v e r y  e f f ic ie n c ie s  In s t ru m e n t  ef f i ci en cy  m et r i cs  t ak e i n t o  acco u n t  m an d at o r y  i n s t r u m en t  dow nt i m e  dur i ng m a ne uve r  o pe r a t i ons  or  l ong s ha dow s   a nd ef f ect i v e d o w n t i m es  d u e t o  o n boa r d r e c or de r  s a t ur a t i on   due  t o a nom a l i e s  A s  s h o w n  in  T a b le  3  e x p e c ta tio n s  f o r  th e  nom i na l  da i l y da t a  vol um e  of  380 M bi t s  we r e  s i g n i f i c a n t l y  ex ceed ed  i n  b o t h  2 0 1 2  an d  2 0 1 3   Dy n a m i c  l in k  c a lc u la tio n s  to  p r e d ic t th e  te le m e tr y  lin k  m a r g in  a n d  to  m a x im iz e  th e    we r e  c o n s t a n t l y  r e f i n e d   a n d  a u t o m a t e d  g r o u n d   pr oc e dur e s  w e r e  i m pr ove d t o obt a i n t he s e  r e s ul t s   Gr o u n d  Da t a  P r o c e s s i n g  a n d  A r c h i v i n g  G ro u n d  p ro c e s s i n g  a n d    an d  en g i n eer i n g   S Lo w  a n d  A R TEM I S  w a s  si m i l a r  t o  t h e  TH EM I S  p r i m e  m i s s i o n   S o f t w a r e  t o o l s  co n t i n u ed  t o  al l o w  da t a  a na l ys i s  a c r os s  a  s i ngl e  co n s t el l at i o n  a s  w e ll a s   ons   9   E S  AND L ES S O N S  L ED  Th e  a m b i t i o u s  a n d  c o m p l e x  A R TEM I S  m i s s i o n  e x t e n s i o n  pr ovi de d oppor t uni t i e s  t o e xpl or e  ne w  na vi ga t i on r e gi  As  a  r e s u l t   a  num be r  of   ex p er i en ces  w er e m ad e an d  v al u ab l e l es s o n s  w er e l ear n ed   S o m e o f  t h es e ar e n o t ed  he r e   r s  ar e d es cr i b ed  i n  t h e r ef er en ced  l i t er at u r e  Te a m  C o o r d i n a t i o n  C oor di na t i on a nd c ol l a bor a t i on of  t he  A R T E M I S  t e a m  acr o s s  m u l t i p l e  ma j o r  o r g a n i z a t i o n s  w o r k e d  v e r y  w e l l  U C B  wa s  t h e  l e a d  i n s t i t u t i o n  a n d  c o n d u c t e d  n u m e r o u s  t e l e p h o n e  co n f er en ces   g ro u p  m e e t i n g s   a n d  re v i e w s  o f  n a v i g a t i o n    Mi s s i o n  R e a d i n e s s  T e s t i n g  A m i s s i o n  r e a d i n e s s  t e s t  c a m p a i g n  wa s  p l a n n e d  a n d  ex ecu t ed  p r i o r  t o  t h e A R T E M I S  m i s s i o n  s t ar t   an d  ag ai n  i n  pr e pa r a t i on f or  t he  c r i t i c a l  L O I  s e que nc e s   A c t i vi t i e s  ne e de d  to  b e  in te r l eav ed  w i t h  o n g o i n g  o p er at i o n s  T h e  p r o c e s s  wo r k e d  v e r y  we l l   a n d  t h e  r e q u i r e d  p r o c e d u r e s  we r e  su c c e ssf u l l y  e x e c u t e d   Na v i g a t i o n  O p e r a t i o n s  RCS  d e s i g n s  s h o u l d  a l l o w  t h r e e  d i f f e r e n t  m e t h o d s   n a m e l y  fu e l  b o o k k e e p i n g     an d  t h er m al  g au g i n g   us ed  f o r  d et er m i n i n g  t h e am o u n t  o f  l i q u i d  i n  a h y d r azi n e pr ope l l a nt  t a nk fr o m  t h e  b e g i n n i n g  o f a  m i s s i o n   3   Al s o   s pa c e c r a f t  bus e s  s houl d be  de s i gne d t o be t t e r  s uppor t  pr ope l l a nt  c ondi t i oni ng  s uc h a s  pr ovi di ng c ont r ol  ove r   he a t e r  t ur n on of f  f unc t i o ns   a s  oppos e d t o s ol e l y r e l yi ng on ha r dw a r e  t he r m os t a t s   St at i onk e e pi ng i n L unar  L i br at i on  Or b i t s  Mi s s i o n  C o n s t r a i n t s  St a t i o n k e e p i n g  ope r a t i ons   in to  a c c o u n t c o n s tr a in ts  su c h  a s l i m i t a t i o n s o n  m a n e u v e r  n  due  t o t hr us t e r  l oc a t i ons  a nd R C S  pe r f or m a nc e   Fr e q u e n t  T r a c k i n g  n  poi nt  or bi t s  a r e  ve ry  s e n s i t i v e  to  p e r tu r b a tio n s  T w o wa y  Do p p l e r  a n d  r a n g i n g  t ra c k s   th e r e f o r e  be  s c he dul e d f r e que nt l y t o di s c ove r  or bi t   an d  s p acecr af t  an o m al i  as  ear l y  as  p o s s i b l e  


  12   wi t h  t i m e  t o  r e c o v e r   Pr o p e l l a n t  U s a g e  M  n   he  re q u i re d   V  fo r  r e l i a b l e  s t a t i o n k e e p in g  w ith  r e a lis tic a lly  m o d e le d  na vi ga t i on e r r or s  onl y a m o unt e d t o  5 m  s  pe r  ye a r  w hi c h wa s a n  o r d e r  o f  m a g n i t u d e  l o w e r   pr e vi ous  s t udi e s  ha d in d ic a te d   Pa r t  o f  t h e  r e a s o n  f o r  t h i s  s u r p r i s i n g  r e s u l t  m a y  b e  re l a t e d  t o  t h e  f a c t  t h a t  t h e  o p t i m i z a t i o n  a l g o ri t h m  u s e d  f o r  st a t i o n k e e p i n g  m a n e u v e r  p l a n n i n g  a l w a y s p l a c e d  th e  m a n e u v e r s  a lo n g  a  s ta b le  e ig e n m o d e  o f  th e  m u l ti body en v i r o n m en t  T h is  te c h n iq u e  f a v o r e d  c o n tin u a tio n  o f  a  st a b l e  o r b i t  r a t h e r  t h a n  t a k i n g  a c t i o n s t o  a v o i d  u n st a b l e  co n d i t i o n s F u r th e r a n a ly s e s a r e s till o n g o in g  1 6   Ri s k  M a n a g e m e n t  S om e  of  t he  on d Fa u l t  D e t e c t i o n  a n d  C o r r e c t i o n   FD C   me c h a n i s ms  a n d  te le m e tr y d  lim it m o n ito r s   pur pos e l y di s a bl e d  to  r e d u c e  r is k s  o f  m is s in g  cr i t i cal  m an eu v er s  due  t o a  f a l s e  t r i p  Cr i t i c a l  E v e n t  Co v e r a g e  Fo r  b o t h  L O I  ev en t s  t h e ag r eem en t  w i t h  N wa s  t o  sc h e d u l e  t w o  3 4 m a n t e n n a s     as  p r i m ar y  an d   se c o n d  a s a  h o t  ba ck up   as  L ev el  3  ev en t s   o n e l ev el  ab o v e r o u t i n e o p er at i o n s   o r  L ev el  4  a s  oppos e d t o sc h e d u l i n g  a   DS N s t a t i o n  a t  L e v e l  2  o r  e v e n  L  Th i s  a p p r o a c h  w a s  c o n s i d e r e d  a  le s s  r is k y  a n d  m o r e  c o s t ef f ect i v e s o l u t i o n  a n d  w o r k e d  v e r y  w e ll   It  pa i d of f  d ng th e  c o n tin u o u s  p r e LO I  t r a c k  f o r     th e  p r im e  an t en n a  D S S 54  l os t  i t s  upl i nk due  t o a  pr obl e m  w i t h t he  gr ound s t a t i on t r a ns m i t t e r   W hi l e  t e l e m e t r y m oni t or i ng co n t i n u ed  v i a D S S k up a nt e nna   D S S 65  r a i s e d th e u p lin k c a r r ie r a n d p r o v id e d c o m m a n d s u p p o r t  Pa s s  S c h e d u l i n g  AR T E M I S  u s e d  ne t w or k a s s e t s  f r om  f i ve  di f f e r e nt  ks  P a s s  s c h e d u lin g  f u n c tio n s  we r e  t h e r e f o r e  co m p l ex   an d  w e re  h a n d l e d  b y  th e  T H E M I S A R T E M I S  ope r a t i ons  t e a m  i n c ol l a bor a t i on w i t h m ul t i pl e r em o t e sc h e d u l i n g  o f f i c e s   2 4    co n s t r ai n t s  on t he  DS N d u e  t o  r e s o u r c e  c o n t e n t i o n  we re  re s o l v e d  i n  n e g o t i a t i ons   pa r t i c ul a r l y w i t h s  r  lu n a r  m is s io n s  n a m e ly  th e  L u n a r  R e c o n n a  Or b i t e r   L R O   t h e  Gr a v i t y  R e c o v e r y  a n d  I n t e r i o r  La b o r a t o r y   G R A I L   a n d  t h e  Lu n a r  A t m o s p h e r e  a n d  D u s t  En v i r o n m e n t  Ex p l o r e r   LA D EE  t h a t  o p e r a t e d  in  th e  s a m e  se c t o r  o f  sp a c e   Spac e c r af t  A nom al i e s  Th e  t w o  A R TEM I S  p r o b e s  e x p e r i e n c e d  n o  r a d i a t i o n  r e l a  an o m al i es  s i n ce l eav i n g  E ar t h  an d   ra d i a t i o n  b e l t s  b e h i n d   Ho we v e r   b o t h  p r o b e s  s a w a n o m a l i e s  in  th e  S S T  d e te c to r s  wh e n  p a s s i n g  t h r o u g h  pe r i s e l e ne  a t  l ow  l una r  a l t i t ude s   T he  de t e c t or s  dr e w  m or e  c ur r e nt   t r i ppi ng t he  hi gh c ur r e nt  l i m i t s  of  t he  l ow v a ge  pow e r  s uppl i e s  T h e s e  p r o bl e m s  s ubs i de d    in  f r o n t o f  th e  S S T  a p e r tu r e s   y  cl o s ed  d u r i n g  pe r i s e l e ne  pa s s a ge s   i n itia l th e o r y  p o s tu la tin g  th e  p r o b le m  w a s  cau s ed  b y  a la r g e  a m o u n t o f  s tr a y  lig h t f r o m  th e  f u ll m o o n  en t er i n g  t h e r  ap er t u r e s  co u l d  n o t  b e co n f i r m ed  b y  a co r r el at i o n  an al y s i s   An o t h e r  t h e o r y  t h a t  s t i l l  n e e d s  t o  b e  c o n f i r m e d  i s  th a t d u s t p a r tic le s  in  th e  lu n a r  v ic in ity  e   de t e c t or s  a nd c a us i ng t he  e l e c t r i c a l  c ur r e nt s  t o r i s e   Co n s t e l l a t i ons  In  g e n e ra l  t e rm s   a l l  f i v e  p ro b e s  w e re  a l w a y s  t re a t e d  a s   a s i n g l e co n s t el l at i o n   al t h o u g h  w i t h  d i f f er en t  ne t w or k i nt e r f a c e s  a nd a dj us t e d ope r a t i ons  pr oc e dur e s   T hi s  ap p r o ach  w o r k ed  v er y  w el l   as  t h e m i mi s s i o n  t e a m co n t i n u ed  t o  ope r a t e  th e  e n tir e  c o n s te lla tio n   in te g r a te d  o p e r a tio n s  f a c ility   AR T E M I S  n a v i g a tio n  a n d  sc i e n c e  o p e r a t i o n s h a d  t o  b e  in te r le a v e d  w ith  th o s e  f o r   ongoi ng T H E M I S  m i ssi o n   A s sh o w n  in  T a b le  4 t he  t ot a l  th r u s t m a n e u v e r  c o u n t e x e c u te d  to  d a te  s ta n d s  at  6 3 1  f o r  al l  fi v e  p r o b e s  c o m b i n e d   4 T H E M I S  A R T E M I S M a n e u v e r M e t r ic s   Mi s s i o n  P h a s e  Ma n e u v e r  C o u n t  TH EM I S Pr i m e  M i s s i o n  297  S  182  S w  152  l  631  10   S ARY  To  d a t e   t w o  o f  t h e  o r i g i n a l  f i v e  TH EM I S  p r o b e s  t h a t   AR T E M I S  s p e n t  m o r e  t h a n  f o u r  i  Ea r t h  o r b i t s   L una r  t r a ns f e r  ope r a t i ons  o  s   st a r t  t o  f i n i sh   a n d  2  5  ad d i t i o n al  ye a r s  w e r e  s pe nt  t o c ol l e c t  sc i e n c e  d a t a  i n  l u n a r  o r b i t s  T h e  j o i n t  A R T E M I S  on an d  o p er at i o n s  m s  s sf u l l y  p l a n n e d  a n d  e x e c u t e d  t h e  co m p l ex  n av i ga t i on t a s ks  w i t hout  a ny f l a w s   AR T E M I S  r e p r e s e n t s  t h e  f ir s t e v e r  ope r a t i on of  sp a c e c r a f t  th n     ov id  bl e ex p er i en ce s  an d  l es s o n s  l ear n ed  fo r  fu t u r e  l u n a r  a n d   so la r  s y s te m  e x p lo r a tio n  m is s io n s  th a t m a y  u s e  lib r a tio n   or bi t s  f or  s t a gi ng poi nt s   or  a s  t r a ns por t a t i on wa y p o i n t s  a n d  s a f e  ha ve n f or  c r e w s   As  pa r t  of  N A S A  s  H e l i ophys i c s  G r e a t  O bs e r va t or y  S  de l i ve r e d   sc i e n c e  c o m m u n i t y  t t ic  tw o nt  obs e r va t i ons  of  t he  Ea r t h  s   ma g n e t o s p h e r i c  t a i l   t h e  s o l a r  w i n d   a n d  t h e  l u n a r  s p a c e  a n d  pl a ne t a r y e nvi r onm e nt   t  in v e s tig a tio n s  b e tw e e n  A R T E M   LR O   a n d  LA D EE  ga ve  s c i e nc e  t e a m s  uni que  oppor t uni t i e s  t o s t udy t he   at m o s p h er e an d  t h e dyna m i c s  of  t he  l una r  e xos phe r e  a nd dus t  e nvi r onm e nt   Th e  t w o   pr obe s  a c c ur a t e l y me a s u r e d   ups t r e a m  a nd ne a r by s ol a r  w i nd  re l a t i v e  t o  t h e  Mo o n  s  l o c a t i o n  an d  p r o v i d e d  pl a s m a  c ondi t i ons  d  ma g n e t o s p h e r i c  t a i l  d r i v e r s  t o  th e o th e r tw o m is s io n s   


  13  R EF ER EN C ES   1  V  An g e l o p o u l o s    Th e  TH EM I S  M i s s i o n     S ci en ce R ev i ew s   V o l   1 4 1   1 5 34   2  S  Fr e y   V   A n g e l o p o u l o s   M   B e s t e r   J   B o n n e l l   T   Ph a n   a n d  D   R u m m e l    O r b i t  D e s i g n  f o r  t h e  T H E M I S Mi s s i o n    S p a c e  S c i e n ce R ev i ew s   V o l   1 4 1   N o   1  2008  pp  61 89  3  M  B e s t e r   M  L e w i s   B   R o b e r t s   J   Mc D o n a l d   D   Pe a s e   J   T h o r s n e s s   S  Fr e y   D   C o s g r o v e   a n d  D   Ru m m e l    T H E M I S  O p e r a t i o n s    S p a c e  S c i e n c e  Re v i e w s   V o l   1 4 1   N o   1 91 115  4  V  An g e l o p o u l o s    T h e A R T E M I S  M i s s i o n   S p ace Sc i e n c e  R e v i e w s   V o l   1 6 5   N o   1 4  2011  pp  3 25  5  M  B e s t e r   M  L e w i s   B   R o b e r t s   L   C r o t o n   R   D u m l a o   M  E c k e r t   J   Mc D o n a l d   D   P e a s e   C   S m i t h   J   Th o r s n e s s   J   W h e e l w r i g h t   S   F r e y   D   C o s g r o v e   D   Ru m m e l   M   L u d l a m   H   Ri ch ar d   T   Q u i n n   J   L o r an   R  Bo y d   C  Q u a n   a n d  T   Cl e m o n s    Gr o u n d  S y s t e m s  an d  F l i g h t  O p er at i o n s  o f  t h e T H E M I S  C o n s t el l at i o n  Mi s s i o n   Pr o c e e d i n g s  o f  t h e  2008 I E E E  A e r os pa c e  Co n f e r e n c e   E d  Br y a n   e d     Bi g  S k y   M T   M a r c h  1  2008  P a pe r  12 0502  6  B  Ro b e r t s   M   L e w i s   J   T h o r s n e s s   G   P i c a r d   G   Le m i e u x   J   M a r c h e s e   D   C o s g r o v e   G   G r e e r   a n d  M   Be s t e r    T H E M I S  M i s s i o n  N e t w o r k s  E x p a n s i o n    Ad d i n g  t h e  De e p  S p a c e  Ne t wo r k  f o r  t h e  AR T E M I S  Lu n a r  M i s s i o n  P h a s e    P r o c e e d i n g s  o f  t h e  AA 2010 Sp a c e O p s  C o n fe re n c e   H u n t s v i l l e   A L   A p ri l  2 5  2010  P a pe r  A I A A  2010 1934  7  M  S h o l l   M  L e e d s   a n d  J   H o l b r o o k   T H E M I S  Re a c t i o n  Co n t r o l  S y s t e m    Fr o m  I  T  t h r o u g h  E a r l y  Mi s s i o n  O p e r a t i o n s    P r o c e e d i n g s  o f  t h e  4 3 rd  AI AA AS M E  S AE  AS E E  J o i n t  P r o p u l s i o n  C o n f e r e n c e  x hi bi t   C i nc i nna t i   O H   J ul y 8 2007  8    Br o s c h a r t   M  K    Ch u n g   S    Ha t c h   J     T   Sw e e t s e r     We i n s t e i n We i s s   a n d  V   An g e l o p o u l o s    P r e l i m i n a r y  T r a j e c t o r y  De s i g n  f o r  t h e  AR T E M I S  L u n a r  M i s s i o n    P r o c e e d i n g s  o f  t h e  AA As t r o d y n a m i c s  S p e c i a l i s t  C o n f e r e n c e   Pi t t s b u r gh  P A   A ugus t  9 13  2009  P a pe r  A A S  09 382  9  D  F o l t a   M   W o o d a r d   T   S we e t s e r   S   B   B r o s c h a r t   an d  D   C o s g r o v e   D es i g n  an d  I m p l em en t at i o n  o f  t h e AR T E M I S  L u n a r  T r a n s f e r  T r a j e c t o r y  Us i n g  M u l t i Bo d y  D y n a m i c s    P ro c e e d i n g s  o f t h e  AA As t r o d y n a m i c s  S p e c i a l i s t  Co n f e r e n c e   G i r d w o o d   A K   Au g u s t  2   2 0 1 1   P a p e r  AAS  1 1 511  0  G  J   W h i f f e n  a n d  T   H  S we e t s e r    E a r t h  Or b i t  R a i s e  De s i g n  f o r  t h e  AR T E M I S  M i s s i o n    P r o c e e d i n g s  o f  t h e  AI AA AAS  As t r o d y n a m i c s  S p e c i a l i s t   Mi n n e a p o l i s   MN   A u g u s t  1 3 16  2012  P a pe r  A I A A  2012 4427  1  D  C o s g r o v e   S   F r e y   J  M a r c h e se   B   O w e n s  S   Ga n d h i  M  B e s te r   Fo l t a   M   W o o d a r d   D   Wo o d f o r k   Na v i g a t i n g  T HE M I S  t o  t h e  AR T E M I S  w En e r g y  Lu n a r  Tr a n s f e r  Tr a j e c t o r y    g s of  t he  A I A A  2010 S pa c e O ps  C onf e r e nc e   H u  AL   Ap r i l  2 5 30  2010  P a pe r  A I A A  2010 2352  2  J  H a sh m a l l   D   F e l i k so n   a n d  J  S e d l a k    U se  o f  Fu z z y c o n e s  f o r  Su n onl y A t t i t ude  D e t e r m i na t i on  TH EM I S  B e c o m e s  A R TEM I S    P r o c e e d i n g s  o f  t h e  2 1 st  In t e rn a t i o n a l  Sy m p o s i u m  o n  Sp a c e  Fl i g h t  D y n a m i c s   To u l o u s e   F r a n c e   S e p t e m b e r  2 8    Oc t o b e r  2   2 0 0 9   3  B  D   O w e n s   D   Co s g r o v e   M   S h o l l   a n d  M   Be s t e r   n Or b i t  P r o p e l l a n t  E s t i m a t i o n   M a n a g e m e n t   a n d  Co n d i t i o n i n g  f o r  t h e  T H E M I S  S p a c e c r a f t  Co n s t e l l a t i o n    P r o c e e d i n g s  o f  th e  A I A A  2 0 1 0  Sp a c e O p s  C o n f e r e n c e   H u n t s v i l l e   A L   A p r i l  2 5  2010  P a pe r  A I A A  2010 2329  4  J  E   M a r c h e se   B   D   O w e n s  D   C o sg r o v e   S   F r e y   a n d  M  B e s t e r     C a l i b r a t i o n  o f  I n Fl i g h t  M a n e u v e r  Pe r f o r m a n c e  f o r  t h e  T H E M I S a n d  A R T E M I S M i s s i o n  Sp a c e c r a f t    Pr o c e e d i n g s  o f  t h e  A I A A  2 0 1 0  Sp a c e O p s  Co n f e r e n c e   H u n t s v i l l e   A L   A p r i l  2 5 30  2010  P a pe r  AI AA 2 0 1 0 2120  5  D  C   F o l t a   T   A  P a v l a k   K  C   Ho we l l   M   A  Wo o d a r d   a n d  D   W  Wo o d f o r k    S t a t i o n k e e p i n g  o f  Li s s a j o u s  Tr a j e c t o r i e s  i n  t h e  Ea r t h Mo o n  S y s t e m  w i t h  A ppl i c a t i ons  t o A R T E M I S    Sp a c e f l i g h t  M e c h a n i c s  2010  V ol   136  P a r t  I   A dva nc e s  i n t he  A s t r ona ut i c a l  Sc i e n c e s   p p   1 9 3 208  6   M  W o o d a r d   a n d  D   C o s g r o v e   S t a t i o n k e e p i n g  o f  t h e  F i r s t  E a r t h Mo o n  L i b r a t i o n  Or b i t e r s   T h e  AR T E M I S  M i s s i o n    Pr o c e e d i n g s   AAS  AI AA As t r o d y n a m i c s  S p e c i a l i s t   d   g t   Pa p e r  1 515  7    Owe n s   J    Ma r c h e s e   D    Co s g r o v e   S   F r e y     Be s t e r    O p t i m i z i n g  A RT E M I S  L i b r a t i o n  Po in t O r b it S ta tio n k e e p in g  C o s ts  T  Pe rfo rm a n c e  C a l i b ra t i o n    P ro c e e d i n g s  o f t h e  2 2 nd  AAS  AI AA S p a c e  F l i g h t  M e c h a n i c s  M e e t i n g   Ch a r l e s t o n   S C  J a n u a r y  2 9    Fe b r u a r y  2   2 0 1 2   Pa p e r  2 182  8  M  W o o d a r d   D   C o s gr ove   P   M or i ne l l i   J   M a r c he s e  B  O w e n s   a n d  D   F o l t a    O r b i t  D e t e r m i n a t i o n  o f  Sp acecr af t  i n  E ar t h Mo o n  L 1  a n d  L 2  L i b r a t i o n  P o i n t  Or b i t s    Pr o c e e d i n g s  o f  t h e  AAS  AI AA As t r o d y n a m i c s  t  Co n f e r e n c e   G i r d w o o d   A K   A u g u s t  2   2 0 1 1   Pa p e r  A A S 1 1 514  


  14  9  J  E   M a r c h e se   D   C o sg r o v e   M   W o o d a r d   D   F o l t a   P   Mo r i n e l l i   B   D   O w e n s   S   F r e y   a n d  M  B e s t e r    Op t i m i z i n g  S o l a r  R a d i a t i o n  C o e f f i c i e n t  a s  a  S o l v e r Pa r a m e t e r  f o r  t h e  O r b i t  D e t e r m i n a t i o n P r oc e s s  dur i ng th e  L ib r a tio n P oi nt  O r bi t  P ha s e  of  t he  A R T E M I S  n   P r o c e e d in g s  o f  t 22 nd  AAS  AI AA S p a c e  Fl i g h t  M e c h a n i c s  M e e tin g  C h a r le s to n  S C  Ja n u a r y  2 9   Fe b r u a r y  2   2 0 1 2   Pa p e r  A A S 1 2 183   0  B  O w e n s   D   Co s g r o v e   J   M a r c h e s e   J   Bo n n e l l   D   Pa n k o w   S  Fr e y   a n d  M   B e s t e r    M a s s  E j e c t i o n  An o m a l y  i n  L i s s a jo u s  O r b i t   R e s p o n s e  a n d  I m p l i c a t i o n s  fo r t h e  A R T E M IS  M is s io n   P r o c e e d in g s  o f  th e  2 2 nd  AAS  AI AA S p a c e  F l i g h t  M e c h a n i cs  M eet i n g   Ch a r l e s t o n   S C  J a n u a r y  2 9    Fe b r u a r y  2  2012  P a pe r  2 181  1  S   Br o s c h a r t   T    Sw e e t s e r   V   A n g e l o p o u l o s   D        Wo o d a r d    A R T E M I S  L u n a r  O r b i t  In s e rt i o n  a n d  S c i e n c e  O rb i t  D e s i g n  T h ro u g h  2 0 1 3    Pr o c e e d i n g s  o f  t h e  AAS  AI AA As t r o d y n a m i c s  Sp e c i a l i s t  Co n f e r e n c e   G i r d w o o d   A K   A u g u s t  2   2 0 1 1   Pa p e r  A A S 1 1 509  2  D  C o s g r o v e   S  Fr e y   J   M a r c h e s e   B   O w e n s   an d  M    AR T E M I S  Op e r a t i o n s  f r o m  E a r t h  Li b r a t i o n  O r b i t s  t o  S t a b l e  Lu n a r  O r b i t s  Pr o c e e d i n g s  o f  e  AA 2012 S pa c e O ps  C onf e r e nc e   St o c k h o l m   Sw e d e n   J u n e  1 1   Pa p e r  A I A A  2012 1296179   3  J  E   M a r c h e se   D   C o sg r o v e   S   F r e y   a n d  M   B e st e r    a nni ng a nd E xe c ut i on of  a  S pe c i a l i z e d M a ne uve r  f or  th e  A R T E M I S  M is s io n  A c h ie v in g  T h r e e  G o a ls  w ith  On e  S e q u e n c e   P r o c e e d in g s  o f  th e  23 rd  AA Sp a c e  Fl i g h t  M e c h a n i c s  M e e t i n g   K a u a i   H I   Fe b r u a r y  10 14  2013  P a pe r  A A S  13 401  4  M  B e s t e r   G  P ic a r d  B  Ro b e r t s   M   L e w i s   a n d  S   Fr e y    i Mi s s i o n  S c h e d u l i n g  O p e r a t i o n s  a t  U C  Be r k e l e y   P r o c e e d in g s  o f  th e  2 0 1 3  I n te r n a tio n a l Wo r k s h o p  o n  P l a n n i n g    S c h e d u l i n g  f o r  S p a c e   Mo u n t a i n  V i e w   C A   Ma r c h  2 5 2013  B   Ma n f r e d  r  re c e i v e d  a  d o c t o ra t e  i n  Ph y s i c s  f r o m  t h e  U n i v e r s i t y  o f  C o l o g n e   y   in  1 9 8 4  wi t h  t h e s i s  wo r k  i n  r wa v e  s p e c t r o s c o p y  a n d  r a d i o    jo in e d  L  in  1 9 8 6  and c ur r e nt l y  hol ds  a pos i t i on as  D i r e c t or  of  O pe r at i ons   m anagi ng m i s s i on and  sc i e n c e  o p e ra t i o n s  n a v i g a t i o n  a n d  g ro u n d  sy st e m s f u n c t i o n s  H e  a l so  se rv e s a s M i ssi o n  O p e ra t i o n s Ma n a g e r  f o r  T H E MI S   A R T E MI S   N u S T A R   a n d I C O N   H e  is  a  m e m b e r  o f th e  AAS   AS P  O S A  and SP I E    me mb e r  o f  t h e  A I A A   o or gani z e r  of  T r ac k  12  Gr o u n d  a n d  S p a c e  Op e r a t i o n s   at  t he  I E E E  A e r os pac e  Co n f e r e n c e   Da n i e l  C o s g r o v e  re c e i v e d  a  B S  i n  P h y si c s fr o m  U C  S a n ta  C r u z  in  2 0 0 1  H e  w o r k e d  at  e  Sant a C r uz  I ns t i t ut e  f or  P ar t i c l e  Ph y s i c s  o n  s u b a t o m i c  p a r t i c l e  d e t e c t o r s  f o r  th e  A T L A S  e x p e r im e n t o n  th e  L  Ha d r o n  C o l l i d e r  at  C E R N  I n  2 0 0 4  h e   L  p r o v id in g  o r b it a n d  at t i t ude  de t e r m i nat i on anal y s i s  and s uppor t  f or  th e  T H E M I S  n  H e  b e c a m e  th e  TH E M I S  a n d  A R TE M I S  N a v i g a t i o n  L ea d  i n  2 0 0 7   Sa b i ne  F r e y  v ed  h er  d o ct o r a t e   Ph y s i c s   th e  U n iv e r s ity  o f L e ip z ig  Ge r ma n y  5  A fte r  a  fe w  y e a r s  o f em p l o ym en t  a t  t h e u n i ver s i t y a n d  i n  in d u s tr y  sh e  st a rt e d  h e r e  eer  in  1 9 9 3  a t th e  M a x Pl a n c k  fo r  E x tr a te r r e s tr ia l P h y s ic s    Ga r c h i n g y  I n  1 9 9 8  s h e  jo in  UC B  S S L  k  on t he  C l us t e r II a n d  L u n a r  P r o s p e c t o r  p r o j e c t s   and de v e l ope d s of t w ar e  t o anal y z e  aur or a and ai r gl ow  obs e r v at i ons  I n  2 0 0 4  s h e  b e c a m e  th e  M is s io n  D e s ig n  L  fo r  T H E M I S  a n d  w o r k e d  s e   on t he  T H E M I S and ART EM I S  m i s s i o n s   Je f f r e y  e  re c e i v e d  h i s P h  D   i n  Co m p u t a t i o n a l  P hy s i c s  f r om  t he  De p a r t me n t  o f  A p p l i e d  S c i e n c e  a t  UC  Da v i s  i n  2 0 0 7  Si nc e  2008  he  wo r k e d  a s  a  me mb e r  o f  t h e  f l i g h t  d y n a mi c s  t e a m a t  UC B  S S L   f o c u s i n g  on nav i gat i on  or bi t   and at t i t ude  de t e r m i nat i on  m os t l y  co n cer n i n g  t h e T H E M I S  a n d  A R T E M I S  m i s s i o n s   Aa r o n  Bu r g a r t  re c e i v e d  a  M S  i n  Ae r o n a u t i c a l  a n d  As t r o n a u t i c a l  En g i n e e r i n g  f r o m  Pu r d u e  U n i v e r s i t y  i n  2011  In  2 0 1 2  h e  j o i n e d   a Fl i g h t  D y n a m i c s  An a l y s t   p r o v i d i n g  i tu d e  d e te r m in a tio n  a n a ly s is  a n d  su p p o rt  f o r t h e  T H E M I S  a n d  A R T E M I S  mi s s i o n s   s  is  th e  M is s io n  O p e r a tio n s  Ma n a g e r  f o r  t h e  N A S A  R H E S S I  m i s s i o n   and t he  D e put y  M i s s i on O pe r at i ons  Ma n a g e r  f o r  t h e  T H E MI S   A R T E MI S   Nu S T A R   a n d  I CO N m i s s i o ns  at  L   He  s t u d i e d  C o mp u t e r  E ngi ne e r i ng at  o n U ni v e r s i t y  and w or k e d  in  m is s io n   and as  P ow e r    T he r m al  E  si n c e  t h e  1 9 9 2  l a u n c h  o f  t h e  E x t re m e  U l t ra v i o l e t  Ex p l o r e r   EU VE   He  al s o w as  th e  M is s io n  O p e r a tio n s  n   FAS T  a n d  C H I PS  mi s s i o n s   Br y c e  Ro b e r t s  re c e i v e d  a  B S  i n  i ca l  E n g i n eer i n g  f r o m  U C B   1996  H e  got  hi s  s t ar t  i n s pac e  f l i ght  ope r at i ons  i n 1996 w he n he  j oi ne  ope r at i ons  s t af f  of  U C B s  C e n t e r  f o r  Ex t r e m e  U l t r a v i o l e t  As t r o p h y s i c s    


  15  a m i s s i on pl anne r and so f t w a re  d e v e l o p e r f o r N A S A  s E U V E  mi s s i o n   I n  1 9 9 8  h e  mo v e d  t o  t h e  J o h n s  Ho p k i n s  Un i v e r s i t y  as  a s of t w ar e  de v e l ope r  and m i s s i on pl anne r  f or  t he  F ar  Ul t r a v i o l e t  S p e c t r o s c o p i c  E x p l o r e r   F US E   s a t e l l i t e   a n d  wa s  a  k e y  m e m b e r  o f  t h e  t e a m  t h a t  d e v e l o p e d  g r o u n d d so f t w a re  t e c h n i q u e s t o  re st o re  t h e  sa t e l l i t e  s a t t i t u d e  c o n t ro l  af t e r  s e v e r al  r e ac t i on w he e l  f ai l ur e s  I n  2 0 0 5  h e  r e tu r n e d  to  L  as  a pr ogr am m e r  and gr ound s y s t e m s  e ngi ne e r  wo r k i n g  o n  t h e  T H E M I S   A R T E M I S   F A S T   R H E S S I   Nu S T A R   a n d  I CO N mi s s i o n s   He  i s  a  me mb e r  o f  t h e  A I A A   Je r e m y  T h or s n e s s  st u d i e d  A p p l i e d  P h y si c s a t  U C  D a v i s a n d  st a rt e d  w o rk i n g  i n  sp a c e  f l i g h t  o p e ra t i o n s sh o rt l y  a f t e r gr aduat i on i n 1996  H e  be gan at  B s  C e n t e r  f o r  E x t r e m e  Ul t r a v i o l e t  A s t r o p h y s i c s   C E A   a s  a  f l i g h t  c o n tr o lle r  fo r  th e  Ex t r e m e  U l t r a v i o l e t  Ex p l o r e r   EU VE   I n  1 9 9 8  h e  m o v e d  t o  L  to  w o r k  a s  a  flig h t c o n tr o lle r  a n d  I T O S  d e v e lo p e r  fo r  th e  R e u v e n  R a m a ty  H ig h  E n e r g y  S o la r  S p e c tr o s c o p ic  r  R H E S S I  mi s s i o n   W h i l e  a t  S S L  h e  wo r k e d  o n  t h e  CH I P S   T H E M I S   A RT EM I S   C I N EM A  N u S T AR  a n d  I C O N  mi s s i o n s   Joh n  M c D on al d  re c e i v e d  a  B A  i n  P h y si c s fr o m  U C  S a n ta  C r u z  in  1 9 8 8  H e  w e n t o n  to  w o r k  a t th e  S m ith s o n ia n  A s tr o p h y s ic a l Ob s e r v a t o r y   pr oc e s s i ng dat a f r om   In t e r n a t i o n a l  U l t r a v i o l e t  E x p l o r e r   jo in e d  U C B  in  1991  k  o n dat a pr oc e s s i ng dur i ng t he  pr e la u n c h  a n d  ea r l y m i s s i o n  of  t he  E x t r e me  Ul t r a v i o l e t  E x p l o r e r   E UV E    and s uppor t e d sp a c e c ra f t  o p e ra t i o n s a n d   t   pr ogr am   Af t e r  r e c e i v i n g  h i s  M S  i n  As t r o n o m y  a t  S a n  D i e g o  ni ver s i t y i n  1 9 9 5   h e w o r ked  f o r  t h e S E T I  I n s t i t u t e o n  ea r l y l a b o r a t o r y s t u d i es  f o r  t h e K ep l er  m i s s i o n   He  wa s  a l s o  a t e l e s c ope  ope r at or  f or  t he  C anada e  Te l e s c o p e   at  M auna K e a  H aw ai i   be fo r e  r e tu r n in g  to  B e r k e le y  A t U C B S S L  he  t he n  k  in  m is s io n  ope r at i ons  f or  C H I P S  T H E M I S    and I C O N    we l l  a s  s e r v i n g  a s  I n s t r u m e n t  S u p p o r t  E ngi ne e r  f or  T H E M I S and A R T E M I S  De r o n  P e a s e  re c e i v e d  a  B S  i n  P h y si c s fr o m  U C  S a n ta  C r u z  in  1 9 9 3  fo llo w e d  b y  a  MS  i n  A s t r o n o m y  f r o m  S a n  D i e g o  S t a t e  Un i v e r s i t y i n  1 9 9 6   La t e r  t h a t  y e a r  h e   to  w o r k  a s  p a r t o f th e  s c ie n c e  in s tr u m e n t ca l i b r a tio n  te a m  fo r  N A S A  s  C h a n d r a  X r ay  O bs e r v at or y   he adquar t e re d  a t  t h e  d  Sm i t hs oni an C e nt e r  f or  A s t r ophy s i c s     Ca m b r i d g e   M A   In  2 0 0 7  h e  j o i n e d    i mi s s i o n  o p e r a t i o n s  t e a m  w h e r e  h e  c o n t r i b u t e d  sp a c e  f l i g h t  o p e ra t i o n s and i ns t r um e nt  e ngi ne e r i ng  fo r  th e  N A S A  mi s s i o n s  TH E M I S   ART EM I S   FAS T   Nu S T A R   a n d  I CO N  Gr e g o r y  P i c a r d  re c e i v e d  a  B S  i n  G e o l o g y  f ro m  t h e  Un i v e r s i t y  o f  M i s s o u r i  a t  C o l u mb i a   He  w o r k e d  a s  mi s s i o n  pl anne r  at  t he  A e r os pac e  C or por at i on  s uppor t i ng e  In f r a r e d  B a c k g r o u n d  S i g n a t u r e  S u r v e y  IB S S  p a y l o a d  o n  th e  S T S 39 m i s s i on  L  to  s u p p o r t th e  T O P E X  mi s s i o n   He  j o i n e d  UC B  i n  1 9 9 7  a s  a  ne t w or k  sc h e d u l e r f o r th e  E U V E  m is s io n  A fte r  w o r k in g  a t S ta n fo r d  U n iv e r s ity  o n  Gr a v i t y  P r o b e  B  P B  f r o m  2 0 0 2  h e   re t u rn e d  t o  UC B  S S L  i n  2 0 0 8  a s  a  s c h e d u l e r  f o r  th e  R H E S S I  N u S T A R  TH E M I S   A R TE M I S   C I N E M A   a n d  I C O N  m i s s i o n s   Ma r t h a  E c k e r t  re c e i v e d  a  B S  i n  C o m p u t e r S c i e n c e  w i t h  mi n o r s  i n  P h y s i c s  a n d  A s t r o n o my  fr o m  S o n o m a  S ta te    She  j oi ne d  th e  o p e r a tio n s  st a f f  o f  U C B  Ce n t e r  f o r  E x t r e m e  o t  As t r o p h y s i c s  C E A    wh e r e  s h e  s t a y e d  u n til th e  VE mi s s i o n  e n d e d  i n  2 0 0 1   She  has  be e n at  SSL  e v e r  s i nc e  and w or k e d i n f l i ght  ope r at i ons  w i t h F A S  RH ES S I   T H EM I S   ART EM I S   Nu S T A R    and I C O N   Re n e e  D u m l a o  st u d i e d  S p a c e  S y st e m s s  and s t ar t e d w or k i ng as  a Spac e  Sy s t e m s  O pe r at or  and A c t i v e  D ut y  m e m be r  of  t he  U ni t e d St at e s  A i r  F or c e  i n 2000  I n 2001  sh e  b e g a n  w o rk i n g  i n  t h e  P ro c e d u re s Un i t   d e v e l o p i n g  a n d  ma i n t a i n i n g  a l l  S pac e  Sy s t e m s  doc um e nt at i on f or  o n t ope r at i ons  and mi s s i o n  r e a d y  p e r s o n n e l  fo r  th e  D e fe n s e  S a te llite  C o m m u n ic a tio n s  Sy s t e m   D SC S   NA T O  I I I  a n d  N AT O  I V S K Y N ET  4  c o n s t e l l a t i o n s    th is  tim e  s h e  w a s  a ls o  a n  I n s tr u c to r  a n d  E v a lu a to r  to  va l i d a t e t r a i ni ng t o e ns ur e  c r e w  pr of i c i e nc y   She  t he n w e nt  e rv e  i n  t h e  C a l i f o rn i a  A i r N a t i o n a l  G u a rd  a s a  S p a c e  Sy s t e m s  O pe r at or  f or  t he  M I LS TA R  s a t e l l i t e  c o n s t e l l a t i o n   In  2 0 0 6   s h e  b e g a n  a t  U C B  S S L  to  w o r k  a s  a  flig h t co n t r o l l er  f o r  t h e F a s t  A u r o r a l  S n a p s h o T  E xp l o r er   F A S  mi s s i o n   W h i l e  a t  S S L  s h e  wo r k e d  o n  t h e    M IS   A R T E M IS    CI NE M A    mi s s i o n s   A CK NO W L E DG E M E NT S   Th e  a u t h o r s  w i s h  t o  t h a n k  P r o f   V a s s i l i s  A n g e l o p o u l o s  U C L A   f o rm e rl y  U C B  f o r t h e  o p p o rt u n i t y  t o  p a rt i c i p a t e  i n  th is  e x c itin g  m is s io n  W e  a ls o  th a n k  o u r  es   NAS   Th e o d o r e  S w e e t s e r   S t e p h e n  B r o s c h a r t    Gr e g o r y  W h i f f e n a nd a t  N A S A  G S F C   D a vi d F ol t a   Ma r k  W o o d a r d   Pa t r i c k  M o r i n e l l i  f o r t h e  e x c e l l e n t  te a m  c o lla bor a t i on w i t h m i s s i on de s i gn a nd na vi ga t i on ope r a t i ons  s uppor t  T h e  D S N  te a m  at  J P L  s  i ns t r um e nt a l  in  th e  A R T E M I S  m is s io n  s u c c e s s   We  a l s o  w i s h  t o  t h a nk our  c ol l e a gue s  a t  U C B  n a m e ly  S a m u e l J o h n s o n  T h o m a s  Cl e m o n s   J a m e s  L e w i s   J o n a t h a n  Lo r a n   G r e g o r y  P a s c h a l l   o  d   Cl a r i n a  Q u a n  B r u c e  S a to w  a n d  J a m e s  Mc C a r t h y  fo r  t h e i r  su p p o r t  w i t h  I T  sy st e m s  sc i e n c e  d a t a  pr oc e s s i ng  a nd  ma i n t e n a n c e  o f  t h e  a  in f r a s tr u c tu r e   Tw o  f o r m e r  c o l l e a g u e s   D r   B r a n d o n  O w e n s  n o w  a t  N A S A  A m e s  R e s e a rc h  C e n t e r a n d  G re g o ry  Le m i e u x   n o w  a t  S p a c e  S y s t e m s  Lo r a l    w e r e  i n s t r u m e n t a l  in  th e  s uc c e s s  of  t he   mi s s i o n  b y  p r o v i d i n g  cr i t i cal  n av i g at i o n  an d   ope r a t i ons  s uppor t   TH EM I S  a n d  AR T E M I S  m i s s i o n  ce ope r a t i ons  a r e  co n d u ct ed   th e  U n iv e r s ity  o f  C a lif o r n ia  B e r k e le y   NAS A c o n t r a c t  NAS 5 02099   


  16  A X   Fi g u r e  A 1   Il l u st r a t i o n  o f  ART E M I S  P 1   a  k  a   T H E M I S  B   lu n a r a p p r o a c h  2  e   in E a r t h C e n t e r e d I n e r t ia l E C I  J 2 0 0 0  c o o r d i n a te s   T h e  g r i d  r e p r e s e n ts  th e  e q u a to r i a l  p l a n e    A 2    ART E M I S  P 1  l u n a r  f l y b y   1   k fl i p   a n d  l u n a r  fl y b y   2  i n  E C I  c o o r d i n a te s   


  17   A 3   ART E M I S  P 1  lu n a r  f ly b y   1  b a c k fl i p   a n d  l u n a r  fl y b y   2  i n  m o r e  d e ta i l  i n  E C I  c o o r d i n a te s    we r e  e x e c u t e d  t o  a c c u r a t e l y  t a r g e t  t h e  B pl a ne s  o f  t he  dua l  l una r  f l y by  s c e na r i o   T r a c ki ng  c o v e r a g e  dur i ng  t he  ba c k fl i p  w a s  c r i ti c a l  to  m o n i to r  th e  s e c o n d  fl y b y   W i th  th e  s p a c e c r a ft a n te n n a  m o u n te d  o n  th e  s o u th e r n  s i d e  o f th e  sp a c e c r a f t  b o d y   c o mmu n i c a t i o n s  c o v e r a g e w i t h  t h e D S N  3 4 m s t a t i o n s  h a d  t o  b e  ca ref u l l y  a n a l y zed     Fi g u r e  A 4   ART E M I S  P 1  f i n a l  E a r t h  o r b i t s   t r a n s lu n a r t r a j e c t o r y w it h t w o d e e p  sp a c e  e x c u r si o n s   Ea r t h  f l y b y   an d  f i r s t  r e vol u t i on  of  E M  L 2  lib r a t io n   or b i t  i n  E C I  co o rd i n a t es   


  18   A 5   ART E M I S  P 1  E M  L 2  an d  E M  L 1  lib r a t io n p o in t o r b it s w it h E M L 2  to  E M  L 1  tr a n s fe r   in s e r t io n i nt o  a  r e tr o g r a d e  l u n a r  o r b i t i n  r o ta ti n g  co o rd i n a t es V ie w o f t h e E a r t h Mo o n  p l a n e  f r o m  a b o v e    A 6    ART E M I S  P 1  c o m p l e t e  tr a je c to r y  fr o m  E a r th  d e p a r tu r e  to  l u n a r  o r b i ts  in E C I c o o r d in a t e s   


  19   A 7    ART E M I S  P 2   a  k  a   T H E M I S  C  E a r t h  d e p a r t u r e  w i t h   la s t E a r t h o r b it a n d lu n a r f ly b y in E C I  co o rd i n a t es   Th e  t a b l e s  o n  ft  in d ic a t e a v a ila b ilit y o f  co m m u n i ca t i o n s  l i n k s  w i t h  s u p p o rt i n g  n et w o rk  a s s et s    Fi g u r e  A 8       lib r a t io n p o in t o b it s in  EC I  c o o r d i n a t e s   


  20   A 9   ART E M I S  P 2  c o m p l e t e  t r a j e c t o r y  f r o m  E a r t h  d e p a r t u r e  t o  l u n a r  o r b i t s  in E C I c o o r d in a t e s   A 1 S u m m a r y o f  ART E M I S  M a n e u v e r s  a n d  F u e l  B u d g e t s   Mi s s i o n  P h a s e  Pr o b e  P1   P2    Mi s s i o n  P h a s e   e  at  B eg i n  o f  e  g  l  Ex p e n d e d  b y  En d  o f  Ph a s e  g  To t a l    Ac h i e v e d   e  s    Mi s s i o n  P h a s e   e  at  B eg i n  o f  e  g  l  Ex p e n d e d  b y  En d  o f  Ph a s e  g  To t a l    Ac h i e v e d   e  s  Ea r t h  O r b i t    5    2    4   553  514  436    27  1    SD M  1   2  140  135  477  s r  1    8  039  433  567    3  TC M  2   5  005  372  672  Li b r a t i o n  Po i n t  t    36  606  351  688    31  633  188  486  Lu n a r  O r b i t  1    2  I  PR M  1   5    4  255  427  629    2  LTI  TC M  1  I  PR M  1   5  M  1  4  1    2  445  394  215  Cu r r e n t  S t a t u s    8 28        051        T  At t i t u d e  P r e c e s s i o n  M a n e u v e r  M  Or b i t  R a i s e  M a n e u v e r   M  De e p  S p a c e  M a n e u v e r  M  Pe r i o d  R e d u c t i o n  M a n e u v e r   FTM  Fl y b y  Ta r g e t i n g  M a n e u v e r  M   De f l e c t i o n  M a n e u v e r   I  Lu n a r  O r b i t  I n s e r t i o n  M a n e u v e r  M  St a t i o n k e e p i n g  M a n e u v e r   LTI  Lu n a r  Tr a n s f e r  I n i t i a t i o n  M a n e u v e r  N  Sp i n  R a t e  C o n t r o l  M a n e u v e r   M  Or b i t  M a i n t e n a n c e  M a n e u v e r  M  Tr a j e c t o r y  C o r r e c t i o n  M a n e u v e r   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


