Dimensionality Reduction for Association Rule Mining with IST-EFP Algorithm   Boby Siswanto 1 The Houw Liong 2  and Shaufiah 3  School of Computing Telkom University Bandung, Indonesia Email 1 boby.siswanto@gmail.com 2 thehl007@gmail.com 3 shaufiah@gmail.com    Abstract Frequent itemset generation is the important phase on association rule mining. With frequent itemset dataset association rules will be obtained. The main problems that exist in association rule mining is the use of large computer main memory at the time of the formation of Frequent Itemset. EFP algorithms \(Expand FP-Growth\ overcome this problem by utilizing secondary storage as a processing area to store it in the table object in the database. Data management processes in a database done by using a DBMS \(Database Management System\. The database used is Oracle database which has its integrated DBMS Table object is a representation of the set on set theory in mathematics. One of set theory type is an intersection, the result of intersection of a set will be smaller than originally set dimensionality reduction\. IST-EFP algorithm apply the concept of intersection of set theory in EFP algorithm that can reduce 2.33% more items while maintaining association rules obtained Keywords—association rule mining; EFP algorithm; Oracle DBMS;  item reductions; set theory I  I NTRODUCTION  Association rules are rules that relate one thing to another thing in which two things are closely interrelated. Association rules categorized as unsupervised learning because it can generate rules dynamically in terms of number and variety [1   There are two reference values are calculated on association rules which are support values and confidence values. Support value is the ratio of one variable against frequency of the entire transaction. Support value will be compared with the value of the specified minimum support, only support values equal to or greater that will be processed further. Confidence value is a value that indicates the relationship of one variable with another ones, it compares the value of two or more variables combination. Result that will be taken from association rules are values that meet the minimum support and minimum confidence [2   3  Th es e re s u lts can be  u s e d  t o det e rm in e  managerial business actions FP-Growth is an association rules algorithm which does not generate candidate itemset. FP-Growth forming tree structures by going through the stages of generation FP-Tree, Conditional Pattern Bases, Conditional FP-Trees and ends with the formation of Frequent Pattern [4 F r e q u e n t p a tt e r n  w ill generate rules that will generate confidence values. Stages of sufficient length requires a relatively large resource that is used depends on the size of the dataset. Many previous studies which seek to minimize the time to make the FP-Growth algorithm process more effective by create new and better methods [7 8 9  There is a previous research that introduces the use of tables in the DBMS to form FP-Tree. The reason of using the tables is to avoid long time of dataset processing in the main memory when forming the FP-Tree due to main memory capacity is not greater than the capacity of secondary storage In this study, the FP-Tree algorithm using the object as a shaper table FP-Tree in the call to Expand Frequent Pattern EFP\table [5 EFP algorithm generates an EFP table that stored in the database. EFP table contains all possible combination between items on a record from the original table \(initial dataset\. A record of the EFP table consists of items which have role as a parent or a child on the FP-Tree. Each record will have top of items that do not have a parent, this is an item that can potentially be eliminated according Frequent Pattern formation theory [5 Set theory is able to analyze related items in a set, which are intersection, union or subsets. A rule is a relationship between the correlated items. In DBMS, relation between tables is the application of set theory intersection. For example found table X = {a, b, c\ is related to the table Y = {z, a, b} on attributes a and b, in theory, the set can be written as X  Y a, b} [6  T h e resu l t s  o f  s e v e r a l  sets o f  r e la ti o n sh ip s w ill generate a new set with a smaller number of attributes. The dataset will consist of one or a few records and a record will be made up of several items. Record is a representation of the set and the item is a representation of the attribute. By using the intersection of set theory and unsupervised learning of FPGrowth algorithm would be produces new dataset with smaller on dimensions / sizes 2015 3rd International Conference on Information and Communication Technology \(ICoICT 978-1-4799-7752-9/15/$31.00 ©2015 IEEE 184 


II  O BJECTIVES  Current research will apply a new method to improve an EFP algorithm processing by reducing the size of the dataset preprocessing phase\n item dimension. Association rules obtained will relatively unchanged III  M ETHODS  Sample dataset used to analyze the results of the proposed algorithm. Used PL/SQL programming language and Oracle Database 11g as environment development Used three real datasets from sales transaction \(10 records with 344 items, 100 records with 1558 items and 1000 records with 3162 items\ch dataset will be processed using EFP algorithm and IST-EFP algorithm. The number of items produced and the number of rules generated from each of the algorithms will be compared and analyzed IV  FP-G ROWTH AL GORITHM  FP-Growth algorithm will generate frequent patterns from a dataset. To gain the frequent pattern must go through the stages of \(1\he ordering of items in descending order of frequency 2\ the formation of the FP-Tree, \(3\ Establishment of Conditional Pattern Bases, conditional FP-Tree and \(4 Frequent Pattern Formation Here is an implementation of fp-growth algorithm to obtain the frequent pattern from the dataset TABLE I  A DATASET WITH MINIMUM SUPPORT TRESSHOLD   20   Frequent Pattern obtained will be as follow      TABLE II    F REQUENT PATTERN WITH MINIMUM SUPPORT TRESSHOLD   20 Item Frequent Pattern ei e : 3 fe f : 2 l i l : 3 q f q : 2, e q : 2, i q : 2 j l j : 2, i j : 3 sg s : 2, f s : 2 rg r : 3 hl h : 2 pf p : 2   V  EFP  AL GORITHM  EFP algorithm will accelerate the frequent pattern formation process conducted by the FP-Growth algorithm. The dataset will be instantly transformed into EFP table where the EFP table  is a representation of the FP-Tree TABLE III  A N EFP  T ABLE  TID ITEM PRV TID ITEM PRV TID ITEM PRV TID ITEM PRV c1 f c3 q f c6 l c8 t h c1 g f c3 a q c6 f l c8 m t c1 s g c4 e c6 j f c9 g c1 p s c4 f e c6 p j c9 q g c2 i c4q f c6h p c9r q c2 l i c4 s q c7 g c9 n r c2 q l c4 t s c7 r g c10 i c2 j q c5 i c7 s r c10 e i c2 n j c5 e i c7 m s c10 g e c3i c5l e c8i c10r g c3 e i c5 j l c8 l i c3 f e c5 a j c8 h l    An EFP table will be resides on DBMS. By using intersection SQL query between original table and EFP Table frequent pattern will be obtained with the results as on TABLE II VI  IST-EFP  AL GORITHM  IST-EFP\(Dataset, minSupCount 1  X = Dataset 2  X 1 CREATE temporary table FROM X WHERE COUNT\(*\minSupCount 3  Y 1 CREATE EFP table FROM X 1  4  Z = Y 1   X on Y 1 previtem IS NOT NULL 5  Return Z Fig. 1.   IST-EFP Algorithm Suppose the data set used is from TABLE I, X1 = {i, e, q h, k, d, s, j, g, r, p, b, a, t, c, f, n, l, m}. By applying the EFP TID Items Sorted Items c1 d,g,f,p,s f,g,s,p,d c2 q,i,l,n,j i,l,q,j,n c3 q,e,f,i,a i,e,f,q,a c4 s,e,f,q,t e,f,q,s,t c5 a,i,l,e,j i,e,l,j a c6 f,p,j,h,l l,f,j,p,h c7 m,r,k,g,s g,r,s,m k c8 h,i,l,t,m i,l,h,t,m c9 q,r,n,g,c g,q,r,n c c10 e,r,b,g,i i,e,g,r,b 2015 3rd International Conference on Information and Communication Technology \(ICoICT 978-1-4799-7752-9/15/$31.00 ©2015 IEEE 185 


algorithm then obtained a dataset with 11 items, Y1 = {i, e, q h, j, g, a, l, m, n, f,}. Based on step number 4 in the IST-EFP algorithm then we will get a new dataset with 9 items, Z = {i, e h, j, g, a, l, n, f}. Items that are removed are {k, d, s, r, p, b, t c} where item {d, c, k, b} has a support count value 1 automatically pruned\ so pruning is done on items {p, r, s, t TABLE IV   A NEW DATASET AFTER PRUNED WITH IST-EFP ALGORITHM    Association rules obtained will be shown on table V TABLE V   A SSOCIATION R ULES O BTAINED BY IST-EFP  A LGORITHM   VII  D ISCUSSIONS  Real dataset from sales transaction used which have 10 records and 19 items. Each of these records are sorted from the largest to the smallest. Each record occurrence rate <2 will be automatically pruned because it is infrequent items that are not needed. EFP algorithm will produce 50 lines of records \(10 records multiply by 5 items\rted from largest to smallest IST-EFP algorithm will discard the item with the smallest value in each record so that the dataset will be more small The table below illustrates the evaluation of the 3 datasets M_Txx_Cxx, Txx stated number of records and CXX stated number of items TABLE VI    E VALUATION ON D ATASETS OF IST-EFP  A LGORITHM  Original EFP IST-EFP EFP IST-EFP 1  M_T10_C344 344  51  43  137  110  2  M_T100_C1558 1,558  748  674  4,864  4,678  3  M_T1000_C3162 3,162  2,700 2,325  110,146  108,887  No Datase t Number of Item Number of Rules                M_T10_C344 dataset has 344 items spread on 10 records If the data is processed using the EFP algorithm found the number of items was reduced to 51 \(85.17%\ of the number of items in the initial dataset \(344\ by the number of association rules generated as many as 137. If the EFP algorithm result data processed further with an IST-EFP algorithm, will get the number of items reduced to 43 \(87.50%\, the number of association rules as much as 110. In the dataset M_T10_C344 seen that IST-EFP algorithm performance is 2.33% better on items reduction then EFP algorithm with reducing the number of rules as much as 19.71 M_T100_C1558 dataset has 1558 items spread on 100 records. If the data is processed using the EFP algorithm found the number of items was reduced to 748 \(51.99%\ of the number of items in the initial dataset \(1558\y the number of association rules generated as many as 4864. If the EFP algorithm result data processed further with an IST-EFP algorithm, will get the number of items reduced to 674 56.74%\, the number of association rules as much as 4678. In the dataset M_T100_C1558 seen that IST-EFP algorithm performance is 4.75% better on items reduction then EFP algorithm with reducing the number of rules as much as 3.82 M_T1000_C3162 dataset has 3162 items spread on 1000 records. If the data is processed using the EFP algorithm found the number of items was reduced to 2700 \(14.61%\ of the number of items in the initial dataset \(3162\y the number of association rules generated as many as 110146. If the EFP algorithm result data processed further with an IST-EFP algorithm, will get the number of items reduced to 2325 26.47%\, the number of association rules as much as 108887 In the dataset M_T1000_C3162 seen that IST-EFP algorithm performance is 11.86% better on items reduction then EFP algorithm with reducing the number of rules as much as 1.14 VIII  C ONCLUSIONS  IST-EFP algorithm able to reduce the dimensions of the dataset items better than EFP algorithm. The performance of reduction depends on the dimensions in the test dataset, the larger the dimensions of items from a large dataset, the algorithm will be more effective then the EFP algorithm to perform reduction. In datasets with datasets that have a dimension of 344 total items, IST-EFP algorithm has 2.33 better performance than EFP algorithm. In datasets with dimensions as many items as 3162, IST-EFP algorithm has 11.86% better performance than the EFP algorithm TID  Items IST-EFP Items  c1 f,g,s p d f,g,s c2 i,l,q,j n i,l,q,j c3 i,e,f,q,a i,e,f,q c4 e,f,q,s,t e,f,q,s c5 i,e,l,j a i,e,l,j c6 l,f,j,p,h l,f,j,p c7 g,r,s m,k g,r,s c8 i,l,h,t,m i,l,h,t c9 g,q,r,n,c g,q,r c10 i,e,g,r,b i,e,g  XY Conf\(X  Y Conf\(Y X l j 75  100 il 60  75 ie 60  75 f s50  66.67 gr 50  100 f q 50  50 e q 50  50 e f 50  50 gs 50  66.67 l h 50  100 i q 40  50 i j 40  66.67 2015 3rd International Conference on Information and Communication Technology \(ICoICT 978-1-4799-7752-9/15/$31.00 ©2015 IEEE 186 


R EFERENCES    B Nath D K B h attach ar y y a a n d A Gh os h   D im e n s i o n alit y Reduction for Association Rule Mining I nternational Journal of Intelligent Information Processing vol. 2, no 1, 2011   T  A  Kum b h a re an d Prof S a n t os h V. Ch obe A n Overview of Association Rule Mining Algorithms I nternational Journal of Computer Science and Information Technologies vol. 5, no. 1, pp. 927-930, 2014 3 A  Se t h i a n d P  M a ha j a n  A sso c i a tio n R u le M i ni ng A  Review The International Journal of Computer Science Applications vol. 1, no. 9, pp. 72-83, 2012   C  Borg el t   A n I m pl e m e n t a t i on of t h e F P g ro w t h Algorithm," in OSDB'05 Chicago, Illinois, USA, 2005   X. Sh ang  K.U Sattler an d I. Geist S QL B a sed Frequ e n t  Pattern Mining with FP-Growth A pplications o f  D eclarative Programming and Knowledge Management Lecture Notes in Computer Science vol. 3392, pp. 32-46 2005   S  L i ps c h u t z a n d M. L i ps on Dis c rete Math e m atic s T h ird Edition, Mc Graw Hill, 2007 7 L V u a n d G   A l a g hb a n d   M inin g Fr e q ue nt P a tte r n s B a se d  on Data Characteristics," 2012 8 M  Su m a n T  A n ur a d ha  K  G o w t ha m a n d A   Ramakrishna, "A Frequent Pattern Mining Algorithm Based On FP-Tree Structure And Apriori Algorithm Research Journal of Comput er Systems Engineering vol 02, no. 05, pp. 275-277, 2011  D. Garg an d H S h ar m a   C om parati v e  An al y s is o f  Various Approaches Used in Frequent Pattern Mining I nternational Journal of Advanced Computer Science and Applications pp. 141-147, 2011  2015 3rd International Conference on Information and Communication Technology \(ICoICT 978-1-4799-7752-9/15/$31.00 ©2015 IEEE 187 


Figure 3 Optimization for minimizing the output The node with 254 is the reporter trigger the next level if the item represented by this level bottom STE is seen in the input If the item of the current level is not seen the activation of the current level will be held by the top symbol until the end of this transaction when separator symbol is seen The itemset matching is restarted in the beginning of each transaction by the Level 0 STE The counting component uses an on-chip counter element to calculate the frequency of a given itemset If the last level has been triggered the matching component waits for the separator symbol to indicate the end of a transaction The separator symbol then activates the counter incrementing it by one If the threshold which is set to  is reached in the counter this automaton produces a report signal at this cycle After processing the whole dataset on the AP the output vectors are retrieved Each itemset with a frequency above the minimum support will appear in the output Although the automata shown in Figure 2 already implement the basic functions of matching and counting for ARM there is still much room for performance optimization We will talk about the performance optimization in the next subsection We only show the automata for 8-bit encoding scheme in this paper The automata for 16-bit encoding scheme are designed in a similar way but use two connecting STEs to match an item In this paper we propose three optimization strategies to maximize the computation performance of the AP The rst strategy is to minimize the output from the AP In the initial automaton design shown in Section V-C the AP chip creates a report vector at each cycle whenever there is at least one counter report Each report vector carries the information about the cycle count for this report Therefore the AP chip creates many report vectors during the data processing These report vectors may ll up the output buffers and cause stalls during processing However solving the ARM problem only requires identifying the frequent itemsets the cycle at which a given itemset reaches the minimum support level is irrelevant We therefore modify the design of the reporting element and postpone all reports to the last cycle Figure 3 We utilize the latch property of the counter to keep activating another STE connected to this counter after the counter is reached We call this STE the reporter One symbol i.e 254 is reserved to indicate the end of a transaction stream and this end-of-stream symbol matches to the reporter STE and triggers the actual output Consequently the global set of items is 0-253 which ensures that the ending symbol 254 will not appear in the middle of the transaction stream With this modi“cation only one output vector will be produced in the end of data stream Another bene“t of this modi“cation is that it eliminates the need to merge multiple report vectors as a postprocessing step on the CPU Instead the counting results can be parsed from only one report vector As shown in Figure 2 when the mining of itemsets nishes the automata for itemset need to be compiled onto the AP to replace the automata for itemsets The automata recon“guration involves both routing recon“guration and symbol replacement steps because the NFAs that recognize itemsets of different sizes have different structures compare Figure 2a and Figure 2b On the other hand the AP also provides a mechanism to only replace the symbol set for each STE while the connections between AP elements are not modi“ed The time of symbol replacement depends on how many AP chips are involved The max symbol replacement time is 45ms if all STEs update their symbol sets To remove the routing recon“guration step we propose a general automaton structure supporting itemsets with different sizes The idea is to add multiple entry paths to the NFA shown in Figure 2 To count the support of a given itemset only one of the entry paths is enabled by matching to the transaction separator symbol while the other entry paths are blocked by a reserved special symbol This special symbol can be the same as the data stream ending symbol i.e 254 discussed in Section V-D1 This structure is called multiple-entry NFA for variable-size itemset MENFA-VSI 10 total recon“guration time 5ms is saved by using the ME-NFA-VFI structure Figure 4 shows a small-scale example of an ME-NFAVSI structure that can count an itemset of size 2 to 4 Figure 4a shows the ANML macro of this ME-NFA-VSI structure leaving some parameters to be assigned for a speci“c itemset e01 e03 are symbols for three entries An entry can be con“gured as either 255 or 254 to present enabled and disable status Only one entry is enabled for a given itemset I represents the global set of items  i01 i04 are individual symbols of items in the itemset SP is the transaction separator and END is the ending symbol of the input stream To count a 2-itemset the rst two entries are blocked by 254 and the third entry is enabled by 255 Figure 4b Similarly this structure can be con“gured to counting a 3itemset and a 4-itemset by enabling a different entry point Figure 4c and 4d Another optimization has been made to reduce STE usage 
minsup D Performance optimization 1 Output optimization minsup 2 Avoid routing recon“guration 
k k k I 
 1 
693 


a AP macro of ME-NFA-VSI  b Automaton for itemset c Automaton for itemset d Automaton for itemset 
006 006 006 006 006 006 
3 Concurrent mining itemset and itemset A Capacity and Overhead Apriori 
k k k k k k k k k k k 
1 3 2 7 8 4 5 25 30 
002\003\004\005\006 007 002\003\004\005\006 010 002\003\004\005\006 011 
        
of ME-NFA-VSI structure by switching entry STEs from all-input mode to start-of-data mode with a bi-directional connection to I STE Figure 4a The max number of the optimized ME-NFA-VSI structures that can t on the AP chip is mainly limited by the number of counter elements Therefore it is possible to compile large ME-NFAVSI structures on the AP chip without sacri“cing capacity In the 8-bit symbol encoding scheme one block of the AP chip can support two ME-NFA-VSI structures that match itemsets of size 2 to 40 For the 16-bit-symbol encoding scheme we use an ME-NFA-VSI structure that matches itemset of size 2 to 24 24 is a reasonable upper bound of itemset size we discovered in our test cases At the very beginning  is small and the end  is large of mining the number of candidates could be too small to make full use of the AP board In these cases we predict the number candidates of the itemset by assuming all itemset candidates are frequent If the total number of itemset candidates and predicted itemset candidates can t onto the AP board we generate itemset candidates and concurrently mine frequent itemsets and itemsets in one round This optimization takes advantage of uni“ed ME-NFA-VSI structure and saves about 5%-10 AP processing time in general VI E XPERIMENTAL R ESULTS The performance of our AP implementation is evaluated using CPU timers host codes and an AP simulator in the AP SDK AP codes assuming a 48-core D480 AP board In our experiments our AP-accelerated algorithm Apriori-AP switches between 8-bit and 16-bit encoding schemes automatically in the data preprocessing stage shown in the owchart Figure 1 In an 8-bit scheme the items are coded with symbols from 0 to 253 If more than 254 frequent items are represented after ltering two 8-bit symbols are used to represent one item 16-bit symbol scheme In both encoding schemes the symbol 255 is reserved for the transaction separator and the symbol 254 is reserved for both the input ending symbol and the entryblockers for the ME-NFA-VSI structure By using the MENFA-VSI structure one AP board can match and count 18,432 itemsets in parallel with sizes from 2 to 40 for 8bit encoding and 2 to 24 for 16-bit encoding In all our experiments 24 is a reasonable upper bound of the sizes of the itemsets If there are more than 18,432 candidate itemsets multiple passes are required Before each single pass a symbol replacement process is applied to recon“gure all ME-NFA-VSI structures on the board which takes 0.045 second Figure 4 A small example of multiple-entry NFA for variable-size itemset support counter for 2-itemset 3itemset and 4-itemset a is the macro of this ME-NFA-VSI with parameters 
 1  1  1  1  1 
694 


B Comparison with other implementations Apriori Apriori Eclat Eclat Eclat C Datasets Frequent Itemset Mining Dataset Repository D AP vs CPU Apriori 
We use the computation times from Borgelts CPU sequential implementation Apriori-CPU as a baseline Because the AP accelerates the counting operation at each iteration we show the performance results of both the counting operation and the overall computation in this section We also compare a state-of-the-art CPU serial implementation of Eclat-1C a multi-threading implementation of Eclat-6C and a GPU-accelerated implementation of Eclat-1G All of the abo v e implementations are tested using the following hardware CPU Intel\(R Xeon\(R CPU E5-1650\(6 physical cores 3.20GHz Mem 32GB 1.333GHz GPU Nvidia Kepler K20C 706 MHz clock 2496 CUDA cores 4.8GB global memory For each benchmark we compare the performance of the above implementations over a range of minimum support values A lower support number requires a larger search space and more memory usage since fewer itemsets are ltered during mining To have all our experiments nished in a reasonable time we select minimum support numbers that produce computation times of the Apriori-CPU implementation that is in the range from 1 second to 5 hours for any dataset smaller than 1GB and from 1 second to 10 hours for larger datasets The relative minimum support number de“ned as the ratio of minimum support number to the total number of transactions is used in the gures of this section Three commonly-used real-world datasets from the  three synthetic datasets and one real-world dataset generated by ourselves ENWiki are tested The details of these datasets are shown in Table I and II T40D500K and T10020M are obtained from the IBM Market-Basket Synthetic Data Generator Webdocs5X is generated by duplicating transactions of Webdocs 5 times The ENWiki is the English Wikipedia downloaded in December 2014 We have removed all paragraphs containing non-roman characters and all MediaWiki markups The resulting dataset contains about 1,461,281 articles 11,507,383 sentences de“ned as transactions with 6,322,092 unique words We construct a dictionary by ranking the words using their frequencies Capital letters are all converted into lower case and numbers are replace with the special NUM word In natural language processing eld the idea that some aspects of word semantic meaning can be induced from patterns of word co-occurrence is becoming increasingly popular The association rule mining provides a suite of ef“cient tools for computing such co-occurred word clusters Apriori Apriori Figure 5 shows the performance comparison between our Apriori-AP solution and the classic Apriori-CPU implementation on three real-world datasets The computation time of Apriori-CPU grows exponentially as minimum support number decreases for three datasets while Apriori-AP shows much less computation time and much slower growth of computation time as minimum support number decreases As a result the speedup of Apriori-AP over Apriori-CPU grows as support decreases and achieves up to 129X speedup The drop in the speedup at the relative minimum support of 0.1 for Webdocs is caused by switching from 8-bit encoding to 16-bit encoding which doubles the size of the input stream The speedup increases again after this point For small and dense datasets like Pumsb data processing time is relatively low while the symbol replacement takes up to 80 of the total computation time Though the symbol replacement is a light-weight recon“guration frequent symbol replacement decreases the AP hardware utilization Also the increasing CPU time of Apriori-AP on small and dense datasets leads to a smaller relative utilization of the AP when the minimum support decreases In contrast larger datasets like Accidents and Webdocs spend relatively more time on data processing and the portion of data processing time goes up as the support decreases This analysis indicates our Apriori-AP solution exhibits superior relative performance for large datasets and small minimum support values Figure 6 shows similar trends of Apriori-AP speedup over Apriori-CPU on three synthetic benchmarks Up to 94X speedups are achieved for the T100D20M dataset In all above the cases the difference between the counting speedup and overall speedup is due to the computation on the host CPU This difference will decrease as the total computation time increases for large datasets The symbol replacement latency can be quite important for small and dense datasets that require multiple passes in each iteration but this latency may be signi“cantly reduced in future generations of the AP Figure 7 shows Table I 
Real-World Datasets Synthetic Datasets 
Name Trans Aver Len Item Size MB Pumsb 49046 74 2113 16 Accidents 340183 33.8 468 34 Webdocs 1692082 177.2 5267656 1434 ENWiki 11507383 70.3 6322092 2997.5 Aver Len  Average number of items per transaction Table II Name Trans Aver Len Item ALMP Size MB T40D500K 500K 40 100 15 49 T100D20M 20M 100 200 25 6348.8 Webdocs5X 8460410 177.2 5267656 N/A 7168 Aver Len  Average number of items per transaction ALMP  Average length of maximal pattern 
   
695 


0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.48 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0   Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time 
0.1 1 10 100 3000 2000 1000 0 1000 2000 3000 4000 5000 
0.6 0.5 0.4 0.3 0.2 0.1 0.0 10 100 
1 10 100 6000 4000 2000 0 2000 4000 6000 8000 10000 
0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07 0.06 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  0 10000 5000 15000 c Webdocs Figure 5 The performance results of Apriori-AP on three real-world benchmarks DP time SR time and CPU time represent the data process time on AP symbol replacement time on AP and CPU time respectively Webdocs switches to 16-bit encoding when relative minimum support is less then 0.1 8-bit encoding is applied in other cases 
E vs Eclat Eclat et al Eclat Eclat 
0.80 0.75 0.70 0.65 0.60 0.55 0 100 200 300 400 Computation Time \(s Relative minimum support 1.0X Symbol replacement time 0.5X Symbol replacement time 0.1X Symbol replacement time Figure 7 The impact of symbol replacement time on Apriori-AP performance for Pumsb how symbol replacement time affects the total AprioriAP computation time A reduction of 90 in the symbol replacement time leads to 2.3X-3.4X speedups of the total computation time The reduction of symbol replacement latency will not affect the performance behavior of AprioriAP for large datasets since data processing dominates the total computation time 
Apriori Eclat Equivalent Class Clustering   is another algorithm based on Downward-closure uses a vertical representation of transactions and depth-“rst-search strategy to minimize memory usage Zhang  proposed a h ybrid depth-“rst/breadth-“rst search scheme to expose more parallelism for both multi-thread and GPU versions of  However the trade-off between parallelism and memory usage still exists For large datasets the nite memory main or GPU global memory will become a limiting factor for performance and for very large datasets the algorithm fails While there is a parameter which can tune the trade-off between parallelism and memory occupancy we simply use the default setting of this parameter for better performance Figure 8 shows the speedups that the sequential 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 AP counting speedup Apriori-AP overall speedup    Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  b Accidents 
T40D500K Counting T40D500K Overall T100D20M Counting T100D20M Overall Webdocs5X Counting Webdocs5X Overall Speedup Relative Minimum Support Figure 6 The speedup of Apriori-AP over Apriori-CPU on three synthetic benchmarks 
1 10 100 
a Pumsb 
696 


0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 10 Pumsb Accidents T40D500K Webdocs Webdocs5X T100D20M ENWiki Speedup Relative Minimum Support 
Figure 8 The performance comparison of CPU sequential and algorithm achieved with respect to sequential Apriori-CPU Though has 8X performance advantage in average cases the vertical bitset representation become less ef“cient for sparse and large dataset high trans and freq item ratio This situation becomes worse as the support number decreases The Apriori-CPU implementation usually achieves worse performance than  though the performance boost of counting operation makes Apriori-AP a competitive solution to parallelized  Three factors make a poor t for the AP though it has better performance on CPU 1 requires bit-level operations but the AP works on byte-level symbols 2 generates new vertical representations of transactions for each new itemset candidate while dynamically changing the values in the input stream is not ef“cient using the AP 3 Even the hybrid search strategy cannot expose enough parallelism to make full use of the AP chips Figure 9 and 10 show the performance comparison between Apriori-AP 45nm for current generation of AP and sequential multi-core and GPU versions of  Generally Apriori-AP shows better performance than sequential and multi-core versions of  The GPU version of shows better performance in Pumsb Accidents and Webdocs when the minimum support number is small However because of the constraint of GPU global memory Eclat-1G fails at small support numbers for three large datasets ENWiki T100D20M and Webdocs5X ENWiki as a typical sparse dataset causes inef“cient storage of bitset representation in Eclat and leads to early failure of EclatGPU and up to 49X speedup of Apriori-AP over Eclat-6C In other benchmarks Apriori-AP shows up to 7.5X speedup over Eclat-6C and 3.6X speedup over Eclat-1G This gure also indicates that the performance advantage of AprioriAP over GPU/multi-core increases as the size of the dataset grows The AP D480 chip is based on 45nm technology while the Intel CPU Xeon E5-1650 and Nvidia Kepler K20C on which we test are based on 32nm and 28nm technologies respectively To compare the different architectures in the same semiconductor technology mode we show the performance of technology projections on 32nm and 28nm technologies in Figure 9 and 10 assuming linear scaling for clock frequency and square scaling for capacity The technology normalized performance of Apriori-AP shows better performance than multi-core and GPU versions of Eclat in almost all of the ranges of support that we investigated for all datasets with the exception of small support for Pumsb and T100D20M Apriori-AP achieves up to 112X speedup over Eclat-6C and 6.3X speedup over Eclat-1G The above results indicate that the size of the dataset could be a limiting factor for the parallel algorithms By varying the number of transactions but keeping other parameters xed we studied the behavior of Apriori-AP and Eclat as the size of the dataset increases Figure 11 For T100 the datasets with different sizes are obtained by the IBM synthetic data generator For Webdocs the different data sizes are obtained by randomly sampling the transactions or by concatenating duplicates of the whole dataset In the tested cases the GPU version of fails in the range from 2GB to 4GB because of the nite GPU global memory Comparing the results using different support numbers on the same dataset it is apparent that the smaller support number causes Eclat-1G to fail at a smaller dataset This failure is caused by the fact that the ARM with a smaller support will keep more items and transactions in the data preprocessing stage While not shown in this gure it is reasonable to predict that the multicore implementation would fail when the available physical memory is exhausted However Apriori-AP will still work well on much larger datasets assuming the data is streamed in from the hard drive assuming the hard drive bandwidth is not a bottleneck VII C ONCLUSIONS AND THE F UTURE W ORK We present a hardware-accelerated ARM solution using Microns new AP architecture Our proposed solution includes a novel automaton design for matching and counting frequent itemsets for ARM The multiple-entry NFA based design was proposed to handle variable-size itemsets MENFA-VSI and avoid routing recon“guration The whole design makes full usage of the massive parallelism of the AP and can match and count up to itemsets in parallel on an AP D480 48-core board When compared with the based single-core CPU implementation the proposed solution shows up to 129X speedup in our experimental 
Apriori Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat F Normalizing for technology Eclat G Data size Eclat Eclat Eclat Apriori 
18 432 
 
697 


0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 1 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm a Webdocs\(1.4GB 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails d Webdocs5X 7.1GB Figure 10 Performance comparison among Apriori-AP Eclat-1C Eclat-6C and Eclat-1G with technology normalization on four large datasets 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm b Accidents 34MB 0.40 0.36 0.32 0.28 0.24 0.20 0.16 0.12 0.08 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm c T40D500K\(49MB Figure 9 Performance comparison among Apriori-AP Eclat1C Eclat-6C and Eclat-1G with technology normalization on three small datasets results on seven real-world and synthetic datasets This APaccelerated solution also outperforms the multicore-based and GPU-based implementations of 0.60 0.55 0.50 0.45 0.40 0.35 0.30 0.25 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails c T100D20M 6.3GB 
ARM a more ef“cient algorithm with up to 49X speedups especially on large datasets When performing technology projections on future generations of the AP our results suggest even better speedups relative to the equivalent-generation of CPUs and GPUs Furthermore by varying the size of the datasets from small to very large our results demonstrate the memory constraint of parallel ARM particularly for GPU 
Eclat Eclat 
0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm 
0.20 0.15 0.10 0.05 0.00 1 10 100 1000 10000 100000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails b ENWiki\(3.0GB 
a Pumsb 16MB 
698 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372…390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94…117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1…2:17 May 2013  P  Dlugosch  An ef“cient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Microns automata processor architecture Recon“gurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Recon“gurable Technol Syst et al IEEE TPDS Proc of IPDPS14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Ef“cient Accelerators and Recon“gurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


