Achieving Cryptographic Modernization Compliance for Reprogrammable Crypto in Space Joseph D Bull Booz Allen Hamilton 5200 Pacific Concourse Drive Suite 390 Los Angeles CA 90045 310-297-5518 bulljoseph@bah.com Abstract The Department of Defense-directed and National Security Agency-led Cryptographic Modernization Initiative CMI mandates the use of spaceborne reprogrammable cryptographic devices for future satellite systems This cryptographic paradigm shift has revealed a void in the knowledge base currently used for implementation of cryptographic space products because reprogrammability has not been addressed within the Defense community 
This knowledge gap creates the need for a systems engineering-based process to enable a satellite system to achieve successful compliance with CMI a process that this paper intends to address The specific steps in the proposed process to achieve CMI compliance regarding the reprogrammability mandate include a review of existing policy integration of government infrastructure and products phasing in of technologies and functions related to reprogrammability use case development and requirements development This process is envisioned as being leveraged 
on all future satellite systems requiring reprogrammability including both commercial and government satellites Additionally this paper provides a rationale for a CMI Space Policy that will support future cryptographic development efforts The goal of the proposed process is to create a CMI-compliant satellite system that does not sacrifice the satellite's mission for the sake of compliance 1 2 TABLE OF CONTENTS 1 INTRODUCTION  1 2 PROGRAMMABILITY APPLIED TO SPACE  2 3 THE BEGINNING  3 4 USE CASES 
 3 5 REQUIREMENTS DEVELOPMENT  7 6 PRODUCTS AND INFRASTRUCTURE  8 7 PHASING IN 
C 2008 IEEE 2 IEEEAC paper 1452 Version 5 Updated December 19 2007 1 INTRODUCTION Until recently the concept of reprogrammable end cryptographic units ECUs did not exist 
THE TRANSITION  9 8 THE CALL FOR AN NSA SPACE CMI POLICY  10 9 CONCLUSIONS  10 REFERENCES   11 BIOGRAPHY   11 11-4244-1488-1/08/$25.00 
yet today there are plans to launch fully reprogrammable ECUs3 into space To date there have been no Department of Defense DoD satellites launched with on-orbit fully reprogrammable mission data or telemetry tracking and control TT&C ECUs Most legacy space ECUs are non-programmable point-to-point solutions supporting only one static mission and one set of users over their lifetime These legacy systems have not kept up with information technology modernization initiatives in the past due to the harsh requirements of the space environment that prohibit inclusion of 
certain technologies in the systems and lowquantity production of the components The major design challenges for satellites were and still are size weight and power SWAP system availability inaccessibility to deployed ECUs and radiation effects When the National Security Agency NSA introduced the Cryptographic Modernization Initiative CMI in 2003 it created more questions than answers NSA's Director of the Information Assurance Directorate IAD Daniel G Wolf said at the time The Cryptographic Modernization Initiative is a Department of 
Defense-directed NSA-led effort to transform and modernize IA capabilities for the 21 st century It will transform cryptographic security capabilities for national security systems at all echelons and points of use while exploiting new and emerging technologies 1 This statement is evidence that the United States Government intends to secure its information and assets from adversaries who are advancing in technological development This NSA-led transformation of cryptographic security capabilities became evident with the six tenets defined within the initiative The transformation was particularly evident in 
the fifth tenet which required all future Type 1 ECUs to be reprogrammable The remaining tenets of the initiative addressed capabilities that were evolutionary rather than revolutionary making them easier to implement The cryptographic paradigm shift from nonprogrammable ECUs to reprogrammable ECUs given the major design challenges stated above revealed a significant gap in the knowledge and experience required for designing 1 3The definition of an ECU is provided in Section 
4 under System Nodes 1 


Not all space ECUs Closing this gap through research development testing and a solid systems engineeringbased process will enable a satellite system to achieve successful compliance with CMI As a result of NSA's CMI a new track was created for reprogrammable ECUs in space which is the path that future satellite programs will traverse The guiding philosophy that any satellite program should adopt is one of complete CMI compliance wherever possible while minimizing any undesirable performance impacts as a result of that compliance Detailed within this white paper is a proposed process to ensure that warfighters and commercial users of the future will have protected communication via ECUs that are able to adapt to emerging threats Additionally this paper provides resolutions of issues that were initially identified by the author of this paper when the CMI policy was first levied on ECUs and that were documented in a 2005 Military Communications Conference MILCOM paper entitled Ensuring The Security Of Warfighters Satcom Via Programmable Cryptographic Devices 2 Note that this paper is not intended to provide cryptographic designs but rather is intended to create the foundation from which a program can achieve CMI compliance with regard to reprogrammability It is written from the perspective of a DoD satellite system but it can also be used as guidance for commercial applications Ultimately the objective of the process is CMI compliance for future satellite systems while maintaining the intended performance and functional aspects of those systems 2 PROGRAMMABILITY APPLIED TO SPACE The primary purpose of programmability is to mitigate the risk of security compromise by an adversary equipped with an unanticipated advance in mathematics or computational power that enables the adversary to inflict extensive bruteforce attacks on algorithms Such an advance would not only allow for the compromise of data and voice communications sent between various warfighters but it could also place the satellite's control in the hands of the adversary Providing programmability within the satellite provides a mitigation method for compromise because advanced algorithms or methods of cryptography could be uploaded and implemented from the ground thus preventing the potential future loss of the satellite asset An additional benefit of programmability is the ability to replace an algorithm in the event that a vulnerability is discovered prior to it being exploited These two benefits eliminate the historic tie between the lifetime of a satellite and the lifetime of the cryptographic algorithm As a third benefit depending on the level of reprogrammability of the ECU additional functions could be added in the future to support new communications mission requirements including the ability to support interoperability with multiple communities of interest The result is a satellite that can provide continued secure communications to warfighters 2 To understand how CMI affects future satellite programs one must understand the information assurance IA functions within the satellite system Figure 1 shows a generic cryptographic IA function flow chart The four dedicated cryptographic IA functions onboard a DoD satellite are as follows  Payload Data Communications Security COMSEC o High Assurance Internet Protocol IP Encryptor HAIPE encrypts and decrypts all classified packet traffic that originates or terminates on the satellite o IP Security IPSec encrypts and decrypts all sensitive packet traffic that originates or terminates on the satellite o Circuit Encrypt and Decrypt processes i.e encrypts and decrypts verifies integrity and authenticates all circuit traffic that originates or terminates on the satellite  Payload and Access Control which authenticates and verifies the integrity of requests to be processed by the payload control  Transmission Security TRANSEC which provides assured service to protect against detection jamming and physical layer denial of service  TT&C o Uplink decrypts authenticates and verifies the integrity of satellite control commands and authenticates the source o Downlink encrypts telemetry information and appends coding used to verify the integrity and authenticity of the data TRANSEC O Other Satellite Function Figure 1 Cryptographic IA Function Flow Chart An understanding of both the purpose and need for reprogrammability and of the IA functions impacted by CMI sets the foundation for how a satellite system can achieve compliance with the initiative 2 Radio Frequency UpI  Down  TT&C Uplink/Downlink I.Satellite Control _ Circuit CircuitorI IPacketSwitchI EncrypV Decrypt Payload Control  HAIPE IPSec Toall Payload Payload Control Functions IA Function DoD communication satellites provide TRANSEC 2 


3 THE BEGINNING The first step in understanding how to achieve compliance was determining which policies and documentation would help to identify the application of reprogrammability in space Unfortunately shortly after NSA's introduction of CMI there were no space-related government policies or publicly available best practices for CMI only generic applications of the initiative Although some documents helped to clarify CMI and defined additional functional requirements there were no further guidelines for space application of reprogrammable ECUs The journey toward compliance began with a white paper created to define what a space CMI policy would look like Entitled Cryptographic Modernization Concept of Compliance the paper not only gave birth to a space CMI compliance process for reprogrammable ECUs but also defined with greater fidelity what programmable means for ECUs In addition the paper identified IA functional compliance considerations space-specific technological issues general areas of concern for space system integration of reprogrammable ECUs and a high-level CMI decisionmaking process All of these topics were published in Ensuring The Security Of Warfighters Satcom Via Programmable Cryptographic Devices Since that publication was released several areas of concern and technological issues that were identified have been resolved The issues include forecasting the size and complexity of future algorithms the level of programmability algorithm transitioning recertification redundancy and reliability and integrity The findings from the past three and a half years of study in this area will be highlighted throughout this paper Finally this white paper takes the initial high-level CMI decision-making process which identified programmatic compliance decisions and provides the foundation to support programs that choose to be CMI compliant This process was refined over the past three and a half years resulting in a mature system-engineering process for applying CMI to satellite systems to support reprogrammable ECUs 4 USE CASES Achieving ECU reprogrammability requires a generic set of functional requirements to be developed and allocated to the appropriate requirements documents Use cases provide an ideal mechanism for capturing functional requirements According to Bittner and Spence Use cases stated simply allow a description of sequences of events that taken together lead to a system doing something useful 3 The set of use cases conceptualized how the ECUs would be reprogrammed throughout the system's life cycle Specifically the use cases addressed the functional aspects of ECU reprogrammability how ECU algorithms are developed the distribution of algorithm data files and the algorithm transition process These use cases would then be used to derive requirements Each of the use cases captures the scope preconditions required steps post conditions and issues that were identified in the course of creating the use case In addition how those issues were resolved is discussed in this paper The following subsections provide an engineering overview of the use case development process In the first step system nodes and actors including external nodes or human operators are identified Figure 2 shows an example of a generic communications satellite system The ECUs identified in the system are representative and may be accompanied by additional ECUs to support various functions The system nodes and actors for the use cases are as follows System Nodes  End Cryptographic Unit Product that utilizes cryptographic algorithms in support of security services The NSA certifies Type 1 ECUs which protect classified information  Mission Operations Segment MOS The segment of a satellite system responsible for the mission of the satellite payload portion of the satellite  Sustainment Factory SF Contractor that performs sustainment activities after the system is operational the SF typically is the developer of the satellite system SFs are not found within the operational chain of command  Space Segment SS The segment consisting of the constellation of satellites in addition to a systems operation center SOC which is responsible for the satellite operation bus portion of the satellite  Terminal Segment TS The segment that develops and maintains the terminals that connect to the Space Segment to communicate with other users on the system Figure 2 Generic Communications Satellite System 3 


Actors  Designated Approval Authority DAA Official with the authority to formally assume responsibility for operating a system at an acceptable level of risk 4  National Security Agency The supplier of key material and algorithms to all DoD programs utilizing Type 1 ECUs The next step is to identify the types of messages and information that would be transmitted between each of the system nodes and actors The following are the primary types of messages that are transmitted  Algorithm Data File ADF The data package delivered to an ECU that contains the Cryptographic Algorithm Code CAC and the Cryptographic Algorithm Support Software CASS  Accreditation Letter The letter authorized by the DAA which states that the system can become operational  Cryptographic Algorithm Specifications CAS The mathematical representation of the algorithm that can be translated into software for use in an ECU  Cryptographic Algorithm Code The code that implements the cryptographic algorithm specifications  Cryptographic Algorithm Support Software The supporting software for the implementation of the algorithm which is needed to allow the hardware to interface with the other management functions of the ECU  Key Specifications The details of the key that will be utilized with a given cryptographic algorithm  Receipt A message generated by the ECU to verify certain events to external components and systems Development Use Case Scope Development of a new algorithm for space-borne ECUs post-launch This development would occur after the NSA and DAA determine that a need exists for a new algorithm regardless of the motivation for the change This development process has dependencies on the NSA SS SF and DAA Preconditions  DAA decided to modify a cryptographic algorithm  A cryptographic algorithm already existed  Memory and processing capacity was available in all affected ECUs  A Recertification Plan existed  Host and ECU specifications were available in the sustainment factory  A representative ECU was available for the Space Segment Steps required Figure 3 is a graphical representation of the steps listed below This example is specific to the SS and assumes that the SS will be the ADF distribution agent The distribution agent is the entity responsible for coordinating the distribution of the ADF files If the MOS was the distribution agent the SS would transmit the ADF to the MOS after the ADF was built 1 NSA Delivers cryptographic algorithm specifications to the SF the ECU vendor/subcontractor 2 SF Develops new key specifications and updates the key management plan KMP 3 SF Delivers key specifications and KMP to NSA 4 NSA Delivers test key and known output 5 SF Implements and validates the cryptographic algorithm and CASS performs cryptographic verification testing 6 SF Delivers cryptographic algorithm code implementation to NSA 7 NSA Performs ECU testing and recertifies the ECU 8 NSA Transmits ECU certification letter to DAA 9 DAA Issues Interim Approval to Operate IATO 10 SF Integrates update into gold standard ECU performs system test and verification T&V and signs and packages CASS 11 SF Transmits evaluated code to the NSA and transmits CASS to the Space Segment for storage 12 NSA Approves the cryptographic algorithm code and encrypts and signs the software with the test modules 13 NSA Transmits the encrypted and signed algorithms to the Space Segment 14 SS Integrates corresponding update into gold standard ECU and validates the algorithm with the test module 15 SS Transmits the Certification Test and Evaluation CT&E of the ECU to the DAA 16 DAA Re-accredits the system if necessary 17 DAA Transmits the accreditation letter to the NSA and to the MOS where the letter is stored 18 NSA Encrypts and signs the algorithm with the operational modules 19 NSA Transmits encrypted and signed algorithm to the Space Segment 20 SS Validates the algorithm for release and builds the ADF that includes the cryptographic algorithm and the associated CASS 21 SS Transmits a change request to MOS Post Conditions KMP is updated  Key Specifications are updated  ADF is completed 4 


an ECU prior to its actual operation recertification would be impossible for space-borne ECUs To resolve this issue a Type 1 ECU the NSA requires recertification of the ECU Because recertification requires physical testing of bJ I I Encrypt  Sign Algorithm Signed Alg  Validate Alg  build ADF Change use case identified the issues surrounding recertification of space-borne ECUs In most cases prior to implementing new algorithms or upgrades to or semi-customized manner into on the ground Typically with most satellite programs Cert Letter IATO Re-accredit Accred Letter  Sustainment A Factory Crypto Alg Spec New K  Upd Key Spec  KMP Test Key  Known t Imple use of reprogrammable ECUs by not only permitting ECU The second issue that use these tools and to develop an ADF by the distribution agent to an ECU This process has dependencies an ECU from any of the segments 1 MOS Transmits the ADFs to all appropriate and authorized ECUs 2 ECU Loads and verifies the ADF 3 ECU ADF accepted Valid Crypto Alg L Code Recertify ECU Crypto Alg lCode Approve  Encrypt Signed Alg CT&E Report Space Segment Key Spec Jate KMP ment  late Alg CASS Update Gold Standard T&V CASS certification but also by facilitating the occurrence of testing MOS and validation prior to on-orbit modifications Without this validation a self-induced denial of service could occur due to update errors Update Gold Standard  validate Alg one or a limited number of space platforms Maintaining the ability to reprogram was identified concerned how to maintain the ground infrastructure to support ADF development Recent DoD satellites have supported their mission for up to 20 are usually developed are prepared for distribution  System is operational Steps Required Figure 4 is a requirement was created that called for a redundant unit is built for testing and anomaly resolution This historical precedent removed the burden that would have been placed on the system to produce an extra ECU The representative ground ECU developed during the acquisition part of the program enabled the years benefiting the warfighter but at over this period of time present a significant cost burden and technical challenge The preservation of the knowledge and expertise required to new programming over this time period is yet another significant challenge that must be met Complicating the maintenance of these capabilities is the fact that in implementation space ECUs or integrated in a customized over the expected mission lifetime and oftentimes beyond the expected lifetime would need to be on the MOS and ECU The process can take place in-band via the mission data link or out-of-band via the TT&C link for satellites Alternatively for the Terminal Segment the process will most likely occur in-band via the mission link or when necessary by physical access to the ECU The distribution agent can send the ADFs to the various segments in parallel in addition to specifying the method of transmission of the ADF Preconditions DAA authorizes the distribution of new ADFs  ADFs a YES Install ADF 5 DAA NS 7 a representative ECU a cost The longevity of satellites will require the supporting ground infrastructure and technology used in the development and testing of the ADF to be maintained throughout the satellite's life The evolution and updating of both the reprogrammable technology e.g architecture cell structure and the software operating system and tools and hardware used to develop simulate test and compile the reprogramming a fundamental feature of all satellite development integration and operations contracts and agreements Distribution Use Case Scope Distribution of a graphical representation of the steps identified below The ECU is representative of request Figure 3 Development Use Case Steps Issues Identified This 


b NO Log rejection 4 ECU Send Receipt to MOS MOS  ADFs have been properly installed in the system ECUs  An ADF Transition Strategy exists  The DAA issues a directive to proceed with the transition ECU Load  Verify ADF ADF Accepted Figure 4 Distribution Use Case Steps Post Conditions ADFs are delivered to appropriate ECUs  ADF delivery validated by receipt from ECU  ADF installation verified by receipt from ECU Steps Figure 5 is a graphical representation of the steps identified below The ECU can represent an ECU from any of the segments 1 MOS Sends key material for the new ADF to the ECU in accordance with the KMP 2 ECU Verifies upon receipt of the new key whether the algorithm identified in the key tag is present within the ECU a YES Do nothing b NO Log error 3 ECU At key rollover ECU begins use of new ADF 4 MOS Confirms to itself and to DAA that the system has successfully transitioned to the next cryptographic algorithm Issues Identified This use case brought up two issues first who should be designated the ADF distribution agent and second how the risk of failure in the ADF transition could be reduced The first issue is discussed in Section 6 Products and Infrastructure As for the second issue any changes to an operational system's critical components add risk The transition itself is risky enough without having dependencies on preceding steps that could prevent the transition It became evident that as many of the preceding steps as possible needed to be included within the distribution use case The inclusion of these steps was achieved by having the ECU upon receipt and validation of the ADF automatically install the ADF within its spare memory By having the ECU autonomously completing all the necessary steps the goal of reducing the risk of failure from the ADF transition was achieved To support the reduction in the risk of failure from the transition the SOC or MOS depending on the system and ECU capabilities may perform on-orbit testing because the ADF is installed prior to its operational transition This capability may require additional support which would include test keys for the space-born ECU in addition to surplus system resources that would not degrade the systems support to the warfighter This area needs further research Transition Use Case Scope Transition between the current ADF in operation and the new ADF uploaded and installed during the distribution use case This process has dependencies on the DAA MOSS and ECU PreconditionsDAA MOS ECU New Keys  Transition 1jConfirmation Verify Alg I1 Presence I_ Alg Transition Figure 5 Transition Use Case Steps Post Conditions ADFs are operational  Previous ADF is still available in the ECU Issues Identified One of the biggest issues in the ADF transition concerned the transition triggering mechanism Two ideas were developed for a triggering mechanism The first was the creation of a crypto midnight message to identify the point in time when the ADF transition would occur The crypto midnight message would require protection based on its system-determined classification level In this case the ECU would still require a new key for the ADF to become operational and the ability to understand the association between the new ADF and the new key This idea created unnecessary complexity for what could realistically be a simple task The second idea for the transition trigger mechanism was to rely on the system's rekey function If the key transmitted to the ECU had an associated key tag with the ADF name a new message type would not be required In recent system key specifications algorithm names have been identified in the key tag which 6 


further supported this idea and highlighted its simplicity Another plus point supporting this idea was the ECU's inherent ability to automatically transition to a new key which could be leveraged to drive the algorithm transition automatically The second major issue also identified in the distribution use case was reduction of ADF transition risk The goal of reducing this risk was achieved by completing all the preceding steps possible within the distribution use case which also would have a direct benefit to the warfighter The only remaining step was the transition itself By performing the installation of the ADF in the distribution use case it would be easy for an ECU to change a pointer within its memory to the new ADF to make the transition Therefore the ECU would be capable of transitioning without degrading the performance of the system and directly affecting the warfighter This capability alone makes it feasible for the system to update ADFs to protect warfighters in the future without interrupting critical communications ADF transition risk issues were initially identified in Ensuring The Security Of Warfighters Satcom Via Programmable Cryptographic Devices which discussed the costs versus the benefits of reduced transmission time By minimizing the steps in the transition the use case minimizes the time for transition In support of reducing ADF transition risk in the use case the ECU will report back to the MOS if the associated ADF for the new key does not exist This is a precautionary measure to ensure that every ECU that is being prepared to transition is capable of doing so The third major issue concerned the need for an ADF rollback use case which was developed to handle a rollback to the previous ADF if a problem arose from the use of the new ADF After a thorough review it was determined that the transition use case supported the necessary steps to achieve rollback given the key's association to an ADF The final piece needed to support rollback was a requirement to mandate the ECUs to hold the previous ADF for a defined period before purging their memory Requiring a system to store the last known good state is a best practice used within most systems to allow a system to revert to that known good state in the event of transition failure The required time period that the ADF should be held in memory would be determined by the system To leverage the transition use case to support rollback an immediate key change would need to occur The emergency re-key function found within most ECUs and supported by their systems enables the immediate transition to a new key by superseding the existing key Because more than one method i.e in-band or S-band,5 often can be used to communicate with the satellite one of the communication pathways could be utilized to send the new keys A related concern is that the terminal ECUs would not be able to receive the emergency re-key if the mission data link is lost which may require user intervention to re-key the ECUs The ECUs may need to be capable of automatically reverting to the previous ADF if the connection is lost for a predetermined period This is an area in which more work needs to be done One consideration in conjunction with the period of time that the ECU has to hold the previous ADF is the number of ADFs the system would be required to transition within that period To limit the burden on a space ECU it may be necessary to allow only one ADF transition to occur during that period thus reducing the potential SWAP impact to the payload The more ADFs that are reprogrammed within the holding period the more memory required within the ECU Use Case Summary Since the durations of three use cases have to be summed it becomes clear that the total time for transition to a new ADF could be quite lengthy The longest transitional stage obviously is the development stage during which there are a lot of participants and steps The time between the distribution and the transition would depend on what was deemed to be a necessary threshold number of ECUs for the ADF to be installed prior to transitioning This threshold would most likely be determined by the DAA The total time for transition could be greatly shortened if there were an immediate need for a shorter transition time due to a potential compromise or technological advance The assumption here is that in most cases the useful life of an algorithm would be known and there would be an opportunity to transition to a new cryptographic algorithm prior to the algorithm expiring 5 REQUIREMENTS DEVELOPMENT Throughout the use case development process various requirements were created for resolution of critical issues that was necessary to support the functions of the use case These requirements in conjunction with the requirements derived from the use cases created the complete set of CMI reprogrammability requirements The goal in this process was to create a set of requirements that can be implemented across all IA functions to reduce redesign per function Although the requirements may be program specific they can broken down into six main categories 7 S-Band for DoD systems is a low data rate band such as the SpaceGround Link Subsystem SGLS and the Unified S-band USB frequency between 1.7 GHz to 2.3 GHz These bands enable communication between the SOC and the satellite in support of launch early orbit activities anomaly resolution LEO&A and for TT&C during normal operations 7 


 Algorithm integrity specifications Identified integrity mechanisms for ADF receipt and retrieval from storage  Storage specifications Identified the verification and validation required when the ADF is pulled from memory in addition to the encryption required during storage Storage specifications also would include any assured hardware backup6 required within the ECU  Margin Identified the memory and processing capability required in excess of supporting the initial set of cryptographic algorithms and security functions for the mission  Process functions Identified the steps required for receipt installation and transition between ADFs in addition to supporting requirements Examples include dynamic transition to the new ADF without a degradation of capabilities ADF installation while not affecting other cryptographic functions operations and use of various algorithm suites  ECU management Identified the management functions that the ECU would support including the logging and auditing of specific events such as the receipt and installation of the new ADF  Delivery Identified the approved process for delivery of the ADFs to the distribution agent To complete the use case development process allocating the requirements to the correct document to be properly verified and tested was essential The two documents which authoritatively support CMI would be a System Technical Requirements Document TRD and a System Security Requirements SSR document An SSR is a document created by the NSA which defines the ECU requirements necessary for the ECU to be certified for use in DoD systems System-level requirements would be allocated to the system TRD and then flowed to the segments These requirements would include detailed exchanges between system segments and external entities to ensure synchronization ECU-specific requirements would be allocated to the SSR as an appendix Inclusion of the CMI ECU requirements into the SSR would ensure that products being reviewed by the NSA Technical Review Board are understood as a whole with all associated requirements The inclusion of the requirements also ensures that no conflicts exist between new requirements supporting CMI and existing SSR requirements Process Summary As identified in this paper many steps need to occur for a program to become CMI compliant in regard to reprogrammability These steps documented above can be leveraged for all future programs Figure 6 shows how the steps all fit together These steps and 8 6 An assured hardware backup is equivalent to an Application Specific Integrated Circuit ASIC or another highly reliable nonprogrammable device Simultaneous use of a reprogrammable technology and an assured hardware backup will ensure continued operation if the reprogrammable component fails supporting details are the beginning of filling the knowledge gap revealed by CMI Use Case Development Review Review Policies System Development IA Functions Requirements Allocation Requirements TRD Derivation Distribution Transition SSR Figure 6 Process Summary 6 PRODUCTS AND INFRASTRUCTURE The IA product trade space for a satellite program includes commercial-off-the-shelf COTS government-off-the-shelf GOTS and program-developed products Identifying early in the program the existing IA products or those that are under-development that meet program requirements will reduce program risk which in turn will translate to cost and schedule savings The added benefit of integrating these products and more specifically the GOTS products into a satellite system will be a reduced burden of CMI on the satellite program GOTS products will be required to be CMI compliant which will reduce the burden on the program to determine how it should implement CMI IA products for a satellite system continue to be among the highest-risk products because of their lengthy certification process in addition to the continuously changing threat environment that the products need to mitigate When considering GOTS or COTS products under development the factors that need to be examined are requirements alignment product delivery dates and finally funding stability An analysis of alternatives should be done to ensure that the product chosen is the one best suited to the program As a risk-mitigation step the program might consider an offramp if there is significant risk of a late or incompatible product Upon selection of a product a best practice is to sign a Memorandum of Agreement MOA between the program and the developer or organization that will be providing the product The MOA ensures that the satellite program will purchase the ECUs and ensures that the requirements and timeline will continue to be aligned After a product selection is made the satellite program should communicate regularly with the supplying organization to avoid surprises due to changed requirements or delivery date slips For DoD systems holding discussions with one of the Service's Cryptographic Modernization Program Offices is 8 


advisable The program office would be aware of currentproduction ECUs in addition to ECUs under development Overall utilizing GOTS and COTS products for any program especially space programs in which research and development are extremely costly is essential for cost containment Additionally the use of these existing products facilitates interoperability with other ECU products which is one of the goals of CMI Relying on existing products is similar to relying on an existing infrastructure Once the requirements are in alignment and the product or infrastructure becomes available integration becomes less burdensome This will eventually become the case once the Key Management Infrastructure KMI becomes available to deliver ADFs An issue in the implementation of reprogrammable ECUs is the difficulty in determining the best method by which the ADFs should be delivered and who should be designated as the distribution agent Although this issue is not space specific due to the long lead-time required for development of space ECUs the issue is an especially time-sensitive one for space This long lead-time was validated by NSA IAD Director Daniel G Wolf who in 2005 said that the NSA established a Space IA Special Program Office to consolidate all of our support to the important and longlead-time programs 5 The end state for the ADF delivery method should be the use of the NSA KMI which was identified as a means to support the delivery of new algorithms in addition to the keys that support those algorithms Unfortunately the KMI program does not plan to support algorithm delivery in the foreseeable future Delaying this delivery support prevents near-term programs from becoming KMI compliant for algorithm delivery which in turn requires alternatives for ADF delivery to be considered After the KMI protocols for algorithm delivery are provided and adopted by the users this problem will become a non-issue How a program should approach this issue and the alternatives for addressing it are identified in Section 9 Phasing in the Transition When considering the choice of ADF distribution agent the primary focus should be on the scope of the ECUs that are affected Are the ECUs within one segment or in all segments or are they part of a larger group in which the satellite system is also just a part An associated issue relates to the ECUs all having the same design and their supporting a standard set of protocols If a limited set of ECUs is within a segment then perhaps that segment could be delegated as the distribution agent A best practice would be to have the MOS be the distribution agent so that the MOS would be aware of all the impacts to the system Instead of the MOS receiving a notice from the Space Segment that a transition is occurring the MOS would be in the driver's seat and would know about a transition before or at the same time that the Space Segment knows about it 7 PHASING IN THE TRANSITION As stated at the beginning of this paper a satellite program's goal should be complete compliance with CMI Because the definition for reprogrammability within CMI is limited and because it is not space-centric there is room for interpretation of the definition Depending on how the definition is interpreted a program may move forward in a way that meets the intent of the CMI and does not compromise the integrity of the satellite system This issue becomes important when discussing various opportunities for phasing in functions and technology to ensure the longterm success of a program The following subsections describe a proposed phased approach for a few areas in which a transition may be of benefit in achieving complete reprogrammability in space Level of Reprogrammability Determining the level of reprogrammability and which components of an ECU can be reprogrammed could follow a phrased approach The three potential levels of programmability could be 1 reprogramming a subset of the ECU's cryptographic algorithms and support software 2 reprogramming all ECU cryptographic algorithms and support software or 3 reprogramming all ECU cryptographic algorithms and support software in addition to ECU management functions Operational algorithms supporting COMSEC and TRANSEC would most likely be the first set of cryptographic functions in the subset that are found to be reprogrammable The second set of functions to consider is the set that supports the ECU itself which would include cryptographic algorithms supporting ADF integrity confidentiality wrapping unwrapping and key storage Applying this phased approach to the level of programmability meets the intent of the CMI without putting excess stress on the first space system to implement CMI As more systems are launched with reprogrammable ECUs the more functions that may be considered for reprogrammability Distribution For ADF distribution a common secure method or common set of methods to reprogram the ECUs needs to be defined to simplify and to standardize infrastructure support for secure rapid and responsive reprogrammability and software ordering distribution and loading which should be the goal of KMI's algorithm delivery capability in the future Meanwhile the near-term solution for ADF delivery will have to draw from the space contractor's proprietary methods Although this solution will meet the intent of the reprogrammability tenet of CMI it may not realize the full potential of connecting to the KMI This potential will be defined by the level of ECU reprogrammability which will determine if the ECU can support changes to the security functions required for KMI interoperability The medium-term solution is for programs to integrate the KMI algorithm delivery protocols once they become available into the ECUs while still utilizing their proprietary methods until KMI becomes available The final 9 


operational capability should be to implement the KMI protocols and utilize them upon product initialization This phased approach is necessary given that the KMI program has yet to deliver the algorithm delivery protocols Technology In Ensuring The Security Of Warfighters Satcom Via Programmable Cryptographic Devices various existing and emerging technologies were identified that could be leveraged for space applications The driving factor behind the use of emerging technology given the cost and inaccessibility of satellites is that absolute reliability and error-free operation are necessities The space community typically insists on the use of proven technologies e.g those at Level Six or Level Seven and above on the Technology Readiness Level scale used by the National Aeronautics and Space Administration NASA and techniques to ensure the reliability of those technologies One path forward in beginning to use emerging technologies without assuming a considerable amount of risk is to utilize an assured hardware backup for reprogrammable components This option provides the ability to reprogram the ECUs if the opportunity presents itself while ensuring that the mission of the system will not be compromised while testing a new technology This ability is especially applicable to critical functions within the ECU After space-qualified Field-Programmable Gate Arrays FPGAs or other reprogrammable technologies have proven their reliability in space environments an assured hardware backup can be minimized to reduce SWAP on the payload.7 Taking steps like these enables reprogrammability to be tested in space without jeopardizing expensive national assets The bottom line is this If the technology is available and proven it should be leveraged in the system Figure 7 shows the previously identified CMI transitions that can occur to support space-borne reprogrammable ECUs Although these transitions are stacked one on top of another in the figure in actuality they are independent of one another therefore one transition may occur prior to another Only time will tell how well space-borne ECUs will converge on complete compliance with CMI requirements Phasing to Space CMI Compliance All Algorithms Level of Operational Aorfithmsg  All Algorithms  Suppo Software  Reprogrammability Supo1rt Softwar Suport Software Managmient Functions Implementation Contractor Specified Transition tece M KMI One-T ime Redundant FPGA wvith Assuedd Space Qua||lified Technology Prlbogmmabile Hardwar Backup FPGA or equivaleint Figure 7 CMI Phased Transition 10 7FPGAs are being considered for use or are currently being used for other 8 THE CALL FOR AN NSA SPACE CMI POLICY Since the initiation of CMI no official NSA guidance related to space has been developed The NSA should create an official guidance document on how to apply the CMI tenets and more specifically how to apply the reprogrammability tenet to support future satellite programs Such guidance will help to alleviate confusion and will create a baseline upon which the CMI compliancy of other satellite programs may be assessed This baseline could include various aspects of reprogrammability but one that would be of great benefit is the margin memory and processing requirement for the ECU This margin requirement could be defined as a percentage over the initial required amount of memory and processing capability However if this margin were not calculated or estimated correctly it could defeat the entire purpose of CMI and negate all the work involved in making the ECU reprogrammable Each program would most likely have a different margin based on the number of IA functions supporting its mission which is why a baseline percentage would be the best way of identifying the margin requirement Although this margin may change over time due to technological or mathematical advances it would be a great starting point for a program The NSA will be the most-qualified organization to perform the margin analysis for the program The suggested process for performing a margin analysis should include a review of historical trends of the algorithms size and complexity discussions with subject matter experts in cryptographic algorithm development and a review of current operational algorithms and existing algorithms for future use This process should also include consideration of reserve capacity for increased code size to support function enhancements Each program and perhaps each of the programs ECUs depending on their function and size may have a different margin requirement 9 CONCLUSIONS This paper and the process it details help to fill a void in the knowledge base for implementation of cryptographic space products that was revealed by the NSA Cryptographic Modernization Initiative which forces space-borne ECUs to become reprogrammable The long lead-time for spaceborne ECUs makes it necessary to reduce the time spent on concept development so that efforts can be focused on ECU development This paper provides a foundation for a satellite program to begin its reprogrammability efforts The first step in this process is understanding reprogrammability and the space IA functions that it affects The next step calls for reviewing existing policies and publicly available best practices Reviewing existing documents will reduce rework especially given that CMI is required for all future parts of communication satellite payloads that are no less critical to mission effectiveness than ECUs 10 


programs and given the probable creation of additional policies and development of new best practices The primary focus of this paper is on three use cases which provide scenarios of the necessary steps to support reprogrammable ECUs Throughout the descriptions of the use cases several issues are identified and their solutions are discussed in depth These issues concern the selection of the ADF distribution agent and the ADF transition which all programs implementing CMI will need to address and can begin to address given the information provided in this paper Although each system may have a slightly different set of requirements the requirements will be similar to those derived from the use cases Products and infrastructure are addressed in this paper because one way of achieving CMI compliance is though the use of existing products Although integrating an existing product into a satellite system might not solve the CMI compliance issue it will certainly reduce the effort required for development The method of distributing the ADFs which should be done by KMI in the future serves to support the products Relying on an existing infrastructure is similar to relying on an existing product Once the requirements are in alignment and the product becomes available integrating the product into the system is easy Problems arise when there is limited understanding of the available functions or when product availability is unknown In the future KMI will be capable of supporting ADF distribution which will reduce a portion of the CMI implementation risk In the near term KMI will be phased in while the contractor-specified distribution method is phased out Technology the enabler of reprogrammable ECUs also needs to be phased in over time As each technology matures to support the space reliability requirement the assured backup can be minimized The remaining feature that should be phased in over time is the level of reprogrammability As satellite programs become more confident of their successful performance and the program's technology becomes more reliable additional portions of the ECU will become reprogrammable Finally the availability of a NSA policy on space-borne CMI compliance would greatly support future compliance efforts and allow for consistency and reuse of potential products In conclusion a CMI-compliant satellite system that supports the warfighter of the future is entirely feasible REFERENCES 1 Leveraging Cybersecurity Interview with Daniel G Wolf Military Information Technology Online Edition Volume 8 Issue 1 February 9 2004 www.militaryinformation-technology.com/article.cfm?DocID=389 Satcom Via Programmable Cryptographic Devices MILCOM 2005 October 19 2005 3 Kurt Bittner and Ian Spence Use Case Modeling 1st Edition Boston Addison-Wesley Professional 2-3 2002 4 CNNS Instruction No 4009 National Information Assurance Glossary June 2006 5 Assurance Provider Designing a Roadmap for Information Security Interview with Daniel G Wolf Military Information Technology Online Edition Volume 10 Issue 1 January 28 2006 http://www.militaryinformation-technology.com/article.cfm?DocID 1294 ACKNOWLEDGMENTS I would like to thank the TSAT Space IA team Maj Ryan Mattson Brian Sessler and Aaron Kuhn and the NSA Space SPO for their continued support to ensure the success of reprogrammable ECUs to support the warfighter BIOGRAPHY Joseph D Bull a Certified Information System Security Professional CISSP has more than five years of experience in system security engineering Currently he is an Associate at Booz Allen Hamilton where he is the Lead Engineer for Information Assurance for the Space Segment of the Transformational Satellite Communications System TSAT program He advises on various aspects of the TSAT architecture including TRANSEC COMSEC and CMI issues impacting the Space Segment and TSAT system He has also participated in the ECU requirements development process for the Air Force Cryptologic Systems Group's GOE Increment One and AVE Increment One TT&C ECU products While employed at Backbone Security he performed program assessments on a government agency enterprise Additionally while at Backbone Joe developed an automated tool to increase the timeliness and reliability of vulnerability assessments While working at the National Security Agency's Information Assurance Directorate he performed research and development on Bluetooth end-toend encryption utilizing Xilinx FPGAs Bull has an MBA from the Smeal College of Business at The Pennsylvania State University a BS in Electrical Engineering from Penn State and a BA in Physics from East Stroudsburg University 2 Joseph D Bull Ensuring the Security of Warfighters 11 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es  the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollobs, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathmatiques Appliques de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


