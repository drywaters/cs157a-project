Mahalanobis Distance and Projection Pursuit Analysis for Health Assessment of Electronic Systems Sachin Kumar Vasilis Sotiris and Michael Pecht Prognostics Health Management Lab Center for Advanced Life Cycle Engineering CALCE University of Maryland College Park MD 20742 301-405-5323 pecht@calce.umd.edu Abstract This paper presents a Mahalanobis Distance and Projection pursuit analysis based prognostic and diagnostic approach for early detection of anomalies in electronic products and systems These have been used to detect deviations in system performance from normal operation and are efficient at characterizing products with short field 
histories A case study is presented to demonstrate that an abnormal system can be distinguished from a normal system and that a new system can be characterized based on existing baselines from different computer models 2 TABLE OF CONTENTS 1 INTRODUCTION 1 2 EXPERIMENTAL SETUP  1 3 METHODOLOGY To IDENTIFY ABNORMALITIES IN ELECTRONIC PRODUCTS  3 4 DATA ANALYSIS AND DISCUSSION 5 5 FAULT ISOLATION 
DOMINANT VARIABLES USING PROJECTION PURSUIT ANALYSIS 7 6 
CONCLUSIONS 7 ACKNOWLEDGEMENT  7 REFERENCES  8 BIOGRAPHY  9 1 INTRODUCTION Prognostics and health management PHM is a process of predicting the future reliability of the system by assessing the extent of deviation or degradation of a product from its expected normal operating conditions in a preemptive and opportunistic manner to the anticipation of failures This can enable continuous autonomous real time monitoring of the health conditions of a system by means of embedded or 
attached sensors with minimum manual intervention to evaluate its actual life-cycle conditions to determine the advent of failure and to mitigate system risks The term diagnostics refers to the detection and isolation of faults or failures and prognostics refers to the process of predicting a future state of reliability of the system based on its cuffent and historic conditions The aim of failure prognosis is intended to identify and estimate the advancement of fault conditions to system failure 1 
1 1-4244-1488-1/08/$25.00 
C 2008 IEEE 2 IEEEAC paper 1640 
Version 2 Updated Nov 29 2007 Quantification of degradation and the progression from faults to failure in electronic products and systems is a challenging task Gu et al 2 identifies six levels of prognostics implementation for electronics from on-chip packaging to complete systems of systems They provided an approach for prognostics implementation at the various levels of electronics based on failure modes mechanisms and effects analysis Zhang et al 3 proposed a model to assess intermittent as well as hard failures The model is 
a fusion of two prediction algorithms based on life consumption monitoring and uncertainty adjusted prognostics Vichare et al 1 4 5 proposed methods for monitoring and recording in-situ temperature loads This includes methods for embedding data reduction and load parameter extraction algorithms into the sensor modules to enable reduction in on-board storage space low power consumption and uninterrupted data collection Two prognostic approaches based on classification theory are presented in this paper They are capable of system level anomaly detection 
in multivariate data-rich environments One methodology uses the Mahalanobis Distance MD and the other uses a projection pursuit analysis PPA to analyze on-line system data Both approaches are used to monitor the health of the system and identify onsets and periods of abnormalities The PPA approach is also used to find the contribution of the system parameters as a means of identifying dominant and potentially faulty parameters 8 11 Experiments were performed on notebook computers to generate data and validate the analysis approaches The experimental details 
the algorithmic approach to anomaly detection and a case study are discussed 2 EXPERIMENTAL SETUP To demonstrate the feasibility of the proposed methodology experiments were conducted to define a baseline for healthy systems and to identify specific parameter behavior Notebook computers were exposed to a set of environmental conditions representative of the extremes of their life cycle profiles The performance parameters the fan speed CPU temperature motherboard temperature videocard temperature C2 state C3 state CPU usage and CPU throttle were monitored in-situ 1 


during the experiments The baseline of healthy systems was used to differentiate unhealthy systems from healthy ones The proposed anomaly detection methodology was verified by injecting an artificial fault into the system Results from the study demonstrate the potential of the approach for system diagnostics and prognostics Operational and environmental ranges and profiles that constitute a healthy system were used to replicate the real time usage of the notebook computer Software was installed on the computer to be used A set of user activities was defined and simulated using script file to run on notebook computers An artificial fault was injected into the notebook computers to create and detect any change in system dynamics Table 1 Environmental conditions TemperatureHumidity 1 5\260C with uncontrolled RH 2 25\260C with 55 RH 3 25\260C with 93 RH 4 50\260C with 20 RH 5 50\260C with 55 RH 6 50\260C with 93 RH Table 2 Experiments Performed Power Setting Usage Level Environmental Antivirus Window 1 6 Idle AC adpter Word Processing 1-6 AC adapter Antivirus I 6 when battery is Music Movie fully charged Antivirus 1 6 Video Game 1 6 Antivirus Antivirus Window 1 6 Idle AC adapter Word Processing 1 6 when battery is Antivirus initially fully Music Movie 1 6 discharged Antivirus Video Game 1 6 Antivirus Antivirus Window 1 6 Idle Word Processing 1 6 Battery only Antivirus Battery only Music Movie 16 Antivirus Video Game 1I 6 Antivirus Experiments were performed on ten identical notebook computers representative of the state-of-the-art in 2007 notebook computer performance and battery life nearly three and half an hours on a single battery For the experiment six different environmental conditions were considered as shown in Table 1 For each temperature/humidity combination four usage conditions and three power supply conditions were considered Factorial experiment were designed to study the effect of each factor on the response variable as well as the effects of interactions between factors on the response variable Table 2 shows the list of all 72 experiments Each computer was turned on for 30 minutes before starting the experiment The software for in-situ monitoring was installed on the notebook computers along with Windows XP Professional operating system Microsoft Office Front page WinRunner Spybot Winamp Real Player Visual Studio Java 5 Minitab iTunes Adobe Photoshop MATLAB Winzip and McAfee Antivirus Selection of this software was based on the authors discretion and experience A script file was written using WinRunner software to simulate user activity Antivirus application McAfee v8.0 was configured to run on the laptop all the time A set of files doc mp3 ppt pdf xls was kept in a folder to be used during simulation Notebook computers were kept at room temperature between each test condition When the laptop was powered by the AC adapter when the battery was fully charged the test duration was 3.5 hours When the laptop was powered by an AC adapter when the battery was fully discharged the test duration was determined by the time it took for the battery to fully charge When the laptop was powered by its battery only the test duration was determined by the time it took for the battery to fully discharge Same usage conditions were applied on all notebook computers to achieve time synchronization between computer and software application responses The notebook computer's power mode was always set to ON The screen saver and hibernation option were disabled to prevent these functions from occurring during an experiment The wireless capability of notebook computer was disabled due to the limited wireless connectivity inside the temperaturehumidity chambers Four level of notebook computer usage were chosen 1 Idle system In this category the operating system is loaded all windows are closed user input from the keyboard or mouse optical drive are disabled USB or Firewire peripherals are not attached 2 Office productivity In this category the usage condition is designed to simulate an office work environment The simulator work is designed to read a word document as well as prepare a new word document The simulator opens the file explorer and locates a file to be opened The simulator opens a technology benchmark report word document of 88 pages and size of 2.6MB The simulator reads through the document and uses arrow keys to move page up page down and selects a paragraph to copy The simulator opens a new document from the word toolbar and pastes thee copied section to a new document The simulator resizes both documents to make it easy to 2 


toggle between the two documents The simulator switches to the original document and reads through pages and copies additional paragraphs and pastes again into the new document as new paragraphs The simulator also types a new paragraph into the new document With these activities the simulator creates a five-page summary and saves it by pressing the save button in the word toolbar Then it saves the file through invoking the save as file explorer and providing a file name for the new document The simulator does a cleanup by resizing and closing all opened document The simulator removes the new files from the desktop and pastes into another folder Finally the simulator closes all opened file explorer windows 3 Media centerIn this category the usage condition is designed to simulate an entertainment need Winamp v5.24 media player started from the start menu The file explorer window is opened by pressing the open button in Winamp MP3 music files are stored on the hard drive and selected to play in Winamp Music is stopped after 4 minutes followed by shutting down the Winamp player window Real media player vlO.5 is started from the start menu The file explorer window is opened to select video files by pressing the open button in Real player Video files from a DVD are selected by maneuvering through the file explorer window and then played in Real player Movie screens are resized to full screen The movie is turned off after 90 minutes and Real player closed 4 Game mode In this category the usage condition is designed to simulate gamming Quake Arena II was started from the start menu and single player option is selected to start the game After an hour of play the game is stopped and exited 3 METHODOLOGY To IDENTIFY ABNORMALITIES IN ELECTRONIC PRODUCTS The Mahalanobis Distance MD methodology is a process of distinguishing data groups 6 9 The MD measures distances in multi-dimensional spaces by considering coffelations among parameters The distance is sensitive to the correlation matrix of the healthy group The MD values are used to construct a normal operating domain also known as Mahalanobis space to monitor the condition of a multidimensional system Health of a system is defined by several performance parameters These parameters are standardized and the MDs are calculated for the normal group These MD values define the Mahalanobis space which is used as a reference set for the MD measurement scale The parameters collected from a system are denoted as Xi where i  1,2 m The observation of the ith parameter on the jth instance is denoted by Xij where i 1,2 m and j  1 2 n Thus the m x 1 data vectors for the normal group are denoted by Xj where j  1,2,...,n Here m is the number of parameters and n is the number of observations Each individual parameter in each data vector is standardized by subtracting the mean of the parameter and dividing it by the standard deviation These mean and standard deviation are calculated from the data collected for normal or healthy system Thus the standardized values are 1 Z  Xi 260 i  i=1,2  m j=l 2 n where xi I XI and S1 n-i Next the values of the MDs are calculated for the normal items using MD zTC-Iz 2 m where Z1T z11,z21.Zmj is a vector comprises of zij  zj is transpose of ZjT and C is correlation matrix calculated as n i~T n-1 j=f Next the test system is considered to determine its health The MDs for the test system are calculated after the parameters are standardized using the mean and standard deviation for normal-group The resulting MD values from test system are compared with the MD values of the normal or healthy system to determine test system's health Projection Pursuit Analysis The Projection Pursuit Analysis uses a Principal Components Analysis PCA least squares optimization LS and a Singular Value Decomposition SVD treatment of the data PCA is used in a wide array of applications to reduce a large data set to a smaller one while maintaining the majority of the variability present in the original data It's also very useful in providing compact representation of temporal and spatial correlations in the fields of data being analyzed PCA facilitates a multivariate statistical control to detect when abnormal processes exist and can isolate the source of the process abnormalities down to the component level Two statistical indices the Hotelling Squared T2 and squared prediction error SPE are used in the PCA The SPE statistic is related to the residuals of process variables that are not illustrated by the PCA statistical model and is a reliable indicator to a change in the correlation structure of the process variables The SPE physically tests the fit of new data to the established PCA models and is efficient at identifying outliers from the PCA model 7 The Hotelling 3 


T2 score measures the Mahalanobis distance from the projected sample data point to the origin in the signal space defined by the PCA model The primary objectives of principal component analysis are data summarization classification of variables outlier detection early warning of potential malfunctions and isolation of fault PCA seeks to find a few linear combinations which can be used to summarize the data with a minimal loss of information Let X  xl X2 X3,..,xm be an m dimensional data set describing the system variables The first principal component is the linear combination of the columns of X i.e the variables which describes the greatest variability in X tl=Xpl subject to pll=l  In the mdimensional space Pt defines the direction of greatest variability and t1 represents the projection of each sample data point onto pi The second principal component is the linear combination defined by t2=Xp2 which has the next greatest variance subject to IP2l 1 and subject to the condition that it is orthogonal to the first principal component t1 10 Essentially PCA decomposes the original signal X as m X TpT E tip T i=l 3 orthogonal matrices U and V are called the left and right eigen-matrices of X Based on the singular value decomposition the subspace decomposition of X is expressed as X=Xs+Xr UsSs s  UrSrV r 4 The signal space S is defined by the PCA model and the residual subspace Sr is taken as the residual space 11 The diagonal Ss are the singular values sI Sk  and SkII,...,sm belong to the diagonals of Sr The set of orthonormal vectors U uI,u2  uk form the bases of signal space Ss The projection matrix Ps onto the signal subspace is given by Ps  Us Us 5 The residual subspace is the orthogonal complement of the signal subspace and the projection of the original data onto it can be expressed as Pr=I-P5 6 Any vector X can be represented by a summation of two projection vectors from subspaces Ss and Sr X Xs\261Xr=PsX  I-PAx 7 where p is chosen to be an eigenvector of the covariance matrix of X P is defined as the principal component loading matrix and T is defined to be the matrix of principal component scores The loadings provide information as to which variables contribute the most to individual principal components and can help isolate the dominant faulty variables In the approach used in this paper each variable is separately scaled to zero mean and unit variance One important feature of the PCA model is that it gives information about the noise structure of the original data which means that it can tell something about variables that do not dominate on the variance level but are indeed degrading or faulty Consequently it is desirable to exclude less influential principal components from the signal space defined by the PCA model which leads to the decomposition of X into the signal and residual subspaces The signal subspace is intended to capture the variables that are contributing to any abnormal process variability and the residual subspace will complement this by examining the variables that are effectively over-shadowed by dominant variables in the signal subspace It's important to note that faulty variables aren't always the ones that exhibit the greatest variability An example of this phenomenon is presented in the data analysis and discussion section of this paper Principal Component Subspace Decomposition Subspace decomposition into Principal Components can be accomplished using singular value decomposition of matrix X The SVD of data matrix X is expressed as X=USVT where S=diag\(sl sm Rn x m and S1>S2 sm The two The subspace decomposition can also be accomplished by the eigen analysis of the correlation matrix of X C which is expressed as follows where the columns of U are actually the eigenvectors of C and the eigen values of C are the squared singular values of the diagonal matrix S The eigen values provide a measure of the variance of each of the eigenvectors and determine the selection of the principal components and the number of principal components to choose C  n 1 n US U 8 Fault Detection Using PPA From the normal historical data one can derive the nominal normal system behavior statistics mean variance and from the above analysis the signal and residual subspaces From the subspaces we extract some statistics to describe the data distributions in two subspaces[12 One is the Hotelling f2 which measures the variation of each sample is the signal subspace For a new sample vector x it is expressed as 2 T 5-iUT\(9 xU S Ux 9 where S is the covariance of X and is equal to UTU Another statistic the squared prediction error SPE indicates how well each sample conforms to the PCA model measured by the projection of the sample vector onto the residual space SPE=lPAl 2 r=jj\(I-P 10 The process is considered normal if 4 


SPE and T72 where 62 and x2 are the control limits for the SPE and j2 statistics respectively given a 1-o confidence level These limits assume that x follows a normal distribution and 72 follows a  distribution with k degrees of freedom where k is defined to be the cut off for the number of principal components used in the PCA model Because SPE is a measure of the deviation in the residual space it can be used to identify when the current operation deviates from the expected in terms of parameters that are not dominant but still abnormal On the other hand the t will be more sensitive to the regular fluctuations that move the process away from normal based on the projections in the model subspace The two statistics function independently in this analysis although a combined index has been developed for process monitoring Yue and Qin 13 proposed a convenient alternative for merging the information from SPE and T2 For the purpose of this analysis though each statistic will be examined separately Fault Isolation and Contribution Plots After a fault has been detected there are several methods that can be used to determine the critical system parameters Contribution plots continue to be a widely used for fault diagnosis During monitoring of the system each new observation that is projected on the model and residual subspaces will have a unique impact onto each subspace respectively as discussed earlier The impact is quantified by calculating the contributions to the SPE and T2 The larger the contribution of an individual parameter the more likely it is that the parameter is the reason for the changes or faults The contribution of the mth parameter to the SPE is found by taking the squared residual associated with that parameter xm by SPEm UM2 12 The contribution of all parameter to T2 is given in terms of the SVD by T2 XUS'12UT 13 4 DATA ANALYSIS AND DISCUSSION The experiments were performed on ten new notebook computers with an assumption that these systems are representative of normal/healthy systems The MD value obtained from these datasets called Mahalanobis space is used to identify anomalies present in system Five thousand data points are selected from the experiments performed at CALCE An effort was made to demonstrate the capability of MD method to detect anomalies present in the test notebook computer and characterization of this computer model based on the baseline experiments In Figure 1 experimental data are used as a baseline to identify the anomalies present in a test abnormal notebook computer From Figure 1 the test system shows problems from the beginning that is verified by observing the data file of test notebook computer in which the fan was not operating and the three monitored temperatures were high by 10\260C The drop in MD value at 2700th data point is an indication that the computer was shutdown and then restarted which caused a temporary drop in these temperatures Very high values of MD at and around 3500th data point are due to the different correlation structure for CPU usage CPU throttle and CPU state variables Since the MD value is very sensitive to the correlation of different parameters the MD value corresponding to an abnormal observation is high Thus it can be inferred that the MD value is a good measure to identify anomalies present in the system 2824 32-.-Test abnormial Baseline X 20o 16 1240 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000  of sample points Figure 1 Comparison of MD values of abnormal test computer with baseline Based on our results we have seen that a test computer can be characterized using experimental data representative of healthy computers of different model This leads us to claim that a comprehensive baseline can be used to characterize the health of new computers without the need to conduct a whole set of experiments for that new computer regardless of the model One can observe that the MD values for the test normal notebook computer model are separated from the MD values corresponding to the baseline experiments Statistical metrics corresponding to the analysis are given in Table 3 Table 3 Statistics of MD value based on experimental data System/Stats Baseline Test Test Normal Abnormal Mean 0.91 4.27 17.65 Std Dev 0.65 1.401 5.14 1 st Quartile 0.53 3.19 16.01 3rd Quartile 1.03 5.32 19.87 The table gives the simple statistics of MD values for different notebook computer models used as the baseline test normal and test abnormal Average MD values for the test normal system are higher by 3.5 as compared to MD values for the baseline The abnormal test system shows very high MD values as compared to the baseline as well as 5 


200_ la i W test normal system baseline The average difference between MD a value for test normal and test abnormal is 13 This indicates that the baseline can be used for anomalies detection across computer models assuming the operating conditions are similar The data from the test normal computer are used as a baseline to identify the anomalies present in test abnormal computer A separation was observed between MD values of normal and abnormal system This strengthens the argument made earlier that the MD method can be used for anomaly detection Statistical metrics of MD values for this are given in Table 4 Table 4 Statistics of MD value based on Test normal data System/Stats Test Normal Test Abnormal Mean 0.83 10.72 Std Dev 1.16 3.13 1st Quartile 0.24 8.07 3rd Quartile 0.88 13.31 Table 4 gives the simple statistics of MD values for the normal and abnormal test systems Average MD values of the test normal system are 0.8 The average difference between MD values of the test normal and test abnormal is 10 By looking at the statistics for the MD values for the different systems it can be observed that the difference in average MD value for the test normal computer in both cases are equivalent to the difference in MD values for the abnormal computer and the trend for the MD values in both cases is similar This illustrates that the baseline can be used for characterization of a new computer model This will allow us to characterize a new notebook computer regardless of the model and reduce the time for analysis The following paragraphs discuss Projection Pursuit analysis approach for system diagnostics The goal of the Projection Pursuit analysis was to use Hotelling T2 and SPE statistics from a healthy computer and successfully classify and detect faults in a new computer of the same model Two healthy baseline sets of T2 and SPE statistics were derived from two sources one from a CALCE baseline set based on 10 healthy computers of a different model and the other based on one computer of the same model From the analysis the known faulty computer was identified as abnormal based on both comparisons Figure 2 and Figure 3 are used as example plots to show the analysis results by plotting the SPE and Hotelling T2 statistics for the above two scenarios versus the number of sample points The lower pink line indicates baseline healthy values for each statistic and the upper blue line indicates the test values for each statistic for the abnormal test computer It is clear from the plots that the abnormal test computer statistics are different from the baseline computer for both scenarios The first five principal components were used to form the model space and the remaining three for the residual space From these results we see that the variability of the process in both the PCA model and residual subspaces can be used to capture abnormal system behavior The detection is based on the geometry of the problem whose dimensions are established by the PCA model and residual subspaces The subspaces as discussed are constructed from the healthy data and represent a fixed frame of reference used to compare incoming new observations New observations are taken as a point in the multi-dimensional space and are projected onto the PCA model and residual subspaces respectively 21 16|Baselinel Test Computer r   t i 4kFLa 0 500 1 000 1 500 2000 2500 3000 3500 4000 4500 5000 Observatiorn Number Figure 2 Comparison of SPE scores of abnormal test computer with baseline 1300 16~00 0  1400 1200 1000 800 600 400-Baseline I t 1 4 Test Cotputer 1 T I~o  X 0 500 1000 1500 2000 2500 3000 3500 400 4500 500f Obser-vation Number Figure 3 Comparison of Hotelling T2 scores of abnormal test computer with baseline With the projection the new observation is reduced from its original dimension R to the lower dimension of the PCA model R'A where k is the number of principal components used to form the model subspace If the projection of the observation falls within the statistical control limits t2 and 62 of the model and residual subspaces respectively then it is taken as normal or healthy otherwise it's treated as abnormal or un-healthy Faults can be masked by the PCA model This can occur for example when the new computer starts to exhibit abnormal behavior yet the variability of test data in both the model and residual subspaces fall within the healthy control limits for the system For the baseline the statistics are modeled with patterns that are similar for both comparisons One of the explanations for this is that the baseline for the computers captures the necessary range and variability of normal operating conditions of such computer models Without the use of the control limits this analysis is left to identify the presence of 6 I  If I 1 


abnormalities between test and training data and also to identify the critical system parameters 5 FAULT ISOLATION DOMINANT VARIABLES USING PROJECTION PURSUIT ANALYSIS The model space is designed to capture the data that varies the most whereas the residual space is designed to capture the data that does not vary but contributes to a faulty state The residual space can therefore detect changes in the distribution from variables that are degrading or have faults and are not effecting the variance Below are the principal components for the entire subspace S Each principal component is composed of the eight parameters with a particular weighting as shown in Table 5 The model/signal subspace is composed of the first four principal components This was chosen based on iterative experimentation to best capture the faults The decision of how many principal components are chosen to represent the model/signal space is based on experience and understanding of the data at hand There are computational/statistical techniques that can provide estimates for the selection of the number of PCs to optimized results The remaining columns span the residual subspace Each variable is represented by each respective row of matrix S The first row shows the contributions of the fan speed and the rest show the CPU temperature motherboard temperature video card temperature C2 state C3 state oCPU usage and 0%OCPU throttle from top to bottom in matrix S Table 5 Principal Component of subspace S and parameter contribution S PCI PC2 PC3 PC4 PC5 PC6 PC7 PC8 Fan 0.999 0.019 0.001 0.001 0.001 0.002 0.004 0.001 Speed CPTm 0.005 0.076 0.042 0.048 0.000 0.484 0.856 0.153 Mother board 0.830 Temp Video card 0.000 0.048 0.107 0.093 0.002 0.670 0.490 0.537 Temp 00C2 0.001 0.060 0.018 0.091 0.994 0.001 0.002 0.001 State 00C3 0.015 0.730 0.384 0.554 0.102 0.005 0.025 0.011 StateI CPU 0.013 0.661 0.271 0.690 0.028 0.114 0.019 0.000 Usage CPU 0.001 0O13 0.872 0.439 0.033 0.166 0.038 0.OOS Throttle From the decomposition of S we can see that the model space variations should be dominated by the fan speed followed by C3 state OCPU throttle and usage In the residual subspace the temperature components are dominant We expect that the temperature variables to be highly dominant Changes in the temperature are expected in turn to be less obvious to changes in system variance and should contribute to the shape of the multivariate data distribution Such a distribution can be modeled as Gaussian mixtures but in general a hard task Intuitively if the fan speed is not functioning we expect that the temperature of the system will rise and become abnormally high This is at first hand validated by the dominance of the temperatures components as observed in the residual subspace in S Mathematically this is also validated through the parameter contribution plots to the T2 and SPE respectively as illustrated in the contribution plots shown in Figure 4 The contribution plots tell us which parameter is contributing the most to the projection onto each subspace 70 70 60 60 50 5040~~~~~~~~~~~~~3 U 4~~~~~~~~~~~~~~0 10 Parameters 0 0.002 0.035 0.077 0.079 0.004 0.523 0.158 Parameter Figure 4 Contribution plot of each parameters towards T2 on right and SPE on left It is shown that on the model space the fan speed is highly dominant and varies the most in terms of standard deviation This phenomenon masks the effect on parameters that are also exhibiting abnormalities but are overpowered by dominant parameters such as the fan speed The residual space statistic SPE captures the inverse information and identifies the parameters that are indeed abnormal but are not dominating in terms of variance Also interesting is the fact that the mathematics validate our intuition that because the fan isn't functioning properly the temperature sensors would be experiencing unusual readings Note that these results are based on picking the model space using k=4 that is the first four PCs in matrix S The selection of more PCs for the model space and consequently fewer PCs for the residual space will change the results slightly If all eight PCs are used to construct the model space then the SPE will be rendered ineffective although the results for the Hotelling T2 will improve Even though the results from the Hotelling T2 improve with the selection of more PCs the information available through the SPE is lost There are ways to select the optimum number of PCs necessary to optimize the information captured from both subspaces often the selection is purely based on experience or experimentation although there are statistical methods such as the maximum likelihood estimator MLE which can estimate the optimum number of PCs to use 6 CONCLUSIONS A set of experiments were conducted to establish the healthy or normal operation on a set of notebook computers subjected to range of usages and environmental conditions A test computer was then subjected to field use conditions and evaluated in-situ using Mahalanobis Distance and Projection Pursuit analysis techniques The Projection Pursuit analysis method was also used to identify 7 


key parameters for root cause analysis of anomalies This study emphasizes that the defined baseline can be used to characterize a new computer model This will allow us to characterize a new notebook computer regardless of the model and reduce the time for analysis In this study PPA and MD were independently used to identify the similarity of new observations to healthy data PPA performed this analysis in a reduced dimension based on an optimization criterion maximum variance It was also found that PPA can identify the faulty parameters based on the data whereas MD requires an understanding of the system The strength of PPA lies in the ability to decompose the signal and extract additional information not originally available used to identify faults in the system PPA overcomes masking effects when working with highly correlated data The strength of the MD method is that it preserves all the information available because it does not reduce the original dimensionality of the data The drawbacks of using just the MD method are that it cannot be used directly for fault identification and it is susceptible to masking effects With the MD results and our understanding of the system functionality four critical parameters were empirically identified the fan speed and the three temperature components CPU temperature motherboard temperature and videocard temperature In parallel in the PPA approach the principal component space also identified the fan speed as the most dominant and from the residual principal component space three temperature parameters were identified to be dominant mathematically confirming the earlier empirical conclusion The cross-validated result shows that these two algorithms can be used for fault detection and isolation The MD method can be used for quick fault detection at a system level and fault isolation can be made if related fault to MD signatures are available PPA can also be used when system faults are not known and where critical parameters need to be identified ACKNOWLEDGEMENT This work is sponsored by the members of the CALCE Prognostics and Health Management Consortium at the University of Maryland College Park REFERENCES 1 N Vichare P Rodgers V Eveloy and M Pecht Environment and Usage Monitoring of Electronic Products for Health Assessment and Product Design International Journal of Quality Technology and Quantitative Management 2\(4 235-250 2007 2 J Gu N Vichare T Tracy and M Pecht Prognostics Implementation Methods for Electronics 53rd Annual Reliability  Maintainability Symposium RAMS Florida 2007 3 G Zhang C Kwan R Xu N Vichare and M Pecht An Enhanced Prognostic Model for Intermittent Failures in Digital Electronics IEEE Aerospace Conference Big Sky MT March 2007 4 N Vichare and M Pecht Enabling Electronic Prognostics Using Thermal Data Proceedings of the 12th International Workshop on Thermal Investigation of ICs and Systems Nice Cote d'Azur France 27-29 September 2006 5 N Vichare P Rodgers and M Pecht Methods for Binning and Density Estimation of Load Parameters for Prognostics and Health Management International Journal of Performability Engineering Vol 2 No 2 April 2006 6 A Fraser N Hengartner K Vixie and B Wohlberg Incorporating Invariants in Mahalanobis Distance based Classifiers Application to Face Recognition in International Joint Conference on Neural Networks IJCNN Portland OR USA Jul 2003 7 J Edward Jackson Govind S Mudholkar Control Procedures for Residuals Associated With Principal Component Analysis Technometrics Vol.21 No.3 1979 8 J Liu Khiang-Wee Lim R Srinivasan and X Doan On-Line Process Monitoring and Fault Isolation Using PCA Proceedings of the 2005 IEEE International Symposium on Mediterranean Conference on Control and Automation pp 658 661 2005 9 G Taguchi S Chowdhury and Y Wu The Mahalanobis-Taguchi System New York McGrawHill 2001 10 E B Martin A J Morris and J Zhang Process Performance Monitoring Using Multivariate Statistical Process CSontrol IEE Proc Control Theory Application Vol 143 No.2 March 1996 11 H Chen G Jiang C Ungureanu and K Yoshihira Failure Detection and Localization in Component Based Systems by Online Tracking KDD 2005 12 H Wang Z Song and P Li Fault Detection Behavior and Performance Analysis of Principal Component Analysis Based Process Monitoring Methods American Chemical Society Vol 41 pp 2455 2464 2002 13 H H Yue S.J Qin Reconstruction-Based Fault Identification Using a Combined Index American Chemical Society Vol 40 pp 4403-4414 2001 8 


BIOGRAPHY Sachin Kumar received the B.S degree in Metallurgical Engineering from the Bihar Institute of Technology and the M.Tech degree in Reliability Engineering from the Indian Institute of Technology Kharagpur He is currently pursuing the Ph.D degree in Mechanical Engineering at the University of Maryland College Park His research interests include reliability electronic system prognostics and health and usage monitoring of systems Vasilis Sotiris received the B.S degree in Aerospace Engineering from Rutgers University in New Brunswick New Jersey and the M S degree in Mechanical Engineering from Columbia University in New York He worked as a Systems Engineer for Lockheed Martin Corporation concentrating on software development projects for the Federal Aviation Administration He is currently pursuing the Ph.D degree in Applied Mathematics at the University of Maryland College Park His research interests are in the field of applied statistics and computational mathematics related to diagnostics and prognostics for electronic systems   _ n Acoustics an M.S in Electrical _  in Engineering Mechanics from the g  University of Wisconsin at Madison He is a Professional Engineer an IEEE Fellow and an ASME Fellow He has received the 3M Research Award for electronics packaging the IEEE Award for chairing key Reliability Standards and the IMAPS William D Ashman Memorial Achievement Award for his contributions in electronics reliability analysis He has written over twenty books on electronic products development use and supply chain management He served as chief editor of the IEEE Transactions on Reliability for eight years and on the advisory board of IEEE Spectrum He has been the chief editor for Microelectronics Reliability for over eleven years and an associate editor for the IEEE Transactions on Components and Packaging Technology He is a Chair Professor and the founder of the Center for Advanced Life Cycle Engineering CALCE and the Electronic Products and Systems Consortium at the University of Maryland He has also been leading a research team in the area of prognostics and formed the Prognostics and Health Management Consortium at the University of Maryland He has consulted for over 50 major international electronics companies providing expertise in strategic planning design test prognostics IP and risk assessment of electronic products and systems 9 


Engineering Education Annual Conference & Exposition 2005 3  Chong N., and M. Yamamoto Collaborative Learning Using Wiki and Flexnetdiscuss: a Pilot Study  Proceedings of the fifth IASTED International Conference on Web-based Education Puerto Vallarta, Mexico, Jan 23-25 2006 4  Engeström, Y. Learning by Expanding. OrientaKonsultit Oy, Helsinki, 1987 5  Forte A., and A. Bruckman From Wikipedia to the Classroom: Exploring Online Publication and Learning  Proceedings of the 7 th International Conference on Learning Sciences, Bloomington, Indiana, 2006, pp. 182-188 6  Grierson H., Nicol D., Littlejohn A., and A Wodehouse Structuring and Sharing Information Resources to Support Concept Development and Design Learning  Proceedin gs of the Networked Learning Conference, 2004 7  Gross Davis, B., Tools for Teaching, Jossey-Bass Publishers, 1993 8  Hon A. and W. Chun The Agile Teaching/Learning Methodology and its E-learning Platform Wenyin Liu, Yuanchun Shi, Qing Li \(Eds Advances in Web-Based Learning - ICWL 2004, Third International Conference, Beijing, China, August 8-11, 2004 9  Johnson, R. and D. Johnson An Overview of Cooperative Learning originally published in J. Thousand A. Villa and A. Nevin \(eds\ativity and Collaborative Learning, Brookes Press, Baltimore, 1994 10  Järvinen, E-M Education about and through Technology. In search of More Appropriate Pedagogical Approaches to Technology Education Acta Universitates Ouluensis, E 50. Oulu: Oulun yliopisto, 2001 11  Kim S., Han H., and S. Han The Study on Effective Programming Learning Using Wiki Community Systems Innovative Approaches for Learning and Knowledge Sharing, Springer Berlin, Vol. 4227/2006, pp 646-651 12  Koufman-Frederick, A., Lillie, M., PattisonGordon, L., Watt, D., and R. Carter Electronic Collaboration: A Practical Guide for Educators The LAB at Brown University, 1999 13  Leuf B., and W. Cunningham The Wiki Way Quick Collaboration on the Web Addison-Wesley, 2001 14  Mirijamdotter A., Somerville M. and Holst M An Interactive and Iterativ e Evaluation Approach for Creating Collaborative Learning Environments The Electronic Journal Information Systems Evaluation, Vol. 9 No. 2, 2006 pp. 83-92 15  Murugesan S Understanding Web 2.0 IT Pro July/August 2007 16  Parker K. and Chao J Wiki as a Teaching Tool  Interdisciplinary Journal of Knowledge and Learning Objects. Vol. 3, 2007 17  Patokorpi, E., Tétard F., Qiao F. and N. Sjövall Mobile Learning Objects to Support Constructivist Learning in Learning Objects: Applications, Implications and Future Directions, Koohang A. and K. Harman \(eds 2007 18  Poikela, E. and A.R. Nummenmaa Ongelmaperustainen oppiminen tiedon ja osaamisen tuottamisen strategiana Problem-based learning as a strategy for knowledge building\ Poikela, E. \(ed Ongelmaperustainen pedagogiikka teoriaa ja käytänt Problem-based pedagogy theory and practice\ Tampere Tampere University Press, 2002 19  Reinhold S., and D. Abawi Concepts for Extending Wiki Systems to Supplement Collaborative Learning in Technologies for E-Learning and Digital Entertainment: Proceedings of the First International Conference on Edutainment, Berlin : Springer, 2006, p. 755 767 20  Richardson, W Blogs, Wikis, Podcasts, and Other Powerful Web Tools for Classrooms Sage Publications: California, US; London UK; New Delhi, India 2006 21  Schaffert S., Bischof D., Bürger T., Gruber A Hilzensauer W., and S. Schaffert Learning with Semantic Wikis in First Workshop SemWiki2006 - From Wiki to Semantics, co-located with the 3rd Annual European Semantic Web Conference \(ESWC\ Budva, Montenegro 11th - 14th June, 2006 22  Schwartz L, Clark S., Cossarin M., and J. Rudolph Educational Wikis: Features and Selection Criteria  International Review of Research in Open and Distance Learning, Vol. 5, No. 1, 2004 23  Stvilia, B., Twidale, M. B., Smith, L. C., and L Gasser Assessing information quality of a communitybased encyclopedia in: F. Naumann, M. Gertz, S. Mednick Eds.\, Proceedings of the International Conference on Information Quality - ICIQ 2005, Cambridge, MA: MITIQ 2005, pp. 442-454 24  Tétard, F. and E. Patokorpi A Constructivist Approach to Information Systems Teaching Journal of Information Systems Education, 16\(2\05, pp. 167-176 25  Viégas F., Wattenberg M., Kriss J., and F. van Hamn Talk Before You Type: Coordination in Wikipedia  Proceedings of the 40th A nnual Hawaii In ternational Conference on System Sciences, 2007, p. 78 26  Wagner, C. and P. Prasarnphanich, P Innovating Collaborative Content Creation: The Role of Altruism and Wiki Technology Proceedings of the 40th Annual Hawaii International Conference on System Sciences, 2007 27  Wang C., and D. Turner D Extending the Wiki Paradigm for Use in the Classroom Proceedings of the International Conference on Information Technology Coding and Computing, 2004, p. 255 28  Wheeler S., Yeomans P. and D. Wheeler D. \(2008 The good, the bad and the wiki: evaluating studentgenerated content for collaborative learning in British Journal of Education Technology, 2008 29  Wiki EduTech Wiki retrieved April 9, 2008 from http://edutechwiki.unige.ch/mediawiki/index.php?title=Wiki oldid=17132   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


impodt Top Leve M d Ile Level Dom1ain Lave i 1po Scenwd1o Level Figure 8 Ilium Ontology Suite typical It is possible and in fact common to employ only a few of these ontologies in particular studies For example many studies are only concerned with physical platforms and therefore only need the MilAsset ontology which imports IlumAsset IliumFramework and DUL Figure 8 illustrates the existing Ilium Ontology Suite and a typical import pattern for a complex Modeling and Simulation application scenario Currently the illustrated suite less Millnfo MilComm and UAV defines 1240 OWL Classes 274 OWL Object Properties and 188 OWL Datatype Properties 6 AN EXAMPLE APPLICATION We are currently using the described approach to conduct systems requirements and other engineering analyses for military aerospace systems In these investigations it is useful to consider detailed aspects of the system design or software prototype behavior in the context of large scale network-centric military operations In the past year in particular we have assembled a collection of well known and widely accepted military simulations to prepare for an extensive investigation of requirements for autonomy in unmanned military platforms It is believed that useful insights in the study will be sensitive to important details of system behavior supporting sensor platform and communications technologies as well as the combined effects of these technologies on an advanced highly networked military force Thus the study environment must provide a means to reliably model and evaluate significant technical detail and to measure the effects as well as propagation of those effects throughout a broad operational context 11 


I r Z  Figure 9 A Military Modeling and Simulation Configuration of the Ilium Framework To do that we have assembled a collection of trusted military simulations that span the range of such applications from fine-grained simulations that focus on detailed interactions between two entities to coarse-grained simulations of military campaigns Figure 9 illustrates the Ilium Framework configured to accommodate those simulations as well as representative prototype systems In this case the legacy applications are simulations analysis systems and design tools Prototypes are notional UAV autopilots route planners decision support systems and similar applications The Framework augments these applications with control objects and agents that coordinate computations in the composed system and specialize agents that typically represent the characteristics of a new system or technology that cannot be easily or adequately simulated in any of the component legacy systems In our current study the following systems have Ilium Framework plugins and are used as components in the study environment 12 Al f<cOmmi.ain FIa 1A  


THUNDER is a campaign simulation that models national military forces and military operations that extend over months 30  Characteristics of individual systems are evaluated statistically and their effects on the overall campaign are not explicitly known Political social and cultural objects and concepts are not included in the model In this configuration THUNDER is used to generate force level tasking mission orders and to evaluate and adjust for the results of missions with respect to campaign plans SEAS is a force level simulation that simulates battles between major forces in combat operations that typically last for hours and as much as a day SEAS features a flexible rules based decision logic that can influence behavior at both the commander and individual combatant level SEAS executes the missions requested by THUNDER and provides a manageable dynamic context for examining the behavior of prototypes in a range of typical operational situations  31 A dynamic plug-in supports interaction with the Framework and other pugged-in components EADSIM is a trusted model of air defense systems that is in particular sensitive to many important design attributes of individual systems In certain modified forms it can reference advanced sensor and engagement decision models In our study configuration EADSIM simulates enemy air defenses in selected e.g significant to our investigation portions of the virtual battle A dynamic plug-in supports system control as well as interaction with the Framework and other pugged-in components A prototype aircraft mission planning system plug-in supports virtual real time mission plan creation and updates In particular the system provides automatic route planning for selected platforms that have been assigned missions by THUNDER and that are subsequently executed in simulation in SEAS and EADSIM The Ilium Framework itself provides software agents that are used to model notional or experimental UAV characteristics and behaviors Depending on the objectives of a particular study the Framework may also provide agents that address advanced Command and Control concepts to coordinate the interaction of the various component systems We maintain a semantic consistency among the plugged-in component applications by developing a single operational scenario as initial input for a study and deriving the necessary application configuration data from that source We create an RDF model of the scenario based on the Ilium suite of ontologies that includes political context issues objectives sensitivities etc military context centers of gravity campaign objectives etc geophysical environment military units order of battle unit equipment lists etc command and control assets platforms weapons ISR systems etc Figure 11 below is an excerpt from an operational scenario set in the Southwest U.S depicting a description of an Air Force Wing assigned to a notional Joint Task Force The Wing is based at Bishop Air Base and has six Fighter Squadrons assigned to it Additional detail about each of those squadrons as well as the base is found in the model in this case an rdf:resource  associated with it morg:AirForceWing rdf:ID="USAF_366 Air_Exp Wg dc:creator>Doug Holmes</dc:creator mgeo:basedAt rdf:resource="#BishopAB geo:positionedAt rdf:resource="#BishopAB_pos morg:assignedOrg rdf:resource="#USAF 390_Ftr_Sq morg:assignedOrg rdf:resource="#USAF_494_Ftr_Sq dc date I 0/20/07</dc date morg:assignedOrg rdf:resource="#USAF_496 Ftr_Sq rdfs:comment>366 AEW F-22 F-15 KC-135 130]</rdfs:comment morg:assignedOrg rdf:resource="#USAF 389_Ftr_Sq morg:assignedOrg rdf:resource="#USAF 391 Ftr_Sq morg:assignedOrg rdf:resource="#USAF 495_Ftr_Sq rdfs:label>366 Air Expeditionary Wing</rdfs:label morg:AirForceWing ab9 7P.b ii uI1 L L CFigure 11 An RDF model of a notional USAF Wing Figure 12 is another excerpt from the same scenario illustrating a model of a particular Fighter Squadron and one of the F-15E aircraft it operates Figure 10 Legacy Components in the Ilium Framework 13 suka 


morg:AirliorceSquadron rd:lD U SAF _8 htr Sq rdfs:label>8th Fighter Squadron</rdfs:label geo:positionedAt rdf:resource="#GeorgeAB_pos msim:hasSeasModel rdf:resource="#BAFGA mast:hasModel rdf:resource="#BAFGA rdfs:comment>An F15E Squadron</rdfs:comment dc:date>9/26/07</dc:date dc:creator>Doug Holmes</dc:creator mgeo:basedAt rdf:resource="#GeorgeAB morg:AirForceSquadron F1 5E rdf:ID="F1 5E 05 dul:isReferenceOfRealization rdf:resource="#AirObj ect_2007 mast:equipment-of rdf:resource="#USAF 7 Ftr_Sq dc:creator>Doug Hollmes</dc:creator dc:date>9/26/07</dc:date msim:hasSeasPlane rdf:resource="#F15E mgeo:basedAt rdf:resource="#GeorgeAB geo:positionedAt rdf:resource="#GeorgeAB-pos rdfs:label>F-15E 005</rdfs:label F-15E Figure 12 An RDF model of a Fighter Squadron and one of its aircraft Note that these models also have associated SEAS models that contain information peculiar to the SEAS simulation about these entities This information is used to configure SEAS to properly represent these particular entities We are also able to insert objects and information that may be of interest in our study that is not represented in any of the component simulations or other applications Where those objects are related to an object that is simulated that relationship permits inferences about the effects on them as a result of actions that are computed in a simulation For example it is possible to indicate that GeorgeAB defends the capital city and lend special significance to the actions of aircraft that are based there even though none of the simulations in the composite system have any notion of capital city Finally the operational scenario provides an explicit record of the assumptions that underly the study and can also include and explicit representation of system and study goals This practice improves analysis and may enable future knowledge based analytical tools Throughout the process of constructing the operational scenario and when it is complete we use one or more Description Logic Reasoners to ensure the logical consistency of the the model A number of these automatic theorem provers are freely available including Pellet 32 33 FaCT  34 and OWLIM 35 are used in the Framework to compute logical entailments and to complete RDF models as well as to ensure the consistency of models In the later capacity frequent checks will identify errors in the construction some e.g Pellet also indicate the source of the error and aid in repairing it As a result we are confident that the operational scenario is a sound model The primary use of the Reasoners allows us to significantly extend the explicit RDF model that is created For example a squadron is explicitly specified as assignedTo a wing and that relationship is the inverse of assignedOrg then the system can infer that the wing has the squadron as an assigned organization even though that fact has never been asserted Similarly if the same relationship is defined as a transitive relationship it is possible to infer that a flight that is assigned to the squadron is also assigned to the wing Once we have an operational scenario that has been classified by a Reasoner we then use the completed model to configure the composite system Ilium and the pluggedin systems to execute the simulation We use SPARQL a W3C standard query language designed to access RDFL to extract data from the scenario to create the input files needed to configure the various applications.[36 SELECT name pos long lat alt vis W1HLERE name rdf:type mast SeasLocation name geo:positionedAt pos pos pos:long long pos pos:lat lat pos pos:alt alt name msim:Visible vis  Figure 13 A typical SPARQL query In a similar fashion SPARQL is used to extract data from the scenario to create the necessary Ilium java surrogate control and agents SPARQL can also be used to review the scenario and answer questions that have arisen since the inception of the study At this point we are able execute the scenario with confidence that it will produce reliable results 7 CONCLUSIONS We have developed a methodology and supporting tools for creating an operational scenario that supports the semantic interoperability of an ad hoc collection of legacy applications and extends their capabilities The method depends on and ensures the logical integrity of the composite system Therefore when we assemble as a collection of legacy component systems that were not originally intended to interoperate with other systems we can be confidant that the composite system will produce consistent results Logical consistency implies that to the degree that we trust the interpretation underlying the model the results of operations on the model are trustworthy If for example the model is based on a Newtonian interpretation of physics the model ought to provide reliable answers to questions about automotive and even aeronautical engineering but probably not to all questions in astronomy or cosmology This methodology extends the utility of trusted simulations allowing integration of finegrained simulations that are sensitive to design requirements with high level coarse-grained simulations that are sensitive to acquisition issues and policy The potential benefits of 14 


this sort of interoperation may range from obvious production efficiencies to clearer insights into system requirements Finally an important side-effect of the approach is the OWL/RDF knowledge base formed by the combination of the operational scenario and the results of the operations of the legacy systems That knowledge base and the use of SPARQL queries and SWRL rules effectively expands the functionality of the system and greatly improves the analysis of the output of the system of cooperating simulations and tools We have prepared a foundation for the application of Semantic Web and other knowledge based tools in the analysis and design of unmanned systems We anticipate the development and application of these tools and the addition of autonomy directed Multiple Agent Systems MAS that use RDF/XML inter-agent communications in the coming year REFERENCES 1 NAFCAM 2001 Exploiting EManufacturing:Ineroperability of Software Systems Used by U.S Manufacturers available at 12 Protege Ontology Editor documentation available at http proteae.stanf6rd.edu 13 Top Braid Composer Datasheet available at 14 W and Nicola Guarino 2001 Support for Ontological Analysis of Taxonomic Relationships J Data and Knowled 39\(1 October 2001 15 Natalya F Noy and Deborah L McGuinness Ontology Development 10 1 A Guide to Creating Your First Ontology  Stanford University Stanford CA 94305 16 Alan Rector Modularisation of Domain Ontologies Implemented in Description Logics and related formalisms including OWL K-CAP'03 October 23-25 2003 Sanibel Island Florida USA pp 121-8 2003 17 Cyc Homepage available at htc.c 18 Open Cyc home page available at 19 SUMO Description and Home Page available at 19 SENSUS Description and Home Page available at 2 Bemers-Lee Tim Hendler Jim Lasilla Ora The Semantic Web Scientific American available at 20 DOLCE A Descriptive Ontology for Linguistic and Cognitive Engineering ontology and documents available at 3 Bemers-Lee Tim Blog on Design Issues available at 4 RDF Primer available at s chema/#ref-rdf-primer 5 Lacey Lee OWL Representing Information Using the Web Ontology Language Trafford Publishing 2005 6 OWL Web Ontology Language Guide available at Jten pHomewPage availal adt 7 Jena Home Page available at 8 Protege Home Page available at h1ttp  protegest nftanford.edu,X 1 9 Baader Calvanese McGuiness Nardi and PatelSchnieder Description Logic Handbook 10 Oberle Daniel Semantic Management of Middleware Springer 2006 OWL Web Ontology Language Guide available at 21 Nicola Gauarino Claudio Masolo Stefano Borgo Aldo Gangemi and Alessandro Oltramari Ontology Infrastructure for the Semantic Web Wonder World Deliverable DI 8 Laboratory for Applied Ontology Trento Italy 2001 available on line at ht X1t1 22 DUL.owl ontology available at www.1oa23t og DLl 23 Amy Knutilla Steven Polyak Craig Schlenoff Austin Tate Shu Chiun Cheah Steven Ray and Richard Anderson Process Specification Language An Analysis of Existing Representations NIST report available at http llwww.mel.nlist.gov/msid.librarZ/d.oc/psl-1 _.df 24 Process Specification Language Ontology available at http-//www,55,me l.nit gov/psl 25 Ontology for Geography Markup Language GML3.0 owl ontology available at ok i.cae.drexel.edu./-wbs/onltology/2004/09/ogc-gmI1 26 Ontology for Geography Markup Language GML3.0 of Open GIS Consortium OGC Home Page available at 15 


 Peter Maguire Using THUNDER for Campaign Studies DSTO-TN-0303 DSTO Melbourne August 2000 28 User Manual SEAS Version 3.7 U.S Air Force SMC/XR February 2007 29 Bijan Parsia and Evren Sirin Pellet and OWL DL Reasoner MINDSWAP Research Group University of Maryland College Park available at 30 Pellet Home Page available at 31 FaCT Home Page available at 32 OWLIM Home Page available at 3 A SPARQL Tutorial available at BIOGRAPHY Douglas Holmes is co-founder and Senior Partner of Java Professionals Inc In the past twenty-two years he has managed and participated in numerous artificial intelligence and knowledge-based programs for DARPA and other research agencies as well as commercial applications in the petroleum and other sectors He is currently developing ontologies and applying Semantic Web technology to support research and development of military unmanned systems He also has over twenty years experience as an Air Force Fighter Pilot and Fighter Weapons School Instructor Mr Holmes has a B.S in Mathematics and Basic Sciences from the U.S Air Force Academy and a M.S in Management Information Systems from Golden Gate University Richard Stocking is the lead Program Investigator/PM for Net Centric Operations Warfare Analysis efforts for Lockheed Martin Aeronautics Advanced Development Programs The Skunk WorksTM He is currently leading efforts researching autonomous UAV operations Current efforts include the integration of Multiple Agent Systems and other autonomy systems within the Ilium Framework He has over thirty years experience and over 11,000 flight hours with multiple C4ISR systems in the US Army and US Navy Mr Stocking has a M.S in Systems Technology from the Naval Postgraduate School 16 


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


