Institute of Manufacturing and controlling Wuhan University of Technology Wuhan P.R China dht_v@163.com;vvuliuvl'eblnaster\(fi]163.coln c!lchen\(fi]111ail vvhut.eclu.cn 
Abstract 
Design and Actualization of SOA-based Data Mining System Haitao Du Bo Zhang  Dingfang Chen 
Combining SOA Service-Oriented Architecture and the latest trend in date mining system development this paper explores the advantages of service-based Data Mining services and a framework for designing the data mining system based 
on SOA and also a way to set up this framework by the use of WCF Windows Communication Foundation is brought forward The detailed analysis into key technologies necessary for achieving this framework is made particularly in this paper These key technologies includes the implementation methods of the following services that are uploading big documents service data cleaning service data Mining algorithm service OLAP OnLine Analysis Processing service date management service based on metadata the 
realization of data warehouse service and OLAP service through programming with ADOMD.NET and how to make the solution of designing Data Mining system by using WCF 
Keywords SOA Data Mining WCF SQL SERVER 2005 1 Introduction These days the demand of companies for universal Business Intelligence is increasing dramatically The introduction of Business Intelligence system enables companies to better know the operating situation of business procedure and enhance the ability of 
departments to make decision at each level According to IDC estimation the market of BI and the application of it in financial performance and strategic management in Asia-Pacific Region excluding Japan will reach 724 million by 2010[1 Meanwhile enterprise information strategy is also turning to SOA in order to make better use of current IT resources adapt to more rapidly changes and larger development IDC predict that as companies are aware of combining IT and business procedure the use 
of SOA in Asia-Pacific Region will continue to grow up Chief Information Officers and IT management have already made it their priority to develop and obtain new products 978-1-4244-3291-2/08/$25.00 2512008 IEEE 338 integrate application programmes reinforce hardware infrastructures End-users also tell that applying SOA will result in optimized and clearer business procedure together with more effective management and supervision Taking the above two aspects into account this paper aims to design a kind of framework for 
Data Mining service and recommends a method of building Data Mining system with WCF 2 Brief introduction to Business Intelligence and SOA theory 2.1 Brief introduction to Business Intelligence and Data Mining BI Business Intelligence was initially put forward by Gartner Group in 1996 At that time it was defined as a collection of technologies and their applications which include Data Mining Data Duplicate and Recovery and are designed to help enterprises to make decisions As a key 
is a combination of Statistics database technology and Artificial Intelligence 1 2.2 Brief introduction to SOA theory W3C\(The World Wide Web Consortium defined SOA as a kind of application programme architecture in which each function is defined as independent 
technology of BI Data Mining is to extract potential unknown but very useful information modes or trends out of large database or data warehouse through a series of technologies that 
It 
is attached with clearly-defined interfaces and could be used in already defined order to form business proceedings SOA consists of three parts 3 Service Provider Service Broker and Service Requester respectively Service Provider offers individual service and responds to the requests for its service Service Broker registers Service Providers that have already issued catalogue them and provide searching function at the same time Service Requester looks for the service it needs through Service Broker and then uses the service 


3.2 Take prompt action To integrate business procedure and BI tools properly will certainly help users to act appropriately and react to information gained through integrative control panel report and warning system F or instance interactive control panel is able to display the conditions of KPI key Performance Indicators in which red lights indicate that some indicators haven't been fully met In the process of performing traditional manual business procedure BI tools can be used to help companies make wiser decisions For example they can choose proper suppliers through key business objectives such as lowering cost and raising customer satisfaction While in the situation where business procedure has already been automated to integrate BI and business procedure platform enables companies to analyze and make decisions as well as acquire necessary key business data 3.3 Ensure business objectives Fi Service Provider Bind FTPIHTTP/SMTP The framework for designing Data Mining system based on SOA Figure Figure 2   Sketch map of SOA 3 Advantages of structuring Data Mining system base on service Advantages of structuring Data Mining system base on service are mainly as follows 1 4 Service Requester 3.1 Better know the operating situation of business procedure It is a key objective for companies dedicated to SOA to analyze the performance and influence of performing business process in business process management Once business process data is captured in databases data warehouses or even in pure text documents BI tools are able to visit the data through control panel that can display information and further to integrate these data with others got from different channels This boosts a company's insight into business and enables the company to improve performance by proper decisions 4 The framework for designing Data Mining system based on SOA As Figure2 shows the system is constructed with four levels which are display level service level component level and data level each correspondent with user interface software service software component and entity class respectively Of the four levels software service is the main job to achieve the system According to the process of Data Mining it could be divided into four parts data uploading service data cleaning service mining Algorithm service and OLAP service 5 Interface User level interface Service Software level services Com pone Software nt level component Financial data analysis non-windows operating platform Entity class Data level 


Metadata management model HID Id NIO\(FK connection 5.2 Data warehouse and OLAP When setting up Data Mining service the definition of data warehouse could be achieved through ADOMD.NET 9.0 ADOMD.NET 9.0 is the component that SQL SERVER 2005 provides for users to visit Analysis Services service by coding sequentially operating data warehouse objectives and achieving targets of the operation of OLAP components The following function succeeds in visiting all dimensions of specific data warehouse with warehouse name and connection string as its parameters Component level mainly provides functional software components For example according to different data types in data cleaning service there are customer data cleaning components product data cleaning components and sale data cleaning components and so on 4.4 Data level Data level is a collection of entity class in Data Mining system Given different data types each data entity is defined according to users demand and configured by users according to business object in metadata-based manner 340 new    Public stringD GetDimensions\(string connectionString,string CubeName provides common Data Mining algorithms which are supported by SQL SERVER 2005 Analysis Services including Decision Tree Algorithm clustering algorithm Bayesian algorithms time-series algorithm related algorithms serial clustering algorithm neural network algorithms linear regression algorithm logistic regression algorithm and so on[6 4.2.4 OLAP services It provides users the service of on-line analyzing with data and enables them to analyse data by choosing customized Dimensions and measurement achieving drilling down and rolling up of data in order to find rules from the curves charts tables or dashboards 4.3 Component level 5 Key Technologies and Implementation Following technologies need paying attention to when implementing Data Mining system based SOA 5.1 Data management based on metadata Data Dictionary managing metadata consists of DataN odes and DataProperties DataN odes are linked into a tree as a whole through ParentId DataProperties are used to describe the properties of DataNodes in which entity and field are defined in correspondence with DataNodes NodeType is used to tell whether DataN odes is to describe a entity or the field of entity The data model of metadata management is shown as Figure 3 3 new string[rowcount for\(int i=O;i<rowcount;i NodeType PrODertyType Name value Figure  P...",s..-re..-I1tl..--.\267\267\267..-d  4.1 Interface level Interface level provides interface for users to interact with the system Since SOA is applied the interface is various depended on different clients called by data mining service As a result this level may include web browser E-mail systems windows operating platforms Linux operating platforms and non-windows operating platforms as well as non-Linux operating platforms 4.2 Service level 4.2.1 Data uploading services It enables users to upload self-defined documents which may include Excel documents flat documents like text documents Access files and data documents exported from DBMS 4.2.2 Data cleaning services It cleans the data uploaded by users through SSIS SQL Server Integration Services functional component that SQL SERVER 2005 provides and allows users to design the data warehouse to their needs 4.2.3 Data Mining algorithm services AdomdConnection AdomdConnectionO connection ConnectionString conn ec tion String connection open 0 int rowcount connection Cubes CubeName}.Dimensions Count strDimensions It 225 l  225.\225\225 


achieves WCF services like metadata model definition of date warehouse authorization and certification defmition of data mining procedure and so on 4 achieves metadata and entities logic S OM Web Seeurityl S OM Wei 18 0 M WO_OwClleotl contains all client agent of WCF service  341 which is used to definite the various processes of data mining achieves the SmartClient-based Workflow Designer 5.4 Solution in WCF WCF is a SOA-based NET framework released by Microsoft WCF previous called Indigo is the common framework that is built through managed code and is to operate Service Oriented application programs 6 It enables developers to build a secure reliable and transactional multi-platform solution which is compatible with currently running system WCF is a successful combination of Microsoft distributive application programs development incorporating all the technologies relevant to distributive system under NET platform such as Net Remoting ASMZ WSE and MSMQ In terms of Communication range it could be inter-process inter-machine inter\255 subnet inter-intranet or even Internet In terms of Host program it could use ASP.NET EXE WPF Windows Forms NT Service and COM as its Host WCF supports protocols including TCP HTTP inter-process and custom protocols it supports many kinds of standards and safe modes such as SAML Kerberos X509 User/Password custom and so on That is to say all constituents related are included in framework of WCF so the development of SOA-based distributive system is more easily The Data Mining system solution created in WCF is shown in Figure4 This solution includes subjects such i CustomerID long key Gender text discrete Income long continuous MemberCard text discrete predict Purchase table  ProductName text key Quantity long continuous using Microsoft_Decision_Trees DMWeb DM WebSecurity DM WorkjlowClient DMDataMode realizes various service of data mining such as data uploading service data cleaning service mining Algorithm service and OLAP service and so on contains interface between business logic and page logic like Web pages and user input check return strDimensions,o connection close 0 DMRuntime.Services Create Mining model MenmberCard_Prediction Caption T oStringO,o DMServices.Proxy  Apart from this ADOMD.NET also support MDX enquiry to obtain CellSets 5.3 Data Mining algorithm OLE DB for DM specifically dermes enquiry language for Data Mining DMX Through the use of MD X sentences it is easy to achieve the three important steps of Data Mining which are Model Creation Model Training and Model Prediction Meanwhile MDX is simple in terms of grammar similar to operating data in a relation data table The following codes achieve the Data Mining Model using Decision Tree Algorithm This model uses Gender Income and a series of goods customers purchase and their quantities to predict MenmberCard OM 225\225 _  Prosy is used to realize custom Provider and so on 8 strDimensions i connection Cubes CubeName].Dimensions       S I Figure as"DM.Web","DM WebSecurity","DM WorkjlowClienf DMServices.Proxy DM.Business.Services DMRun time.Services","DMDataModef DMBusiness.Services  ii _     Solutions of date minning-based item inWCF 


Combining SOA and new natures in BI development this paper explores the advantages of SOA-based Data Mining system and framework for designing it and makes detailed analysis into key technologies necessary for achieving this framework In the long term SOA\255 based Data Mining system is bound to become a new application mode for BI in companies which is defined as Saas This paper is supported by National Key Technology R&D Program during the 11 th Five-Year Plan Period Key Technology and Platform of the Integrated Management 2006BAH02A06 China Academic Journal Electronic Publishing House China June 2007 pp 284-286 4 Liu Shuiping Han Weihong Liu Jiahong and Yin Gang Reaserch on a Framwork of Application Integration Based on SOA 7 Acknowledgement Wuhan March 2006 pp 3-6 3 Hou Zhanwei Mo Lin Zhen Hua and Zhai Haixia Research and Design of DatabaseMiddleware Based on SOA C Services Computing A Dissertation Submitted to Academic Degree Evaluation Committee of Wuhan University of Technology for Doctoral Degree of Philosophy in Engineering Academic Journal Electronic Publishing House China 2006 pp 199-201 5 Krogdahl P Luef G and Steindl 6 Conclusion 2005 IEEE International Conference onVolume 2 11-15 July 2005 Page\(s 100 vol.2 6 ZhaoHui Tang Jamie MacLennan Microelectronics and Computer Data Mining with SQL Server 2005 Professional WCF Programming.NET Development with the Windows Communication Foundation References International Workshops Springer Verlag Wiley publishing 10475 Crosspoint Boulevard pp 365-398 7 Scott Klein Application Research of Computers 342 1 Yang Jae-Min Kim Jong-Geun and 1m Jong-In Information security and consideration in building internet data centers for pervasive environment ApWeb/WAIM 2007 Heidelberg D\255 69121 Germany Huang Shan China Jun 16-182007 2 Li Cunrong Knowledge Extraction and Application of Production Manufacturing Information Wiley publishing pp 47-58 


The robustness of the authentication interface to brute force attacks depends on the risk that the Attacker would be able to guess the correct set of symbols to be entered to authenticate. The robustness against such attacks increases with the number of symbols to be entered at the interface r and with the number of symbols in the set m E The usability of the scheme decreases with both of these measures because a larger number of authentication symbols takes longer to enter and also leads to higher error rates. A large number of symbols in m E also leads to longer authentication times and to higher error rates The robustness of the scheme is reduced if the set of valid entered symbols m E is a large fraction of m E because the chance of a random choice from m E to be in m E increases with the number of elements in m E Conversely, a larger m E leads to higher usability, because the User is more likely to locate an acceptable symbol if m E includes more choices  4. Theoretical framework for assessing robustness to eavesdropping attacks  4.1. Basic assumptions  In this work we make the basic assumption that the security of the authentication scheme is entirely based on the shared secret; there is no security through obscurity Kerkhoff's Principle\The Attacker knows exactly what the alphabet of symbols  A the maximum length of the shared secret p  the operation of the authentication process on the Verifier side, and has full access to the User input ample space to store several authentication sessions as well as full access to extensive computing power to process the stored session data. Any interface parameters that the attacker does not already know can be assumed to be part of the shared secret any for example, in the 2000 Hopper and Blum work, the shared secret would include both secret vectors, the secret and the noise generator  We also assume that the Attacker is able to distinguish between successful and unsuccessful authentication sessions. A protocol where a valid User is subject to random false positives and false negatives would greatly complicate the Attacker s work, but would also confuse and frustrate the User Two other important issues have been analyzed extensively elsewhere: whether the User is able to recall the shared secret \(Brostoff and Sasse, 2003 and whether the User is able to determine the correct entered symbols \(i.e., what is a reasonable cognitive load\. We do not make further mention on either of these two issues in the remainder of the paper  4.2. Formal results  In this section we develop the theoretical framework that quantifies the usability-security tradeoff. We define the false positive error as the probability that an attacker is able to randomly guess the shared secret or the entered symbol successfully The term does not imply that the interface will mistake an incorrect entered symbol as correct  Lemma 1  Given a single session of communication between the User and the Verifier, where the User proves her identity to the Verifier with probability of false positive error not to exceed P given a known set of all possible shared secrets S the Attacker is  Figure 3. As an attacker observes two or more authentication sessions, the space of possible shared secrets that could have led to the observed entered symbols shrinks. Eventually, the attacker is able to find out the shared secret as the intersection of all sets of possible shared secret values  E e 1 S R 1 s  S m1 S m2 R 2 e 2 E m1 E m2 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


able to narrow down the location of the shared secret to a set with   S P  012 elements on average   S  denotes the number of elements in the set S   Proof  The probability of a false positive error in identifying the User is the probability that the User has randomly chosen a symbol from m E that happens to be in the set of valid symbols m E or equivalently a shared secret that happens to map into m E for a given session with randomness factor  The probability of this happening is       m m m E E S S P        On the other hand, having observed the entered symbol e the attacker is able to infer that the shared secret is in the set             m m E s e S s S of all possible shared secret values that map on the actual entered symbol. The space of possible values for the shared secret is thus reduced from the set S to the subset m S with     S P S m  012   elements   The result of the lemma above is true on average In general, the number of elements in the set m S will vary depending on the entered symbol e some entered symbols will correspond to more possible values of the shared secret than other entered symbols, e.g. Sobrado and Birget, 2005\or those relations that map each entered symbol to the same number of possible shared secrets, the result of the lemma will be exact, not just on average Another important observation is that the proof of the lemma is valid even if the entered symbol includes random user errors, as proposed in Hopper and Blum \(2000\ the protocol is such that the authentication allows for up to Z random errors among the elements of the entered symbol e this will result in a larger set m S Even if the user actually makes fewer errors in a given session, the set m S will necessarily include all the possible shared secrets that might match all but Z of the elements of the entered symbol e At the same time, the Verifier must be ready to accept as valid sessions with up to Z errors hence the probability of a false positive will also increase. The proof above does not need to assume that all the elements of the entered symbol e are correct, but only that the authentication is successful hence the lemma holds for any tolerance to errors Z   Lemma 2  Given k sessions of communication between the User and the Verifier, where the shared secret remains unchanged for all k sessions; given that in each session the User proves her identity to the Verifier with probability of false positive error not to exceed P given a set of possible shared secrets S the Attacker is able to narrow down the location of the shared secret to a subset with   S P k  012 elements  Proof  To conduct such an attack, the Attacker needs to capture and store the set of entered symbols j e and the randomness factors j  for each authentication session j of the k sessions captured. Let  j m S be the sets that map onto each of the entered symbols j e  The shared secret is in each of the  j m S sets, hence it will also be in the intersection     k j E s e S s S j j m j k j j m 1       1         Based on Lemma 1, the probability that the shared secret is in any given set  j m S is equal to the false positive probability P Because the randomness factor is different for each session, the sets  j m S are independently random distributed, so the probability that the shared secret is in the intersection of  j m S  will be the product of the probabilities that that shared secret is in each of the sets, i.e k P Hence the intersection set has   S P k  012 elements   Theorem 1  To gather sufficient information to uniquely determine the shared secret, the Attacker needs to capture      P S k o log   log sessions is the ceiling function, returning the smallest integer greater than the argument of the function  Proof  The proof is trivial when applying Lemma 2 The average number of possible shared secrets that match all the observed entered symbols over o k  captured sessions is 1     012 S P o k hence the actual shared secret can be determined exactly   The theorem above describes how many sessions the Attacker must capture to collect sufficient information to uniquely determine the shared secret It makes no claim about whether the Attacker has sufficient computing power at her disposal to actually determine the shared secret Most authentication schemes are based on NP complete or NP hard problems with a search space that is exponential with respect to the length of the shared secret. On the other hand, if the attacker has Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


collected the needed number of sessions and if she has sufficient computing power to perform an exhaustive search in a reasonable amount of time, the difficulty of solving the problem is irrelevant We can now use the framework above to evaluate authentication interfaces proposed in the literature. For any of the proposed interfaces to be secure, they need to exhibit either a sufficient computational complexity or to require the capture of a sufficiently large number of sessions \(or both  5. Application of theory to reported authentication schemes  In this section we analyze a few particular cases of authentication procedures. We first evaluate how much computing power an attacker is likely to have available, then discuss the robustness of the proposed protocols in view of the theory we presented. We consider both the number of observations the scheme is able to withstand, as well as the time required for an exhaustive search of the space of shared secrets  5.1. What is sufficient computing power  An attacker having sufficient computing power to conduct exhaustive searches over the shared secret space is able to circumvent the NP hardness of the one-way mapping. Fortunately for the Attacker, the problem of searching for the shared secret values that could have led to a set of observed entered symbols is by its nature highly parallel, and can be farmed out to an army of computers in an efficient manner. The amount of data to be sent to the worker nodes in such an army would include the definition of the alphabets A and B and the list of observed entered symbols and randomness factors for the 5-10 eavesdropping sessions captured, a very small data set Such a brute force attack is greatly aided by technological progress. The computing power of devices is increasing as the cost of computing devices is decreasing. Even more, by using self replicating virus programs to take over unprotected computers attached to the Internet, Attackers can marshal the resources of armies of computer zombies to carry out nefarious activities for almost no cost to the Attacker In 2002 a 64 bit RC5 key was cracked in 1757 days in a distributed manner using donated processing power \(equivalent to 32,504 800MHz Apple PowerBook G4\g to Distributed.net \(2007 With a typical CPU speed of 1 GHz, an army of 10,000 zombies, assuming an admittedly optimistic processing speed of one shared secret evaluation per CPU cycle, today s Attackers would be able to process 18 10 shared secrets per day. The next section considers what the implications of this processing power are on several protocols proposed to date  5.2. Recommendations based on theoretical findings  As discussed in the introduction, the shared secret can be protected either by not disclosing sufficient information about it, or by making the shared secret computationally difficult to find from the disclosed information. Table 1 below summarizes results on the two considerations. To be able to realistically hide the shared secret, the schemes would need to withstand hundreds of repeated observations, for example in the case of daily use over a period of a few months, the standard recommended lifetime of a shared secret. None of the protocols reported to date are able to withstand more than ten observations without disclosing the full information about the shared secret to the Attacker Regarding the computational complexity, all but the Hopper and Blum scheme allow the Attacker to conduct a fast search of all the possible shared secrets. Armed with all the information about the shared secret and able to conduct an exhaustive search in less than an hour, the Attacker can uniquely locate the shared secret for all but the Hopper and Blum scheme. For this last scheme, the attacker is still able to capture all the information needed to identify the shared secret, but the complexity of the Reference or description  of possible shared secrets Probability of false positive of sessions needed to capture shared secret Time needed for exhaustive search Strong password  traditional N=10 4 10 1 6  1 N 1 0 Matsumoto, 1996 27 1/3 3 0 Sobrado and Birget, 2005 10 13 0.001  5 1 second Strong password  ideal  10 16 0.001  6 15 minutes Hopper and Blum, 2000 10 25 0.001  10 30,000 years No value given: using a conservative P 0.001 A strong password requires at least one digit, one symbol and one upper case letter Assuming some to be discovered eavesdropping-resistant scheme with excellent user friendly capabilities  Table 1. Robustness of authentication interfaces to eavesdropping attacks. A scheme is secure if the number of session needed to be captured and the time to perform an exhaustive search exceeds the attacker s resources Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


search space will prevent her from actually locating the shared secret within a reasonable timeframe As illustrated in Table 1, the main result of the paper is that the security of the schemes is in the complexity of the space of shared secrets S not in information hiding. Information that can be gathered in fewer than ten repeated observations is not well hidden. Regardless of the type of one-way mapping the User s credentials can be easily compromised if the Attacker can complete an exhaustive search of the space of shared secrets S within a reasonable time frame. On the other hand, if S is sufficiently complex to prevent a brute force search in a reasonable amount of time, the Attacker is unable to determine the shared secret even though she may have all the information needed to uniquely identify it Another important result is that the amount of information transmitted by the User to the Verifier in any given session has no ultimate implications on the ability of the scheme to withstand repeated observations by the Attacker. A user could be asked to make very simple decisions \(binary choices select items among a large number of possibilities An interface with increased complexity of choice \(in the m E space\will be more resistant to guesses, but will not be more resistant to repeated eavesdropping attacks. The added complexity in m E  leads to a lower probability of false positive, but will also disclose the shared secret in fewer observations. Although, this appears to pose a valuable tradeoff, a closer analysis shows that this tradeoff is very shallow. For example decreasing the acceptable probability of false positives from 0.001 to 0.0001 in the example above will disclose a strong password in only 4 observations instead of 6. An Attacker who can capture 4 sessions will be equally able to capture 6 sessions if needed. Hence the tradeoff between probability of error and robustness to eavesdropping is not worth making. The best approach is to have the lowest probability of false positives, limited by the usability of the interface \(usability introduces a tradeoff that is worth considering, because increased complexity in the m E space leads to exponentially decreasing usability With these two conclusions in mind, we propose the following guidelines for devising an interface resistant to eavesdropping 1  Use a space of possible shared secrets as complex as the user can handle. The space should include at least 22 10 possible shared secrets \(tens of years of computing time for an exhaustive search 2  Use a one-way mapping as simple as possible. This mapping does nothing to hide information from the Attacker, so it can be designed only to maximize usability 3  Use an interface that requires as much identifying information as possible in one session, limited by the user s cognitive and speed abilities. The higher the amount of information exchanged, the lower the probability of false positives. The Attacker will receive more information in each captured session, but she will not be able to exhaustively search the space of shared secrets to make use of this information, if the space S is complex enough  Even by following the guidelines above, it is unlikely that the resulting scheme will be truly user friendly. All of the schemes analyzed above have serious usability issues, because they make the authentication process more complex and significantly more time consuming than the traditional passwords; yet they all provide minimal gains in robustness. To transcend the low usability and the low security a paradigm shift is needed There are three ways to allow both robustness to attacks and more user friendly interfaces 1  Invent a new type of interface that would allow the User to handle a much more complex space of shared secrets, without much more cognitive and memory effort than the current password scheme OR 2  Invent a new type of interface that would prevent the Attacker from using computing power to search the space of shared secrets Such schemes intended to prevent automated agents from impersonating human users have been used in other types of applications \(see Completely Automated Public Turing Test to Tell Computers and Humans Apart www.captcha.net R 3  Design interfaces that combine the shared secret with specialized hardware or biometrics. The added computing power that the User can carry in hardware tokens or in out of band authentication devices is a way to counteract the extensive computing power the Attacker has at her disposal. This can include the use of one-time passwords \(e.g transmitted over a cell phone or generated on-the-fly in a smart token  While we await innovations in 1 and 2 above, the only feasible solution for now is to focus on combing shared secrets with specialized hardware \(tokens, out of band devices or biometrics\s we mentioned before, this has the drawback of higher costs and more limited availability. Based on the theoretical Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


framework we developed in this paper, the approach of combining only shared secrets and standard hardware is simply unable to deliver both resistance to eavesdropping attacks and reasonable user friendliness at the same time  6. Conclusion  We presented a theoretical framework that evaluates the robustness of protocols intended to be resistant to eavesdropping attacks. Such protocols can attempt to either not disclose information about the User s shared secret, or to make it difficult to locate the shared secret, even after collecting sufficient information about it. After reviewing protocols reported in the literature, we conclude that even the most robust protocols \(those to the point where usability is a concern\ are only able to withstand ten observations before the attacker has sufficient information to uncover the shared secret On the other hand, authentication interfaces that use complex shared secrets could be secure even if the Attacker has captured all the needed information to determine the shared secret.  Having collected the required number of sessions, the Attacker will now be limited by computing power, if the space of shared secrets is complex enough. The drawback of complex shared secrets is the exponentially decreasing usability of the resulting interface. As such, interfaces that are both secure and usable can only be devised by combining shared secrets with other authentication technologies \(biometrics or specialized hardware\ multiple factor authentication systems  7. References  APWG. \(2008\Phishing Activity Trends, January 2008. Retrieved June 11, 2008, from www.apwg.org/reports/apwg_report_jan_2008.pdf  Bedworth, M. and Allison, C. \(2008\periments with a Visual Probabilistic One-Time Password Authentication System Proc. SAM 2008   Brostoff, S., & Sasse, M. \(2000\re Passfaces more usable than passwords? A field trial investigation Proc. HCI 2000 pp. 405-424 Springer Verlag  Brostoff, S. and Sasse, M.  \(2003\"Ten strikes and you're out": Increasing the number of login attempts can improve password usability CHI 2003 Workshop on Human-Computer Interact. and Security Systems   Distributed.net. \(2007\Project RC5.  Retrieved Jan 27, 2008, from http://www.distributed.net/rc5  Hoanca, B. and Mock, K. \(2007 Phishing Attacks and Countermeasures: Implications for Enterprise Information Security in D. Khadraoui and F Herrmann \(Eds.\dvances in Enterprise Information Technology Security. IGI, Hershey, PA  Hopper, N. and Blum, M.  \(2000 A Secure HumanComputer Authentication Scheme CMU Tech Report CMU-CS-00-139  Kumar, M., Garfinkel, T., Boneh, D., and Winograd T.  \(2007\. Reducing shoulder-surfing by using gazebased password entry Proc. SOUPS 07 vol. 229 ACM Press, New York, NY, 13-19  Matsumoto, T. \(1996\uman-computer cryptography: An attempt 3rd ACM CCCS pp. 6875, New Delhi, March 1996  Naor, M.  and Pinkas, B. \(1997\isual authentication and identification. In Proc. Advances in Cryptology pp. 322 336  Ross, B., Jackson, C., Miyake, N., Boneh, D. and Mitchell, J.C. \(2005\ Stronger Password Authentication Using Browser Extensions, in Proc 14th Usenix Security   Roth, V., Richter, K., and Freidinger, R. \(2004 PIN-entry method resilient against shoulder surfing In Proceedings of the 11th ACM CCS 04 ACM Press, New York, NY, 236-245  Sobrado, L. and Birget, J.-C. \(2005 Shoulder surfing resistant graphical passwords Retrieved January 27, 2008, from http://clam.rutgers.edu/~birget/grPssw/srgp.pdf  Sobrado, L. and Birget, J.-C. \(2002\raphical passwords The Rutgers Scholar vol 4, 2002 Retrieved January 25, 2008 at http://rutgersscholar.rutgers.edu/volume04/sobrbirg/s obrbirg.htm  Tan, D. S., Keyani, P., and Czerwinski, M. \(2005 Spy-resistant keyboard: more secure password entry on public touch screen displays. In Proc  19th CHISIG ACM International Conference Proceeding Series, vol. 122, 1-10  Tari, F., Ozok, A. A., and Holden, S. H. \(2006 comparison of perceived and real shoulder-surfing risks between alphanumeric and graphical passwords Proc SOUPS '06 vol. 149. ACM Press, New York NY, 56-66  Wiedenbeck, S., Waters, J., Sobrado, L., and Birget J. \(2006\esign and evaluation of a shouldersurfing resistant graphical password scheme Proc AVI '06 ACM Press, New York, NY, 177-184 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


Test Environment 


Implant Application 1 Low level analog signal Stimulation Sensor interface Signal digitized and formatted Transmitter Application  receiver controller with MAC RF encoded packets of data Data packets organised into function Change response or cause direct stimulation 


Product Design and Manufacture 225 Work with end product manufacturer Each design is unique 225 No off the peg solution 225 Thorough testing during product development 225 Work with component suppliers 225 World class manufacturing capability 225 Post manufacture test and burn in 225 Reliable parts shipped on time NO SURPRISES NO CHANGES NOTHING EXCITINGI 


Module Encapsulated ZL70101 Crystal Matching network Antenna connection 


References 225 Krauss J D Antennas 2 nd Edition McGraw-Hili 1988 225 Yang G-Z Body Sensor Networks Springer 2005 225 Higgins H Implant Communication Made Real Body Sensor Networks Conference 2007 225 Sivard A et al Challenge of Designing In-body Communications Embedded System News 2004 225 Higgins H Human Body Implant Communication Making it Possible European Conference on Antennas and Propagation 2007 225 Hodgins D et al Healthy Aims Developing New Medical Implants and Diagnostic Equipment IEEE C5 Pervasive Computing January March 2008 225 Higgins H Implant Communications Out of the Lab and Into Patients lET Body-Centric Communication Conference London April 20 2009 225 Higgins H Body implant Communications Is It a Reality Antennas and Propagation for Body\255 Centric Wireless Communications lET/loP London April 24 2007 225 Higgins H Radio Frequency Technology and In-Body Communication Systems Implantable and Body Centric Conference Imperial College London 2005 225 Higgins H In-body Communications the Challenges and The Opportunities COST 2005 225 Rahmat-Samii Y and Kim J Implanted Antennas in Medical Wireless Communications Morgan  Claypool 2006 225 The Antenna Book 20th Edition American Radio Relay League ARRL 255 Main Street Newington CT,USA 225 Fujimoto K et al Small Antennas Research Studies Press 1967 225 Bancroft R Microstrip and Printed Antenna Design Noble Publishing 225 http://www.zarlink.com/zarlink/hs/82_ZL70101.htm 


Conclusions 225 Communication with very small implants is possible 225 The human body can both help and hinder communication 225 Power consumption is an issue for long term implants 225 The communication system antenna and implant should be designed together 225 There is no one size fits all solution 225 Transmission of data to and from an implant is practical and is being done and is making a difference to the lives of real patients 


Thank you Questions 


 18  Astronautical Congress Valencia, 2006 27  Bu reau  In tern atio n a l d e s Po ids et Mesures. \(2 008  August\SI Base Units. [On http://www.bipm.org/en/si/base_units   B IOGRAPHY  Author, Karl Strauss, has been employed by the Jet Propulsion Laboratory for over 22 years.  He has been in the Avionics Section from day One.  He is considered JPL\222s memory technology expert with projects ranging from hand-woven core memory \(for another employer\o high capacity solid state designs.  He managed the development of NASA\222s first Solid State Recorder, a DRAM-based 2 Gb design currently in use by the Cassini mission to Satu rn and the Chandra X-Ray observatory in Earth Orbit.  Karl was the founder, and seven-time chair of the IEEE NonVolatile Memory Technology Symposium, NVMTS, deciding that the various symposia conducted until then were too focused on one technology.  Karl is a Senior IEEE member and is active in the Nuclear and Plasma Scie nce Society, the Electron Device Society and the Aerospace Electronic Systems Society Karl is also an active member of SAE Karl thanks his wonderful wife of 28 years, Janet, for raising a spectacular family: three sons, Justin, Jeremy Jonathan.  Karl\222s passion is trains and is developing a model railroad based upon a four-day rail journey across Australia\222s Northern Outback   


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


